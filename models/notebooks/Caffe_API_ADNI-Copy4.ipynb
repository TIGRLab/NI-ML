{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(1)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "import h5py\n",
    "import numpy as np\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "# Some defs to load data and extract encodings from trained net\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    #print net.blobs.items()[0]\n",
    "    #print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        #print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        #print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        #print net.blobs[input_node].shape\n",
    "        \n",
    "        data_layers.append(net.blobs[input_node])    \n",
    "        #print 'X_list shape: {}'.format(X_list[i].shape)\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "    \n",
    "     \n",
    "    net.reshape()            \n",
    "    #print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        #print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas_bl  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas_bl,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1, concat_param=dict(axis=1))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas_bl)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas_m12)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas_bl,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.adas_m12,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT,n.adas_bl  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_CT,n.adas_bl,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas_bl)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas_m12)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas_bl,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.adas_m12,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT_unified(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_HC_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_HC_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_HC_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_HC_CT,n.adas_bl,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_HC_CT, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=4, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.adas_bl,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas, n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "#     n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,n.ff1, concat_param=dict(axis=1))\n",
    "    #n.concat = L.Concat(n.X_L_HC,n.X_R_HC,n.X_CT, concat_param=dict(axis=1))\n",
    "    \n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=lr['COMB']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.dropC1 = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2) #This is done automatically by caffe! \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        #n.ff2_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2ADAS13 = L.ReLU(n.ff2_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        #n.ff2_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2MMSE = L.ReLU(n.ff2_MMSE, in_place=True)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        n.ff1_DX = L.InnerProduct(n.ff3, num_output=node_sizes['DX_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1DX = L.ReLU(n.ff1_DX, in_place=True)\n",
    "        \n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas_bl,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.adas_m12,loss_weight=tr['MMSE'])  \n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_DX = L.InnerProduct(n.ff1_DX, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.DX_loss = L.SoftmaxWithLoss(n.output_DX, n.dx_cat3,loss_weight=tr['DX'])  \n",
    "    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_CT, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.dropC1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "        \n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_R_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_L_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))       \n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='HC_CT': #multimodal AE - experimental stage\n",
    "        HC_node_split = 0.8\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_L_HC = L.InnerProduct(n.X_L_HC, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.9*node_sizes['En1'])))        \n",
    "        n.NLinEn_L_HC = L.ReLU(n.encoder_L_HC, in_place=True)\n",
    "        \n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_CT = L.InnerProduct(n.X_CT, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.1*node_sizes['En1'])))        \n",
    "        n.NLinEn_CT = L.ReLU(n.encoder_CT, in_place=True)\n",
    "        \n",
    "        #Concat         \n",
    "        n.encoder1 = L.Concat(n.encoder_L_HC,n.encoder_CT, concat_param=dict(axis=1))\n",
    "        \n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers (or common encoder layers for multimodal) \n",
    "    \n",
    "#     n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.Sigmoid(n.encoder2, in_place=True)\n",
    "    #code layer\n",
    "#    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='xavier'))  \n",
    "    \n",
    "#     n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "    \n",
    "    #Decoder layers\n",
    "    if modality == 'CT':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)                    \n",
    "    \n",
    "    elif modality =='R_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    elif modality =='HC_CT':        \n",
    "        n.decoder_L_HC = L.InnerProduct(n.code, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_L_CT = L.ReLU(n.decoder_L_HC, in_place=True)\n",
    "        n.decoder_CT = L.InnerProduct(n.code, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_CT = L.ReLU(n.decoder_CT, in_place=True)\n",
    "        \n",
    "        n.output_L_HC = L.InnerProduct(n.decoder_L_HC, num_output=node_sizes['out_HC'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_HC = L.SigmoidCrossEntropyLoss(n.output_L_HC, n.X_L_HC)\n",
    "        \n",
    "        n.output_CT = L.InnerProduct(n.decoder_CT, num_output=node_sizes['out_CT'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_CT = L.EuclideanLoss(n.output_CT, n.X_CT)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    return n.to_proto()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.01 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode):\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 20\n",
    "    \n",
    "    if multimodal_autoencode:\n",
    "        train_loss_HC = zeros(niter)\n",
    "        test_loss_HC = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_loss_CT = zeros(niter)\n",
    "        test_loss_CT = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "        for it in range(niter):            \n",
    "            solver.step(1)  # SGD by Caffe \n",
    "            train_loss_HC[it] = solver.net.blobs['loss_HC'].data\n",
    "            train_loss_CT[it] = solver.net.blobs['loss_CT'].data\n",
    "            \n",
    "            if it % test_interval == 0:                        \n",
    "                t_loss_HC = 0\n",
    "                t_loss_CT = 0                                \n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()   \n",
    "                    t_loss_HC += solver.test_nets[0].blobs['loss_HC'].data\n",
    "                    t_loss_CT += solver.test_nets[0].blobs['loss_CT'].data\n",
    "                \n",
    "                test_loss_HC[it // test_interval] = t_loss_HC/(test_iter)\n",
    "                test_loss_CT[it // test_interval] = t_loss_CT/(test_iter)\n",
    "                print 'HC Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_HC[it], np.sum(train_loss_HC)/it, test_loss_HC[it // test_interval])\n",
    "                print 'CT Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_CT[it], np.sum(train_loss_CT)/it, test_loss_CT[it // test_interval])\n",
    "                \n",
    "        perf = {'train_loss':[train_loss_HC, train_loss_CT],'test_loss':[test_loss_HC,test_loss_CT]}\n",
    "    else: \n",
    "        \n",
    "        #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "        # losses will also be stored in the log\n",
    "        test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "        if not multi_task:\n",
    "            train_loss = zeros(niter)\n",
    "            test_loss = zeros(int(np.ceil(niter / test_interval)))  \n",
    "\n",
    "        else: \n",
    "            if multi_label:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_DX_loss = zeros(niter)\n",
    "                test_DX_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "            else:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_MMSE_loss = zeros(niter)\n",
    "                test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "\n",
    "        #output = zeros((niter, batch_size))\n",
    "        #solver.restore()\n",
    "        #the main solver loop\n",
    "        for it in range(niter):\n",
    "            #solver.net.forward()\n",
    "            solver.step(1)  # SGD by Caffe    \n",
    "            # store the train loss\n",
    "            if not multi_task:\n",
    "                train_loss[it] = solver.net.blobs['loss'].data        \n",
    "            else: \n",
    "                if multi_label:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_DX_loss[it] = solver.net.blobs['DX_loss'].data\n",
    "                else:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "            # store the output on the first test batch\n",
    "            # (start the forward pass at conv1 to avoid loading new data)\n",
    "            #solver.test_nets[0].forward()\n",
    "            #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "            # run a full test every so often\n",
    "            # (Caffe can also do this for us and write to a log, but we show here\n",
    "            #  how to do it directly in Python, where more complicated things are easier.)\n",
    "            if it % test_interval == 0:        \n",
    "                t_loss = 0\n",
    "                t_ADAS13_loss = 0\n",
    "                t_MMSE_loss = 0\n",
    "                t_DX_loss = 0\n",
    "                correct = 0\n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()                \n",
    "                    if not multi_task:\n",
    "                        t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                        if multi_label:\n",
    "                            correct += sum(solver.test_nets[0].blobs['output'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "                    else: \n",
    "                        if multi_label:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_DX_loss += solver.test_nets[0].blobs['DX_loss'].data\n",
    "                            correct += sum(solver.test_nets[0].blobs['output_DX'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "\n",
    "                        else:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "                if not multi_task:\n",
    "                    test_loss[it // test_interval] = t_loss/(test_iter)\n",
    "                    if multi_label:\n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                    print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                        it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval], test_acc[it // test_interval])\n",
    "                else:\n",
    "                    if multi_label:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_DX_loss[it // test_interval] = t_DX_loss/(test_iter) \n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'DX Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                            it, train_DX_loss[it], np.sum(train_DX_loss)/it, test_DX_loss[it // test_interval],test_acc[it // test_interval])\n",
    "                    else:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_MMSE_loss[it // test_interval] = t_MMSE_loss/(test_iter)             \n",
    "\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "        if not multi_task:\n",
    "            perf = {'train_loss':[train_loss],'test_loss':[test_loss],'test_acc':[test_acc]}\n",
    "        else:\n",
    "            if multi_label:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_DX_loss],'test_loss':[test_ADAS13_loss,test_DX_loss],'test_acc':[test_acc]}\n",
    "            else:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "                \n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,snap_prefix):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    \n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    #s.solver_type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 100000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    #s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 4000\n",
    "    s.snapshot_prefix = snap_prefix\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MC # 1, Hype # hyp1, Fold # 1\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold1/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (226.974349976,inf), test loss: 154.638035202\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (435.407409668,inf), test loss: 302.458047485\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (76.3124160767,105.13537294), test loss: 74.7853755951\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (173.390563965,231.003189339), test loss: 213.017954826\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (113.058197021,74.8041616797), test loss: 42.6358439445\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (346.049346924,174.478488691), test loss: 97.8918009758\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (39.4443283081,61.0762187789), test loss: 33.9176275253\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (28.7140235901,135.867761672), test loss: 60.8324968338\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (22.5468540192,54.0369088354), test loss: 42.1266312122\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (46.3162765503,115.612861936), test loss: 72.8321567535\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (39.0344276428,49.759690564), test loss: 39.9151038647\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (74.8684844971,103.305211585), test loss: 72.6128342152\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (44.7654075623,46.8052631756), test loss: 33.9435112953\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (33.9413223267,94.9344026349), test loss: 58.0412024021\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (36.7490310669,44.5964040786), test loss: 41.630074501\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (36.4559593201,88.8555776971), test loss: 73.4996584892\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (51.888343811,42.8894494168), test loss: 36.9111636162\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (125.284912109,84.2194655551), test loss: 69.2760797501\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (25.9351387024,41.5064452039), test loss: 35.5613570213\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (36.5346374512,80.5365526955), test loss: 59.8481289864\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (80.2989425659,40.3602900594), test loss: 41.1277885437\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (227.543029785,77.5446478602), test loss: 72.6063791275\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (32.4500312805,39.3718217065), test loss: 35.6649392605\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (27.7357997894,74.991181778), test loss: 60.8263486862\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (20.0139503479,38.5083933155), test loss: 35.8862359047\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (36.080291748,72.7903293624), test loss: 59.2390554428\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (34.7986984253,37.7607746957), test loss: 41.0557081699\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (66.3699874878,70.8675365314), test loss: 70.6399878502\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (33.2282066345,37.0807121405), test loss: 33.5068690777\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (29.9686985016,69.1469013925), test loss: 56.9920446873\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (32.9931755066,36.4472401629), test loss: 35.8820076466\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (39.0150146484,67.600548741), test loss: 58.3091305733\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (43.8203811646,35.8685412592), test loss: 39.5411428452\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (99.7020721436,66.2026859486), test loss: 67.9294560432\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (20.6132888794,35.3313655608), test loss: 32.319377327\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (26.0807304382,64.9257657489), test loss: 54.8949326515\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (57.3599128723,34.8346469368), test loss: 36.7700193882\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (183.52444458,63.7643170033), test loss: 58.8416848183\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (28.1388092041,34.3641128299), test loss: 37.4801728487\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (24.3885002136,62.6649796419), test loss: 67.5268369675\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (20.1825752258,33.9204401995), test loss: 31.2453963041\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (29.821182251,61.6405246959), test loss: 54.338915205\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (24.3106613159,33.5107232938), test loss: 37.0016040564\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (33.501209259,60.688801213), test loss: 61.9450753212\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (24.733417511,33.1186906903), test loss: 35.6663907528\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (28.357755661,59.7854958267), test loss: 66.0950356483\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (26.6262664795,32.736598297), test loss: 29.3578289509\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (39.1121177673,58.9290687112), test loss: 51.7124150276\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (36.2406272888,32.3739521574), test loss: 37.6368697166\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (81.9151153564,58.1196421797), test loss: 65.0434731007\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (9.67085075378,32.0268232984), test loss: 34.5123232603\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (11.3726854324,57.3522355276), test loss: 64.5590743542\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (47.9769477844,31.6980027879), test loss: 29.7225029945\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (162.540374756,56.6336441348), test loss: 49.9949543953\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (26.54829216,31.3802288691), test loss: 37.7069744825\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (43.1073188782,55.936099567), test loss: 66.4430610657\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (21.555606842,31.0751469222), test loss: 32.6014997959\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (25.156583786,55.2666247521), test loss: 62.0523417473\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (19.0105934143,30.7892584962), test loss: 32.6094688892\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (17.7175598145,54.634009869), test loss: 53.1283601761\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (19.5774040222,30.513548305), test loss: 37.5746753931\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (25.2785930634,54.0239506463), test loss: 66.2294206619\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (22.7473068237,30.242114632), test loss: 31.3290615559\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (33.0218048096,53.4362894421), test loss: 53.5478954792\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (30.7029151917,29.9822213279), test loss: 33.607303381\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (64.2074127197,52.8732586321), test loss: 54.2523055077\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (9.40479850769,29.7313229713), test loss: 37.7155265331\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (9.55118370056,52.3295984952), test loss: 65.5145682335\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (42.0905532837,29.4940033862), test loss: 30.7018517017\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (159.188446045,51.8055761414), test loss: 52.2523313522\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (24.6783027649,29.263354593), test loss: 34.363886404\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (57.8894271851,51.2859565204), test loss: 54.9805588245\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (20.1281852722,29.0405831769), test loss: 36.7323222637\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (19.6226100922,50.7778902903), test loss: 64.4239919424\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (16.418050766,28.8309094954), test loss: 30.5470861435\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (15.6625671387,50.2937307786), test loss: 52.0649726868\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.7991514206,28.6274535405), test loss: 35.5350533962\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (21.6398925781,49.824753436), test loss: 56.1752445221\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (20.8832740784,28.4254477835), test loss: 35.4816847086\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (27.9722080231,49.3708638746), test loss: 65.3846673965\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (19.6821689606,28.2301787487), test loss: 30.1229722977\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (30.5276222229,48.9347384732), test loss: 52.7065826893\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (10.3625545502,28.0394489856), test loss: 35.7188074827\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (8.95266914368,48.5136325355), test loss: 61.7423043251\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (39.6900291443,27.8574042809), test loss: 33.5980924368\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (133.336303711,48.1176914808), test loss: 62.646865654\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (17.7296028137,27.6768658792), test loss: 28.7061512709\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (54.4044265747,47.7247299154), test loss: 50.675934124\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (18.8965129852,27.5018698981), test loss: 36.4673647404\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (13.4693126678,47.3425825986), test loss: 63.0946194887\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (11.0602989197,27.3356459153), test loss: 32.4451123714\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (11.6279325485,46.9779402592), test loss: 61.1045826674\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (13.9745922089,27.1736019883), test loss: 29.1241288424\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (19.6290740967,46.6227947693), test loss: 49.3687230587\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.166469574,27.0118502231), test loss: 36.3561413765\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (24.7057514191,46.2765608567), test loss: 64.2562807083\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (11.7436218262,26.8546299864), test loss: 31.4878728867\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (12.6597013474,45.9412650095), test loss: 59.7101907253\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.6756744385,26.7002607418), test loss: 31.9581212997\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (11.499704361,45.6150478497), test loss: 52.4454912663\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (29.3015727997,26.5523069334), test loss: 36.1714121819\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (107.443817139,45.3065119334), test loss: 63.1150934219\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (12.2856483459,26.4049341721), test loss: 29.9842324495\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (46.3561668396,44.9976858005), test loss: 51.2625212669\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (19.9193687439,26.2618755534), test loss: 32.743176651\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (14.0512428284,44.6956170626), test loss: 53.2178713799\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.3266077042,26.1251457388), test loss: 36.2927741528\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (13.2051181793,44.4054333558), test loss: 62.701941061\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (15.358622551,25.9912409967), test loss: 29.8095289469\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (18.7388095856,44.1208077694), test loss: 51.5482076168\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.7031364441,25.8572973563), test loss: 33.2364834785\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (21.3237457275,43.8420104857), test loss: 53.7976013184\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (11.6161956787,25.7263788555), test loss: 34.914654088\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (9.44136619568,43.5704267427), test loss: 62.3379359961\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (8.81554412842,25.5971547622), test loss: 30.0075420141\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (13.9419002533,43.3046336103), test loss: 51.8867630005\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (25.0088195801,25.472869193), test loss: 34.8151345491\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (101.865371704,43.0519227794), test loss: 54.9180968285\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (12.9026250839,25.3487222626), test loss: 34.3382655144\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (41.2728004456,42.7975578061), test loss: 63.0962872505\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.6032924652,25.2277042034), test loss: 29.4063921213\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (13.2944192886,42.5477411115), test loss: 51.606788826\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (12.4060058594,25.1115857319), test loss: 34.9642484188\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (14.8555669785,42.3063877751), test loss: 61.1604013443\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (18.881067276,24.9975336944), test loss: 32.7619760036\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (13.9910306931,42.0685896248), test loss: 61.5916091919\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (14.2268009186,24.8830302216), test loss: 28.0562413216\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (21.0517349243,41.8348728704), test loss: 49.6989510536\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (9.8146648407,24.770757736), test loss: 35.5160752773\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (7.62329864502,41.6061533447), test loss: 61.8775974989\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (5.79758453369,24.6595556364), test loss: 31.5684123516\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (15.7912082672,41.3814839235), test loss: 60.2613859177\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (21.5007266998,24.5521802015), test loss: 28.9142613411\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (83.2498092651,41.1669074255), test loss: 49.2699304819\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (15.7265348434,24.4447476703), test loss: 35.3512034893\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (40.9259643555,40.9500364997), test loss: 62.4521079063\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (19.7741909027,24.3396442971), test loss: 30.6036481142\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (10.0542497635,40.7363601904), test loss: 56.3302287817\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (12.8166732788,24.2384265882), test loss: 31.567106843\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (14.6635951996,40.5290759152), test loss: 51.7025537491\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (19.0897636414,24.138792333), test loss: 34.936560154\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (8.44278717041,40.3240865962), test loss: 61.3626698494\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.8479089737,24.0385793582), test loss: 29.3831353903\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (20.391708374,40.1221901364), test loss: 51.0897269249\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (8.73316669464,23.9400206431), test loss: 32.1157526731\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (7.11276578903,39.9238651188), test loss: 52.7154977798\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (10.179854393,23.8422320285), test loss: 35.4451501369\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (20.3528366089,39.7285903947), test loss: 62.0702821732\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (20.7594337463,23.7472521852), test loss: 29.2125015736\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (55.8855895996,39.5411316082), test loss: 50.3074238062\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.7274646759,23.6521601621), test loss: 32.762970686\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (37.8464736938,39.3511695954), test loss: 53.3695176125\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (17.6532859802,23.559015963), test loss: 33.9776175976\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (9.17158222198,39.1636216445), test loss: 60.6372310162\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (12.9913768768,23.4689021152), test loss: 29.3196896553\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (15.9491405487,38.980962402), test loss: 50.5838879585\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (14.3500614166,23.3800528984), test loss: 34.0346662521\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (5.30950403214,38.7998549529), test loss: 54.5021881104\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.2010269165,23.2905967025), test loss: 33.5191954613\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (15.1766872406,38.6210799479), test loss: 62.2419043541\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (8.73144435883,23.2024449048), test loss: 28.9390282154\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (6.16213321686,38.4449218556), test loss: 51.4909330845\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (11.3371601105,23.1147614532), test loss: 34.2467781067\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (19.3643722534,38.2711097846), test loss: 60.9077785015\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (19.5158042908,23.0292820089), test loss: 32.0979872465\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (47.7986717224,38.1035809715), test loss: 59.9405198574\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (12.1676559448,22.9436638695), test loss: 27.6648215055\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (34.0629501343,37.9334438106), test loss: 48.8884927273\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (17.0243682861,22.8596592962), test loss: 35.1971940994\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (14.6283168793,37.7653424198), test loss: 61.5949193001\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (13.1081008911,22.7780439651), test loss: 30.4578444004\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (14.7537021637,37.6007562908), test loss: 58.0940209866\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (14.1785793304,22.6974348884), test loss: 28.8988081932\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (4.92497634888,37.4373278616), test loss: 50.0806218147\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (8.40054512024,22.6162192075), test loss: 34.6682473183\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (10.0957336426,37.2757646206), test loss: 62.1094283104\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (8.85220623016,22.5359507793), test loss: 30.2431821346\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (5.97875499725,37.116167186), test loss: 54.5038949966\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (10.9449577332,22.4560318112), test loss: 31.1625666618\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (25.8588542938,36.9585133421), test loss: 51.8180586815\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (20.256362915,22.3778681054), test loss: 34.6691618919\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (45.7055435181,36.8057350187), test loss: 60.6366051674\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.2058515549,22.2994882665), test loss: 28.9903342247\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (31.3340530396,36.6506205062), test loss: 50.3499802589\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (14.8710460663,22.2224311898), test loss: 31.949782753\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (15.2838888168,36.4971017438), test loss: 52.8471888542\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (12.8948516846,22.1474335206), test loss: 35.3147278309\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (12.4751625061,36.3463312592), test loss: 61.6060929298\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.9855928421,22.0731522274), test loss: 29.0870104074\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (7.74893569946,36.1964737944), test loss: 50.5746195793\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (7.47678565979,21.9983469317), test loss: 32.5112587929\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (5.7320728302,36.0479518598), test loss: 53.6998153687\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (9.6937122345,21.9242765231), test loss: 33.5396038055\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (8.47884178162,35.9010845601), test loss: 60.5713690758\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (10.0970277786,21.8504101985), test loss: 29.3551314354\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (27.6502532959,35.7557391129), test loss: 51.1783679962\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (19.4675140381,21.7779300748), test loss: 34.1837351084\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (44.5153274536,35.6142976901), test loss: 55.0199267864\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (12.3433990479,21.7053099597), test loss: 32.8433725834\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (20.4532012939,35.4708138158), test loss: 60.9874350548\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (15.5213699341,21.6337789129), test loss: 28.5546261787\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (17.5280246735,35.3285697515), test loss: 50.8076889038\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (11.8108596802,21.5639115378), test loss: 34.4964009285\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (11.0493183136,35.188425888), test loss: 61.1439041138\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (11.0532341003,21.494665276), test loss: 31.792591095\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (11.5002040863,35.049089183), test loss: 60.1483898163\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (6.70714330673,21.4249562687), test loss: 27.5766209602\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (4.39619255066,34.9107116745), test loss: 49.1376947403\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (8.88569545746,21.3557957797), test loss: 34.9222894192\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (8.60869312286,34.7737199958), test loss: 62.246953392\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (11.3652029037,21.2867915009), test loss: 30.2289023876\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (28.0734558105,34.6379797534), test loss: 58.6308791637\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (14.0194835663,21.2188758806), test loss: 29.4179955006\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (21.1848545074,34.5054901213), test loss: 51.3900468349\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (10.8650894165,21.1508385166), test loss: 34.6660183907\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (13.0606460571,34.3711171784), test loss: 62.3593719006\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (29.369638443,21.0840201545), test loss: 29.9441577911\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (28.100353241,34.2378677969), test loss: 52.0893404961\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (11.9750175476,21.0180731652), test loss: 31.6193295002\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (11.5625400543,34.1060020452), test loss: 52.5352131367\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (10.601568222,20.9528524964), test loss: 34.5349272251\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (11.2238950729,33.9748843433), test loss: 61.1824884415\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (5.3532948494,20.8872676753), test loss: 29.1517015934\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.65196871758,33.8445933562), test loss: 51.0735940933\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (7.91733932495,20.8220934019), test loss: 31.830132103\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (7.32110881805,33.7154216185), test loss: 53.6512533188\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (12.0654277802,20.757037369), test loss: 35.0749814987\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (27.8714008331,33.587366579), test loss: 62.0512818336\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.73189640045,20.6928552466), test loss: 29.028203392\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (10.8192386627,33.4620065821), test loss: 50.1287936211\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (10.6607580185,20.6285531987), test loss: 32.9947029591\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (10.6595926285,33.3349114196), test loss: 54.7913051605\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (33.1148986816,20.5654226076), test loss: 33.2230490685\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (28.6186523438,33.2088272156), test loss: 59.9797656059\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (11.0780258179,20.5028143405), test loss: 29.1055961609\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (10.3588447571,33.0837565325), test loss: 50.4278292179\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (9.88288211823,20.4408907106), test loss: 34.2209634542\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (10.1129341125,32.9593398185), test loss: 56.0671212673\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.29448318481,20.3786840786), test loss: 32.6629527092\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.36420106888,32.8356841528), test loss: 61.732639122\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 2\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold2/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (318.836914062,inf), test loss: 229.056659698\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (379.436889648,inf), test loss: 251.077330017\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (74.7189941406,151.424762828), test loss: 97.3961215973\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (108.5834198,160.517142128), test loss: 119.243434238\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (18.9003391266,104.198355438), test loss: 45.007930088\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (56.5015716553,117.185234935), test loss: 74.4509161949\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (32.0943641663,82.5535447156), test loss: 36.8386326313\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (42.2274398804,98.0490745012), test loss: 60.7210864067\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (52.0555686951,71.4838767052), test loss: 41.0174098969\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (58.2279281616,88.0916547967), test loss: 70.2935037613\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (40.5850219727,64.7606562866), test loss: 38.0042204857\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (44.3585243225,82.0028815414), test loss: 68.788369751\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (40.8004798889,60.239363207), test loss: 36.3190609932\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (69.7643051147,77.8144360902), test loss: 57.9932649612\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (116.985687256,56.9355638252), test loss: 40.3220381737\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (88.7365188599,74.6678576941), test loss: 70.8573690414\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (6.10362291336,54.3708469869), test loss: 37.9281367302\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (6.91436338425,72.1879739122), test loss: 61.4146232605\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (34.6121635437,52.3323211586), test loss: 36.9180332184\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (39.9686203003,70.1756527002), test loss: 59.4716691017\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (26.5385360718,50.660413187), test loss: 40.2223728657\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (18.9789886475,68.4810450092), test loss: 69.2357691765\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (28.0825099945,49.2446060928), test loss: 34.8956475258\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (34.9391860962,67.0197249204), test loss: 56.433484745\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (66.1417312622,48.0244145528), test loss: 37.3244670868\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (100.524978638,65.7248921232), test loss: 58.5505353928\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (7.43121528625,46.9542977488), test loss: 37.3166472435\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (7.19216060638,64.57345853), test loss: 64.3983675003\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (39.6315116882,46.0034773042), test loss: 32.2952033043\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (30.4213047028,63.4992288836), test loss: 53.2266116142\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (72.5890655518,45.1469148931), test loss: 36.0794939995\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (99.169708252,62.5288414202), test loss: 61.9017512321\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (43.2137908936,44.367774603), test loss: 33.3307459831\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (47.5437774658,61.6127489707), test loss: 60.4245386124\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (45.2297401428,43.6506000627), test loss: 29.6695573807\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (26.4667701721,60.7461852382), test loss: 47.8159723282\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (30.7366390228,42.9674644497), test loss: 35.6918308735\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (69.9238891602,59.9265500168), test loss: 62.8261676788\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (21.815284729,42.3319293633), test loss: 31.9167550564\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (25.0454902649,59.1445314947), test loss: 52.2337672234\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (35.9937667847,41.7390481374), test loss: 31.9905620098\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (40.6003875732,58.4077529174), test loss: 51.8350866318\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (10.2953414917,41.1713217893), test loss: 35.1097024441\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (21.0109996796,57.7028974728), test loss: 60.9233353615\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (57.4164199829,40.6393637937), test loss: 29.6204969883\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (176.446655273,57.0454974631), test loss: 48.2875567436\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (23.1317749023,40.1257285944), test loss: 32.7869002819\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (19.6800136566,56.3965795561), test loss: 51.9514757156\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (25.1474971771,39.6380750193), test loss: 32.6065365314\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (24.1983947754,55.774450134), test loss: 57.898088932\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (13.0927066803,39.1701688756), test loss: 27.9036489725\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (17.4927787781,55.1869489549), test loss: 47.1649461746\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (37.2357330322,38.7223536659), test loss: 31.3920226574\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (47.695892334,54.6204767348), test loss: 50.2159257889\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (18.2886047363,38.2825390513), test loss: 29.3947199821\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (20.2578868866,54.0686317281), test loss: 56.2453368187\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (27.4273033142,37.8447003665), test loss: 25.0733206511\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (32.6610641479,53.5402624178), test loss: 44.3751256943\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (25.5029144287,37.4228273642), test loss: 31.152807498\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (23.4370613098,53.0277402268), test loss: 57.8371925831\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (16.3010101318,37.0196918615), test loss: 27.5501780212\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (42.2374992371,52.54118105), test loss: 55.1733505249\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (35.7447280884,36.6311076944), test loss: 26.8916108489\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (41.3892326355,52.0728072263), test loss: 47.4049623013\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (40.2333946228,36.2607462182), test loss: 29.5598225951\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (79.1494293213,51.6363423929), test loss: 56.5011787891\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (36.4236831665,35.8967437572), test loss: 26.3098334134\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (24.9773540497,51.1986628868), test loss: 46.1867396355\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (5.84852981567,35.5444816909), test loss: 28.1768541574\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (12.2558307648,50.7783157275), test loss: 48.4767822981\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (23.0030021667,35.2089822004), test loss: 29.6819170773\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (18.5186653137,50.3807374746), test loss: 55.533826828\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (40.6182174683,34.8884775431), test loss: 25.8852616251\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (46.968963623,49.9968710254), test loss: 45.575526619\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (9.98337554932,34.5760391227), test loss: 29.0195466042\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (31.0870513916,49.6191971083), test loss: 48.7397801161\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (18.0572357178,34.2720397912), test loss: 28.0728779078\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (26.0791511536,49.2556394752), test loss: 54.5921221733\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (22.8999252319,33.9788400225), test loss: 24.3004832149\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (23.1342735291,48.9007047624), test loss: 44.2873235941\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (9.52369976044,33.6937395076), test loss: 28.5896805763\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (43.9081878662,48.5603574667), test loss: 53.9383061409\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (3.55055308342,33.4153604147), test loss: 25.661338377\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (9.60199451447,48.2276297402), test loss: 52.4512081385\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (12.530008316,33.1451497229), test loss: 24.1192654848\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (30.1204376221,47.9173139795), test loss: 42.1375096083\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (24.4600486755,32.8777330069), test loss: 28.0295648575\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (67.3692932129,47.6041008725), test loss: 54.7757293701\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (21.8899917603,32.6148854029), test loss: 25.0477610826\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (112.337890625,47.3004988422), test loss: 45.2563462257\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (46.0067214966,32.3616004226), test loss: 26.5164052486\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (68.8472595215,47.0060487142), test loss: 46.5707941055\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (13.8222827911,32.1141395977), test loss: 28.2377456188\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (15.7541818619,46.7181494746), test loss: 53.8692035913\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (17.3548774719,31.8726032414), test loss: 24.7230824351\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (50.4470558167,46.4356798411), test loss: 44.3972407818\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.8129014969,31.6363002207), test loss: 28.1401832104\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (15.2319068909,46.1585073229), test loss: 47.8575205326\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (9.83840370178,31.4065850662), test loss: 27.1349304914\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (27.1314716339,45.8873447741), test loss: 53.4606842518\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (16.9994621277,31.1825772975), test loss: 24.1907087505\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (23.9152030945,45.6240085068), test loss: 44.9520498753\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (8.25135231018,30.964927223), test loss: 27.9205791354\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (35.0452423096,45.3656633478), test loss: 53.2831731796\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (18.7938079834,30.7555070806), test loss: 24.5936314225\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (15.2406349182,45.1213153214), test loss: 50.8040634632\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (18.21210289,30.549565144), test loss: 22.7285983324\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (20.8216629028,44.8741332664), test loss: 40.8925038815\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (23.9350185394,30.348654933), test loss: 28.0613257766\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (21.2489871979,44.6327443823), test loss: 53.584104538\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (26.8131542206,30.1556844544), test loss: 24.6345364153\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (39.0905380249,44.3976371371), test loss: 50.2653258324\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (48.2249298096,29.9675035074), test loss: 25.9622870445\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (44.3634300232,44.166019099), test loss: 45.3584212303\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.5739631653,29.7810447298), test loss: 26.9402031422\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (10.8438682556,43.9360646792), test loss: 52.5768414021\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (17.2986106873,29.5986335528), test loss: 24.2318211555\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (25.2416877747,43.7109939163), test loss: 42.8398637295\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (16.3054733276,29.4201881809), test loss: 26.8181920528\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (27.7840194702,43.4895603598), test loss: 46.5767611504\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.7939224243,29.244923482), test loss: 27.1656190872\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (21.2715053558,43.2723500261), test loss: 50.1726467133\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (12.7954349518,29.0736427347), test loss: 24.8398855686\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (48.9513626099,43.060059744), test loss: 44.1487496376\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (5.7704076767,28.9076228966), test loss: 27.7067982197\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (21.9642028809,42.855908732), test loss: 46.5011322975\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (16.0706672668,28.7440874279), test loss: 25.4412402391\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (14.6293287277,42.6494073871), test loss: 50.5777413368\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (50.9646759033,28.5846894774), test loss: 23.0972774506\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (58.1830444336,42.4487973987), test loss: 42.1515359879\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (31.0061588287,28.4290873446), test loss: 27.4014264822\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (32.0300750732,42.2502216449), test loss: 51.3256326675\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (26.7122459412,28.2772645974), test loss: 24.4443666935\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (11.8941373825,42.0542067677), test loss: 49.4109564304\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (8.14708423615,28.125876617), test loss: 24.0371835113\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (18.1933937073,41.859999821), test loss: 41.2799332142\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (7.75248622894,27.9777852242), test loss: 27.3506931782\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.49949073792,41.668527232), test loss: 52.924447155\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (11.9991703033,27.8324618054), test loss: 24.8564629555\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (12.0753135681,41.4801101374), test loss: 44.185296917\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (8.36289787292,27.6890304886), test loss: 26.3725345016\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (13.4728784561,41.2943867953), test loss: 45.82061553\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (49.6093521118,27.5496309466), test loss: 27.3750539303\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (139.164154053,41.1160176143), test loss: 51.6867391586\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (13.3933935165,27.4117671329), test loss: 24.399651444\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (9.66520690918,40.9361977181), test loss: 43.3007459164\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (10.1847524643,27.2765781391), test loss: 27.8994658172\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (15.7893199921,40.7580650935), test loss: 46.7325800419\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.0540084839,27.1445312666), test loss: 26.2374196291\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (24.4755764008,40.5845235562), test loss: 50.6366823196\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.1617412567,27.0148947808), test loss: 23.820923084\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (24.6376743317,40.4117143677), test loss: 42.9566725731\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (16.1471881866,26.888169567), test loss: 27.7105098486\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (14.6702270508,40.2405007752), test loss: 51.8742759705\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (19.5525360107,26.7615739368), test loss: 23.913380301\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (34.9580993652,40.071476856), test loss: 48.7842803955\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (29.2881984711,26.6373977016), test loss: 23.0328950763\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (26.1713314056,39.9034319666), test loss: 40.6308112621\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.60522842407,26.5142980926), test loss: 27.9869306564\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (25.2571525574,39.7377753468), test loss: 53.0959597588\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (29.6033802032,26.3935273606), test loss: 24.4838227749\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (49.0886993408,39.5746360386), test loss: 42.7916717052\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (24.8129959106,26.2750181873), test loss: 26.2307034492\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (70.6298980713,39.4174682561), test loss: 45.037364769\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (14.1316843033,26.1574514749), test loss: 27.3168972731\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (19.0200138092,39.2564745694), test loss: 51.4496311188\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.10545921326,26.0418154722), test loss: 24.1516546249\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (6.77294063568,39.0977321574), test loss: 42.4079115868\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (9.74172306061,25.9287897403), test loss: 27.3916791439\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (11.6832733154,38.9429819911), test loss: 46.436262846\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (39.2191619873,25.8181272797), test loss: 26.8104937553\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (39.5739898682,38.7890968202), test loss: 50.8509042263\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (12.200378418,25.7082358623), test loss: 24.6910530567\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (20.907497406,38.6350518552), test loss: 43.2637643814\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (24.6663894653,25.599246094), test loss: 27.9251567364\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (13.8622121811,38.4831162212), test loss: 46.4906642437\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.086725235,25.4913182592), test loss: 25.5309022188\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (27.0206203461,38.3320536133), test loss: 50.2219704628\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (7.97243881226,25.3843808813), test loss: 23.7271281719\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (18.1203651428,38.1823416422), test loss: 42.758523941\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (4.97162151337,25.2791381972), test loss: 28.0494980335\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (16.0859394073,38.0346417339), test loss: 52.2167153835\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (9.33389186859,25.1755850371), test loss: 24.2147951007\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (9.28543949127,37.8916136938), test loss: 47.9434453964\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (20.3840827942,25.0729248642), test loss: 25.2070695877\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (21.1289253235,37.7456737203), test loss: 43.7543157101\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (13.797580719,24.9712094929), test loss: 27.5961554289\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (72.3189163208,37.6021606779), test loss: 52.7854414463\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (39.1538619995,24.8722482674), test loss: 24.4147591352\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (35.9471054077,37.4600215631), test loss: 42.6696209431\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (17.5798416138,24.7740049879), test loss: 27.0619808912\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (17.587726593,37.3187346112), test loss: 46.5565234184\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (15.3618040085,24.6767366578), test loss: 28.0468147993\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (37.740272522,37.178017196), test loss: 52.5499967575\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (10.7229824066,24.5800770519), test loss: 24.9790544152\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (18.651008606,37.0382154516), test loss: 43.3895113945\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (12.7594928741,24.4841999605), test loss: 28.4074210167\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (9.20382881165,36.8990725625), test loss: 47.3165492535\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (7.61494016647,24.3889133889), test loss: 26.7061079979\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (23.4885044098,36.7614722668), test loss: 50.9362512112\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (7.09105205536,24.294985312), test loss: 24.0111647129\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (25.7506923676,36.6251769039), test loss: 42.3954337597\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (11.6330499649,24.2023521945), test loss: 28.4002016068\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (14.5377225876,36.4923093997), test loss: 51.9434511185\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (9.2800579071,24.1103531194), test loss: 24.1765174389\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (31.3932476044,36.3575680754), test loss: 48.3819005013\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (10.9212417603,24.0189926395), test loss: 24.0652393341\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (24.8608798981,36.2241523216), test loss: 41.0161873341\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (10.8566007614,23.9298770016), test loss: 28.1401401043\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (29.9967079163,36.0920582347), test loss: 52.7944311619\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (31.7068462372,23.8416760695), test loss: 24.8554162502\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (24.5004291534,35.9603946978), test loss: 43.0651181698\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (19.6864891052,23.7535168333), test loss: 27.1014570236\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (20.3039321899,35.82910226), test loss: 46.1305439949\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (5.2861700058,23.6658316778), test loss: 28.7100893021\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.62600851059,35.6983520149), test loss: 52.9053844452\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (7.58551120758,23.5789699389), test loss: 24.993796587\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (21.9692649841,35.5687167667), test loss: 43.0044346809\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (15.4555959702,23.4926441471), test loss: 29.2909498453\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (20.5186271667,35.4398676104), test loss: 48.6693473816\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (6.2590470314,23.4070702765), test loss: 27.163236618\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (39.4326705933,35.3125511459), test loss: 50.8254037857\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (11.9189729691,23.3227111965), test loss: 24.5068394542\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (21.0756168365,35.1871418972), test loss: 43.0552392483\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.23215770721,23.2387570883), test loss: 29.0685937762\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (8.23327159882,35.060396232), test loss: 50.1137548923\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (21.9814796448,23.1554650646), test loss: 25.2074644566\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (13.5395545959,34.9349302066), test loss: 49.1105741978\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (16.8268890381,23.0737415309), test loss: 24.1257789254\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (13.4828891754,34.8104117299), test loss: 41.2889634609\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (17.0526638031,22.9928187944), test loss: 29.1628601313\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (8.11173820496,34.6861390189), test loss: 53.7810415745\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (4.95047521591,22.9116542126), test loss: 25.1398976326\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (9.14396476746,34.5622569294), test loss: 48.903963089\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (12.6300878525,22.8312227669), test loss: 27.6424285412\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (16.8330745697,34.4389763516), test loss: 46.8024466991\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (9.48577785492,22.7512130529), test loss: 28.3368426561\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (7.92573165894,34.3162454654), test loss: 54.0072656155\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (14.3556842804,22.6717586703), test loss: 25.4946259499\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (9.38563537598,34.1943741711), test loss: 43.5087321758\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 3\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold3/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (315.045593262,inf), test loss: 216.094301605\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (338.474853516,inf), test loss: 215.217492676\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (69.8261489868,148.470999142), test loss: 95.5181879044\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (34.8959884644,105.826843328), test loss: 70.8344822407\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (41.9088859558,106.56642277), test loss: 47.9002672195\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (198.185501099,82.6233741503), test loss: 72.9677897453\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (107.46647644,84.002766173), test loss: 37.7331202984\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (96.8901672363,74.2158566376), test loss: 71.4223132133\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (33.8754196167,71.985698292), test loss: 35.2270362854\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (63.909362793,69.7142128086), test loss: 62.9453233719\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (15.027882576,64.6776484003), test loss: 37.8730748653\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (43.4970932007,66.868048353), test loss: 68.2966434479\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (28.7040481567,59.74832061), test loss: 37.1237104893\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (31.2266960144,64.7746213911), test loss: 69.7370794296\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (28.0279769897,56.1886400956), test loss: 33.0151417971\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (30.5467586517,63.1827459224), test loss: 58.3440805435\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (21.2960128784,53.4633141155), test loss: 36.3906485081\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (26.7145080566,61.8373392657), test loss: 60.5204424858\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (49.657119751,51.2979922557), test loss: 37.0849251747\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (64.5984420776,60.7267828556), test loss: 66.9375442505\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (39.906829834,49.5216687905), test loss: 32.5088838577\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (37.9950294495,59.7373765509), test loss: 55.9481876373\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (24.0528793335,48.0442994169), test loss: 34.4863865852\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (49.2697906494,58.8592577117), test loss: 56.9766768932\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (22.3546676636,46.7800744205), test loss: 36.6482562065\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (22.7396774292,58.0303935983), test loss: 66.3027971268\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (9.98196601868,45.6691430521), test loss: 33.6378194332\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (21.900144577,57.2701461115), test loss: 57.6247641087\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (38.17628479,44.68228675), test loss: 32.750693655\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (32.1672973633,56.5578563528), test loss: 54.8662602425\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (38.8804626465,43.8057251816), test loss: 36.7011348724\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (63.8345794678,55.8792135612), test loss: 65.3428237915\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (27.5694465637,43.0142950073), test loss: 32.1169670343\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (42.8268966675,55.2175859164), test loss: 61.7751524448\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (19.8943080902,42.2833952738), test loss: 29.5173450708\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (45.6388397217,54.5843446119), test loss: 50.3176224709\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (25.7038631439,41.6093043306), test loss: 33.6804694176\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (13.7984304428,53.9801011172), test loss: 59.3503569603\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (68.8140258789,40.9908164296), test loss: 33.5015982628\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (78.7873077393,53.3922337795), test loss: 60.751309967\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (24.6855049133,40.4167581448), test loss: 28.8068510771\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (66.2642440796,52.8210267009), test loss: 48.8740188599\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (25.1613616943,39.8725372502), test loss: 32.8754053354\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (75.7623214722,52.2649968312), test loss: 52.2232114792\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (24.4685878754,39.3581959733), test loss: 33.7150801182\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (18.767490387,51.7386565281), test loss: 58.3817511559\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (15.1101665497,38.870824634), test loss: 29.6977702141\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (18.5893363953,51.2161665788), test loss: 49.9201226234\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (8.01751041412,38.4175260669), test loss: 30.4095862865\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.32742214203,50.7128427569), test loss: 49.5367823601\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (27.6877479553,37.9776453337), test loss: 34.6329841137\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (28.1244087219,50.2208055189), test loss: 61.0119040489\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (18.103811264,37.5562574434), test loss: 29.240841341\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (25.7671661377,49.7612350592), test loss: 55.192570591\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (21.3641319275,37.1500780824), test loss: 27.8722073078\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (11.5380764008,49.2995414476), test loss: 48.1618612289\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (25.1452713013,36.7741675305), test loss: 32.2114515781\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (23.4277019501,48.8623154818), test loss: 57.3900911331\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (18.8773193359,36.4008275053), test loss: 29.8702895164\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (25.089466095,48.4317553937), test loss: 56.4769174576\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (61.1190643311,36.0400362681), test loss: 25.6521467447\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (102.968994141,48.0200098192), test loss: 44.3506892204\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (16.0390605927,35.6879037276), test loss: 30.778056097\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (43.6514892578,47.6131579727), test loss: 49.4918089867\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (46.3797073364,35.3588256501), test loss: 30.5016739368\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (21.6440429688,47.2176232055), test loss: 55.29234519\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (36.2927856445,35.0308784484), test loss: 26.3209628582\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (16.8605995178,46.8283665941), test loss: 45.2414786339\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (12.0944442749,34.7117508594), test loss: 29.0410194159\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (57.600315094,46.4606134088), test loss: 47.421271801\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (10.5943965912,34.4071242485), test loss: 31.9312052727\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (15.3037891388,46.1070076192), test loss: 57.5868403435\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (13.2995605469,34.1164176056), test loss: 27.1235465288\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (19.8848075867,45.7700510529), test loss: 52.4676147938\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (23.2237586975,33.8320125938), test loss: 27.5083600044\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (17.9510307312,45.4345346105), test loss: 46.2530770302\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (4.95647525787,33.5526747917), test loss: 31.2387535095\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (13.598733902,45.1138041397), test loss: 56.3975958824\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (9.72489356995,33.28375847), test loss: 26.4848690748\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (9.78935432434,44.8093891996), test loss: 52.9900048256\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (32.1570625305,33.0255819445), test loss: 23.6898942709\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (36.4451370239,44.5143354041), test loss: 42.1093659401\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (21.9468631744,32.7735826313), test loss: 29.18616786\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (36.3058509827,44.2217697736), test loss: 47.722547245\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (14.1013622284,32.5251491845), test loss: 28.3820053101\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (41.5800170898,43.9392857619), test loss: 54.1388226509\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (30.9887905121,32.2860116429), test loss: 24.1533466816\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (39.1572303772,43.6720740345), test loss: 42.8616270065\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (41.9717597961,32.0549395322), test loss: 28.9618613482\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (40.8360862732,43.4102814987), test loss: 47.2140016556\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (16.2659416199,31.8301835595), test loss: 30.1911699295\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (17.348077774,43.1507292097), test loss: 54.1439378738\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.8429889679,31.6074632012), test loss: 25.6250117779\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (19.0550231934,42.8986033538), test loss: 45.097666502\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (9.24509239197,31.3912768376), test loss: 27.0968044281\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (32.4190216064,42.6595474893), test loss: 45.7045505047\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (19.0588035583,31.182484881), test loss: 30.9717144728\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (31.2331790924,42.4238303102), test loss: 56.233952713\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (29.0624008179,30.980964949), test loss: 25.6810759306\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (28.2298679352,42.1919054348), test loss: 51.3982287884\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.9483242035,30.7793503213), test loss: 24.0493065596\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (35.1335220337,41.9638232761), test loss: 44.3826309204\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (9.36915111542,30.5832738965), test loss: 28.7993859529\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (30.9810714722,41.7471622319), test loss: 53.2517328262\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (12.6769580841,30.3923075395), test loss: 26.5037602663\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (21.6788578033,41.5317019231), test loss: 52.0304154396\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (9.36622238159,30.2089132067), test loss: 23.3337976933\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (16.1645526886,41.3205133875), test loss: 42.0825839996\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (14.2508106232,30.025564912), test loss: 28.2169199467\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (24.2697715759,41.1117017183), test loss: 46.6523097992\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (15.7468338013,29.8461037021), test loss: 28.417578578\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (22.6519355774,40.914278175), test loss: 52.1828095436\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (51.3720855713,29.6710540089), test loss: 23.8158436298\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (67.5904083252,40.7162483951), test loss: 42.7987900257\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.0925254822,29.5027922356), test loss: 26.9613418102\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (44.2121238708,40.5222462596), test loss: 45.6070710182\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (11.9289474487,29.334175619), test loss: 29.9250791073\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (8.21455097198,40.3287593233), test loss: 55.2607323647\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (30.3684654236,29.1687342021), test loss: 24.7060622931\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (106.381484985,40.1464232389), test loss: 49.646810627\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.9534301758,29.006018461), test loss: 24.9768606186\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (24.8106899261,39.9615903371), test loss: 44.0121357441\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (15.8529243469,28.8516816769), test loss: 29.0336627722\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (20.4020023346,39.7820042518), test loss: 53.3341626644\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (26.2050209045,28.6960032315), test loss: 25.5011203289\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (52.4779205322,39.6029673033), test loss: 51.260790205\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (14.7258939743,28.5411797637), test loss: 22.2781358242\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (55.8191604614,39.4284047718), test loss: 41.0186443329\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (13.3474979401,28.3913155237), test loss: 27.8855904341\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (84.1792144775,39.2594925227), test loss: 46.3868145943\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (41.6199607849,28.2473334297), test loss: 26.5937490225\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (29.033908844,39.0915932553), test loss: 51.2455724239\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (12.8609056473,28.1027876826), test loss: 22.8310276985\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (22.8209991455,38.9230691303), test loss: 41.4861645222\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (8.6156244278,27.9590108741), test loss: 26.634015274\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (22.7307815552,38.7586185233), test loss: 45.039287138\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (10.9156208038,27.8190357719), test loss: 28.8783722639\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (19.3274097443,38.5991668368), test loss: 53.8769648075\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (40.5825462341,27.683674184), test loss: 24.0981997013\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (37.770866394,38.4425543518), test loss: 42.7005994797\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (18.3012809753,27.5490616663), test loss: 25.7720526218\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (13.8324489594,38.2836474046), test loss: 44.479150629\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (33.7039871216,27.4149079189), test loss: 29.5310904026\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (39.4305801392,38.128487457), test loss: 54.3091119766\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (23.625995636,27.2834869545), test loss: 24.2076453209\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (16.7748413086,37.9777783734), test loss: 49.5223211765\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.6573963165,27.1553096177), test loss: 22.3278451204\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (9.88595104218,37.828246462), test loss: 40.4437148094\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (6.56019544601,27.0296503231), test loss: 27.7944343567\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (5.8421421051,37.6782009274), test loss: 52.5205109596\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (10.7347946167,26.9033317836), test loss: 25.8380625963\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (7.49610519409,37.5301555751), test loss: 51.0438532352\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (13.6196098328,26.7797264011), test loss: 22.5533020496\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (15.8993206024,37.3873998592), test loss: 41.2643976688\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (21.7535018921,26.6588936195), test loss: 27.4789241552\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (32.2413711548,37.2449645332), test loss: 45.4482383728\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (18.1994552612,26.5411869916), test loss: 27.415558672\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (17.5815734863,37.1023301728), test loss: 50.8061283588\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (10.4643135071,26.4217555414), test loss: 22.9877260685\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (16.0796318054,36.9607744257), test loss: 41.7345659256\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.7538375854,26.3050313896), test loss: 25.7735654831\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (6.79840517044,36.8237182998), test loss: 44.1479186058\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (30.2152462006,26.1905726036), test loss: 29.1196541071\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (38.065284729,36.687257439), test loss: 53.2763020515\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (19.548833847,26.0790056628), test loss: 23.830192256\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (29.9166030884,36.5510452635), test loss: 48.8335776806\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.75891399384,25.9663726969), test loss: 24.2279230356\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (30.1985549927,36.4151624396), test loss: 44.4137983322\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (16.5377941132,25.8555881421), test loss: 28.3548745394\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (9.86451911926,36.2841362786), test loss: 52.5516637325\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (17.9009399414,25.7465240773), test loss: 24.9931411743\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (25.1767654419,36.1525037684), test loss: 49.6652380943\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (19.955745697,25.640756695), test loss: 21.9107721806\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (14.9606647491,36.0217281816), test loss: 40.8841389179\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (9.73534965515,25.5338285981), test loss: 27.6000236988\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (29.1364917755,35.8910368821), test loss: 45.9976182938\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (23.4007110596,25.4281292819), test loss: 26.1907436371\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (35.7562141418,35.7652198907), test loss: 49.5389323235\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (20.2677726746,25.3236332964), test loss: 22.5258202076\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (14.1512899399,35.6373237494), test loss: 40.6778748035\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (12.8489704132,25.2230951044), test loss: 26.1515672684\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (7.89665937424,35.5110896303), test loss: 44.9218883991\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (13.9497003555,25.1216020121), test loss: 28.4705390453\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (33.2601585388,35.3850245756), test loss: 53.2630764961\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (25.6856613159,25.0203400788), test loss: 23.3385498762\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (43.9156036377,35.2615241874), test loss: 41.7609209061\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (17.8128395081,24.920716148), test loss: 25.1621878624\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (78.1256866455,35.1387002122), test loss: 44.0624281883\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (25.9971313477,24.824776913), test loss: 28.8781623363\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (9.42133712769,35.0163809994), test loss: 54.1700680256\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (8.81764793396,24.7277254008), test loss: 23.6811725855\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (6.82437372208,34.8933789813), test loss: 49.0279776573\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (8.29682350159,24.6307561232), test loss: 22.0222855568\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (37.0370445251,34.7727743061), test loss: 40.3954408169\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (8.48035240173,24.5356658117), test loss: 27.5474813938\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (8.26915454865,34.6533788586), test loss: 51.3271427631\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (17.154083252,24.4430113423), test loss: 25.8670203686\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (16.0223178864,34.5355120121), test loss: 50.8097821236\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (25.8161048889,24.3508088473), test loss: 22.4083964825\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (22.0007705688,34.4159599052), test loss: 41.2738785744\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (8.90772724152,24.2579031672), test loss: 27.5789752722\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (14.7942790985,34.2981998458), test loss: 46.1047386646\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (14.9992790222,24.1665588115), test loss: 27.7066328049\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (10.6162691116,34.1822275508), test loss: 50.9042593002\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (20.3695526123,24.076982444), test loss: 22.6700393438\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (28.2688045502,34.066925418), test loss: 41.3902703285\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (12.892115593,23.9885499742), test loss: 25.5139456511\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (21.7127227783,33.9505688538), test loss: 44.7289487362\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.9761447906,23.8993377034), test loss: 28.9354747295\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (31.3004302979,33.8353217444), test loss: 53.8100792885\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (19.9255371094,23.8115278991), test loss: 23.7358068943\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (23.802318573,33.7224438405), test loss: 49.0889181614\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (34.8922920227,23.7252577868), test loss: 23.4240321159\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (22.5659370422,33.6095031566), test loss: 44.217000246\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (9.95597267151,23.6402415033), test loss: 28.4032696486\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (12.7658958435,33.4959505695), test loss: 53.7901864052\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (14.4368114471,23.5541606652), test loss: 25.1987776995\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (17.9720802307,33.3830288671), test loss: 50.9891239166\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (8.86074829102,23.4694294294), test loss: 22.2604173422\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (26.2265739441,33.2726313721), test loss: 41.6739376545\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (7.27462291718,23.3858021866), test loss: 27.6622089386\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (13.391872406,33.1617886074), test loss: 46.5441220284\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (19.9028339386,23.3040920288), test loss: 26.3168629169\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (17.6474208832,33.0511256317), test loss: 50.349756813\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (3.79543161392,23.2209708385), test loss: 22.7255401373\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (29.7536125183,32.9404526212), test loss: 41.7603884697\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (10.4492769241,23.139110705), test loss: 26.416435051\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (15.9813480377,32.8321648943), test loss: 45.6456596851\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (5.53672504425,23.0580294167), test loss: 28.7966151476\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (12.5755224228,32.7233394821), test loss: 53.9306011677\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (4.71504354477,22.9788368584), test loss: 23.9458142281\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (10.9834785461,32.614759472), test loss: 49.8656243801\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.58051204681,22.8987938491), test loss: 25.3417791367\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (18.1072845459,32.5061098112), test loss: 45.2904878616\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (10.9864006042,22.8193290828), test loss: 28.9535590887\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (12.093038559,32.4002479722), test loss: 54.5395496368\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (30.0661010742,22.7407762458), test loss: 23.4978763819\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (32.5163345337,32.2932933841), test loss: 49.3529089928\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (13.1078033447,22.6638664989), test loss: 21.8067481041\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (33.4950675964,32.1867067254), test loss: 41.4791475773\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 4\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold4/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (352.803833008,inf), test loss: 245.023957825\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (327.428497314,inf), test loss: 194.43335228\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (188.964584351,181.300415771), test loss: 144.145283127\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (114.419494629,116.804477892), test loss: 73.0545832634\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (79.1691589355,150.2974552), test loss: 102.824941254\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (19.8474349976,89.5917072034), test loss: 70.3330147266\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (31.606464386,123.532498353), test loss: 51.2286718369\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (50.3041152954,79.6463678516), test loss: 58.1296776772\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (23.9642467499,103.587253), test loss: 39.6683107376\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (49.9145126343,74.4367684042), test loss: 65.1710699081\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (12.6167001724,90.3407355824), test loss: 38.5789660454\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (36.9511680603,71.1401398705), test loss: 68.214995575\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (21.8544101715,81.4021935992), test loss: 35.7764141083\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (20.2363033295,68.6744016279), test loss: 54.5718957901\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (41.005065918,75.0108962721), test loss: 38.7681048393\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (57.404876709,66.7556636393), test loss: 64.9492469788\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (71.4356460571,70.1800727961), test loss: 37.9844087601\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (56.9208717346,65.2033469257), test loss: 60.0431050301\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (22.7671527863,66.3475695498), test loss: 36.0291820526\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (31.4252319336,63.879495299), test loss: 53.901727581\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (18.4237594604,63.2605862661), test loss: 39.9473646641\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (20.7181625366,62.760260534), test loss: 64.8174164772\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (51.4814071655,60.7157764308), test loss: 35.2539624691\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (36.3457717896,61.7817828263), test loss: 54.840660429\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (37.0523071289,58.5799850728), test loss: 36.2412123203\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (104.781448364,60.9496915618), test loss: 53.6455190659\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (37.7752342224,56.739113492), test loss: 38.2605426788\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (52.1872406006,60.1606444362), test loss: 65.6947998047\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (51.9758110046,55.1463497609), test loss: 32.636396122\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (128.164672852,59.4642954072), test loss: 52.1785327911\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (38.7687606812,53.7411810125), test loss: 37.2622999191\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (45.5559005737,58.7730713335), test loss: 60.7991120338\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (38.2568054199,52.5012002322), test loss: 35.8557942867\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (41.4995880127,58.1194343252), test loss: 62.5636808395\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (69.1423568726,51.388752528), test loss: 33.5007155895\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (88.314453125,57.5163586615), test loss: 49.4032716751\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.5474014282,50.3871294435), test loss: 36.4572188854\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (49.2270622253,56.9343711119), test loss: 60.4661690712\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (78.1368179321,49.470759025), test loss: 34.4732367039\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (68.0632019043,56.3724166106), test loss: 52.6302270889\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (10.1867752075,48.6228365838), test loss: 33.5870570421\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (21.7095031738,55.8236710657), test loss: 49.499298954\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (33.6611938477,47.8390068066), test loss: 37.5731039762\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (40.5587158203,55.297592035), test loss: 61.6672278881\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (25.4194374084,47.1086979981), test loss: 32.3477859974\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (43.1978569031,54.784925368), test loss: 50.3409716606\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (26.3328514099,46.43625037), test loss: 33.7815437317\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (44.1439590454,54.3026884416), test loss: 52.3995148182\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (6.73872756958,45.7975075086), test loss: 35.7969339848\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (13.2491950989,53.8274558148), test loss: 62.1826184273\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (56.9609451294,45.1983032927), test loss: 29.6545261383\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (113.394470215,53.3718240792), test loss: 46.5318398952\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (25.4528141022,44.6312956892), test loss: 35.6331353664\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (7.66591262817,52.9275530747), test loss: 59.2914184093\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (28.4575634003,44.0963072817), test loss: 33.095140028\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (15.2269458771,52.4904949338), test loss: 53.2361420631\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (27.0202484131,43.5859456916), test loss: 30.698388195\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (17.5237197876,52.0703006902), test loss: 46.1605511665\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (39.5882339478,43.1057222966), test loss: 34.5048112392\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (53.4358596802,51.6641802669), test loss: 57.8634105682\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (23.1706542969,42.6410898811), test loss: 31.2581408501\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (32.3072052002,51.2653286806), test loss: 49.0368211746\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (12.8748092651,42.1970475487), test loss: 31.1369976521\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (28.2392845154,50.8738776071), test loss: 47.549584198\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (25.254404068,41.7643898307), test loss: 35.1853457451\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (19.7997779846,50.4908020979), test loss: 60.8024472237\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (20.5773677826,41.3483777431), test loss: 29.0997236013\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (21.8947200775,50.1201758844), test loss: 49.0777055264\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (25.7163009644,40.9483184005), test loss: 31.4941732883\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (16.0436191559,49.7672304069), test loss: 55.6878919601\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (18.0616436005,40.5407299699), test loss: 32.1117557526\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (23.9466629028,49.422867331), test loss: 60.7742731571\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (11.2179307938,40.1355773325), test loss: 26.4622974873\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (50.0267448425,49.087227119), test loss: 45.5080494881\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (11.6419229507,39.7453338316), test loss: 32.5429168224\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (33.1921920776,48.7707253571), test loss: 58.308065176\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (34.2250671387,39.3667442033), test loss: 30.0343058109\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (23.7912750244,48.4539406699), test loss: 51.0762993336\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (16.7430820465,38.9991191917), test loss: 27.8148639202\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (22.4356231689,48.1483004989), test loss: 46.207799387\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (75.3047180176,38.6473843429), test loss: 32.5034799576\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (73.5297470093,47.8525305365), test loss: 57.3594325542\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (28.7938308716,38.3045457879), test loss: 27.9000771046\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (31.2960109711,47.5640119569), test loss: 48.2339865208\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (19.7894763947,37.9724650901), test loss: 29.5700982332\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (17.6470565796,47.2803450022), test loss: 48.2138542652\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (14.5158519745,37.6451356021), test loss: 32.3490089417\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (25.2494010925,47.0023246026), test loss: 60.3783901215\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (8.50115966797,37.3291537429), test loss: 26.9123461246\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (7.18851232529,46.7329442501), test loss: 49.9105596542\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (29.2691802979,37.0245383683), test loss: 29.7810125828\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (59.646572113,46.4746826245), test loss: 55.9021384239\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (20.7659645081,36.7256936522), test loss: 29.8266765594\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (44.3964118958,46.22060297), test loss: 60.2267970085\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (4.96806573868,36.4343409318), test loss: 25.2974546432\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (5.28180551529,45.9706633568), test loss: 45.7480154037\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (16.9688034058,36.1526000733), test loss: 30.9563196659\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (28.5846672058,45.7358353029), test loss: 58.0187373161\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (22.100446701,35.8767176252), test loss: 28.8399780273\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (61.2036476135,45.4982511373), test loss: 50.9978516579\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (9.03069972992,35.6078566963), test loss: 27.0815522194\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (25.1107463837,45.2636978002), test loss: 46.7871268988\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (8.35376834869,35.3470707327), test loss: 31.6953320503\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (9.61659526825,45.038765674), test loss: 57.3237267256\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (17.6299781799,35.094388808), test loss: 26.8937852859\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (24.2742557526,44.8187878426), test loss: 48.1946593046\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (29.8548774719,34.8466453057), test loss: 29.0027565002\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (11.4192848206,44.6007329158), test loss: 48.40854249\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.2848501205,34.6010804214), test loss: 31.2447989702\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (14.7570915222,44.3848350311), test loss: 59.6627455235\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (24.1549129486,34.3623093357), test loss: 26.411379528\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (39.734954834,44.1745589684), test loss: 49.8033593178\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (54.6523513794,34.1299610329), test loss: 29.5636142254\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (67.2925186157,43.9689545239), test loss: 55.8243784904\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (14.3065299988,33.9013080203), test loss: 29.0344452858\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (32.1900177002,43.7674912308), test loss: 59.5237110615\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (33.4620933533,33.6781325585), test loss: 24.921032238\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (46.1134223938,43.5687401538), test loss: 45.6741786957\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (29.7720603943,33.46035807), test loss: 30.3027755737\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (41.1209106445,43.3785976379), test loss: 57.4836392403\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.6256828308,33.2460407341), test loss: 27.3705736876\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (18.2216091156,43.1843949236), test loss: 49.4319088936\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (17.0561332703,33.0374161472), test loss: 26.7637596369\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (11.1363096237,42.9939372285), test loss: 46.6319495201\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (55.254234314,32.8339455852), test loss: 31.0081152916\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (68.356628418,42.8097244565), test loss: 56.4539473534\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (18.2423648834,32.6349919728), test loss: 26.5563266039\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (13.2015638351,42.6268081267), test loss: 48.0001377583\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (44.6545333862,32.4411707328), test loss: 28.8247064114\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (40.7638626099,42.4468022893), test loss: 48.2317006111\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (15.0539598465,32.2487234343), test loss: 30.7049061298\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (13.4299354553,42.2666314241), test loss: 58.6251569748\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (12.1549568176,32.0599848935), test loss: 25.7876425743\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (14.1079978943,42.0896566802), test loss: 48.6516701698\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (15.5398321152,31.8753183882), test loss: 30.0900093555\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (21.0305747986,41.9156078361), test loss: 56.4171755791\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (11.7872037888,31.6949078405), test loss: 28.7414570332\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (21.5286464691,41.7455015554), test loss: 58.6170249462\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (11.3473911285,31.516620603), test loss: 25.552841568\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (21.0032558441,41.5762010732), test loss: 45.9387557983\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (34.5925521851,31.3424640136), test loss: 30.0784624577\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (96.9263687134,41.4123532515), test loss: 56.7674921989\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.4590597153,31.1711890874), test loss: 26.6608183861\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (9.34795761108,41.2481845022), test loss: 48.4133010387\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (17.4022216797,31.0033546033), test loss: 26.4822135925\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (22.2678413391,41.0851028845), test loss: 46.5162503719\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (19.1070041656,30.8383388251), test loss: 30.7314413071\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (12.047208786,40.9248614411), test loss: 56.5048736095\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (20.8466949463,30.6781411073), test loss: 26.4607122421\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (43.3733596802,40.7679881921), test loss: 47.6703271866\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (16.6824455261,30.5202112356), test loss: 28.6213797569\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (16.8624458313,40.611478664), test loss: 47.9641618729\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (9.10161590576,30.364234441), test loss: 30.4707492352\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (22.7935905457,40.4554679622), test loss: 59.2408694744\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (18.310174942,30.2101741724), test loss: 25.1992394924\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (21.226896286,40.3010650937), test loss: 47.6961733818\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (19.732963562,30.0590458588), test loss: 29.7896431923\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (26.1567268372,40.1487413255), test loss: 55.780462265\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (15.1278438568,29.9108025932), test loss: 28.4109298468\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (20.5933189392,39.999044528), test loss: 57.568599987\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (13.3775882721,29.7638564225), test loss: 25.6286867142\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (14.8535270691,39.8503011383), test loss: 44.7770082951\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (10.4610290527,29.6195887305), test loss: 30.0536119223\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (56.3303833008,39.7041228082), test loss: 56.4079951286\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (6.62839031219,29.4782083261), test loss: 26.5640100479\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (22.536989212,39.561122476), test loss: 48.0667705059\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (19.992729187,29.3386774765), test loss: 26.9420405388\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (13.8934364319,39.4162924998), test loss: 46.8119665623\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (17.1737785339,29.2010365604), test loss: 30.6135772705\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (30.0393257141,39.2742155951), test loss: 56.2556386948\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (49.8812179565,29.06713265), test loss: 26.5140174866\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (46.5363388062,39.1335352369), test loss: 47.6465455055\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (37.9391212463,28.9348180009), test loss: 28.9558904648\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (37.6538848877,38.9940115312), test loss: 48.8204752922\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (16.4865036011,28.8042156539), test loss: 29.7426883698\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (7.12119340897,38.8543135026), test loss: 58.5456871033\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (16.4215393066,28.6743059046), test loss: 25.0215961456\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (24.0620193481,38.715776772), test loss: 47.3626851082\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (10.8281402588,28.5466521057), test loss: 29.7470192909\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (7.16893196106,38.5783806632), test loss: 55.4660730362\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (14.1610908508,28.4214542756), test loss: 28.0177418232\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (30.1529140472,38.4438045763), test loss: 56.8472004414\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (11.5788612366,28.2968800031), test loss: 25.6541671276\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (14.4035673141,38.3097477746), test loss: 44.8207480431\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (6.63607120514,28.1744057876), test loss: 29.8626266479\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (18.0478057861,38.1765635957), test loss: 56.1349606991\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (11.8009395599,28.0538883825), test loss: 26.2593700409\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (14.6099777222,38.0473589976), test loss: 47.6414594173\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (15.2034082413,27.9342821987), test loss: 27.2174181938\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (39.2210769653,37.9159938773), test loss: 46.9593988419\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.95125770569,27.8164159824), test loss: 30.5918130398\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (64.0674285889,37.7859327634), test loss: 55.9146274567\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (8.3221282959,27.7008392881), test loss: 26.4759110928\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (15.4645442963,37.6569921455), test loss: 47.8674698353\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (11.0989437103,27.5872595897), test loss: 29.1701537132\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (17.6075687408,37.5292774369), test loss: 48.5759116173\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (15.7639894485,27.4748874438), test loss: 29.4272904873\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (12.1613731384,37.4014681283), test loss: 58.2363274574\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (9.42904281616,27.36283169), test loss: 25.1376897812\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (16.2999343872,37.2741230739), test loss: 47.6663810253\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (21.2971248627,27.2524925095), test loss: 29.9570774078\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (33.1787452698,37.1482779047), test loss: 55.9923964024\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (44.6861991882,27.1438523181), test loss: 28.1148031712\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (56.3303527832,37.0237101651), test loss: 57.1555429459\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (7.14437961578,27.0354946288), test loss: 25.9751288891\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (25.6350212097,36.899901872), test loss: 45.454403019\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (24.2129402161,26.9287998134), test loss: 29.7124676228\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (32.6645355225,36.7768538333), test loss: 55.875900507\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (13.4449462891,26.8233663794), test loss: 26.0809290886\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (18.5688152313,36.656995922), test loss: 47.1055715561\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (13.0331573486,26.7186504205), test loss: 27.174059248\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (11.758436203,36.5344641267), test loss: 46.874083519\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (8.65551757812,26.6154677359), test loss: 30.4581490517\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (4.32938098907,36.4128892113), test loss: 56.3850818634\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (40.6079444885,26.513835366), test loss: 26.3705805779\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (42.5464286804,36.2932165898), test loss: 47.6207679749\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (26.8366851807,26.4137337205), test loss: 29.0760651588\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (22.9888305664,36.1737618582), test loss: 49.7421494484\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (33.1307373047,26.314494953), test loss: 29.3878825188\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (22.0043411255,36.0545222181), test loss: 58.1747419834\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (17.6397438049,26.2156906644), test loss: 24.826335144\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (13.8349876404,35.9354298344), test loss: 46.3196383476\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (7.98623228073,26.1177887718), test loss: 30.3600881577\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (6.36525630951,35.817093007), test loss: 56.9470795631\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (8.0433216095,26.0208937933), test loss: 27.7893009186\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (14.1327600479,35.6997325967), test loss: 52.7806583405\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (6.72347068787,25.9251827084), test loss: 26.1102014542\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (25.3853054047,35.5838720594), test loss: 45.7449192524\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (9.32791805267,25.8298488243), test loss: 29.7683318615\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (22.4135093689,35.4675813774), test loss: 55.6272569656\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (32.2596740723,25.7359155177), test loss: 26.1739823341\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (116.477973938,35.3545435498), test loss: 47.0416193962\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (13.6267375946,25.6424795319), test loss: 27.2137564659\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (14.5758075714,35.2388451244), test loss: 47.1393349171\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (15.7973709106,25.550123767), test loss: 30.7536193848\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (18.819070816,35.1238064245), test loss: 57.6056300163\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (17.2893409729,25.4585307706), test loss: 25.706763792\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (13.9643173218,35.0098809273), test loss: 47.3395481586\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (20.1502456665,25.3686341929), test loss: 29.6171886444\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (35.3754043579,34.8969967828), test loss: 56.0202430725\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 5\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold5/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (304.299743652,inf), test loss: 203.567974091\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (377.480621338,inf), test loss: 238.880838394\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (83.2269134521,92.3455543537), test loss: 40.5088936806\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (129.828674316,102.498502945), test loss: 62.8959464073\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (28.8921966553,65.5900251069), test loss: 36.1610200167\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (58.6535720825,81.3321924605), test loss: 61.4382978439\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (28.2521209717,55.5575353139), test loss: 32.5872415304\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (29.3737049103,73.7313136371), test loss: 54.3086393356\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (20.3253269196,50.2836908908), test loss: 34.7092661858\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (24.453119278,69.6327124689), test loss: 63.9437318802\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (21.1992111206,46.9500407845), test loss: 29.9397061348\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (34.4432449341,66.9273609228), test loss: 54.6770500183\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (28.7867336273,44.5908008536), test loss: 33.1649698496\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (36.1836395264,64.9223017516), test loss: 62.1014881611\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (30.1670703888,42.7919139772), test loss: 31.5559911728\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (61.4732017517,63.3216982768), test loss: 56.9830846786\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (31.4399738312,41.3494945481), test loss: 30.8163756847\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (49.8150138855,61.9709380061), test loss: 51.4546956062\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (35.6963500977,40.1408297356), test loss: 31.3418849945\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (67.6723480225,60.7778346288), test loss: 60.7388663292\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (6.36743021011,39.0682730346), test loss: 26.6768045664\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.09142541885,59.5851944366), test loss: 48.004731226\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (9.91506385803,38.1242698072), test loss: 30.1239762783\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (33.2174453735,58.4880608983), test loss: 57.6103363991\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (14.9450521469,37.2798188777), test loss: 27.9819723129\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (43.1433868408,57.4770031239), test loss: 49.4402878761\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (21.3441848755,36.5177654545), test loss: 28.2842028856\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (24.4895839691,56.5456058725), test loss: 48.0537384033\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (36.4555587769,35.8213232517), test loss: 29.2847904444\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (25.8615875244,55.6834713999), test loss: 59.4045609474\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (70.2157440186,35.1740601553), test loss: 25.3798979759\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (57.1604042053,54.8772292046), test loss: 44.8456873894\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (10.8373746872,34.5566746325), test loss: 28.9162056923\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (18.0154457092,54.1165316837), test loss: 56.0467540741\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (31.616558075,33.9869236708), test loss: 26.9236809731\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (31.2246055603,53.4068764846), test loss: 48.5996619701\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.517326355,33.4545538246), test loss: 27.9983596087\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (50.0597190857,52.7359181916), test loss: 48.2445714474\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (45.9567718506,32.9590582738), test loss: 27.6696986437\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (48.7180175781,52.0998912185), test loss: 56.7620093822\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (17.7047233582,32.4955924133), test loss: 26.4075808287\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (45.8267021179,51.4976392653), test loss: 45.3949212551\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (62.6450576782,32.0654096835), test loss: 29.386618495\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (80.2194824219,50.9270022906), test loss: 55.5773284912\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (30.1107997894,31.6580432678), test loss: 25.7887285948\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (40.814239502,50.3806881063), test loss: 47.5755866051\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (11.3829889297,31.2786848314), test loss: 27.7941326618\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (14.2257947922,49.8658017568), test loss: 53.6212610245\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (63.9134902954,30.9232517462), test loss: 27.2318171263\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (89.5164337158,49.3775850374), test loss: 52.5360409737\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (16.5015869141,30.5837791033), test loss: 27.1439993382\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (15.8554811478,48.9055534243), test loss: 46.3743685246\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (16.1203708649,30.2663901376), test loss: 28.9328862429\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (20.1327400208,48.4602352865), test loss: 56.3788833141\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (18.1965332031,29.9657239095), test loss: 24.3707221746\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (120.671127319,48.0336359173), test loss: 46.3157028198\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (10.8013067245,29.6797785887), test loss: 28.9269699097\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (16.9509811401,47.617450443), test loss: 55.3483203888\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (21.8958797455,29.4076543927), test loss: 25.8465478659\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (22.1484298706,47.2253023765), test loss: 47.5326992989\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (14.1710777283,29.1471443541), test loss: 27.3349714041\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (19.6994400024,46.8477956896), test loss: 46.7357475281\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (20.3844299316,28.8981049283), test loss: 28.1641288757\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (60.5572433472,46.4841491764), test loss: 56.2435480118\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (32.1984710693,28.6591656869), test loss: 23.4051501513\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (33.5766906738,46.1310821731), test loss: 43.6251679897\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (20.439043045,28.4291658362), test loss: 28.1249954939\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (31.263584137,45.7918204456), test loss: 54.7780761957\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (7.23280191422,28.20833047), test loss: 24.9079589605\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (7.25454235077,45.4638127777), test loss: 45.3474116802\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (15.2383403778,27.9965213323), test loss: 27.1583916426\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (13.6738433838,45.1474798877), test loss: 46.6434807777\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (12.9661989212,27.7917047264), test loss: 27.0470509052\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (21.4168701172,44.8406008274), test loss: 56.0539957047\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (31.3568477631,27.5941378017), test loss: 24.2004528522\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (69.4867858887,44.5423030411), test loss: 43.8674654484\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (63.2177085876,27.4021364197), test loss: 27.775995636\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (162.384140015,44.2488934393), test loss: 54.1342663288\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (9.06787967682,27.2146315378), test loss: 25.1845769882\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (26.8982658386,43.9601712913), test loss: 46.1257137775\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (7.40931177139,27.0353920487), test loss: 27.3138948917\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (21.4550304413,43.6849323933), test loss: 47.1347161293\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (4.81534290314,26.8617422265), test loss: 26.789567852\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (13.9484710693,43.4182902448), test loss: 55.5179972172\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (22.9431228638,26.6932598761), test loss: 25.0507604599\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (47.9928283691,43.1585856769), test loss: 43.8680685043\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (6.97294855118,26.5282068969), test loss: 27.9145493507\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (15.2212991714,42.9032757939), test loss: 52.9776737213\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (16.4629936218,26.3685650662), test loss: 25.3646470308\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (21.8953514099,42.6558701372), test loss: 46.8126467228\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (9.66546440125,26.2126797523), test loss: 26.8038714886\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (19.9888343811,42.4135991666), test loss: 52.5461731434\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (7.06806898117,26.0609598953), test loss: 26.4104394436\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (37.4561462402,42.1764941982), test loss: 54.491143465\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (6.19698047638,25.9132107465), test loss: 26.0393161297\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (31.4370098114,41.9442342868), test loss: 45.122095108\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (13.0274047852,25.7689644259), test loss: 27.1542197227\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (11.1186218262,41.7169392786), test loss: 52.4674714565\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (9.29978370667,25.6277699619), test loss: 23.9670186281\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (9.34343338013,41.4951873311), test loss: 45.8429106236\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.8638916016,25.4899899649), test loss: 27.7404406309\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (47.3180236816,41.2777890129), test loss: 53.2826758862\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (42.9877319336,25.3548371431), test loss: 25.8144292355\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (31.3004837036,41.0618035797), test loss: 47.0073163986\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (11.2384204865,25.2215730873), test loss: 26.4979877949\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (31.9192695618,40.8518406556), test loss: 45.8976293087\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (21.1456642151,25.0924526867), test loss: 27.3128276825\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (17.0717430115,40.6456568958), test loss: 54.2882446289\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (20.192855835,24.9655152012), test loss: 23.0949056864\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (22.7316913605,40.4435667736), test loss: 43.1483866215\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (8.96344947815,24.8407267715), test loss: 27.4894340038\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (10.8457984924,40.2440895654), test loss: 53.3894405365\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (13.4674816132,24.7191876431), test loss: 24.5350128889\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (20.7423858643,40.0490320289), test loss: 44.9253283978\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (21.035358429,24.599735381), test loss: 26.4214139462\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (19.9148674011,39.8565850993), test loss: 45.7367939472\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (18.1914672852,24.4821834183), test loss: 26.1237518787\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (38.9606704712,39.6674083385), test loss: 54.2182197571\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.4282112122,24.3667860705), test loss: 23.5690756798\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (18.601234436,39.4803382688), test loss: 43.5844948769\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.4158630371,24.253751583), test loss: 27.103580451\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (8.827085495,39.2967746135), test loss: 52.9432305336\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.3815689087,24.142495018), test loss: 24.5339800358\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (47.2066879272,39.1162804391), test loss: 44.5210999966\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (5.06832361221,24.033036641), test loss: 26.6018683434\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (21.1971321106,38.9370881133), test loss: 46.3067873955\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (15.663942337,23.9256207397), test loss: 25.7959939003\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (15.7143936157,38.7611199243), test loss: 53.7849462986\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (29.1985340118,23.8197475485), test loss: 24.3878946304\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (12.0637969971,38.587742345), test loss: 43.2315702438\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (38.2017707825,23.715065049), test loss: 26.8483155727\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (31.0753383636,38.4166475947), test loss: 51.3844033718\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.2645807266,23.6116233814), test loss: 24.7897505283\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (14.7259979248,38.2471219685), test loss: 45.2589971066\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (33.4660797119,23.5105086593), test loss: 26.6264630795\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (29.5920352936,38.0802009251), test loss: 50.9348186016\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (17.4122142792,23.4099833607), test loss: 25.3010532618\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (29.2431240082,37.9145738542), test loss: 52.3618575096\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (31.7090759277,23.3115203922), test loss: 25.7429532051\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (30.7273731232,37.7511452735), test loss: 44.831980896\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.8913173676,23.2140974866), test loss: 27.1430620193\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (28.1291389465,37.5895568906), test loss: 51.7150614738\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (39.5457077026,23.1184969032), test loss: 23.8860680103\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (53.9692993164,37.4297979371), test loss: 45.4375472546\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (19.109664917,23.0232231172), test loss: 27.6629750252\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (15.7365856171,37.2709356112), test loss: 52.8827141762\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (7.72508525848,22.9298275292), test loss: 25.1964787006\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (9.2312374115,37.1149717555), test loss: 45.6787138462\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (38.1846389771,22.8378376928), test loss: 26.1374722958\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (53.9389762878,36.9606228362), test loss: 45.7560834885\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (16.4250507355,22.7462468), test loss: 26.8345565796\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (9.9031791687,36.8067851922), test loss: 52.9363974094\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (7.23498487473,22.656221013), test loss: 23.6990531445\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (16.4883537292,36.6553243948), test loss: 45.0619486809\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (7.81624937057,22.5674973308), test loss: 27.5159475803\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (45.4581069946,36.5046344402), test loss: 53.2777180195\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (9.67062759399,22.4798416684), test loss: 24.3868228197\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (8.62321090698,36.3553807443), test loss: 44.7688233376\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (19.6227397919,22.3930945594), test loss: 26.3323784828\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (20.9138774872,36.2084095883), test loss: 45.6914343834\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.2900314331,22.3069506146), test loss: 25.5957898617\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (12.8665723801,36.0622649807), test loss: 52.6123203278\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (10.1712818146,22.2220027709), test loss: 23.0831289291\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (34.1987380981,35.9176703568), test loss: 42.8372780323\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (12.279586792,22.1378159718), test loss: 26.5545988798\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (16.0836486816,35.773623008), test loss: 52.2940368176\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (16.0341758728,22.0544338586), test loss: 24.3627254486\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (17.5494632721,35.6312634745), test loss: 44.007659483\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (4.30294466019,21.9718697028), test loss: 26.0263801098\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (7.50375747681,35.4900568946), test loss: 45.4610176086\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.6719226837,21.8903298752), test loss: 25.2387770891\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (11.4901447296,35.3501116124), test loss: 52.8080472469\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (8.55024719238,21.8093605593), test loss: 23.8874434948\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (11.4785575867,35.2111143414), test loss: 42.7572005272\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (21.6233196259,21.7292501591), test loss: 26.3331971645\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (74.0642471313,35.0730986151), test loss: 51.2061114311\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (28.7212982178,21.649468335), test loss: 24.6573965073\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (64.0083465576,34.9344682926), test loss: 44.7192065716\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (8.16381263733,21.5703068948), test loss: 26.2722557545\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (50.1399841309,34.7972813616), test loss: 45.6774030685\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (8.83013153076,21.4923420205), test loss: 25.1740283012\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (17.840719223,34.6614297362), test loss: 52.2172807693\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (4.23752689362,21.4149634233), test loss: 25.3314831734\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (19.2532920837,34.5271567677), test loss: 44.219899416\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (21.364566803,21.3383085411), test loss: 26.7246439457\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (40.8117141724,34.3936759135), test loss: 51.3310478687\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (12.4037017822,21.2619647209), test loss: 24.048327899\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (11.2934303284,34.2606508241), test loss: 45.6218205452\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (13.1102895737,21.1863514223), test loss: 26.6458973408\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (19.8987159729,34.1290610125), test loss: 52.2813527107\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (7.4752869606,21.1113361627), test loss: 25.0666172981\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (15.7759895325,33.9981138106), test loss: 45.3769776821\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (5.71220064163,21.0369961992), test loss: 25.787603569\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (23.1104393005,33.8680589379), test loss: 45.1002356052\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (4.2438492775,20.9632366023), test loss: 26.6403440475\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (28.2755126953,33.7385665296), test loss: 52.7160427094\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (7.80434656143,20.8900571538), test loss: 23.5005495548\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (6.73158597946,33.6097638833), test loss: 44.9569967747\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (5.78910827637,20.8173674004), test loss: 27.2902242184\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (10.468495369,33.482125239), test loss: 53.1016524315\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (14.5458965302,20.7451531163), test loss: 24.4623863697\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (53.9396896362,33.3549736325), test loss: 44.6846544266\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (29.2330532074,20.673310774), test loss: 26.6086731434\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (10.1631336212,33.2278357061), test loss: 46.216509819\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (9.49826908112,20.60171087), test loss: 25.5458226204\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (25.8678150177,33.1022430591), test loss: 51.4142230511\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (18.5719165802,20.5309227778), test loss: 23.2228432178\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (18.1214675903,32.9769571511), test loss: 43.2513164997\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (20.0827560425,20.4604825804), test loss: 26.2571864605\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (30.6600379944,32.8525508162), test loss: 52.2928050041\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.5580573082,20.3904925151), test loss: 24.4853294373\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (4.61783313751,32.7285125635), test loss: 44.1881128788\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (10.905878067,20.3212013203), test loss: 26.3162179947\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (14.9351863861,32.6055700143), test loss: 45.9412298203\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (21.883808136,20.2522818076), test loss: 25.0447497368\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (7.85911083221,32.4830285494), test loss: 52.308768177\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (17.9765853882,20.1835552757), test loss: 24.4163751125\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (29.1267852783,32.3612868376), test loss: 43.9388507843\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (6.95515155792,20.1153505161), test loss: 26.3369885921\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (12.636598587,32.2397477118), test loss: 51.918423605\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (16.4580039978,20.0478172311), test loss: 24.569055891\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (12.1186819077,32.1191661475), test loss: 44.6475162983\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (12.4657554626,19.9804805056), test loss: 26.6406607628\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (32.8602981567,31.9991047331), test loss: 46.6720367432\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.65166950226,19.9136261383), test loss: 25.3486918926\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (12.233338356,31.8792433954), test loss: 52.4794826984\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (12.6707439423,19.8472421319), test loss: 25.3610940456\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (12.1311206818,31.7603676917), test loss: 44.4797151566\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (21.1383590698,19.7811367873), test loss: 26.8227778912\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (8.02867221832,31.6419844367), test loss: 51.0854989052\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (23.0998039246,19.7153038402), test loss: 24.4484856129\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (13.1924362183,31.5242423648), test loss: 45.0752057076\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (13.5520906448,19.649850454), test loss: 26.8918799877\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (13.0928401947,31.4069633649), test loss: 53.0023357153\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (20.0258255005,19.5848362497), test loss: 25.5010118008\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (15.084318161,31.2901481895), test loss: 52.1784784317\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (9.08287811279,19.5201453629), test loss: 26.4484191895\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (12.6127128601,31.1738002802), test loss: 45.9604206085\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (19.5072784424,19.4560703622), test loss: 26.7181680679\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (17.4075222015,31.0581432625), test loss: 51.5751551628\n",
      "run time for single CV loop: 5170.92089605\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 1\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold1/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (226.974349976,inf), test loss: 154.513979149\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (435.407409668,inf), test loss: 302.354362106\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (41.0747146606,82.4572744551), test loss: 48.5536780357\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (40.8460845947,132.496672107), test loss: 72.0719944\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (117.591949463,59.9595758028), test loss: 44.0210655212\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (271.167602539,96.0962895336), test loss: 76.7560329437\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (42.0712051392,51.9598547153), test loss: 35.7503857136\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (33.4481658936,83.3509691137), test loss: 61.0552059174\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (24.1870956421,47.832065434), test loss: 43.8951714993\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (48.0227813721,76.6023042171), test loss: 73.8571893692\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (41.5467224121,45.3278349945), test loss: 41.9250828743\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (75.9602508545,72.3263349009), test loss: 73.1732466221\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (48.4143257141,43.5672346265), test loss: 35.8322872639\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (36.256439209,69.2169312946), test loss: 58.4349809647\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (39.2584915161,42.2216284362), test loss: 43.5290939331\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (36.7061080933,66.8162583963), test loss: 73.7530347824\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (59.1079483032,41.171399229), test loss: 39.0034116745\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (125.695785522,64.8708970083), test loss: 69.3190393448\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (28.3559455872,40.3033491486), test loss: 37.501566267\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (34.9764556885,63.218065495), test loss: 59.4255550385\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (86.9903259277,39.5503486882), test loss: 42.7193399906\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (224.602233887,61.7985268567), test loss: 71.5752861023\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (33.8153495789,38.8063005106), test loss: 36.9552715302\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (26.5120315552,60.4904249047), test loss: 59.3974266052\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (20.3991794586,38.1187167525), test loss: 36.8721292019\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (34.9114570618,59.3077793646), test loss: 58.1913891315\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (35.8992805481,37.5115271331), test loss: 42.0024274349\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (63.7010421753,58.2347789305), test loss: 69.2893082619\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (34.9405899048,36.9450978206), test loss: 34.4306572437\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (29.244052887,57.221955993), test loss: 55.9435673237\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (33.8032684326,36.4059134372), test loss: 36.6104306698\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (34.4299240112,56.2693502373), test loss: 57.3426038742\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (47.7436904907,35.908653592), test loss: 40.3138644695\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (91.9271697998,55.3738375372), test loss: 66.8872874737\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (21.7426071167,35.4446332964), test loss: 33.1341542244\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (22.5201339722,54.5225154788), test loss: 54.1193067074\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (60.2650222778,35.013472032), test loss: 37.4225732803\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (171.624908447,53.7291351044), test loss: 57.4879228592\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (29.8660125732,34.5997647671), test loss: 38.1005966187\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (21.6454696655,52.9544100357), test loss: 66.1942501545\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (20.7146568298,34.2047235507), test loss: 31.7833948374\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (26.6297988892,52.2186969975), test loss: 53.2266419888\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (25.2402019501,33.8368075819), test loss: 37.5029312849\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (31.6306495667,51.5284561323), test loss: 60.6861341\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (25.6120567322,33.4798556846), test loss: 36.2311304092\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (26.1286525726,50.8658050598), test loss: 64.9695397377\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (27.4767532349,33.1271461309), test loss: 29.6652388334\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (31.5318241119,50.2312057048), test loss: 51.3808844566\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (39.7223052979,32.7892546638), test loss: 37.9065781116\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (70.1876296997,49.6291749307), test loss: 64.220252037\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (9.91695213318,32.4625317095), test loss: 35.0177471638\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (9.37400436401,49.0551607068), test loss: 63.9552927494\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (49.7435874939,32.1503828557), test loss: 29.9243172169\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (146.086593628,48.5198754216), test loss: 49.5076126814\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (27.092376709,31.8455581149), test loss: 37.8265392303\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (40.5015869141,47.9973014902), test loss: 65.5046033859\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (22.1324043274,31.5496336768), test loss: 32.7861417532\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (22.2970466614,47.4938855423), test loss: 61.5155340195\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (19.1482486725,31.2682734), test loss: 32.553937602\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (18.2394294739,47.0194799758), test loss: 52.9135678291\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.0105381012,30.9918670704), test loss: 37.3076129913\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (22.644317627,46.5615556291), test loss: 65.1304783821\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (23.4742965698,30.7151824571), test loss: 31.2201094627\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (27.6342887878,46.1192947172), test loss: 52.715644455\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (32.6877250671,30.4463794819), test loss: 33.1125827312\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (54.340423584,45.6963110447), test loss: 53.9721831322\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (9.151927948,30.1832686457), test loss: 37.2327693939\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (9.60677433014,45.2886206864), test loss: 64.573834753\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (42.0033798218,29.9309237675), test loss: 30.1853706598\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (144.217315674,44.9073447673), test loss: 51.5232019186\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (23.1751365662,29.6833367605), test loss: 33.7106109619\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (54.1113929749,44.5303667777), test loss: 54.5816714287\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (20.2525482178,29.4428001239), test loss: 36.0393733501\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (18.4574260712,44.1626038298), test loss: 63.5897623539\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (16.0836811066,29.2142508968), test loss: 29.9760613918\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (16.4884529114,43.8140633999), test loss: 51.4749367714\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.9909687042,28.9913757347), test loss: 34.8997907639\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (19.6964492798,43.4747987019), test loss: 55.6290726662\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (21.2951202393,28.769908519), test loss: 34.9314315796\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (27.0806827545,43.1440986217), test loss: 64.8862020016\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (19.3277626038,28.5548728123), test loss: 29.7531409979\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (28.0674190521,42.8250771882), test loss: 52.6275999069\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (10.6724510193,28.3433724534), test loss: 35.0536242247\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (9.91292095184,42.5148370363), test loss: 61.2162326813\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (36.2844238281,28.1391361799), test loss: 32.8495708227\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (120.987159729,42.2236009683), test loss: 61.267624855\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (16.2378120422,27.936011794), test loss: 28.3856953382\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (50.6367759705,41.9309192691), test loss: 50.3611711502\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (19.3156776428,27.7383732259), test loss: 35.8003162384\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (12.5187129974,41.6443130389), test loss: 62.3256380081\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (11.0434093475,27.5489209195), test loss: 31.6909241676\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (13.0256137848,41.369925168), test loss: 59.6596127748\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.0067453384,27.3643278386), test loss: 28.893510437\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (17.5032081604,41.1011530319), test loss: 49.1034405231\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.4740028381,27.1807944191), test loss: 35.684224844\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (24.2704391479,40.8373370291), test loss: 63.4610636711\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (10.8086071014,27.0025178778), test loss: 31.1089095592\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (11.9130954742,40.5807336344), test loss: 59.2338010788\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (11.8862752914,26.8278023877), test loss: 31.6795255661\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (12.402712822,40.3295104345), test loss: 51.975249958\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (27.1818008423,26.6602356326), test loss: 35.7861619473\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (96.5496749878,40.0919711286), test loss: 62.1236482143\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (11.1771297455,26.4943291069), test loss: 29.358998394\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (42.6978988647,39.851659155), test loss: 50.1811979294\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (20.5528755188,26.3337552812), test loss: 32.2357831478\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (14.4176235199,39.615469935), test loss: 52.5757169247\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.3507003784,26.179766049), test loss: 35.7020839214\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (14.6982440948,39.3878102029), test loss: 61.6335982323\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.3081359863,26.0295772053), test loss: 29.4436340094\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (16.4253234863,39.1633750532), test loss: 51.1446825504\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.7721157074,25.8801429299), test loss: 32.6211286068\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (21.0780944824,38.9425546875), test loss: 53.2416612625\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (10.6335296631,25.73417645), test loss: 34.1810174942\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (8.54573822021,38.7267215415), test loss: 61.5541651249\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (9.54307937622,25.5903208697), test loss: 29.7530208111\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (14.5561990738,38.5143952138), test loss: 51.5982245445\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (23.3709754944,25.4517851391), test loss: 34.7387720108\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (91.1477508545,38.3123807331), test loss: 54.9683763027\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.3181524277,25.3141352225), test loss: 33.6064052582\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (37.6239395142,38.1074188962), test loss: 61.8163298607\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.4105491638,25.1802139723), test loss: 28.9414698601\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (13.9990243912,37.905354745), test loss: 50.9097336769\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (11.7664613724,25.0512610279), test loss: 34.5701818228\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (15.3874835968,37.7093832302), test loss: 60.664648962\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (20.0885314941,24.9250212438), test loss: 32.2466965675\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (11.998087883,37.5155346538), test loss: 61.1534930706\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (14.0996923447,24.7988086201), test loss: 27.7791866302\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (21.0469722748,37.3243044907), test loss: 49.6114835262\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (8.94630718231,24.675034412), test loss: 34.8187878132\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (6.97286701202,37.136528274), test loss: 61.414097023\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (6.12120771408,24.5526423399), test loss: 31.0473082066\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (15.8859424591,36.9512760554), test loss: 59.6267629147\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (20.4080543518,24.4341628924), test loss: 28.7957146406\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (75.6299591064,36.7739917853), test loss: 49.2635215759\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (14.1678218842,24.3162020198), test loss: 34.8368876934\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (37.25207901,36.5935935623), test loss: 61.8595530033\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (20.9601020813,24.2009231513), test loss: 29.9605034113\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (11.3649711609,36.4152318548), test loss: 55.3713521004\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (11.7550039291,24.0894633765), test loss: 31.3432826996\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (14.4788284302,36.2415491194), test loss: 51.4470868111\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (20.3770103455,23.9800154128), test loss: 34.1783112049\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (7.00388526917,36.0690565861), test loss: 60.7157277107\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.496553421,23.8703960565), test loss: 28.9028700829\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (20.2449264526,35.8986906504), test loss: 50.7190831184\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (8.21615314484,23.7624528359), test loss: 31.5290883303\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (6.48999643326,35.7308129619), test loss: 52.3091459274\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (9.87084579468,23.6554939408), test loss: 34.8326281548\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (19.4045448303,35.5648542216), test loss: 61.7481426239\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (19.6014270782,23.5513385053), test loss: 28.5283209324\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (51.8742866516,35.4051020438), test loss: 48.9581185102\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (12.4160995483,23.4475094735), test loss: 32.3709555626\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (34.5349006653,35.242359221), test loss: 53.0629534245\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (18.2960109711,23.3457949647), test loss: 33.1776893377\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (10.0950222015,35.0811521612), test loss: 59.6860918999\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.5623130798,23.2470157429), test loss: 28.7013077021\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (15.3001909256,34.9234994723), test loss: 49.3904012203\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.4515399933,23.1497963949), test loss: 33.5311478138\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (4.09848833084,34.7667057541), test loss: 54.3162690639\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.65487003326,23.0522609861), test loss: 32.8368096352\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (13.9692087173,34.6115165518), test loss: 61.7247902393\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (8.53709793091,22.9560320568), test loss: 28.5269228935\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (5.59734582901,34.4581491471), test loss: 51.0204854012\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (10.6635284424,22.8603927914), test loss: 33.7618478298\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (17.9547729492,34.306411499), test loss: 60.7650588036\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (18.2422904968,22.7668662981), test loss: 31.3547101021\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (44.6749076843,34.1596502171), test loss: 58.2406878948\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (10.8318386078,22.6735954587), test loss: 27.3197502136\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (31.0374469757,34.0100551703), test loss: 48.2436140537\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (16.4345932007,22.5820136492), test loss: 34.6219412088\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (13.6747016907,33.8618254496), test loss: 61.142461586\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.4371948242,22.4927500747), test loss: 29.60660429\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (13.5550823212,33.7161433609), test loss: 56.3906502247\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (14.4799375534,22.4046801804), test loss: 28.5832553387\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (4.31265354156,33.5710203815), test loss: 49.7867991924\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (8.04615306854,22.3163126091), test loss: 34.0244317055\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (8.61848068237,33.4273597322), test loss: 61.8347045422\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (8.86364936829,22.2288266895), test loss: 29.6006956577\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (5.91683578491,33.285070555), test loss: 53.6245516777\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (10.0194301605,22.1418444818), test loss: 30.8242205381\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (23.7037811279,33.1441590153), test loss: 51.3643090248\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (19.1060638428,22.0565041713), test loss: 34.4434336662\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (42.5203323364,33.0071599014), test loss: 60.2344063759\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (10.4275913239,21.9712714984), test loss: 28.1939339161\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (28.3858718872,32.8677720661), test loss: 48.7132015228\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (14.1753463745,21.8874025529), test loss: 31.3616205931\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (14.1244258881,32.7293884165), test loss: 52.1846025467\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (11.2127895355,21.8055633273), test loss: 34.7883784771\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (10.9032754898,32.5930814044), test loss: 61.0461605072\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.8852272034,21.7245460766), test loss: 28.4337815762\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (6.246322155,32.4571940798), test loss: 49.5657441139\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (7.61258220673,21.6432731079), test loss: 31.9477377892\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (4.27546215057,32.3224222812), test loss: 53.3243555069\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (9.6983423233,21.562671472), test loss: 32.8173000574\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (8.39942836761,32.1888368208), test loss: 60.0746798038\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (8.97117233276,21.4824210495), test loss: 28.7991116047\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (24.6399116516,32.056430692), test loss: 49.9904186726\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (18.5152931213,21.4034442871), test loss: 34.151766634\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (40.8231315613,31.9271389514), test loss: 55.1621275902\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (10.2764396667,21.3246309715), test loss: 31.9966123581\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (17.8462333679,31.7958952679), test loss: 59.4933934212\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (14.3821992874,21.2468906084), test loss: 27.9392735958\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (15.4807815552,31.665487526), test loss: 49.4959750175\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (10.2097606659,21.1708139861), test loss: 34.1789665222\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (9.23365592957,31.5366620431), test loss: 60.9298485756\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (11.405172348,21.0954531893), test loss: 31.117048645\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (8.31090736389,31.4082303988), test loss: 59.343023634\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (7.14973545074,21.0198357479), test loss: 27.2551757812\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.15696430206,31.2807655002), test loss: 48.667419529\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (9.07481575012,20.9447021398), test loss: 34.2201714993\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (8.58523368835,31.1543309335), test loss: 61.9235501289\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (9.83775520325,20.8698771084), test loss: 29.4214586258\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (23.9300479889,31.0288853395), test loss: 57.1493877888\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (13.9712734222,20.7960603731), test loss: 29.2274238825\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (20.1871242523,30.9061680397), test loss: 50.9186408043\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.68922805786,20.7223420281), test loss: 34.1085912704\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (10.1753807068,30.7817094683), test loss: 61.8638118744\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (25.2716064453,20.6497974868), test loss: 29.049010849\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (22.2318267822,30.6580198813), test loss: 50.1870375156\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.2251291275,20.5782147011), test loss: 31.3545225143\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (9.52901077271,30.5355099111), test loss: 51.9086283207\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.217218399,20.5074047336), test loss: 33.9156974792\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (7.84879302979,30.4133984918), test loss: 60.8386750221\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (6.22678804398,20.4364129441), test loss: 28.5422030687\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.15090417862,30.2921530312), test loss: 50.0024291992\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (7.96798419952,20.3657801183), test loss: 31.2534725189\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (7.15570878983,30.1717695498), test loss: 53.1135315418\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (10.2224302292,20.2954114502), test loss: 34.5050346375\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (23.1160068512,30.0524123582), test loss: 61.6440014839\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (9.13763332367,20.225866309), test loss: 28.2115788937\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (12.4618053436,29.9352672584), test loss: 48.0826149464\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (8.3494310379,20.1564138955), test loss: 32.5399547577\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (7.65667533875,29.8166456756), test loss: 54.2491099358\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (28.0693588257,20.0880307144), test loss: 32.4697053909\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (21.5763282776,29.6987647172), test loss: 59.0209136009\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (9.35549640656,20.020326137), test loss: 28.4424682617\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (8.45303821564,29.5817629337), test loss: 48.7297703266\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (10.8513641357,19.953357288), test loss: 33.6991283417\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (6.79978179932,29.4651909979), test loss: 55.6201972008\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (6.00539445877,19.8862404586), test loss: 32.0293810844\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.21327114105,29.3495026184), test loss: 61.1676419258\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 2\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold2/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (318.836914062,inf), test loss: 228.872742081\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (379.436889648,inf), test loss: 251.001423264\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (42.5131530762,133.280574464), test loss: 65.850753212\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (74.4365692139,143.442101887), test loss: 86.2060443878\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (25.4898414612,88.9132151959), test loss: 42.7878234386\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (58.792224884,103.497222427), test loss: 73.5098136902\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (33.7390785217,72.549398184), test loss: 37.4185353279\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (43.1131019592,89.3801128658), test loss: 62.2196775436\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (52.7706413269,64.1724647781), test loss: 41.5371100426\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (61.4274139404,82.0032784145), test loss: 72.0181868553\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (40.7784347534,59.047652529), test loss: 38.5921785831\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (46.2515068054,77.487422011), test loss: 70.680500412\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (41.1138153076,55.5782642243), test loss: 36.7588447571\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (71.9201507568,74.3578478163), test loss: 59.7625090599\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (119.725341797,53.0140307119), test loss: 40.7176165104\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (92.2650756836,71.9710220351), test loss: 72.8408657074\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (5.81635189056,50.9940555812), test loss: 38.2974444389\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (7.6176366806,70.0666839812), test loss: 63.3837809563\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (35.142829895,49.3705090365), test loss: 37.0932727814\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (42.7437667847,68.5038818222), test loss: 61.3324406624\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (25.7594642639,48.0223850384), test loss: 40.4180891037\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (20.0399837494,67.1698041789), test loss: 71.3486854553\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (28.5628318787,46.8646030674), test loss: 35.0410159588\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (37.2335166931,66.0040129844), test loss: 58.3837489128\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (66.4007339478,45.8525682068), test loss: 37.2974369049\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (103.089103699,64.9531717481), test loss: 60.2773924828\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (7.64255905151,44.9520243887), test loss: 37.2535564899\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (7.31452941895,64.006242075), test loss: 66.1028868675\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (40.0386886597,44.1400286087), test loss: 32.1047612667\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (31.989824295,63.1040953511), test loss: 54.8550065994\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (72.9965667725,43.3978952099), test loss: 35.7914248466\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (102.133155823,62.2802380362), test loss: 63.4839295387\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (43.3123474121,42.7134651722), test loss: 32.9994271278\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (47.1950836182,61.4876269856), test loss: 61.9392285347\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (42.7228393555,42.0748585481), test loss: 29.2624153137\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (29.6196308136,60.7252158083), test loss: 49.2389533997\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (29.3522663116,41.457087467), test loss: 35.205748558\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (72.0227661133,59.9934021419), test loss: 64.2580343246\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (20.9715061188,40.875685629), test loss: 31.4467473984\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (25.1287269592,59.2842846013), test loss: 53.6983565331\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (34.7926673889,40.3273236984), test loss: 31.3942076206\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (42.4138412476,58.6078551907), test loss: 53.0563720703\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (9.7811164856,39.7968084764), test loss: 34.482032299\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (22.0329914093,57.9516939435), test loss: 62.1310419083\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (55.9304580688,39.2958736652), test loss: 28.9057805538\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (178.362182617,57.3326112831), test loss: 49.1062057495\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (22.2597160339,38.8083058006), test loss: 31.9585019112\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (19.9524536133,56.7127887443), test loss: 52.7002416611\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (23.8563289642,38.3332755367), test loss: 31.4337704182\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (24.6716957092,56.1100782543), test loss: 58.314688015\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (12.2260532379,37.8604556691), test loss: 26.8476799011\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (16.9536437988,55.5323170808), test loss: 47.3812950134\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (35.1008033752,37.409190975), test loss: 30.2991270781\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (48.9097824097,54.9708077437), test loss: 50.4699655533\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (19.3837852478,36.9731293839), test loss: 28.6318364024\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (20.6520290375,54.4197675539), test loss: 56.2882060051\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (26.9824905396,36.5465523079), test loss: 24.5170715034\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (32.9850692749,53.8889523537), test loss: 44.4489769459\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (25.1204147339,36.1371315413), test loss: 30.3968871295\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (23.3986701965,53.3709513395), test loss: 58.0272082329\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (14.3139772415,35.7441087957), test loss: 27.2312349319\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (42.0603065491,52.8760107299), test loss: 55.6234508038\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (37.0740432739,35.3651447619), test loss: 26.2758641958\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (42.6977272034,52.3970581348), test loss: 47.4231841564\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (38.271648407,35.0034484022), test loss: 28.8010926127\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (78.0684967041,51.9476912161), test loss: 56.2301849842\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (34.0414962769,34.6520541117), test loss: 25.7292653084\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (25.3868865967,51.4972719598), test loss: 46.1380342007\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (5.73767709732,34.313551837), test loss: 27.6333571672\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (12.3547935486,51.0633544015), test loss: 48.3771888733\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (22.9702911377,33.9906599854), test loss: 28.9261643529\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (18.862203598,50.650961327), test loss: 55.2748605728\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (42.9863128662,33.6819840539), test loss: 25.4312189579\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (46.6183013916,50.2517891628), test loss: 45.3419257879\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (8.74777698517,33.3803354346), test loss: 28.4255481958\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (30.3742790222,49.8578695133), test loss: 48.5286445618\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (18.5570907593,33.086752531), test loss: 27.3457152367\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (26.4822978973,49.4777932668), test loss: 54.3166383266\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (18.8985385895,32.8021244309), test loss: 23.9268587708\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (23.9338417053,49.1061022629), test loss: 44.1539171696\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (8.24539756775,32.5226164705), test loss: 27.8010564804\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (45.9582061768,48.7484756106), test loss: 53.7559141159\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (3.96721458435,32.2500368268), test loss: 24.6902545691\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (10.5195407867,48.3985123368), test loss: 51.8820236683\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (12.3456745148,31.9856541855), test loss: 23.5581869125\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (28.8738708496,48.0702780232), test loss: 41.7541105032\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (24.931848526,31.7256043918), test loss: 27.3317193031\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (65.0117797852,47.7396721171), test loss: 54.2507674217\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (20.159538269,31.4723292649), test loss: 24.5007159472\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (110.07408905,47.4189406627), test loss: 44.8563401699\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (42.0877532959,31.2304000973), test loss: 26.2162375927\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (67.8192672729,47.1075493807), test loss: 46.2156092644\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (13.6333646774,30.9953648305), test loss: 27.6635560513\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (15.6672115326,46.8034268035), test loss: 53.31676054\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.1893901825,30.765999535), test loss: 24.4471423626\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (49.6266937256,46.5048038587), test loss: 44.2234474182\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.3451023102,30.5419793678), test loss: 27.8610747814\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (15.127737999,46.2122281661), test loss: 47.8061137199\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (9.61861419678,30.3239855096), test loss: 26.8492123127\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (25.3988571167,45.9262338468), test loss: 53.3547523499\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (15.6334285736,30.1110747665), test loss: 24.1201730549\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (22.1592922211,45.6482128911), test loss: 45.0359767437\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (8.18658828735,29.9046561286), test loss: 27.5110897958\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (34.5608177185,45.3760133269), test loss: 52.8637835979\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (20.6354560852,29.7056038907), test loss: 24.1007168114\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (15.4934196472,45.1174869556), test loss: 50.1447340488\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (17.4079990387,29.5096980134), test loss: 22.6012453675\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (19.8906822205,44.8570133459), test loss: 40.8232498169\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (22.696521759,29.3184904406), test loss: 27.6307097673\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (21.2572250366,44.6027774235), test loss: 53.201244545\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (25.5293827057,29.1345929372), test loss: 24.1830603123\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (38.83543396,44.3550998984), test loss: 49.5806575298\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (45.561882019,28.9555265607), test loss: 25.805206728\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (41.9271240234,44.1116788383), test loss: 45.3130691528\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (21.8229255676,28.7778311539), test loss: 26.6175143719\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (12.1534738541,43.870132519), test loss: 52.2665147305\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (17.3679599762,28.604099745), test loss: 24.3715405703\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (25.4699001312,43.6339394204), test loss: 43.311837244\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (14.2558994293,28.4340423956), test loss: 26.6308797836\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (25.9275188446,43.4018401623), test loss: 46.6744175434\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.5104751587,28.2669364564), test loss: 27.036224556\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (19.9606647491,43.174040449), test loss: 50.3531239033\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (12.9596824646,28.1040130709), test loss: 24.5373506308\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (47.4005126953,42.9517856234), test loss: 43.9445487499\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (5.79939889908,27.94580281), test loss: 27.4190424204\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (20.6088829041,42.7373481409), test loss: 46.3225981712\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (17.0799274445,27.7900041769), test loss: 25.0117480516\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (15.0382604599,42.5212670754), test loss: 50.1057227612\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (47.1509933472,27.6380809034), test loss: 22.8184981346\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (54.7819137573,42.311165446), test loss: 41.8083892345\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (29.5622177124,27.4897246815), test loss: 27.2956682682\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (31.3465538025,42.1032843788), test loss: 51.1860598564\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (24.2150440216,27.3451690592), test loss: 24.2723927498\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (10.7707386017,41.8983730696), test loss: 49.3220863819\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (7.9673075676,27.2008497273), test loss: 23.9346015692\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (17.5188140869,41.6953196159), test loss: 41.3619469643\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (8.35050773621,27.0596514009), test loss: 27.2057440758\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.63317871094,41.4952345033), test loss: 53.0777848244\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (10.9200620651,26.9209542377), test loss: 24.84569664\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (11.6196212769,41.2985563079), test loss: 44.385563755\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (9.04689598083,26.7839370304), test loss: 26.2508930683\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (13.8863945007,41.1044755245), test loss: 45.9392682552\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (44.3155593872,26.6508287169), test loss: 27.1196778774\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (129.8540802,40.9178484641), test loss: 51.2528828144\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (13.2254371643,26.5191138381), test loss: 24.0734723687\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (10.2301311493,40.7300286162), test loss: 43.0436984539\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (8.94837188721,26.3897604051), test loss: 27.7229767382\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (14.674703598,40.5440081254), test loss: 46.7249948978\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (15.8243846893,26.2634051478), test loss: 25.8220437527\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (23.6400051117,40.3626604306), test loss: 50.0730179787\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.4130659103,26.1391975643), test loss: 23.535399735\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (24.9020767212,40.182103505), test loss: 42.6422308445\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (16.9302597046,26.0179213056), test loss: 27.5547060728\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (14.3563814163,40.0033166397), test loss: 51.977256918\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (20.2718734741,25.8965874386), test loss: 23.4675433993\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (34.0878448486,39.8266665996), test loss: 48.3484872341\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (29.0740680695,25.7774534809), test loss: 23.0870492339\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (27.8039932251,39.6511443939), test loss: 41.052892828\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.20091152191,25.65929633), test loss: 27.8766251564\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (23.2425575256,39.4781712519), test loss: 53.4161018848\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (28.9441146851,25.5432099127), test loss: 24.0826636791\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (48.4262466431,39.3076261786), test loss: 42.3237328529\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (23.8238067627,25.4292879833), test loss: 26.1523805141\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (67.6310119629,39.1429516559), test loss: 44.9624964237\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (13.4415855408,25.3162532116), test loss: 27.1034065962\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (19.7821960449,38.9749589476), test loss: 51.2536099911\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (8.01885986328,25.2048615275), test loss: 23.8826336622\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (6.20331764221,38.8090990586), test loss: 42.1339606285\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (9.21088600159,25.0959533363), test loss: 27.2002267838\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (12.3848085403,38.6474751229), test loss: 46.2447340488\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (37.9359741211,24.9891785802), test loss: 26.5637917519\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (38.2936210632,38.4866367451), test loss: 50.7719450951\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (11.5489549637,24.8832074715), test loss: 24.5018957138\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (18.2355213165,38.3258208467), test loss: 43.2253259182\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (24.911491394,24.7780238974), test loss: 27.6752043724\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (13.1073646545,38.1671966469), test loss: 46.3253402233\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.1741733551,24.6736790188), test loss: 25.3377444267\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (28.2640953064,38.00958354), test loss: 50.3794617176\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (8.37515449524,24.5703365118), test loss: 23.6939625502\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (17.6608009338,37.8534758925), test loss: 42.9392628193\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (6.19571208954,24.4684085061), test loss: 27.7478179932\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (17.8860301971,37.6993383337), test loss: 51.9477669716\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (9.60812854767,24.368083135), test loss: 23.6591389775\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (10.2744026184,37.5497343511), test loss: 47.0820520401\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (18.8061962128,24.2686209403), test loss: 25.073418045\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (19.825717926,37.3976439316), test loss: 43.6670311451\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (12.4043560028,24.1698724762), test loss: 27.3506217003\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (70.568069458,37.2478685028), test loss: 52.5564152718\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (36.6018943787,24.0738100174), test loss: 23.9416577578\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (32.6522369385,37.0997081305), test loss: 41.9826822281\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (17.5058135986,23.9783119502), test loss: 26.8277384996\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (16.896648407,36.952347758), test loss: 46.3907214642\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (14.581161499,23.883767554), test loss: 27.8018352628\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (35.8396835327,36.805631856), test loss: 52.4732025862\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (9.74754047394,23.7897810119), test loss: 24.8862724423\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (17.0228290558,36.6599920918), test loss: 43.6282773495\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (12.7048473358,23.696393972), test loss: 28.1804064751\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (9.2511920929,36.5150926378), test loss: 47.1633080959\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (6.11353778839,23.6036364257), test loss: 26.3593859673\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (21.9495258331,36.3718660107), test loss: 50.8274046898\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (6.9309463501,23.5120225439), test loss: 23.5783852577\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (25.1864585876,36.2299225135), test loss: 41.7714192867\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (12.4265937805,23.4216392922), test loss: 28.1389405727\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (15.8879852295,36.0913110144), test loss: 51.6334942818\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (8.27889156342,23.331861714), test loss: 23.6930957317\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (31.8368034363,35.9511831869), test loss: 48.0026350021\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (10.295331955,23.2425473882), test loss: 23.8245593548\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (25.6001586914,35.8122884807), test loss: 40.7744354248\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.2579421997,23.1554716426), test loss: 27.9512757301\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (28.7550430298,35.6748969994), test loss: 52.4826991558\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (30.7679004669,23.069126097), test loss: 24.5535523415\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (22.2269020081,35.5379078095), test loss: 42.7042379856\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (19.9605731964,22.982854638), test loss: 26.7696182728\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (19.091796875,35.4013707644), test loss: 45.6989031792\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (4.65065097809,22.8970303323), test loss: 28.4829400539\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.88745045662,35.2655838298), test loss: 52.8884719372\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.81404018402,22.8118666594), test loss: 24.4897463799\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (20.5137062073,35.1308973408), test loss: 42.5445319176\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (14.4030208588,22.7273170888), test loss: 29.058100152\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (19.7115364075,34.9971753477), test loss: 48.3955756187\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (6.24067974091,22.6433465), test loss: 26.6025815725\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (37.6222953796,34.8649365252), test loss: 50.213902092\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (13.5409603119,22.5605549921), test loss: 24.2461175203\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (19.9538459778,34.7345603958), test loss: 42.8211711884\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.21875,22.4781369241), test loss: 28.8137339473\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (7.00127458572,34.6031840591), test loss: 49.8745019913\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (21.0900917053,22.3962235719), test loss: 24.6217421055\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (12.6448402405,34.4729825198), test loss: 48.4711430073\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (16.339630127,22.3159306775), test loss: 23.8159554005\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (13.4945602417,34.3439003669), test loss: 40.8382509708\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (15.2942695618,22.2362611107), test loss: 28.7099982977\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (6.91586446762,34.2150246084), test loss: 53.4147053719\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (4.49718570709,22.1564012377), test loss: 24.5659785867\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (8.27914619446,34.0865949553), test loss: 48.3794212818\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (11.4229297638,22.07728122), test loss: 27.4953168631\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (15.3392305374,33.9590158199), test loss: 46.7583055496\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (9.57385635376,21.9984500221), test loss: 27.9580097198\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (7.65676212311,33.8319472363), test loss: 53.6931262016\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (13.8689117432,21.9202377126), test loss: 25.2007499456\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (8.48097038269,33.7058835414), test loss: 43.2560628891\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 3\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold3/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (315.045593262,inf), test loss: 216.106476974\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (338.474853516,inf), test loss: 214.901042175\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (57.4727478027,143.13299661), test loss: 79.7027129173\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (36.6397209167,98.5389571438), test loss: 67.4177293777\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (44.2959060669,97.5819578805), test loss: 42.7059741974\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (199.109588623,78.9421175804), test loss: 73.609610939\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (108.190345764,77.5628152145), test loss: 37.4879159927\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (96.7706680298,71.8744224418), test loss: 71.8052267075\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (33.9850006104,67.229106631), test loss: 35.3763337135\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (64.3487854004,68.0080535495), test loss: 63.0412405968\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (14.2089004517,60.9190959417), test loss: 38.0141261101\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (44.0506591797,65.5395633696), test loss: 68.436919117\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (29.2457256317,56.6271280069), test loss: 37.0612625599\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (31.1756439209,63.6813315485), test loss: 69.7372182846\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (27.6093215942,53.498421693), test loss: 32.7459806919\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (30.5186500549,62.224053392), test loss: 58.0083233356\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (21.0506153107,51.0785139308), test loss: 36.2240228653\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (26.3768291473,60.9566206819), test loss: 60.2528929234\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (49.3449287415,49.1381240644), test loss: 36.7664804459\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (64.5994186401,59.8956064731), test loss: 66.6578028679\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (39.0645141602,47.5289013035), test loss: 31.9757171631\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (38.0242576599,58.9378335671), test loss: 55.3803901672\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (23.3041820526,46.1625112786), test loss: 33.886135745\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (48.3151702881,58.0673167941), test loss: 56.3452135563\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (22.3186836243,44.9729425999), test loss: 36.0410874367\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (22.7686004639,57.2282349671), test loss: 65.8692771435\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (9.16370296478,43.9069685961), test loss: 32.4968296528\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (20.903837204,56.4476255507), test loss: 56.7836937904\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (37.3344726562,42.9418148499), test loss: 31.7720488071\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (30.729927063,55.6745301524), test loss: 53.7794948578\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (36.2564086914,42.0740234558), test loss: 35.7582099915\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (60.4345169067,54.9271327018), test loss: 64.1447030067\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (25.9825019836,41.282912805), test loss: 30.8891268492\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (39.7409362793,54.2002469888), test loss: 60.5118786335\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (18.4723148346,40.5446495597), test loss: 28.1449615717\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (42.5481338501,53.5050380802), test loss: 49.0688385963\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (23.2813224792,39.8590686051), test loss: 32.4974807024\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (13.4531135559,52.8441895752), test loss: 58.0883195877\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (60.9906196594,39.2243043792), test loss: 31.8887131929\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (73.3919448853,52.1996607689), test loss: 58.9430439949\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (24.4663543701,38.6316813657), test loss: 27.1807605505\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (62.2777786255,51.5720032714), test loss: 47.4358925819\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (20.8238945007,38.0666806609), test loss: 31.5014298439\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (68.4448471069,50.9621693467), test loss: 51.085469532\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (23.6639652252,37.532224371), test loss: 32.3406303406\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (18.6698455811,50.3875674192), test loss: 56.7636313438\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (15.4973697662,37.0211817073), test loss: 27.6120205641\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (21.1534729004,49.8151834197), test loss: 47.4365022659\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (10.9810247421,36.526210989), test loss: 28.7111123323\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (7.53100585938,49.2356403853), test loss: 47.657750988\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (21.4110851288,36.0418189773), test loss: 32.7497770309\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (26.9430484772,48.6695481803), test loss: 58.802247715\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (15.7397518158,35.5773167952), test loss: 26.9883640766\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (23.1532020569,48.1470200282), test loss: 52.390339756\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (19.5766525269,35.1306421376), test loss: 25.8719194889\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (10.1812915802,47.6340614888), test loss: 46.3868747234\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (21.4751319885,34.7176456994), test loss: 30.5861843586\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (20.372013092,47.1530707864), test loss: 55.8137049675\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (15.875535965,34.3126260226), test loss: 27.8742021084\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (22.6446094513,46.6868176813), test loss: 54.7360277176\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (52.2958297729,33.9251097735), test loss: 23.9258264899\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (91.9357070923,46.249487477), test loss: 42.749642086\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (15.7123756409,33.5542901222), test loss: 29.3552742004\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (41.9502830505,45.8308266147), test loss: 48.1275362968\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (37.987411499,33.2097362915), test loss: 28.7505714893\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (17.864610672,45.4287723249), test loss: 53.7436847687\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (30.4383449554,32.8704981488), test loss: 24.7011179924\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (13.9861049652,45.0363431181), test loss: 44.1961946487\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (12.7984237671,32.5417274249), test loss: 27.8791094065\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (62.4303512573,44.6663461119), test loss: 46.3611125946\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (11.7586154938,32.2297586853), test loss: 30.5746477842\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (15.601940155,44.3111741909), test loss: 56.0000235558\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (12.1005477905,31.9339203085), test loss: 25.7477933168\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (17.5230941772,43.9730605481), test loss: 51.3499125957\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (22.9846229553,31.6463748441), test loss: 26.4598700047\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (18.5843200684,43.6375749977), test loss: 45.5445081711\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (4.17033243179,31.3648695296), test loss: 30.1226815701\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (15.8895463943,43.3167048254), test loss: 54.9928415298\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (8.82035255432,31.0957459563), test loss: 25.2730414867\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (11.1907844543,43.0119910107), test loss: 51.7005002975\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (29.3544445038,30.8384437792), test loss: 22.6858804703\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (36.8601951599,42.7164443342), test loss: 41.2791090012\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (20.7724685669,30.5890366262), test loss: 28.1569760561\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (34.6924552917,42.4239371024), test loss: 46.8760793209\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (14.5628299713,30.3434738851), test loss: 27.1600239754\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (41.5610923767,42.141422381), test loss: 52.6630188465\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (29.934627533,30.1082526959), test loss: 23.3571561813\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (35.7501602173,41.8735601555), test loss: 42.179709053\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (40.3032913208,29.8816681778), test loss: 28.1249769449\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (36.4760246277,41.6110253425), test loss: 46.3336854935\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (13.2658481598,29.6623429597), test loss: 29.2229607582\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (16.1746635437,41.3510537637), test loss: 53.0397274494\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (14.4584875107,29.4449170706), test loss: 24.6998926163\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (17.7854423523,41.0982951893), test loss: 44.5292102814\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (9.24283504486,29.2350415321), test loss: 26.3443501949\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (33.4200134277,40.8580192795), test loss: 44.906005764\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (13.5109596252,29.0325142596), test loss: 29.9478452921\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (25.8732452393,40.620771831), test loss: 54.8031609535\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (25.7131080627,28.837860047), test loss: 24.7714933157\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (25.91746521,40.3874603509), test loss: 50.0935104847\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (10.0997753143,28.6429750909), test loss: 23.4329815149\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (34.531829834,40.158038467), test loss: 43.9908842564\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (8.61251068115,28.4542889398), test loss: 27.9735297441\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (26.7006893158,39.9391833245), test loss: 52.1163861275\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (11.3591346741,28.2708682409), test loss: 25.5461661577\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (20.3350601196,39.7216742191), test loss: 50.5421448708\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (8.45194149017,28.0950171359), test loss: 22.8285214663\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (16.291305542,39.5082976302), test loss: 41.6855957985\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (10.5300645828,27.9192972642), test loss: 27.5977873802\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (19.9986305237,39.2973066108), test loss: 46.2379210472\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (14.6331653595,27.747756353), test loss: 27.5265261173\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (21.7425823212,39.0968851659), test loss: 50.850440836\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (46.4304122925,27.5808077479), test loss: 23.1081964016\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (58.9570884705,38.8960549265), test loss: 41.8960491657\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.4775009155,27.4203403937), test loss: 26.4559165716\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (44.3476715088,38.6989134844), test loss: 45.2312748909\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (8.94980144501,27.2600004751), test loss: 29.1351956367\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (7.46663093567,38.5026446743), test loss: 54.4890783787\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (26.8057193756,27.1025527904), test loss: 24.1337001801\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (93.2685394287,38.3165171434), test loss: 48.2823737144\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.3057098389,26.9481276226), test loss: 24.6588675976\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (27.4618968964,38.1284442428), test loss: 43.764986515\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (16.6172027588,26.8015469452), test loss: 28.3755111217\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (18.487909317,37.9449269109), test loss: 52.763811636\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (23.9087352753,26.6542148871), test loss: 24.9333319664\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (48.0595970154,37.7624240157), test loss: 50.862130928\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.4101219177,26.50758753), test loss: 21.8527778864\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (50.4546546936,37.5841485084), test loss: 40.2766620159\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (9.89736938477,26.3655963538), test loss: 27.3057482243\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (76.7926483154,37.4106696817), test loss: 45.7595584869\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (36.6721954346,26.2293879658), test loss: 26.0471208096\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (25.7488594055,37.2385364894), test loss: 50.8703380108\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.2273492813,26.0930113335), test loss: 22.5915175676\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (24.3346633911,37.065982047), test loss: 41.2251055241\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (8.13658618927,25.9571106718), test loss: 26.320641613\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (21.9738426208,36.8972946856), test loss: 44.6017669678\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (9.42214584351,25.824825097), test loss: 28.3964310646\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (17.7648334503,36.7329010689), test loss: 53.6713173866\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (39.2740745544,25.6970722905), test loss: 23.7919969082\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (33.5545158386,36.5714317963), test loss: 42.6081035137\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (18.0626010895,25.5701999516), test loss: 25.5683225393\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (13.9848823547,36.4078441532), test loss: 44.7364900589\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (29.4872894287,25.4436533057), test loss: 29.112843442\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (33.3803749084,36.2480419899), test loss: 53.7113189697\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (21.5784568787,25.3196815379), test loss: 23.969900465\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (14.616230011,36.0921633096), test loss: 49.5461854458\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (10.7700901031,25.1987694026), test loss: 22.0951645613\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (7.89804363251,35.9375159489), test loss: 39.9519958496\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (5.02787923813,25.0804098163), test loss: 27.4471138\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (4.56118202209,35.782486113), test loss: 52.7924463749\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (12.5986328125,24.9612931731), test loss: 25.4544004202\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (6.78118467331,35.6296043726), test loss: 50.9039725304\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (11.0853500366,24.8446612524), test loss: 22.7155152559\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (15.0206279755,35.4814700137), test loss: 41.8642236233\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (19.2682418823,24.7306626532), test loss: 27.2347809315\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (31.1157207489,35.3338709077), test loss: 45.306256485\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (18.1358375549,24.6197203652), test loss: 26.9660971642\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (15.8633537292,35.1860982931), test loss: 50.8842135429\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.40597629547,24.5069400108), test loss: 22.7423195362\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (13.4641885757,35.039508609), test loss: 41.4267385483\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.9688930511,24.3968370706), test loss: 25.6166303873\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (6.10157728195,34.8968945652), test loss: 44.0066820621\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (28.902261734,24.2887836869), test loss: 28.7400535583\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (33.7001800537,34.755084487), test loss: 53.0041129589\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (20.0861091614,24.1834401318), test loss: 23.6510621309\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (27.1019134521,34.6134099893), test loss: 48.7854486465\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (6.99456787109,24.0769847087), test loss: 24.3231938362\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (24.7339248657,34.4721686158), test loss: 44.8765460014\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (16.3255157471,23.9723176916), test loss: 28.0825402498\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (9.7115688324,34.3352640892), test loss: 52.7504346848\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (16.2355880737,23.8692421484), test loss: 24.6557743549\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (22.0379695892,34.1979831426), test loss: 49.2203038216\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (21.5975990295,23.7691962314), test loss: 21.8634486914\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (15.5120716095,34.0614138087), test loss: 41.0089519978\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (9.15697669983,23.668124563), test loss: 27.3428746462\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (27.6450614929,33.9251216466), test loss: 46.0941604614\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (20.9822349548,23.5680911899), test loss: 26.0124178171\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (32.3302726746,33.7929459071), test loss: 49.4600728035\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (21.2536811829,23.469357228), test loss: 22.5596451759\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (14.4830579758,33.6594510634), test loss: 40.6751702309\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (12.0415410995,23.3739655935), test loss: 26.0612236738\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (6.54319381714,33.527235688), test loss: 45.1210481167\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (11.7954006195,23.2779264136), test loss: 28.2974564552\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (30.0944309235,33.3956792451), test loss: 53.6512653351\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (19.5885028839,23.1818456329), test loss: 23.2512242794\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (32.7592353821,33.2662414962), test loss: 41.2183943272\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (14.8057346344,23.0873840717), test loss: 25.3009644032\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (69.7415542603,33.1378272088), test loss: 44.4685810566\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (20.6218833923,22.9960875508), test loss: 28.6508483887\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (6.73898029327,33.0098689728), test loss: 54.7644010544\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (7.29648113251,22.9039834645), test loss: 23.5532649994\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (7.03659248352,32.881475138), test loss: 49.3163383484\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (8.37371253967,22.8118306128), test loss: 21.9970909834\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (32.9080200195,32.7552330896), test loss: 40.061115551\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (7.90135765076,22.721298798), test loss: 27.5907432556\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (5.406291008,32.6301520911), test loss: 52.301044035\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (15.7380714417,22.6329340274), test loss: 25.9079363823\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (13.2253026962,32.506611709), test loss: 51.7514830589\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (24.7770118713,22.5450492346), test loss: 22.5955208302\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (21.7707500458,32.3815758188), test loss: 41.8472860813\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (8.13226985931,22.4564602401), test loss: 27.6317228317\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (15.9429683685,32.2583198004), test loss: 46.2398274422\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (14.6093158722,22.3692371037), test loss: 27.7607598543\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (9.8077507019,32.1367372892), test loss: 52.1506140709\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (17.8073539734,22.2835680233), test loss: 22.7042913914\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (27.208026886,32.0158985463), test loss: 41.2845028877\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.5471200943,22.1989518424), test loss: 25.6975543976\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (15.8027772903,31.8939990051), test loss: 45.4662278175\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (10.9313850403,22.1137109017), test loss: 28.8014948845\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (27.2242450714,31.7734493187), test loss: 54.0808229923\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (16.8624134064,22.0296073366), test loss: 24.0372871876\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (22.8216018677,31.6550755053), test loss: 50.2429278374\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (31.1676864624,21.9469225455), test loss: 23.5717413902\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (20.2375507355,31.536698868), test loss: 44.107116127\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.68236351013,21.8653722346), test loss: 28.4755068779\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (9.23979282379,31.4177821815), test loss: 54.8005722046\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (13.817358017,21.7828151097), test loss: 25.2380372286\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (16.189994812,31.2996504008), test loss: 51.6293783188\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (8.00053787231,21.7014928191), test loss: 22.6794304848\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (23.0207424164,31.1837703536), test loss: 42.4823900223\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (7.2320394516,21.6211229058), test loss: 27.71239748\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (10.6137886047,31.0677521459), test loss: 46.7870717049\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (18.5577278137,21.5424873874), test loss: 26.3221191883\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (17.0647525787,30.9519417391), test loss: 50.9394782066\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (4.39785957336,21.4626012552), test loss: 22.8944738626\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (26.2968254089,30.83620682), test loss: 41.9063688755\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (10.6318368912,21.3838173703), test loss: 26.6180151939\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (15.2386903763,30.7226536684), test loss: 46.1267765999\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (4.90680217743,21.3057489829), test loss: 28.8355672121\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (12.7880811691,30.6089545466), test loss: 54.2564768314\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (5.05233860016,21.2293849075), test loss: 24.0604474783\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (9.09862804413,30.4954529423), test loss: 50.3354912281\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.6029791832,21.1522681308), test loss: 25.6424095631\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (16.2115154266,30.3821016497), test loss: 45.7634175301\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (11.9368686676,21.0756540185), test loss: 29.0147797108\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (13.3677864075,30.2711380037), test loss: 55.002695179\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (24.5665779114,20.9998942842), test loss: 23.4435173035\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (25.6884460449,30.1596336568), test loss: 49.4278605461\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (11.4610528946,20.9255830752), test loss: 22.165686655\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (29.1469802856,30.0484990583), test loss: 41.9991487026\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 4\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold4/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (352.803833008,inf), test loss: 244.809313965\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (327.428497314,inf), test loss: 194.029033279\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (162.152252197,170.264990135), test loss: 122.507045937\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (76.5773391724,100.974363184), test loss: 61.8154994488\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (45.0207061768,131.190698901), test loss: 70.2812707901\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (19.0047645569,80.8133869538), test loss: 69.657660675\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (23.2681732178,102.979188832), test loss: 36.7249258518\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (49.0250358582,73.3269785601), test loss: 57.2679677486\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (31.9204959869,86.4675958941), test loss: 38.421739006\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (44.6522140503,69.1995816464), test loss: 64.299277401\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (12.2379741669,76.3973776495), test loss: 37.7161756039\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (34.6816291809,66.6188102437), test loss: 67.8838376999\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (20.271894455,69.6332348472), test loss: 34.9648273945\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (20.120803833,64.8340157017), test loss: 54.459423542\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (42.0577087402,64.764948187), test loss: 38.1006784439\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (58.3180847168,63.4573693982), test loss: 65.1049804688\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (67.6981964111,61.0626297213), test loss: 37.0812227249\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (57.0352210999,62.3129920774), test loss: 60.2694561005\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (20.058467865,58.1138971504), test loss: 35.1970965385\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (30.3892841339,61.3261796316), test loss: 54.0810493469\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (17.8701095581,55.7282982955), test loss: 39.2824206829\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (20.4908828735,60.4759831851), test loss: 65.3390916824\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (49.3765869141,53.7499940918), test loss: 34.3294477463\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (35.431098938,59.712738202), test loss: 55.2115777016\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (33.4000244141,52.0789317509), test loss: 35.4262864113\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (102.937110901,59.0561138974), test loss: 53.7921960831\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (39.4007263184,50.6261537779), test loss: 37.374146843\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (53.4602661133,58.411049457), test loss: 66.0459136963\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (50.9940795898,49.3598431949), test loss: 31.5147280693\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (128.817642212,57.8342541064), test loss: 52.1752309799\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (36.7503128052,48.2327383837), test loss: 36.3604541779\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (45.3792266846,57.2563378043), test loss: 61.1855421066\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (37.5124588013,47.2312100337), test loss: 34.7198666096\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (42.1900177002,56.7120643882), test loss: 62.9668745995\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (69.1695251465,46.3251249672), test loss: 32.1816249371\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (89.4073028564,56.2049539901), test loss: 49.610115242\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (33.5777244568,45.5022659899), test loss: 35.4501793385\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (49.8354873657,55.7065338579), test loss: 60.879090023\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (74.3794631958,44.7433176305), test loss: 32.9667615891\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (66.8652801514,55.2175827153), test loss: 52.8915361404\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (8.96300125122,44.0316170204), test loss: 32.0187422752\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (21.4290847778,54.7312473982), test loss: 49.5534502983\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (29.7075157166,43.3573366635), test loss: 36.1574492455\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (39.1123123169,54.2529801361), test loss: 62.0224906921\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (23.04895401,42.7218848925), test loss: 30.7059608698\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (42.9463653564,53.7817165162), test loss: 50.6239931583\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (24.0288543701,42.1311505326), test loss: 32.1871318817\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (44.1609115601,53.3359045408), test loss: 52.4918551445\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (6.9140586853,41.5646461221), test loss: 34.19489007\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (13.6638526917,52.8932808954), test loss: 62.4708869934\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (54.294380188,41.0296739907), test loss: 27.6940458775\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (113.568977356,52.4671003339), test loss: 46.4074868202\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (22.1570720673,40.5193003992), test loss: 34.0194385529\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (7.8461151123,52.0491531024), test loss: 59.3560688496\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (22.0755577087,40.0339063288), test loss: 31.1820670605\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (14.7919120789,51.6358062306), test loss: 53.1471241951\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (23.9040107727,39.5678158555), test loss: 28.7259299278\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (16.9475288391,51.2369445314), test loss: 46.0969026089\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (34.8511199951,39.1272947377), test loss: 32.8550974369\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (52.4952850342,50.85030898), test loss: 57.8883706093\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (24.4179611206,38.700344461), test loss: 29.1635912418\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (32.6496620178,50.469792513), test loss: 48.9527323246\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (9.077501297,38.2896657539), test loss: 29.3551701069\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (28.0085468292,50.0947811548), test loss: 47.5383991718\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (22.6308860779,37.8892335385), test loss: 33.5762759686\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (19.4775066376,49.7267250771), test loss: 60.9592321873\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (21.6968517303,37.5049681689), test loss: 27.5583551884\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (22.9966964722,49.3702077213), test loss: 49.4977152824\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (22.0393447876,37.137681095), test loss: 30.2767460108\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (15.3022966385,49.0301458742), test loss: 55.7790828228\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (16.9075336456,36.7799239874), test loss: 31.2812018394\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (23.672958374,48.6976706299), test loss: 61.0438083649\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (11.0043058395,36.4334426985), test loss: 25.6233134747\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (49.4960861206,48.3723899603), test loss: 45.4266611099\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (11.6378688812,36.1010654483), test loss: 31.937216568\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (32.6037330627,48.064628436), test loss: 58.1959309101\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (31.7322540283,35.778601262), test loss: 29.1959168434\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (23.7843513489,47.7556437103), test loss: 50.8434707165\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.9627838135,35.4652742688), test loss: 27.1356379986\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (22.8691864014,47.4564842337), test loss: 45.9595676899\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (69.6243515015,35.1663092611), test loss: 31.9107728481\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (72.6371002197,47.1663007726), test loss: 57.1011888027\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (28.0517292023,34.8745877999), test loss: 27.2036940575\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (32.1919021606,46.8820031389), test loss: 47.8153171062\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (20.573015213,34.5921260589), test loss: 29.0254561424\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (16.7497406006,46.601800831), test loss: 47.7740040302\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (15.8343734741,34.3133343842), test loss: 31.8589192867\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (24.5895500183,46.3268222588), test loss: 59.8223328114\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (8.9405412674,34.044568546), test loss: 26.6476794004\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (7.67094087601,46.059890679), test loss: 49.5237999439\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (24.8722648621,33.7856905689), test loss: 29.4255867958\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (57.4284667969,45.803253752), test loss: 55.2214438438\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (19.2530727386,33.5312006288), test loss: 29.4986421585\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (44.2395553589,45.5500681782), test loss: 59.5078011036\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (5.192466259,33.283382956), test loss: 24.9039402008\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (6.61512947083,45.3005873558), test loss: 44.9705247402\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (16.9088249207,33.0428472718), test loss: 30.652488327\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (26.9669075012,45.0651476468), test loss: 57.3565692902\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (22.5538597107,32.8071169674), test loss: 28.3306727409\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (60.5079193115,44.8266552511), test loss: 49.9346230507\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (7.48606157303,32.5773347112), test loss: 26.7647393227\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (24.4063682556,44.5905068087), test loss: 45.780853343\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (8.46747875214,32.3547763953), test loss: 31.4210542202\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (9.58015823364,44.3630393142), test loss: 56.5038537025\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (17.208732605,32.1396162923), test loss: 26.4238307953\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (22.9597244263,44.1403150874), test loss: 47.0248140335\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (25.4434375763,31.9289932099), test loss: 28.8234061003\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (11.2727584839,43.9193513054), test loss: 47.3302580833\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (10.3258771896,31.7203341883), test loss: 31.0578451633\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (12.788351059,43.6993996053), test loss: 58.4701426506\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (24.9938278198,31.5176085144), test loss: 26.5253805161\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (39.2608642578,43.4841477972), test loss: 49.0015418053\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (51.0021362305,31.3202803118), test loss: 29.5023891926\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (61.5920677185,43.2734794504), test loss: 54.7506939411\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (12.500164032,31.1255762049), test loss: 28.9242744923\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (29.7320613861,43.0668848398), test loss: 58.6661725998\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (32.9695320129,30.9358623532), test loss: 24.7716459274\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (46.5592155457,42.8632292429), test loss: 44.654792881\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (28.0151977539,30.7507316047), test loss: 30.3635485649\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (40.4067344666,42.6679222866), test loss: 56.8259973049\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (18.1356678009,30.5680671509), test loss: 26.982282877\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (17.9933128357,42.4683176517), test loss: 48.1497115612\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (16.2309932709,30.3903792261), test loss: 26.5740360975\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (10.356098175,42.2727691518), test loss: 45.5599779606\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (50.5290374756,30.2169382761), test loss: 30.9607527256\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (62.1791534424,42.0834611109), test loss: 55.6690360069\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (19.149017334,30.0473907718), test loss: 26.1622558594\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (14.0333929062,41.8957821432), test loss: 46.8047923565\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (42.2528266907,29.8823362561), test loss: 28.6900211811\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (36.5814857483,41.7110053009), test loss: 47.2101968765\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (16.392873764,29.7178060695), test loss: 30.5096755028\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (14.033782959,41.5262517332), test loss: 57.5884699345\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.2364597321,29.5562480547), test loss: 25.6885140419\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (12.1685705185,41.3446943027), test loss: 47.8454415321\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (13.9682159424,29.3979872123), test loss: 29.8639696121\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (20.9192466736,41.1662264991), test loss: 55.487293148\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (10.1987190247,29.2430085214), test loss: 28.5103800297\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (19.269695282,40.9915069161), test loss: 57.976228714\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (11.6820764542,29.089503395), test loss: 25.1709686279\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (20.6121864319,40.8175336811), test loss: 45.028041172\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (32.4206809998,28.9395279033), test loss: 29.9766918182\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (92.1365737915,40.6489519648), test loss: 56.2032060623\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.9788608551,28.7918243109), test loss: 26.1362880707\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (9.87666320801,40.4804125191), test loss: 47.2694271803\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (17.8283538818,28.6469760606), test loss: 26.0887861252\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (23.4475631714,40.3132280677), test loss: 45.729783535\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (19.4421653748,28.5044794936), test loss: 30.5986300468\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (12.1005821228,40.1488782442), test loss: 55.9443470955\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (19.6710643768,28.365839204), test loss: 25.9341674805\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (39.7982711792,39.987514481), test loss: 46.7060722828\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (16.192987442,28.2292065417), test loss: 28.3501627445\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (16.4542789459,39.8269675556), test loss: 47.1923226357\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (9.16767215729,28.0938517473), test loss: 30.2615176201\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (21.6909351349,39.666794839), test loss: 58.6526612282\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (17.9388275146,27.9600621673), test loss: 24.9395975113\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (20.271396637,39.5081932096), test loss: 47.1311108589\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (21.3746871948,27.8287156569), test loss: 29.479780817\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (27.4315032959,39.3516268455), test loss: 55.2506957054\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (14.3093671799,27.6995110202), test loss: 28.0920016766\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (21.4440994263,39.1973180603), test loss: 57.0738775253\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (12.6975393295,27.5712793859), test loss: 25.2075312138\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (14.9060821533,39.0442173927), test loss: 44.0488301277\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (10.2964000702,27.4453960366), test loss: 29.9066732407\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (51.5221328735,38.8939000627), test loss: 55.9915691376\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (6.56118869781,27.3219661321), test loss: 26.0254660606\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (21.6849689484,38.74670467), test loss: 47.1150490761\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (19.605381012,27.19990338), test loss: 26.5958147049\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (12.6968345642,38.5977284289), test loss: 46.373404026\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (17.5089683533,27.0793074744), test loss: 30.4346975803\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (31.7343235016,38.4514641321), test loss: 55.8734822273\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (45.1275177002,26.9618160118), test loss: 25.9028326035\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (41.0483398438,38.3064954798), test loss: 46.7464148521\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (36.9721069336,26.8456670045), test loss: 28.7113406181\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (36.7377700806,38.1626388258), test loss: 48.2949999809\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (16.3156528473,26.7308798484), test loss: 29.4335110664\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (6.75143671036,38.0186620496), test loss: 58.1570798874\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (16.1818351746,26.6164492936), test loss: 24.6393240452\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (22.5033187866,37.8754735151), test loss: 46.8418275833\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (11.4500312805,26.5039502065), test loss: 29.3991594791\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (7.28928995132,37.7335224191), test loss: 55.2605773449\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (12.640460968,26.3934619734), test loss: 27.6001997948\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (27.4420547485,37.5945315098), test loss: 56.5223502636\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (9.55891513824,26.2833615275), test loss: 25.1815035343\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (13.6441354752,37.4562025826), test loss: 44.1814866066\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (6.69387435913,26.1750640893), test loss: 29.7477558136\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (17.4172401428,37.3187673458), test loss: 55.9107765198\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (12.7430095673,26.0683258951), test loss: 25.6523251057\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (15.818116188,37.1850802372), test loss: 46.7708406448\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (14.0383625031,25.962197761), test loss: 26.860029459\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (37.7663803101,37.0495144715), test loss: 46.4780277252\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.24419212341,25.8574310976), test loss: 30.3863342762\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (60.2343406677,36.914967531), test loss: 55.6030583382\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (8.56529808044,25.7546767533), test loss: 25.8316126347\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (16.215587616,36.7816784946), test loss: 47.0303097248\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (10.3082408905,25.6535690286), test loss: 29.1432064056\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (16.6722679138,36.6493213837), test loss: 48.4788357258\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (14.4672985077,25.553443751), test loss: 29.1770019054\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (11.7534275055,36.5168730269), test loss: 58.197782135\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (9.32755470276,25.4535099852), test loss: 24.7508101463\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (13.2084083557,36.3850253218), test loss: 47.2232988358\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (18.941532135,25.3549853568), test loss: 29.553438282\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (30.1912574768,36.2547730961), test loss: 55.9112725258\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (39.7789993286,25.2579470407), test loss: 27.7209701538\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (50.263885498,36.1258912955), test loss: 57.2074965477\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (7.35904693604,25.1609023128), test loss: 25.4720093727\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (23.7518005371,35.9976478121), test loss: 44.7996357918\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (21.0187969208,25.065173752), test loss: 29.5971941948\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (28.8723945618,35.870270718), test loss: 55.7219090462\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (13.9038333893,24.9705682865), test loss: 25.3694427013\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (19.9416007996,35.7461294559), test loss: 46.0680247784\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (11.3257808685,24.876442141), test loss: 26.7173895836\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (11.6432886124,35.6193359659), test loss: 46.1998510838\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (8.55487918854,24.7836204401), test loss: 30.1885942459\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (3.77237892151,35.4934232585), test loss: 56.1087608337\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (35.0317230225,24.6920243082), test loss: 25.651554966\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (36.4431762695,35.3692219667), test loss: 46.6503038883\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (27.1968898773,24.6018630574), test loss: 29.096189642\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (23.6406669617,35.2454186311), test loss: 49.7846390247\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (30.9557991028,24.5123135724), test loss: 29.0114109993\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (18.8299179077,35.1219780394), test loss: 57.9835633755\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (17.8562545776,24.4231257888), test loss: 24.4251698017\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (14.1237812042,34.9988260914), test loss: 45.826974678\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (8.71208572388,24.3346634922), test loss: 29.9212677956\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (5.55990839005,34.8763980941), test loss: 56.7035992146\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (7.9703207016,24.2469840649), test loss: 27.2511676311\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (12.8547058105,34.7551446815), test loss: 52.4831225395\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (6.38809394836,24.160220924), test loss: 25.5758869171\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (21.7814254761,34.635359184), test loss: 45.1077904224\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (8.48680782318,24.0736594258), test loss: 29.5750892639\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (20.621553421,34.5152054955), test loss: 55.4694471359\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (29.943271637,23.9883995419), test loss: 25.4546698809\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (110.538116455,34.3983058809), test loss: 46.0023634434\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (13.117149353,23.9035276218), test loss: 26.8149654388\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (14.420671463,34.2788068081), test loss: 46.5682363033\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (14.314040184,23.8195338733), test loss: 30.4288969994\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (15.8661499023,34.1599867051), test loss: 57.3164948463\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (16.7118453979,23.7361958828), test loss: 24.9783255577\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (13.0095291138,34.0424969202), test loss: 46.3037022591\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (19.6154918671,23.654296531), test loss: 29.8126997948\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (32.1726150513,33.9260400348), test loss: 56.6074973106\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 5\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold5/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (304.299743652,inf), test loss: 203.252246094\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (377.480621338,inf), test loss: 238.693301392\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (75.4346542358,76.4937966557), test loss: 35.0997285843\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (124.728256226,90.9640026722), test loss: 62.4313259125\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (27.71991539,56.2752637706), test loss: 34.7659045696\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (56.7144088745,75.6865063286), test loss: 62.4676490307\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (27.6145381927,49.0083570566), test loss: 32.0170320034\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (27.6424617767,70.1098943736), test loss: 54.7419046402\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (19.1677799225,45.1286234405), test loss: 34.1103122711\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (23.7001037598,66.9622035434), test loss: 64.6159210205\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (20.9031276703,42.6332558716), test loss: 29.3807990551\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (33.9361686707,64.7847893763), test loss: 54.9382040024\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (27.8036251068,40.8391238119), test loss: 32.7157613277\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (35.440032959,63.0967618448), test loss: 62.3008254051\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (28.5473117828,39.4465237597), test loss: 30.9398672104\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (60.222694397,61.691018461), test loss: 56.8176548004\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (30.8938598633,38.3086961503), test loss: 30.3252177238\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (49.1430053711,60.4587358514), test loss: 51.275749588\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (34.5186920166,37.3443383933), test loss: 30.9011778831\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (66.3339767456,59.3521124094), test loss: 60.9469579697\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (6.40042352676,36.5030651499), test loss: 26.4036676168\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (5.94830417633,58.3355994696), test loss: 48.3715898037\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (9.88826370239,35.7594407757), test loss: 29.9332510948\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (33.2253684998,57.4001601479), test loss: 57.8239539146\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (14.3978366852,35.0853741222), test loss: 27.6835569859\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (44.8136940002,56.51310311), test loss: 49.5508698463\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (21.4006576538,34.4699074362), test loss: 28.0430513382\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (24.1534557343,55.6753437506), test loss: 47.9860267639\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (34.874130249,33.9011916673), test loss: 28.960837698\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (25.0746574402,54.8840081791), test loss: 59.4091932297\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (70.380355835,33.3704421385), test loss: 25.1021586895\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (56.7392272949,54.1308986794), test loss: 44.5304772377\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (11.1093864441,32.8722831082), test loss: 28.59786129\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (17.2554130554,53.4088779179), test loss: 55.7198592186\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (31.0170173645,32.4137219556), test loss: 26.7794651747\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (31.5672950745,52.7262036217), test loss: 48.1261379719\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.9162101746,31.9807648071), test loss: 27.6132772207\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (50.6405258179,52.0736471908), test loss: 47.8339276314\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (44.5704193115,31.5733947875), test loss: 27.4029876232\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (49.8115692139,51.4495498751), test loss: 56.0161343575\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (17.9229831696,31.1881830548), test loss: 26.0401189327\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (44.4016723633,50.8542514283), test loss: 44.8111910343\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (61.7862739563,30.8271652357), test loss: 29.0293369293\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (78.6950073242,50.2869748114), test loss: 54.9927449703\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (31.8652839661,30.4812132488), test loss: 25.4080010176\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (40.7849845886,49.7409237646), test loss: 46.8630351067\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (11.4441184998,30.1562078941), test loss: 27.207702589\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (14.2233705521,49.2237463339), test loss: 53.1404078484\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (60.9486694336,29.8494469944), test loss: 26.8019699097\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (88.2137145996,48.7317742697), test loss: 51.646006012\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (16.2963867188,29.5538562442), test loss: 26.5325604916\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (14.2365665436,48.2549583035), test loss: 45.7796122074\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (15.9644031525,29.2757342983), test loss: 28.3976199865\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (20.0178337097,47.8041971574), test loss: 55.6578155756\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (17.5216999054,29.0107159571), test loss: 23.8402965546\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (116.048370361,47.371733608), test loss: 45.6185892105\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (11.9865455627,28.7573399106), test loss: 28.2553291082\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (16.4070339203,46.9497997335), test loss: 54.835188961\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (23.0210189819,28.5149990557), test loss: 25.4279663801\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (22.4210395813,46.5515893668), test loss: 46.6166126251\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (14.3619604111,28.2821211536), test loss: 26.7757191181\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (19.5777359009,46.167993092), test loss: 46.2445794106\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (19.9426612854,28.0588006105), test loss: 27.6322981596\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (61.2616653442,45.7982405461), test loss: 55.4519405365\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (30.2585487366,27.8437878), test loss: 23.0319345713\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (32.6433906555,45.4391154314), test loss: 43.137458849\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (20.8907051086,27.636234941), test loss: 27.515336442\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (30.5852851868,45.0941230048), test loss: 54.2517324924\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (7.33725452423,27.4362473247), test loss: 24.527021265\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (7.60571193695,44.7606016126), test loss: 44.5286881447\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (16.0414848328,27.2440664007), test loss: 26.6515767336\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (14.1670780182,44.4390308357), test loss: 46.2783269882\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (13.0205373764,27.0576893049), test loss: 26.500655818\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (19.5587940216,44.1271096938), test loss: 55.2389949799\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (30.5746765137,26.8775161667), test loss: 23.8369771481\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (68.9505233765,43.8241258726), test loss: 43.536574173\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (61.8658599854,26.7020049399), test loss: 27.2982179642\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (155.357513428,43.5262623901), test loss: 53.7701522827\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (9.22570228577,26.5301768988), test loss: 24.7860158443\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (26.923871994,43.2336212293), test loss: 45.5868615627\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (8.44725608826,26.3656682262), test loss: 26.7353169918\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (20.7824478149,42.9541927808), test loss: 46.8284657001\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (5.57483243942,26.2060378672), test loss: 26.264184618\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (15.5064840317,42.6836863625), test loss: 54.8876319885\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (22.8175754547,26.0509297809), test loss: 24.6754889488\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (48.7209854126,42.4200972839), test loss: 43.5761623859\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (6.65853261948,25.8986077509), test loss: 27.361723423\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (15.6457748413,42.1610399018), test loss: 52.4650605202\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (15.6545095444,25.7511748745), test loss: 25.107717061\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (21.7797737122,41.9100407297), test loss: 46.775164032\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (8.5928440094,25.6069938536), test loss: 26.2275507689\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (18.9138202667,41.6642177215), test loss: 52.3143846512\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (8.18441104889,25.4664239025), test loss: 26.0848403931\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (34.62241745,41.423634368), test loss: 54.27001791\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (5.53239822388,25.3293159101), test loss: 25.5813993931\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (32.0577354431,41.1881249719), test loss: 44.9342088699\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.2299308777,25.1953180633), test loss: 26.720546484\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (9.85443210602,40.9574429193), test loss: 52.243318367\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (9.8732585907,25.0638948643), test loss: 23.6753556728\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (8.67786598206,40.7324477014), test loss: 45.8156236172\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (12.2587900162,24.9354757892), test loss: 27.1328090906\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (42.2227478027,40.5119143414), test loss: 53.0288580894\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (42.6108818054,24.8092738599), test loss: 25.4753116131\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (29.2407054901,40.2930633789), test loss: 46.7213918686\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (12.3213825226,24.6845970591), test loss: 26.1153083563\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (32.9254684448,40.0801028996), test loss: 45.8009588242\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (21.1290969849,24.563689446), test loss: 26.8497108936\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (18.9671764374,39.8708956138), test loss: 54.0043166637\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (19.0973472595,24.4446165042), test loss: 22.9283923388\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (21.9311408997,39.6656883834), test loss: 43.3148349285\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (8.38298130035,24.3273925736), test loss: 26.8213491917\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (10.1036672592,39.4632799367), test loss: 53.0360071182\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (13.757390976,24.2131410397), test loss: 24.2586298943\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (18.8065338135,39.2653723425), test loss: 44.6672490597\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (23.0639648438,24.1005822805), test loss: 25.966277647\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (19.3925876617,39.0700741405), test loss: 45.5700194836\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (17.6658630371,23.9896144992), test loss: 25.5456551552\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (35.5672950745,38.8781054736), test loss: 53.6303988934\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.9544467926,23.8806086115), test loss: 23.2556232452\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (17.9862041473,38.6882981142), test loss: 43.4334354401\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (18.8126125336,23.7736002881), test loss: 26.5921244144\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (9.98566913605,38.5019046988), test loss: 52.6125761032\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.3741455078,23.668146874), test loss: 24.1979454994\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (45.6415557861,38.3186188952), test loss: 44.2564342499\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (4.97094345093,23.5642971638), test loss: 26.0875537872\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (18.1214408875,38.1367235795), test loss: 46.1557287216\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (15.6111755371,23.4622005018), test loss: 25.2309922934\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (15.3246517181,37.9580240229), test loss: 53.4185558319\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (27.6470890045,23.3614789135), test loss: 24.0402457952\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (10.9753952026,37.781918286), test loss: 43.09072752\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (38.5295524597,23.2617698445), test loss: 26.2834527969\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (28.0279636383,37.6080857601), test loss: 50.9754253387\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (14.0321035385,23.1629933001), test loss: 24.3917690277\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (14.4471883774,37.4358545043), test loss: 44.9562887192\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (33.1589736938,23.0663802526), test loss: 26.1256536961\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (29.2211513519,37.2661966885), test loss: 50.7622529507\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (18.085401535,22.9701701253), test loss: 24.6994229555\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (30.865644455,37.0977759609), test loss: 51.6219248295\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (30.1558036804,22.8757863139), test loss: 25.5072306156\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (30.2506313324,36.9314789331), test loss: 45.0308011055\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.8068599701,22.7823832367), test loss: 26.7738584995\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (27.0969772339,36.7671371924), test loss: 51.622442627\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (38.2670173645,22.6905485239), test loss: 23.4074807882\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (48.6600189209,36.6045506346), test loss: 44.852799964\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (19.8290519714,22.5988763525), test loss: 27.2132331848\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (17.2379970551,36.4429640553), test loss: 52.8162764549\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (7.19327497482,22.5089344681), test loss: 24.5348027229\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (8.76093292236,36.2841186877), test loss: 44.8603447437\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (35.31848526,22.4202302652), test loss: 25.8224122286\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (49.7864952087,36.1268442287), test loss: 45.7206673622\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (16.2685260773,22.3318427114), test loss: 26.2305606842\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (9.93104553223,35.9701849604), test loss: 52.4332775116\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (7.63928890228,22.2448197267), test loss: 23.2165537834\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (17.4925498962,35.8157540895), test loss: 44.4130766869\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (6.98568439484,22.1589011385), test loss: 26.9525911331\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (42.236995697,35.6620941006), test loss: 52.9885127068\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (10.2299842834,22.0739944914), test loss: 23.8057314157\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (7.32317733765,35.5099627273), test loss: 43.9441969395\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (19.7697601318,21.989852774), test loss: 25.8815108061\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (19.8289489746,35.3600783221), test loss: 45.4129570961\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.0842170715,21.9061456646), test loss: 24.8268396378\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (11.7810688019,35.2109121076), test loss: 51.8882431507\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (9.87522125244,21.8235786615), test loss: 22.6879746199\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (35.4630508423,35.0633822257), test loss: 42.3279389858\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.9531583786,21.7416729186), test loss: 25.8571005106\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (16.1728973389,34.9163429008), test loss: 51.8300487041\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (15.3309288025,21.6604340113), test loss: 23.7545166492\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (17.4965591431,34.7709455625), test loss: 43.1585285664\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (4.69635391235,21.5799339217), test loss: 25.5303174019\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (7.05742931366,34.6267657061), test loss: 45.2056571484\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (12.3375234604,21.50030056), test loss: 24.4363990307\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (11.2340097427,34.4838467438), test loss: 51.9274413586\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (9.63704586029,21.4211206707), test loss: 23.5103756905\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (12.0296792984,34.3418302281), test loss: 42.4639822006\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (21.2944431305,21.3427563385), test loss: 26.0649777412\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (71.1322174072,34.2008564241), test loss: 51.1709658623\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (27.1095428467,21.2646407707), test loss: 23.9367402315\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (56.5162277222,34.0593358134), test loss: 43.6568481445\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (7.84121417999,21.1870702617), test loss: 25.6986645699\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (49.2124786377,33.9193474783), test loss: 45.2314511299\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (8.62577819824,21.1106307329), test loss: 24.3286291122\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (16.4935054779,33.7806334108), test loss: 51.1964361668\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (5.37229299545,21.034664823), test loss: 24.7542674065\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (22.2747631073,33.64349535), test loss: 43.5771392345\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (19.0877952576,20.9593199741), test loss: 26.1702215195\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (36.7869911194,33.5070582999), test loss: 50.9229554653\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (12.0638093948,20.8843192219), test loss: 23.5092276096\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (10.3369827271,33.3713356538), test loss: 45.0185417175\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (12.3122692108,20.8098910346), test loss: 26.0274156094\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (19.7477302551,33.2369383825), test loss: 51.958045578\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (6.547123909,20.7360238459), test loss: 24.1771615505\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (14.1780490875,33.1032154022), test loss: 44.2284594536\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (5.80466413498,20.6627926882), test loss: 25.1601740122\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (20.4514026642,32.9705130124), test loss: 44.3671210289\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (3.78698968887,20.5900216273), test loss: 25.8850309372\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (29.6826820374,32.8383456026), test loss: 52.1698748589\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (8.1332025528,20.517819084), test loss: 22.9622749329\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (6.70483493805,32.7068671433), test loss: 44.2387595177\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (5.72083950043,20.4460429988), test loss: 26.5491910934\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (9.21326065063,32.5766396182), test loss: 52.4391842842\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (13.9836931229,20.3746491955), test loss: 23.5949327946\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (47.3984069824,32.4469241148), test loss: 43.4580227852\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (29.0053253174,20.3036413693), test loss: 26.0127038002\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (9.84075546265,32.3174075673), test loss: 45.5198426247\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (9.44818878174,20.2328101937), test loss: 24.6944921017\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (26.0386199951,32.1893692048), test loss: 50.6814689159\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (18.9810009003,20.16271172), test loss: 22.7000159979\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (19.6186408997,32.0615714661), test loss: 42.5263665199\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (18.0799064636,20.0929617571), test loss: 25.4216939926\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (25.9356479645,31.9347482456), test loss: 51.7315360069\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (4.91441392899,20.0236484973), test loss: 23.7259714603\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (4.26095724106,31.8084601668), test loss: 43.2287696838\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (10.2640705109,19.9549486464), test loss: 25.6128843784\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (12.3936729431,31.6831718447), test loss: 45.2039330482\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (22.7542304993,19.8866178713), test loss: 24.0836788416\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (7.47558879852,31.558428697), test loss: 51.3163020611\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (18.0429897308,19.8183862792), test loss: 23.8645442963\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (27.6339111328,31.4344768697), test loss: 43.1698226929\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (7.04120731354,19.7506498219), test loss: 25.6118034363\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (11.2187824249,31.3107550793), test loss: 51.2345504284\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (16.6382751465,19.683591666), test loss: 23.6864674091\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (12.2927207947,31.1881130203), test loss: 43.3177880764\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (12.3031492233,19.6166542187), test loss: 25.9210263729\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (31.3547210693,31.0659617992), test loss: 45.9408419609\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (10.0839920044,19.5501819388), test loss: 24.3382810593\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (9.15916061401,30.9440570208), test loss: 51.5436364651\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (13.136510849,19.4841719268), test loss: 24.6862819672\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (11.3077983856,30.8232820703), test loss: 43.6129908562\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (19.2835273743,19.4183626135), test loss: 25.9472360134\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (7.03130435944,30.7029505589), test loss: 50.509383297\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (23.1743927002,19.3528790613), test loss: 23.6287297249\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (10.9151973724,30.5833296887), test loss: 44.0237744331\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (13.6417169571,19.2877034501), test loss: 26.1914519787\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (12.0726222992,30.4642744358), test loss: 52.5272424221\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (18.9406433105,19.222902557), test loss: 24.4597074032\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (13.7652511597,30.3456637361), test loss: 51.0819458008\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (8.52690315247,19.1584503449), test loss: 25.7908315659\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (12.722869873,30.2275844714), test loss: 45.2550422668\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (19.5568733215,19.0945891357), test loss: 26.2365020752\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (16.2412204742,30.1102752988), test loss: 51.4172270775\n",
      "run time for single CV loop: 5176.49507785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:124: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:126: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Deign Net Architecutre and Run Caffe\n",
    "exp_name = 'Exp14_MC'\n",
    "cohort = 'ADNI1'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'HC_CT'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "multi_label = False\n",
    "start_MC=1\n",
    "n_MC=1\n",
    "MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "start_fold = 1\n",
    "n_folds = 5\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "niter = 60000\n",
    "batch_size = 256\n",
    "\n",
    "pretrain = False\n",
    "multimodal_autoencode = False\n",
    "load_pretrained_weights = True\n",
    "\n",
    "if Clinical_Scale in ['BOTH','ADAS13_DX'] :\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "# dr: Dropout rate, lr: learning rate of each modality, tr: loss-weight for each task\n",
    "hype_configs = {\n",
    "                'hyp1':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,\n",
    "                                       'ADAS_ff':10,'MMSE_ff':10,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-3}},\n",
    "    \n",
    "    \n",
    "                 'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,\n",
    "                                       'ADAS_ff':10,'MMSE_ff':10,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':8,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-3}},\n",
    "    \n",
    "#                 'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':10,'HC_CT_ff':25,'COMB_ff':10,\n",
    "#                                        'ADAS_ff':10,'MMSE_ff':10,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':10,'COMB':1},\n",
    "#                       'tr':{'ADAS':2,'MMSE':1,'DX':1},'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-3}},\n",
    "    \n",
    "    \n",
    "#                 'hyp2':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':10,'HC_CT_ff':25,'COMB_ff':10,\n",
    "#                                        'ADAS_ff':10,'MMSE_ff':10,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':20,'COMB':1},\n",
    "#                       'tr':{'ADAS':2,'MMSE':1,'DX':1},'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-3}},\n",
    "    \n",
    "    \n",
    "#                 'hyp3':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':10,'HC_CT_ff':25,'COMB_ff':25,\n",
    "#                                        'ADAS_ff':10,'MMSE_ff':10,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':16,'COMB':1},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-3}},\n",
    "   \n",
    "#                 'hyp3':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':10,'HC_CT_ff':25,'COMB_ff':10,\n",
    "#                                        'ADAS_ff':10,'MMSE_ff':10,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':20,'COMB':1},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-2}},\n",
    "       \n",
    "    \n",
    "#                 'hyp4':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':10,'HC_CT_ff':25,'COMB_ff':10,\n",
    "#                                        'ADAS_ff':10,'MMSE_ff':10,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':20,'COMB':1},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-4}},\n",
    "    \n",
    "    \n",
    "                }\n",
    "\n",
    "CV_perf_MC = {}\n",
    "for mc in MC_list:\n",
    "    CV_perf_hype = {}\n",
    "    for hype in hype_configs.keys():    \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "        tr = hype_configs[hype]['tr']\n",
    "        solver_configs = hype_configs[hype]['solver_conf']\n",
    "        \n",
    "        if hype in ['hyp0']:\n",
    "            HC_snap = 4000 #20000 for ADNI2 5000 for ADNI1\n",
    "            CT_snap = 20000 #20000 for ADNI2 5000 for ADNI1\n",
    "            pre_hype = 'hyp1'\n",
    "        elif hype in ['hyp1','hyp3','hyp2','hyp4']:\n",
    "            HC_snap = 20000#4000 #20000 for ADNI2 5000 for ADNI1\n",
    "            CT_snap = 32000#32000 #20000 for ADNI2 5000 for ADNI1\n",
    "            pre_hype = 'hyp1'\n",
    "        else:\n",
    "            print 'unknown hyp config'\n",
    "            \n",
    "            print hype, pre_hype,HC_snap,CT_snap\n",
    "            \n",
    "        CV_perf = {}\n",
    "        start_time = time.time()\n",
    "        for fid in fid_list:            \n",
    "            print ''\n",
    "            print 'MC # {}, Hype # {}, Fold # {}'.format(mc, hype, fid)\n",
    "            snap_prefix = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}'.format(mc,fid,exp_name,hype,modality)\n",
    "\n",
    "            train_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/train_C688.txt'.format(mc,fid)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "\n",
    "            train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "            with open(train_filename_txt, 'w') as f:\n",
    "                    f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "                    \n",
    "                    \n",
    "            print 'Defining train net'\n",
    "\n",
    "            # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "            if pretrain:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_train.prototxt'.format(mc,fid)\n",
    "                with open(train_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(train_filename_txt, batch_size, node_sizes, modality)))            \n",
    "            else:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(train_net_path, 'w') as f:            \n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "                            \n",
    "            print 'Defining test net'\n",
    "\n",
    "            if pretrain:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_test.prototxt'.format(mc,fid)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(test_filename_txt, batch_size, node_sizes,modality)))\n",
    "            else:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            # Define Solver\n",
    "            print 'Defining solver'\n",
    "            solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "            with open(solver_path, 'w') as f:\n",
    "                f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, snap_prefix)))\n",
    "\n",
    "#             ### load the solver and create train and test nets\n",
    "            #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "            #solver = caffe.get_solver(solver_path)\n",
    "            solver = caffe.NesterovSolver(solver_path)\n",
    "            \n",
    "            if load_pretrained_weights:                    \n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_HC_CT_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                snap_path = baseline_dir + net_name.format(mc,fid,cohort,pre_hype,HC_snap,CT_snap)\n",
    "                print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "                solver.net.copy_from(snap_path)\n",
    "\n",
    "            #run caffe\n",
    "            results = run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode)\n",
    "            CV_perf[fid] = results\n",
    "            \n",
    "        print('run time for single CV loop: {}'.format(time.time() - start_time))\n",
    "\n",
    "        CV_perf_hype[hype] = CV_perf\n",
    "        \n",
    "CV_perf_MC[mc] = CV_perf_hype\n",
    "\n",
    "# pickleIt(CV_perf_MC, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Time for training \n",
    "#HC_CT fold 9,10, number of iterations: 40k, two hyper-params\n",
    "#start time: 14:47\n",
    "#end time: 15:31\n",
    "#runtime_mean = (13+31)/4\n",
    "#print runtime_mean\n",
    "\n",
    "tx=670/4 #time for 10k iters\n",
    "itx=4 # num of 10k iters\n",
    "hx=2 #hyp choices\n",
    "fx=5 #k-folds\n",
    "mx=5 #mc-folds\n",
    "\n",
    "num_hrs = (tx*itx*hx*fx*mx)/(3600.0)\n",
    "print num_hrs\n",
    "\n",
    "time_for_single_model = num_hrs*36/60\n",
    "print time_for_single_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbcAAAJoCAYAAABVztwOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIFIUYPtC3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fab5aea2b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "#niter = 30000\n",
    "test_interval = 500\n",
    "n_mc = len(CV_perf_MC.keys())\n",
    "\n",
    "for m, mc in enumerate(CV_perf_MC.keys()): \n",
    "    CV_perf_hype = CV_perf_MC[mc]\n",
    "    \n",
    "    for hype in CV_perf_hype.keys(): \n",
    "        CV_perf = CV_perf_hype[hype]\n",
    "        n_CV_configs = len(CV_perf)\n",
    "        pid = m*n_CV_configs + 1\n",
    "        \n",
    "        for f, fid in enumerate(fid_list):  \n",
    "            train_loss_list = CV_perf[fid]['train_loss']\n",
    "            test_loss_list = CV_perf[fid]['test_loss']\n",
    "            \n",
    "            for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "                #plt.figure()\n",
    "                ax1 = plt.subplot(n_mc,n_CV_configs,pid)            \n",
    "                ax1.plot(arange(niter), train_loss, label='train',linewidth='.5',alpha=0.25)\n",
    "                ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test_{}'.format(hype), linewidth='3')                            \n",
    "                ax1.set_xlabel('iteration')\n",
    "                ax1.set_ylabel('loss')\n",
    "                ax1.set_title('fid: {}, hyp: {}, Test loss: {:.2f}'.format(fid, hype, test_loss[-1]))\n",
    "                ax1.legend(loc=1)\n",
    "                #ax1.set_ylim(0,100)\n",
    "            pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp11'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'R_HC'\n",
    "batch_size = 256\n",
    "encoding_layer = 'code'\n",
    "weight_layers = 'code'\n",
    "multi_task = False\n",
    "\n",
    "if modality in ['L_HC', 'R_HC']:\n",
    "    snap_iter = 10000\n",
    "    \n",
    "    if modality == 'R_HC':\n",
    "        output_node_size = 16471\n",
    "    else: \n",
    "        output_node_size = 16086\n",
    "        \n",
    "    hype_configs = {\n",
    "                   'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':8000,'out':output_node_size},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "\n",
    "elif modality in ['CT']:  \n",
    "    snap_iter = 100000\n",
    "    \n",
    "    hype_configs = {\n",
    "                    'hyp1':{'node_sizes':{'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-4, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "elif modality in ['HC_CT']:  \n",
    "    snap_iter = 20000\n",
    "    hype_configs = {\n",
    "                 'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':500,'out_HC':16086,'out_CT':686},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "hype = 'hyp1'\n",
    "pretrain_preproc = 'ae_preproc_sparse_HC_10k_CT_100k_{}'.format(hype)\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "\n",
    "# Save either L or R HC encodings. Turn on CT + CS save only for one of these. \n",
    "# (Also decide if you want to copy CT raw scores or encodings)\n",
    "save_encodings = True\n",
    "save_scores = False\n",
    "CT_encodings = True\n",
    "\n",
    "for cohort in ['inner_train', 'inner_test','outer_test']:\n",
    "    data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "    test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)       \n",
    "    input_nodes = ['X_{}'.format(modality)]\n",
    "    #input_nodes = ['X_L_HC','X_CT']\n",
    "    net_file = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "    with open(net_file, 'w') as f:\n",
    "        f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "\n",
    "    test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "    with open(test_filename_txt, 'w') as f:\n",
    "        f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "    sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "    if modality == 'CT':\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "    else:\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "        \n",
    "    print 'Check Sparsity for model: {}'.format(model_file)\n",
    "    \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    \n",
    "    encodings_hdf_file = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,pretrain_preproc)\n",
    "    \n",
    "    if save_encodings:    \n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        input_data.create_dataset(input_nodes[0],data=encodings)           \n",
    "        input_data.close()\n",
    "        \n",
    "    if save_scores:\n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        if not CT_encodings: #copy raw scores for CT instead of encodings\n",
    "            CT_data = load_data(data_path, 'X_CT','no_preproc')                    \n",
    "            input_data.create_dataset('X_CT', data=CT_data)    \n",
    "            \n",
    "        adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "        mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "        input_data.create_dataset('y', data=adas_scores)           \n",
    "        input_data.create_dataset('y3', data=mmse_scores)    \n",
    "        input_data.create_dataset('dx_cat3', data=dx_labels)\n",
    "        input_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting TSNE embeddings \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "exp_name = 'Exp11'\n",
    "#preproc = 'no_preproc'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "Clinical_Scale = 'ADAS13_DX' \n",
    "batch_size = 256\n",
    "snap_interval = 2000\n",
    "snap_start = 2000\n",
    "encoding_layer = 'ff3'\n",
    "weight_layers = 'ff3'\n",
    "cohort = 'outer_test'\n",
    "multi_task = False\n",
    "\n",
    "modality = 'HC_CT'\n",
    "snap_end = 21000\n",
    "\n",
    "\n",
    "hype_configs = {\n",
    "            'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-2}},\n",
    "}\n",
    "\n",
    "hype = 'hyp1'\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "tr = hype_configs[hype]['tr']\n",
    "\n",
    "data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "test_net_path = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)       \n",
    "#input_node = 'X_{}'.format(modality)\n",
    "\n",
    "net_file = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)\n",
    "with open(net_file, 'w') as f:\n",
    "    #f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "\n",
    "test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "with open(test_filename_txt, 'w') as f:\n",
    "    f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "n_snaps = len(np.arange(snap_start,snap_end,snap_interval))\n",
    "tsne_encodings = {}\n",
    "net_weights = {}\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    print sn, snap_iter\n",
    "    model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    net_weights[snap_iter] = results['wt_dict']\n",
    "    print encodings.shape\n",
    "\n",
    "    adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "    mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "    dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "    tsne_model = TSNE(n_components=2, random_state=0, init='pca')\n",
    "    tsne_encodings[snap_iter] = tsne_model.fit_transform(encodings) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "plot_encodings = True\n",
    "plot_wts = False\n",
    "\n",
    "color_scores = dx_labels\n",
    "cm = plt.get_cmap('RdYlBu') \n",
    "marker_size = 100\n",
    "marker_size = (np.mod(color_scores+1,2)+1)*50\n",
    "\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    plt.subplot(np.ceil(n_snaps/2.0),2,sn+1)\n",
    "    if plot_encodings:\n",
    "        plt.scatter(tsne_encodings[snap_iter][:,0],tsne_encodings[snap_iter][:,1],c=color_scores,s=marker_size,cmap=cm)\n",
    "    if plot_wts:\n",
    "        plt.imshow(net_weights[snap_iter][weight_layers])\n",
    "        \n",
    "    plt.title(snap_iter)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp14_MC 1 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.5553216583187075, 0.5752987918804201, 0.5077407487826372, 0.60039020566127344, 0.58474380396461001, 0.6711001137854341, 0.60044370653404899, 0.58117683453983471, 0.54218873996617112, 0.61402970794004474]\n",
      "ADAS mse: [54.255272207754096, 42.921968322128151, 65.707971713923541, 68.875645206821801, 51.426599684526771, 34.596873399634497, 56.533142145271647, 37.608710444161517, 68.572776305679113, 45.867584995981382], rmse: [7.3658178234160863, 6.5514859629039996, 8.1060453806972692, 8.2991352083709184, 7.1712341814032801, 5.8819106929325695, 7.5188524486966521, 6.1325941039792875, 8.2808680888949748, 6.7725611843660287]\n",
      "ADAS means: 0.583243431137, 52.6366544426, 7.20805050757\n",
      "\n",
      "MMSE corr: [0.55933014607542952, 0.56210046841248151, 0.54743995695396841, 0.64636706117812592, 0.58498690350750226, 0.58783507619629527, 0.58074589086603245, 0.614998854726304, 0.61470202455099832, 0.6279917725643871]\n",
      "MMSE mse: [96.403217731749464, 72.849420875520224, 98.687132356684032, 88.802540936235161, 77.789026039337443, 76.717879080396187, 104.88532360037455, 48.30140940902649, 120.12863495172483, 54.890435876128052], rmse: [9.8185140286985106, 8.5351872197111316, 9.9341397391361497, 9.4235100114678687, 8.8198087303148149, 8.7588743044067137, 10.24135360195978, 6.9499215397748548, 10.960320932879878, 7.4088079929316599]\n",
      "MMSE means: 0.592649815503, 83.9455020857, 9.08504381013\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 13, 2: 13, 3: 14, 4: 14, 5: 7, 6: 7, 7: 11, 8: 14, 9: 9, 10: 8}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.55772865506582947, 0.5752987918804201, 0.5077407487826372, 0.60039020566127344, 0.58433751840464998, 0.6711001137854341, 0.60044370653404899, 0.58225311735172092, 0.55527337963314338, 0.61783633024254625]\n",
      "ADAS mse: [54.113127483177863, 42.921968322128151, 65.707971713923541, 68.875645206821801, 51.406984984895942, 34.596873399634497, 56.533142145271647, 37.429136481319716, 67.08798026926857, 45.765442031624758], rmse: [7.3561625514379347, 6.5514859629039996, 8.1060453806972692, 8.2991352083709184, 7.1698664551646942, 5.8819106929325695, 7.5188524486966521, 6.1179356388670616, 8.1907252590517636, 6.7650160407514743]\n",
      "ADAS means: 0.585240256734, 52.4438272038, 7.19571356389\n",
      "\n",
      "MMSE corr: [0.55569492879744609, 0.56210046841248151, 0.54743995695396841, 0.64636706117812592, 0.58324028578699794, 0.58783507619629527, 0.58074589086603245, 0.61060508372609279, 0.62043386473212037, 0.63224152604021477]\n",
      "MMSE mse: [97.154652728890284, 72.849420875520224, 98.687132356684032, 88.802540936235161, 78.229038038844664, 76.717879080396187, 104.88532360037455, 49.066723360156494, 123.58044424608462, 55.260088402130656], rmse: [9.8567059776017611, 8.5351872197111316, 9.9341397391361497, 9.4235100114678687, 8.8447180870192046, 8.7588743044067137, 10.24135360195978, 7.0047643329491462, 11.116674153994289, 7.4337129620486868]\n",
      "MMSE means: 0.592670414269, 84.5233243625, 9.11496403903\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 11, 2: 13, 3: 14, 4: 14, 5: 6, 6: 7, 7: 11, 8: 14, 9: 6, 10: 9}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.5553216583187075, 0.5752987918804201, 0.5077407487826372, 0.60039020566127344, 0.58274447293086162, 0.64764769393891086, 0.60460382144657154, 0.58117683453983471, 0.54218873996617112, 0.60721651211619576]\n",
      "ADAS mse: [54.255272207754096, 42.921968322128151, 65.707971713923541, 68.875645206821801, 52.397752694574223, 36.732157638430643, 57.796090619728268, 37.608710444161517, 68.572776305679113, 46.454790520577795], rmse: [7.3658178234160863, 6.5514859629039996, 8.1060453806972692, 8.2991352083709184, 7.2386291999641905, 6.0607060346489865, 7.6023740120917669, 6.1325941039792875, 8.2808680888949748, 6.8157751225064489]\n",
      "ADAS means: 0.580432947958, 53.1323135674, 7.24534309375\n",
      "\n",
      "MMSE corr: [0.55933014607542952, 0.56210046841248151, 0.54743995695396841, 0.64636706117812592, 0.5846082894494794, 0.55513631716685707, 0.5907506204433004, 0.614998854726304, 0.61470202455099832, 0.62096263671729235]\n",
      "MMSE mse: [96.403217731749464, 72.849420875520224, 98.687132356684032, 88.802540936235161, 77.647654991908396, 76.653659298896429, 104.40404542442064, 48.30140940902649, 120.12863495172483, 54.553171524218016], rmse: [9.8185140286985106, 8.5351872197111316, 9.9341397391361497, 9.4235100114678687, 8.8117906802141182, 8.7552075531592291, 10.217829780556174, 6.9499215397748548, 10.960320932879878, 7.3860118822147864]\n",
      "MMSE means: 0.589639637567, 83.84308875, 9.07924333678\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 8}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 1e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 13, 2: 13, 3: 14, 4: 14, 5: 8, 6: 4, 7: 13, 8: 14, 9: 9, 10: 7}\n"
     ]
    }
   ],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "#exp_name = 'Exp11'\n",
    "#niter = 10000\n",
    "#modality = 'CT'\n",
    "start_fold = 1\n",
    "n_folds = 10\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "#preproc = 'no_preproc'\n",
    "#batch_size = 256\n",
    "snap_interval = 4000\n",
    "snap_start = 4000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "#Clinical_Scale = 'ADAS13'\n",
    "\n",
    "#MC_list = np.arange(1,11,1)\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "idx = 0  \n",
    "df_perf_dict_adas = {}\n",
    "df_perf_dict_mmse = {}\n",
    "df_perf_dict_adas_tuned = {}\n",
    "df_perf_dict_mmse_tuned = {}\n",
    "df_perf_dict_opt = {}\n",
    "\n",
    "for mc in MC_list:\n",
    "    print exp_name, mc, modality\n",
    "\n",
    "    for hype in hype_configs.keys():      \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "\n",
    "        for fid in fid_list:\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "            #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "            #with open(test_filename_txt, 'w') as f:\n",
    "            #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                    f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC']\n",
    "                elif modality == 'CT':\n",
    "                    f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_CT']\n",
    "                elif modality == 'HC_CT':\n",
    "                    f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "                elif modality == 'HC_CT_unified_hyp1':\n",
    "                    f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_HC_CT']\n",
    "                else:\n",
    "                    print 'Wrong modality'\n",
    "\n",
    "            #print 'Hype # {}, MC # {}, Fold # {}, Clinical_Scale {}'.format(hype, mc, fid, Clinical_Scale)\n",
    "            data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            if Clinical_Scale == 'ADAS13':\n",
    "                act_scores = load_data(data_path, 'adas_bl','no_preproc')\n",
    "            elif Clinical_Scale == 'MMSE': \n",
    "                act_scores = load_data(data_path, 'adas_m12','no_preproc')\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                act_scores_adas13 = load_data(data_path, 'adas_bl','no_preproc')\n",
    "                act_scores_mmse = load_data(data_path, 'adas_m12','no_preproc')\n",
    "            else:\n",
    "                print 'unknown clinical scale'\n",
    "\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort)\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            sub.call([\"cp\", baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort), baseline_dir + \n",
    "                      'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)])\n",
    "            #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "            #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "\n",
    "            if Clinical_Scale in ['ADAS13', 'MMSE']:                \n",
    "                multi_task = False\n",
    "                cs_list = ['opt']\n",
    "                iter_euLoss = []\n",
    "                iter_r = []        \n",
    "                iter_pred_scores = []\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = np.squeeze(results['X_out'])            \n",
    "                    iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                    iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "                fold_r[config_idx] = np.array(iter_r)\n",
    "                fold_act_scores[fid] = act_scores\n",
    "                fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                multi_task = True\n",
    "                cs_list = ['adas','mmse']\n",
    "                iter_euLoss_adas13 = []\n",
    "                iter_r_adas13 = []        \n",
    "                iter_pred_scores_adas13 = []\n",
    "                iter_euLoss_mmse = []\n",
    "                iter_r_mmse = []        \n",
    "                iter_pred_scores_mmse = []\n",
    "\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = results['X_out']   \n",
    "                    encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                    encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                    iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                    iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                    iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                    iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "                fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "                fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "                fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "                fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "                fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "                fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "                fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "\n",
    "    \n",
    "    if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "        fold_perf_dict = {'fold_r':fold_r,'fold_euLoss':fold_euLoss,'fold_act_scores':fold_act_scores,'fold_pred_scores':fold_pred_scores}\n",
    "    else: \n",
    "        fold_perf_dict = {'fold_r_adas13':fold_r_adas13,'fold_r_mmse':fold_r_mmse,'fold_euLoss_adas13':fold_euLoss_adas13,'fold_euLoss_mmse':fold_euLoss_mmse,\n",
    "                         'fold_act_scores_adas13':fold_act_scores_adas13,'fold_act_scores_mmse':fold_act_scores_mmse,\n",
    "                          'fold_pred_scores_adas13':fold_pred_scores_adas13,'fold_pred_scores_mmse':fold_pred_scores_mmse}\n",
    "        \n",
    "    opt_metric = 'euLoss' #euLoss or corr or both\n",
    "    #task_weights = {'ADAS':1,'MMSE':0} \n",
    "    save_multitask_results = False\n",
    "    CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/output/'\n",
    "    save_path = '{}{}_{}_NN_{}'.format(CV_model_dir,exp_name,mc,modality)\n",
    "    \n",
    "    # Create dictionaries of perfromance based on differnt task weights \n",
    "    for key, task_weights in {'adas':{'ADAS':1,'MMSE':0},'mmse':{'ADAS':0,'MMSE':1},'both':{'ADAS':1,'MMSE':1}}.items():\n",
    "        print 'Weights Tuned for: {}'.format(key)                              \n",
    "        results = compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path)  \n",
    "\n",
    "        # populate the perf dictionary for 10 MC x 10 folds\n",
    "        model_choice = 'APANN'    \n",
    "        for f, fid in enumerate(fid_list): \n",
    "            if key in ['adas','mmse']:\n",
    "                r_valid = results['{}_r'.format(key)][f]\n",
    "                MSE_valid = results['{}_mse'.format(key)][f]\n",
    "                RMSE_valid = results['{}_rmse'.format(key)][f]\n",
    "\n",
    "                if key == 'adas':\n",
    "                    df_perf_dict_adas_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                elif key == 'mmse':\n",
    "                    df_perf_dict_mmse_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                else: \n",
    "                    print 'unknown key'\n",
    "\n",
    "            elif key == 'both':                    \n",
    "                for cs in cs_list:            \n",
    "                    r_valid = results['{}_r'.format(cs)][f]\n",
    "                    MSE_valid = results['{}_mse'.format(cs)][f]\n",
    "                    RMSE_valid = results['{}_rmse'.format(cs)][f]\n",
    "\n",
    "                    if cs == 'adas':\n",
    "                        df_perf_dict_adas[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                    elif cs == 'mmse':\n",
    "                        df_perf_dict_mmse[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}                    \n",
    "                    else: \n",
    "                        print 'unknown key'\n",
    "\n",
    "\n",
    "            else: #single task dictionary\n",
    "                df_perf_dict_opt[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "\n",
    "            idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.2195444572928871"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbkAAAJoCAYAAABLB+y9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3X+MnPV9J/D3M7ue3bW8rEGWgRWmIbdBpkYWbZXwB8uP\npApJDxglKmelTYCqKMofzanyrS6Iu56QTiKJ1LpS0Ul3l/pUAs2R8iMxc2mbIJdKsI2yUiAE6sSy\nJkaNkWmJMVnb2V/2ztwfrrd2PIt3vbPszuzrJUV6dp4f+x3ij8fP+/nO51s0Go1GAAAAAACgDZVW\negAAAAAAAHCxhNwAAAAAALQtITcAAAAAAG1LyA0AAAAAQNsScgMAAAAA0LaE3AAAAAAAtC0hNwAA\nAAAAbat7KSfv2bMnTzzxxNzPf/VXfzXvsePj43n22Wfz8ssv58iRIymXy9myZUtuvfXWfOQjH1nK\nMAAAAAAAWKMuOuQ+fPhwnn766QUde/DgwTz88MM5ceJEkqS3tzdTU1PZv39/9u/fn+9973t54IEH\n0tXVdbHDAQAAAABgDbqodiWNRiP/83/+z5w8eTLXXnvtux47MTGRL3/5yzlx4kSuuuqqfPnLX85X\nv/rVPP7447n//vvT1dWVH/7wh3n00UcvZigAAAAAAKxhFxVy/+3f/m0OHDiQm2++Odu3b3/XY6vV\nasbHx1Mul/Pggw/mmmuuSZJ0dXXl9ttvz44dO5Ike/fuzT//8z9fzHAAAAAAAFijFh1yv/XWW/n6\n17+eSy65JPfdd98Fj3/xxReTJDfddFM2bdp03v6Pf/zj6e3tTb1enzsWAAAAAAAWYtEh9//+3/87\n09PTuffee9Pf3/+uxx4+fDhHjhxJktxwww1Nj+nt7c3WrVuTJK+++upihwMAAAAAwBq2qJB77969\n+cd//Mds3749N9988wWPP3To0Nz21VdfPe9xW7ZsSZK88cYbixkOAAAAAABr3IJD7qNHj+ZrX/ta\nyuVyPvvZzy7onHfeeWdu+7LLLpv3uDP7JiYmMj09vdAhZd++fQs+FtYStQHzUx/QnNqA5tQGNKc2\nYH7qA5pbztpYcMj9la98JRMTE9mxY0c2b968oHMmJyfntsvl8rzH9fT0ND3nQvylAc2pDZif+oDm\n1AY0pzagObUB81Mf0NyKh9wvvPBCfvCDH+Saa67JHXfcsWyDAQAAAACAxbhgyD0+Pp6vfvWrKZVK\n+dznPpdSaeFtvPv6+ua2Z2Zm5j3u7BYlZ58DAAAAAADvpvtCB3zta1/LiRMncvvtt+fKK6/M1NTU\nOftPnTo1t31mX3d3d7q7u3PppZfO7Tt69GgGBweb/o6jR48mSdavX39O65Jftm/fvnOmte/YseNC\nw4c1SW3A/NQHNKc2oDm1Ac2pDZif+oDmduzYkSeffHLu523btmXbtm0tufYFQ+6f/exnSZLnnnsu\nzz333Lsee9999yVJ/v2///e57777smXLlrl9P/3pT+cNuQ8dOpQkueqqq971+s3e+OHDh9/9DcAa\n1N/fn+PHj6/0MGBVUh/QnNqA5tQGNKc2YH7qA5obHBxctodAC+89sghFUSQ5PfBNmzYlSV555ZWm\nx05PT2f//v1Jku3bty/HcAAAAAAA6FAXnMn90EMPvev+p556Kk8//XSS5K/+6q/O23/LLbfkG9/4\nRr773e/m7rvvngu9z/j2t7+dqamplEql3HzzzYsZOwAAAAAAa9yyzOQ+21133ZWNGzdmeno6X/rS\nl3Lw4MEkp3t5P/fcc3N9WD760Y/miiuuWO7hAAAAAADQQS44k3up1q9fnwceeCBf/OIX88Ybb+TB\nBx9Mb29vTp48mdnZ2STJDTfckHvvvXe5hwIAAAAAQIdpWch9pg93M+9///vzp3/6p9mzZ09efvnl\nvP322+nt7c2WLVty22235cMf/nCrhgEAAAAAwBpSNBqNxkoPYikOHz680kOAVcdKzjA/9QHNqQ1o\nTm1Ac2oD5qc+oLnBwcFlu/ay9+QGAAAAAIDlIuQGAAAAAKBtCbkBAAAAAGhbQm4AAAAAANqWkBsA\nAAAAgLYl5AYAAAAAoG0JuQEAAAAAaFtCbgAAAAAA2paQGwAAAACAtiXkBgAAAACgbQm5AQAAAABo\nW0JuAAAAAADalpAbAAAAAIC2JeQGAAAAAKBtCbkBAAAAAGhbQm4AAAAAANqWkBsAAAAAgLYl5AYA\nAAAAoG0JuQEAAAAAaFtCbgAAAAAA2paQGwAAAACAtiXkBgAAAACgbQm5AQAAAABoW0JuAAAAAADa\nlpAbAAAAAIC2JeQGAAAAAKBtCbkBAAAAAGhbQm4AAAAAANqWkBsAAAAAgLYl5AYAAAAAoG0JuQEA\nAAAAaFtCbgAAAAAA2paQGwAAAACAtiXkBgAAAACgbQm5AQAAAABoW0JuAAAAAADalpAbAAAAAIC2\nJeQGAAAAAKBtCbkBAAAAAGhbQm4AAAAAANqWkBsAAAAAgLYl5AYAAAAAoG0JuQEAAAAAaFtCbgAA\nAAAA2paQGwAAAACAtiXkBgAAAACgbQm5AQAAAABoW0JuAAAAAADalpAbAAAAAIC2JeQGAAAAAKBt\nCbkBAAAAAGhb3Qs98PXXX89LL72UgwcP5s0338yxY8cyMTGR9evXZ3BwML/2a7+W22+/PRs2bDjv\n3KeeeipPP/30BX/HI488kssvv3xx7wAAAAAAgDVrwSH33//93+c73/nO3M/lcjk9PT05ceJEDhw4\nkAMHDuRv/uZv8oUvfCHXXntt81/W3d00BD+jq6trEUMHAAAAAGCtW3DIPTQ0lM2bN2fr1q0ZHBzM\n+vXrkyTT09MZGxvL448/nmPHjuVP/uRP8md/9mfp6+s77xrXXnttHnroodaNHgAAAACANW3BIfct\nt9zS9PWenp7ccsst2bhxYx5++OGMj4/npZdeyvDwcMsGCQAAAAAAzbRs4ckPfOADc9tHjx5t1WUB\nAAAAAGBeLQu5f/zjH89tWzwSAAAAAID3woLblTRz6tSpvPPOO3nppZfy5JNPJkmuvPLK/MZv/EbT\n4w8dOpSRkZG89dZbKYoil112Wa677rp87GMfy/ve976lDAUAAAAAgDXookLuT3/60zl16tR5r2/d\nujV/+Id/mO7u5pc9fvx4fvGLX2T9+vWZnJzMm2++mTfffDPPP/98PvnJT+ZTn/rUxQwHAAAAAIA1\n6qJC7ksvvTQnT57M1NRUpqamkiTXX399Pv3pT+eyyy477/grr7wyn/nMZ/LBD34wmzdvTqlUyuzs\nbPbt25cnnngiBw8ezDe/+c1s2LAhd95559LeEQAAAAAAa0bRaDQaS7nAsWPH8sILL+Qb3/hGfvGL\nX+S3f/u3s2PHjgWff/LkyTz00EP5yU9+kt7e3vyv//W/0tfXt+DzDx8+fDHDho7W39+f48ePr/Qw\nYFVSH9Cc2oDm1AY0pzZgfuoDmhscHFy2ay954clLLrkkd955Z/7Lf/kvKYoizzzzTF5++eUFn79u\n3br8zu/8TpJkamoqr7322lKHBAAAAADAGrGkhSfPNjQ0lK1bt+bHP/5x9u7dm1//9V9f8LnXXnvt\n3PZbb70173H79u3Lvn375n7esWNH+vv7L27A0MHK5bLagHmoD2hObUBzagOaUxswP/UB83vyySfn\ntrdt25Zt27a15LotC7mTzPXj/pd/+ZdWXnZOszfu6x9wPl+NgvmpD2hObUBzagOaUxswP/UBzfX3\n9y+qzfViLLldydnOhNu9vb2LOu/AgQNz25s3b27lkAAAAAAA6GALCrnr9foFj3nttddSq9WSZFHT\nzE+dOpWvf/3rSU6H49dff/2CzwUAAAAAYG1bULuSt99+O3/8x3+c22+/Pdu3bz9ntvXbb7+dF198\nMd/4xjeSnJ52fscdd8zt/9GPfpRvfvObue2227Jt27Zs3LgxSTI7O5sf/ehH+b//9//m4MGDSZK7\n774769evb9mbAwAAAACgsy24J/c//dM/5c///M9Pn9Tdnb6+vszMzGR6enrumMsvvzwjIyMZGBg4\n59xXX301r776apLTzfd7enoyMTGR2dnZJEmpVMonPvGJ3HXXXUt+QwAAAAAArB0LCrkvvfTS/Kf/\n9J+yb9++1Gq1vPPOOzl27FhKpVI2bdqUX/mVX8mHPvSh3HTTTVm3bt0551599dW55557cuDAgRw6\ndCjHjh3LxMREenp6snnz5lx33XX5zd/8zWzZsmVZ3iAAAAAAAJ1rQSF3d3d3brzxxtx4442L/gUb\nNmzInXfeuejzAAAAgKUriiKzs0W6uhppNBorPRwAaLkFtysBAAAA2kut1pdqtSejo+UMD8+kUpnO\n0NDkSg8LAFpKyA0AAAAdqFbrS6UykPHxUpJkbKw7u3f3plqNoBuAjlJa6QEAAAAArVUURarVnrmA\n+4zx8VKq1Z4URbFCIwOA1hNyAwAAQIeZnS0yOlpuum90tJx6XcgNQOcQcgMAAECH6epqZHh4pum+\n4eGZlEoWoASgcwi5AQAAoMM0Go1UKtMZGKif8/rGjfVUKtNpNITcAHQOC08CwCpQFEVmZ4t0dTXc\ndAIALTE0NJlqNalWezI6Ws7w8EwqlWmLTgLQcYTcALDCarU+N58AwLIYGprMyMhUdu4sUip5mA5A\nZxJyA8AKqtX6UqkMZHz8dAexsbHu7N7dm2o1gm4AoCUajUaKohH5NgCdSk9uAFghRVGkWu2ZC7jP\nGB8vpVrtSVEUKzQyAAAAaB9CbgBYIbOzRUZHy033jY6WU68LuQEAAOBChNwAsEK6uhoZHp5pum94\neCalku8UAwAAwIUIuQFghTQajVQq0xkYqJ/z+saN9VQq0xaGAgAAgAWw8CQArKChoclUq0m12pPR\n0XKGh2dSqUxbdBIAAAAWSMgNACtsaGgyIyNT2bmzSKnUMIMbAAAAFkHIDQCrQKPRSFE0It8GAACA\nxdGTG4CWKYoi9XopRVGs9FAAAACANcJMbgBaolbr01caAAAAeM8JuQFYslqtL5XKQMbHT39BaGys\nO7t396ZajaAbAAAAWFbalQCwJEVRpFrtmQu4zxgfL6Va7dG6BAAAAFhWQm4AlmR2tsjoaLnpvtHR\ncup1ITcAsHTW/gAA5iPkBmBJuroaGR6eabpveHgmpVLjPR4RAMtFyMhKqdX6smvXQO6+e1N27RpI\nrda30kMCAFYRPbkBWJJGo5FKZTq7d/ee07Jk48Z6KpXpNBpCboBOYIFhVoq1PwCACxFyA7BkQ0OT\nqVYj/ADoUEJGVsqF1v4YGZnyQB0A0K4EgNYYGprMyMh4nnnmSEZGxoUeAB3CAsOsJGt/AAALIeQG\noGUajUaKom5GFUAHETKykqz9AQAshJAbAACYl5CRlXRm7Y+Bgfo5r1v7AwA4m5AbAACYl5CRlXZ6\n7Y/xjIxM5MYbT2VkZCLPPqs1GgDwb4pGm/+r9PDhwys9BFh1+vv7c/z48ZUeBqxK6gOaUxtcSK3W\ntyYXGFYbq0dRFKnXi5RKDQ9XVgG1AfNTH9Dc4ODgsl27e9muDAAAdIzTCwxPZedOISMr4/TaH434\nowcA/DIhNwAAsCBCRgAAViM9uQEAAAAAaFtCbgAAAAAA2paQGwAAAACAtiXkBgAAAACgbQm5AQAA\nAABoW0JuAAAAAADalpAbAGCZFUWRer2UoihWeigAAAAdp3ulBwAA0Mlqtb5Uqz0ZHS1neHgmlcp0\nhoYmV3pYAAAAHUPIDQCwTGq1vlQqAxkfP/3lubGx7uze3ZtqNYJuAACAFtGuBABgGRRFkWq1Zy7g\nPmN8vJRqtUfrEgAAgBYRcgMALIPZ2SKjo+Wm+0ZHy6nXhdwAAACtIOQGAFgGXV2NDA/PNN03PDyT\nUqnxHo8IAACgMwm5AQCWQaPRSKUynYGB+jmvb9xYT6UynUZDyA0AANAKFp4EAFgmQ0OTqVaTarUn\no6PlDA/PpFKZtugkAABACwm5AQCW0dDQZEZGprJzZ5FSqWEGNwAAQIsJuQEAllmj0UhRNCLfBgAA\naD09uQEAAAAAaFtCbgAAAAAA2paQGwAAAACAtiXkBgAAAACgbS144cnXX389L730Ug4ePJg333wz\nx44dy8TERNavX5/BwcH82q/9Wm6//fZs2LBh3muMj4/n2Wefzcsvv5wjR46kXC5ny5YtufXWW/OR\nj3ykJW8IAAAAAIC1Y8Eh99///d/nO9/5ztzP5XI5PT09OXHiRA4cOJADBw7kb/7mb/KFL3wh1157\n7XnnHzx4MA8//HBOnDiRJOnt7c3U1FT279+f/fv353vf+14eeOCBdHV1teBtAQAAAACwFiw45B4a\nGsrmzZuzdevWDA4OZv369UmS6enpjI2N5fHHH8+xY8fyJ3/yJ/mzP/uz9PX1zZ07MTGRL3/5yzlx\n4kSuuuqqfP7zn88111yT2dnZ/N3f/V0effTR/PCHP8yjjz6a+++/v/XvEgAAAACAjrTgnty33HJL\n7rzzzgwNDc0F3EnS09OTW265Jf/xP/7HJKdbkrz00kvnnFutVjM+Pp5yuZwHH3ww11xzTZKkq6sr\nt99+e3bs2JEk2bt3b/75n/95yW8KAAAAAIC1oWULT37gAx+Y2z569Og5+1588cUkyU033ZRNmzad\nd+7HP/7x9Pb2pl6vzx0LAAAAAAAX0rKQ+8c//vHc9uWXXz63ffjw4Rw5ciRJcsMNNzQ9t7e3N1u3\nbk2SvPrqq60aEgAAAAAAHW5JIfepU6fys5/9LN/+9rfzP/7H/0iSXHnllfmN3/iNuWMOHTo0t331\n1VfPe60tW7YkSd54442lDAkAAAAAgDVkwQtPnu3Tn/50Tp06dd7rW7duzR/+4R+mu/vfLvvOO+/M\nbV922WXzXvPMvomJiUxPT6enp+dihgYAAAAAwBpyUTO5L7300mzcuDG9vb1zr11//fW57777zguy\nJycn57bL5fK81zw71D77HAAAAAAAmM9FzeQ+05okSY4dO5YXXngh3/jGN/Lggw/mt3/7t7Njx46W\nDRAAAAAAAOZzUSH32S655JLceeed2bp1a/7oj/4ozzzzTIaGhvLrv/7rSZK+vr65Y2dmZs6Z/X22\n6enpue2zzznbvn37sm/fvrmfd+zYkf7+/qW+Beg45XJZbcA81Ac0pzagObUBzakNmJ/6gPk9+eST\nc9vbtm3Ltm3bWnLdJYfcZwwNDWXr1q358Y9/nL17986F3JdeeuncMUePHs3g4GDT848ePZokWb9+\n/bz9uJu98ePHj7di+NBR+vv71QbMQ31Ac2oDmlMb0JzagPmpD2iuv79/2TqAXFRP7vmc6cf9L//y\nL3OvbdmyZW77pz/96bznHjp0KEly1VVXtXJIAAAAAAB0sJaG3GfC7bNbkgwODmbTpk1JkldeeaXp\nedPT09m/f3+SZPv27a0cEgAAAAAAHWxBIXe9Xr/gMa+99lpqtVqSnNdS5JZbbkmSfPe7382RI0fO\nO/fb3/52pqamUiqVcvPNNy9kSAAAAAAAsLCQ++23384XvvCF7N27N2+99dZ5+/bs2ZM//uM/TnK6\nt8odd9xxzjF33XVXNm7cmOnp6XzpS1/KwYMHkySnTp3Kc889N9dw/KMf/WiuuOKKJb8pAAAAAADW\nhgUvPPlP//RP+fM///PTJ3V3p6+vLzMzM5menp475vLLL8/IyEgGBgbOOXf9+vV54IEH8sUvfjFv\nvPFGHnzwwfT29ubkyZOZnZ1Nktxwww259957W/GeAAAAAABYI4pGo9G40EGnTp3KSy+9lH379qVW\nq+Wdd97JsWPHUiqVcskll+RXfuVX8qEPfSg33XRT1q1bN+91jh07lj179uTll1/O22+/nXXr1mXL\nli257bbb8uEPf/ii3sDhw4cv6jzoZFZyhvmpD2hObUBzagOaUxswP/UBzQ0ODi7btRcUcq9mQm44\nnw9UmJ/6gObUBjSnNqA5tQHzUx/Q3HKG3AvqyQ0AAAAAAKuRkBsAAAAAgLYl5AYAAAAAoG0JuQEA\nAJooiiInT9ZTFMVKDwUAgHfRvdIDAAAAWG1qtb5Uqz0ZHS1neHhdKpXpDA1NrvSwAABoQsgNAABw\nllqtL5XKQMbHT3/xdWysO7t396ZajaAbAGAV0q4EAADgXxVFkWq1Zy7gPmN8vJRqtUfrEgCAVUjI\nDQAAdISiKFKvl5YURM/OFhkdLTfdNzpaTr0u5AYAWG2E3AAAQNur1fqya9dA7r57U3btGkit1ndR\n1+nqamR4eKbpvuHhmZRKjaUMEwCAZaAnNwAA0NZa2UO70WikUpnO7t2957Qs2bixnkplOo2GkBsA\nYLUxkxsAAGhby9FDe2hoMtXqeEZGJnLjjacyMjKRZ58dt+gkAMAqJeQGAADa1nL10B4amszIyHj+\n9m9/kZERATcAwGom5AYAANrWcvbQbjQa6e4uaVECALDKCbkBAIC2daaH9sBA/ZzX9dAGAFg7LDwJ\nAAC0tdM9tJNqtSejo+UMD8+kUpnWYgQAYI0QcgMAAG3vdA/tqezcWaRUapjBDQCwhgi5AQCAjtBo\nNFIUjci3AQDWFj25AQAAAABoW0JuAAAAAADalpAbAAAAAIC2JeQGAAAAAKBtCbkBAAAAAGhbQm4A\nAAAAANqWkBsAAAAAgLYl5AYAAAAAoG0JuQEAAAAAaFtCbgAAAAAA2paQGwAAAACAtiXkBgAAAACg\nbQm5AQAAAABoW0JuAAAAAADalpAbAAAAAIC2JeQGAAAAAKBtCbkBAAAAAGhbQm4AAAAAANqWkBsA\nAAAAgLYl5AYAAAAAoG0JuQEAAAAAaFtCbgAAAAAA2paQGwAAAACAtiXkBgAAAACgbQm5AQAAAABo\nW0JuAAAAAADalpAbAAAAAIC2JeQGAAAAAKBtCbkBAAAAAGhbQm4AAAAAANqWkBsAAIBVryiK1Oul\nFEWx0kMBAFaZ7pUeAAAAALybWq0v1WpPRkfLGR6eSaUynaGhyZUeFgCwSgi5AQAAWLVqtb5UKgMZ\nHz/9ReSxse7s3t2bajWCbgAgySJC7hMnTuT73/9+Xnvttbz++uv52c9+lnq9nksuuSTvf//7c+ut\nt+ZDH/pQ03OfeuqpPP300xf8HY888kguv/zyhY8eAACAjlUURarVnrmA+4zx8VKq1Z6MjEyl0Wis\n0OgAgNViwSH3Zz/72dTr9bmfy+Vyuru7c/To0Rw9ejTf//73c8MNN2RkZCTlcrn5L+vuzoYNG+b9\nHV1dXYsYOgAAAJ1sdrbI6Gjz+8vR0XJ27ixSFEJuAFjrFhxy1+v1fOADH8htt92W7du3Z/PmzUmS\nI0eO5Jlnnsnzzz+fV155JV/5ylfy+c9/vuk1rr322jz00EOtGTkAAAAdraurkeHhmYyNnX/rOjw8\nk1KpERO5AYAFh9wPPfRQfvVXf/W81zdt2pTPfe5zKZVK2bt3b1588cX87u/+bi677LKWDhQAAIC1\npdFopFKZzu7dvee0LNm4sZ5KZVqrEgAgSVK68CGnNQu4z/aRj3xkbvsnP/nJxY8IAAAA/tXQ0GSq\n1fGMjEzkxhtPZWRkIs8+O27RSQBgzoJncl/IunXr5rbP7t0NAAAASzE0NJmRkans3Fn8a4sSM7gB\ngH/TspB73759c9tXX31102MOHTqUkZGRvPXWWymKIpdddlmuu+66fOxjH8v73ve+Vg0FAACADtNo\nNFIUenADAOdrScg9MTGRPXv2JDnd1uTKK69setzx48fzi1/8IuvXr8/k5GTefPPNvPnmm3n++efz\nyU9+Mp/61KdaMRwAAAAAANaIJYfcjUYjjzzySH7+85+nXC7n93//98875sorr8xnPvOZfPCDH8zm\nzZtTKpUyOzubffv25YknnsjBgwfzzW9+Mxs2bMidd9651CEBAAAAALBGLHjhyfn8xV/8RX7wgx8k\nSe6///5s2bLlvGOGh4dz11135YorrkipdPpXdnV1Zfv27fnv//2/59/9u3+XJHnqqacyOWnxEAAA\nAAAAFmZJIfdjjz2W73znO0mS3/u938ttt9226GusW7cuv/M7v5MkmZqaymuvvbaUIQEAAAAAsIZc\ndLuSv/zLv8xf//VfJ0nuvffe/NZv/dZFD+Laa6+d237rrbfmPW7fvn3nLHC5Y8eO9Pf3X/TvhU5V\nLpfVBsxDfUBzagOaUxsXr9Fo5NSpRrq7ixRFsdLDocXUBsxPfcD8nnzyybntbdu2Zdu2bS257kWF\n3I8//ni+9a1vJUnuueee3HHHHS0ZzIU0e+PHjx9/T343tJP+/n61AfNQH9Cc2oDm1MbFqdX6Uq32\nZHS0nOHhmVQq0xka0pqyk6gNmJ/6gOb6+/uzY8eOZbn2okPuxx57bG4G9z333NOShSIPHDgwt715\n8+YlXw8AAICVUav1pVIZyPj46e6YY2Pd2b27N9VqBN0AwLJYVE/uswPue++9tyUB96lTp/L1r389\nSdLb25vrr79+ydcEAADgvVcURarVnrmA+4zx8VKq1R5tSwCAZbHgkPvsHtz33XffgluU/OhHP8rD\nDz+cf/iHf8jPf/7zuddnZ2fz2muv5b/9t/+WWq2WJLn77ruzfv36xYwfAACAVWJ2tsjoaLnpvtHR\ncup1ITcA0HoLaldy5MiR/L//9/+SnH4yv2fPnuzZs2fe4yuVyjmzvF999dW8+uqrSU433+/p6cnE\nxERmZ2eTJKVSKZ/4xCdy1113XfQbAQAAYGV1dTUyPDyTsbHzbzWHh2dSKjXSaKzAwACAjragkLtx\n1r9CGo1GxsfH3/X4qampue2rr74699xzTw4cOJBDhw7l2LFjmZiYSE9PTzZv3pzrrrsuv/mbv5kt\nW7Zc5FsAAABgNWg0GqlUprN7d+85LUs2bqynUpk+594SAKBVikab/yvj8OHDKz0EWHWs5AzzUx/Q\nnNqA5tTGxanV+lKt9mR0tJzh4ZlUKtMWnewwagPmpz6gucHBwWW79oJmcgMAAMBCDQ1NZmRkKjt3\nFv/aoqSt51YBAKuckBsAAICWazQaKQo9uAGA5Ve68CEAAAAAALA6CbkBAAAAAGhbQm4AAAAAANqW\nkBsAOlCUYoZ/AAAgAElEQVRRFKnXSymKYlVeDwAAAFrFwpMA0GFqtb5Uqz0ZHS1neHgmlcp0hoYm\nV831AAAAoJWE3ADQQWq1vlQqAxkfP/1lrbGx7uze3ZtqNRcVTLf6egAAANBq2pUAQIcoiiLVas9c\nIH3G+Hgp1WrPoluNtPp6AAAAsByE3ADQIWZni4yOlpvuGx0tp15fXCjd6usBAADAchByA0CH6Opq\nZHh4pum+4eGZlEqNFb0eAAAALAchNwB0iEajkUplOgMD9XNe37ixnkplOo3G4kLpVl8PAAAAloOF\nJwGggwwNTaZaTarVnoyOljM8PJNKZfqiF4ls9fUAAFjdiqLI7GyRrq6GSQ1A2ygabf431uHDh1d6\nCLDq9Pf35/jx4ys9DFiV1kp9FEWRer1IqdSam5NWX4/VZzXXhpttVtJqrg1YSWpj9Wjl52St1mdy\nQwuoD2hucHBw2a5tJjcAdKhWZoGNRiNF0WjpNWEh3GwDwPxa+TlZq/WlUhnI+PjpzrZjY93Zvbs3\n1Wp89gKrnpAbADqMUJBO4WYbAObXys/JoihSrfbMXeuM8fFSqtWejIxM+TYVsKpZeBIAOsiZm51d\nu9ZnbKw7u3atT6UykFqtb6WHBotyoZvtoihWaGQAsPJa/Tk5O1tkdLTcdN/oaDn1us9dYHUTcgNA\nhxAK0kncbAPA/Fr9OdnV1cjw8EzTfcPDMymVzOIGVjchNwB0CKEgncTNNgDMr9Wfk41GI5XKdAYG\n6ue8vnFjPZXKtFYlwKon5AaADiEUpJO42QaA+S3H5+TQ0GSq1fGMjEzkxhtPZWRkIs8+O24dDKAt\nFI02v0M4fPjwSg8BVp3+/v4cP358pYcBq1Kn18cvL0CUnL7ZcYPChazW2rCQKitttdYGrDS1sTos\nx+dkURSp14uUSg0PlS+S+oDmBgcHl+3aQm7oQD5QYX5roT6EglyM1VwbbrZZSau5NmAlqY3Vw+fk\n6qM+oLnlDLm7l+3KAMCKGBqazMjIVHbudLNDZ2g0GimKRvxRBoDz+ZwEEHIDQEdyswMAAMBaYeFJ\nAAAAAADalpAbAAAAAIC2JeQGAAAAAKBtCbkBAAAAAGhbQm4AAAAAANqWkBsAAAAAgLYl5AYAAAAA\noG0JuQEAAAAAaFtCbgAAAAAA2paQGwAAAACAtiXkBgAAAACgbQm5AQAAAABoW0JuAAAAAADalpAb\nAAAAAIC2JeQGAAAAAKBtCbkBAAAAAGhbQm4AAAAAANqWkBsAAAAAgLYl5AYAAAAAoG0JuQEAAAAA\naFtCbgAAAAAA2paQGwAAAACAtiXkBgAAAACgbQm5AQAAAABoW0JuAAAAAADalpAbAAAAAIC2JeQG\nAAAAAKBtdS/0wBMnTuT73/9+Xnvttbz++uv52c9+lnq9nksuuSTvf//7c+utt+ZDH/rQu15jfHw8\nzz77bF5++eUcOXIk5XI5W7Zsya233pqPfOQjS34zAAAAAACsLQsOuT/72c+mXq/P/Vwul9Pd3Z2j\nR4/m6NGj+f73v58bbrghIyMjKZfL551/8ODBPPzwwzlx4kSSpLe3N1NTU9m/f3/279+f733ve3ng\ngQfS1dXVgrcFAAAAAMBasOCQu16v5wMf+EBuu+22bN++PZs3b06SHDlyJM8880yef/75vPLKK/nK\nV76Sz3/+8+ecOzExkS9/+cs5ceJErrrqqnz+85/PNddck9nZ2fzd3/1dHn300fzwhz/Mo48+mvvv\nv7+17xAAAAAAYAmKosjsbJGurkYajcZKD4dfsuCQ+6GHHsqv/uqvnvf6pk2b8rnPfS6lUil79+7N\niy++mN/93d/NZZddNndMtVrN+Ph4yuVyHnzwwWzatClJ0tXVldtvvz0TExN54oknsnfv3txxxx25\n4oorWvDWAAAAAACWplbrS7Xak9HRcoaHZ1KpTGdoaHKlh8VZFrzwZLOA+2xn99T+yU9+cs6+F198\nMUly0003zQXcZ/v4xz+e3t7e1Ov1uWMBAAAAAFZSrdaXSmUgu3atz9hYd3btWp9KZSC1Wt9KD42z\nLDjkvpB169bNbZ/du/vw4cM5cuRIkuSGG25oem5vb2+2bt2aJHn11VdbNSQAAAAAgItSFEWq1Z6M\nj58boY6Pl1Kt9qQoihUaGb+sZSH3vn375ravvvrque1Dhw41ff2XbdmyJUnyxhtvtGpIAAAAAAAX\nZXa2yOhouem+0dFy6nUh92rRkpB7YmIie/bsSXK6rcmVV145t++dd96Z2z67T/cvO7NvYmIi09PT\nrRgWAAAAAMBF6epqZHh4pum+4eGZlEoWoFwtlhxyNxqNPPLII/n5z3+ecrmc3//93z9n/+TkvzVh\nL5ebP/lIkp6enqbnAACLVxRF6vWSr88BAABcpEajkUplOgMD9XNe37ixnkplOo2GkHu16F7qBf7i\nL/4iP/jBD5Ik999//1zbEQBgZVj5GwAAoDWGhiZTrcY91iq3pJD7sccey3e+850kye/93u/ltttu\nO++Yvr5/W2l0ZmYmvb29Ta91douSs88BYPkURZHZ2SJdXQ1PoDvEmZW/zyyMMjbWnd27e1Otxj/C\nAAAALsLQ0GRGRqayc2eRUsn982p00SH3X/7lX+av//qvkyT33ntvfuu3fqvpcZdeeunc9tGjRzM4\nONj0uKNHjyZJ1q9ff07rkrPt27fvnAUud+zYkf7+/osaP3SycrmsNnhXjUYj//iPyZ496/LCC+ty\nyy0n84lPnMz116fj21t0cn00Go1861vrmq78/a1v9eaP/qir4///5eJ1cm3AUqgNaE5twPzUB8zv\nySefnNvetm1btm3b1pLrXlTI/fjjj+db3/pWkuSee+7JHXfcMe+xZ7cv+elPfzpvyH3o0KEkyVVX\nXTXvtZq98ePHjy943LBW9Pf3qw3eVbPZvl/5Sk+q1fGOn+3byfVRr5fywgubmu574YV1OX58PEVR\nb7ofOrk2YCnUBjSnNmB+6gOa6+/vz44dO5bl2oteePKxxx47J+C+88473/X4wcHBbNp0+ob7lVde\naXrM9PR09u/fnyTZvn37YocEwCIURZFqtafpbN9qtcdM3zZm5W8AAADWokWF3I899tg5LUouFHCf\nccsttyRJvvvd7+bIkSPn7f/2t7+dqamplEql3HzzzYsZEgCLNDtbZHS03HTf6Gg59bqQu11Z+RsA\nAIC1aMEh99k9uO+77753bVHyy+66665s3Lgx09PT+dKXvpSDBw8mSU6dOpXnnnturhfLRz/60Vxx\nxRWLGT8Ai2S2b2c7vfL3eEZGJnLjjacyMjKRZ5/t/DY0AAAArF1FYwHTuo4cOZI/+IM/OH1CUeSS\nSy551+Mrlcp5s7wPHjyYL37xi3M9iXp7e3Py5MnMzs4mSW644Yb85//8n9Pdvbg24YcPH17U8bAW\n6P/FhfxyT+7k9GzftRCGrpX6KIoi9bqVv1m4tVIbsFhqA5pTGzA/9QHNzbdWYyssKFE+++a40Whk\nfHz8XY+fmpo677X3v//9+dM//dPs2bMnL7/8ct5+++309vZmy5Ytue222/LhD394kUMH4GKdnu2b\nVKs9GR0tZ3h4JpXKdMcH3GtJo9FIUTQi3wZYPYqiyOxska4uDyABAFppQTO5VzMzueF8nhqzUGtx\ntq/6gObUBjTXqtqo1fo8XKaj+NyA+akPaG7FZ3ID0JnM9gWA5ffLbcLGxrqze3dvqtUIugEAWmDB\nC08CAACwOEVRpFrtOWcdjCQZHy+lWu1JURQrNDIAgM4h5AYAAFgms7NFRkfLTfeNjpZTrwu5AQCW\nSsgNAAB0hNNrTZRW1ezorq5Ghodnmu4bHp5JqaRnGADAUgm5AQCAtler9WXXroHcffem7No1kFqt\nb6WHlOT0+heVynQGBurnvL5xYz2VyvSaWfgZAGA5WXgSAIBVrSiKzM4W6epqCARparUv7Dg0NJlq\nNalWezI6Ws7w8EwqlelVMTYAgE4g5AYAYNWq1foEg7yrCy3sODIytSoejgwNTWZkZCo7dxYplTyw\nAQBoJSE3AACr0mqfncvqcKGFHXfuLFIUqyNQbjQaKYpG5NsAAK2lJzcAAKvOhWbnrqaFBVlZFnYE\nAEDIDQDAqnOh2bn1upCb0yzsCACAdiUAAKw6Z2bnjo2d/8/VM7NzZZecYWFHAIC1TcgNAMCqc2Z2\n7u7dvee0LDE7l/lY2BEAYO0ScgMAsCqZnctiWdgRAGBtEnIDALBqmZ0LAABciJAbAIBVzexcAADg\n3ZQufAgAAEDrFUWRer2UoihWeigAALQxM7kBANa4oihy8mQ9RVFoB8J7plbr028dAICWEHIDAKxh\n5waN6wSNvCdqtb5UKgMZHz/9xdKxse7s3t2bajX+/AEAsGhCbgCANUrQyEooiiLVas/cn7szxsdL\nqVZ7MjIy5RsFAAAsip7cAABr0IWCRj2SWS6zs0VGR8tN942OllOv+7MHAMDiCLkBANYgQSMrpaur\nkeHhmab7hodnUiqZxQ0AwOIIuQEA1iBBIyul0WikUpnOwED9nNc3bqynUpnWqgQAgEUTcgMso6Io\nUq+XfO0fWHUEjaykoaHJVKvjGRmZyI03nsrIyESefXZcL3gAAC5K0WjzO5jDhw+v9BBg1env78/x\n48dXehhrXq3Wl2q1J6Oj5QwPz6RSmXbzvgqoDziXv6tYSacfBhcplRqr9sGKzw1oTm3A/NQHNDc4\nOLhs1xZyQwfygbryarW+VCoD5yzoNjBQT7VqltpKUx9wvqIo0tu7PlNTE6s2aISV4nMDmlMbMD/1\nAc0tZ8itXQlAixVFkWq155yAO0nGx0upVnu0LgFWnUajke7ukoAbAABoS0JugBabnS0yOlpuum90\ntJx6XcgNAAAA0CpCboAW6+pqZHh4pum+4eGZlEpmSkKrWeQVAABg7RJyA7RYo9FIpTKdgYH6Oa9v\n3FhPpTKtHQC0WK3Wl127BnL33Zuya9dAarW+lR7SeYTwAAAAy6d7pQcA0ImGhiZTrSbVak9GR8sZ\nHp5JpTJt0UlosV9e5HVsrDu7d/emWs2qqbdarc/fBQAAAMtIyA2wTIaGJjMyMpWdO4uUSg0zuKHF\nLrTI68jI1IrXXTuE8AAAAO1OuxKAZdRoNFIU9RUP2qATrfZFXi8UwmtdAgAA0BpCbgDgPdeKHtWr\nfZHX1R7CAwAAdAohNwDwnmrVQpGrfZHX1R7CAwAAdAo9uQGA90yre1Sv5kVez4Twu3f3ntOyZLWE\n8AAAAJ1CyA0AvCeWa6HI1bzI62oO4QEAADqFkBsAeE9cqEf1zp1FiuLiAurTi7w2sory7TmrOYQH\nAADoBHpyAwDvibXco/p0CF8XcAMAACwDITcA8J5Y7QtFAgAA0J60KwEA3jN6VAMAANBqQm4A4D2l\nRzUAAACtJOQGAN5zq3mhSAAAANqLntwAAAAAALQtITcAAGtKURSp10spimKlhwIAALSAdiUAAKwZ\ntVqfhU8BAKDDCLkBoAMVRZHZ2SJdXRZ2hDNqtb5UKgMZHz/9Zcaxse7s3t2bajWCbgAAaGPalQBA\nh6nV+rJr10DuvntTdu0aSK3Wt9JDosW021i8oihSrfbMBdxnjI+XUq32+G8JAABtzExuAOggZqp2\nPu02Ls7sbJHR0XLTfaOj5ezcWaQofOsBAADakZncANAhzFTtfGceYuzatT5jY93ZtWt9KhWz9Rei\nq6uR4eGZpvuGh2dSKgm4AQCgXQm5AaBDXGimar0u5G5nHmIsTaPRSKUynYGB+jmvb9xYT6UyrXc9\nAAC0MSE3AHQIM1U7m4cYSzc0NJlqdTwjIxO58cZTGRmZyLPPjmv3AgAAbW7BPblnZmbyox/9KAcP\nHszBgwfz+uuv58iRI0mS//Af/kPuvvvuec996qmn8vTTT1/wdzzyyCO5/PLLFzokAFiUoihy8mQ9\nRVF05KzNMzNVd+/uPWe2r5mqneHMQ4yxsfP/+XbmIYb/iy9saGgyIyNT2bmz+Nf/Zv6jAQBAu1tw\nyF2r1fKlL31pab+suzsbNmyYd39XV9eSrg8A8zl3sb51HbtY3+mZqrEwYQfyEKN1Go1GisJDAQAA\n6BQLDrmTZMOGDbnmmmvm/vfVr341P//5zxd8/rXXXpuHHnpo0YMEgKU4s1jfmWBwbKw7u3f3plpN\nR4a/Zqp2Lg8xAAAAzrfgkPu6667L//k//+ec1772ta+1fEAA0EoXWqxvZGSqI0NgM1U7l4cYAAAA\n51rwwpNFYTEjANqPxfroRKcfYtQF3AAAAFlEyA2sTUVRpF4vedBF2zqzWF8zZxbrAwAAANrXonpy\nL9WhQ4cyMjKSt956K0VR5LLLLst1112Xj33sY3nf+973Xg4FWIBzF+rT95X2ZLE+AAAA6Gzvach9\n/Pjx/OIXv8j69eszOTmZN998M2+++Waef/75fPKTn8ynPvWp93I4wLtYawv10dks1gcAAACd6z0J\nua+88sp85jOfyQc/+MFs3rw5pVIps7Oz2bdvX5544okcPHgw3/zmN7Nhw4bceeed78WQgHexVhfq\no7OdWazvv/7X9ZmamvBnGAAAADrEe9KTe3h4OHf9//buPzjOu74T+PtZKSvJiSw55ELsxvmFagLO\nhFA4mE4UEsgAoZC9Huf4hmmBtly5KWSm5/GV0vYypEwpcNR3LcNMpkx6B6Rc2+RoycIdkAEuJBqo\nSxKSELtp0DlcknPBNiGyY0urWLv3h09bK17Fv6TVrvR6zXjm0fNj9V1rP3rs9373873++px33nkp\nlY58y56enlx++eX58Ic/nJe85CVJkjvuuCOTk2bVwVKzUB/LVaPRSG9vScANAAAAy8iSLzx5xhln\n5B3veEeSZGpqKt///veXeESAhfoAAAAA6BZt7ck9nw0bNjS39+zZM+95O3bsyI4dO5pfb968OYOD\ng4s6NuhG5XL5tGvj7W8/nFtvrR+zUN/b3344Z5111ukOEZbMQtQHLEdqA1pTG9Ca2oD5qQ+Y3+23\n397c3rhxYzZu3Lggj9sRIfeJavXEDxw4sESjgc41ODh42rVx0UVJtTpzzEJ9F100GWVHN1uI+oDl\nSG1Aa2oDWlMbMD/1Aa0NDg5m8+bNi/LYHRFyP/bYY83tc889dwlHAhxtdqG+LVuKlEoNfYwBAAAA\n6DhLHnIfPnw4f/mXf5kk6e/vz2WXXbbEIwKO1mg0UhSNyLcBAAAA6EQnFXIfPHgw9Xo9yZHga3ZW\nZ61Wm/MxjDPOOCP9/f1Jkp07d+Zv/uZvcs0112Tjxo0ZHh5OkszMzGTnzp35b//tv2XXrl1Jkk2b\nNmXVqlWn/6wAYJEVRZGZmSI9PT7lAAAAAEvppELuD3zgA9m3b98x+6vVaqrVavPrq6++Ou973/ua\nXz/88MN5+OGHkxxpvt/X15dDhw5lZmYmSVIqlfKLv/iLuf7660/pSQBAO42PDxzTr35kZHKph9VV\nvEkAAADAQjmpkLsoipM+74ILLsg73/nOPPbYY3nyySezf//+HDp0KH19fTn33HPzspe9LNdee23W\nr19/ciMHgCUwPj6QSmUoExOlJMn27b259db+VKsRdJ8gbxIAAACwkE4q5P7Upz510t/grLPOytve\n9raTvg4AOk1RFKlW+5oB96yJiVKq1b5s3TplVvJxeJMAAACAhVY6/ikAC6coitTrpRP+ZAh0kpmZ\nImNj5ZbHxsbKqde9rl/I8d4k8HsBAACAUyHkBtpmfHwg27YNZdOmc7Jt21DGxweWekhwUnp6Ghkd\nnW55bHR0OqWSWdwvxJsEAAAALAYhN9AWsy0Ktm1ble3be7Nt26pUKoJuukuj0UilUsvQUH3O/uHh\neiqVmlYlx+FNAgAAABaDkBtYdFoUsJyMjEymWp3I1q2H8trXHs7WrYdy550T+kmfAG8SAAAAsBhO\nauFJgFNxvBYFW7YUKQrhFt1jZGQyW7dOZcuWIqVSQzh7Eo68SZBUq30ZGytndHQ6lUrNmwQAAACc\nMiE3sOhmWxRs337sr5zZFgUyQrpNo9FIUXjtngpvEgAAALCQtCsBFp0WBcDzHXmToK7+AQAAOG1m\ncgNtoUUBAAAAAItByA20jRYFAAAAACw0ITfQVvoYAwAAALCQ9OQGAAAAAKBrCbkBAAAAAOhaQm4A\nAAAAALqWkBsAAAAAgK4l5AYAAACABVAURZ57rp6iKJZ6KLCi9C71AAAAAACg242PD6Ra7cvYWDmj\no2ekUqllZGRyqYcFK4KQGwAAAABOw/j4QCqVoUxMHGmasH17b269tT/VagTd0AbalQAAAADAKSqK\nItVqXzPgnjUxUUq12qd1CbSBkBsAAAAATtHMTJGxsXLLY2Nj5dTrQm5YbEJuAABWlKIoUq+XzKoC\nABZET08jo6PTLY+Njk6nVGq0eUSw8gi5AQBYMcbHB7Jt21A2bTon27YNZXx8YKmHBAB0uUajkUql\nlqGh+pz9w8P1VCq1NBpCblhsFp4EAGBFsCAUALBYRkYmU60m1WpfxsbKGR2dTqVS828MaBMzuQEA\nWPYsCAUALLaRkcls3TqRr3zlYLZunRBwQxsJuQEAWPYsCAUAtEOj0Uhvb0mLEmgzITcAAMueBaEA\nAGD5EnIDXa0oitTrJR8zB+AFWRAKAACWLwtPAl1rfHzAoh4AnDALQgEAwPJUNLp82sru3buXegjQ\ncQYHB3PgwIGlHsaiGh8fSKUyNGcBsaGheqpVi3vwwlZCfcCpWEm1ceRTQEVKpYYZ3BzXSqoNOBlq\nA+anPqC1devWLdpja1cCdJ2iKFKt9s0JuJNkYqKUarVP6xIAXlCj0UhR1AXcAACwTAi5ga4zM1Nk\nbKzc8tjYWDn1upAbAAAAYKUQcgNdp6enkdHR6ZbHRkenUyqZmQcAAACwUgi5ga7TaDRSqdQyNFSf\ns394uJ5Kpebj5wAAAAArSO9SDwDgVIyMTKZaTarVvoyNlTM6Op1KpWbRSQAAAIAVRsgNdK2Rkcls\n3TqVLVuKlEoNM7gBAAAAViAhN9DVGo1GiqIR+TYAAADAyqQnNwAAAAAAXUvIDQAAAABA1xJyAwAA\nAADQtYTcAAAAAAB0LSE3AAAAAABdS8gNAAAAAEDXEnIDAAAAANC1hNwAAAAAAHQtITfQVkVRpF4v\npSiKpR4KnBavZQAAAOgMvUs9AGDlGB8fSLXal7GxckZHp1Op1DIyMrnUw4KT5rUMAAAAnUPIDbTF\n+PhAKpWhTEwc+QDJ9u29ufXW/lSrEQ4yr6IoMjNTpKenkUajsdTDSeK1DAAAAJ1GuxJg0RVFkWq1\nrxkKzpqYKKVa7dPugZbGxweybdtQNm06J9u2DWV8fGCph+S1DAAAAB1IyA0supmZImNj5ZbHxsbK\nqdcFg8w1O1t627ZV2b69N9u2rUqlsvRBt9cyAAAAdB4hN7DoenoaGR2dbnlsdHQ6pVJntKGgM3Ty\nbGmvZQAAAOg8Qm5g0TUajVQqtQwN1efsHx6up1KpdUyvZTpDJ8+W9loGAACAzmPhSaAtRkYmU60m\n1WpfxsbKGR2dTqVSs1Afx5idLb19+7G3qNnZ0kuZJXstAwAAQGcpGic47Wx6ejo7d+7Mrl27smvX\nrjz++OPZt29fkuSGG27Ipk2bjvsYExMTufPOO/PAAw9k3759KZfLWb9+fa6++uq84Q1vOKUnsHv3\n7lO6DpazwcHBHDhwYKmH0VJRFKnXi/8fVJr1SmuzPbmPblkyPFzPnXdOnHaYvFD14bXMctPJ9w5Y\nSmoDWlMbMD/1Aa2tW7du0R77hGdyj4+P56Mf/egpf6Ndu3blIx/5SJ599tkkSX9/f6ampvLoo4/m\n0Ucfzd/+7d/mt3/7t9PT03PK3wPofI1GI0WxtDNx6XzdMFvaaxkAAAA6w0m1KznrrLNy8cUXN/98\n9rOfzTPPPHPc6w4dOpSPfexjefbZZ3P++efnxhtvzMUXX5yZmZl84xvfyGc+85k89NBD+cxnPpP3\nvOc9p/xkAFg+RkYms3XrVLZsMVsaAAAAmN8Jh9wve9nL8md/9mdz9n3+858/oWur1WomJiZSLpfz\nO7/zOznnnHOSJD09PXnTm96UQ4cO5S/+4i/y9a9/PW9961tz3nnnncRTAGC5MlsaAAAAOJ7S8U85\noiiKU/4m9957b5LkyiuvbAbcR7vuuuvS39+fer3ePBcAAAAAAI7nhEPuU7V79+7mApVXXHFFy3P6\n+/tz6aWXJkkefvjhxR4SAAAAAADLxKKH3E8++WRz+4ILLpj3vPXr1ydJnnrqqcUeEgAAAAAAy8Si\nh9w//elPm9tnn332vOfNHjt06FBqtdpiDwsAAAAAgGVg0UPuycnJ5na5XJ73vL6+vpbXAAAAAADA\nfBY95AYAAAAAgMXSu9jfYGBgoLk9PT2d/v7+lucd3aLk6GuOtmPHjuzYsaP59ebNmzM4OLhAI4Xl\no1wuqw2YR6fWR6PRyOHDjfT2FimKYqmHwwrUqbUBS01tQGtqA+anPmB+t99+e3N748aN2bhx44I8\n7qKH3GvWrGluP/3001m3bl3L855++ukkyapVq+a0Ljlaqyd+4MCBBRopLB+Dg4NqA+bRifUxPj6Q\narUvY2PljI5Op1KpZWRE6y7aqxNrAzqB2oDW1AbMT31Aa4ODg9m8efOiPPaih9zr169vbj/xxBPz\nhtxPPvlkkuT8889f7CEBQMcYHx9IpTKUiYkjHcS2b+/Nrbf2p1qNoBsAAABOwKL35F63bl3OOeec\nJMmDDz7Y8pxarZZHH300SXL55Zcv9pAAoCMURZFqta8ZcM+amCilWu1b1m1LiqJIvV5a1s8RAACA\n9mjLwpOve93rkiTf/va3s2/fvmOOf/WrX83U1FRKpVKuuuqqdgwJAJbczEyRsbFyy2NjY+XU68sz\nAB4fH8i2bUPZtOmcbNs2lPHx1mtxAAAAwIk4qZD74MGDOXDgQA4cOJD9+/en0WgkOTITe3b/gQMH\nMgYXlikAACAASURBVDU1Nee666+/PsPDw6nVavnoRz+aXbt2JUkOHz6cu+66q9lw/I1vfGPOO++8\nhXheANDxenoaGR2dbnlsdHQ6pVKjzSNafLPtWbZtW5Xt23uzbduqVCqCbgAAAE5d0ZhNqk/A+9//\n/pYzsZ/v6quvzvve9745+3bt2pU//MM/bDbe7+/vz3PPPZeZmZkkyRVXXJHf+q3fSm/vybUJ3717\n90mdDyuBRS5gfp1WH8/vyZ0kw8P13HnnxLLryV0URbZtOxJwP9/WrYeydetETuKfJSywTqsN6BRq\nA1pTGzA/9QGtzbdW40I4qUT5RPtmtjrvkksuyX/6T/8pX/ziF/PAAw/kJz/5Sfr7+7N+/fpcc801\nef3rX38yQwGAZWFkZDLValKt9mVsrJzR0elUKrVlF3Anx2/PsmVLkaIQcgMAAHByTmomdycykxuO\n5V1jmF+n1seRhRiLlEqNZTub2UzuztaptQFLTW1Aa2oD5qc+oLXFnMndloUnAYAX1mg0UhT1ZR3y\nNhqNVCq1DA3V5+wfHq6nUqkt6+cOAADA4jm5BtgALKmiKDIzU6SnZ/nO9mV5W0ntWQAAAGgPITdA\nlxgfHxAMsiyMjExm69apbNmyvNuzAAAA0B5CboAuMD4+kEplKBMTR7pMbd/em1tv7U+1GkE3XelI\ne5ZG5NsAAACcLj25ATpcURSpVvuaAfesiYlSqtW+FEWxRCMDAAAAWHpCboAONzNTZGys3PLY2Fg5\n9bqQGwAAAFi5hNwAHa6np5HR0emWx0ZHp1Mq6fcAAAAArFxCboAO12g0UqnUMjRUn7N/eLieSqVm\n0T4AAABgRbPwJEAXGBmZTLWaVKt9GRsrZ3R0OpVKzaKTAAAAwIon5AboEiMjk9m6dSpbthQplRpm\ncAMAAABEuxKArtJoNFIUdQE3bVcURer1UorCQqcAAAB0FjO5AYAXND4+oFUOAAAAHUvIDQDMa3x8\nIJXKUCYmjnz4a/v23tx6a3+q1Qi6AQAA6AjalQAALRVFkWq1rxlwz5qYKKVa7dO6BAAAgI4g5AYA\nWpqZKTI2Vm55bGysnHpdyA0AAMDSE3IDAC319DQyOjrd8tjo6HRKJQugAgAAsPSE3ABAS41GI5VK\nLUND9Tn7h4frqVRqaTSE3AAAACw9C08CAPMaGZlMtZpUq30ZGytndHQ6lUrNopMAAAB0DCE3APCC\nRkYms3XrVLZsKVIqNczgBgAAoKNoVwLAgimKIvV6KUVhQcKlttA/i0ajkaKoC7gBAADoOGZyA7Ag\nxscHtLToEH4WAAAArCRCbgBO2/j4QCqVoUxMHPmA0Pbtvbn11v5UqxGutpmfBQAAACuNdiUAnJai\nKFKt9jVD1VkTE6VUq31al7SRnwUAAAArkZAbgNMyM1NkbKzc8tjYWDn1umC1XfwsAAAAWImE3ACc\nlp6eRkZHp1seGx2dTqlkocJ28bMAAABgJRJyA21VFEXq9ZK2CctIo9FIpVLL0FB9zv7h4XoqlVoa\nDcFqu/hZAAAAsBJZeBJom/HxgVSrfRkbK2d0dDqVSs1CeMvEyMhkqtUs+M+3KIrMzBTp6WkIaE/Q\nYv0sAAAAoFMVjS5PDXbv3r3UQ4COMzg4mAMHDiz1MOYYHx9IpTI0Z0G8oaF6qtUJ4dsycmSmfpFS\n6fRD6cV6U6QT62MxLOTPgpVhpdQGnCy1Aa2pDZif+oDW1q1bt2iPbSY3sOiKoki12jcn4E6SiYlS\nqtW+bN061TEhnJnDp6fRaKQoGjndv7rnvymyfXtvbr21P9VqvClyghbqZwEAAACdTk9uYNHNzBQZ\nGyu3PDY2Vk693hn9ucfHB7Jt21A2bTon27YNZXx8YKmHtCId700R/dwBAACAowm5gUXX09PI6Oh0\ny2Ojo9MplZZ+qunszOFt21Zl+/bebNu2KpWKoHspdMubIgAAAEBnEHIDi67RaKRSqWVoqD5n//Bw\nPZVKbcnbgpg53Fm64U0RAAAAoHMIuYG2GBmZTLU6ka1bD+W1rz2crVsP5c47O2PRyZU8c/jI4oSl\nBQvyF+LxOv1NEQAAAKCzWHgSaJuRkcls3TqVLVuKlEqds7Dj7Mzh7duP/ZU4O3O4Q4a6oMbHB1Kt\n9mVsrJzR0elUKrXTetNhIR/vyJsiWdDxAQAAAMtT0eiUlOkU7d69e6mHAB1ncHAwBw4cWOphdJXZ\nntxHtywZHq53zGzzhdbq+Q4N1VOtntrzXejHm3VkZvjCvimiPqA1tQGtqQ1oTW3A/NQHtLZu3bpF\ne2ztSgDS2e1UFtpC9yBfzJ7mjUYjRVHvmFn/AAAAQOcRcgP8f0faqUzkC1/Yl61bl2fAnSx8D/KV\n3NMcAAAAWHpCboCjrISZw7M9yFuZ7UG+lI8HAAAAcDKE3AArTKPRSKVSy9BQfc7+4eF6KpXaSQf8\nC/14AAAAACejd6kHAED7HelBnlSrfRkbK2d0dDqVSu2UW7TMPt6XvtSXe+8t56qrpnP99af+eAAA\nAAAnSsgNsEId6UE+lS1bipRKjQWZcb16dSNXXHE4q1ebvQ0AAAC0h5AbYAU70oO8kdPNt8fHB1Kp\nDGVi4p+6YA0NDaRajdncAAAAwKLSkxuA01IURarVvjkBd5JMTJRSrfalKIolGhkAAACwEgi5ATgt\nMzNFxsbKLY+NjZVTrwu5AQAAgMUj5AbgtPT0NDI6Ot3y2OjodEol/bkBAACAxSPkBuC0NBqNVCq1\nDA3V5+wfHq6nUqktyIKWAAAAAPOx8CQAp21kZDLValKt9mVsrJzR0elUKjWLTgIAAACLTsgNwIIY\nGZnM1q1T2bKlSKnUMIMbAAAAaAshNwALptFopCgakW8DAAAA7aInNwAAAAAAXattM7nvvvvu3HLL\nLcc976abbspll13WhhEBAAAAANDt2t6upFQqZfXq1fMe7+3VQQVYOkVRZGamSE+PntIAAAAA3aDt\nifKLXvSifOpTn2r3twU4rvHxgVSrfRkbK2d0dDqVSi0jI5NLPSwAAAAAXoBp0wA5EnBXKkOZmDiy\nVMH27b259db+VKsRdAMAAAB0MAtPAiteURSpVvuaAfesiYlSqtW+FEWxRCMDAAAA4HiE3MCKNzNT\nZGys3PLY2Fg59bqQGwAAAKBTtb1dyf79+/PBD34wu3fvTr1ez5o1a7Jhw4Zce+21efnLX97u4QCk\np6eR0dHpbN9+7K/E0dHplEqNWIMSAAAAoDO1fSZ3rVbL448/njPOOCONRiN79uzJ2NhYfv/3fz+3\n3HJL6vV6u4cErHCNRiOVSi1DQ3N//wwP11Op1NKQcAMAAAB0rLbN5D777LNzww035DWveU3WrVuX\n3t7eNBqN/OAHP8gdd9yRhx9+OHfffXf6+/vzq7/6q+0aFkCSI4tLVqtJtdqXsbFyRkenU6nUOm7R\nyaIoMjNTpKenIXwHAAAASFI0OiQl+cQnPpH77rsvpVIp//k//+ecd955J3Td7t27F3lk0H0GBwdz\n4MCBpR5GVyqKIvV68f9blHTEr8em8fGBjg/hu4H6gNbUBrSmNqA1tQHzUx/Q2rp16xbtsTtm4cl3\nvvOdSZJ6vZ77779/iUcDrFSNRiNFUe/IgLtSGcq2bauyfXtvtm1blUplKOPjA0s9NAAAAIAl1faF\nJ+dz3nnnNd/p+vGPf9zynB07dmTHjh3Nrzdv3pzBwcF2DRG6RrlcVhvLSKPRyJe/fEYmJua+Lzkx\nUcqXv9yf//AfelIUxRKNrvuoD2hNbUBragNaUxswP/UB87v99tub2xs3bszGjRsX5HE7JuQ+Ea2e\nuI9/wLF8NGp5qddLueeec1oeu+eeM3LgwESKwqK9J0p9QGtqA1pTG9Ca2oD5qQ9obXBwMJs3b16U\nx+6YdiU//vGPm78Azj333CUeDUDn6OlpZHR0uuWx0dHplEqd1VoFAAAAoJ06JuS+7bbbkiSlUimv\netWrlng0AJ2j0WikUqllaGjubO3h4XoqlVrH9Q8HAAAAaKe2tCvZu3dv/viP/zjXXnttLr/88pxz\nzpGP3TcajfzgBz/IHXfckYcffjhJ8sY3vjFr165tx7AAusbIyGSq1aRa7cvYWDmjo9OpVGoZGZlc\n6qEBAAAALKm29eQeHx/P+Pj4kW/a25uBgYFMTk7m8OHDzXNe//rX51d+5VfaNSSArjIyMpmtW6ey\nZUuRUqlhBjcAAABA2hRyDw0N5dd+7dfy2GOP5Yc//GH279+fgwcP5owzzsiLX/zibNiwIW94wxuy\nYcOGdgwHoGs1Go0URSPybQAAAIAj2hJyl8vlvPnNb86b3/zmdnw7AAAAAABWiI5ZeBIAAAAAAE6W\nkBsAAAAAgK4l5AYAAAAAoGsJuQEAAAAA6FpCbgAAAAAAupaQGwAAAACAriXkBgAAAACgawm5AQAA\nAADoWkJuAAAAAAC6lpAbAAAAAICuJeQGAAAAAKBrCbkBAAAAAOhaQm4AAAAAALqWkBsAAAAAgK4l\n5AYAAAAAoGsJuQEAAAAA6FpCbgAAAAAAupaQGwAAAACAriXkBgAAAACgawm5AQAAAADoWkJuAAAA\nAAC6lpAbAAAAAICuJeQGAAAAAKBrCbkBAAAAAOhaQm4AAAAAALqWkBsAAAAAgK4l5AYAAAAAoGsJ\nuQEAAAAA6FpCbgAAAAAAupaQGwAAAACAriXkBgAAAACgawm5AQAAAADoWkJuAAAAAAC6lpAbAAAA\nAICuJeQGAAAAAKBrCbkBAAAAAOhaQm4AAAAAALqWkBsAAAAAgK4l5AYAAAAAoGsJuQEAAAAA6FpC\nbgAAAAAAupaQGwAAAACAriXkBgAAAACgawm5AQAAAADoWkJuAAAAAAC6lpAbAAAAAICuJeQGAAAA\nAKBrCbkBAAAAAOhaQm4AAAAAALqWkBsAAAAAgK4l5AYAAAAAoGv1tvsbTk1NpVqt5u/+7u+yZ8+e\nlEqlrF27NldeeWWuu+669Pa2fUgAAAAAAHSptibKe/fuzc0335x9+/YlSfr6+nL48OHs2rUru3bt\nyr333psPfehDWbVqVTuHBQAAAABAl2pbu5J6vZ6Pf/zj2bdvX9asWZObbropn/vc5/Lnf/7n+Xf/\n7t9lYGAgP/zhD/PJT36yXUMCAAAAAKDLtS3kvvvuu/Pkk08mSf79v//3ueyyy5rHfv7nfz6//uu/\nniT53ve+l0ceeaRdwwIAAAAAoIu1LeT+1re+lSS57LLLMjIycszxK6+8Mueee26S5J577mnXsAAA\nAAAA6GJtCbmnp6fzD//wD0mSK664Yt7zXvGKVyRJHnrooXYMCwAAAACALteWkPupp55Ko9FIkqxf\nv37e8y644IIkyTPPPJODBw+2Y2gAAAAAAHSxtoTcP/3pT5vbZ5999rznHX3s6GsAAAAAAKCVtoTc\nk5OTze2+vr55zyuXyy2vAQAAAACAVtq28CQAAAAAACy0toTcAwMDze1arTbvedPT0y2vAQAAAACA\nVnrb8U3WrFnT3H766aebC0w+39NPP93ymlk7duzIjh07ml9v3rw569atW8CRwvIxODi41EOAjqU+\noDW1Aa2pDWhNbcD81Ae0dvvttze3N27cmI0bNy7I47ZlJvf555+foiiSJE8++eS85z3xxBNJkuHh\n4Zx55pnHHN+4cWM2b97c/HP0XwrwT9QGzE99QGtqA1pTG9Ca2oD5qQ9o7fbbb5+T7S5UwJ20KeQu\nl8t56UtfmiR58MEH5z3voYceSpK84hWvaMewAAAAAADocm1bePLqq69OcqTlyPj4+DHHv/3tb2fP\nnj1Jkte97nXtGhYAAAAAAF2s5+abb765Hd/owgsvzHe/+91MTEzkwQcfzEUXXZRzzz03jUYjf/u3\nf5tPf/rTOXz4cF75ylfm7W9/+wk/7rnnnruIo4bupTZgfuoDWlMb0JragNbUBsxPfUBri1UbRaPR\naCzKI7ewd+/efPjDH27O2C6Xy2k0GnnuueeSJJdcckluuummrFq1ql1DAgAAAACgi7U15E6Sqamp\nfOlLX8rf/d3fZc+ePSmKImvXrs3o6Giuu+669PT0tHM4AAAAAAB0sbaH3AAAAAAAsFDatvAkAAAA\nAAAstN6lHsDJmJ6ezs6dO7Nr167s2rUrjz/+ePbt25ckueGGG7Jp06YTepyJiYnceeedeeCBB7Jv\n376Uy+WsX78+V199dd7whjcs5lOARTM1NZVqtdpsBVQqlbJ27dpceeWVue6669Lb21XlDse1EPcE\n9wOWq2effTb33Xdfvv/97+fxxx/P3r17U6/Xs3r16lxyySW5+uqr85rXvOYFH0N9sBw9/vjjuf/+\n+7Nr16784z/+Y/bv359Dhw5l1apVWbduXV75ylfmTW96U84666x5H0NtsJJ88YtfzF/8xV80v/6r\nv/qrec9VGyxXd999d2655ZbjnnfTTTflsssua3lMfbDcTU5O5mtf+1ruv//+/OM//mMmJyezevXq\nnHfeeXn5y1+et771rS3XYFzI2uiqdiU7d+7M7//+77c8dqKBxq5du/KRj3wkzz77bJKkv78/zz33\nXGZmZpIkr3jFK/Lbv/3beoPTVfbu3Zubb765GfD19fWlXq83F3W96KKL8qEPfciiriwrp3tPcD9g\nOXvHO96Rer3e/LpcLqdUKmVqaqq574orrsjWrVtTLpePuV59sFz9l//yX/K1r32t+XW5XE5PT08m\nJyeb+wYHB/OBD3wgGzZsOOZ6tcFKsnv37nzgAx9o/p8imT/kVhssZ7Mhd6lUyurVq+c9b8uWLbn0\n0kuP2a8+WO4eeeSR/Mmf/En279+fJOnt7U1fX18OHjzYPOc//sf/mAsvvHDOdQtdG103tfOss87K\nxRdf3Pzz2c9+Ns8888wJXXvo0KF87GMfy7PPPpvzzz8/N954Yy6++OLMzMzkG9/4Rj7zmc/koYce\nymc+85m85z3vWeRnAgujXq/n4x//ePbt25c1a9bkxhtvbL57/J3vfCd/+qd/mh/+8If55Cc/mQ9+\n8INLPFpYWKd6T3A/YLmr1+v52Z/92VxzzTW5/PLLc+655yZJ9u3bly984Qv55je/mQcffDCf/vSn\nc+ONN865Vn2wnI2MjOTcc8/NpZdemnXr1jUnANRqtWzfvj233XZb9u/fnz/6oz/Kn/zJn2RgYKB5\nrdpgJWk0Grnlllvy3HPPZcOGDXnsscfmPVdtsFK86EUvyqc+9amTukZ9sNw9+uij+fjHP57p6em8\n9rWvzb/8l/8yF198cZIjn75+6qmn8t3vfveYSZeLURs9N998880L/QQXyznnnJN/8S/+RV73utfl\n8ssvz/r16/PVr341hw4dysaNG/Pyl7/8Ba//whe+kAcffDDlcjl/8Ad/kJ/5mZ9JkpRKpbzkJS9J\nT09PHnnkkTz++OMZHR19wY8pQqf4X//rf+Wb3/xmkiMfjzr6neP169fnn/2zf5bt27fnRz/6UV72\nspc1gw7odqdzT3A/YLnbuHFj/vW//te55JJLcuaZZzb3r1q1Kq9+9avzzDPPZNeuXXniiSdy7bXX\nzgny1AfL2YUXXpgNGzbk7LPPzhlnnNHc39vbmwsvvDAXXXRR7r333tRqtaxfvz4XXHBB8xy1wUry\nla98Jd/85jdz1VVX5SUveUl27tyZ5Min5Z5PbbDc/fCHP8x9992XM888M7/wC79wUteqD5az6enp\n/MEf/EH279+ft7zlLfmN3/iNrFmzpnm8p6cna9asyWWXXTbn/yTJ4tRGVy08WRTFaV1/7733Jkmu\nvPLKnHPOOcccv+6669Lf3596vd48Fzrdt771rSTJZZddlpGRkWOOX3nllc1g+5577mnr2GAxnc49\nwf2A5e54b/wf3d/uf//v/z3nmPpgJfvZn/3Z5vbTTz8955jaYKXYs2dP/vIv/zKrV6/Ou9/97uOe\nrzZgfuqD5exb3/pW9uzZk+Hh4fzyL//ySV27GLXRVSH36di9e3ezX/EVV1zR8pz+/v7mLNiHH364\nbWODUzU9PZ1/+Id/SDL/6zo50scoSR566KG2jAs6mfsBZM4M1qN7d6sPVrq///u/b26/+MUvbm6r\nDVaSP/3TP02tVsu73vWuDA4OvuC5agPmpz5Y7mYnUv78z/98entPvCP2YtVG1/XkPlVPPvlkc/vo\njx0+3/r16/Pggw/mqaeeasew4LQ89dRTmV07dv369fOeN/uaf+aZZ3Lw4MFjPiYCK4n7ASQ7duxo\nbh9dB+qDlejw4cP56U9/mvvvvz+33357kmTt2rV51ate1TxHbbBSfP3rX88jjzySyy+/PFddddVx\nz1cbrCT79+/PBz/4wezevTv1ej1r1qzJhg0bcu2117b8FJ36YDk7fPhwdu3alSS55JJLmmv/PPjg\ng5mYmMiZZ56ZkZGRvPGNb8zP/dzPzbl2sWpjxYTcP/3pT5vbZ5999rznzR47dOhQarVa+vr6Fn1s\ncKpO9nU9e42Qm5XM/YCV7tChQ/niF7+Y5Ehbk7Vr1zaPqQ9Wkl/6pV/K4cOHj9l/6aWX5jd/8zfn\nzEhSG6wETz/9dD7/+c+nXC7n13/910/oGrXBSlKr1fL444/nrLPOytTUVPbs2ZM9e/ZkbGws11xz\nTf7tv/23KZX+qWGC+mA527NnT/PfUT/60Y/yZ3/2Z5mamkpvb2/6+/uzf//+PPDAA3nggQdy7bXX\n5r3vfW/z2sWqjRUTck9OTja3y+XyvOcd/Rc2OTnplwsd7ejX9Qu9Vo9+zR99DaxE7gesZI1GI5/8\n5CfzzDPPpFwu59d+7dfmHFcfrCRr1qzJc889l6mpqUxNTSU5ssbJL/3SLx3zHy61wUrw6U9/OocO\nHcov//Ivn/Bi9WqDleDss8/ODTfckNe85jVZt25dent702g08oMf/CB33HFHHn744dx9993p7+/P\nr/7qrzavUx8sZwcPHmxu//Vf/3XOPPPMbN26Na9+9atTKpXyk5/8JLfddlu+853v5Bvf+EZ+5md+\nJm9961uTLF5tLGrIfffdd+eWW2455et/93d/t9lLGACA0/Nf/+t/zfe+970kyXve854XbHUFy92n\nPvWp5vb+/ftzzz335K//+q/zO7/zO/lX/+pfZfPmzUs4Omive+65J9/73vdy8cUXN0MI4IjLL788\nl19++Zx9RVFkw4YN+b3f+7184hOfyH333Ze77rorb3nLW3Leeect0UihfWZb585u/8Zv/EZe/epX\nN/e96EUvym/+5m9m9+7d+T//5//kb/7mb/KWt7xlzqcdFlrHLjxZFMWCPt7AwEBze3p6et7zarVa\ny2ugEx39Gj36tft8R7/mva5Z6dwPWKk+97nP5Wtf+1qS5Fd+5VdyzTXXHHOO+mClWr16dd72trfl\nd3/3d1MURb7whS/kgQceaB5XGyxnExMT+exnP5tSqXRMu4XjURuQvPOd70xyZDHv+++/v7lffbCc\n9ff3N7fXrl07J+CeVRRFrr/++iTJgQMHmj28F6s2FnUm9+joaMsneaJWrVq1YGNZs2ZNc/vpp5/O\nunXrWp739NNPN7+3j4jQ6Z7/up6vYf/s6/r518BK5H7ASvTnf/7n+R//438kSd71rnflLW95S8vz\n1Acr3cjISC699NL8/d//fb7+9a83F0pSGyxnn//85/Pss8/mTW96U9auXdts3zPr6N71s8d6e3vT\n29urNiDJeeedl8HBwRw4cCA//vGPm/vVB8vZ0a3d5nttJ8n555/f3N63b19GRkYWrTYWNeTu7e3N\nWWedtZjf4oQd/XHcJ554Yt6/wNkVPo/+IUCnOv/881MURRqNRp588slcccUVLc974oknkiTDw8MW\nnWTFcz9gpbntttvy5S9/OcmRmUYv9DF09QH/9J+2o4MKtcFytnfv3iTJXXfdlbvuuusFz333u9+d\nJPmFX/iFvPvd71Yb8ALUB8vZWWedlbPPPnvOpMpWjm5rMmuxaqNj25UstHXr1uWcc85Jkjz44IMt\nz6nVann00UeT5Jh+S9CJyuVyXvrSlyaZ/3WdJA899FCS6HEPcT9gZfnc5z43J+B+29ve9oLnqw/4\np3D76I/hqg2Ya7a9qNqAI/eNAwcOJMmcRVvVB8vd7Gv2//7f/zvvOU899VRze7Y+Fqs2VkzInSSv\ne93rkiTf/va3s2/fvmOOf/WrX83U1FRKpVKuuuqqdg8PTsnVV1+dJNmxY0fGx8ePOf7tb387e/bs\nSfJPNQArnfsBK8HnPve5OS1Kjhdwz1IfLFf1ev2453z/+99v/ntq48aNc46pDZarD33oQ/mrv/qr\nef9s2rSpee7svne9613NfWqDle62225LkpRKpbzqVa+ac0x9sJy9/vWvT5L86Ec/yn333XfM8Uaj\nkS996UtJjnxS7pJLLmkeW4za6LqQ++DBgzlw4EAOHDiQ/fv3N6e912q15v4DBw4c00csSa6//voM\nDw+nVqvlox/9aLPh+eHDh3PXXXfl9ttvT5K88Y1vtBouXeOaa67JBRdckEajkW3btuWRRx5JcuSX\nyXe+8518+tOfTpK88pWvzGWXXbaUQ4UFd6r3BPcDlruje3C/+93vfsEWJc+nPliufvKTn+QDH/hA\nvv71rzcnABx97Itf/GI+8YlPJEkGBwePqRu1Aa2pDZazvXv35vd+7/fyzW9+c04Q12g08thjj+Uj\nH/lIvvvd7yY58hpfu3btnOvVB8vZpZdemte+9rVJkltuuSXbt29vTirYt29f/viP/7jZPvcd73jH\nnGsXozaKRqvmKB3s/e9/f8uE//muvvrqvO997ztm/65du/KHf/iHzY+S9Pf357nnnsvMzEyS5Ior\nrshv/dZvpbd3UduVw4Lau3dvPvzhDzf/w1Yul9NoNPLcc88lSS655JLcdNNNC7qYK3SC07knuB+w\nXO3bty/vf//7kxz5OPnq1atf8PxKpXLMLG/1wXK0d+/e3Hjjjc2ve3t7MzAwkOnp6dRqteb+P5za\niwAAAgBJREFUF7/4xdm6dWsuvPDCYx5DbbAS3XHHHfnv//2/Jzkyk7sVtcFyNd+9Y3Jycs6irK9/\n/evz3ve+N6XSsXNJ1QfLWa1Wy8c+9rHs3LkzyZEa6evry8GDB5vn3HDDDXM+FTRroWuj5+abb775\nNJ9PW33lK1/JoUOHjnvexRdfnH/+z//5MfvXrFmTa665JvV6Pc8++2wOHTqUvr6+vOQlL8mmTZvy\nzne+s+UvJehkZ555Zt7whjekt7e3+bru6enJBRdckOuvvz7vfe97rdLMsnQ69wT3A5argwcP5n/+\nz//Z/LpWq73gn5e+9KV5+ctfPucx1AfLUV9fXy666KLmGz+NRiMHDx5MURRZs2ZNXvayl+UXf/EX\n82/+zb9pLj75fGqDlWjnzp3ZuXNniqJoGVIkaoPl64wzzsiaNWty5plnpl6vp16v59ChQznjjDPy\n4he/OK9+9avznve8J29+85ubveqfT32wnPX29uaaa67Ji170okxOTubgwYOp1WoZHh7Oz/3cz+W9\n733vvO1GFro2um4mNwAAAAAAzPJWEQAAAAAAXUvIDQAAAABA1xJyAwAAAADQtYTcAAAAAAB0LSE3\nAAAAAABdS8gNAAAAAEDXEnIDAAAAANC1hNwAAAAAAHQtITcAAAAAAF1LyA0AAAAAQNcScgMAAAAA\n0LX+H+IGxrFAT9AxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02f945fad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.scatter(fold_act_scores_mmse[2], fold_pred_scores_mmse['hyp1_2'][3],s=50)\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "\n",
    "np.sqrt(85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results at: /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/output/\n"
     ]
    }
   ],
   "source": [
    "#Save df style dictionaly for seaborn plots\n",
    "cohort = 'ADNI1'\n",
    "update = 0\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_bl_up_{}.pkl'.format(exp_name, cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_m12_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse,df_perf_dict_path)\n",
    "\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_bl_tuned_up_{}.pkl'.format(exp_name,cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas_tuned,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_m12_tuned_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse_tuned,df_perf_dict_path)\n",
    "\n",
    "\n",
    "print 'saving results at: {}'.format(CV_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 25)\n",
    "n_rows = n_folds\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "plot_perf = False\n",
    "\n",
    "if multi_task:\n",
    "    euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "    corrs = [fold_r_adas13,fold_r_mmse]\n",
    "else:\n",
    "    euLosses = [fold_euLoss]\n",
    "    corrs = [fold_r]\n",
    "    \n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        \n",
    "        if plot_perf:\n",
    "            plt.figure(fid)\n",
    "            plt.subplot(n_rows,n_cols,1)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "            plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "            plt.legend()\n",
    "            plt.subplot(n_rows,n_cols,2)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "            plt.title('correlation, fid: {}'.format(fid))\n",
    "            plt.legend(loc=2)\n",
    "\n",
    "print preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path):\n",
    "    fold_r_dict = {}\n",
    "    fold_euLoss_dict = {}\n",
    "    fold_act_scores_dict = {}\n",
    "    fold_pred_scores_dict = {}\n",
    "\n",
    "    if multi_task:\n",
    "        fold_r_dict['ADAS'] = fold_perf_dict['fold_r_adas13']\n",
    "        fold_r_dict['MMSE'] = fold_perf_dict['fold_r_mmse']\n",
    "        fold_euLoss_dict['ADAS'] = fold_perf_dict['fold_euLoss_adas13']\n",
    "        fold_euLoss_dict['MMSE'] = fold_perf_dict['fold_euLoss_mmse']\n",
    "        fold_act_scores_dict['ADAS'] = fold_perf_dict['fold_act_scores_adas13']\n",
    "        fold_act_scores_dict['MMSE'] = fold_perf_dict['fold_act_scores_mmse']\n",
    "        fold_pred_scores_dict['ADAS'] = fold_perf_dict['fold_pred_scores_adas13']\n",
    "        fold_pred_scores_dict['MMSE'] = fold_perf_dict['fold_pred_scores_mmse']\n",
    "        \n",
    "        NN_results = computePerfMetrics(fold_r_dict, fold_euLoss_dict, fold_act_scores_dict, fold_pred_scores_dict, opt_metric, hype_configs, \n",
    "                                        Clinical_Scale,task_weights)\n",
    "        adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "        mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "        adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "        mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "        adas_rmse = NN_results['opt_ADAS']['CV_RMSE'].values()\n",
    "        mmse_rmse = NN_results['opt_MMSE']['CV_RMSE'].values()\n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['opt_ADAS']['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['opt_ADAS']['actual_CV_scores']\n",
    "        \n",
    "        print 'ADAS corr: {}'.format(adas_r)\n",
    "        print 'ADAS mse: {}, rmse: {}'.format(adas_mse,adas_rmse)\n",
    "        print 'ADAS means: {}, {}, {}'.format(np.mean(adas_r),np.mean(adas_mse),np.mean(adas_rmse))\n",
    "        print ''\n",
    "        print 'MMSE corr: {}'.format(mmse_r)\n",
    "        print 'MMSE mse: {}, rmse: {}'.format(mmse_mse,mmse_rmse)\n",
    "        print 'MMSE means: {}, {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse),np.mean(mmse_rmse))\n",
    "        print ''\n",
    "        print 'opt_hyp: {}'.format(opt_hyp)\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        \n",
    "        return {'adas_r':adas_r, 'adas_mse':adas_mse, 'adas_rmse':adas_rmse, 'mmse_r':mmse_r, 'mmse_mse':mmse_mse, 'mmse_rmse':mmse_rmse,\n",
    "               'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        NN_results = computeSingleTaskPerfMetrics(fold_perf_dict, opt_metric, hype_configs)\n",
    "        opt_r = NN_results['opt_r'].values()        \n",
    "        opt_mse = NN_results['opt_mse'].values()        \n",
    "        opt_rmse = NN_results['opt_rmse'].values()        \n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['actual_CV_scores']\n",
    "        print 'opt corr: {}'.format(opt_r)\n",
    "        print 'opt mse: {}'.format(opt_mse)\n",
    "        print 'means: {}, {}'.format(np.mean(opt_r),np.mean(opt_mse))\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        print ''\n",
    "    \n",
    "        return {'opt_r':opt_r, 'opt_mse':opt_mse, 'opt_rmse':opt_rmse,'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    \n",
    "\n",
    "\n",
    "    if save_multitask_results:\n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')        \n",
    "        save_path = save_path + '_' + st + '.pkl' \n",
    "        print 'saving results at: {}'.format(save_path)\n",
    "        output = open(save_path, 'wb')\n",
    "        pickle.dump(NN_results, output)\n",
    "        output.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = np.squeeze(results['predicted_CV_scores'][7])\n",
    "b = np.squeeze(results['actual_CV_scores'][7])\n",
    "\n",
    "print a , b                               \n",
    "print stats.pearsonr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    \n",
    "     # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "\n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    opt_rmse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "\n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    opt_rmse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "\n",
    "    print 'Clinical_Scale: {}'.format(Clinical_Scale)\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        print 'This function does not work for single clinical scale models. Use the code from the next cell'\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "\n",
    "        fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "        fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "        fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "        fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "\n",
    "        for fid in fid_hype_map.keys():\n",
    "            r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "            r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "            r_perf_array = np.array(fid_r_perf[fid])\n",
    "            euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "            euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "            euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "            if opt_metric == 'euLoss':\n",
    "                h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "            else:\n",
    "                h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "\n",
    "            opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "            opt_snap[fid] = snp\n",
    "\n",
    "            opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "            opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "            opt_rmse_adas[fid] = np.sqrt(2*euLoss_perf_array_adas[h,snp]) #RMSE\n",
    "            opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "            opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "            opt_rmse_mmse[fid] = np.sqrt(2*euLoss_perf_array_mmse[h,snp]) #RMSE\n",
    "\n",
    "            actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "            opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "            actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "            opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "        opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'CV_RMSE':opt_rmse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "        opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'CV_RMSE':opt_rmse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_snap':opt_snap, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "def computeSingleTaskPerfMetrics(fold_perf_dict,opt_metric,hype_configs):\n",
    "    corrs = fold_perf_dict['fold_r']\n",
    "    euLosses = fold_perf_dict['fold_euLoss']\n",
    "    act_scores = fold_perf_dict['fold_act_scores']\n",
    "    pred_scores = fold_perf_dict['fold_pred_scores']\n",
    "\n",
    "    \n",
    "    NN_multitask_results = {}\n",
    "   \n",
    "    snap_array = np.arange(snap_start,niter+1,snap_start)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = {}\n",
    "    opt_mse = {}\n",
    "    opt_rmse = {}\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "    actual_scores = {}\n",
    "    opt_pred_scores = {}\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "        # if want to find best hyp from mse values\n",
    "        if opt_metric == 'euLoss':\n",
    "            h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        else:\n",
    "            h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "            \n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        r = r_perf_array[h,snp]        \n",
    "        opt_r[fid] = r\n",
    "        opt_mse[fid] = 2*eu_loss\n",
    "        opt_rmse[fid] = np.sqrt(2*eu_loss)\n",
    "        #print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)        \n",
    "        opt_snap[fid] = snp\n",
    "        opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "        actual_scores[fid] = fold_act_scores[fid]\n",
    "        opt_pred_scores[fid] = fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp]\n",
    "\n",
    "    #print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r.values()), np.mean(opt_mse.values()))\n",
    "    #print opt_r\n",
    "    #print opt_mse\n",
    "    return {'opt_r':opt_r,'opt_mse':opt_mse,'opt_rmse':opt_rmse,'opt_hype':opt_hype,'opt_snap':opt_snap, 'actual_CV_scores':actual_scores,'predicted_CV_scores':opt_pred_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "\n",
    "model_file = 'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-05-06-17-14-49.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {4:0,5:1}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_ADAS13_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(5.64658243068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update fold performance for multitask\n",
    "print NN_results['opt_ADAS'].keys()\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "#model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl'\n",
    "model_file_soa = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-23-52.pkl'\n",
    "print 'current state of art: ' + model_file_soa\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file_soa,'rb'))\n",
    "\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "#load old file\n",
    "# model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-02-48.pkl'\n",
    "# print 'updated fold result file: ' + model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "idx_pairs = {1:1,3:3}\n",
    "CV_data = update_multifold_perf(boxplots_dir + model_file_soa, NN_results, idx_pairs)\n",
    "\n",
    "print \"\"\n",
    "print 'updated CV_data'\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "#print 'saving results at: {}'.format(save_name)\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(CV_data, output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():        \n",
    "        for idx in idx_pairs.keys():            \n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "\n",
    "    return CV_data\n",
    "\n",
    "def update_multifold_perf(saved_perf_file, new_perf_dict, idx_pairs):    \n",
    "    perf_metrics = ['CV_r', 'actual_CV_scores', 'CV_MSE', 'predicted_CV_scores']    \n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        if key == 'opt_hype':\n",
    "            for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                CV_data[key][idx_key] = new_perf_dict[key][idx_val]\n",
    "                \n",
    "        else:\n",
    "            for pm in perf_metrics:\n",
    "                for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                    CV_data[key][pm][idx_key] = new_perf_dict[key][pm][idx_val]\n",
    "    \n",
    "    return CV_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "model_files = ['Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl',             \n",
    "             'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-05-23-07-31-29.pkl']\n",
    "\n",
    "#'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-03-19-29-16_Fold9_10.pkl'\n",
    "\n",
    "Clinical_Scale = 'ADAS'\n",
    "perf_array = []\n",
    "for mf in model_files:\n",
    "    CV_data = pickle.load(open(boxplots_dir + mf,'rb'))['opt_{}'.format(Clinical_Scale)]        \n",
    "    perf_array.append(CV_data['CV_r'].values())\n",
    "\n",
    "print (np.max(np.array(perf_array),axis=0))\n",
    "print np.mean(np.max(np.array(perf_array),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "\n",
    "\n",
    "save_detailed_results = True\n",
    "multi_task = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results    \n",
    "    if not multi_task:\n",
    "        NN_results = {}\n",
    "        NN_results['CV_MSE'] = opt_mse\n",
    "        NN_results['CV_r'] = opt_r\n",
    "        NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "        NN_results['actual_CV_scores'] = actual_scores\n",
    "        NN_results['tid_snap_config_dict'] = opt_hype\n",
    "        print opt_r\n",
    "        print opt_mse\n",
    "        \n",
    "    else:\n",
    "        NN_results = NN_multitask_results\n",
    "        print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "        print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "        print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "        print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])\n",
    "        \n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_BOTH_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    for modality in modalities:\n",
    "        in_data = load_data(in_data_path, 'Fold_{}_{}'.format(fid,modality), preproc)         \n",
    "\n",
    "        # HDF5 is pretty efficient, but can be further compressed.\n",
    "        comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "        with h5py.File(out_data_path, 'a') as f:      \n",
    "            if modality == 'X_R_CT': #Fix the typo            \n",
    "                modality = 'X_CT'            \n",
    "\n",
    "            f.create_dataset('{}'.format(modality), data=in_data, **comp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold1_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold1_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold2_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold2_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold3_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold3_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold4_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold4_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold5_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold5_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold6_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold6_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold7_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold7_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold8_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold8_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold9_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold9_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold10_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold10_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n"
     ]
    }
   ],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp15'\n",
    "exp_name_out = 'Exp15_MC'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "modalities = ['X_L_HC','X_R_HC','X_R_CT','adas_bl','adas_m12','dx']\n",
    "dataset = 'ADNI2'\n",
    "preproc = 'no_preproc' #binary labels\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/'\n",
    "\n",
    "for mc in np.arange(1,2,1):\n",
    "    for fid in np.arange(1,11,1):\n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_train_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_valid_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_ADAS13_NN_valid_MC_{}.h5'.format(exp_name,dataset,mc)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name_out)\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC: fid[1:4] 8000; fid[5,6] 6000, fid[7,8,9]: 8000 fid 10: 6000\n",
    "CT: fid[1:5] 12000 fid[6:10] 6000\n",
    "\n",
    "#spawn net\n",
    "ADNI_FF_train_hyp1_CT.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "pretrained ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold1/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold1/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "pretrained ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold2/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold2/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "pretrained ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold3/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold3/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "pretrained ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold4/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold4/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "pretrained ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold5/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold5/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "pretrained ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold6/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold6/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "pretrained ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold7/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold7/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "pretrained ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold8/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold8/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "pretrained ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold9/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold9/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "pretrained ff1 weights are (25, 686) dimensional and biases are (25,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold10/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/API/data/MC_1/fold10/pretrained_models/ADNI1_ff_hyp1_HC_CT_HC_snap_20000_CT_snap_32000_Sup_Concat.caffemodel\n"
     ]
    }
   ],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "cohort = 'ADNI1'\n",
    "modality = 'HC_CT'\n",
    "pretrain_snap_HC = 20000 #ADNI1: 5000\n",
    "pretrain_snap_CT = 32000 #ADNI1: 5000\n",
    "n_folds = 10\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/' \n",
    "#Custom snaps per fold\n",
    "#HC_iter = [8000,8000,8000,8000,6000,6000,8000,8000,8000,6000]\n",
    "#CT_iter = [12000,12000,12000,12000,12000,6000,6000,6000,6000,6000]\n",
    "#mc=1\n",
    "hc_hyp = 'hyp2'\n",
    "ct_hyp = 'hyp2'\n",
    "pretrain_hyp = 'hyp1' #This is hyp generated using optimal combination of hc and ct hyps. Prototxt file is saved with this hyp extension\n",
    "\n",
    "exp_name = 'Exp14_MC'\n",
    "\n",
    "for mc in np.arange(1,2,1):\n",
    "    for fid in np.arange(1,n_folds+1,1):\n",
    "        print 'fid: {}'.format(fid)\n",
    "        #for AE_branch in ['CT','R_HC','L_HC']:\n",
    "        for AE_branch in ['CT','HC']:\n",
    "            print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "            if AE_branch == 'L_HC':\n",
    "                params_FF = ['L_ff1', 'L_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'R_HC':\n",
    "                params_FF = ['R_ff1', 'R_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'HC':\n",
    "                params_FF = ['L_ff1','R_ff1']\n",
    "                AE_iter = pretrain_snap_HC\n",
    "                hyp = hc_hyp\n",
    "            elif AE_branch == 'CT':\n",
    "                #params_FF = ['ff1', 'ff2']\n",
    "                params_FF = ['ff1']\n",
    "                AE_iter = pretrain_snap_CT\n",
    "                hyp = ct_hyp\n",
    "                #fid for pretain is 1 because it's same definition for all the folds.\n",
    "                #Only use this during 1 of the modalities to avoid overwritting\n",
    "                print 'Spawning new net'\n",
    "                pretrain_net = caffe.Net(baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,pretrain_hyp,modality), caffe.TRAIN)\n",
    "            else:\n",
    "                print 'Wrong AE branch'\n",
    "\n",
    "            # conv_params = {name: (weights, biases)}\n",
    "            conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "            for conv in params_FF:\n",
    "                print 'target {} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "            # Review AE net params \n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hyp,AE_branch)\n",
    "            #model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "            model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hyp,AE_branch,AE_iter) \n",
    "\n",
    "            AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "            #params_AE = ['encoder1', 'code']\n",
    "            params_AE = params_FF #if you are using pretrained NN\n",
    "\n",
    "            # fc_params = {name: (weights, biases)}\n",
    "            fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "            for fc in params_AE:\n",
    "                print 'pretrained {} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "            #transplant net parameters\n",
    "            for pr, pr_conv in zip(params_AE, params_FF):\n",
    "                conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "                conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "            save_net = True\n",
    "            if save_net:\n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_{}_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                save_path = baseline_dir + net_name.format(mc,fid,cohort,pretrain_hyp,modality,pretrain_snap_HC,pretrain_snap_CT)\n",
    "                print \"Saving net to \" + save_path\n",
    "                pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_data['opt_ADAS']['CV_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Wxh = 0.5\n",
    "Whh = -1.0\n",
    "hb = -1.0\n",
    "x0 = 9\n",
    "x1 = 4\n",
    "x2 = -2\n",
    "\n",
    "h0 = 1/(1+exp(-Wxh*x0+hb))\n",
    "h1 = 1/(1+exp(h0*Whh - Wxh*x1 + hb))\n",
    "h2 = 1/(1+exp(h1*Whh - Wxh*x2 + hb))\n",
    "\n",
    "print h0,h1,h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
