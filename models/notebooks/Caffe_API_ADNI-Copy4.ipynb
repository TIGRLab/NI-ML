{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import caffe\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(3)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "# Some defs to load data and extract encodings from trained net\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    #print net.blobs.items()[0]\n",
    "    #print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        #print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        #print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        #print net.blobs[input_node].shape\n",
    "        \n",
    "        data_layers.append(net.blobs[input_node])    \n",
    "        #print 'X_list shape: {}'.format(X_list[i].shape)\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "    \n",
    "     \n",
    "    net.reshape()            \n",
    "    #print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        #print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1, concat_param=dict(axis=1))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT_AAL_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT_ALL_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_CT_AAL_dyn,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT_AAL_dyn, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT_unified(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_HC_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_HC_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_HC_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_HC_CT,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_HC_CT, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=4, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.adas, n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "    n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "    n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "    n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff2,n.R_ff2,n.ff2, concat_param=dict(axis=1))\n",
    "    #n.concat = L.Concat(n.X_L_HC,n.X_R_HC,n.X_CT, concat_param=dict(axis=1))\n",
    "    \n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.dropC1 = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2) #This is done automatically by caffe! \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        #n.ff2_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2ADAS13 = L.ReLU(n.ff2_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        #n.ff2_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2MMSE = L.ReLU(n.ff2_MMSE, in_place=True)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        n.ff1_DX = L.InnerProduct(n.ff3, num_output=node_sizes['DX_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1DX = L.ReLU(n.ff1_DX, in_place=True)\n",
    "        \n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_DX = L.InnerProduct(n.ff1_DX, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.DX_loss = L.SoftmaxWithLoss(n.output_DX, n.dx_cat3,loss_weight=tr['DX'])  \n",
    "    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_CT, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.dropC1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "        \n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_R_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_L_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))       \n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='HC_CT': #multimodal AE - experimental stage\n",
    "        HC_node_split = 0.8\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_L_HC = L.InnerProduct(n.X_L_HC, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.9*node_sizes['En1'])))        \n",
    "        n.NLinEn_L_HC = L.ReLU(n.encoder_L_HC, in_place=True)\n",
    "        \n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_CT = L.InnerProduct(n.X_CT, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.1*node_sizes['En1'])))        \n",
    "        n.NLinEn_CT = L.ReLU(n.encoder_CT, in_place=True)\n",
    "        \n",
    "        #Concat         \n",
    "        n.encoder1 = L.Concat(n.encoder_L_HC,n.encoder_CT, concat_param=dict(axis=1))\n",
    "        \n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers (or common encoder layers for multimodal) \n",
    "    \n",
    "#     n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.Sigmoid(n.encoder2, in_place=True)\n",
    "    #code layer\n",
    "#    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='xavier'))  \n",
    "    \n",
    "#     n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "    \n",
    "    #Decoder layers\n",
    "    if modality == 'CT':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)                    \n",
    "    \n",
    "    elif modality =='R_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    elif modality =='HC_CT':        \n",
    "        n.decoder_L_HC = L.InnerProduct(n.code, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_L_CT = L.ReLU(n.decoder_L_HC, in_place=True)\n",
    "        n.decoder_CT = L.InnerProduct(n.code, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_CT = L.ReLU(n.decoder_CT, in_place=True)\n",
    "        \n",
    "        n.output_L_HC = L.InnerProduct(n.decoder_L_HC, num_output=node_sizes['out_HC'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_HC = L.SigmoidCrossEntropyLoss(n.output_L_HC, n.X_L_HC)\n",
    "        \n",
    "        n.output_CT = L.InnerProduct(n.decoder_CT, num_output=node_sizes['out_CT'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_CT = L.EuclideanLoss(n.output_CT, n.X_CT)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    return n.to_proto()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.01 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode):\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 20\n",
    "    \n",
    "    if multimodal_autoencode:\n",
    "        train_loss_HC = zeros(niter)\n",
    "        test_loss_HC = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_loss_CT = zeros(niter)\n",
    "        test_loss_CT = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "        for it in range(niter):            \n",
    "            solver.step(1)  # SGD by Caffe \n",
    "            train_loss_HC[it] = solver.net.blobs['loss_HC'].data\n",
    "            train_loss_CT[it] = solver.net.blobs['loss_CT'].data\n",
    "            \n",
    "            if it % test_interval == 0:                        \n",
    "                t_loss_HC = 0\n",
    "                t_loss_CT = 0                                \n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()   \n",
    "                    t_loss_HC += solver.test_nets[0].blobs['loss_HC'].data\n",
    "                    t_loss_CT += solver.test_nets[0].blobs['loss_CT'].data\n",
    "                \n",
    "                test_loss_HC[it // test_interval] = t_loss_HC/(test_iter)\n",
    "                test_loss_CT[it // test_interval] = t_loss_CT/(test_iter)\n",
    "                print 'HC Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_HC[it], np.sum(train_loss_HC)/it, test_loss_HC[it // test_interval])\n",
    "                print 'CT Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_CT[it], np.sum(train_loss_CT)/it, test_loss_CT[it // test_interval])\n",
    "                \n",
    "        perf = {'train_loss':[train_loss_HC, train_loss_CT],'test_loss':[test_loss_HC,test_loss_CT]}\n",
    "    else: \n",
    "        \n",
    "        #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "        # losses will also be stored in the log\n",
    "        test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "        if not multi_task:\n",
    "            train_loss = zeros(niter)\n",
    "            test_loss = zeros(int(np.ceil(niter / test_interval)))  \n",
    "\n",
    "        else: \n",
    "            if multi_label:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_DX_loss = zeros(niter)\n",
    "                test_DX_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "            else:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_MMSE_loss = zeros(niter)\n",
    "                test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "\n",
    "        #output = zeros((niter, batch_size))\n",
    "        #solver.restore()\n",
    "        #the main solver loop\n",
    "        for it in range(niter):\n",
    "            #solver.net.forward()\n",
    "            solver.step(1)  # SGD by Caffe    \n",
    "            # store the train loss\n",
    "            if not multi_task:\n",
    "                train_loss[it] = solver.net.blobs['loss'].data        \n",
    "            else: \n",
    "                if multi_label:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_DX_loss[it] = solver.net.blobs['DX_loss'].data\n",
    "                else:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "            # store the output on the first test batch\n",
    "            # (start the forward pass at conv1 to avoid loading new data)\n",
    "            #solver.test_nets[0].forward()\n",
    "            #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "            # run a full test every so often\n",
    "            # (Caffe can also do this for us and write to a log, but we show here\n",
    "            #  how to do it directly in Python, where more complicated things are easier.)\n",
    "            if it % test_interval == 0:        \n",
    "                t_loss = 0\n",
    "                t_ADAS13_loss = 0\n",
    "                t_MMSE_loss = 0\n",
    "                t_DX_loss = 0\n",
    "                correct = 0\n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()                \n",
    "                    if not multi_task:\n",
    "                        t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                        if multi_label:\n",
    "                            correct += sum(solver.test_nets[0].blobs['output'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "                    else: \n",
    "                        if multi_label:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_DX_loss += solver.test_nets[0].blobs['DX_loss'].data\n",
    "                            correct += sum(solver.test_nets[0].blobs['output_DX'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "\n",
    "                        else:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "                if not multi_task:\n",
    "                    test_loss[it // test_interval] = t_loss/(test_iter)\n",
    "                    if multi_label:\n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                    print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                        it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval], test_acc[it // test_interval])\n",
    "                else:\n",
    "                    if multi_label:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_DX_loss[it // test_interval] = t_DX_loss/(test_iter) \n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'DX Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                            it, train_DX_loss[it], np.sum(train_DX_loss)/it, test_DX_loss[it // test_interval],test_acc[it // test_interval])\n",
    "                    else:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_MMSE_loss[it // test_interval] = t_MMSE_loss/(test_iter)             \n",
    "\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "        if not multi_task:\n",
    "            perf = {'train_loss':[train_loss],'test_loss':[test_loss],'test_acc':[test_acc]}\n",
    "        else:\n",
    "            if multi_label:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_DX_loss],'test_loss':[test_ADAS13_loss,test_DX_loss],'test_acc':[test_acc]}\n",
    "            else:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "                \n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,snap_prefix):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    \n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    #s.solver_type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 100000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 5000\n",
    "    s.snapshot_prefix = snap_prefix\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp6_MC'\n",
    "preproc = 'no_preproc'\n",
    "fid = 1\n",
    "mc=1\n",
    "train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "\n",
    "train_data_CT = load_data(train_filename_hdf, 'X_CT',preproc)\n",
    "train_data_y = load_data(train_filename_hdf, 'y','no_preproc')\n",
    "test_data_CT = load_data(test_filename_hdf, 'X_CT',preproc)\n",
    "test_data_y = load_data(test_filename_hdf, 'y','no_preproc')\n",
    "\n",
    "print train_data_CT.shape, train_data_y.shape, test_data_CT.shape, test_data_y.shape\n",
    "np.mean(train_data_y), np.std(train_data_y), np.mean(test_data_y), np.std(test_data_y)\n",
    "\n",
    "ct_corr_train = []\n",
    "ct_corr_test = []\n",
    "for c in np.arange(0,train_data_CT.shape[1],1):\n",
    "    ct_corr_train.append(stats.pearsonr(train_data_CT[:,c],train_data_y)[0])\n",
    "    ct_corr_test.append(stats.pearsonr(test_data_CT[:,c],test_data_y)[0])\n",
    "    \n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(np.mean(train_data_CT, axis=0)-np.mean(test_data_CT, axis=0))\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.array(ct_corr_train) - np.array(ct_corr_test),label='train-test')\n",
    "#plt.plot(ct_corr_test,label='test')\n",
    "plt.legend()\n",
    "\n",
    "mean_diff = np.mean(train_data_CT, axis=0)-np.mean(test_data_CT, axis=0)\n",
    "\n",
    "print mean_diff[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "150*4*100*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MC # 1, Hype # hyp1, Fold # 2\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (358.895355225,inf), test loss: 190.857524872\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (335.058837891,inf), test loss: 389.517437744\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (24.9087257385,54.6119755564), test loss: 34.8058885574\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.99193000793,16.4948161739), test loss: 2.97678962052\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (49.9126815796,49.8709838996), test loss: 36.8606753349\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.6129565239,9.88081004399), test loss: 3.21727130711\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (26.1999435425,48.2062459249), test loss: 36.9140093803\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.728871583939,7.66683292917), test loss: 3.17323867083\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (11.7288608551,47.3710460486), test loss: 35.8171908855\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.14855241776,6.55836602007), test loss: 3.14943475723\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (67.2356185913,46.8257416199), test loss: 34.3887831211\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.12207508087,5.89354793427), test loss: 2.73584492505\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (26.1926593781,46.4229042894), test loss: 38.3205067158\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (6.31652164459,5.45243467391), test loss: 3.22809038162\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (41.4855957031,46.1046674448), test loss: 36.6946498871\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.14329004288,5.13241583619), test loss: 3.12344787121\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (16.6605072021,45.8169155408), test loss: 35.5103680611\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.2402472496,4.8929115393), test loss: 3.07755871415\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (32.7499389648,45.5937651595), test loss: 34.8036381245\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.23802220821,4.7052472109), test loss: 2.80376452804\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (68.2498931885,45.3647008451), test loss: 36.1112677097\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.5680835247,4.55496763293), test loss: 2.98200700879\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (20.8535804749,45.1576920873), test loss: 35.7826497078\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.31463718414,4.43054497537), test loss: 3.09484075904\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (52.9579849243,44.9483253555), test loss: 35.1621869087\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.856786489487,4.3266104876), test loss: 3.07223750353\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (99.2070465088,44.7483132298), test loss: 32.6694435596\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.90770483017,4.23706733615), test loss: 2.72078771591\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (36.8745422363,44.5271513914), test loss: 34.9607013226\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.90806627274,4.16255734183), test loss: 3.03589137197\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (30.3044662476,44.3011890763), test loss: 34.7998413563\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.6600291729,4.09619150753), test loss: 3.05181652904\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (43.7147598267,44.0564711862), test loss: 36.3745724201\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.80544829369,4.03783659464), test loss: 2.98609479666\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (60.4607620239,43.8106596185), test loss: 31.8959483862\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.60595798492,3.98577390561), test loss: 2.81565267444\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (28.7893333435,43.5542533562), test loss: 34.0325842381\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.78820085526,3.93863778349), test loss: 2.87159668803\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (51.2879562378,43.299393607), test loss: 33.7673856258\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.94118010998,3.89652282072), test loss: 3.03041109443\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (49.8383140564,43.0451801988), test loss: 36.7908232212\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.38036584854,3.85687259921), test loss: 3.15644906759\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (45.9821357727,42.8000576647), test loss: 30.650751996\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.74794459343,3.82061057404), test loss: 2.75781809688\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (98.5962982178,42.5617239158), test loss: 34.2543830872\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.81259822845,3.78722095173), test loss: 2.751819098\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (73.0098114014,42.3230659848), test loss: 33.7371708155\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (7.1468539238,3.75678905624), test loss: 3.10050976872\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (16.6724662781,42.0878687729), test loss: 36.9457266331\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.09730625153,3.72712011912), test loss: 3.07620257735\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (17.5848999023,41.8608011653), test loss: 30.8657356739\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.92720210552,3.69964238906), test loss: 2.79450542927\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (26.5485572815,41.6496974434), test loss: 44.2916920662\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (4.13270187378,3.67366582215), test loss: 2.67622568905\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (32.4051132202,41.4357301315), test loss: 36.1349327087\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (4.01851320267,3.64902140103), test loss: 3.00072616637\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (15.1645469666,41.2315670772), test loss: 35.45315485\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.53943586349,3.62493382667), test loss: 3.01507106423\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (64.4966430664,41.0394127856), test loss: 38.0143042088\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.964266061783,3.6021211019), test loss: 2.86511270404\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (62.7313194275,40.8472157056), test loss: 35.4567566156\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.52583837509,3.58013107561), test loss: 2.68441242576\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (26.7066745758,40.6622766615), test loss: 33.0752436638\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.53819227219,3.55996345878), test loss: 3.03770446777\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (24.3454685211,40.4840319616), test loss: 39.5651748657\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.45261263847,3.54006780751), test loss: 3.0283113271\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (24.4666748047,40.3057810466), test loss: 34.9664475918\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.44342255592,3.52098616297), test loss: 2.7677408874\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (44.9473838806,40.1340524487), test loss: 32.091610384\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.800303936,3.50254640667), test loss: 2.89450931251\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (12.0663108826,39.9665586802), test loss: 31.9158574581\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.13789272308,3.48470015114), test loss: 3.06111908257\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (69.7272033691,39.8029765675), test loss: 34.5300785542\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.77888202667,3.46758313396), test loss: 2.94025770426\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (25.309928894,39.6430571317), test loss: 31.0723293781\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.11033725739,3.45052412888), test loss: 2.8215903163\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (54.6609535217,39.491791717), test loss: 31.5881046772\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.77807426453,3.43429689733), test loss: 2.62163535058\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (35.2426147461,39.3424095175), test loss: 31.9633374214\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.22775554657,3.4187056703), test loss: 2.80872539282\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (59.7941360474,39.1958680259), test loss: 33.1422448635\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.1098575592,3.40399845745), test loss: 2.94073469937\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (22.0179157257,39.0524849815), test loss: 31.6088171005\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.85173392296,3.38911416166), test loss: 2.7267836988\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (25.3281097412,38.9105594079), test loss: 30.3931083202\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.23010158539,3.37477015191), test loss: 2.62858370245\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (30.8165607452,38.7737484223), test loss: 31.794596386\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.82693552971,3.36095497388), test loss: 2.7658216238\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (27.4130535126,38.6396109839), test loss: 36.0886909008\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.88134670258,3.34751558781), test loss: 2.94608971775\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (26.6481170654,38.5070905297), test loss: 31.8503148079\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.879956007,3.33409776675), test loss: 2.82181140184\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (39.4695129395,38.3827022635), test loss: 31.154060626\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.837973952293,3.32115465991), test loss: 2.74035178721\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (15.5567169189,38.2584889733), test loss: 35.5626153946\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.45891833305,3.30853832988), test loss: 2.71787063479\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (33.3196105957,38.1369962306), test loss: 34.8902990818\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.38528943062,3.2966508039), test loss: 2.92174054384\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (26.4279460907,38.0183517608), test loss: 34.0435567856\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.30185031891,3.28484040126), test loss: 2.72498472333\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (10.9368772507,37.9033088704), test loss: 35.607481575\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.761364877224,3.27323561205), test loss: 2.76757509112\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (26.6831607819,37.788162161), test loss: 31.1466647625\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.5291364193,3.2619937583), test loss: 3.02577317655\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (3.78435397148,37.6723238368), test loss: 31.2823055267\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.57006931305,3.25090556404), test loss: 3.30047551394\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (32.1306800842,37.5650547683), test loss: 34.2858695984\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.6490316391,3.24017969155), test loss: 3.00993713439\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (29.2605648041,37.4555575669), test loss: 31.3365359783\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.01327490807,3.22948769077), test loss: 2.59253283143\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (12.2665958405,37.3518665067), test loss: 30.4016956806\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.6731543541,3.21917023151), test loss: 2.77861604095\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (33.4951171875,37.2529336273), test loss: 31.7901269913\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.74573302269,3.20914785229), test loss: 3.01980709434\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (49.2300682068,37.1521721044), test loss: 34.8033879757\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (5.02238368988,3.19970109978), test loss: 2.81081628799\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (49.9445037842,37.0521755214), test loss: 31.3003074169\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.29256534576,3.19006237923), test loss: 2.57640010417\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (34.881275177,36.9570592197), test loss: 33.0420170784\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.78251862526,3.18068256088), test loss: 2.82522287965\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (36.5660667419,36.8608107031), test loss: 30.781687808\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.14823675156,3.17164047561), test loss: 3.02143902779\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (33.1802749634,36.7670958319), test loss: 39.9053762436\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.18586874008,3.16282637032), test loss: 2.80509265065\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (17.0375747681,36.6743563745), test loss: 32.2635438919\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.4809422493,3.15396899084), test loss: 2.6470893383\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (37.3004837036,36.5880155629), test loss: 31.3940280437\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.16932296753,3.14533635715), test loss: 2.73561981022\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (9.54634284973,36.4987435115), test loss: 38.5514249802\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (4.62828826904,3.13698792308), test loss: 2.99398997426\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (38.8819122314,36.4161574407), test loss: 33.464632082\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.43520283699,3.12897094726), test loss: 2.79964864552\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (30.8940486908,36.3319943586), test loss: 31.0379988194\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (4.25070858002,3.12110245919), test loss: 2.58453276753\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (20.0674686432,36.2473912094), test loss: 39.7322576523\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.868583917618,3.11317600425), test loss: 2.93646274805\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (33.8990631104,36.168664513), test loss: 30.6958312511\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.86114883423,3.10561734717), test loss: 3.23775700629\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (19.7456703186,36.0855186852), test loss: 33.0904967308\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.29436945915,3.09805251874), test loss: 2.90130954683\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (20.24477005,36.0082957488), test loss: 30.2034198761\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.48709821701,3.09074892325), test loss: 2.6142772615\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (47.4786682129,35.9303958676), test loss: 36.1353302956\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (5.26059389114,3.08343136126), test loss: 2.76834377944\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (21.9470291138,35.8555008496), test loss: 30.2055278778\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.2897977829,3.07628109172), test loss: 2.80065850616\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (34.6591453552,35.7829230191), test loss: 37.0948994637\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (6.78274059296,3.06952596916), test loss: 3.31192403436\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (44.5917015076,35.7120003103), test loss: 30.7966576099\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.85447359085,3.06285257498), test loss: 2.54042676687\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (43.7999725342,35.6385841196), test loss: 31.3740784168\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (4.70871162415,3.05615465251), test loss: 2.64584829509\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (18.9849224091,35.56691701), test loss: 30.5669977427\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.35823392868,3.04958005476), test loss: 3.06025457382\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (20.3207473755,35.4980540302), test loss: 35.2726503849\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.750172138214,3.04320575158), test loss: 2.96218879819\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (25.5226554871,35.428572427), test loss: 30.0324074745\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.08032917976,3.03698870023), test loss: 2.66467843652\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (36.904083252,35.3610269417), test loss: 31.430711174\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.02662467957,3.03078760847), test loss: 2.64430006146\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (13.8834199905,35.2963213585), test loss: 29.3241917133\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.28628349304,3.02461089253), test loss: 2.88759278059\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (15.970582962,35.2302278188), test loss: 34.3548940659\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.65237855911,3.01872701451), test loss: 2.88659478724\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (28.4377593994,35.1669361704), test loss: 30.8788917065\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.83397197723,3.01299490567), test loss: 2.71439258754\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (16.7791500092,35.1052995593), test loss: 30.9083497524\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.58034193516,3.00736932348), test loss: 2.60493245125\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (28.8462257385,35.0418249424), test loss: 30.0465424538\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.4735673666,3.00162549372), test loss: 2.94509271681\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (13.6548461914,34.980362812), test loss: 33.3784156322\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.04798603058,2.99625431707), test loss: 3.02494503856\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (25.3584423065,34.9176384744), test loss: 31.0484978199\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.2832865715,2.99076525904), test loss: 2.69002218246\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (20.4143562317,34.8586653304), test loss: 31.8342814922\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.14188766479,2.98544674338), test loss: 2.74535731673\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (40.6957321167,34.7991485019), test loss: 34.3810782909\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (3.77750110626,2.9801802833), test loss: 3.01253990829\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (31.3738918304,34.7417045988), test loss: 33.99509058\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.1671911478,2.97490965755), test loss: 2.94870012701\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (19.130569458,34.6861423685), test loss: 31.130242157\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (5.04892301559,2.97003253752), test loss: 3.15193409324\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (27.0604057312,34.6294875339), test loss: 31.6410729408\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.24934923649,2.9650669836), test loss: 2.59352857172\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (40.0059738159,34.5749230865), test loss: 36.5972120762\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (4.36355733871,2.96020958878), test loss: 2.86454858184\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (33.9186248779,34.5193037953), test loss: 42.563556385\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.62356591225,2.95526209941), test loss: 3.52665985823\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (14.0463600159,34.4647933176), test loss: 30.4297924042\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.767611265182,2.95057578649), test loss: 2.88697240651\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (31.3232307434,34.4106080841), test loss: 32.6133746147\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.18903279305,2.94594207986), test loss: 2.61234832108\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (33.9160003662,34.357740978), test loss: 30.34443717\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.18285143375,2.94127393008), test loss: 2.84727555513\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (12.888671875,34.3066427531), test loss: 34.2785061836\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.11614727974,2.93670725812), test loss: 2.96955461502\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (67.5414428711,34.2563690435), test loss: 33.4249010086\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.13979792595,2.93223954172), test loss: 2.70987097025\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (19.728805542,34.2060433157), test loss: 31.4454943657\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.52189826965,2.92790247383), test loss: 2.69147021174\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (20.6071968079,34.1559836226), test loss: 29.0462264061\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.574908971786,2.92363058847), test loss: 2.70237272978\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (25.9124107361,34.1071372717), test loss: 31.9481462002\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (3.33873748779,2.91936715621), test loss: 2.82547600269\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (10.5103435516,34.0589824646), test loss: 30.1794141769\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.463196456432,2.91514768362), test loss: 2.86074640751\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (51.0755348206,34.0099034946), test loss: 32.4733861446\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.51474809647,2.9110734159), test loss: 2.66159497201\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (32.5481758118,33.9623648498), test loss: 29.4666697502\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.9125007391,2.90692933699), test loss: 2.70182575583\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (56.5574188232,33.9154091705), test loss: 33.8490833282\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.99997496605,2.90286307601), test loss: 2.86571649015\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (24.4253234863,33.8690581174), test loss: 32.1946397781\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.81325674057,2.89885794042), test loss: 2.73354716003\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (21.8690509796,33.825136449), test loss: 32.4125306606\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.45641613007,2.89507885497), test loss: 3.07272467017\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (24.2364807129,33.7801210458), test loss: 29.6666004658\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.1146146059,2.8911814611), test loss: 2.68468693197\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (44.1485671997,33.7353631603), test loss: 46.5623429298\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.89276087284,2.88744103906), test loss: 2.88895912766\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (53.1172409058,33.6909118539), test loss: 37.6496815205\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (4.17685365677,2.88364715256), test loss: 2.97885832191\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (19.9098472595,33.6473152985), test loss: 30.5792901516\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.02682089806,2.87991331699), test loss: 2.85707161129\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (23.4686946869,33.6031603706), test loss: 29.6873680592\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.01875758171,2.87632626488), test loss: 2.70761322081\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (23.1219902039,33.5603367255), test loss: 34.8862292767\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.973001480103,2.87265795183), test loss: 2.95354747474\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (5.18545484543,33.5186180115), test loss: 32.301860714\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.68583774567,2.86907388921), test loss: 2.76018471122\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (68.9110717773,33.4777000584), test loss: 30.6362967014\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.43806147575,2.8655868126), test loss: 2.70783869028\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (42.6492462158,33.4370719782), test loss: 29.4550782204\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (4.16930866241,2.86221137326), test loss: 3.2262401402\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (25.7960338593,33.3962905353), test loss: 30.8310238838\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.913759529591,2.85874126242), test loss: 2.87833893299\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (18.4014129639,33.3558271), test loss: 31.8026414871\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (3.71182584763,2.85540794302), test loss: 2.68671768308\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (24.085647583,33.3157511708), test loss: 33.9538961411\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.04554462433,2.85209727234), test loss: 2.66964899898\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (16.9691162109,33.2756756419), test loss: 34.0256906509\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.10299301147,2.84881267483), test loss: 2.85984349549\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (15.9622459412,33.2361724123), test loss: 30.8657755375\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.31351304054,2.84554662388), test loss: 2.9339899838\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (32.9463844299,33.197858779), test loss: 33.3018414497\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.6839196682,2.8422955784), test loss: 2.90723044574\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (49.5552406311,33.1598843623), test loss: 34.2448001862\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.49605751038,2.83907558224), test loss: 2.63096880615\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (18.8662528992,33.1220628745), test loss: 31.4045394421\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.89258933067,2.83604946087), test loss: 3.05225995779\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (16.3356761932,33.0849398715), test loss: 31.3971871376\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.12663888931,2.83295196734), test loss: 2.9338465333\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (27.5530643463,33.04872191), test loss: 37.4545366764\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.54276442528,2.8299008005), test loss: 2.72839999497\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (43.2327003479,33.0112388392), test loss: 29.9474233627\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.95504879951,2.82684474863), test loss: 3.1489010185\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (17.4097862244,32.9736774175), test loss: 29.6743927956\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (4.2181353569,2.8238778261), test loss: 3.38016566932\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (42.5727577209,32.9377068804), test loss: 30.5763266563\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.24107789993,2.82092623955), test loss: 3.10038098097\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (27.4973106384,32.9010228738), test loss: 32.2496195316\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.958580970764,2.81793298716), test loss: 2.92391274571\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (40.676158905,32.8662363881), test loss: 32.3719591618\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.07430791855,2.81500971719), test loss: 2.70421503484\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (47.8833312988,32.8319463621), test loss: 30.9394846439\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.43529653549,2.81214981186), test loss: 2.96523449421\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (45.7094078064,32.7969171331), test loss: 30.0553274155\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (3.59023666382,2.80940437463), test loss: 3.15179761648\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (24.6506309509,32.7624183538), test loss: 32.0901551247\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.48370695114,2.80660566138), test loss: 2.79352079928\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (19.68491745,32.7287387844), test loss: 30.8667694092\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.36870026588,2.80381529796), test loss: 2.56403132081\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (19.3312911987,32.6941858126), test loss: 35.4455310345\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (3.16142988205,2.80109662315), test loss: 2.7420106709\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (15.6447582245,32.659826905), test loss: 31.5411449909\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (3.04195952415,2.79843258072), test loss: 2.8911300838\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (13.9772167206,32.626389673), test loss: 32.5735100269\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.31445646286,2.7956815351), test loss: 2.84102754891\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (64.5231018066,32.5940064001), test loss: 30.7204185009\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.8950138092,2.79300026293), test loss: 2.59918870032\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (44.7471618652,32.5608343063), test loss: 37.1958971024\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.32956838608,2.7903344344), test loss: 2.7215452075\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (25.4597587585,32.5291788927), test loss: 30.4673867702\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.33642816544,2.78779618718), test loss: 3.02833625078\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (25.7174129486,32.4971416805), test loss: 36.5135763168\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.13703489304,2.785253109), test loss: 2.76719430089\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (11.0744190216,32.4651094639), test loss: 36.5764989853\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.05835700035,2.78272597171), test loss: 2.61786647439\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (26.9205055237,32.4338593841), test loss: 31.5327009678\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.86102485657,2.78019691443), test loss: 2.95246432722\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (10.3246021271,32.4011109592), test loss: 29.5591030598\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (6.02048206329,2.77769565559), test loss: 3.57212506235\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (46.9083709717,32.3699849909), test loss: 38.4439541817\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.99430561066,2.77522463703), test loss: 3.22905573249\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (21.6205387115,32.338243512), test loss: 30.699692297\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.89287638664,2.77270451833), test loss: 2.74048181474\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (27.7680797577,32.3082572118), test loss: 32.0406349659\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (3.15225553513,2.77027033691), test loss: 2.78688549995\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (20.5211296082,32.2781662131), test loss: 29.9357105732\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.20307731628,2.76785683059), test loss: 3.07739116251\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (30.7749652863,32.2487442042), test loss: 33.2421352863\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (4.6932888031,2.76555472038), test loss: 2.97578936219\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (28.8876914978,32.2183373705), test loss: 29.7339111805\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.83589458466,2.76315098488), test loss: 2.6641490072\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (34.8535346985,32.1883060674), test loss: 30.9227336407\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.956699728966,2.76078745341), test loss: 2.61424745619\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (18.6184120178,32.1589672895), test loss: 29.3688897133\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.57196688652,2.75850749112), test loss: 2.96591825187\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (27.5459079742,32.1286156795), test loss: 47.8254752636\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.55966329575,2.756185998), test loss: 3.11865687668\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (18.6885662079,32.099429775), test loss: 31.3527718544\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.15625572205,2.75388750684), test loss: 2.91815381646\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (31.0837879181,32.071231724), test loss: 32.106411171\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.740887641907,2.7515438966), test loss: 2.69861623347\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (17.3077831268,32.042022376), test loss: 37.8931434155\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (4.05344295502,2.74931757289), test loss: 2.84264851809\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (30.174331665,32.0138853614), test loss: 34.8136144161\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.20154178143,2.74710774894), test loss: 3.07750945687\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (27.6575946808,31.9864668583), test loss: 31.4531755924\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (4.10938310623,2.74494904746), test loss: 2.72681045532\n",
      "run time for single CV loop: 466.619174957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:124: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:126: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Deign Net Architecutre and Run Caffe\n",
    "exp_name = 'Exp6_MC'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'CT'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "multi_label = False\n",
    "start_MC=1\n",
    "n_MC=1\n",
    "MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "start_fold = 2\n",
    "n_folds = 2\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "niter = 80000\n",
    "batch_size = 256\n",
    "\n",
    "pretrain = False\n",
    "multimodal_autoencode = False\n",
    "load_pretrained_weights = False\n",
    "HC_snap = 4000\n",
    "CT_snap = 6000\n",
    "\n",
    "if Clinical_Scale in ['BOTH','ADAS13_DX'] :\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "# dr: Dropout rate, lr: learning rate of each modality, tr: loss-weight for each task\n",
    "hype_configs = {\n",
    "                'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},\n",
    "                      'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':5e-5, 'wt_decay':1e-4}},\n",
    "                \n",
    "#                 'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':100,'HC_CT_ff':25,'COMB_ff':25,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},\n",
    "                \n",
    "#                 'hyp3':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':50,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},\n",
    "                \n",
    "#                 'hyp4':{'node_sizes':{'HC_L_ff':100,'HC_R_ff':100,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':25,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},\n",
    "                \n",
    "#                 'hyp5':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':150,'HC_CT_ff':25,'COMB_ff':25,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':10,'HC_CT':2},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}}\n",
    "\n",
    "                }\n",
    "\n",
    "CV_perf_MC = {}\n",
    "for mc in MC_list:\n",
    "    CV_perf_hype = {}\n",
    "    for hype in hype_configs.keys():    \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "        tr = hype_configs[hype]['tr']\n",
    "        solver_configs = hype_configs[hype]['solver_conf']\n",
    "        \n",
    "        CV_perf = {}\n",
    "        start_time = time.time()\n",
    "        for fid in fid_list:            \n",
    "            print ''\n",
    "            print 'MC # {}, Hype # {}, Fold # {}'.format(mc, hype, fid)\n",
    "            snap_prefix = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}'.format(mc,fid,exp_name,hype,modality)\n",
    "\n",
    "            train_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/train_C688.txt'.format(mc,fid)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "\n",
    "            train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "            with open(train_filename_txt, 'w') as f:\n",
    "                    f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "            if pretrain:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_train.prototxt'.format(mc,fid)\n",
    "                with open(train_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(train_filename_txt, batch_size, node_sizes, modality)))            \n",
    "            else:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(train_net_path, 'w') as f:            \n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            if pretrain:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_test.prototxt'.format(mc,fid)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(test_filename_txt, batch_size, node_sizes,modality)))\n",
    "            else:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            # Define Solver\n",
    "            solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "            with open(solver_path, 'w') as f:\n",
    "                f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, snap_prefix)))\n",
    "\n",
    "            ### load the solver and create train and test nets\n",
    "            #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "            #solver = caffe.get_solver(solver_path)\n",
    "            #solver = caffe.NesterovSolver(solver_path)\n",
    "            solver = caffe.NesterovSolver(solver_path)\n",
    "\n",
    "            if load_pretrained_weights:    \n",
    "                if hype in {'hyp1','hyp3','hyp5'}:\n",
    "                    pre_hype = 'hyp1'\n",
    "                else:\n",
    "                    pre_hype = 'hyp2'\n",
    "                #snap_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_HC_CT_iter_10000_concat50.caffemodel'.format(fid)\n",
    "                #snap_path = baseline_dir + 'API/data/fold{}/train_snaps/Exp11_{}_HC_CT_iter_10000.caffemodel'.format(fid,hype)\n",
    "                snap_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_{}_HC_CT_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'.format(mc,fid,pre_hype,HC_snap,CT_snap)\n",
    "                print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "                solver.net.copy_from(snap_path)\n",
    "\n",
    "            #run caffe\n",
    "            results = run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode)\n",
    "            CV_perf[fid] = results\n",
    "            \n",
    "        print('run time for single CV loop: {}'.format(time.time() - start_time))\n",
    "\n",
    "        CV_perf_hype[hype] = CV_perf\n",
    "        \n",
    "CV_perf_MC[mc] = CV_perf_hype\n",
    "\n",
    "# pickleIt(CV_perf_MC, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Time for training \n",
    "#HC_CT fold 9,10, number of iterations: 40k, two hyper-params\n",
    "#start time: 14:47\n",
    "#end time: 15:31\n",
    "#runtime_mean = (13+31)/4\n",
    "#print runtime_mean\n",
    "\n",
    "tx=70 #time for 10k iters\n",
    "itx=4 # num of 10k iters\n",
    "hx=4 #hyp choices\n",
    "fx=10 #k-folds\n",
    "mx=10 #mc-folds\n",
    "\n",
    "num_hrs = (tx*itx*hx*fx*mx)/(3600.0)\n",
    "print num_hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbcAAAJoCAYAAABVztwOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3X24nXV5L/jvnRcRCsHEl4QGiBFsG7UWrdVRLmuEIr6h\n7XQaT9Wm6pyxF9pTp3Z6DHoEPBytVkpPz7RybIUCii9pPa0y2uILJpwyymkVHaaRiDK8C9RaIQmF\nbFi/+WOtJCshCTuQtXeePJ/Pde3LZz/reZ7fvVbWH/rdt/evWmsBAAAAAIAumTPbBQAAAAAAwL4S\nbgMAAAAA0DnCbQAAAAAAOke4DQAAAABA5wi3AQAAAADoHOE2AAAAAACdI9wGAGC7qvqJqrqmqu6u\nqt+sqvOr6l17uX5QVU+ZgbrOqqqPTnqd3az7oqq6ZabXPVBV1fer6gWzXQcAACTCbQAAdvbvk1zR\nWjuytfbHrbXTW2vv3cv1bboPrqoPVtV3RsH5hqr6tX2sbdpr7WcTWbeq3lpVf19V91XVhftw3+er\nalNV3VNVW6vq/tHxPVX1oUdRz+9V1Z8+0vsnrareWVX/OHqf362q39rLtW8a+4zuqaotoz/ErBi9\n/u+r6obRd/GWqnp/VdXMvRsAAPYH4TYAAOOWJfnHfbh+XwLBzUle0Vo7MskbkvxRVf1P+3D/wea2\nJOckuWBfbmqtvby1dkRrbUGSS5N8oLW2YPTzlkkUeoB4MMm/SXJkklcleUdVvWp3F7bWLtz2GY0+\np7cn2dBa+/bokr9M8qzRd/FnkpyY5Dcm/g4AANivhNsAACRJqurLSV6c5E9G3a7HV9WfV9V/HLvm\nd6vq9qq6taremH3oam6tvae1dv3o+H8k+e9Jnr8PJR5SVReParu2qp49qun/qKq/3OW9/Jeq+sPR\n8Veq6n1VdfWoU/evqupx+7BuVdXbq+rOqrqtqt4wOvmcqrpjvOO3qv7nqrpmdHxWVf1FVX1yVPM/\nVNUzxz6Pv26tfTbJD/ehlukW/EtV9a2q+peqWr+tY3n02rtH/4Z3jzqhT6yqV2cYAP/6qNavTWON\nx1bVn4yedXNV/X5VzR29triq/ma0/j9V1Zf2tv503lNr7QOttWvb0IYk/1eGofR0/HqSi8aedUNr\n7e7Rr3OTDJIcP81nAQBwgBBuAwCQJGmtnZxh4PzWUcfrd8dfr6qXZhiAnpzkqUl+YZfXf7Wqvjmd\ntarq0CQ/l33rEj8tyccz7Ny9LMmfjM5/LMmpVbVg9Oy5SV6T5OKxe38tw27xJRl2AP+fY7V8q6r+\nzV7WXZLkiCQ/nuTfZhj+H9la+4ckP0jykrFrX7/Luq9K8qkkC5N8IslfbwuAJ2XUDf/HGQa6i5J8\ndLTunFG4/oYkzxx1Lb8iya2ttc8kOS/JxaN/++l01P/HJM9I8vQkP5tkZYZjbZLkHUmuG61/VJKz\nR7Xtdv3RaydV1e3TfI+VYbD9sN+fqvqJJM/JsMt9/PwbquqeJHdm+H3+yHTWBgDgwCHcBgBgun4l\nyZ+31r7dWvvXjALLbVprn2itnTDNZ/3XJNe01r6wD+v/XWvt8tZayzCwfeZo3TuSXDmqL0leluSf\nWmvjQftHx+p+d5Jf2dZx3Vr7mdbaJ/ey7tYk57TWHmyt/U2G41V+cvTaJRkG56mqRUlOzTCA3+br\nrbW/aq09mGF4/Ngkkx7F8uYkf9xa++aoy/kjSQ7JMIB+YFTDM6pqbmvtxtbaTY9wndcmObO19i+t\ntX9K8p8y+iySTGX4x4Ant9YeaK393ej8HtdvrV3RWvvxaa79/iRbsktgvQerk3yptfb98ZOttYtG\nI0t+KsmfJfmnaa4NAMABQrgNAMB0/XiSW8Z+vyn7NnM7yXBjySRPy7C7el/cMXZ8b5LHVtW2/z57\nSYZd00nyugzD73G71v2YJE+Y5rr/3Fob7LL24aPjjyV55agTfVWSK1trd+1u3VEof2uGn+MkLUvy\nzqr64ejnXzJ8r0tH4zzWJHlvkjur6qNV9cRHuM6SJDeP/X5TkqWj4/cm+X6Sr9RwE9HfTpI9rP+k\nfVm0qn4nyS8meeXojwZ7u7YyDNwv2tM1rbXvJLkhyX/ZlzoAAJh9wm0AAKbr+0mOGft9WfZh5naS\nVNV7MuxuPqW1tnk/1vbXSZ5ZVU9P8so8tKN317q3ZjhS5FFprd2e5KtJfjnDcH3XUH37uqOg9egk\n0xq98SjckmFH9aLRz8LW2uGttb8e1fzR1tqJSZ6S5NAMO66Tffy3zPCPDcvGfl+W4SaZaa3d01r7\n31trT87ws/kPVfX8Pax/znQXrKq3JHlrkpNG3eIP56QMR8p85mGumzeqBwCADhFuAwAwXWuTvKGq\nVlTVYUnO3Jebq+qMJL+a5Bdaaz/azev/X1Wt3pdHbjtord2f5NMZjgS5urV26y7Xvr6qfmpU93uS\n/MWok3p/+GiGs6afkeS/7fLaz1bVL47mbP92kvuSfC0ZzgavqsdmuKHhvKo6ZHwed1UNqurnH0E9\nf5rk31XVz46ec3hVnTbaAHJFVf18VT0myf1J/jXDzRST4ezp5fuwzieSnFVVi0bd1+/MKNwfrbft\nWZsyHEcyeJj196qq3pTkXRn+YeS2adb460nWjr4f48/6t1X1hNHxT2f47/el3dwPAMABTLgNAMC4\nXQPf7b+31v42yX9OckWS7yT58viFVfXaqrp2L89+b4adzN+tqk1VdU9VrRndOz/DzQe/9ihqvTjJ\nT2c4omRXHx29fnuGI0neNlb3/1tVv/oo1v2rDLuW/1tr7b5dXvtMhuNX/iXDcSm/NDZK4z9kOOLk\nHaPX7s0wvE1VHZPkniR7+zx3V0taa/93kt9K8uHRSJLrMvyjQsuwU/oPMpwvfVuSH8twBnmSfDLJ\nj41Gmfzdrs/dzXpnJtmQ4aaO38hwM9IPjl5bkeFIknuSrEvywdba1Xtbv6pOrqrxkS67+k9JHp/k\nmrHvz3nbXqyq71bVL439/mMZji+5aDfPenGSDVW1KcN/v7/I8I8eAAB0SO2/hpW9LDKchfj1JLe0\n1l5VVWcl+d+SbPsvr+8c/Y+lbR09b8qwu+Nt+7jJEAAAHVRVJyZ5S2vtdY/iGcck+XaSJeMjT6rq\nKxluKHnho690j2t/N8mbW2tXjJ07K8lxrbV96Ubfdu/rkjyttfau/VgmAAAcVObN0Dpvy7CjY8HY\nufNaa+eNX1RVKzLciGdFhvMIv1RVT92P/5dRAAAOQK21q5Jc9UjvHzVT/E6ST+7nWd7TWfuXkwzG\ng+1Hq7W268xwAABgFxMfS1JVRyd5eZKP7PrSbi5/dYb/g+SB1tqNSa5P8tzJVggAQJeN5mjfneHm\ngWft5pKJNUqMusL/JMlbJrUGAACwezPRuf2HSX43yZG7nP/Nqvq1JP+Q5Hdaa3cnWZrhbvPb3DY6\nBwAAu9VauzfJEXt5/aQJrv3ivbxmhjMAAEzQRDu3q+oVSe5srX0zO3dqfyjJU1prJyS5I8NNZQAA\nAAAAYFom3bl9YpJXVdXLM9wZ/YiqumSXTXX+LMllo+Pbkhwz9trRo3M7qSozuAEAAAAAOqC1trsR\n1Y9azdRejVX1ogzHj7yqqpa01u4Ynf/tJD/XWnttVT0tyaVJnpfhOJIvJnnIhpJVZY9JOunss8/O\n2WefPdtlwD7xvaWLfG/pKt9dusj3lq7y3aWLfG/poqqaWLg9EzO3d+f3q+qEJIMkNyb5jSRprW2o\nqrVJNiSZSvIWKTYAAAAAALuasXC7tbY+yfrR8eq9XPd7SX5vpuoCAAAAAKB7JrqhJLCzlStXznYJ\nsM98b+ki31u6yneXLvK9pat8d+ki31vY2YzN3N6fzNwGAAAAADjwHYwztwEAAAAAOuXJT35ybrrp\nptku44C0bNmy3HjjjTO6ps5tAAAAAIBpGHUhz3YZB6Q9fTaT7Nw2cxsAAAAAgM4RbgMAAAAA0DnC\nbQAAAAAAOke4DQAAAABATj/99Lz3ve+d7TKmzYaSAAAAAADTcKBvKLl8+fJccMEFOemkk2Z8bRtK\nAgAAAACw3z344IOzXcJ+J9wGAAAAAOi41atX5+abb84rX/nKLFiwIB/84AczZ86cXHjhhVm2bFlO\nPvnkJMmqVaty1FFHZeHChVm5cmU2bNiw/RlvfOMbc+aZZyZJ1q9fn2OOOSbnnXdeFi9enKVLl+ai\niy6ajbe2R8JtAAAAAICOu+SSS3Lsscfmc5/7XO65556sWrUqSXLllVfmuuuuy+WXX54kefnLX57v\nfe97ueuuu/LsZz87r3vd6/b4zDvuuCObNm3K7bffno985CN561vfmrvvvntG3s90CLcBAAAAAPaD\nqv3z82iMz72uqrznPe/JoYcemkMOOSRJ8oY3vCGHHXZY5s+fnzPPPDPf+ta3smnTpt0+6zGPeUze\n/e53Z+7cuXnZy16Www8/PBs3bnx0Be5Hwm0AAAAAgP2gtf3zsz8dffTR248Hg0HWrFmT448/Po97\n3OOyfPnyVFV+8IMf7Pbexz/+8ZkzZ0eEfNhhh2Xz5s37t8BHQbgNAAAAAHAQqN20fY+f+/jHP57L\nLrssV1xxRX70ox/lxhtvTGttp27vLhFuAwAAAAAcBJYsWZIbbrghSXYbWm/atCmHHHJIFi5cmC1b\ntuSMM87YbSDeFcJtAAAAAICDwJo1a3LOOedk0aJF+fSnP/2Q4Hr16tU59thjs3Tp0jzjGc/IC17w\ngn16/oEWhFcXW86rqnWxbgAAAACgu6qqsyM8Jm1Pn83o/ERScZ3bAAAAAAB0jnAbAAAAAIDOEW4D\nAAAAANA5wm0AAAAAADpHuA0AAAAAQOcItwEAAAAA6BzhNgAAAAAAnSPcBgAAAACgc4TbAAAAAAB0\njnAbAAAAAOAgsHz58lxxxRWP6hkXX3xxXvjCF+6niiZLuA0AAAAAQJKktZaqmu0ypkW4DQAAAADQ\ncatXr87NN9+c0047LQsWLMi5556bq6++OieeeGIWLlyYZz3rWVm/fv326y+66KIcd9xxWbBgQY47\n7rh84hOfyHXXXZfTTz89X/3qV3PEEUdk0aJFs/iOHl611ma7hn1WVa2LdQMAAAAA3VVV2VMuWe/Z\nv93O7ax9zz+XL1+eCy+8MC9+8Ytz++2355nPfGYuvfTSnHrqqfnyl7+c17zmNdm4cWMOPfTQHHXU\nUfn617+e448/PnfeeWd++MMfZsWKFbn44otzwQUX5Morr9yntff02YzOT6QVXOc2AAAAAMBBYlvA\n/LGPfSyveMUrcuqppyZJTj755DznOc/J5z//+STJ3Llzc+211+a+++7L4sWLs2LFilmr+ZESbgMA\nAAAAHGRuuummrF27NosWLcqiRYuycOHCXHXVVfn+97+fww47LJ/61Kdy/vnn56ijjsppp52WjRs3\nznbJ+2zebBcAAAAAANB1j2SMyP42vhHkMccck9WrV+fDH/7wbq895ZRTcsopp+T+++/Pu971rrz5\nzW/O+vXrO7OZZKJzGwAAAADgoLBkyZLccMMNSZLXv/71ueyyy/KFL3whg8Eg9913X9avX5/bb789\nd911Vz772c/m3nvvzfz583P44YdnzpxhVLx48eLceuutmZqams23Mi3CbQAAAACAg8CaNWtyzjnn\nZNGiRVm7dm0+85nP5H3ve1+e+MQnZtmyZTn33HMzGAwyGAxy3nnnZenSpXnCE56QK6+8Mueff36S\n5KSTTsrTn/70LFmyJE960pNm+R3tXe1pd88DWVW1LtYNAAAAAHRXVUUuuXt7+mxG5ycy60TnNgAA\nAAAAnSPcBgAAAACgc4TbAAAAAAB0jnAbAAAAAIDOEW4DAAAAANA5wm0AAAAAADpHuA0AAAAAQOcI\ntwEAAAAA6BzhNgAAAAAAOf300/Pe9753tsuYtmqtzXYN+6yqWhfrBgAAAAC6q6pyIOeSy5cvzwUX\nXJCTTjppxtfe02czOl+TWFPnNgAAAADAQe7BBx+c7RL2O+E2AAAAAEDHrV69OjfffHNe+cpXZsGC\nBfngBz+YOXPm5MILL8yyZcty8sknJ0lWrVqVo446KgsXLszKlSuzYcOG7c944xvfmDPPPDNJsn79\n+hxzzDE577zzsnjx4ixdujQXXXTRbLy1PRJuAwAAAAB03CWXXJJjjz02n/vc53LPPfdk1apVSZIr\nr7wy1113XS6//PIkyctf/vJ873vfy1133ZVnP/vZed3rXrfHZ95xxx3ZtGlTbr/99nzkIx/JW9/6\n1tx9990z8n6mQ7gNAAAAALA/VO2fn0dhfO51VeU973lPDj300BxyyCFJkje84Q057LDDMn/+/Jx5\n5pn51re+lU2bNu32WY95zGPy7ne/O3Pnzs3LXvayHH744dm4ceOjqm9/Em4DAAAAAOwPre2fn/3o\n6KOP3n48GAyyZs2aHH/88Xnc4x6X5cuXp6rygx/8YLf3Pv7xj8+cOTsi5MMOOyybN2/er/U9GsJt\nAAAAAICDQO2m63v83Mc//vFcdtllueKKK/KjH/0oN954Y1prO3V7d8mMhNtVNaeqvlFVnx39vrCq\nvlBVG6vq8qo6cuzaM6rq+qr6dlW9ZCbqAwAAAADouiVLluSGG25Ikt2G1ps2bcohhxyShQsXZsuW\nLTnjjDN2G4h3xUx1br8tyYax39ck+VJr7SeTXJHkjCSpqqclWZVkRZKXJflQdfnTBQAAAACYIWvW\nrMk555yTRYsW5dOf/vRDguvVq1fn2GOPzdKlS/OMZzwjL3jBC/bp+QdaVFuTbjmvqqOT/HmS9yZ5\ne2vtVVV1XZIXtdburKolSda11n6qqtYkaa21D4zu/ZskZ7fWrt7lma2rrfIAAAAAQDdVVWdHeEza\nnj6b0fmJpOIz0bn9h0l+N8n4O1vcWrszSVprdyR50uj80iS3jF132+gcAAAAAABsN9Fwu6pekeTO\n1to3k+wtnffnDgAAAAAApm3ehJ9/YpJXVdXLkxya5Iiq+miSO6pq8dhYkrtG19+W5Jix+48enXuI\ns88+e/vxypUrs3Llyv1fPQAAAAAA07Zu3bqsW7duRtaa+Mzt7QtVvSjJ74xmbv9+kn9urX2gqt6R\nZGFrbc1oQ8lLkzwvw3EkX0zy1F0HbJu5DQAAAADMNDO392w2Zm5PunN7T96fZG1VvSnJTUlWJUlr\nbUNVrU2yIclUkrdIsQEAAAAA2NWMdW7vTzq3AQAAAICZpnN7z2ajc3uiG0oCAAAAAMAkCLcBAAAA\nAOgc4TYAAAAAAJ0j3AYAAAAAOAgsX748V1xxxaN6xsUXX5wXvvCF+6miyRJuAwAAAACQJGmtpWoi\n+z/ud8JtAAAAAICOW716dW6++eacdtppWbBgQc4999xcffXVOfHEE7Nw4cI861nPyvr167dff9FF\nF+W4447LggULctxxx+UTn/hErrvuupx++un56le/miOOOCKLFi2axXf08Kq1Nts17LOqal2sGwAA\nAADorqrKnnLJWrduv67VVq7c53uWL1+eCy+8MC9+8Ytz++2355nPfGYuvfTSnHrqqfnyl7+c17zm\nNdm4cWMOPfTQHHXUUfn617+e448/PnfeeWd++MMfZsWKFbn44otzwQUX5Morr9yntff02YzOT6QV\nvLOd22/9zndmuwQAAAAAgAPKtoD5Yx/7WF7xilfk1FNPTZKcfPLJec5znpPPf/7zSZK5c+fm2muv\nzX333ZfFixdnxYoVs1bzI9XZcPu/3333bJcAAAAAAHBAuummm7J27dosWrQoixYtysKFC3PVVVfl\n+9//fg477LB86lOfyvnnn5+jjjoqp512WjZu3DjbJe+zebNdwCP1gLEkAAAAAMAB4pGMEdnfxjeC\nPOaYY7J69ep8+MMf3u21p5xySk455ZTcf//9ede73pU3v/nNWb9+fWc2k0w63Lk9JdwGAAAAANhu\nyZIlueGGG5Ikr3/963PZZZflC1/4QgaDQe67776sX78+t99+e+6666589rOfzb333pv58+fn8MMP\nz5w5w6h48eLFufXWWzM1NTWbb2VauhtuDwazXQIAAAAAwAFjzZo1Oeecc7Jo0aKsXbs2n/nMZ/K+\n970vT3ziE7Ns2bKce+65GQwGGQwGOe+887J06dI84QlPyJVXXpnzzz8/SXLSSSfl6U9/epYsWZIn\nPelJs/yO9q72tLvngayq2tKrrsqtL3jBbJcCAAAAAPREVaWLeepM2NNnMzo/kVkn3e3c9iUCAAAA\nAOgt4TYAAAAAAJ3T2XD7AeE2AAAAAEBvdTbc1rkNAAAAANBfnQ23dW4DAAAAAPRXp8NtO5MCAAAA\nAPTTvNku4NF4oLXMr5rtMgAAAACAHli2bFlKHrlby5Ytm/E1ux9uz3YRAAAAAEAv3HjjjbNdAmM6\nO5YksakkAAAAAEBfCbcBAAAAAOicTofbDwi3AQAAAAB6qdPh9tRgMNslAAAAAAAwC7odbuvcBgAA\nAADopU6H28aSAAAAAAD0U6fDbZ3bAAAAAAD9JNwGAAAAAKBzOh1uG0sCAAAAANBPnQ63pwaD2S4B\nAAAAAIBZ0O1wW+c2AAAAAEAvdTrcNpYEAAAAAKCfOh1u69wGAAAAAOgn4TYAAAAAAJ3T6XDbWBIA\nAAAAgH7qdLg9NRjMdgkAAAAAAMyCbofbOrcBAAAAAHqp0+G2sSQAAAAAAP3U6XBb5zYAAAAAQD8J\ntwEAAAAA6JxOh9vGkgAAAAAA9FOnw+2pwWC2SwAAAAAAYBZ0O9zWuQ0AAAAA0EudDreNJQEAAAAA\n6KdOh9s6twEAAAAA+km4DQAAAABA53Q63DaWBAAAAACgnzodbk8NBrNdAgAAAAAAs6DT4bbObQAA\nAACAfup0uG3mNgAAAABAPwm3AQAAAADonE6H28aSAAAAAAD000TD7ao6pKqurqprquraqjprdP6s\nqrq1qr4x+nnp2D1nVNX1VfXtqnrJ3p6vcxsAAAAAoJ/mTfLhrbX7q+rFrbV7q2pukquq6m9GL5/X\nWjtv/PqqWpFkVZIVSY5O8qWqempru0+xpwaDSZYPAAAAAMABauJjSVpr944OD8kwTN8WVNduLn91\nkk+21h5ord2Y5Pokz93Ts40lAQAAAADop4mH21U1p6quSXJHki+21v5+9NJvVtU3q+ojVXXk6NzS\nJLeM3X7b6NxuGUsCAAAAANBPM9G5PWitPSvDMSPPraqnJflQkqe01k7IMPT+g0fybOE2AAAAAEA/\nTXTm9rjW2j1VtS7JS3eZtf1nSS4bHd+W5Jix144enXuoiy7Kxsc+NmcvWZKVK1dm5cqV+79oAAAA\nAACmbd26dVm3bt2MrFV72Ktx/zy86glJplprd1fVoUkuT/L+JN9ord0xuua3k/xca+21o67uS5M8\nL8NxJF9M8pANJauq5StfycrHPS5fOeGEidUPAAAAAMAjV1Vpre1u/8VHbdKd20clubiq5mQ4AuVT\nrbXPV9UlVXVCkkGSG5P8RpK01jZU1dokG5JMJXnLrsH2uKnBYMLlAwAAAABwIJpo5/akbOvcft4R\nR+RrP/uzs10OAAAAAAC7McnO7YlvKDlJNpQEAAAAAOgn4TYAAAAAAJ3T6XD7AeE2AAAAAEAvdTrc\n1rkNAAAAANBP3Q63B4PZLgEAAAAAgFnQ6XDbWBIAAAAAgH7qdLhtLAkAAAAAQD8JtwEAAAAA6JxO\nh9vGkgAAAAAA9FOnw22d2wAAAAAA/dTtcHswmO0SAAAAAACYBZ0Otx9M0nRvAwAAAAD0TqfD7cTc\nbQAAAACAPup8uG3uNgAAAABA/3Q+3Na5DQAAAADQP50Pt3VuAwAAAAD0T/fD7cFgtksAAAAAAGCG\ndT7cNpYEAAAAAKB/Oh9uG0sCAAAAANA/wm0AAAAAADqn8+G2sSQAAAAAAP3T+XBb5zYAAAAAQP90\nPtzWuQ0AAAAA0D+dD7enBoPZLgEAAAAAgBnW/XBb5zYAAAAAQO90Ptw2lgQAAAAAoH86H27r3AYA\nAAAA6B/hNgAAAAAAndP5cNtYEgAAAACA/ul8uD01GMx2CQAAAAAAzLDuh9s6twEAAAAAeqfz4bax\nJAAAAAAA/dP5cFvnNgAAAABA/wi3AQAAAADonM6H28aSAAAAAAD0T+fD7anBYLZLAAAAAABghnU/\n3Na5DQAAAADQO50Pt40lAQAAAADon86H2zq3AQAAAAD6R7gNAAAAAEDndD7cNpYEAAAAAKB/Oh9u\nTw0Gs10CAAAAAAAzrPvhts5tAAAAAIDe6Xy4bSwJAAAAAED/dD7c1rkNAAAAANA/wm0AAAAAADqn\n8+G2sSQAAAAAAP3T+XB7ajCY7RIAAAAAAJhh3Q+3dW4DAAAAAPRO58NtY0kAAAAAAPqn8+G2zm0A\nAAAAgP4RbgMAAAAA0DmdD7eNJQEAAAAA6J+JhttVdUhVXV1V11TVtVV11uj8wqr6QlVtrKrLq+rI\nsXvOqKrrq+rbVfWSh1tjajCY5FsAAAAAAOAANNFwu7V2f5IXt9aeleSEJC+rqucmWZPkS621n0xy\nRZIzkqSqnpZkVZIVSV6W5ENVVXtbw1gSAAAAAID+mfhYktbavaPDQ5LMS9KSvDrJxaPzFyf5xdHx\nq5J8srX2QGvtxiTXJ3nu3p5vLAkAAAAAQP9MPNyuqjlVdU2SO5J8sbX290kWt9buTJLW2h1JnjS6\nfGmSW8Zuv210bo90bgMAAAAA9M9MdG4PRmNJjk7y3Kp6eobd2ztd9kifr3MbAAAAAKB/5s3UQq21\ne6pqXZKXJrmzqha31u6sqiVJ7hpddluSY8ZuO3p07qEuuihJcse8eVl3771ZuXLlZAoHAAAAAGBa\n1q1bl3Xr1s3IWtUm2PlcVU9IMtVau7uqDk1yeZL3J3lRkh+21j5QVe9IsrC1tma0oeSlSZ6X4TiS\nLyZ5atulyKpq+cpXkiTLDjkkNz7/+RN7DwAAAAAAPDJVldZaTeLZk+7cPirJxVU1J8MRKJ9qrX2+\nqr6WZG1VvSnJTUlWJUlrbUNVrU2yIclUkrfsGmzvylgSAAAAAID+mWjn9qSMd24/af783HniibNc\nEQAAAADBlfL4AAAgAElEQVQAu5pk5/bEN5SctKkOhvMAAAAAADw6nQ+3jSUBAAAAAOifzofbOrcB\nAAAAAPqn++H2YDDbJQAAAAAAMMM6H24/mKSLm2ICAAAAAPDIdTbcnjt2bO42AAAAAEC/dDbcnl87\nSjd3GwAAAACgXzobbs+r2n6scxsAAAAAoF8OinBb5zYAAAAAQL90N9zOWLg9GMxiJQAAAAAAzLTO\nhtvzjSUBAAAAAOitzobbxpIAAAAAAPRXh8PtHaULtwEAAAAA+qW74XaMJQEAAAAA6KvuhtvGkgAA\nAAAA9FZnw+3xDSWnBoNZrAQAAAAAgJnW2XDbWBIAAAAAgP7qbLg911gSAAAAAIDe6my4PX+wI9AW\nbgMAAAAA9Et3w+2prduPjSUBAAAAAOiXzobb4zO3dW4DAAAAAPRLh8PtHaYGg1mrAwAAAACAmdfh\ncHtH57axJAAAAAAA/dLhcHsHY0kAAAAAAPqls+H2fDO3AQAAAAB6q7Ph9tyxY2NJAAAAAAD6pbPh\nts5tAAAAAID+6my4bUNJAAAAAID+6nC4vcPUYDBrdQAAAAAAMPM6HG4bSwIAAAAA0FcdDrd3MJYE\nAAAAAKBfOhxu69wGAAAAAOirzobb88eOhdsAAAAAAP3S2XB7vHPbWBIAAAAAgH7pbLg9d3wsyWAw\ni5UAAAAAADDTOhtuG0sCAAAAANBfnQ23540dG0sCAAAAANAv3Q23t27dfqxzGwAAAACgXzobbs9/\n4MHtx8JtAAAAAIB+6Wy4bSwJAAAAAEB/dTfcHsuzpwaD2SsEAAAAAIAZ19lwe/5Yt7axJAAAAAAA\n/dLZcHu8c9tYEgAAAACAfulsuD3nyIXbj3VuAwAAAAD0S2fD7XlV24+F2wAAAAAA/dLdcDs7wm1j\nSQAAAAAA+qWz4fb88c7twWAWKwEAAAAAYKZ1NtyeO9asbSwJAAAAAEC/dDbcnj+WZxtLAgAAAADQ\nL50Nt+eNHevcBgAAAADol+6G2+Mzt4XbAAAAAAC90t1we6x0Y0kAAAAAAPqlu+H2jsbtTA0Gs1cI\nAAAAAAAzbqLhdlUdXVVXVNU/VtW1VfXvRufPqqpbq+obo5+Xjt1zRlVdX1XfrqqX7OnZ88aatY0l\nAQAAAADol3kPf8mj8kCSt7fWvllVhyf5elV9cfTaea2188YvrqoVSVYlWZHk6CRfqqqntvbQ9Hru\nYMcpY0kAAAAAAPplop3brbU7WmvfHB1vTvLtJEtHL9dubnl1kk+21h5ord2Y5Pokz93ds+fXjtJ1\nbgMAAAAA9MuMzdyuqicnOSHJ1aNTv1lV36yqj1TVkaNzS5PcMnbbbdkRhu9k3lg2LtwGAAAAAOiX\nGQm3RyNJ/jLJ20Yd3B9K8pTW2glJ7kjyB/v6zPF5KsaSAAAAAAD0y6Rnbqeq5mUYbH+0tfaZJGmt\n/dPYJX+W5LLR8W1Jjhl77ejRuYf44wv/KBn8KEmy9YQTkp//+f1cOQAAAAAA+2LdunVZt27djKxV\nu9mrcf8uUHVJkh+01t4+dm5Ja+2O0fFvJ/m51tprq+ppSS5N8rwMx5F8MclDNpSsqnbz1bfl2Hu/\ns/3c4EUvStXuxngDAAAAADAbqiqttYkEtxPt3K6qE5O8Lsm1VXVNkpbknUleW1UnJBkkuTHJbyRJ\na21DVa1NsiHJVJK37Bpsjz07c5M8OPr9gdYyX7gNAAAAANALE+/cnoRh5/bt+Yn7rs99g0GSZMsL\nX5jD5s6d5coAAAAAANhmkp3bM7Kh5KSMd2rbVBIAAAAAoD86HW7PGwu3p4TbAAAAAAC90elwe7xz\ne2o0ngQAAAAAgINfd8PtXTaQNJYEAAAAAKA/uhtux1gSAAAAAIC+6nS4PX/OjvKF2wAAAAAA/dHZ\ncLu1GEsCAAAAANBTnQ23E2NJAAAAAAD6qtPh9njn9tRgMIuVAAAAAAAwk7obbrdmLAkAAAAAQE91\nN9yOsSQAAAAAAH3V6XB7/pwd5Qu3AQAAAAD6o9vhtrEkAAAAAAC91Nlw+2tfM5YEAAAAAKCvOhtu\nr1u3c+f21GAwe8UAAAAAADCjOhtuV5qxJAAAAAAAPdXZcDsxlgQAAAAAoK86HW7Pn7OjfOE2AAAA\nAEB/dDvcNpYEAAAAAKCXOh1uG0sCAAAAANBP0wq3q+ptVbWghi6oqm9U1UsmXdxetZ03lJwaDGax\nGAAAAAAAZtJ0O7ff1Fq7J8lLkixM8mtJ3j+xqqbJWBIAAAAAgH6abri9LUV+eZKPttb+cezcrDGW\nBAAAAACgn6Ybbn+9qr6QYbh9eVUdkWRW54Bcc00yf86O8oXbAAAAAAD9MW+a1/2vSU5IckNr7d6q\nWpTkjZMr6+HdcuvOndvGkgAAAAAA9Md0O7efn2Rja+1HVfX6JP8hyd2TK2t65htLAgAAAADQS9MN\nt89Pcm9V/UyS30nyvSSXTKyqaai0ncPtwaxOSQEAAAAAYAZNN9x+oLXWkrw6yR+31v4kyRGTK2t6\njCUBAAAAAOin6c7c3lRVZyT5tSQvrKo5SeZPrqzpMZYEAAAAAKCfptu5/Zok9yd5U2vtjiRHJ/ng\nxKqapvlzdpQv3AYAAAAA6I9phdujQPvSJEdW1SuT3Ndam9WZ24mxJAAAAAAAfTWtcLuqViX5H0l+\nJcmqJFdX1f8yycKmw1gSAAAAAIB+mu7M7Xcl+bnW2l1JUlVPTPKlJH85qcIeTqXtHG4PBrNVCgAA\nAAAAM2y6M7fnbAu2R/55H+6dGGNJAAAAAAD6abqd239bVZcn+cTo99ck+fxkSpo+Y0kAAAAAAPpp\nWuF2a+13q+qXk5w4OvWnrbW/mlxZ0zN/zo7mcZ3bAAAAAAD9Md3O7bTWPp3k0xOsZZ/N07kNAAAA\nANBLew23q2pTkt2lxpWktdYWTKSqaTKWBAAAAACgn/YabrfWjpipQvZVpe0UbhtLAgAAAADQH3Me\n/pID105jSQaDWawEAAAAAICZ1Olw21gSAAAAAIB+6na4PWdH+caSAAAAAAD0R6fD7Xk6twEAAAAA\neqnT4baxJAAAAAAA/dTZcLvSdgq3jSUBAAAAAOiPzobbyS5jSQaDWawEAAAAAICZ1Olw21gSAAAA\nAIB+6na4PWdH+caSAAAAAAD0R6fD7Xk6twEAAAAAeqnT4baxJAAAAAAA/dTZcLvSdgq3jSUBAAAA\nAOiPzobbyS5jSQaDWawEAAAAAICZdNCE2w8mabq3AQAAAAB6YaLhdlUdXVVXVNU/VtW1VfVbo/ML\nq+oLVbWxqi6vqiPH7jmjqq6vqm9X1Use5vk7BdxGkwAAAAAA9MOkO7cfSPL21trTkzw/yVur6qeS\nrEnypdbaTya5IskZSVJVT0uyKsmKJC9L8qGqsfR6N+bZVBIAAAAAoHcmGm631u5orX1zdLw5ybeT\nHJ3k1UkuHl12cZJfHB2/KsknW2sPtNZuTHJ9kufu7tmVYZA9X7gNAAAAANA7MzZzu6qenOSEJF9L\nsri1dmcyDMCTPGl02dIkt4zddtvo3B4ZSwIAAAAA0D8zEm5X1eFJ/jLJ20Yd3Lum0I84ld6pc3sw\neKSPAQAAAACgQ+ZNeoGqmpdhsP3R1tpnRqfvrKrFrbU7q2pJkrtG529LcszY7UePzj3ED/NHOfvs\nRfnXW25JfvqnkxNOMJYEAAAAAGAWrVu3LuvWrZuRtapNOBCuqkuS/KC19vaxcx9I8sPW2geq6h1J\nFrbW1ow2lLw0yfMyHEfyxSRPbbsUWVXtKfluvteOy5O/+tXcdP/9SZIbnve8LD/00Im+HwAAAAAA\npqeq0lqrh79y3020c7uqTkzyuiTXVtU1GY4feWeSDyRZW1VvSnJTklVJ0lrbUFVrk2xIMpXkLbsG\n27uaP2fHZBWd2wAAAAAA/TDRcLu1dlWSuXt4+Rf2cM/vJfm9h3t2jcZ07zRzW7gNAAAAANALM7Kh\n5CTNGwu3HxBuAwAAAAD0QufD7Z06tweDWawEAAAAAICZcnCF2zq3AQAAAAB6ofPhtrEkAAAAAAD9\n0/lwe/6cHW9B5zYAAAAAQD90Ntyu4/42yc5jSXRuAwAAAAD0Q2fD7TznvybZeSyJzm0AAAAAgH7o\nbrg9/97hf4yH24PBbFUDAAAAAMAMOqjCbWNJAAAAAAD6obvh9rxhuG0sCQAAAABA/3Q33N7WuT1n\nx1sQbgMAAAAA9ENnw+2a82C2PrjVWBIAAAAAgB7qbLidJJu3bjaWBAAAAACghzodbm/ZumWnzu2p\nwWAWqwEAAAAAYKZ0OtzevHWzsSQAAAAAAD3U6XB7y9QWY0kAAAAAAHqo0+H25q2bM3/Ojrcg3AYA\nAAAA6IfOhtuVh87cNpYEAAAAAKAfOhtuJ8PObWNJAAAAAAD6p9Ph9papnTu3pwaDWawGAAAAAICZ\n0ulwe/PWzcaSAAAAAAD0UKfD7S1btxhLAgAAAADQQ50Otzdv3Zz5c3a8BeE2AAAAAEA/dDbcrmYs\nCQAAAABAX3U23E6GG0oaSwIAAAAA0D+dDrd37dyeGgxmsRoAAAAAAGZKp8PtXTu3jSUBAAAAAOiH\nTofbD+ncFm4DAAAAAPRCp8PtLVu3ZP6cHW9BuA0AAAAA0A+dDbcrw85tY0kAAAAAAPqns+F2Mpy5\nbSwJAAAAAED/dDrcfsjM7cFgFqsBAAAAAGCmdDrc3rJ1i7EkAAAAAAA91Olw+/4H709lR7e2sSQA\nAAAAAP3Q2XC7Rjn2gw/cv/2ccBsAAAAAoB86G25vs/WBf91+bCwJAAAAAEA/dD7cnnpwR7itcxsA\nAAAAoB86H25vfeDe7cdTg8FergQAAAAA4GDR+XD7/qkd4baxJAAAAAAA/dD5cHunzm3hNgAAAABA\nL3Q23K7Rf943tWX7OZ3bAAAAAAD90Nlwe5vxsSQ6twEAAAAA+qHz4fa/Tm3efizcBgAAAADoh86H\n2/dv3RFuG0sCAAAAANAPnQ+37x3v3B4MZrESAAAAAABmSufD7fGxJA8mabq3AQAAAAAOep0Nt2uU\nYW/ZujnzqrafN5oEAAAAAODg19lwe5stU1t2CrdtKgkAAAAAcPDrfLi9eevmzBduAwAAAAD0SufD\n7S1bt+wUbhtLAgAAAABw8Ot8uL15l5nbU4PBLFYDAAAAAMBM6Hy4vWVqi7EkAAAAAAA909lwe1uc\nvXnr5syfs+NtGEsCAAAAAHDwm2i4XVUXVNWdVfX/jJ07q6purapvjH5eOvbaGVV1fdX/z959h0lN\nrX8A/waWIqACUuwiIipgQRG7YgHBhgWxYcfey1X8eUW9dryo1y42QFEpogIiRQQEBKUXQUCkSe8s\ny7I1vz/OxpwkJ5M6le/nefbZmUzm5MxMpuQ9b96jLdA0rZ2fbTjKkjC4TURERERERERERJTzkp25\n/SmA8xXLX9N1/fiKvxEAoGnaUQA6AzgKQAcA72qaFLV2YZ9QksFtIiIiIiIiIiIiotyX1OC2rusT\nAWxR3KQKWncE8JWu66W6ri8DsBhAa69tFJYWIk9qjWVJiIiIiIiIiIiIiHJfumpu36tp2ixN0z7S\nNG3vimUHAFgprbOqYplayR7/XKwsLy4vj7GbRERERERERERERJSJ0hHcfhdAY13XjwOwFkDPUK1I\nwe1KMAPaLEtCRERERERERERElPvyUr1BXdc3SFc/BDC04vIqAAdJtx1YsUxp/fQSYJm4vKPBNOCo\nFgBYloSIiIiIiIiIiIgoXcaNG4dx48alZFuanuRgsKZpjQAM1XX96Irr++q6vrbi8kMATtR1/VpN\n05oB6AfgJIhyJKMBHK4rOqhpmn505yaY2+xPAECrDhMwbWcpAGDMscfinDp1kvqYiIiIiIiIiIiI\niMibpmnQdV01B2NkSc3c1jTtCwBtAOyjadoKAE8DOFvTtOMAlEPkXt8BALquz9c0bQCA+QBKANyt\nCmz/o6SGuR29zFzMzG0iIiIiIiIiIiKinJfU4Lau69cqFn+aYP2XALzkq3Gp5jb0UhjTSrIsCRER\nEREREREREVHuS8eEkvGQMrchZ26XlytWJiIiIiIiIiIiIqJckr3B7VIzc1svL/3nMsuSEBERERER\nEREREeW+rA1ua1Lmtl5e8s9lliUhIiIiIiIiIiIiyn1ZG9yWa27LwW1mbhMRERERERERERHlvpwI\nbpfrDG4TERERERERERER7U6yN7gt1dwuKys2FzO4TURERERERERERJTzsje4LdXcloPbJeXl6egN\nEREREREREREREaVQ1ga3tZLq/1wuLy/65zLLkhARERERERERERHlvqwNbsuZ26VlZnCbZUmIiIiI\niIiIiIiIcl/OBbeZuU1ERERERERERESU+7I3uC1NKFlSusu8zOA2ERERERERERERUc7L3uC2JXPb\nDG6zLAkRERERERERERFR7svi4LY5oWRxaaG5uLw8Hb0hIiIiIiIiIiIiohTK2uC2ViKXJTGD28zc\nJiIiIiIiIiIiIsp9WRvclmtuF5fu/Ocya24TERERERERERER5b7sDW6XyMFtqSwJg9tERERERERE\nREREOS97g9uojOp5FXW39dJ/lrIsCREREREREREREVHuy+LgNlCrai1xQS/7Zxkzt4mIiIiIiIiI\niIhyX1YHt2tWqSkulJuZ2yXl5WnqDRERERERERERERGlStYGtzXoUuY2y5IQERERERERERER7U6y\nNrgNADWrVmRusywJERERERERERER0W4lq4PbrLlNREREREREREREtHvKkeA2y5IQERERERERERER\n7U6yOrj9z4SSzNwmIiIiIiIiIiIi2q1kdXBblbldUl6ept4QERERERERERERUapkbXBbg67M3GZZ\nEiIiIiIiIiIiIqLcl7XBbUDK3C6XMrcZ3CYiIiIiIiIiIiLKeVkd3K5ZlTW3iYiIiIiIiIiIiHZH\nWR3cNmtuhytLouvij4iIiIiIiIiIiIiyS1YHt82a2+HKkvTuDVTK6meAiIiIiIiIiIiIaPeU1aFd\nM3NbCm6Xl/u+/7x5cfeIiIiIiIiIiIiIiFIha4PbGnRlzW23siRFRUCAuDcRERERERERERERZbCs\nDW7XxlYzc7vcuyxJ9epAt27J6YuuAx9+6Fy+Zg1QWJicbRIRERERERERERHtzrI2uH0rPpZqbpuZ\n24lqbv/+e3L6smsXcPvtzuX77w/cc0/09gsLgc2bo7dDRERERERERERElCuyNritQ5NqbnuXJQni\njDOAbdsiNwMAWLcuehs33wzss0/0doiIiIiIiIiIiIhyRY4Et61lSZ58MlrbEycCf/2lvq1PH+CW\nW6K1n4gqNr98efK2R0RERERERERERJSNsjq4Xb2yUZZECm6Xl+PFF13uEz2pG++/D3z6afR23FSq\nBAwalLz2iYiIiIiIiIiIiHJBXro7EFZ/XIUtvZ01t4OUJfFadf16oFYtoEaNMD0Mb+HC1G6PiIiI\niIiIiIiIKNtkbeb2ZtTF5o1VgNKqluC2uBQ8RbtjR6C83LqsYUPg+usjdTOhNWvC31fXgQEDgt1n\n0SJA05zLzzgDmDYtfF+IiIiIiIiIiIiIUi1rg9sadLz0EoDiirrb5WZpEuS5B7eXLQPGjKloQwr0\nDhkClJU51x88OFi/5s8Hdu2yLvvkE6B9e3F51y5gwQJg6lRg//2DtS3btQu46ir320tKgNJS67KV\nK9XrTpwIjB4dvi9EREREREREREREqZa1wW0AKCoCUOIsTYLK6uC2rgN33AGcd174baoyn2XNmwM9\neliX9e8PjBwpLr/0EtCsGbB9u3sbicqlrF/vXNarF/D339ZlZ50FnHlm4r4SERERERERERERZaus\nDm4DMDO3dffM7fr11XeNY4JJANi82Xq9oMB6fedO8/KOHcHbN/o5f74olWJ3xx3AO+9Yl02eLP5S\n6Y8/WC+ciIiIiIiIiIiIUiNrJ5TUjLraxUbmthTcrmwtnr1xY+K2Vqzw3t4zzwB16qjve8ghie87\ncaJ3+3b9+4tA+K23msvy883L9sD8li3B2i8tBfJsr/6mTaLuuNtggGzWLODYY62Z7EcdJdosKQnW\nFyIiIiIiIiIiIqKgcihz27ssiZ0RmF282HvdZ58VAW47e5a2F7/Z4l27ij+/Zs3yXkcORFep4pxE\n8pRTgKZN/W2vZUtgyhTncvuknERERERERERERETJkP3BbVXNbZcJJaOWIdm61Sz1sXmzqJ2tYp9Q\nUiVR7W57P4115eVetb/9WLPGen3VKvEYAVHPfNs2MQHnc8+p719cHL0PRERERERERERERGFkf3Db\nyNwuV9fctgeBR40yL0cJdi9ZAixYoL5NrnVt336ioLRbENnoZ+/evrsX2S23ALVrA336AN27h2+n\nuBj4/PP4+kVEREREREREREQEZHFw21lz21mWZNIkn20pAs5ugeugggTQvYLIf//t3q5xffDg8JM6\nyhNf/vmn9/qqx6brwMyZwPHHi+vjxwPXXy8u33mnqOtNREREREREREREFFXWBrf/8U/NbWfm9umn\nW1cNUsrjuusi9suFERBetizxejt2hGv/iiuAhx82r+/aBUyYAPz+O3Duueq+RDF3rihnIhs3TgS4\n7T74APjlF+82S0uBpUudy/0E3ImIiIiIiIiIiGj3kEPB7eATSkZRWup+m5+g8a23Wq8PHeq+rioo\n7zdQ/+GHwJlnqoPKiWpm+w18H3MM0LGjv3VVJkxwBrLffRdo3Ni57uGHA7/9Fn5bRERERERERERE\nlDvy0t2BsP4pS6KaULJyufI+cWQqG957z996fmpu25e5lRyR10v0WIYPNy8nCsL74SeIXlLiv71y\n20tz5pkiw37CBHPZli3u9y8s9L8tIiIiIiIiIiIiyl05lLmtnlDSjyDlSgxbt/pbzy1QHbeo7QaZ\n+DLRNnUd+OEH9/uUlbnfpvL6686AOBEREREREREREVEOBLeNzG0puB1zWZLXXnO/rVkz57K4A82J\nDBmS+PZRo9xvk/sZpc/2bOrRo8O3Ze/Lww9bM7l1HTjqKOCBB6Jtg4iIiIiIiIiIiLJb9pclUdXc\nztORnx/ftp5+Ovx9N29OfHvQTGY7r3rXI0aEa1dVCsVgX7Z4sXc7frbl1x9/AFWrWu8bJvueiIiI\niIiIiIiIsldSM7c1TftY07R1mqbNkZbV0TRtlKZpCzVNG6lp2t7SbU9omrZY07QFmqa187URo+Z2\nubUsyYwZzlXlIOqiRXI/fT4gid/7TJmS+H6qkhtuwd5E20xFuZMff4yvveuvB+bPV68T5PU45xyg\nnb895R/5+Ykn0yQiIiIiIiIiIqLMl+yyJJ8CON+2rBuAH3VdPwLATwCeAABN05oB6AzgKAAdALyr\naT7CnKrMbR9lSY44QtRz9mPHDn/rGRIFmpMZhL7jjuD3MbjV3Jaz1tu29T+ppVu7hs8/B777zry+\nZAkwcKC13VWr3NvduVP8HzcO+OknEaxW1UFfssS5bK+9gDvv9Ow6ERERERERERERZbCkBrd1XZ8I\nYIttcUcAfSou9wFwacXlSwB8pet6qa7rywAsBtDarW2zLIlRc1sOboebgTBZgee4eIX6e/Vyv+23\n3xLf12viy2RnOnfrBnTubF22erVzPaNff/5pXX7//UCdOuJycbGZMd+kCVBS4mzHXkpl2DD/GeNP\nPQV89pm/dYmIiIiIiIiIiCg50jGhZANd19cBgK7rawE0qFh+AICV0nqrKpYl9k/mtrUsSRBxlyVx\nC5KPHu0/W9zQrx/w66/e2/R6DB995FwWJJg/e7b/dRNtQ7VNXRd1tAFg4kTg2WeDb2fpUvPy558D\np5ziXKdSJfdSKNOnq5fn5QEFBdZlzz8v/oiIiIiIiIiIiCh90hHctouWL12iytxWN+kWzG3TJlIP\nfPvPf/ytJ/ezS5fg9/EraqZ61EkcjdIiADBvnvh/xhnR2gSA0lL1cl0Hfv/dvDxrFvDmm4nbKitT\nlztJ5IADnLXUS0udy1auBIYODdY2ERERERERERERCXlp2OY6TdMa6rq+TtO0fQGsr1i+CsBB0noH\nVixTWoreAMYDBYXAUgBHhs/clrVvH/qu/3ALGk+cGL3tZHGruZ1IlJrbgHf2c9jgeSXFkE3Hjs5l\nL7wADBokSprIatUCNm8GqlYNt/3Vq53Pzd57A7fdBrzxhrmsWzfgiy+c6y5fDhxySLhtExERERER\nERERpdO4ceMwbty4lGwrFcFtreLPMATATQBeAXAjgO+k5f00TXsdohxJEwCulaIb40YsxzmAXgQc\n2gMolzK3IwS3R470t16iwOusWeo6z1FFzZROZNIk8/JzzzlrdHtte+pU9XIjcHvvveb1f/9bvY4f\nbuvK/VMFt4cMSdyufP+CAvHnFdwuLxd/eT7eRTt3AjNneq8HAI0aAevXA/XrO297/XWgb1//bQW1\nejXQoIG/x0RERERERERERGTXpk0btJFKZTwbpgaxT0ktS6Jp2hcAfgHQVNO0FZqm3QzgZQBtNU1b\nCODciuvQdX0+gAEA5gMYDuBuXfcR9iyrCk3P81WWJE5ewd7evZO37Q0bgJo1o7cjP7vDhpmXu3cP\n3tYXXyS+/Z13zMsvvBCs7ZUrvdeRa4KrgtsqgwYF64fdE08A1aurb4ta8sWttMrw4WLwxM5e8gQA\n6tUDPvkk2HYPOMC7VAsREREREREREVEmSGpwW9f1a3Vd31/X9Wq6rh+s6/qnuq5v0XX9PF3Xj9B1\nvZ2u61ul9V/Sdb2JrutH6bo+yt9WNOSV1bJOKFlZEelLseLi8Pf1CoyuWBH8Pskml9uQlZWpl/th\nDCC0bOm97tq14v+6dUDPnt7ruz1fqxSFcBYuBC6/HJgyxbp81izz8W3a5P4ceG3TzZw5QGGh93ob\nNwKVKzuXb9pkzciXrV7tPkCzYYP/PropKUnO2QtERERERERERESGTJhQMhRNmoeytLCmNbgdsixJ\ntXqfqwcAACAASURBVGoBtu+RuT13bqguJBR3KYq771Zn/KqoHq9XsHb5cuDOO8O17WddI7tZfgy9\ne5uTU4axerVz2dq1wDffiD9VPwBgwADgoYfCb1fl2GOBV1/1Xi8/P3jbfrLhDbt2ATt2BGv//POB\nVq2C3SeIU09Vv1ZERERERERERLT7yNrgtkwvquUoS/Lzz+nrDwB88EH8bS5fHm97W7f6ywwO6+uv\nnVnmqoB42JrbBx3kvF1+jjp1ct6uCqQvX544wO4VfN+5UwwUuPUzCvvrk8y66wZ73y+/3Plc5+e7\nl2QBgF9/FZnndn//Hb1/ADB5cvLqjlO81q6N/7OLiIiIiIiIiAjIkeA2ip2Z26q60aNHp65LyXT7\n7c5l6S5LovLII9Hu7xXINUqRuPn6a/fb5OerUSPfXbIw+vfTT+rbzzwTGDFCvU17X9wm5MwECxaI\ngRDZxo1AUVGwdubNUw9IrFzp/tzs2uVeWiUO550HPP+8dVlRkXNC1bA0TTyGZNi4Mb79Jq5BB5Uz\nzwz/HiMiIiIiIiIiSiRrg9tyWRIUOzO3k23hwqRvwtWMGfG19ccf/tYLU5YkHcL2c8EC57Lzzxf/\nCwrMdi65xLmePBmnbMIEYOhQ723PnAm0bq2+LY7nePt2Z3A6Tqo65SrG82h38MHA4MHq2z76CDj9\n9HD98mPMGOcgSK9ewEknxbeNOM6OmDnTWX7mnnvc95sgli1TDzrEZdu2eNr59dfknrmwcWO4Ej+p\n9vbbwO+/p7sX3qLMt0BERERERETkV9YGty1KaqY8uK0KLrz2WvB2VMHL+fODtxPW5ZenbltA9GBt\nsgLqX37pXDaqYkrT1183l/kJVssmT7Zef/RR4LbbrMuSPfHi6acDzZolr/0DDwTGj4/WhlvwPdFz\nk6x9IcpksMly/PFwnI1SWqpeN6g4SxNpmnOfj4uqzE1YqprzDRoAF18cve2CAv+DhmHcdx/Qo0f0\ndnbsEJPOJktenvkZSkRERERERJQsuRHcLq4FlMtlSXzOkhizMGU4Fi92Lhs4MHg7YesP22tiBxFX\nUHbJEvXyVNSX9sNt0s1E/TMCr+vXm6+xrgPvvSeykcMK85wsWQKsWSMuf/SRyJT20/cgtm/3XidZ\n5TkML74IfPKJddnmzc6yI8nQrx/Qpo36triC8JkYdFdZtizdPfB28MHme8Kg68EmWnXzxBPAUUdF\nbyfZOnYE6tVL7jaWLo3exqZN8WX/q7z4Yva8t4iIiIiIiMgpR4Lbzprb2aJFi3T3wB9VMDRMpvqE\nCc5l69cHb8dNMgPicpBy40b/k+Q1bSr+y7WjN24UZRbs7OVuggZG3Up/GMaOjSeA50X1OriVHgna\njpsnnxR/su+/B556SlyOWnJiyxb32wYPjp697iWOIPny5eoSPMmUKYNUdskKaMZV2mTr1uQ+d1EG\nNmVlZfHVqFdp1Ag4+2zrMl13HxQN6skn1YPMRERERERElB2yNrid7prbBPz4Y/D7vPNO/P3wIgcF\nwwSLVFnJmzZFCxI+8IA6OHPkkc5leXmi3506maf5l5SoA0q1aqmD5kG4PUfl5cDq1erb4gi8btrk\nHZyPss299hKlGLzacXv8deumZmDArzD78plnOkvUZGLw+bffgEWLrMtS0c9MmkcgUZA8Wf0sLAQO\nOyzYfYYMibdGvd2OHc733Q8/AE2aJG+bYZx9NvDZZ9Zl27YBv/wSve3i4sx8nxIREREREWWCrA1u\nW9hrbmdR5na26Nkz9dtUHcx//nlyt6mqY+wW0E3ECD7Zs0ONEidB6iUbE7PJEx/+5z8ioKTrzhrH\n9kz4oEERt8BZv37AAQdEayORevWAq64Kfr8gopbS2bkznn6kS7aUXzjpJKBDB3/rjh+f/Lr1cXj0\n0czv58aNwF9/BbtPOvapMNnx48YBF10Ue1cs7dvPTnnmGeC006K3HWdd/PXrnaW2Vq6Md6JqIiIi\nIiKiVMqN4HZxLWtZEmZuJxRmwsp+/eLvRxBGsLR378Tr2evo2nXqZG3P7tRTxf+6dd37EMSGDdbr\nfutOG9ty2+a6deL/kiXAddeJy3KZFPt2AVHmYO5c921ef33iPm3cmPj2OBgZmgMHigGVuLMVk5Xx\nGqSG+W23ASNHWpcNGyYCutu3i3rQftqJi1u78+eHC+b7zYSPS5s24UrexPF8btnif6CqZ09g7dro\n28wmmZQJ//XXokxRKsU16WucGjYE+va1Lrv8cuCEE6K3PX9+ct/vAwcC332XvPaJiIiIiCg7ZW1w\n21qWpCbLkgRwyinp7kFwH3zgb70wQS6/Nm9OTrtRggFl0m4vZ1w2aCD+y8Gl++9XB7eN8jJyVnyH\nDsALL/jvR9xBrMceE5mu6eD1emzebH3eg/roI+ekot9/L0pxrFoVrfSJrgPffut+e5B9rXlz4Nln\n1betXw907+6/T8nmNulrstWt6/95iFPQ57R7990vsB5EJgXh+/YF7r03uduwD37G9f6JYwJRw59/\nOpd17gxcfXX0tktKgp+hEMSaNcFKbBERERERUTRZG9y2KK4FlMsTSqYp0pElVDWkM5EciLvrrvDt\nxBUo+PTTeNqxizMgoGI8j6p60wDQtq2od20oLARGjAAGDBABhmXLwm/TTY8eyQtIjhsH/Otf4e/v\ndZbCPvsAr7xiXZYp9XA3bwYuuyzYfRL13S1z+7vvgOeeC7addMjPB37/PbnbsA9GZMq+IHvuOWfG\nq6qf6e57//7qgaNMCj4n2//+l565KTLN4YcDM2cmp+033gheWz6I/fcXZ+gQEREREVFq5EZwu6Sm\ntSwJa27nhClT4mknUSZrHMIEhOTJ8uLIRJO5Za9/8437faZNMy+/9ZZ5+ayznOsWFwP/93/WZboO\nLF4sAuPHHWdmrRUUiOD9woXAm2+a6z/+uCjpYOenpIXX8/3WW8B//6u+zU+QbPp073VWrRL/hw0D\nJk3yXj8OQQN8K1dGDwra7z9pUupKFAXpu9u6TzwBtGgRT3/iEOQxJdrP0x2ElsXZl6uvDlc2K1Ps\nTkH4++4Tn+PJ5LeMV1Bbt8bTjq677/9G6bAoRowAnn46ejtumjUTg8FERERERNkua4Pb1rIktViW\nJActWBBPO3JWsiHdQYhZs5zL4qrPKtcl95ttvnCh//aXLgW++sq5vGlToEYNYPZsc9lDDwGNGwO/\n/OK/fb/S/RoCwMUXi1PlM9HBBztrDAcNRNoD2XfeCXTpEqwNt20amfuq+vBxyZbSAFu2mAMmfmTC\nvp9O6Qzul5UBN9/sb924+pmJcw+8/bZ1wJLi99JLYvLoqMrLxcS7dgsWAGPHRm9/zBjnPBJx0jR1\nmRoiIiIiIkPWBrctGNymBJIdCFIFHsJs88knzcvbtoVvx86tHElYRUX+11VlZxvWrweOOUZclh/n\n7Nn+S6FcfLE4sE4Wv4/Vvg/ouvfB/iOPBCshEmaiRmM/SiRRO2EyHP1mx1euLC43aOCvHnQqAnxh\n3m9xvEcvvBA48MDo7STbnDniDA2/MikIH2T/0XX3vu/Y4T2xcbbJpDMBvATdp2bPFt81u7MpU8TE\nu8nSvr34i6qoCJg6VX3bkiXR2//hh+SWqfr77+S1TURERESJ5Uhwm2VJctH//pfuHqSWHNB9++3E\n6xrBCDko8eqr/rflJ5gxZ451PeOyfTJEwD3goFpunGp+yinqCS4vuMC5bMMGdZ+HDQMGDQKuukq9\nfb/9NHz8sXl51SqgevVw7axbZx7su637xRfOkjnJCjJddpkoVxOm/c2b4x8gsQcBggyY+LFuHbBx\nY7xtJlPU7PXCQmD58sTrhBkcsTv22GgTEt92m5ioNNPYn5sTTsjcMzJkCxa4D2Bt2yYm5k2lTMww\nP+444IYborcTVCYN7ESZADmV3nsPaN06ee1fcAFw003Ja/+gg8RncVSLFiWvJA8RERFRrsra4DbL\nklAUmXTgGUavXs5lP/zg//72SSwfeMD7PsZzFqR8iqrOtxEA8ZNVbNi8OfHtAwaI/2vXmu2vXm3N\nhjds3y7q+k6caC5btEhMXiaXT4ky8Woq9q9Bg/yv++23wJdfqm9zmzTSsO++wCWXWJcFCWKlIit0\n9Wrr9cMPB04+Ob72i4qADh3ia8+PIM/b//0f0KhRsPbD7qNRXs9x47xraseRUd+okZg0MKyZM4Gf\nfw5+vzje90Ge32bNgLvvVt/2yy/BBjzD8Pt4NS25ZSu8lJRYr7s9x/Xrh6uVHcfr/uqr3gNUxrai\nTJicKvn5wB13BLtP3IOcfqxZEzyRYskSYOhQ9W1xTJR9xBHAiy9al5WWiuc0qu3bgTPOiN4OERER\nUabJ2uC2hWNCyRh+XRL5pMqwiXqwa9Qb92rntNPCte8301mWKKjtVVfZS3Gx+B/keTOynt9/X337\niy+KSRVlug7svbfIHjUO8HRdTJx5/PHqdnTdOslmGI89JrJz4zjwBaylIZYtAy69VL2e1/M5aZL7\n4zaUlPgLuhiiBD9nzjQDUUEyjVetArp1AyZPFtfz80WQKmpgfc4c4MgjRRb4iBHR2oqT/bkxBn8W\nLABq1UptXzJx4svly+OpJZxsq1b5+0xI9Dx6DU5linnz1MunTnX//Eq1jRuBv/5Kz7Yfe0x9VpRd\nebn7hMlhJGsgdtYs9SB8WMnqZ+/ewIMPBrvPgw86B3zjZg9k/+tfwF57RW936VLrwH4UF1/snCti\n7VpgxYrobZeUADNmRG/HTWlpfL/HiIiIKDPkRnC7uBZQLkXemLlNKST/kI8SAJMP3vwGZpJdekEO\nqiQ6xd3rwNNrMqh77jEvyxndEya43yeuAzQv//43cP/94rL8OJ991rluWZkziKPrIitv1CizznRU\n8oH1mDHAd9+Fa8dPreug3PaF4mKRJZfoPscfby0NYygtdWZn2+//yivAu+/676efwGubNsEmW/US\nJEATJpgzZ060STTDBKMT9dPttvHjRZ1xv+IIkqcjCO/V7oEH+pv0N+7AXiadufTtt8E+vzKp78mW\nibXQa9TwPycGxSfMhJqvvy7O6kmWYcOck4WfeipwyCHR2+7bV5SHisP11zvP3mjc2P/EwImUlkY7\ny8/L5s3xTTZPRESU63IkuF3TWpaENbfJw2+/JafdVJcuSDe/wdEgP/7l4Jy99EaiAKcfYYIFquwh\nXQeeeca5fMAA4Oij/bVrZA0VFJiZ+vb+2WtdL1jgzEaP0/ffxxs8mj/fDGh37w7sv7/3fVSnpr/6\nKvDUU/H1C4jvcQZtp2dPs8b22LEiyzgVQax0PV6VwYOB4cOjtxNE0H7ruphQ1X7WQtCJKb14lVxK\nBa/H9O23/gIsmRiMBeLZZ/v1E0GybBTn61JY6Bzwy8TX/bnn1Jm5yR4cyaTBl+efB156KbXbTDSJ\nuJsNG5yZ6nHWHP/8c2ciyMqV8RwHPPOMOBswqoICEXC322cfsS9HtWFDsDPwgho/3jwDk4iIKF2y\nNrhtqbldWh0ok37FsiwJpVmfPvG0k+4DJa+62EZQNohE9S3dTl03dO9uXrZPSmhIdKCtej41LfqE\nW8Y2jRIBqok4u3Sx9sMIFj3xhDgwsPdP04A99zQzmXVd1Ng991x1H4qKgHr1oj2Oiy4SWXmnn25d\nHnYyQnnywKC1bOVtrF/vf90ogtQQVmWhDx2qzsSVA4OPPmoO2pxzDnD77cH7GQev5+z99+PPGPPz\nOqUjCO+27pVXBqtlnuwMcxW3vscZhL/sMmDKlGjthHld0/H957bNjz4SQbKo7STbpk3RJ6hNlziD\n5N27RzuLJd3S/dsvlRo0EJ8x2cg+f01Y69e7t+X2WzeItm2Dz8uhMm+e+F1q16aN+7wuQTz5ZLAz\n8IJ67jlnFj8REeWOrA1uW2lASVXzal6WTA1POUFVQuSuu4K3ozqwS3c9Va8JjM45R/wPciD2+OPh\n+yOXrCgsDN+OTNfNYIAqKO2HMaGlW/t2claS1wF4167W624/zFUZ3bpunXDOeEybN5v1qVUmTbJe\ntwemvV7vDRuiH0C4bWPXLuDEE9Xrhg3Cyxnl48eLLE2vdlQHnM8/r24/TKAiyP7Xt2+wtjduFEGw\nRO66y9/gVbqzzpMZwAqaVR1HQCpMGwMHqifvdRPmNVPdJx0BOLe+67r/xxX3PptJZ0WccIJ1UDEV\n21TJxGzuTLNpU3ylyvzK1NfFXrs7E336abjyMJkgTEa9ysyZzjMKDXHUMH/xReCFF6K389ln6hr1\n3bvHM6/C1VeLsjzJEtfvmm3bdq+BMiKiHAluAyiuZl5mcJtSKK5JaVQ/QNwmS9zdBDkgs5cyAYCD\nDoqnH0bbQU5nHTPGuez77+Ppj+zNN9XL33vPuaxbN1Eb06+dO0UZCVVm/fz5zjrZDRqIWuUqhYXO\nCeSC/PjetAmYNk1cbtjQ//0SkbOwb7/dmmXvJZXBgieeMC+/844zU0rui65bS7nIt7VooQ5uuw1I\n7LmnqBmfKYzHUlhonUDz00+9zzZJpbgy1b3a6dxZHGz7la6DXdUkbukItqXzYF/XgenT/a2rem68\nnq9Vq5KfuR3H83fOOcFKZmRSED6uffbvv/3/fkx3UNot2JYtgTNVP4P2/ZZbUl/mJYxk1gDPFr/8\n4p2cE0X//iKAHtXEic4zgrZvj29i8Nq11cdEcVi/HrjmmnjaYkkbIopL1ga3LWVJAKBIztxmWRLK\nPgMHprsH4V1xReLb3347Nf2QyVnvRpZtmFIF8n2MSSzlOtyqkhlyfUe3LJeggQujH8uWqdfzc5Bs\nZLsbZVhU7TzwgPq+jzziXLZuncgS7NrVnBTO6KdbfXS3bBfVGRD5+YkDNV7lSlJx4O22jfJycz+x\nTzQbNsP85ZfN+997rznRqUpxsTWTXN6mW4kYt0lad+wAfv3V2Y6b664z150yxdze4sXiv/3xyq9j\nkCCOvWzKLbe4n0WRjnIfhrIys/SQ0Ybb5KqZorTUPDMnLrVqAf/6l//1/WSLl5YCP/yQuJ1kfQ4k\natdtP5k2DWjVKjn9CdoXlS1bgmenhnlPjB0rau+HNXJkuEHioPvCkCHe3zOJ2u/RI9pE41EEeV2W\nLnUPMA0d6j/YFlcQ3q2duXPjSyhJpueei17uzhB0n917b+/PREDMKZHOQRO/r+O8eSJIm+kGD/Y/\n547hrrucczmo5p2Jwv5b/J57gCZNgrXx2WfA119bl02eDHz1VbS+GapVc2bUjxvnrxyalzlzxO/l\nqHRdfRz08cfijIaotm6Nr8yRSmFh9gxGEkWRtcFtB0vmdhb88iGipNq6Ndr9mzVzLhs50rlMDrAZ\nZQyC/sBNFa9SFIA4kPViHBDJjz3qpEeqQNqll4os8CAHYN9+K/7bf8Rt324NlOs6cOGFwKJF6naG\nD492Ku+wYaJEQEEBcMkl6j5lgtJSc2JYe8A1bH+/+MK8fMopwB13uK87Y4Y1A1/e5kcfBd+2kX0e\nJCM6SCA7zHMyZoyoSSpzK3mi2mairKY4gvDy82AMgOXnWwecomaY67o4aJ81y3+//Gxz1Cjgggv8\nt+lnm4MGOSegixoE6t9ftBEkQy0V5U7sj+uGG4DDDwcmTAD23Td6+0EEeY4vukj8JRJHvzp2jJat\n+/jjwNNPB79f0L5HLdPWuLH744yj5rMXv4/3mGPcA7fffZf6Sd3d9tnu3YMNisQdZPYz2buf34NB\nBN1nK1dWD/La25k5M/gZWekI2o8c6T1vUCb4+WdgyZJg97nhBuCmm5LSnX/Yf2+ffTZw/vnR2/3s\nM3GmY1TFxcBrrzmXd+0K/N//RW//uuvUk8oGlZ8PPPSQc3mNGtbf5mG9+25y6+IH+dxMpLg4vgFG\nyi45FNyubl7Oy8AIAlEGiutLJNniOnh+/XXnMvnUxTCngcvkWtoGVVkSwDywkNu317qWBXkOjKCz\nHKhOdmDV61R743GqMrHdnuOgB9Vjx5oTUxUUmO2OHy+ymRo0AEaPNtcfPtz6+ixfbl6W15P7l6hW\nuSzO7JtENSKD1oW269ED2G8/kSHz2GPR2nKTaN9LdNB6223WNsrLxaDV8OHu9/n55+D9A8RrLL9f\n4vxRHKWtxYv9TRQbx/u7d29xABRGfr559kYq2R+31/NQXOwd1LnyysQTH3v1QbVszpxw7SRad9q0\n8GeBuG3TKGnwyy/BJwGOKkwm/PffWz8ndlc1aogBiSiiJgSkiup3FiDO2kl2lnwcn7Nr16a+dney\nM+rDSkcJlSCv4c6died5iCrd5YaCSPYxRBzt16sH/Oc/0dtJNlUWdVyfv9OmAW+8ob5t2bLo7d9z\nj/iLasIEoGlT5/KGDf2Xbkukdu14MvaLitTHWYMHx7PPbtuWHWcjZZOsDW47ypLskoLblRncJvLD\nXvs4l6h+NBplHWTJPA0MCHba3q23Bm/fnmEImJNIepW6kbM34q5VbK8HbfwIUGW2hKmHKf9I8yqF\nIg/iuNUCN3gdbKhqlT/6qNi3ghyo1K3rXLZkiTkAI7d12GHqDHNdF/WWDWEOlIwAlt/X374N+bpb\n0AGIVgdY14H77hMTf77+ugjC+u1fEH/8YV62ZxGNHSsGARKR99khQ9zr4LvdR8X+/n7oIfdSR0Ha\nNRQVma9blIm2evUCPvgg+P3CvF7jxwP//W+4bTz6aHbMZdG9u7WUjcqJJ6rLOSWbvG/9/be6Vn9c\n9ea91u3VK9gZHok+v8K24ee20aOdp/WHObsiET/ZunELenCvacmdBNhNHDW349K2rTg7ItMtXpya\nrP1kiCtonI59NYziYvfB5Uw8YzBZNm1yJqBk4sBOjRrOWuupkEn7wrhxZqlCuzgGvQoL/SUVeLn/\nfmCffZzLr7jCfZ6iIGrXjicTfvNm53FWQUG4uILKK68kdx6DOGVtcNuhSEo3YnCbyBe/WajZKMyX\neJgyCOnWrZtz2Ycf+ruvnKk7d27ideUfdkYgNNGPPbdsVa8fiH5PrzbqOgPpzSjRdaBnT+DVV4O1\npyp58vbbwMMPA1OnOn/cuWWCy0Fpo59FRUD16s51H30U+PFH//0EgGOPtV7XdREQ//136/Jdu4A9\n9lC3oeuiNrshzEHCr786M0kTlQ0JG8RyO1134kQRPPI6u2PgQOCbb7wzf8NOoPTGG8Ds2eptA2aG\neyLy/nzOOck5ldqY9NXgVgLG73v30UfN9Z95JljtbnkbK1cmXjfqAaw8mPn779bXKkj7bgMF9ufL\nz8GVapvyvBFRuE3WLPdT0xIPfKWTn/0vyD7rFkS9+mqgUyf//VK9r4MKk8W/caO/wbMo2/QTuPAz\n/0iy7NoFtG8fvR23fgZ9fsM83jieo6ZNgbPOit5Oso0YAaxYke5ehBP1bFHDDz8kP1kp6llCgPi9\n6jYgG9cAVDoCuGG2ac+izsQg/NVXRyvPGFaySrLpevAM+VR8tth/l27ZAixcGKyNU091DprOnw98\n8km0vhm6dQN++sm6bMYMcZZfEIsXJ79qQA4Ft+XM7fR1g4jiF/TDE0hNvdI4/Pabc5nXLOxRs83k\n8huGM87w35ZqUhUv8+c7lxmnzskZbV4lHKL+cFM935mmdWvguOPC318VODWC8G+95b8dXVdnPnTq\nBLRoYV3mFWSz1zsHxH6oqrX+/ffBMzf23tu5bOVKZ+bZokWJ9yF5oqUw+1rnzuKAQCa3YwSeo56G\naJydYc/Weuop90EG2YYNYiDJLcgZJbim6yKrGDDPpoh6kNqzp3qdqJ8HbgcuYQ/kr7nGvHz88cHe\nx6k8Rf2EE8zLK1f6G7CMImwQ3q0/QTPDFyxwLpNt3y4+d8MoLRUDLnGKI6ijuu51Bk39+s4gWZhM\n9aCCDnoVFSXvVO41a9Tzq6iks6zE6tWiFnJYfvoeZFAqrkBk0M/eDh2cv0nT8bp4bfObb9JThsUu\nnfvsSy/FP1G1F7fHO2GC+z67ZYvzTLa4nzc/+3lZWbzbDfoe7d/f3zwpCxYE72ecA5h+53Lp3Ruo\nUydY26lgf7xduwJHHhmsjVWr4j8D287ez7POAk47LVgbTZua81AlS+4Et3dJR3MMbhMlTZhTz6O6\n5Zbg98mWWnaq4IqqlEkcpz8Z4piBPChVUNmY9MQrC1OulX7AAc7b3SZJCVNyRpU5HvcAh5/Tx+Os\n2R0344D3xhu9M2ETkcuAGHRdTBT33nvWZV6Mg0Y5Q/jII53BxUQTKbkFWH74QZSJsFPVPPZSWfH7\nZONG8zk1+r9tm7Xuu538/Mh9nT5dDG6oPv/k906DBsDtt1tvl5+7vn3dt23fZiKq9ysgDhyDZqb4\nEeT1mDULOOQQcdlrctGg2bp2Ucp06Lo4vbSwULzn5HXjyKY7+GDgqqv8t+O1DV33d/pq0PfO2rXW\nSYyDUE0QLVu6VJwxE8aqVcCzz4a7b1A7d4rBvzB+/BE47zzv9aJ8psfxXdm/v3dN1+rVnaWJgu6z\nYd4r8uN75530lBYwjB3rnQgBiDJMqoSGMII8Z6Wl4rvop5/8BTPiKCHgh30fnT07+ASLqna8XH45\n8Omn1mVBns9zzxXvjWR79VX170+/j1fT3AeHon6P+uG3nTPPdA+KdukiygGmm9f33fLlwTLAk5UJ\n71YSMlVatvS3z4YpsxTX81lY6D7xqL2dKGdPyTI1DhLX43OTtcFtR81tuSxJpQx9NYlyQLZMfDBx\nYjztZMqXgxxsUs0wn4v8PvduE8QZpz8masc+UYgx2CBnPx94oPN+Xqf1G+3I7xfjB8zzz6vv43fU\n3ajFq+vWx2ZcVv3Ii2MCGEA8X0ZJkr59RRmVsFQH3C1bOpd5feaoTpscNkwEgjZtUtcrD8JePx4Q\nz/WJJwY7E0A+Fc/YF7p1E9mS9kDzf/8LtGuXuPSKrLRUZHPbJ1Rr1UqclqjrzgMQtx/606YBDzxg\nXTZpkggC2Ad/duzwFwwzJgiaMkUEPo87Tgw+GI/PLZCvacCgQerbpk4FLrzQuizIhHKJ6qka8W7f\n8AAAIABJREFUA2MXXWTNxk404CCTX6O1a53fR0EOBgcMAGrVEgMC/fqp1+nUSbwfje36PfXWWN/t\nM1Q12OcWCNJ18TdsGHDBBert5OWZE7du3+7/94Sui5r3PXpYlyf6bPf6PB00CHjiCfVtRx7p/hp5\nfS+tXRt+wEN1htPo0eZz1qeP2CcNgwb5P8BWzc8RlbztuXPNz2K59m1REfDii+r7GOTnwu9vtyCD\nY36y+P/8UwQg/fJTxk7TnI/H7+8a43vWvv6MGc73gZ382O66y3tbbokAfjLqjc92ez+vvFKcCTVw\noHWyZjf2EmhRqJ5jt/3luOOANm2Ap592nsGYjt//bgODP/0kJrFbuzbYHCZBA5qPPRZ9oEGVuOAl\n6Nlhr7zib91Er6Hb94/q+zMdmdtemjf3HrjNBGvXOgOaqQjCxzFgEtfrPm+eOHMhWduMq58zZzrP\nzDXEsc+mQtYGtx121TQvV9YAe/CbiHYrUSawk02Y4H9dVXZnMoSpgWavkRwHP9mdUcSV5XHTTe63\nuU0wqfrSlk8BNgIFxkFhfr4IphqMGsHy6bKNGzvbHDXKDL7as3pkcr3pVavE/82b1dmG9eu7t+P2\n40QVaHYrqZCoLroqYGo/sDW2eccd7u3I5AMQVSBVnpxT9bo1a+Zv/3/8cfVyOTvO3r4qs6a4WH0g\n3bCh+7bnz/dfh9uthJA9mxsQgQe3CWXGjVOfdq4qz/DEE2J/sGdL3nqryPxVPe/yGSjGflFYCOy1\nl/MAvl07MUEPAFSy/TJ1m4jv5JPNZcZ+feGF4pROedDKqLO4ZYu6n/b64IA4G6RjR5ElKz+Odu2c\n6xp9chvwOv54c0DK2L4qo338eHNfl/tpDJSp6hR26CDeu19/bd1P5TkJZG5ln6ZNE5ls9kksa9Rw\nrnvvveoziZ57DqhaVR0UNsoGlZWZz/eff6r32UTmzBGfl/btX3utc10jK9+uuFjs96qAm9G3hQvN\nz4z//c//ac+ACMLLcxvIg5Dyd7exz953n7lMnpvAcOed7tu68kpzkMY+OZfXgaf9tHujP6Wlzprp\ngPfvjmOOMS8/8og5+fL8+cCTTya+b9DAVhRyMGjCBPPzZeRIUToiLkY/w56h4hZgePVV9++qsBo3\ndmaA+tWqlXr5jBneA59eAx2y8nLgtdf8tyMrKzNP83dr/9tv/Q2suL0uYdj7Mneu+neirEkT63ef\nqp1kkJ/j2bPF92Mi8+ZFH1Bze12Li9XzDdnvs3p1uLKWQfvk1k+vMniGhQud83P4YZzRJUtm0Nie\nDOTXfvuZZ52lk9dr0aePeXzlR7rKn6rKG8V1dtLPPycnXiBLdrnX3AluF9cEyqWjzDwGt4koumTP\nlq7KLhs+PHg7kyYlvt3IVgLCPSb5/gZV1ptXsMLvrNDLlyfOuA0yGWqYx/vOO85lqlOAjRIvJSXq\nSTn9Tu7ppW1b73U6d3Yusx9c2n9AGxMfyoFm1QGJV2DD+IHtNsFcUPKPNzm4nWgAAFD/aCorc06k\n6Sc4EOWAUf6B7OdH5/TpwG23OddLx2Q+icydawbhS0oSl9iRM579GDxY/Le/hqoyTYB71tXHH4sJ\nSA3jx4s6i6rAIWDWBwesGcxDhpiXVWdI2Ml1rOX15DNt7EFD+cB00iQRaHM7ZdOtVmmjRs5+DR8O\n/Oc/zuV5ee7ZyvJArnwfIzPTy/TpIjDqFoxVle6Qy3LJ28zPV7+X+/cHjjjCuVx1doWbiy4SdelV\nE6meeKJ6wMpvWQejz7feqj7jR5UZ/Pbb6nY2b3b/7pLLPBjP27//HWyyrMMOE4Oml11mXf7XX9Y+\nGY9JHkS8+ebEbRuDKKrPr+OP999HXTfPlrCfDfDJJ9bPH+N5sM9DkMgNNzgn+jTaUQU6vb4Tysu9\ns6oNfkthGFmqvXv7W9+P7dvFYJQsaik0XY+nFrr8vi8oMAfwt2wJN99L3FQZ9evXm58nK1YARx+t\nvq9XUMdP8LCgINiEbHI/N26MJ/HHzzHK0Uc7S0r6DbxG2Rfldm67LXGCkq6Lz4BkTKoNJA6Uyv08\n+eT01mOfPNn5+0Vln33Cb8P+WzFMEN44O8zrPh06qNvx+vy+6SZzLig/9wkSpA1yPOG17t57xz9o\n40fQ5/PPP5PXl0SyNrjtKEtSXAvQpZSUygxuE9HuI0hmWTJF/cJN1WlOUUeOU9XPIBM6qXz8sfW6\nn5m/VZnfct1zFVXg360UimoQwE4u/xDnKL+Rqe83AOHFPnu4it/stD//NA/kjdI19oCOX/b2Z84M\nfh+VgQOtz93PP4v/XkEHv6+h10Gm37kHLrjAmcm6Zo31c9JtAic/tm5NnInv19tvWx9T584iG9er\n1qbq+Rw92vqYnn5afd+XX/bul9y+KjOzZ8/g5X6MSRfltnv0sJbZAEQgea+9nBn8KvLjdctWl+m6\n+V6QM+3ldqpUcd7v/vudg367don3vyqYt3KlOrAxbpz6TAGV+vXNcjR2Q4eq99+g82nsu6/IWLXz\nKr8jB1qnTXP//VG3rjPbdt0665lIiSxfLs6WKCgQcx/YjRljXjb2K/uEmH4sWeIcMFedIeP2ObZ0\nqTjzYds258Bp167qQYdzzzUve332dusmBqXkwKZxH3swBnDPFi8vF4Nn48c7g+Wq+6xZY83i8+pn\n9erifV5U5PysVmWGv/pq4vZ69ADefDPxOkCwBAZdF4Pd8pl2bq+r8XjdJuk2dOliJpjMnJm8YGmY\nYJp8n5YtzSDjmDHupWCi/uYy7h82SC0PpG3bFv6sAj8++8x5xkaicnCqs1qSRW6/dm1/JeDCZPD+\n+KP7mWdBS2xFydb36ufTTwM1ayZeBwhWni6owkLrZ7ds7VrxWfTbb+Zv40SM8j1h3m/2QSr7c1dU\n5F77vlkzcdbwihXOAeg49+nDD1cP2DFz26+SmoDOzG0iomzm5wdBHJo0Sc12ooo66Z5qAsNkMMqw\n+CEHJAz2mr9yuZXCQnVASPUD0y3L16D6AS8f5AYllyLyE7Q3hPlxF2UeATkrZ8AA9Tp+++RVy9hP\n8NRg/yEtZ7x16eLvPm5U2flybVmvfSWROnXMgzi3CW39UNU3HTHC+n6Isn9ecUW4+qd29uf8iSec\nGdR+XxdjLgTD999bA+VBMtjkffaLL5x90LTwGa9y2ytWAGefLV4LYxt9+4rPIK8yAppm7deJJ4rg\njbxsxAjn+08VNPeqwd+hg7OMzx9/iEkFvQLKcttDh4ozBewBelXW54knqudKMKgmXfz3v91fZ7kf\nxmOrVUt9e9eu7met7dplXfess9TBcED8HlDVs7dnG/bubQYD5LZ/+UVkiMqZavLt33/vDEq7BWTd\nylOtXStKa9gHVI2JuWWvvqoOhL32GrDnnur2O3YUQX77AJ/fOrGG334TA2LGGSvG4z30UOe6bmdi\nrVwpBkxUz0W/fs5B/xEjrGfaeGnbFvjgA1Fr2844g0hWrVri9lRBXFWQ76GHgs2Zc8ghoh37QOWO\nHer9x61si0EuGTVqlHtt7aDloowzO/3WwZZ/16rKGxrfeZomfgeo9h3Afyk3O3vSR1SaJvZBYwAu\nP997oNHr91Z5uTVIu22bc0A5aiCyqMj795xXP0tLrYOb9et7DwYBwfr+1FPib+pU97KIfp5Po1/G\nAJcf9n6uXm1+Btu3ud9+Ivv73HPF902idgDgqKPU7cQRYO7f370c2IIF4rvwkEOCDQaH6affhJE4\n5U5wu7iWNbhdOUtmvSMion+4jTTvrtzqXvvlJ/sxE7jVhU4kyGSOiZxyinq5KqCeqAyHW3aRKuit\nCjTZM43t3GptB+WWxe337A+vLCrVjPBhfqy7lYLwky0PeAeF33orWH/cqGpJjh+vDniongev0jNG\nxnMYgweb760rr3TeHqSMhRevbHODqi7/EUeYB06qSRWjsJfQ8Lsv2gNo8+aZdaQB/4NNqlOp7a95\nhw7xfVZ36iQGdozH2aKFmFRQFQS191M2dqworSI/Xw0aiIlr3e5j8HqOP/pInFpt1HhftEgEBjTN\n+30pb3PXLuD0051njlSrZv380HUxcH7eeYnbtvd9v/1EYFre5kcfuU/AKs9XYM/sveEGEVwxlm/c\naE60KQ9kVKtmLZUkc5sEWPV8qwJMRikUt4HDJk3E57f82dmvnzNIUlYGPPiguo2RI90zl+X68ol0\n7CgGTFQTcnfpIsoKvfCCNcDbsaM5uCA/H24B0Icfts4xYNzniivMoK9XwKxdO+Dqq9W31a8v5iB4\n8EHrd8Fnn5nter1PVqwQfTFKTMn228/5+e12BtWvv7oH1W+5BWjfHjjtNHNZ797O3zpuz0VpqVnH\n3l4He9QodQkTo/45kLgWs1GeCRD71H33WQcLq1XzHzxbu9YckO3a1Xpb9+7qwYh771X3SbWsQwdx\nxphR6umUU8x+ur2fVcrKxCD8nDnOuuaXXqrevt/fRLouPtMfe0x8B3XsKDLCDWHmjerd27nfxZGZ\nO326+f5+/nnzbMawXnjBfB/NmuVe6jFq3xMdI/hx993mIMnSpepyhYB3oNnrcRi32zOrVZ9J9oQE\nQLwexsBIWVmwJAxmbrtwliWpybIkRESUEsk67TRuUbJ9UylIndR0Ovzw4Pd57DHnsgsvdC6LaxJc\nINyBgGqCPVVW5KBBwdt2y66Ma1Ja1QQ4YQbKVBO0AvFkPwPqA3G/AR9ZmLq2qtdNNbcAEG4w4vvv\ng99HZrxPVMEi1fsljJkz1e8z1eNVBYPk/UxVikmuhW3wW4ZEFnXgRR74U+1zUQaR5azOIBNuq4wa\nZV5WBQX8TrL22mvOgKYcQHOrY69iP/C+6CLrwf1jjwH16lkHCRNNsmwYOdIZyHvySXGmi5H9b2xb\n3mf8DiBomjWQ++yzIntPZnxuJDo7okcP50TbRhDesGmTmK9DHjjyE7B4+21nRusjj1gHTHXd3GcT\nlQ/797+dZyHZzyCYNk0EQL0+z+x9P/ZYZ71Y1bwzo0eL59jtjMOSEvE8yeXgHn/cOVgwZYoINKr6\nKZ+hJvczP1+cQWSn2hdPPtl9QOPTT8W+aS8puN9+zs+uXbtEMEx1doU8l4PczwsvNCeLltkHiHRd\nlPpyK4l09NHq+Qleesk6eOBWNmTBAmfms7zu3XeLslHy/Xr1MvsqP9dff+1eIu2qq5xnJ06b5n/e\nhvffF58HqvfTjh0i69xeXvCpp/y1DYizcSZMEMkSxveA8XjtdfgT+eknEQxX7W+nny4GVOTvnsmT\n/Z8tp+vi7I8XX1TP0aAaxFF9p+m66F9BgXXQ3PgcnD/fOSg/Y0aw4Ou114oBDVWCjlxSzHiOvc7G\nlX+HDxkiBlNV+vYN1k/jM984dg1yX9V8L/I+9+yz7uVikh3IVsna4LaDveY2y5IQEVGSBJnALJ2S\nWXuOMlfUoJPh9NPjaceN3wM+L8ccE087bqfgy6dzR6HKvg8TOK9aVb087GnadqoDd/vEg36oBtfi\nPNhRBYPcaocmojqbQdXP0lJrtqddotvswgwgqIK9qn6uWpW4Pn379s5lYQbX3CaIVgUCwwizz6kY\np3/LFi92z4S286od7JZdZ7dokbrEj0GVvd+nj7ot1esuB5o++8w5UGQE67zk5zuX1a1rZmsb8w3I\nJX/cngP747WfKfXaa9ayE+vWqctl+Hm/GME/e8asXJ7NyDL2cvjh1m02a2YdiAwzyGv4+GNResVo\n/5RTRCBLHjhKlNEss+8zNWpY99eHHxb/7fMD+PkcHjbMWj+7dWvnpIJGrfpE5cgGD3Zur1496/W/\n/hKle4KWwXnqKev3zJgxYvDAPiBjZJm67UcDB5qlzIy+lpSI9VesMLOIZ8wQZ8e4DWi4fQc0amQ9\nG+jjj0UG8fr11j4ZAVFVsgEg3mfGRPCGX34x52cxtrF1q2hX/h4wHteUKeqEAMD5/LRrp95XXn3V\nPRg+bZoYULFn7V9zjTmIbWynsND9jINhw6zfzcZ9XnvN+f5QTdYMiMEht8nEmzcXc2zIpVnGjhVz\nn6jOUFSdnfbll+L5/+QTcV1+rlq3NgdUjL7Lg66yCRPcA99jxpiTZBq++UZ8z/s9c2HvvcXjtE92\nO2+eGGS2l9tSDTK5fWYY/d6wwTlheRxz0wSVO8HtkppAOWtuExERERGFparVqJp40IsqmB/XgAag\n7pNcX94vVSDprrvU64Yp5aIqY2E/yPTj/fedy3r2dC5bvDh4P90GbFXb9KLKVlcFz+0BKD+M0hp2\nRk3rIFTBCtWgQJh+qmrZ7twZ7qwLv/xmusv81rjfsiXcZHFuGahBqSZWVena1d8klGGoMoftVLXO\n7c/B+vUioG8fRJAzW/2e1aTrzoBk9ere9/OqtQyIjEx53zdKrMkBLHttYTeqAaY77jAvG5nl8lkS\nO3aoB2ztj/e886xB0BdfFN9h8tlEqoEyezu//aaeE0Y+q8QoPSSXOHIryWVvXw4or1olzjS46irr\nOkbddLcJoQH1e+GHH0RAvF07cd3INJdLYPktPWd32mlmVrvBSJxJdJaR6rPUPols48bOWvHDhon/\nbjXhAfX7o21b6zwRU6eKwZflyxMPjtlLkA0aZA4AG/dr2VIEwuV2jDr/9sC6sc6mTc65SQBR71v+\nTHj9dTEhcadO6n6ed546Uemgg5xnt/TqJc5qkdsxvhvl95K8/fPPNydxNe7Xrp3zDOU1a8TtH3zg\n7AsgzvZQDXJpmnXC5qRnc+u6nnV/APRL8K1uVrLTddRZouP7z3SMHSv+Diyw3s4//vGPf/zjH//4\nxz/+8Y9//OMf//iXgr+jj46nnRYt1MtPOCGe9n/4IZ52Xn9dvbxz53ja3749nnZ++029vGXLeNof\nO9a57LLLgrej685lAwbouqbF08/nnrNer1JF1xcutC676KLg/ezRQ9dLS53rvfSSc9nKld7td+hg\nvb5jh3ObgwZ5tzNtmvX6//2fru/c6Vxv0yZdb9vWumzRosRtn3aaru+5p/N5mTHDumz9eud9W7QQ\nz71x/Y03rLeffLKul5VZlzVrpuvl5bpep451eZ8+zvZvvNG8/PDD6tfv7be9973bb7deV72v7ff7\n6CP7MujJihNrFcHirKJpmn4JvsUQSJX2a64H+g8HajYS128+EVjmUgCGiIiIiIiIiIgoA519tnpS\nv7jUrx/PnC9nnuleriUOJ57onBelRo1gkxkC4qwp+0Tv3bolLq3j15FHirM35MluW7UCbrrJOkHp\nuHFAmzaJ22rSxFn7v7jYWpZOtU6i5YaHH3ZmzE+aJPYzec6Fv/8Wk5DKbr/du8SVrluzx3v1EnMr\nmWdTaNB1PURxNm+5E9yuUgAM/gqodZi43vUEYMme6ekgERERERERERERURY59FBg6dLg92vb1lq3\n+5BD7CXpkhfczp2a26V7wDqhpM8K60RERERERERERES7uTCBbcA5IWWcc614ydrgtgZbxrleCSiT\nZumoXggiIiIiIiIiIiIiyk1ZG9xWkmegrh6wAA8RERERERERERERZY3cCm7LlUiqMXObiIiIiIiI\niIiIKFdlbXDbUZYEsAa3WZaEiIiIiIiIiIiIKGdlbXBbyZK5vStt3SAiIiIiIiIiIiKi5Mqt4Hap\n9HCqMrhNRERERERERERElKuyNritLEtSqpmXmblNRERERERERERElLOyNritJGduVytKXz+IiIiI\niIiIiIiIKKlyK7hdVtm8XJXBbSIiIiIiIiIiIqJclVvB7RK55nZx+vpBREREREREREREREmVl64N\na5q2DMA2AOUASnRdb61pWh0A/QEcAmAZgM66rm9T3l9Zc1t6OAxuExEREREREREREeWsdGZulwNo\no+t6S13XW1cs6wbgR13XjwDwE4AnArVYIpclKYmnl0RERERERERERESUcdIZ3NYU2+8IoE/F5T4A\nLg3UYkkV83IVZm4TERERERERERER5ap0Brd1AKM1TZuqaVrXimUNdV1fBwC6rq8F0MDtzsqyJCVS\nWZIqpTF2lYiIiIiIiIiIiIgySdpqbgM4Tdf1NZqm1QcwStO0hYAjYq2IYCdgydxmcJuIiIiIiIiI\niIgoV6UtuK3r+pqK/xs0TfsWQGsA6zRNa6jr+jpN0/YFsN7t/vMxAMC8imttxF8xg9tERERERERE\nRESUrXSIas7ZbFzFX/KlJbitaVoNAJV0Xd+haVpNAO0APAtgCICbALwC4EYA37m10RxXYgGutC4s\nrgoxTyUY3CYiIiIiIiIiIqLssfcKoEt7QCsH+g0HtjROd49CalPxZ3g2aVtKV83thgAmapo2E8AU\nAEN1XR8FEdRuW1Gi5FwALwdqtaSqeTmvPK6+EhEREREREcWnSgHQvD9Qe1m6e0JERJnkjBeA+guA\neguBs59Kd2+yQloyt3VdXwrgOMXyzQDOC91wUTUAu8TlvLLQzRARERERERElzaU3A80HAsU1gY9/\nAdYdk+4eERFRulXZCRz9pXm9+UBgVE9gx77p61MWSFfmdmSaaq7JYmZuExERERERUQbbZ5EIWABA\n1QLgqsuA6lvS2yciIkq/owYD1fLN65VLgBM+SF9/skTWBreViqqblxncJkoerQw4+Q3g1P8ClYvS\n3RsiIiIiouzR6n3r9bp/AZd3EfVViYho99XyE+eyVu8DlYtT35cskmPB7Wrm5TydPw6IkuX0V4D2\nDwHt/gVcfj3fa0REREREflTZCRz3qXN50+HAWf9JfX+IiCgz1F4KHDpWXC6vBBTUF5f3XAs0G5S+\nfmWB3Apul1U2L2uVgbzC9PWFKFftsRk47RXzevOBwDlPpq8/RERERJmi0Tjg2D5ApdJ094QyVYsv\ngT22isubDwMm/cu8rc2zQNNh6ekXERGl13G9zctLzgd+vc+83vqtlHcnm2RtcFtZc7tUk1bIA6ru\nSF2HiHYXp/UAqm+3LjvjZeD4D9PTHyIiIqJ008qB8x4HbjobuOwmoP0D6e4RZSQdaP2OeXXqXcCY\nF4El55nLLu8C1F2c+q4REVH6aOXW4PbMW4DptwOlFXMLHjQF2H9qWrqWDbI2uK3kCG4XpK8vRLmo\n1hrgpDfN65sONy9fdBdw2KjU94mIiIgonaoUAJ07Aaf3MJed+B6w76z09Yky04G/AvvNFJdLqgOz\nbgbK84CvvwS2HiyWV98GXHU5E7WIiHYnh/4E1F4hLu+sCyy8GChoCPx+lbnOSczedpNbwe0yObhd\nmT8IiOJ25gtAlYpyP2taAh9MF/8BoFKZOLBrMC99/SMi8qtKAVBzfbp7QUTZbs/VwM1nAUd9Y12u\n6UCH+wHV2aa0+zpRytqeey1QWFdc3lkP6D8YKK2YQ6rhPOCSruD+Q0S0m5AnkpzTBSir+D6QS5M0\n78/jFxdZG9xWliUpkx6OVlkcuBJRPGovBU7oZV4f8wJQvCfwxTBg24FiWbV84LoLRIZ3VtFFDasr\nrwSajEh3Z4go2fZZCNx/OPDI/sDJr6e7NzHQgbO7A4/sB5z+Uro7Q7T72HcWcFtrYP/p5rJZNwJl\neeLyIRPE3CREgAhINB9gXp96t/X2NScAw943r7foD5ySC99RRESUUPUtwFGDzeuzbjYvrz4R+Psk\ncTmv2BqToX9kbXBbSS5LUilLam5r5cBeK8XOTJTJ2jwLVC4Rl5efAfzZXlzO318EuItqiet7rwSu\nvTh7BpcqlQIdbwUuuB9oPgjo0gG47AYxcSYR5Z4qBcBVVwB7rhFnnLR/2JopEbcGc4ETPkhulkXr\nd4CznhMzqZ/3f8DR/ZK3LSISmg4Fbjkd2GuVuF5eGRj6PvBtb+DX+8312v4LqLIzLV2kDNPyExGY\nAESgYs0JznVm3STqcBvaPgY0GpuS7hERpUXtpcBhIwGtLN09SZ8WXwF5ReLympbA2uOst8u/K1q9\nB1QqSV3fskRuBbcdZUkyPLh2yHiR7fHwwUC3usDdLYCL7gCO7QvUWQKehkYZo/584JjPzOtjXgAg\nvd/WHQsMHCgO7ACRwdTpmmhfUHm7gGaDRDb19W2BU/8rTv2NU94uUUql5afW5cd+BtzTDDjq63i3\nR0RppgMX3wE0+N26+OLbgCO/jX9zR34D3HE8cPGd4jv+8OHxb6PRWKD9g9Zll9wGNJwT/7aICIAO\nnPIacE1H81hj197A5z8A0+8Q139+CiioLy7XXgGc+mp6ukqZQysDWklZ2fasbdmIN4CVJ4vLlcqA\nK68SyVBEXvIKGfSi7LLfDODuo4Hr2wNXX7r77r9yPGLmzc7b53cC8vcVl/dabc3yjqJSCXBaD+Ci\nO0V8MotjkJquZ1/nNU3Tr8YX+ArXWG84bgvw+mxxeess4Ol6olaN3V5/A43GAdW2i8xptz+9ErC1\nEbDxSDFxXuke8TyAfRaJUfgjv0u8Xv6+wMrTgBWnAStPBdYfDZTUiKcPlFqVi4ADpwAbmgE766e3\nLw3mAoeNBv48H9jQ3N99Ol8BNKv4AF3cAejnEqA54QMRxDFMeUD8QPdNBw6eKILLzQeICXVk5ZWA\nv84D5lwPLLgMKKkZoG2batuBqzsCh44zl61v7gx6/d4JGP62mMyBiLJbq/eAi6SAwvb9xQ9EQNQ5\n/XwEsKxNPNtqOlRkiFe2/Uif/CDw48tmHb0oai8Dbm8F1NjkvG1TE+DDqcCu2tG3kw57LwdO7Qls\nPAKYdiegV053j4jEQeAF9wKtpFOCtxwqzmDb0My67vEfApfcLi6X7AG8/Qew7eDU9ZUyS9OhwLWX\niMs79wFe+xsore6+/p6rgDtOAGqtE9fXHQ30+UnU5iZSOelN4JwnRR33/oPVZwZQ9qu9FDj3SREX\n+vGlmGILupgn4p9YWJl5uaRG8n6D7bFJfM7VXm4um3Uj8O2nsCTS5boG80SAHwBKqwI9VwOF+zjX\na/OMOJseEDHCTyZG2+4em4DOVwKHSmcHrWkJTHkQmHdVPMcqDhp0XU/Ki5tbwe0WW4ELWDsjAAAg\nAElEQVS3KmYl3zYPeK6KyKDQykRgsen3Imtq39nBN6prZqBb/ttwlP8PlD02AWf9BzjxXaByqbm8\ntKoYla/kkeWqa+IH9IbmIgi3oTmwvoXoR6IfR550oM5SUTew7p/iAL+4lvtf0V7if6Z+4Oy5Gjhw\nMnDQL+KDYuNRwIQn0hOc3HO1CKa0+gCouQHYtRcw4GsRoE0pHWg8RmQ/NxkpFpVWA77pY519V2X/\nqcDtrc3rH0wH1hzvvn7bfwGn/de8PuUBcerljv2A/P3E/6I9Ydl/6i4WAe1jPhf7oh/FNYEFlwOz\nrweWnhPsS7fmeuC6DsD+M8xlkx4FRvcAjhgqRi73lOqG76wLjPgfMOc6ZOx+n+sqlQJHDBEBgj/b\ng68DBXbAb8DNZ5inhM+4VZyFcsvpwD5/imVFewK9xyX+jPPj8OHAVZeZ27Jbcxzw9Zfi+zusKgXA\nraeZv2ny9wW+6QtcfZmZTfpHR3GAq6f4RL2Gc8RA5fxOQEGD4PffZyFw09nm5/D8y4HB/SL+1iGK\naK+VwOXXA43Gm8tWnAp89a36WEArA24/Edhvprg+7ypg0Fep6Stlni7tzd/gEx8DfnzF+z4HTwBu\nPMc8blx7LNBnjDroQbsvrQxo/xBw0lvmsoJ6wKc/i+Ngyh0N5wBdzhdl6AARi+r7oygTGkSVAqDD\nAxWlMAqBSuXu6xbUA356Hph+O2I9/tLKRDnQw0Y7b/P7GZkr2j0CnPqauPz7lcDAAer1aq0BHjrY\n/E7wisskUv93UUrWLfayo6E4w2janeF+y7ticNvCNbh91Dbg3YofkNsXAP1+FAdCTUYANZJYP3fb\nQcDqVuJv1YlilNSY+RoQWbut3wbOfB7YY6v1vrO7AGNeFOsf8Btw8CTgoEnAQZOdWatuyisBWw4D\nNjUFth9QEUTcv+Kv4nJBAxH8yysUman7zgIazq74Pweovj3YYy6pLtosaCj+72hovb6hmfgBFuUD\nMK9QHJCXV3E/MK9UIg7sD/rFDGjXXuFcr7AOMOq/Fad4JDsopgMH/ipGz5sNtA5kAGKSoSEfAbNv\nTHI/IJ6f5gNFUNs4uLL78SVg4uNwfV66nA80GSUuJ/qwNWjlwJWdgWYJSnoU1xD76Y59xfvjgGnq\n9TY3Flna2w8Ejv5CnHGhKT6ztu8PzLtGHDiubuX+WACRDXh9O6DeInPZ6JeBSY+b16tvBdo9Chz/\nsfW+iy4QE/1sP8i9fYrfgZOBi+4yg3jLTweGvwOsOyb+be29AjjjBaD+gorPtn0rBmX2rfiruFzQ\nACjPC95+9a3AAb+Kz/gDJ4tls24C5l0NBuyTaI9NojyI8f2w5jjg41/EGVm1l4ogsRFILagvMiE2\nNQ23rcNGAddcYtbN29xYDGqe/RRwxDBzveIaYtBsxq0I/trrovRTi/7ialkVEZRfeao46+VKadDy\nx5eAid3CPZYwWr0HXHCfGLDP3w/4ckjF57JP9eeLYI6RrWhYfgbw5XfArjrx9teP6lvFgMXqE4BN\nR6R++5R+Lb4CLrzL+jt+zrXAkI8TD7ocPAG45Uzz+ic/AyvOSF4/KTPV/VNMYgyIZKX/LQG2Hurv\nvsd8LuaCMX7/rjlOBLMY4CZAzDF2xTXW3xeG7QeI3zNbG6W8Wzmn6g7gpP+JM3+n3pOes3AOmgRc\ne5EznrT5MDHote0Qf+3UWiOCmvJEyH4suFTEMOL67Dn3CeCMl83ry86yDh6P7AlMfjiebWWySiXA\nIweIREhAlDgz5jZTufw64JgvxOWZNwHffeq+rpumQ4ErrgWqSXMULroQOPQnoEqhdd3SqsDc60TC\n4rpjg2/LgcFtC9fgdtN8MXoBAPmLgBl3qBsoqyLePJsPF0FTt7/KRcA+i4F6f4iD30QjWnabG4uD\nuY1HiVrFdf+y3r7sTGBUT/cDPq1MHOAdPFEEvPefJn4YeWV3uymvJLJKamwM30ZQG48A5l4rsl23\nHObjDroItjf7WtRarrfQvKm8MlBW1flXa63zDZjI0rOBoR+I1z5ulYtEIPmkN4EDpjpv1zVrYHbs\ns8D4pxAoqFFlpyirU1hHHOC7Bdeq5ovA7MmvO4P95ZXEKY21pMnNpncFvn9XDCTIGo0T2XPG/d79\n3V+mYV6huN+Bv/p+aP8orC2yyWdfLwI18vOz10rgmH6iLn39Ber7b24M/N5ZtGEfYKm3QAS29/7b\nfEzD3gdm3KZuq/GPohZvnWXmsuKawOwbxGQ/648O/vi8VCkQGcq11or3z+5cDmWPTcB53YATPnLe\nVl4Z+O1e8T4q2jv6tiqVAq3fAs55yt98DbomgtxbDxE/Jo3/2w42LxfXEvucEcg+aLL7fru4g3gP\nJuMAJG8XUOcv8ZmcttIOujiDqvU7Iit3TheRARJXua9EtHLg2guBw0eI64W1gV7TgS2NzXUazBNZ\n3cYBw9ZDgI8nAfkHBNvWoWPEgUeVXeL6lkZA7/EVB0G6ePztHjUD34AofTS0V7Cg7ekvA+c9YV4f\n8iEwo6t5/fyHgFMqSkKVVwI+GwUsPTfYYwmqUqnIPDn5Tevykj1ERvn8Tt5tNJgL3Hiu+QO/LM86\nQLy+ufjRn8oBxqbDgItvF4MfZXkik2jyQ0jbYFTlIvEbY93R8Xz2ZYM9NovkBb2SOHtw197if9Fe\nQPGe4QYa/aq2TZQhOfZzc1l5JWDcs8DPT8LXftDpanMgak1LoNfU1H0WV9sm9t1tB0Ur5UbRyFl5\niy4UZWyCOLYvcOlNUoC7ZUWAu27Cu/lWuVgMwB7bV2QBzr4BWHhJar6jdweVi8Xv+h0N4z3Nf8//\nZ+86w6QotvY7eTZncs5hJUclqCgGFBVQRBSviuJVr957P8w5p2sOgBERUEQxIyoISpScJGeW3WWX\nzWF24vl+nE4z3bM7s+ySrPd56umemp7umuoKp94TKptlDq0B0+6LgZZLVVm2oB3w8VKWWQVqh7a/\nsBwgh87wOoFV/2HDAXfiiSlD+/m8T5TMe7gTeL0ty0glzZngronjaLAFGD8CSAoTwz9gUbmwgIV5\nI63MWtoE+PpT9po+HnT+ChirkQt/fxT4/TFg7Cj2opbx1UwmVs9kdPqGvS4BVki9drB6GaHZKmDi\nQD73OYBXD0cRmoaAQS8Cwx5S5xNPHPDVLGDnFbz27v0+G+bKm2Vrse98YNmDwL5hqL0cLMjtIJhM\nJhqHWfgM1wV/0aYc+FCyAC3fB6y7Rf2urDGw+1IWKPZdwMJwNLBWMbmctpPJ7vQdTFJk/KUuYiNB\nQTvg15fZVTjaBmFxM+Gb8RdbX2f8xQvy1L3G1qzRoDKNd2TNk2Iw28vDp5ii6AhlgMNSbB7PhGOQ\nWwOxxXqXr3iQC1UC1AbeGLagzxrI5MWgF4LdLbxOHjxXTNaTuVqk7GOtVsfvOE66z8lCnjdWSprz\ngJXD3oRamgGsyPjzblZkjBsJNNJs8rXhH0xqVFcOgBd2Z7/M7mZa4q0qid+dK009+hy8wUCoVtcb\nw5brK//LwvDYUcHxpvcM500hlUma2F2/xQqprDcB335UfTm1sLqAHp9wO03IYS1xfC6fh7Yfv437\n5uYb+Fij4Ee8+UT3T4HMz4KJei2OdVCJblslMP5SNTatzw58NRvYPrr6R9nLeQLo97a+nx06h111\nto05Pnd5U4Ctu3p8wtb+shbV6+T7r7g3enez0xmmANBjOu9NoI0l7I1hAk0bw7isEXtlbLkOtZ5k\nG69joVUbpqYu4Lfp4y1XB08ssPhpHi/qgqxJOsRhsHq9z95LxS24Pa2feOJi/1vcTOr0e0vvoVHW\niD0m1k6q3wX00KeA8x5XP8/+Dth1uf665iuACReo41NeV3bpjZQ8aPk7u1fKvy9uwcR2qMKi4WYm\nu7SKjuIWwLyZkVl0tp/Pi1l5PFp9B3syaGH2svVzSykWX0UGMG09e8HUBxyl/J/a/xT+mkXPcoiw\ncP200Uauf7nPu+OBWT+xp8NFk9XrSppxbPRI94yoLZzFvFFnj0/03+28DPhm+om3nGyyhi1t0vZw\nmLM/7+G4iHVFcNUKhHoh+i1ubuvdPwXa/xg+xA/AY6c7kZXB28awN0R54+MvQ4ulHIZEGwu0qDUw\n71PeEydSJB0C7uqkjg3fvRdeqV5XMAqFWJmmV8jKx/zOfx8iM30792uzDzg0iGW5+pwTbZXAf5vx\n+gkAZs4H9lwS/X26fwJceVPdE9wpe3n8Dp2jqxJZht40gevpdPIuS9vJe1u1/ZnnFDl+sNkfch5g\nAu/AuUxW1tojkHitlrqX149Kkj4nHeb3VpXE9bl20vHPYQ03s+JeNtgB2BN30XNsnDT+UpWUPHoW\n8PHvJ8fz6XSGswi46L9Az+nG31dkAEueANbdWvNa/nhw1ixWbsljeXkDloMSs5jwlufHskY8JoRr\nW21/4esdZfw5YOF9pdbdKnnJG/Rxq4vXY/3fVvPIBCy/D1j8FBsbRouMbcDE/up6d9elwGffcxls\nlWyI1mI5f+e38nfVWTKf7hg3UiX0/3gI+O3ZGn5AwK391DF74XNMONcEqwsYOVG1+gbYCOez7/QG\ne2YvG5sOfM3YaDOrH7D0IV5PRR36UJDbQQhLbresAKZLlV9xEPh6KrB7BJNluT1QL5Oy2ccdtMla\nNTXcpBfCXSnAkseBtf+s3SBQHawuybr8ABOHCdnBKT4HiDvG15KJN8c82p3rJFc6ljVBVPVjL+e4\nxXFHmdCVz+PyeKBtszDYzUFGwMLKhW2jmaDvPC+85tBv5c5S3YIGYMH88NnA4YFMaOd2D55gbBUc\nfH/gq8HW97ndgO/fB45I8aRNASbaO37HKXRjwWjgczCZv/pfUtuT4CjlSUUbW2rvhcAXXxprfh0l\nPKgMeC360DEyKtLZwnXNncGb0Fg8PMB1/1TNO3oWMOtHtopr/yMw/jLO99uAN3dH7u5ULYjrQSa8\n7RX87mpLEpi93N4y57DmM1w4n4BZff+eOI6RGU3s8xbLgBF3AA236L+rTAM23MzCakReChJS9rGV\nTPcZ1cca9zlY8Fh+f/2RUzIcJTymyQq0jG0skGf3AbIGsKKqPq3JG27mepaFGhk7RnIIB6uLreja\n/Bb8/YEhTPDlZUb+LHsZcN5j7G2hHRvyurDAZvZJ7TQ3WDkTn6talkaLgIXHhMMDuT5la2at4iS7\nN1vi5vasxQOIXfr6vcX9wcjjyGdnhc/qu9Txr66RkA30mcqbzIZTPskoa8RC8rpJNWyaTNweWy1h\nK6XSZryx7NFuCDt/tf2F45zK9bv0AWDR8+Ef0e4nFjLlBcThAbxQqMniscUyfo6sfCxpxmFCwo0H\ntkpeMPWZFpyf1Z9D1Px1jbFCK20XC7TyOHdgCJfPaFEVnwPc3lNVumb158VtOOWhrYKtlNv9rHqo\nZPet/n8DLHuMuxxouFXN+2sM8McjvElN2m41f9MN3LZDy9B4HTDhQpX8qUrkhVuWZJly1iwmdWRl\nkSuZhfH6Cu/Qfj577cibjQLBcwjAVlJffhYdyVlbmPzAOS+zZWVoqDN3AvDnvyTF9Qki201+9lQ7\n5yWWefPO4rBl266ufTgfAACxkkneVFpuD9HCbwV2XMnKvAPnRb/wsnhYbhz0QvDYvPFG4Kc3a2ep\nN/RJ4Lwn+LwiA3hrVw2bvRL//6qk6Ky8LW6eU4Y8rTdyqA5eJxN8ey7mVNABpxWZGQmsLmDIs9xu\nQxXPxzoygXtwMB+L2qDO/n/Pj4ArJIOrwjbAW7trvw9Cj+nAFTer7TK7F88BtSUtu84BRt6qkl3h\nUNSa54RNN0Qn554omPw8dshrOG3owWiw90Jgxf8Be4ej5vcvGWhlfs7joZGVY3U4OIjHqO2jozeQ\nafszz69akvKHKcFKs47f8qbWssf24QHAp79K+2YJ1IhOX/N6RI5tDfAeTCUtgMYbg6/N78T7Nu26\nDBG1G0eZfv+pcOj/JsfGllHUir3xZAvtNr8C465QlaeVafx9aBzmXu9ziEe5PbgTONRopKRxhx+A\nK25SOSWA1ytfzY5u3neUsBwr99HCtuzNpB3DnEXAzYNVLsYTy5vpHukf+XNqA1OADRsHvsbRDtb8\nE9hwS/3u9xKfy8pP+b28uSuyCAPdPgVGTeDzkmbAG/urN4xKOAJce2WwEvPAEOagqlXuSp63A15n\nY9TQCBBHM5lY/+uaKAyzBLkdhLDkdtNKYOZqPs92AOMHnvjCASwUN9gqEd2beQG+7raTa1ljcTPx\nXJVyYiY1WyUPDt1mMVkQuhgLh6pE1gBtGwPsuUiyIiHJWtPDwqjFoyZPXOREW+N1LMBpXbcCZiZU\nLB4ub00kTE0obcqB99fdGn6gMHuByycBPTXxkXK7AbN/VIlLezmTU+e8rF/clTfgenAWV2+xX9iW\nBbRNN1ZDGBHvuCvvuguwl8Ps71kQl2Mc//kvXtCd6rC4mczKnMNCnZGCpTINmDW/lqQeAS3/APpO\nYcWMkVXunuHAwaGsxPI5pBA6DjWUjs/BE1n3T9lt0AjHOrKVcqjw5LMzib7sgegVDRY3KxUcZdKx\nlMnd+KPB3iBaC5BwKGrNRFnWAE65PY7fzdJRyov/AW8ET5zFLbnt7RypuZh4IXHRf0LIJwtbMa6+\ni9txdWXq+B2T5Frlms+henXUpIS0eFhQSDrEVn1JB0OOh9hiprwBE3Sy8i27j74/NlvFRJqWHAxY\nOPTBkidrIHwl2CqZAOz/lrECRnYtDMWRPqz42jq2Dqz2iEOv9HuLtf2h476s9DvWmRV22ncHsLvu\n8vt4seeNZSGzwVa2iG71Ox+1QrWMwra8uez2UdyvZdIg6RDH2ZYtgfefxwJ/TcLXWbOB0RoXyCN9\nWRFW3EpNJS1UYbfZSrYykcebssZMIkcinHaex0rG0HGeTBw+beu1vPCtTOc+MrE/kLGDrylpDry3\ntvpNXlr+wRbc8rsPtfK2VvEcnTmH50B7ZfDvs3uzcL91nHE7bLaSF1VaZc8fD7NyiMxsyXbNmOBd\n2A+dA3z+tTpHNv0TuOEilbB3JfN7CiXW2yzkTTrlevY5JO+bUeH/f7RwFktWWiHxC7eM49iP57zM\nix4ZAQtvsrT8vvrbtDMxC7jqhmBPKyO447kvr/i/+rNCtbjZ4vWcl9hC0QhHNUR3JGHMbJVs3dh1\nbvWbSmf34v8lz1/aVJ0sVNCeFc8b/xEZ+Z++Axh1fXAsUlcKh7TbdnXNvw8HWyVbb8tzzor/cnhC\nLcw+VpR1+oZlmJQDbKCw40pu5/uHVTM3ES88L7hf7wVZ1ojHwWg8iYpaqUT3/vOj93gNhSnA3hmm\ngN4IJRIkHZS8POfx3Lv/fL0BSXVo8ysTO+HabSjKGrN843PwOBkueaXNzTfcHCasGAG39VE9w355\nmWWM40HPj3jeUAju3sCMX6MjuK0u9kzp856a57cBvz3N8k33GepGy6E4PJBJ4EODuY5OFlnqLOK5\npeN3TLxpPf2OF0czWWG45Tq9LNlgC3DWZ0xqV2eYogWZWFFlpHCqTGPF2bpJkZGEvacBI+4MJim/\n+FIi5EPQbSZ7n8jYN4xD4tQnWWf2sdKzxTIOT+ko5bWv7Hlp9qmfzZKceKyTun9ZTu8aFH9RwuTn\ncjRbxbLU0bNYDg1XB3FHeX3Q9cvg/K3XAD+9xffI/Iy9ekNDf+4/l71JcyTZM20XkLqbj2m7WNmf\nuptlrZJmLFvuH8bvRedtROx1OPRpNetoJjDzZ73xQ4ulHGpEVnZUJXEIt6yBPOYOexAY9JJ6fUkz\n5h2i9VSIz2ELcnkvLoC5mPlv8RxbE1lvCrAc1+k76bexwAerjMN8JhwBbjlbrePKNI4fH06usJdz\nu0vbxWvkg4Mjn2fMXpb9B72oDx9Z2pS9DuuL5D77ZWD4fXx+cDB7jEYCi5s3lpS5qy/mquH/TH5e\n3yccYRky6TAr6+W9hQCWi356Mzqj2+T9LAf3/Cg4VA3Aa7Fl97MStEZOQJDbQTCZTHQdZmI2QuLv\nNHQBn0sxfnMdwLiTRG4LBCOmgImObrOMybzKVI7xs20MD+51GYssFGYfkyrnPV5zaBWvkyecnSPZ\nmgVgYdBWyb+1VXKS80qbMyEf0UBKvMHo+Y+pWaVNWThpvpwH11DL0PxOTHRtG8OLaJOfF+KxBVzH\nsQVMJDiLgMJ2vCCJ1Nqn+ydM/MsLH22cU08s8Obe0y9Wm9XFm8lqSZuS5sCnP9fNzuFxR3lw7zMt\n2GW5tnClMJG18UaVeG8/n92Km60OvtZvZaXF9lHcBpT3r2kHMXJ7KGESuyYPiOOBz86CSV4mp6PS\neTiPEKuLFX9N1rHSqck6Jta1ZKjfxgvAPx4ObzVrL+P6GfC6sQKtMk3aDLKxurlueWMmSbvMC752\n3zCOv17YrtbVEAzJQ8GdiIgsMywe3vh16FPBAkNRa3b78sZIiwKv/igrS4ysHPcNY+XUvgs59FO/\nt/XtCeBxeMt4fndFbdgyq7RZeCLYFGABvfF66T2u52TkOVHSjEm39RNV7xFrFdDzQ2Dw83prp/IG\nbJ3RfHn0m0GXNmVr7h1XAsMeVuP+lzUGpm6IXBna7y3g0rurv6asEZMZDf5SFxTlDZnYjmbTwcTD\nTDJ0/M64HQcsTCTYXOpGO14nC/k5vWu+/8BXgYv+T/387YdczszP2W27Jos9gBdKG29kxYM8fp41\nmy0I5fbqt0mbJU8I/q3Fw9ZP2g16i1rzAttZzBbvchkqU9myLNzO7402cOgX2RqdTLyoWnMHt73Y\nfJ47Y49pzvO5jKVNWSlR2lw6Ng2er9stYMJI2x7LG/C4sOMqNa/jd7y40/a33RdzXHEjUtns5TbS\nWOonJuJ+uXd4zYRh5694btY+6/BAflaTNbzoDV2IeWK5PlZMlto7SSSYwbG6Dbu1sJczoTLwVb1S\nqjoczWRCOGsAz5lJh3mhlXhYPa+OkCpqxfH5t4wPv6A1BVgGc5SwMqfPVKCVweLQ52DLor3D+drQ\nuVKeQ5MOBc+X+4ZxCJq68JrSbvbqtwJTtrBc0vYXJrQ7/FD9mKcYgIwG9l6kKpya/sl9PNTjqaAd\nWxPuuJLfeXyOsTI2dXd4IlMu66FBfJ9tV0cXJi2mkK2N+0xVvTjc8Xy/A+eyZX1OL+O5JnUP94Eu\nX4bfePzgYA7lteNK43vEHWWFldYNG+B+dHAwE3BN10RH/BuBTExUrZ/IoSfltUzTP4FbB/C51wm8\nmlU3HhZaa3CAFdWf/hoZKZixDRgzNlihXtgG+HKOZi8oyVqv+wyeK8J5AQQsHB7l0CAmuw8Nql7h\nGhWI5ZvUvdwWUqRj6l4+r66veGK5j+wcycoUsqgxhIPOzdzn+7/JipNQb7eyRmwwsXsEe7Se9Vl4\nz15PHBM8RW1UOUo+L27J422rxdwXOn1jPN8fGKLfU0LLATmLgzeOLGnOBjvVeS32fQcYcZf6efuV\nHIayrvYqsLrYgr3lUiZZm6+MTK6oDgXtVbI7uw+3sWgUbLH57IXWfr4UmiakrQTMbICgXbPkncX9\n9eJ/B19f1gj4cQqPMVpYXdxuBj+n9652JUfnOQOw16hMdh8cDJz/CNDvXfX7Q2ez3BROidV0NctT\nsrzgiWPL7J4fsWJQRnYvDvNR23CXpgCvuy54IHjc3HORapwYbl+UIc/w3kYyvvyM177hkL6DQ6TK\nckJxC94MPmBjebDxBvWYuidY0V2VyFzIrst5XyOjcddWwbLp2f8LH0lARmkTtlBePzFyktvkr4GL\nIeDOrqoc981HwMabIrs3wN58Q5/h85JmLKckZjGJHW6PvYAF+OkNlhNr650Un8PyYN8p+n2qSpuw\n/Lnutmq8XgW5HYSw5HZ6FTB3FZ/n24Frzj7xhROoHkkHWUhquZQ74LbRLNzWZ5wqI6TsAy6bBLRd\nGJxf3oBdinaO5Ammvjff6T4DGHlL9ZbthW05nteWcfW7AVGrxbyZQSg5tfRBjt92OsNWwVboR7vV\nvYWJyc+kSJ+pHHc9mvj3AQtPuJtuBHZeHkaxQ7zwHfqUGv+8PuG3seV4fleOOZzfNdjqofG6yPcZ\ncCWrAmNxS94zoPE6XhhUt7Ht/nN5c8VIlRAZf7EVi3aH7UhRkQ78/BpbFJ8KLthpu3hsqslKszp4\nYplgXH2Xcdy9JmvYbT3zc73mXQu/la0flMVZKyYKmqxjYdLIM0KLA0PYsi4c6QBUT3IboTINODiE\nybJGG5kMqmkRFbBwiJBDg2q+vxZDng5WQNaE8gbAJ4uB/C7RPUdGTAEvrjM/Z2u06saSr2ZJceYj\nAQFXX6O3QjJCfmcmAJMPsLu6UV/ffy73zb5T1LyKdGDO19XUMbEgPPxe9X9VJfIiSW5HFensXl/T\nTuwp+3gBpw13Em18e0DdFLakBS8CQ0MdbR3LsSi14bxkJB3iGLXNV6p5pU2YdK5KUZV2jdexIs9I\nuei3sfyz83JefGmtPu3lvMDWKgQCZg718sejan8yBZj4G/J0MFEVKXx2aRPcVmr8Ze25L4ZjNvd/\nU688cyUzqbjxJrbg7zqXvQCi2YfGCK5kboObr+eQL7WxiM/4ixXP3T+pfUg3n51DGK36dx1a5RNw\n01DV0KO0Kff7cHUWGgpHC08sx20OWIDML4K/q00oxKSDTAa1WyCFFgwzrpKJx+CtY5nICOcl0GQN\nt53Mz2tuE+4EHjv2n8cKu+YruF3L3oORoKQZL9bX38p91hQAen3AluxaksmVzJvCrp+ovleri71t\nWy5lsrv58tq3G4DnqU0TOPb7oBfV8H8b/gF8+3G1P40KvT5g5ZeMnJ68VihuBRS3ZiWiKxWqbEOs\naLj0rmAvna1j2TMh3Aa11iomdnt8Epk37rEOPEfnZaqyZEkLVCtjWdySEnC9mhpsjWyDbxlljXj9\ntnMkW/ZH642Wso9Ju54fRf7cqiQ2NNkyjhU1kRLG8bn8nN7v1d5AJrs3e9pGsr0HmD0AACAASURB\nVL/A4GeBYY+onzdO4LYYzdhmL2fLzZT96rHJmrpRDkWCinRu11ovuuJW3M5LWjBB2H4+p6arj38/\nMoD3m/r5leq9ImLzpf0NplS/ttEidLPsmrD7ErbMrYmXaLiZ9y4JFzpx5+VMKNcFv9FoA+8BInsT\napHXVfX8OTSIyeB2P7F1ufxejLyXjND0T/ZAlMcsnz16g62AmUPY7rqM5a3yRqz0GfCGXsHuTmCP\nxYqGbFWtDUsDGJPc9nJWGsr74Wm9oYtbsAV91gBWqmq9nbUbQ3rigP/lRsdTJBwB/t0q8rZUmcqK\nrePdDFRGTCHLh0Yy4vfTmOA2hCC3gxCW3E7xAPMk8qfIBow6AXEQBU5jEFueZX7O5NvOkcEu7ScK\nrRfx5o6hgnRxS945eNOEE0f+p2/nDUhSDvBnVzLwxj6xAUmkSDrIC7LYY8Hhc6xuzWc3ABOTGpvH\nRxG/mpjwGvqksVVaTfBb2YrYncjWD/J5VXIwmV3Yrvr2ZvGw8NRsFaemf1Zv8RUN8rqwwFAropnY\noqbvuyx0xx+tWchcfzNbtZ3oTeFqBHFYhOGTo4s5W9iGLaQ33BRZn409JnkfTFH7/PGiMo2t1qJx\nFwe4X/T6EBj0fHBonIoMDs9xcCj3mfwuwWO0tYrH0M7z2ArZyAp0wau8UVRtkPEXL5KSD/ACNPkA\np8Ss4PZVkc7EdjQx36tDfA6ThZmfBxOoALB8Mm9MHQ3sZRzj0GgRUtCOY7BvHSuVX+p7MQUai8tq\n+nh+Z7YmKmpTczk6fseLoVDioLwBMGNR5PUXm8+bahp5IRwvKjJYuSa7d4aD2cuWVVpX3+NBXlde\neOb0Yq8DLXlf3FLacDSM8sAU4PigQ58K3rS6PlDWmN31107SW9LZy1jJ22UuEwyREN1+K1DWlP/3\n5vFsIVlXrr+2Cu5DfaaGt/w1QnYv3kC7JkVLbdBoAzCpd3jipbQpj6E7r2ASufF6Ht86f1XzOO23\nsVLzj0eOLxSixcMKi3YLOIWGSJMRsPAieetY9m7wOaX6nmJc31WJTKDWZCFnBL9V2rNnDBNcvd4P\nH/5qyzhWFIcaBGy+Dvj51ZrlLpOfyYmGm/k/+pzGye9gJWuvD7iejN4pmdT899ZoLKPrCL3eB0aG\nIxDAVvIyIWj2Au0XqN95neyWvn4iIpa5YvOBNovYQrfFMg6DFgmJ6I7n+Tu/C8ubxzpyO9AS2dES\npF4nK1p3X8pruOw+dbOGcxYx6dz/TWNPFW8MP2/LOCbvjsfj2ORnA5Y+09jLNJwyKxQ7RnJYrohJ\nSuJNAc/5n5qV35llNl8Me4GEJlNAknkkIjuasC+lTdX49SUtmfT32/gYsAV/tlZxP2qylonyBlui\nI34jQVlj9oBzlPH9U/dW326LW/L+IPsujPwZaTvZmrnzN/zZE8dhZgraS8cObC1e0J7HwSZrWZHY\nehEr9MIRtpuvY++hSLmA9B3AhGH6trvqHibq69JQzlbJISK14Y1C4Yll+b35ClXRuP9c9jSJVBnU\nbgHv7VJduwhY2MPrWCdWuoSGjAm9NnSNWJHByuw1d6geMFYXjwWDXjAmuXN78johmjWUz8HyTtYA\n5l3kMXnDTSx3RIvLbwN6v6/Pr0hn2aq0KRuUFrfmPRPKmkb/jJpgL+MxbOArXE9ljTgGeFhZTpDb\nQTCZTDQen2IWrg/+IsELfCe55JVZgZFRWmkJCJwsNNjKO14nH+LB8o9HOLZTXW8+GgnijgJXTWDi\n8oepHGtV4NRCy9/ZQiCmgIlZVyoLqK406Ziq5lclM4ntc6DeLJNjCniB02CrJm2p3vLpWAe20Mru\nzcecnuGthmoDk5/J24RsdfNS+RzE7frgkLp7Xn0g7ijH903dI8Vst7FwqzvaOQZ6NKGItDD52WKv\n6Z9suSSnUEEuFBUZ6vvL7s3CWk3WWTXB4uYdw53FbGWR3zny+5l9HJag8zwm+hKzef+D76cdX5nC\nPSvhCBPecUd5AReJ9VRtkHyAwxm0Wch1vOjZ2r3n9O1sNRqXzwu3rWOZ1M7piWrrxxTgxVffKUxO\naxcEey9kt9do4mM23MTEtKzEKGvEGwVFGy7KVsGxkbULyYoMtiQNPfptatzBpEMcFiMhR7+4/etq\n4Md3ootZ3e4nnjON4sHLKG6p9hWri9t4pFapW65ll+hI6tgU4Hc06Hk98U8myb1dc4yGTCpsw7HF\nN90YGflsL2eCu/M8HnvLmrALfWlzXmiVSMeKhifGqKDJWvaWi8/Rz5vKuTR3VmSgXj15RvwT6DtV\n/Xw0k8nsHVdyOzF8NrHrtUx0hyqqto0GFr5Qh6G1NIjP4XaVOYc39DUihfw2Jv2M5v2cnkwYbJFi\n9ycfYGV9qyXsNRhurw+fg13ct49mxU+o0jbSjYsL27LCyigmcV0h6RArA3t+aEyqHOkLvF8PyjiA\nyZfLJ0X3m7wuHIbkeBWyziImrVosY8v3JmvqNgxeVRK36cJ2UsiPtup5eeP6HTssHvZg6v8mk7uH\nz+YQCjtH1k+c8cQs9hoIGpcN+lpJS5Y5ov7vJBFhHxxPKY1xrKNEZg/mY3Er1HoMtVZJoQvXqpbh\nabuim68CZraW3X0pp9zuweWxVbCldwNp7SKvYSwe9hr67Znav+PYfC5rWWNEXAe2Su5DMtndeAOP\ns6vuYYVctO86ZR8T3CkHuC4WvM5GJ/WF9B0837dbwGvU6saAkmbAe+uiD1901iwOCWfx8VyT252N\naHJ78hyTl6nx1iB+nx2/Zw/PZqvCKzOKWwLL72UvtHB7HFVHctcVPloavYcpoK6bzF6JzG7G8lZ9\nxtUPB2sVz4M+Rw3hVQS5HYSw5HasD/hxGZ9XWoARg0984QQEagurixe7uT1OzoCkA+GUCNMgcJqC\nWFCXye7kg2zZmd2bBRF34skuoEBNsFUwCSGT3ckH+L3lSER2aVOcsmOEKcCWBHWpMDkT4CzmhVdh\nO9Tq3SVmqVaK+y7gkFm1idsZnwNc8CDHpf/1xcg23zQEsTWZJy56F3SLh8mxRCn2c2H72ltVJhwB\nrrqRLRqLWmuUdr04GYU2STzMi66O3wOtf9OHCHLH8+afm25Ard6VKaCS2OFgL5fiLh9QPRPkOMzJ\nB3gj8tzuTGpvu7ruYrT+3WH2spWTKQDsGsGEXbRI385xVJMP8EZetVkU1wbxOWwxnTlHH+NbC5+D\nFWhr7pD2EQnXDomJw1YS2d1wC49P20azFX8ksoLFzQrA/m8GW4z7bdx2/3i4DjZMjhAmP48DvT6Q\n4ipLhJx2s6/6QIOtbAEsW9omH+B6NQqvsf5mttiuj9CL1iomJRtuUl3zG/wVmdVvYRt1zMzpxbJi\nfSua/m4w+VkR0vOj6MN2+OxSyKpWPM8Vt2Yr2UPn1N8mxjJMflZIy95zchuXU9IhVn7tuZjJ7L3D\na+m9coqsfWMKWYYtaXkc9yjgcfrwQO5LJwq2Cg4T2W4BxztP36V+53MwiRu6YXikSDjCckthu+iM\nPOLy2BCh4/dcJkc5e8ste4DnqUit4q0utpIO3ZwxYGHL/LxMNaRnXlfuKxnb2ROq+Uom2Y02NS5o\nD7y1E6dE2zshEOR2EMKS23Y/8LMUx85jAi4aeuILJyAgICAgICAg8PeDxVM7jytbBVtsdfiBie6i\ntsAPU2pHetYlTIETH6pN4PRB4mE1fFLTNZxX2IZjfW+46SSE/CLeH6TX+0yyLn249nsg1AVi8zlM\nT0UGE/UnHJLyTyYDE7JZ6XaiFCHacsTlBZPdqXt4Y2OFyO4hQiCeSCRmsdeZrTJ8MgXY00Ymssua\nnLrzgZirTl2k7GNCucFW9hY82V6zFjeHp3Gl1r7NWF3s8WqtYkK7oEPksl9cHnvKNl/JpLfVzeFi\nsgbWriynJQS5HQSTyUTXYwZm4obgL8wBYJEUi9YP4IJzT3TRBAQEBAQEBAQEBAQE/j5IOsSWhsc6\nC5JJQEBAQEBAIAzqj9w+s/wLA5o6soDdbeqn3gQEBAQEBAQEBAQEBARKWpzsEggICAgICAj8jXGG\nqdZNgE9LcJ9+VukCAgICAgICAgICAgICAgICAgICAgI14wwjtwH4BbktICAgICAgICAgICAgICAg\nICAgIHCm47Qlt00IQ1xrLbetgtwWEBAQEBAQEBAQEBAQEBAQEBAQEDgTcWaT25bAiSmMgICAgICA\ngICAgICAgICAgICAgIDACcVpS26bEYa4FmFJBAQEBAQEBAQEBAQEBAQEBAQEBATOeJy25HZYy22/\nCEsiICAgICAgICAgICAgICAgICAgIHCm48wjt32avyTIbQEBAQEBAQEBAQEBAQEBAQEBAQGBMxJn\nHrktwpIICAgICAgICAgICAgICAgICAgICJzxOPPIbZ8ISyIgICAgICAgICAgICAgICAgICAgcKbj\nzCO3gyy3w2w6KSAgICAgICAgICAgICAgICAgICAgcFrjtCW3zQhDXIuwJAICAgICAgICAgICAgIC\nAgICAgICZzxOW3JbhCUREBAQEBAQEBAQEBAQEBAQEBAQEPj74gwktzV/SZDbAgICAgICAgICAgIC\nAgICAgICAgJnJM48cjuKsCRpaXVYIAEBAQEBAQEBAQEBAQEBAQEBAQEBgROGM4/cjiIsCdWRYffD\nD9fNfQQEBAQEBAQEBAQEBAQEBAQEBAQEBCLDmUduB1luh9l0so6RmnpCHiMgICAgICAgICAgICAg\nICAgICAgICDhjCO3MztHHpbECK+8UtsSHT+6dIn82ieeiP7+kyfr8664Ivr7CAgICAgICAgICAgI\nCAgICAgICAicbJy25LYZxlbZFoo8LImM++/X540bV5tSRY5779Xnde8e+e8TEvjYokXkvzGZ9Hny\nfQQEBAQEBAQEBAQEBAQEBAQEBAQETiectuR2qOX2Y4/x0UyavySR2+npxveQY24fb+xto983aWJ8\nbdOmtX/O2Wfr8+65J/Lf1+Z/9uunz3v11ejvIyAgICAgICAgICAgICAgICAgICBQlzhjyO1HHuFj\nkOW2FJYkPj7y+8bFSfcPsXK++eaaf/v44+q53W58zfff81FLNEdDUMswIs/btAE6ddLnn39+9Pef\nOJGPVquad+BA9PeRYVR/iYnG1x6PAkBAQEBAQEBAQEBAQEBAQEBAQEDg74Ezhty2kQc3YjrMGnL7\n3AsIt2MK0r05Qdc64UIqChDjJMSiIui7+HggE1tAgeD7tzy2Dr2xFgAQEwMcOwbs3B7AMCxUrgkl\na+NRBhs8QXmNUtzogQ1BeX37ApdgPkwBf1B+sww37HAb/v/+/YGGyNVZY7dz/4UM5AEAzjsPWLgQ\nmPGBB5nYolxjZHmtJbEB4OJzq5COfN113bBJOf/9d6BrV8PiKQgEAICQmanmOZ1AExyB3ewDAPTo\nwfkO8DMHD+bPW7agRpgQgMkgRI1cB8cLC3wRP1NAQEBAQEBAQEBAQEBAQEBAQEDgxOG0JbcTEwgN\nkYtn8RAIJuCyyzAdNwVZbl/Qbhem4A6MLf8Q3+AK3Im3sWVpMba1uwIFSMfmXjeiAmzW3RjZKEUC\n2i5+H1vQDR2OrUBXbMVXGIXJeBmPfdcHa9EXA7AS9wZeRJonBx3WzsZCXIg+xxaAYIINXqSiAAQT\nJhc9jDIk4n3cijiU43p8irdxJ5L/cxM2oBdMFMDF+AlP4jE0t2RjPkagc/4fmInxWPDkn6BKF5b7\n+2Mv2mJz/4k4igYAmND+GcPhXP0HctEYFp8bLXAQH+AW9Hctwff7M/EZxqEX1qFP+RIM61+OhHdf\nxBZ0w5i0xXDBiWHDgJ9+DIBgQs/D34FgwiTHdNjgwRX4BhPwCc5+ZRTy0QBm8qM7NuJGTAf8fmxC\nD9gCbjyOJzCkWzE2bgSOWJpjGm7D4h7/wTu4AzNmAC1wEI/iKZh+XwKCGWlpTF7fhxfxj8sLcATN\ncKf7FbTDbvzf+FyQP4A5Gf9CPhpgxuTNcN98u0KI56ARJl6WCzfseHXSTiQnA32xGp2xDT/jIqzC\nAJQU+rH5l1yci8XogJ3IQ0OMHAm8dfGP2PYXawBy0RDXYA4exjMYha8AAK2xD5dgPoZhIUqgaicu\nxk8AAB9s6IvVaICjeOpRLwDgO4xEftuBaIxsXIBfAQCxqMBUTAIALMQwRanRHrvghAszMR734HUA\nQBKK0R67cC4Wc9sF0BRZyrMX4Xw0wREMxRKk4Zjy3psiCz2xHv+FuutpcxwCAFQiBg5UBfWRqZiE\nSZiKeJQhBYUA+B2cg2UACJdgvnKtrET5F95EB+xUrnXChXOxGG/hLl0fvB1TYEIALXEAkJRNDlTB\nAh+aIgvtsSvoeiu8eADP6+5zA2ZI91BhgQ9JKAZAaIQc3W/6YrVO8dAeu9AL63TXdsVWXZ6MeJTB\nCVdQnhl+w99kYkvUSo0ElOry0pGPRJTo8sMpssKDdMqz2t1HQEBAQEBA4GTD6TzZJYgMDRqc7BII\n1AdGjz55z46NjfxaI8/faH5/POjV6/h+37593ZSjJlx6ad3c5/rr6+Y+4dC7d93c56qr6uY+9Y2h\nQ092CQQETg4uvPAEPYiITrsEjuphmIZ/sJiwmNNbV14Z9jptyu56gS7vx47/Mbx2NfpEdE9tKkOc\nLm/KNYuivk/xDXfp8j58ZJ8ubzkGUgViwt/rnnt0eQtsI8hlT9Tl/z7hA13eLzd/Vn1ZX35Zl7dx\nI+ny3nL+X/X3+eYbXZ5/zlwqdDTUX/vww/q8gwf52Ef/zrxpDWnTLW/o8i/oeUyXN/vqedWWcygW\n6/LSkE/r0DMobwu60hv4l+7aZjhEBNCLuJdWYEDQd9NwK10J/fNbYy9lo1FQXirUsq9E/+B6g4n6\n4k+lTB5YiQACiO7Fi8FtE5Pofjyve2Y/rCIC6A68TZMwhQigObiaCKAr8DX1xZ+63zTFYXoftxAB\ntB49lGeuQ086gsZ0O94lAuivbtfSGvSmeTHXUVvspm8wkgigLzFK+Y38/O9wGRFA1+BzugQ/0h60\noS8wRnnmufiNvsYVNBwL6GrMIQJoAFYQAZSOPGqJ/bQPrWgqbuO2j+GUimN0F96k3zGYvscIIoBM\n8NMdeJu+xCjqjg1EAL0+6Av6DGPpuT5fUTIKaQom0aqYofQ0HiYXHAQQNcYRmonr6CZ8SARQQ+RQ\nJjbT+7iFxmEWEUDLcDb1xDoai8+oA3Yo5UxDvvJ/rfDQLIyj4VhAx5BKt+NdcqKSzsYyGo9P6WPc\nqJSzF9bSaMylRBQTATTx8ly6Fy9SA+QSQDQZL9H9zWbSHXib3sYdBBDZUUXnYRENxHLlmXEoo0vw\nIzVALrngoA/Oep2a4yC1wAFyopKuwle02dGHUnGMXsfdyqseiW8oDmX0G86lTtim1ENr7KVn8SB9\njmvIBD8loYg6YjvFopwIoBYtiM7FbwQECCAaisV0w/Bcuhpz6BL8SACRBV5KxTFqgz30PUYoz2yM\nIwQQrUYfuqr9FkpCEdngJoDoIvxEswe/S0koovOwSPlNMxwigOglTCYnKgkgcqKSGiKHHsMTNBHv\nEUBkgp9iUEGNkE3zcCUBRO2wS7nPWdhEFnjpfCykFjgQ1Oxb4ADdimm6oeJV/Ft5ppwysZnmPL2T\nElGslE1+PhCgy/Cd7j4X4BfdM1NQQBfiZ81vOT8BJZSUxM+R60ZOGTiqvCttku+jTU1xmGZMrSA7\nqnT3AQLUEdt1vzF6ZhKKdGXXpkbI1uUloYiSUKR7Zlvs1l3L9RswvLcJfkpGoS4/HXmG93HAFbac\n2jqWkw1uMsOnK6cdVWHvE5o++iiy62qbOnWqm/vMn1+/5UxLq5v7vP123dzHZtPnde5cd//3/wxE\noQjF1xrTvfca5w8ZUjf3/+OPurnPL78Y5198cd3cf+3aurlPVZVxfnJy3dz/rbeO7/eNJLGQ9CJ3\nVGnQoOq/Hzfu+O5/33183L37+O7zxBN8dDj033XoQNS8ufo5ppplUbh0zTV8/OQTNa9BAz6OGRP5\nfd55p+ZnyKlrV/01Z59t/Ns4aXnZpAkfb7tN/W7yZD7OmVNz+fr1C1/O/tJy4gP9cpAAotRU9fzZ\nZ2u+Jlzd7Nqlf2ZoSpSWqRMn6r/7j7Rs/+sv49821CwdjeaGtm2Nf9e0KR9v4aUMffWV+p3R2K1N\n7dur5zNn8vFdXvaQ31/ze7nhBn3ev//Nxy5dgvMvvJCP8twkv/9w/zdcys4OLqfHo37XoUPk95HH\nstCxUS6nnK66Sv/fIklLlwa/98OHiVJS+Dyavjl7dvD/Bbi9NmumfnY6ec6U+5lRH62pDfTuzcec\nHKKpU4Pb1BVX1Hyfy3jpS29oqIvbb+ejxcLHyy8nGjWKaODAyMsXLhUVEe3cyedyXbz6auS/79tX\nPX/hheC6mDmT6IEHap5nIk2HDhm3jWjThAl8jI3lY24u0ddfB/djgGXUpKTo769tYwDRhg3qe40m\nxccHf3a5iDZu1F/XvTtR69bR3/+cc4I/79oVPO7VNvn93P75M6jeeOKTTVTXNbl9z513KuT2q9GM\nbiKJdJqnf2fMjPo3T+ERXd7XMJ5llyD61fBcjNblAdGvumTSV5smJcwyvHYWwq+6PCY9S7EXrWkZ\n9CuHRsiOupyD8bsu7+6R+3V57mataRvCM01VsOvyZuI62oV2uvxL8YO+DvpUL4GMwpcRvZcj519f\n7X0W4nxd3u14l3aivS7/bryuy+vRujjsvQuQQmOhV6RZ4dHlLR77btj7EED34QXKQ3pQXjdspM3I\nDMorQhK9hMm63ztRSQRW5hxG06DvvscIGodZVIxg5WBjHNHljWrDSpISJAS9f/m6izGffsUwIoCy\n0ER5L2/jDipEctAzn8Sj5INZUVQRQJnYTATQaMyld/BPvrb/U0QA/QtvUD+sojm4mvyxqsK1NfbS\nJ7iB9qGVohQFiCoQQ6/gP/QsHiQCyHPZVbQNnWg9elAnbKOn8AiVpreir3CV8ptR+JLexF1UZWKm\n4R68Rv/Gq/QZxtJ7UFej52MhHUFjysRmuha8shjVeRsRQD2xjrphI92P56nqH5OIANqNttQcB+l6\nzKAn8ajSvgBWbgzDr3Q5viUCaMP9n9FOtKfH8AQ1xhH6Bz6i/Lufolfxb+k3AeqJdXQBfqFFcZcr\nbWEUvqSR+IYuwY9UjETamdCbrsDXNAWTKAUFdBm+o1J7KtngJgIrlZriMA3Dr7TiGVZWv4m7qBGy\nqTs20EX4ib4F398BFz2A56gV9lFPrCMC6PEJ+2gNetNQLKYUFLBycP16egH3UQ4aEhCgdORRa+yl\nMfhCeWZ/rKSO2E7dsYHykUZP42G6BD/SKHypKKz+6jOBmiBLqSMnKikTm+mFO3kFcBVYOZeGfHpk\n9Db6EDfRX+hMTlTSECwhO6qoMY4o9TUPV1IGjpIdVdQR28l7tIBewz30HiYSECAT/NSnUxkNxwLl\nmS2xn2JRTi1wgLahE52PhdQPq6gV9pEFXhqAFbTvkQ+pGzbS7xisdJNWsUfp3HM8VIAUaoV9koIk\nQF+/c4Rewz30Au6jWJRTS+ynAQOIGiKH1jjOJoDocTyuKB5aYy/9schDb+IumoDpyv2/nuulIVhC\nSzCEAKJYlBNAFIMKWon+1BhHqCu2UCzK6cYbifriT/IuWUb9sIoewHPKfS4930UWC9EWdCUrPEr+\nZ+8W0jN4iEZjLjlRSfEopTvv5PqYEvdfAgJ0KX5Qrm+GQ/TCC0Rv4i7qhbVKflWln4ZgCT2JR3VD\n2mxcS2b4JAVPgKZPJ+qJdVS0+RD1wyoahl+Va598kv/bFxgTdI8XHy2jp/AIZWIzWeEhM3w0ezZR\nZ/xFd8ZPJwu8QeXJwFEaMYKVdFqFEBErE6/F7KD72+BWyp6CAgKInnmGqDfW0KyppZSJzdQCB6hb\nN77+11+J0pBP9+P5oPuMGEH0b7xKiShW6/gzoi7YSv2wimxwBynFzutdQqmpRPfgNTLDR9dey/kb\nNrBisAfWB90/FuV0JeYRwArl3r2ZUO+ODXTnP/3UGX9RPEqVRb3LxUq44VigttlWTOBcjTkEBBSy\nskkTLmczHCIb3BSDCsXW4t573EFt4ZJLOP+tt4jOxjJKQ75CnKWkECWjkPpgNb+LDFZaJCYSdcI2\nuvhiopFd95AJfmXxTMRtqwu2KsQvEyEBSaFMdN55aj10xRaKRyldd62fkhID9L//8bWzP/WRGT7q\nb+W2kJDA1193HdE5WEo2uBXi7PnnuZ20xl6KjWWl0Bc8dFH3pP2Ulkb025fcFmSC5sABVgg3RI5C\nen31FSsQu2MDASqhCRC1x04ywU/LlzOxK5MY+/cTtW7qpm5JqvK0SxeiFi2IumFj0DNnz+ZyJqOQ\nhg/nvM2b+f92Tc8lu51owffcp8eO5e9XruRyxqKcHn+c83Jz2UigJfbr+qic5/cz2bhyJecfOkQ0\n9Y0qSkCJcu3Eidx+2mAPAUSTeNqjadN4fHPApTxz504iM3yUauH+8NxznC8TsMuXE00el0Um+OnD\nD9W2EIMKSsUxXTnTkE8AE5ivvUb08cecv24dUV62lyzwKkTz9OlMcvVqxIYFoyXx/rbbSDKkCNDX\nX7MSYdUqLqcdVZSeTnTppeozmzVjsmTHH0cJIPruO84vLmYDi1iU68jteJQq/e/PP4luvZXz33mH\n8wCi//6Xj7t2ET1wn5/O6cLK7GHDmLw/91xS6n3HDia7Z8/mcprgpwEDuG/Jz7zmGn5fG5aWEcBt\nuXNnoj17iFqmlZEVnhByO6Ao9P1+9d0DRHfdRRQI8Pl776nvZcVSH2UksCJ95EiiGTOYWOrQwqW0\nFyCYIJ48mfvWSLYBopdfJsrLI3rzdZ77Pv2UicO8PKJvPndxP54dXE55npQBEJnNTNrKn7/8Ui1n\neamfTPBT585sS3bgABP7Iy7h+xw5Evy+AKJ581jZNmMGf/7hB/6dPCb/6GPSXwAAIABJREFU+CPR\nkiVE5eVEOUf4PocP83dawq9rV1Yq5uXx5969WcEkK0Pk+2v/y623Ei1cyHX+8ssqWSf/pmtXlVjN\ny+P6Xb2aP+fkEJWUqITuypVM+Pt83G4AVcn5+OOqIvrWW4myslSF3bBhPHZPn86f336b35m2nB9/\nzNcTES1bpvbjrVvVMVtWWBERDR9O9Lu0zNy4kai0VO3/CxcSlZXxdfIzvV6271uyRDVK+OQTTu++\ny/+xTx+ew2SF0UMPET36aHA55TISqe+6eXNVcTZ3rmo36fOx8vPrr/nzihVcDnk8Xb6c3wuR2o/9\nfn5ubq5af9u383v75z+5bDYbvzNZqXPfffyMvDz+nbYNaMs+ZgzfA2BlvDw/HzvGytGPPuL598UX\n1XK++SaPgTLkZ/r9PP64XDznyHUDEI0fz7IJoJLkFgvXxYoV3Da0fSq0nA89RGSXlv579qjGKPPn\nczv44AOi//2PZSL5N+vX85wQ2o+PHCHKz+c8+Zny+x84kGjLFj5/9FG1v911F7fvPXvUsdTt1pdT\nS9BrFXQ33sj33rCBaNEilhUqK9XrDh4Mvo8gt0MLHTqCatLkSZMUcvtFeQQVSSSRRDpJKSu5S73e\nX7aMP940urne8t7TLnpTz+mYYJj/Ou6uk3Jm4Kgub9O1z0X/fzFXlxdISqqVd45ROh8LdXkvjtsQ\n9X0ccOnyVjccYXjtUpwT9f0fwxO6vA4dKOr7APrfPADj9zIb4efmQ2imy/PBTAsw3OCZgajL2QPr\ndXmDYGB+ajbTbzg37H1mQK/4WYIhhuXvj5W6vCMNe4a9NwF0HfTKSjN8uryqAUOq9dR6Dg/o8p6G\ngacTjBVQ/9eNTWpz0cDwNw/hmYjawh+DHqz2/96Ij3V5F8PYRNxozBuWyaubV6D3uluLXob5steU\nNo2J0SsLtWkCpuvyklGoy1uPHrQqZqi+7sexWeH/NfxUyStIZ2XN67ibVnf9h+43Rt5hSSgiAsjb\nKZP8Fqvu+/sSp4RtP5VwKnlv4U4qS2muu/bws2yq+vV4dZz8vRcriMbiM9rbYxSVIzboN5MTpxEB\n5DGrylm5LVT9azJVxLJJvv8iNsc+jKY0o+sL5AKzr16w+Vk68qjElEgbrb2U+1yL2eS32SnflK4y\nAACV33U/EUB7X5xLBWCzvQUTmVFthX20N/Ny2o6OFLByHe1CO/q80+O0B22ovIVqfi97vnmnz6Ss\nWFYiB96dorS3pZc+RzvQgfzdeyrl7Yot9LP5YlrRXp3zWoG9Kde1GkVlPZg9cbfpRN4RV1DA4aCq\nDz6lfWhFBFDWK58Tgcf4dR3G0mIMpcLObHL3MW6k9YPvps8wlvznD1Pu3wuSCfo339AfFml8ktiG\ncZhFqy99nJZjYJAJ70jHAnrVei9tH3mfkueAi/zOGFow+Bna1oENGo5cPon83XqwaeWMGbQbbMrq\ne4bH8UQU03dt7qGlySPo0GXMFtyED+nwiNtoVqenVCYYoOsTvmGTsZ9+Ug0OpFVta+yl5Ve+yGP6\n6zzm7EAHeqL7PHqx3TQqfvAFykea0rdc7c+iy/EtfZbMZopvnzOb/C1aspnwl1/SHrQhb8s2ClPS\nBFn0Xq8ptKnvzXTkkXeUd+W65Q5aePtcKr+VWZgN6E7/6/ohM/W//UYzcR2Xs6JCqaO5F0ylxRhK\n9O23SluYc82X9MXNP5F/6nt01MpK6I7YTqVjbqIrU3+nj5qy0Uhb7GYNxy+/0JEZCykHDZndkpje\nrthCz/X5inZPeIrKP5mr9Gv3g49T1pyllP9PZh7ewT/plwtfInrpJaLVqxWPxorVzIAN6FpKL/Sf\nR+sd/RVW7EZ8TJsfm0tZny8l+uILcsWzcn8IllDew6/TJY030Ad9p6p9tEULosOHacmzy9gQ4KWX\niLp3p4U4n64/ayNNu/pXKn7zE+V9NUAu+T+aTt41G+jYo5x3Az6hQ1fcyazmhg20BEPIl5hMBbMX\nkLtBM3r/pUL6V7cltLvROQpT2QPrqeqrH8j1x2qiRYso4HTSWvSi/3T8kYq++IU+vm8bLRnxklTO\nANFZZxER0byH1vC7+u03okaN6Ek8Sq+MX0fzH/yDfPN/ZqYIoJbYz6zfvn1U/gmzPGdhE3keeJRo\n0ybybt1B2QntqaDvRZT70idUOvI62vl7Dj08eAll9xvJbCFA8ShlhnDtWma9AHobd9C00T+Te+1m\nWj1nL+Vcy+NjAkqY6SGib17YzuU8fJgoIYEuxnxa+MpG2jFzDdGmTUQTJpAXFmqHXcwsZWdzeQH2\ncHz7bfJk55N71wHyOWJoz9iHqPyFt6jy2VfJtSeL3h+zgMqv/ofCuiahiFnB9euJ9u1T5qylj/1C\ngcNZ5N13iLxXX0sl5iRqG5vNGg0i2r0kS+2jDge1wj7KW7qDShev5bIPHUpLcQ6NaLud392xYwpr\nloASoi+/pOnTqmjfUr7PN9fPJdfzr7FLTnY2rb5vLvkff4IZbXm8zclhM/uCAiKwkcGxb5cxY5uV\nRTRoEC3AcLqkywG+TyBAxcs2K+X0pTdg+Tg7m7UpFRVE7dvT87ifHr1mB7+7oiJFm5CKY0TLlhFA\ntPRbfuZ/Ruwk3xtvE+3dS1RaSkeefJ/oww+Jpk2jDba+1BA5fJ9duxQtSiKKmcHz+fjZ7dvTM3iI\nHhy7l2jbNmbylixhzUZpKRX1PJcNZIqKON/vJ0pMpDH4gua9sJMZxIoKomefpQOW1vzMHTsIIMrb\nx+2vdVM3M9YVFUQeD5U89ALRggVEzz9P/8N/qUfjXGZmDx1STO/jUaoyivn5RImJNBpzuU3m5jLD\nPGcO+cdcQ1V7DtOeK/5LQ/u7WNO1aJEyXnfBVjrwx0HOr6wkuv12moOrqW+TLCrZxd7BshYGIO7/\ngQCn++5jzeEDD9AETKebRhVzPR49SrRyJbktTm6zhYX8vJISIrChzuG5K/n/EhG98gpV/OchKlu3\nk7beO52mvePlevzlF6WcSSji/+l2c11ceik9hido0pW5tH1NGY8fK1dSRVwGn8v/kYhN6I8eJbr3\nXuqLP+md/1VyOywqIpozh9bGDOL37vPx9ZJ2Ix6l/M5ltv6OO+jQw1OpZNU22vPpCtq1i/g//Pwz\nUWkpVSWkkR1VnCczx9260WjMpWkvFdOShV4aNoyIvviCNqScyzLbhg1qOS+7jH/3wAPUALm0/k8P\nl7GiguiVV+gd/JOaJJap1xezAZkFXqKcHNq2TarqCy+kZQ/9SBWrt1Le6v1UWUn8/5cuJdq/n3a2\nuIDL6fFwXRIRNW1KmdhM61dW0YED3OXo2Wfpc+eNNLBXFWuuiLiNXHABERF5HnmSDdLkNuj1Ek2e\nTP/CGyTI7dBChwjh2vTAxIkKuf3s+PFhrxNJJJFEEkkkkU7f9GfDWvjznYRU2vqsuvm/6Fsn99mC\nKHxqa5E2olud3OcnXHTi39fd0SvhPsc1urzCpJb1W87ly6P+ze8YrMv71aJXGtVlKl+zrU7uY1TH\ntUkyYRuaih97pU7ub6T0qk0yUuYRQP70jDq5/w+4VJ8fLhZENUn23DnepPVM0iatZ5KSHtF7HNaU\njLwIq5KNFXbVxdAIDR1Ybfrtt6jL+TMu1OVtbRamj1YTZyqauaLqr+hjw4R6pRHCtCmAaLB+3JGT\n0VykVb5pU84H1SseI03zECbmU3O9kk9Oe9BGl7cYQw2vrRg4LOoyGSU57GNQkmNbRJHmI4rYTgOi\naN9S2o6O+vwHq1dmGyWjPho2GYRXlVNUfbQWMdaMjDNKOvUzvr6aegjbX4xSVlbU5VwDfVvZ3S3M\neH1LeCMpWZkWUZLDwRqkEiREfJ8f48PM98PDyyufQs/5hfOQ9szWGzfJaZ8t8tg703CrPr9dO6LG\njcP+xqg/yuFQQ5N7iMFYIrnMHMzoVWP55GQ4TxvFdtKkUOOFcHUcNknKr4Ku+vE/C02oPsltk0QW\nn1YwmUxhS/3YTTfh6QkTAABPfPwxHp8x48QVTEBAQEBAQEBAQEDgbwuP1QqP1Yq4qiqYar5cQEBA\nQEDgjAQBqHA6UZiYCIfXi4TKSsS43WJu/BvDBICI6qUJWOvjpicTNp9POV/arRueHT8eLocDVXY7\nXA4HJ7sdVXY7AmYzTEQwBwIwATAx2w+zdDQRwQTw99J56PV2rxeJlZVIqqjgVF6OpIoKJEvHpIoK\n+CwWFCYkoDAxMfgonRfHx4MAmIlgCQQ4+f3qZ78flkCAPxucWwIBmAMB2H0+xFZVIdbt5iSfa45k\nMsFrtcJrscBjs0V2brHAa7Uq536LBdDUl7Zu5OTwehHvciHB5UJCZaWS5Lx4lwseqxXlMTGocDr5\nGBMT9Nllt/NzwjxLKUMNZbH6/XB4vbD7fLBrjg6vF3avFza/HwDgN5sRMJkQMDj6zWauA6uV60ZO\nUt3ovtPky9/5LBbEuN1IqqhQ2kyi1EYSpbyEykqYQ1Q3oaocr1RvZTExKIuN5fPYWD6X6s4nv6OQ\nupDzzIEAYuR2IrWNuKoqpa3EVVXB6fHA5vPB5vPB6vcrSfvZZ7Eo7y00ye/RbzaDTOr4JZ9r/1Xo\nuwk92vz+ar+3+3zwWK1KPYSrG/nZ2rrQ1rM5EECMx6PUTYxUNzEeDx/dblgCAcN+EfG51Qq/xaLU\npc3ng1062nw+5b8ez7nV7w/6bwTAJ/Vnj6Y82s9y+zW6BgAcXi+cHk9QcmjObX4/LFK7sAQCyjG0\nPQdMJvX+Un0YlUf7nUfTj8hkCmqP8rNCz62a8oR+b5HHdGm8MBrP/GYz3DYbqux29ag9t9lgJoLD\n41HqxiGNKw7NufyfSTOmkMmkjC0+iwUuhwOV0vxU6XCg0ulUz+V8p1N/jd2uXGv3ernvulxKH5b7\ntHwujzdBc5R0lMdBub14rVblv8pJ91lzLn/vttlg9/l0bUVOMW63Yb7D64UlEIARAiZT0HOMkjzP\na5PHaoXN51PfizTma9+TXf5eKoPd5wt6fzafDyapDH7pffnNZvjlo+YdyvKF/E60516LRXmWtu9o\n207QUfre7vPp+hABqLLbDcfdcOOwNvksFsS63Yh3uRDnciGuqorPNUflXJqvtXmhi5KAJFeE9mWv\n1QqSxvyA2aycKwlApdOJgsREFCYmokBKhQkJKEhKQkFiIori45HgcqFZfr5hSi8piWiBFDCZUBob\ni5L4eJTExakpPh7FUp5PGpeVsUMeLzRjiDwXyPUh15/c92I8HkVGDAcC4LbZ1DYjtRWtrBow1fyv\nrH4/bOHmEYO5Qc4PvbPfbEaF06nOmSHtxWu16tq89jMBiKuqUmQ7OckyX7zLBYfXCwKUMdBvsejk\nLLfNph//NOOey+FQ5mlZPjZrZGC5n8jjUOjYLX+2+XxILylBRkkJ0qWUUVysnNuldQQB8EhlCi1P\nWWwsjqakIDc1VUk5aWl8npKCwqQkAEBMVRUaFhWhUWEhGhYVKUn+nFJWZiirx3g8sHu9yruSx0Cl\njYSMeT5JvvBZLEHnXumzCVBkGEW2CZFxrJo5IBzCmxZJbVsaJ/1msyI7h7aXSL6zhYyV8ngsj5F2\nr1eZR7XjS0CSN+XxOlTGCT2apTWCts/IMq7N54MlEEClw6HIknKf0PYVn8WCBJdLkeMVuV46T6ys\nhDkQ4HYZMpdq26pPWl8FzTPyucUCkvp8aL8P7d/VfZbldu2YEzoGuW022Pz+oHoPlWvs0nwtr5fl\n/qc9VsTE8FiemKiM58oYL43tVr9fWSfKY0ZCyPgR+uzQzzafTyfLhR5l2datTZr699hsuvYotyFl\nnNOsH8KtN4PW4QZHayAAn9mszEPFmvmoOD4eJfHxqHA6YZXH7mrWPrYa1ktVdrsyxwU9QzovjYtD\npWbOCZWlXFKblMcK7RpR+79i3G7lnThD35NBHzbKs/l88ITIl3IZopH5LIGAyjmE8A/y51hpfRuj\nkT/DyZ7KuAbo1iQ1HQGo/SNMH9HKxfLaM8btDpLHQ8sh8xHyOO9yOFDhdKpJI/tVSPOVze8PWsuG\njv0xbndQ29HKE1p5wWc268ZC7Tq7IDERR1NSkJecjKMpKTiamqqcu5zOoP9i9vtVWUHT5xMqK5Fa\nVsaptBSpZWVICfkc43YHyQ9k0nM38pxdERPDR6cTlZo6qbLblfE/3FrS7vUGy8UG8rCRTGX03iqd\nTkW2UrgKqR4rnM6gda7XYO1rAgzXDpGca9cTPoslLJclP9MVMi64Qvqc1e8P4vSCOD7NGB57kpT7\npyS5bTKZLgbwOgAzgA+J6MVIf6sVzhb17o1FvXvXfQEFBAQETnFYJcHEZ7HAKxHUJwMmSUgwEymL\nNIFTDzFVVXB4vcpCm8zmE14Gm0YoMxEpgtTJbL+nAuR6cXi9ilI4cBL7kSkQQKy0uPDYbCe1Tzs8\nHjQqLIQ5EFCIPZ+GIJKTR1KWnwyYpIWzojg7yWOgRbOA9VksqIyJOanlOdWQUFEBMplQ6XAcdz9z\nOZ040LgxDjRuHPVvzZIyxXuS26+AgEDdwOrzwWc9JamXvzVsXq9ieGH3+fTGNSdYBrX4/Uxyy2s4\nWXF5EtqOvJYMmM1w1+E8FLBYUBofj9L4+Dq758mEWVrnapUXsvKrLmSJ0xE7JkxAx8OHT/hzT7kR\n1mQymQG8DWAYgGwAa0wm07dEtCOS37fKza3P4gkIHB82bgR69DjZpRD4G8BntdadIHQc7ZbMZnhP\nAlEqEB1cTqfOsuJEw2uzwWuzoSwurm5ueIaMt0q9nOyCSCCzGRWnCCHqtttxsFGjk12MaiEriqIK\nAliPbddvscBlscBVL3c//VEX449Zsq49HjIgIHnGnVY4Q8Zcgb8hTkDbFcT2qQlZxio92QWR4LdY\nUB4bG9nF9dxu62ot6fB4kFZSAo/NhrLY2Dolyk8FBCwWVG////dDvOvkSJmn4ijbD8BuIjoIACaT\n6XMAVwCIiNweu3gx8lJSsK1lS8W9Q3a50Lp+OCXXUa17rOzOpnWdDWhdaA2uc9vt7Ooa6vKjcf2x\n+v1IKy1V3CkUFwvpc3J5OSyBgLG7nmSBJLtxhnPpk106Q10ng86dTpiIdO6rdl904RAsknV8aL1o\nP7ulwatM434RGkpDDl0S5P6scfuI8Xj4OQbPqun52s+yC4asgZVd0LR5JkAJBWPkOmQ2qLea3IFl\nlzGtO+BX+/bhnD59FJewkrg4pf2UxsWhPCYmaAGsDechQ3G7CnHd0+bZfD5dPSh1JrVhbVup0LSR\nCo0bsNbyTevyKiezVBb5HWqT7LYjhwoKDQEiH+VQOUFuXVG4fsmuhnavV+fapHV3jHe5glzPjOrW\nr6mXcEe/xaJz3zI8N2gD8rnsGqp1DTI817j31/S99tzIMlC21gvr1hjq2ijn+XzYumQJ2lVU6ENR\naNwIFYtJrfVkGE21XbKOCCpHTZ+lujMR6awydZ+r+076XN1YArA7odbFMtQdzOH1ImAyBbm2at2M\n5XxTiFuiNtSV7Fof5CYY4u6puKmHhBHSfuf0eOC1WhWXxEqNm6Lcp8tjYlAaF4fikPlJTqFWBRa/\n3zB0SLiQIvK53euF12o1diOtIbRIddbiMdL/rClpXU3XrViB3gcOBL0TeQ7QuSVbrUGf5e+1bdgi\nzX/a8DbyuSxfyO8t9FwOnRQuVEI4V/VwFpsOj0c35oaOv+G+s/r9SltRwoJp2klQXoiLa3lMDKoc\njrB9WjsfWv1+XZsPcuMGu1mmlZQgtawMaaWliqwknyeXl6MsNhZZGRnISk9HVkYGDjdowJ8zMlAS\nhdVPfEgIOW34uKSKCth8PkXu0o4V2jHEyAVY+zlSBZFdaqty+3CGfN7/889oJ8lARpBlm2hCo3mr\nWaTGa9xKtW7CcVVVSggCbZg8bUg8gF1uy2NiUK4Jb6JNchgnrVylDScih7PTukuHjoFOqT7ChTbx\nS+NHaAiLUBd4j82G/KQkHNOk/ORkHJPCJ2j7vE0qU+gYHOdyoUFxMRoVFqJxQQEaSaFG5JReUgJz\nIBAUvkR209Z+Lo2NNZTXK2JidPOnU9tW3G44JWtD2a1fG05DDl0hf5Zlm3ByTaXTWWMoHCOZKRTu\n1asR27Zt2PCKoW3HqF2ZA4GgsFjasVr+7LXZYNLMo0FHacyxhAlJqA3fIIcb0PYbn6bv+M1mxNYQ\ncsfq96MsNhalkjxfqj2X51eTSRd+KujcILRb6Dwjyz6GfTykv4d+p/1PVskqNDQUgnzukObwoLkw\nJKSKHCJO7n/ac/no9Hh4HC8p4XFdHuOlzynl5fBZLDpX/dC1otF8rc3zSSFbtG00dM0jh6c0DLEi\nhSI7/NNPaCvXd5hwoLr1JoL5Aq/VGrTmDu3XAYsFpkDAMDxcsnSMd7ngl8b2SNdBoWsot80Gh9er\nC5MaGo4uNnRMCZGjrH6/El4tlE+QxylXmD6qtJmQd6f73m6H12IJK1/GGOQZhbhzeL3cnkK4h6Dz\n2FhUSVyJIpdGOGfbqhlLjMYWE1G1/cNvNutCTMljdHXeXRZ/cIhQ74oVaJSRERQmTbsGj62qUsKX\naEN8hc4DhuvMEIt1o1AiWpkzrbQUDQsL0aC4GA2LitBAE4orobIyiAfwWiy60CblMTEoiYtDkSZ0\nb2FCQtDngsREJaSUdi2llSXk9VtoaEYlBKvUdrThBo3WjlV2e5B8FxpKt8LpjJj4d7rdhtxEgsR/\nyfN4qBytnatC1wxG64fqzj1SWLZwPJX8XEUu1Xg0aPub0s9Cxmu5v8nvM6GyMqK6qWucchtKmkym\n0QAuIqLbpM/XA+hHRHdrrjnFSi0gEBmekJKAQH1CFrK9UgxZmySc1xZPoHbtVlakyEo4OYal2ETk\n1AIByiIyRiKCrMfRXmpbBi0pHjCZlMV2JDHtjPAEjn+8lQmzmuIo1xfkGPWyUOuQBPZwMRlPBPxm\nM1xSvEKZ+DxZfbosJgZ5KSnK/hrhYu8f7xhYG8jEBwCd4iw0jnoonkD9yAry/gvyAtbi9yPW7a6x\nPH8XyLHZzUTVxj49EfBKyhR5n4DTYd58AkLGFTg98QTqt+3KMfxtBvtoCJw8yO9FJpo9krFUqIHN\niRx/5bHfa7Xq9gAIbTtPoP7arVZeACA2gTRAqIIrNPY3mUxKvH0BFfW5oeTp6ys+axbw8MPAzJnA\nJZcATz4JDBoELFgAfPwxcPPNwPjxQEwM8NprQKNGwNNP828fegjo1g34//buPcqrst7j+Ps7DDhc\nZgbBQAG5yVVDGhQEScAbEgIixE3yFpiaoJihYYZaKaKoUCItVwhmanlZHjmt4xFbmto5eTl5PWbW\nOq7UNPVUaJ4xyeR7/vjuh71n5qep4ZrfTz6vtWY9+/fsy+/Z+/fdz372M/uydi1MmwaHHgoXXwxd\nu8KNN0KnTnDLLfC978HKlTBrFjQ0wIknwtlnw9VXg1ksp6YGfvKTWG6XLnDUUdC5M1xxBXTrBuef\nH/N997swYQJ89aswYACsXx/T7LsvnHNOzH/TTZHOmAG77QYTJ8Lhh8OcOXDwwbDrrlGe4rQbNsR6\nHXJIfBdE+vnPwze/GZ9PPTXSlYVHl3/965HedVekS5fG9556KkyeDIcdBkOHxjIuvBDatYPly2Pa\nTZsi7dULjjgCamthyZLYFp/9bIxL333VVZF+61uxnLo6uOaayNu4MW6lWbgQOnSIdairi7InV18d\n6YMP5utywgmwzz6waBF07BhlWLIk/30XLYr0xhvz5SxbFum110Y6cWKkp52Wj+/QIX6f73wn4ua2\n2/JtNHw4HHccHHgg7L13lHXkyCgHwA9+EOn990f65S/DmWfGb3bllZF33nkwalTEbo8eeTl/+MNY\nfnF73XlnpLNmQffusY719VHeSZNi+957L+y5Z8wP8OijMHMmHHssTJ8OgwbFth01KlKIeEnL79s3\nlr98eaz7tdfCpz4Ft94a8XDnnTBkCMydC3vtFdsjxdj3vx/pAw9EevTR8VsOHgyjR8Mpp0QZjzkG\n7rgD+vSBFStgv/3gmWfgrLMizqZOhT32iNiYNCl+R4CLLor05ptjuiFDIn67do34aWiAzZtjH1y7\nFkaMiN9l/vxYx7TNUznvuSfG9+sX22bYsIiBo4+GSy6JcmzaFOVfvjzW5b774Nvfju86+eQoZ6oz\njj46lnvOORGDF10U+RD1BsS8Q4bE9vzKV2LaceNieWefDZddBhdcENOedVYeR2PHxvDSpZFOnx6x\ntmhRjFu/Prbn7NmRf9VVsW27dYv6B6hat45dDj6YTnvsQU1NDW2mTInxc+fCQQfFctN+uGhRxMLK\nlVEvzJ0bMdOpU5Rv8eKI+2KZli2LOglg1ao8Vvv3hzFjttc/1r07bQYOpN3IkbRfuJDqyy6LhlH/\n/jHPihVRf0PUhcOHQ+/eEQsDBkR+2kZHHglt2kS6enWsx+DBMW758qgbZs+OzyNGRHryyfm+tXFj\npGmf79s31n3WrIjv9u0jdk89FS69NKbZe+9IL7886mLYvo23l7dTp9hHv/jFyJ87N9KGBrj++hh3\nwAGRd+65EePDhtHEvHlRnxbL+aUv5eMvvji+a+jQfNlf+ELsNxD7L8Dpp8OUKTF80kmRjhmTr8e+\n++a/5WGHRdqjB/bjH1O7dSu7detGx7ffpvqkk+K3rqtrWs7p02M/gLy+Pf74fPy6dXEcSLdI9usX\n9Wbar5MZM2KdIepcwKZMod2AAdS99RbdRoxgd6C+sZF2gwdHzNTWwnXXxTypDBMmRHw0f4zAlClR\n9wIsWNB0e0B+XEnHq113jTo7lSnZf3/azJkTnbcpbmbMiPWCfJ+HaGNA/N5r1sRwz56Rdu2a151J\nfX1szyTFxDHH5HkrV8YVKIccQn1jI923bKHzggW0nTSp6bL69cuPmymdNCmPu/Qb1dbm8wwenB+n\ni88DTjGVdO7ctJzE3UOdRo2iw9atVG/bhqX6K9VdEPvk2LFQvBr6IAMrAAAML0lEQVS/ri7KBfk+\nOno0jB8fw2eeGWnv3vk848bl+0LxKu2snLV//St7vfwy/Rsb6T12LD3+9Ce6b9lC17/8hfrGRjr2\n7k1N1gHfJFaT1atj/+/SJc/r0yeO8RDxDFHXpne4pDZV0cyZ0ZYsMKBq+fLtV69WZ39VqZ56r6uz\n0r5ZtG5dTN+tW57X0JDXb0WpXpw6tcUoO/102vbsSYfs5dad3n6bqqqqqJuaO+GESNO+VNS+fdO4\nSa64omXekUeWnjZ76eL2eq0oxWZRz555W6Yo7dvF3zAZNKhlHsRxv7lly6iqrqZz9hLAFh3bpa7w\nK7WcqVNb7DNAnAsAFK+GS3Vs2gcK2r77LnXnnUdN847t1L75MM+BLbU9Fy9uus8mqc4sSnVZsX2e\n7LFHtPmaS+UsKvVbF6WYK1q1CgYObJpXX5+3w4r23DPS+fNbjhs/vuQ+wYoVLfP23//9y9m8DoZo\nvzU/Zu63X+ltnGIptX+KFixo2UaAaFM2l+KsxN00QN7WKkrnAUXz50ebCOJ8L0kxNnJky3kuuKBp\nnQxxLCpVl8yZE2mpR/6kts8HKeeFF75/3Jda37VrW+ZNnJi32Ypmzow01U0Q7WGIY1Upqc1flM7R\nC4y4q6TKvenyk3TeU7R4cR6zxTsnUjuvuJx03Jg1q+Vy6urydm3RDTdEmtqIULq+L0rn05DHypo1\nsX91756Xs7o6b2NNnpzPk7ZjqTqjoSHasc2lc/d0/gL5PvJej5EqdZy++eY4b0+xDtjQoezSty/1\njY3s/ulP0/u119h9yxa6dOxIp7ffpt3ChS07dKdPL92eSLFQPIanffy9jkUzZrTIanvTTdTV19N1\n0CDq3nqLDlu30nbyZKpS/VKqHi1V5y1Z0rI90bVr3h9UrH9S7Be2saWXUbZvT4dBg1puhw0bou4p\nHr+yNj0QfR3Nlaq3V66M7VNXl88zenTeJiyuQzoHO+KIPC/V+8OHl25XpfPd1PcDeX9LbW3Tdmoy\nblzLvFWr4nyrpib6SwCbM4eq2bNps20bbUePZpd33qH93/5Gx3Hj4kXDEybkHdup3Vsse7YcIPoK\noelxat26SFO7DqJdnhT33eQb38iHU5tx6dI4Z+rcOfYziH09lal4PpP2rWJdkr6zR4+o+9u1y/sB\nII+pMWPyvNRHBk2PKx+zcrxyezRwgbtPyj5/DfDiSyXNrLwKLSIiIiIiIiIiIiIlfVxXbpdj53Yb\n4FnihZJ/AB4G5rn7M61aMBEREREREREREREpG2X3Qkl3f9fMFgGbicemrFfHtoiIiIiIiIiIiIgU\nld2V2yIiIiIiIiIiIiIi/0jFvVDSzCaZ2a/N7Ddmdk5rl0d2Pma23sxeNbMnC3m7mtlmM3vWzO4y\ns/rCuGVm9lsze8bMJhbyR5jZk1ksry7ktzOzH2Xz/MLMCm/XEvlozKyXmd1jZk+b2VNmdnqWr9iV\nsmVmu5jZQ2b2WBa352f5ilspe2ZWZWaPmtmm7LPiVsqemf3OzJ7I6t2HszzFrpQ1M6s3s1uyOHza\nzA5Q3Eq5M7NBWV37aJa+YWanK3al3JnZmWb231nM3ZDFWavGbUV1bptZFXAVcASwDzDPzIa0bqlk\nJ7SBiMGirwE/dffBwD3AMgAz2xuYDQwFPgdcbbb9ldfrgAXuPggYZGZpmQuAP7v7QGA1UOIV1yIf\n2t+Br7j7PsAY4LSs/lTsStly963Awe7eAHwG+JyZjUJxK5XhDOBXhc+KW6kE24AJ7t7g7qOyPMWu\nlLs1wL+5+1BgOPBrFLdS5tz9N1ldOwLYD2gEbkexK2XMzHoAi4ER7r4v8bjrebRy3FZU5zYwCvit\nuz/v7u8APwKOauUyyU7G3X8ObGmWfRRwXTZ8HTA9G54G/Mjd/+7uvwN+C4wys92BWnd/JJvuB4V5\nisu6lXi5qsg/xd1fcffHs+H/A54BeqHYlTLn7m9lg7sQjSdHcStlzsx6AZOB7xeyFbdSCYyW54iK\nXSlbZlYHHOTuGwCyeHwDxa1UlsOA/3H3F1HsSvlrA3Q0s2qgPfASrRy3lda53RN4sfD591meSGvr\n5u6vQnQiAt2y/OYx+1KW15OI36QYy9vncfd3gdfNrMvHV3TZ2ZhZX+Iq2AeB7opdKWcWj3Z4DHgF\nuDtrAClupdxdCSwl/hmTKG6lEjhwt5k9YmYLszzFrpSzfsAfzWxD9niHa8ysA4pbqSxzgBuzYcWu\nlC13fxm4HHiBiME33P2ntHLcVlrntkil2JFvarV/PInIB2NmnYj/fp6RXcHdPFYVu1JW3H1b9liS\nXsR/+fdBcStlzMyOBF7N7pZ5v3hS3Eo5GpvdIj+ZeITZQajOlfJWDYwA1max20jcHq+4lYpgZm2J\nq1tvybIUu1K2zKwzcWV1H6AHcQX3fFo5biutc/sloPgg8V5Znkhre9XMugNkt1e8luW/BOxZmC7F\n7HvlN5nHzNoAde7+54+v6LKzyG4buhW43t3vyLIVu1IR3P0vwM+ASShupbyNBaaZ2XPATcAhZnY9\n8IriVsqdu/8hS/8X+BfisZCqc6Wc/R540d3/K/t8G9HZrbiVSvE54Jfu/sfss2JXytlhwHPu/ufs\nqurbgQNp5bittM7tR4ABZtbHzNoBc4FNrVwm2TkZTf97tAk4IRs+HrijkD83e9trP2AA8HB2m8Yb\nZjYqe5j+cc3mOT4bnkU8jF9kR7gW+JW7rynkKXalbJnZbulN22bWHjiceF684lbKlruf6+693b0/\n0Va9x92PBf4Vxa2UMTPrkN3hhZl1BCYCT6E6V8pYdhv8i2Y2KMs6FHgaxa1UjnnEP8MTxa6UsxeA\n0WZWk8XbocQL1Fs1bs19R14p/vEzs0nE25CrgPXufkkrF0l2MmZ2IzAB6Aq8CpxPXNlyC/HfpeeB\n2e7+ejb9MuJtr+8Qj4LYnOXvB2wEaoi3e5+R5e8CXA80AH8C5mYP3hf5yMxsLHA/cZLq2d+5wMPA\nzSh2pQyZ2TDiZSJV2d+P3f2i7Jlrilspe2Y2HjjL3acpbqXcZSedtxNthGrgBne/RLEr5c7MhhMv\n8G0LPAecSLzwTHErZS17PvzzQH93fzPLU50rZc3Mzicu4HgHeAxYCNTSinFbcZ3bIiIiIiIiIiIi\nIiKV9lgSERERERERERERERF1bouIiIiIiIiIiIhI5VHntoiIiIiIiIiIiIhUHHVui4iIiIiIiIiI\niEjFUee2iIiIiIiIiIiIiFQcdW6LiIiIiIiIiIiISMVR57aIiIiIyEdkZj/P0j5mNm8HL3tZqe8S\nEREREZFg7t7aZRARERERqWhmNgE4y92nfoh52rj7u+8z/k13r90R5RMRERER+STSldsiIiIiIh+R\nmb2ZDa4APmtmj5rZGWZWZWaXmtlDZva4mZ2UTT/ezO43szuAp7O8283sETN7yswWZnkrgPbZ8q5v\n9l2Y2WXZ9E+Y2ezCsu81s1vM7Jk0n4iIiIjIJ1V1axdARERERKSCpdsgv0ZcuT0NIOvMft3dDzCz\ndsB/mNnmbNoGYB93fyH7fKK7v25mNcAjZnabuy8zs9PcfUTz7zKzmcC+7j7MzLpl89yXTfMZYG/g\nlew7D3T3//yY1l1EREREpFXpym0RERERkR1vInCcmT0GPAR0AQZm4x4udGwDLDGzx4EHgV6F6d7L\nWOAmAHd/DfgZMLKw7D94PHvwcaDvP78qIiIiIiLlSVdui4iIiIjseAYsdve7m2SajQcam30+BDjA\n3bea2b1ATWEZH/S7kq2F4XdRe19EREREPsF05baIiIiIyEeXOpbfBIovf7wL+LKZVQOY2UAz61Bi\n/npgS9axPQQYXRj3tzR/s+96AJiTPdf7U8BBwMM7YF1ERERERCqKruQQEREREfno0jO3nwS2ZY8h\n2ejua8ysL/ComRnwGjC9xPz/DpxiZk8DzwK/KIy7BnjSzH7p7sem73L3281sNPAEsA1Y6u6vmdnQ\n9yibiIiIiMgnksXj+EREREREREREREREKoceSyIiIiIiIiIiIiIiFUed2yIiIiIiIiIiIiJScdS5\nLSIiIiIiIiIiIiIVR53bIiIiIiIiIiIiIlJx1LktIiIiIiIiIiIiIhVHndsiIiIiIiIiIiIiUnHU\nuS0iIiIiIiIiIiIiFUed2yIiIiIiIiIiIiJScf4fmuU1NaYFhgoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1608a02750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "#niter = 30000\n",
    "test_interval = 500\n",
    "n_hype_configs = len(hype_configs.keys())\n",
    "pid = 1\n",
    "for hype in CV_perf_hype.keys(): \n",
    "    CV_perf = CV_perf_hype[hype]\n",
    "    n_CV_configs = len(CV_perf)\n",
    "    for fid in fid_list:        \n",
    "        train_loss_list = CV_perf[fid]['train_loss']\n",
    "        test_loss_list = CV_perf[fid]['test_loss']\n",
    "        \n",
    "        for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "            #plt.figure()\n",
    "            ax1 = plt.subplot(n_hype_configs,n_CV_configs,pid)            \n",
    "            ax1.plot(arange(niter), train_loss, label='train')\n",
    "            ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test', linewidth='3')                            \n",
    "            ax1.set_xlabel('iteration')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_title('fid: {}, hyp: {}, Test loss: {:.2f}'.format(fid, hype, test_loss[-1]))\n",
    "            ax1.legend(loc=1)\n",
    "            #ax1.set_ylim(0,10)\n",
    "        pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp11'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'R_HC'\n",
    "batch_size = 256\n",
    "encoding_layer = 'code'\n",
    "weight_layers = 'code'\n",
    "multi_task = False\n",
    "\n",
    "if modality in ['L_HC', 'R_HC']:\n",
    "    snap_iter = 10000\n",
    "    \n",
    "    if modality == 'R_HC':\n",
    "        output_node_size = 16471\n",
    "    else: \n",
    "        output_node_size = 16086\n",
    "        \n",
    "    hype_configs = {\n",
    "                   'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':8000,'out':output_node_size},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "\n",
    "elif modality in ['CT']:  \n",
    "    snap_iter = 100000\n",
    "    \n",
    "    hype_configs = {\n",
    "                    'hyp1':{'node_sizes':{'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-4, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "elif modality in ['HC_CT']:  \n",
    "    snap_iter = 20000\n",
    "    hype_configs = {\n",
    "                 'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':500,'out_HC':16086,'out_CT':686},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "hype = 'hyp1'\n",
    "pretrain_preproc = 'ae_preproc_sparse_HC_10k_CT_100k_{}'.format(hype)\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "\n",
    "# Save either L or R HC encodings. Turn on CT + CS save only for one of these. \n",
    "# (Also decide if you want to copy CT raw scores or encodings)\n",
    "save_encodings = True\n",
    "save_scores = False\n",
    "CT_encodings = True\n",
    "\n",
    "for cohort in ['inner_train', 'inner_test','outer_test']:\n",
    "    data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "    test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)       \n",
    "    input_nodes = ['X_{}'.format(modality)]\n",
    "    #input_nodes = ['X_L_HC','X_CT']\n",
    "    net_file = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "    with open(net_file, 'w') as f:\n",
    "        f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "\n",
    "    test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "    with open(test_filename_txt, 'w') as f:\n",
    "        f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "    sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "    if modality == 'CT':\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "    else:\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "        \n",
    "    print 'Check Sparsity for model: {}'.format(model_file)\n",
    "    \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    \n",
    "    encodings_hdf_file = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,pretrain_preproc)\n",
    "    \n",
    "    if save_encodings:    \n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        input_data.create_dataset(input_nodes[0],data=encodings)           \n",
    "        input_data.close()\n",
    "        \n",
    "    if save_scores:\n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        if not CT_encodings: #copy raw scores for CT instead of encodings\n",
    "            CT_data = load_data(data_path, 'X_CT','no_preproc')                    \n",
    "            input_data.create_dataset('X_CT', data=CT_data)    \n",
    "            \n",
    "        adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "        mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "        input_data.create_dataset('y', data=adas_scores)           \n",
    "        input_data.create_dataset('y3', data=mmse_scores)    \n",
    "        input_data.create_dataset('dx_cat3', data=dx_labels)\n",
    "        input_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting TSNE embeddings \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "exp_name = 'Exp11'\n",
    "#preproc = 'no_preproc'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "Clinical_Scale = 'ADAS13_DX' \n",
    "batch_size = 256\n",
    "snap_interval = 2000\n",
    "snap_start = 2000\n",
    "encoding_layer = 'ff3'\n",
    "weight_layers = 'ff3'\n",
    "cohort = 'outer_test'\n",
    "multi_task = False\n",
    "\n",
    "modality = 'HC_CT'\n",
    "snap_end = 21000\n",
    "\n",
    "\n",
    "hype_configs = {\n",
    "            'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-2}},\n",
    "}\n",
    "\n",
    "hype = 'hyp1'\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "tr = hype_configs[hype]['tr']\n",
    "\n",
    "data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "test_net_path = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)       \n",
    "#input_node = 'X_{}'.format(modality)\n",
    "\n",
    "net_file = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)\n",
    "with open(net_file, 'w') as f:\n",
    "    #f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "\n",
    "test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "with open(test_filename_txt, 'w') as f:\n",
    "    f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "n_snaps = len(np.arange(snap_start,snap_end,snap_interval))\n",
    "tsne_encodings = {}\n",
    "net_weights = {}\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    print sn, snap_iter\n",
    "    model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    net_weights[snap_iter] = results['wt_dict']\n",
    "    print encodings.shape\n",
    "\n",
    "    adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "    mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "    dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "    tsne_model = TSNE(n_components=2, random_state=0, init='pca')\n",
    "    tsne_encodings[snap_iter] = tsne_model.fit_transform(encodings) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "plot_encodings = True\n",
    "plot_wts = False\n",
    "\n",
    "color_scores = dx_labels\n",
    "cm = plt.get_cmap('RdYlBu') \n",
    "marker_size = 100\n",
    "marker_size = (np.mod(color_scores+1,2)+1)*50\n",
    "\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    plt.subplot(np.ceil(n_snaps/2.0),2,sn+1)\n",
    "    if plot_encodings:\n",
    "        plt.scatter(tsne_encodings[snap_iter][:,0],tsne_encodings[snap_iter][:,1],c=color_scores,s=marker_size,cmap=cm)\n",
    "    if plot_wts:\n",
    "        plt.imshow(net_weights[snap_iter][weight_layers])\n",
    "        \n",
    "    plt.title(snap_iter)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp6_MC 1 CT\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.305955097371226]\n",
      "ADAS mse: [75.334463327225507]\n",
      "ADAS means: 0.305955097371, 75.3344633272\n",
      "\n",
      "MMSE corr: [0.36003910913047499]\n",
      "MMSE mse: [7.3928481013992817]\n",
      "MMSE means: 0.36003910913, 7.3928481014\n",
      "\n",
      "opt_snap: {2: 14}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-19d528f0e172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfid_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcs_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mr_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{}_r'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mMSE_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{}_mse'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mRMSE_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{}_rmse'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "#exp_name = 'Exp11'\n",
    "#niter = 20000\n",
    "#modality = 'CT'\n",
    "#start_fold = 1\n",
    "#n_folds = 10\n",
    "fid_list = [2] #np.arange(start_fold,n_folds+1,1)\n",
    "#preproc = 'no_preproc'\n",
    "#batch_size = 256\n",
    "snap_interval = 5000\n",
    "snap_start = 5000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "#Clinical_Scale = 'ADAS13'\n",
    "\n",
    "#MC_list = np.arange(1,11,1)\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "idx = 0  \n",
    "df_perf_dict_adas = {}\n",
    "df_perf_dict_mmse = {}\n",
    "df_perf_dict_opt = {}\n",
    "\n",
    "for mc in MC_list:\n",
    "    print exp_name, mc, modality\n",
    "\n",
    "    for hype in hype_configs.keys():      \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "\n",
    "        for fid in fid_list:\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "            #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "            #with open(test_filename_txt, 'w') as f:\n",
    "            #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                    f.write(str(adninet_ff_HC(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC']\n",
    "                elif modality == 'CT':\n",
    "                    f.write(str(adninet_ff_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_CT_AAL_dyn']\n",
    "                elif modality == 'HC_CT':\n",
    "                    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "                elif modality == 'HC_CT_unified_hyp1':\n",
    "                    f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_HC_CT']\n",
    "                else:\n",
    "                    print 'Wrong modality'\n",
    "\n",
    "            #print 'Hype # {}, MC # {}, Fold # {}, Clinical_Scale {}'.format(hype, mc, fid, Clinical_Scale)\n",
    "            data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            if Clinical_Scale == 'ADAS13':\n",
    "                act_scores = load_data(data_path, 'adas','no_preproc')\n",
    "            elif Clinical_Scale == 'MMSE': \n",
    "                act_scores = load_data(data_path, 'mmse','no_preproc')\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                act_scores_adas13 = load_data(data_path, 'adas','no_preproc')\n",
    "                act_scores_mmse = load_data(data_path, 'mmse','no_preproc')\n",
    "            else:\n",
    "                print 'unknown clinical scale'\n",
    "\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort)\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            sub.call([\"cp\", baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort), baseline_dir + \n",
    "                      'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)])\n",
    "            #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "            #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "\n",
    "            if Clinical_Scale in ['ADAS13', 'MMSE']:                \n",
    "                multi_task = False\n",
    "                cs_list = ['opt']\n",
    "                iter_euLoss = []\n",
    "                iter_r = []        \n",
    "                iter_pred_scores = []\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = np.squeeze(results['X_out'])            \n",
    "                    iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                    iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "                fold_r[config_idx] = np.array(iter_r)\n",
    "                fold_act_scores[fid] = act_scores\n",
    "                fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                multi_task = True\n",
    "                cs_list = ['adas','mmse']\n",
    "                iter_euLoss_adas13 = []\n",
    "                iter_r_adas13 = []        \n",
    "                iter_pred_scores_adas13 = []\n",
    "                iter_euLoss_mmse = []\n",
    "                iter_r_mmse = []        \n",
    "                iter_pred_scores_mmse = []\n",
    "\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = results['X_out']   \n",
    "                    encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                    encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                    iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                    iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                    iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                    iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "                fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "                fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "                fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "                fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "                fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "                fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "                fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "\n",
    "    \n",
    "    if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "        fold_perf_dict = {'fold_r':fold_r,'fold_euLoss':fold_euLoss,'fold_act_scores':fold_act_scores,'fold_pred_scores':fold_pred_scores}\n",
    "    else: \n",
    "        fold_perf_dict = {'fold_r_adas13':fold_r_adas13,'fold_r_mmse':fold_r_mmse,'fold_euLoss_adas13':fold_euLoss_adas13,'fold_euLoss_mmse':fold_euLoss_mmse,\n",
    "                         'fold_act_scores_adas13':fold_act_scores_adas13,'fold_act_scores_mmse':fold_act_scores_mmse,\n",
    "                          'fold_pred_scores_adas13':fold_pred_scores_adas13,'fold_pred_scores_mmse':fold_pred_scores_mmse}\n",
    "        \n",
    "    opt_metric = 'corr' #euLoss or corr\n",
    "    task_weights = {'ADAS':1,'MMSE':0}\n",
    "    save_multitask_results = False\n",
    "    CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/'  \n",
    "    save_path = '{}{}_{}_NN_{}'.format(CV_model_dir,exp_name,mc,modality)\n",
    "\n",
    "    results = compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path)  \n",
    "    \n",
    "    # populate the perf dictionary for 10 MC x 10 folds\n",
    "    model_choice = 'APANN'    \n",
    "    for fid in fid_list: \n",
    "        for cs in cs_list:            \n",
    "            r_valid = results['{}_r'.format(cs)][fid-1]\n",
    "            MSE_valid = results['{}_mse'.format(cs)][fid-1]\n",
    "            RMSE_valid = results['{}_rmse'.format(cs)][fid-1]\n",
    "        \n",
    "            if cs == 'adas':\n",
    "                df_perf_dict_adas[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                     'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                \n",
    "            if cs == 'mmse':\n",
    "                df_perf_dict_mmse[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                     'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                \n",
    "            if cs == 'opt':\n",
    "                df_perf_dict_opt[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                     'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "            idx+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Save df style dictionaly for seaborn plots\n",
    "cohort = 'ADNI1'\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_tuned_2.pkl'.format(exp_name, cohort,modality)                \n",
    "pickleIt(df_perf_dict_adas,df_perf_dict_path)\n",
    "# df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE_tuned.pkl'.format(exp_name, cohort,modality)  \n",
    "# pickleIt(df_perf_dict_mmse,df_perf_dict_path)\n",
    "\n",
    "print 'saving results at: {}'.format(CV_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 25)\n",
    "n_rows = n_folds\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "plot_perf = False\n",
    "\n",
    "if multi_task:\n",
    "    euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "    corrs = [fold_r_adas13,fold_r_mmse]\n",
    "else:\n",
    "    euLosses = [fold_euLoss]\n",
    "    corrs = [fold_r]\n",
    "    \n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        \n",
    "        if plot_perf:\n",
    "            plt.figure(fid)\n",
    "            plt.subplot(n_rows,n_cols,1)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "            plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "            plt.legend()\n",
    "            plt.subplot(n_rows,n_cols,2)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "            plt.title('correlation, fid: {}'.format(fid))\n",
    "            plt.legend(loc=2)\n",
    "\n",
    "print preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path):\n",
    "    fold_r_dict = {}\n",
    "    fold_euLoss_dict = {}\n",
    "    fold_act_scores_dict = {}\n",
    "    fold_pred_scores_dict = {}\n",
    "\n",
    "    if multi_task:\n",
    "        fold_r_dict['ADAS'] = fold_perf_dict['fold_r_adas13']\n",
    "        fold_r_dict['MMSE'] = fold_perf_dict['fold_r_mmse']\n",
    "        fold_euLoss_dict['ADAS'] = fold_perf_dict['fold_euLoss_adas13']\n",
    "        fold_euLoss_dict['MMSE'] = fold_perf_dict['fold_euLoss_mmse']\n",
    "        fold_act_scores_dict['ADAS'] = fold_perf_dict['fold_act_scores_adas13']\n",
    "        fold_act_scores_dict['MMSE'] = fold_perf_dict['fold_act_scores_mmse']\n",
    "        fold_pred_scores_dict['ADAS'] = fold_perf_dict['fold_pred_scores_adas13']\n",
    "        fold_pred_scores_dict['MMSE'] = fold_perf_dict['fold_pred_scores_mmse']\n",
    "        \n",
    "        NN_results = computePerfMetrics(fold_r_dict, fold_euLoss_dict, fold_act_scores_dict, fold_pred_scores_dict, opt_metric, hype_configs, \n",
    "                                        Clinical_Scale,task_weights)\n",
    "        adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "        mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "        adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "        mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "        adas_rmse = NN_results['opt_ADAS']['CV_RMSE'].values()\n",
    "        mmse_rmse = NN_results['opt_MMSE']['CV_RMSE'].values()\n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['opt_ADAS']['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['opt_ADAS']['actual_CV_scores']\n",
    "        \n",
    "        print 'ADAS corr: {}'.format(adas_r)\n",
    "        print 'ADAS mse: {}'.format(adas_mse)\n",
    "        print 'ADAS means: {}, {}'.format(np.mean(adas_r),np.mean(adas_mse))\n",
    "        print ''\n",
    "        print 'MMSE corr: {}'.format(mmse_r)\n",
    "        print 'MMSE mse: {}'.format(mmse_mse)\n",
    "        print 'MMSE means: {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse))\n",
    "        print ''\n",
    "        #print 'opt_hyp: {}'.format(opt_hyp)\n",
    "        #print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        \n",
    "        return {'adas_r':adas_r, 'adas_mse':adas_mse, 'adas_rmse':adas_rmse, 'mmse_r':mmse_r, 'mmse_mse':mmse_mse, 'mmse_rmse':mmse_rmse,\n",
    "               'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        NN_results = computeSingleTaskPerfMetrics(fold_perf_dict, opt_metric, hype_configs)\n",
    "        opt_r = NN_results['opt_r'].values()        \n",
    "        opt_mse = NN_results['opt_mse'].values()        \n",
    "        opt_rmse = NN_results['opt_rmse'].values()        \n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['actual_CV_scores']\n",
    "        print 'opt corr: {}'.format(opt_r)\n",
    "        print 'opt mse: {}'.format(opt_mse)\n",
    "        print 'means: {}, {}'.format(np.mean(opt_r),np.mean(opt_mse))\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        print ''\n",
    "    \n",
    "        return {'opt_r':opt_r, 'opt_mse':opt_mse, 'opt_rmse':opt_rmse,'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    \n",
    "\n",
    "\n",
    "    if save_multitask_results:\n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')        \n",
    "        save_path = save_path + '_' + st + '.pkl' \n",
    "        print 'saving results at: {}'.format(save_path)\n",
    "        output = open(save_path, 'wb')\n",
    "        pickle.dump(NN_results, output)\n",
    "        output.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = np.squeeze(results['predicted_CV_scores'][7])\n",
    "b = np.squeeze(results['actual_CV_scores'][7])\n",
    "\n",
    "print a , b                               \n",
    "print stats.pearsonr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    \n",
    "     # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "\n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    opt_rmse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "\n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    opt_rmse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "\n",
    "    print 'Clinical_Scale: {}'.format(Clinical_Scale)\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        print 'This function does not work for single clinical scale models. Use the code from the next cell'\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "\n",
    "        fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "        fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "        fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "        fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "\n",
    "        for fid in fid_hype_map.keys():\n",
    "            r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "            r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "            r_perf_array = np.array(fid_r_perf[fid])\n",
    "            euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "            euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "            euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "            if opt_metric == 'euLoss':\n",
    "                h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "            else:\n",
    "                h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "\n",
    "            opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "            opt_snap[fid] = snp\n",
    "\n",
    "            opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "            opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "            opt_rmse_adas[fid] = np.sqrt(2*euLoss_perf_array_adas[h,snp]) #RMSE\n",
    "            opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "            opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "            opt_rmse_mmse[fid] = np.sqrt(2*euLoss_perf_array_mmse[h,snp]) #RMSE\n",
    "\n",
    "            actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "            opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "            actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "            opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "        opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'CV_RMSE':opt_rmse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "        opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'CV_RMSE':opt_rmse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_snap':opt_snap, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "def computeSingleTaskPerfMetrics(fold_perf_dict,opt_metric,hype_configs):\n",
    "    corrs = fold_perf_dict['fold_r']\n",
    "    euLosses = fold_perf_dict['fold_euLoss']\n",
    "    act_scores = fold_perf_dict['fold_act_scores']\n",
    "    pred_scores = fold_perf_dict['fold_pred_scores']\n",
    "\n",
    "    \n",
    "    NN_multitask_results = {}\n",
    "   \n",
    "    snap_array = np.arange(snap_start,niter+1,snap_start)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = {}\n",
    "    opt_mse = {}\n",
    "    opt_rmse = {}\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "    actual_scores = {}\n",
    "    opt_pred_scores = {}\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "        # if want to find best hyp from mse values\n",
    "        if opt_metric == 'euLoss':\n",
    "            h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        else:\n",
    "            h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "            \n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        r = r_perf_array[h,snp]        \n",
    "        opt_r[fid] = r\n",
    "        opt_mse[fid] = 2*eu_loss\n",
    "        opt_rmse[fid] = np.sqrt(2*eu_loss)\n",
    "        #print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)        \n",
    "        opt_snap[fid] = snp\n",
    "        opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "        actual_scores[fid] = fold_act_scores[fid]\n",
    "        opt_pred_scores[fid] = fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp]\n",
    "\n",
    "    #print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r.values()), np.mean(opt_mse.values()))\n",
    "    #print opt_r\n",
    "    #print opt_mse\n",
    "    return {'opt_r':opt_r,'opt_mse':opt_mse,'opt_rmse':opt_rmse,'opt_hype':opt_hype,'opt_snap':opt_snap, 'actual_scores':actual_scores,'opt_pred_scores':opt_pred_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "\n",
    "model_file = 'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-05-06-17-14-49.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {4:0,5:1}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_ADAS13_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(5.64658243068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update fold performance for multitask\n",
    "print NN_results['opt_ADAS'].keys()\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "#model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl'\n",
    "model_file_soa = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-23-52.pkl'\n",
    "print 'current state of art: ' + model_file_soa\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file_soa,'rb'))\n",
    "\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "#load old file\n",
    "# model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-02-48.pkl'\n",
    "# print 'updated fold result file: ' + model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "idx_pairs = {1:1,3:3}\n",
    "CV_data = update_multifold_perf(boxplots_dir + model_file_soa, NN_results, idx_pairs)\n",
    "\n",
    "print \"\"\n",
    "print 'updated CV_data'\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "#print 'saving results at: {}'.format(save_name)\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(CV_data, output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():        \n",
    "        for idx in idx_pairs.keys():            \n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "\n",
    "    return CV_data\n",
    "\n",
    "def update_multifold_perf(saved_perf_file, new_perf_dict, idx_pairs):    \n",
    "    perf_metrics = ['CV_r', 'actual_CV_scores', 'CV_MSE', 'predicted_CV_scores']    \n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        if key == 'opt_hype':\n",
    "            for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                CV_data[key][idx_key] = new_perf_dict[key][idx_val]\n",
    "                \n",
    "        else:\n",
    "            for pm in perf_metrics:\n",
    "                for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                    CV_data[key][pm][idx_key] = new_perf_dict[key][pm][idx_val]\n",
    "    \n",
    "    return CV_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "model_files = ['Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl',             \n",
    "             'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-05-23-07-31-29.pkl']\n",
    "\n",
    "#'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-03-19-29-16_Fold9_10.pkl'\n",
    "\n",
    "Clinical_Scale = 'ADAS'\n",
    "perf_array = []\n",
    "for mf in model_files:\n",
    "    CV_data = pickle.load(open(boxplots_dir + mf,'rb'))['opt_{}'.format(Clinical_Scale)]        \n",
    "    perf_array.append(CV_data['CV_r'].values())\n",
    "\n",
    "print (np.max(np.array(perf_array),axis=0))\n",
    "print np.mean(np.max(np.array(perf_array),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "\n",
    "\n",
    "save_detailed_results = True\n",
    "multi_task = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results    \n",
    "    if not multi_task:\n",
    "        NN_results = {}\n",
    "        NN_results['CV_MSE'] = opt_mse\n",
    "        NN_results['CV_r'] = opt_r\n",
    "        NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "        NN_results['actual_CV_scores'] = actual_scores\n",
    "        NN_results['tid_snap_config_dict'] = opt_hype\n",
    "        print opt_r\n",
    "        print opt_mse\n",
    "        \n",
    "    else:\n",
    "        NN_results = NN_multitask_results\n",
    "        print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "        print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "        print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "        print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])\n",
    "        \n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_BOTH_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    for modality in modalities:\n",
    "        in_data = load_data(in_data_path, 'Fold_{}_{}'.format(fid,modality), preproc)         \n",
    "\n",
    "        # HDF5 is pretty efficient, but can be further compressed.\n",
    "        comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "        with h5py.File(out_data_path, 'a') as f:      \n",
    "            if modality == 'X_R_CT': #Fix the typo            \n",
    "                modality = 'X_CT'            \n",
    "\n",
    "            f.create_dataset('{}'.format(modality), data=in_data, **comp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp6'\n",
    "exp_name_out = 'Exp6_MC'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "modalities = ['X_L_HC','X_R_HC','X_R_CT','adas','mmse','dx']\n",
    "dataset = 'ADNI1'\n",
    "preproc = 'no_preproc' #binary labels\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "\n",
    "for mc in np.arange(1,3,1):\n",
    "    for fid in np.arange(1,11,1):\n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_train_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_valid_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_ADAS13_NN_valid_MC_{}.h5'.format(exp_name,dataset,mc)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name_out)\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC: fid[1:4] 8000; fid[5,6] 6000, fid[7,8,9]: 8000 fid 10: 6000\n",
    "CT: fid[1:5] 12000 fid[6:10] 6000\n",
    "\n",
    "#spawn net\n",
    "ADNI_FF_train_hyp1_CT.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "modality = 'HC_CT'\n",
    "pretrain_snap_HC = 4000\n",
    "pretrain_snap_CT = 6000\n",
    "n_folds = 10\n",
    "\n",
    "#Custom snaps per fold\n",
    "#HC_iter = [8000,8000,8000,8000,6000,6000,8000,8000,8000,6000]\n",
    "#CT_iter = [12000,12000,12000,12000,12000,6000,6000,6000,6000,6000]\n",
    "\n",
    "hyp = 'hyp2'\n",
    "for fid in np.arange(1,n_folds+1,1):\n",
    "    print 'fid: {}'.format(fid)\n",
    "    #for AE_branch in ['CT','R_HC','L_HC']:\n",
    "    for AE_branch in ['CT','HC']:\n",
    "        print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "        if AE_branch == 'L_HC':\n",
    "            params_FF = ['L_ff1', 'L_ff2']            \n",
    "            AE_iter = 12000\n",
    "        elif AE_branch == 'R_HC':\n",
    "            params_FF = ['R_ff1', 'R_ff2']            \n",
    "            AE_iter = 12000\n",
    "        elif AE_branch == 'HC':\n",
    "            params_FF = ['L_ff1','R_ff1']\n",
    "            AE_iter = pretrain_snap_HC\n",
    "        elif AE_branch == 'CT':\n",
    "            #params_FF = ['ff1', 'ff2']\n",
    "            params_FF = ['ff1']\n",
    "            AE_iter = pretrain_snap_HC\n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            #Only use this during 1 of the modalities to avoid overwritting\n",
    "            print 'Spawning new net'\n",
    "            pretrain_net = caffe.Net(baseline_dir + 'API/data/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(1,hyp,modality), caffe.TRAIN)\n",
    "        else:\n",
    "            print 'Wrong AE branch'\n",
    "\n",
    "        # conv_params = {name: (weights, biases)}\n",
    "        conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "        for conv in params_FF:\n",
    "            print '{} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "        # Review AE net params \n",
    "        #fid for pretain is 1 because it's same definition for all the folds.\n",
    "        net_file = baseline_dir + 'API/data/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(1,hyp,AE_branch)\n",
    "        #model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/Exp11_{}_{}_iter_{}.caffemodel'.format(fid,hyp,AE_branch,AE_iter) \n",
    "\n",
    "        AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "        #params_AE = ['encoder1', 'code']\n",
    "        params_AE = params_FF #if you are using pretrained NN\n",
    "        \n",
    "        # fc_params = {name: (weights, biases)}\n",
    "        fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "        for fc in params_AE:\n",
    "            print '{} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "        #transplant net parameters\n",
    "        for pr, pr_conv in zip(params_AE, params_FF):\n",
    "            conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "            conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "        save_net = True\n",
    "        if save_net:\n",
    "            save_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_{}_{}_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'.format(fid,hyp,modality,pretrain_snap_HC,pretrain_snap_CT)\n",
    "            print \"Saving net to \" + save_path\n",
    "            pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_data['opt_ADAS']['CV_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print n_snaps/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
