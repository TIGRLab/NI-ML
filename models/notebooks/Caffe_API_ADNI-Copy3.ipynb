{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "import h5py\n",
    "import numpy as np\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "# Some defs to load data and extract encodings from trained net\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    #print net.blobs.items()[0]\n",
    "    #print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        #print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        #print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        #print net.blobs[input_node].shape\n",
    "        \n",
    "        data_layers.append(net.blobs[input_node])    \n",
    "        #print 'X_list shape: {}'.format(X_list[i].shape)\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "    \n",
    "     \n",
    "    net.reshape()            \n",
    "    #print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        #print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas_bl  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas_bl,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1, concat_param=dict(axis=1))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas_bl)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas_m12)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas_bl,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.adas_m12,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT,n.adas_bl  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_CT,n.adas_bl,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas_bl)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas_m12)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas_bl,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.adas_m12,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT_unified(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_HC_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_HC_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_HC_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_HC_CT,n.adas_bl,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_HC_CT, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=4, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.adas_bl,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas, n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "#     n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,n.ff1, concat_param=dict(axis=1))\n",
    "    #n.concat = L.Concat(n.X_L_HC,n.X_R_HC,n.X_CT, concat_param=dict(axis=1))\n",
    "    \n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=lr['COMB']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.dropC1 = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2) #This is done automatically by caffe! \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        #n.ff2_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2ADAS13 = L.ReLU(n.ff2_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        #n.ff2_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2MMSE = L.ReLU(n.ff2_MMSE, in_place=True)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        n.ff1_DX = L.InnerProduct(n.ff3, num_output=node_sizes['DX_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1DX = L.ReLU(n.ff1_DX, in_place=True)\n",
    "        \n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas_bl,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.adas_m12,loss_weight=tr['MMSE'])  \n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_DX = L.InnerProduct(n.ff1_DX, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.DX_loss = L.SoftmaxWithLoss(n.output_DX, n.dx_cat3,loss_weight=tr['DX'])  \n",
    "    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_CT, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.dropC1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "        \n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_R_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_L_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))       \n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='HC_CT': #multimodal AE - experimental stage\n",
    "        HC_node_split = 0.8\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_L_HC = L.InnerProduct(n.X_L_HC, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.9*node_sizes['En1'])))        \n",
    "        n.NLinEn_L_HC = L.ReLU(n.encoder_L_HC, in_place=True)\n",
    "        \n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_CT = L.InnerProduct(n.X_CT, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.1*node_sizes['En1'])))        \n",
    "        n.NLinEn_CT = L.ReLU(n.encoder_CT, in_place=True)\n",
    "        \n",
    "        #Concat         \n",
    "        n.encoder1 = L.Concat(n.encoder_L_HC,n.encoder_CT, concat_param=dict(axis=1))\n",
    "        \n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers (or common encoder layers for multimodal) \n",
    "    \n",
    "#     n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.Sigmoid(n.encoder2, in_place=True)\n",
    "    #code layer\n",
    "#    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='xavier'))  \n",
    "    \n",
    "#     n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "    \n",
    "    #Decoder layers\n",
    "    if modality == 'CT':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)                    \n",
    "    \n",
    "    elif modality =='R_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    elif modality =='HC_CT':        \n",
    "        n.decoder_L_HC = L.InnerProduct(n.code, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_L_CT = L.ReLU(n.decoder_L_HC, in_place=True)\n",
    "        n.decoder_CT = L.InnerProduct(n.code, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_CT = L.ReLU(n.decoder_CT, in_place=True)\n",
    "        \n",
    "        n.output_L_HC = L.InnerProduct(n.decoder_L_HC, num_output=node_sizes['out_HC'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_HC = L.SigmoidCrossEntropyLoss(n.output_L_HC, n.X_L_HC)\n",
    "        \n",
    "        n.output_CT = L.InnerProduct(n.decoder_CT, num_output=node_sizes['out_CT'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_CT = L.EuclideanLoss(n.output_CT, n.X_CT)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    return n.to_proto()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.96 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode):\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 20\n",
    "    \n",
    "    if multimodal_autoencode:\n",
    "        train_loss_HC = zeros(niter)\n",
    "        test_loss_HC = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_loss_CT = zeros(niter)\n",
    "        test_loss_CT = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "        for it in range(niter):            \n",
    "            solver.step(1)  # SGD by Caffe \n",
    "            train_loss_HC[it] = solver.net.blobs['loss_HC'].data\n",
    "            train_loss_CT[it] = solver.net.blobs['loss_CT'].data\n",
    "            \n",
    "            if it % test_interval == 0:                        \n",
    "                t_loss_HC = 0\n",
    "                t_loss_CT = 0                                \n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()   \n",
    "                    t_loss_HC += solver.test_nets[0].blobs['loss_HC'].data\n",
    "                    t_loss_CT += solver.test_nets[0].blobs['loss_CT'].data\n",
    "                \n",
    "                test_loss_HC[it // test_interval] = t_loss_HC/(test_iter)\n",
    "                test_loss_CT[it // test_interval] = t_loss_CT/(test_iter)\n",
    "                print 'HC Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_HC[it], np.sum(train_loss_HC)/it, test_loss_HC[it // test_interval])\n",
    "                print 'CT Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_CT[it], np.sum(train_loss_CT)/it, test_loss_CT[it // test_interval])\n",
    "                \n",
    "        perf = {'train_loss':[train_loss_HC, train_loss_CT],'test_loss':[test_loss_HC,test_loss_CT]}\n",
    "    else: \n",
    "        \n",
    "        #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "        # losses will also be stored in the log\n",
    "        test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "        if not multi_task:\n",
    "            train_loss = zeros(niter)\n",
    "            test_loss = zeros(int(np.ceil(niter / test_interval)))  \n",
    "\n",
    "        else: \n",
    "            if multi_label:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_DX_loss = zeros(niter)\n",
    "                test_DX_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "            else:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_MMSE_loss = zeros(niter)\n",
    "                test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "\n",
    "        #output = zeros((niter, batch_size))\n",
    "        #solver.restore()\n",
    "        #the main solver loop\n",
    "        for it in range(niter):\n",
    "            #solver.net.forward()\n",
    "            solver.step(1)  # SGD by Caffe    \n",
    "            # store the train loss\n",
    "            if not multi_task:\n",
    "                train_loss[it] = solver.net.blobs['loss'].data        \n",
    "            else: \n",
    "                if multi_label:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_DX_loss[it] = solver.net.blobs['DX_loss'].data\n",
    "                else:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "            # store the output on the first test batch\n",
    "            # (start the forward pass at conv1 to avoid loading new data)\n",
    "            #solver.test_nets[0].forward()\n",
    "            #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "            # run a full test every so often\n",
    "            # (Caffe can also do this for us and write to a log, but we show here\n",
    "            #  how to do it directly in Python, where more complicated things are easier.)\n",
    "            if it % test_interval == 0:        \n",
    "                t_loss = 0\n",
    "                t_ADAS13_loss = 0\n",
    "                t_MMSE_loss = 0\n",
    "                t_DX_loss = 0\n",
    "                correct = 0\n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()                \n",
    "                    if not multi_task:\n",
    "                        t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                        if multi_label:\n",
    "                            correct += sum(solver.test_nets[0].blobs['output'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "                    else: \n",
    "                        if multi_label:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_DX_loss += solver.test_nets[0].blobs['DX_loss'].data\n",
    "                            correct += sum(solver.test_nets[0].blobs['output_DX'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "\n",
    "                        else:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "                if not multi_task:\n",
    "                    test_loss[it // test_interval] = t_loss/(test_iter)\n",
    "                    if multi_label:\n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                    print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                        it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval], test_acc[it // test_interval])\n",
    "                else:\n",
    "                    if multi_label:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_DX_loss[it // test_interval] = t_DX_loss/(test_iter) \n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'DX Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                            it, train_DX_loss[it], np.sum(train_DX_loss)/it, test_DX_loss[it // test_interval],test_acc[it // test_interval])\n",
    "                    else:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_MMSE_loss[it // test_interval] = t_MMSE_loss/(test_iter)             \n",
    "\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "        if not multi_task:\n",
    "            perf = {'train_loss':[train_loss],'test_loss':[test_loss],'test_acc':[test_acc]}\n",
    "        else:\n",
    "            if multi_label:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_DX_loss],'test_loss':[test_ADAS13_loss,test_DX_loss],'test_acc':[test_acc]}\n",
    "            else:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "                \n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,snap_prefix):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    \n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    #s.solver_type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 100000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    #s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 4000\n",
    "    s.snapshot_prefix = snap_prefix\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MC # 1, Hype # hyp1, Fold # 1\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (283.225036621,inf), test loss: 199.925467682\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (392.50604248,inf), test loss: 267.016678619\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (218.277450562,186.924684616), test loss: 196.395841599\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (43.8631858826,120.284702505), test loss: 76.8728272438\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (387.103607178,186.650449242), test loss: 212.557409286\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (276.03137207,92.1751233735), test loss: 78.8885192871\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (86.8439102173,186.007922185), test loss: 193.148869324\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (34.9865760803,82.1599458672), test loss: 62.6079571724\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (128.642211914,185.655445393), test loss: 192.346458817\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (45.7013549805,76.855078375), test loss: 74.2803033829\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (163.684677124,185.539579945), test loss: 214.683376312\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (78.829284668,73.45316928), test loss: 74.868424511\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (270.360076904,185.272895924), test loss: 192.191951752\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (38.5262413025,70.9215219978), test loss: 59.6921814442\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (191.861694336,185.061704981), test loss: 200.19712944\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (40.1089019775,68.9312227832), test loss: 74.5327666283\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (273.433013916,184.856439445), test loss: 205.81277771\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (130.340057373,67.2935933617), test loss: 70.5288711548\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (200.9269104,184.617273248), test loss: 190.724083328\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (33.6937446594,65.8800790259), test loss: 59.7142743111\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (331.891784668,184.50286226), test loss: 199.257257462\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (233.759643555,64.6518324326), test loss: 72.322436142\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (73.8182830811,184.308122459), test loss: 206.107878876\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (30.198797226,63.4946662174), test loss: 60.92706213\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (147.337524414,184.138991589), test loss: 188.822022247\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (33.4575462341,62.4309381739), test loss: 58.400945282\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (156.991592407,184.029377075), test loss: 200.62930336\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (67.6725692749,61.4509979057), test loss: 70.0862118721\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (234.778762817,183.857601551), test loss: 202.904919815\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (32.6190185547,60.5131895567), test loss: 56.2990159988\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (155.847991943,183.696007968), test loss: 184.7064888\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (35.1261940002,59.6204088492), test loss: 57.4019382477\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (260.854309082,183.534549441), test loss: 200.82950058\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (93.5487442017,58.7779264243), test loss: 68.4561328888\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (202.669616699,183.351284942), test loss: 197.987709427\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (24.5740356445,57.9779727738), test loss: 54.5191561222\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (301.867248535,183.230823351), test loss: 187.6389534\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (186.025695801,57.2324503173), test loss: 57.7564078331\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (58.84582901,183.061123667), test loss: 202.574822617\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (26.9365234375,56.5005417082), test loss: 68.0109806061\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (142.085357666,182.906866004), test loss: 190.908801651\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (24.749546051,55.8016406683), test loss: 53.7650297642\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (125.571914673,182.788657134), test loss: 189.411278152\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (34.1182327271,55.1420973189), test loss: 61.58292799\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (197.203521729,182.631676269), test loss: 206.74607811\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (29.8615493774,54.5050137088), test loss: 67.1556594372\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (105.557975769,182.478961753), test loss: 187.323942566\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (32.5207481384,53.8882536016), test loss: 52.3249490261\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (260.440673828,182.327442132), test loss: 186.30413475\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (71.2516708374,53.2982390853), test loss: 65.422441864\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (207.873123169,182.162131446), test loss: 208.39451828\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (14.271314621,52.7328129579), test loss: 66.2021224499\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (300.831634521,182.034730476), test loss: 186.425553894\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (158.752578735,52.2018229767), test loss: 50.0379328728\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (85.8915710449,181.877401337), test loss: 194.419522476\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (47.6434593201,51.6784229441), test loss: 67.3192008018\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (125.405807495,181.726621219), test loss: 199.497377396\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (22.2054347992,51.1689424824), test loss: 63.0219517946\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (122.862854004,181.60494715), test loss: 184.351616287\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (19.3555622101,50.6831681789), test loss: 52.5133359432\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (205.422668457,181.453805973), test loss: 193.283323288\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (23.5907764435,50.2090261487), test loss: 67.1887666702\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (102.291297913,181.305110111), test loss: 200.371866226\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (29.4091243744,49.7451972751), test loss: 54.5082046032\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (247.289611816,181.157844015), test loss: 183.381031036\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (56.3601493835,49.2953021692), test loss: 53.9273710251\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (191.150115967,181.00168394), test loss: 195.98550148\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (10.7002763748,48.8580001705), test loss: 66.3231155872\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (297.437805176,180.872510251), test loss: 196.280789948\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (152.728302002,48.4437634183), test loss: 51.8219197035\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (114.775634766,180.720804874), test loss: 179.044254684\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (60.0687561035,48.0296939343), test loss: 54.6886301994\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (112.111022949,180.572504833), test loss: 195.557172394\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (19.0123786926,47.6213720896), test loss: 66.2230797529\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (122.481536865,180.449077445), test loss: 192.031268692\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (16.7816810608,47.228512431), test loss: 51.9203117132\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (217.689498901,180.301261957), test loss: 182.872657394\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (18.1491928101,46.8415295287), test loss: 55.9702111244\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (92.8658905029,180.15516478), test loss: 196.910521317\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (27.9798049927,46.4603044663), test loss: 67.2159489632\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (169.390457153,180.010811266), test loss: 185.598622894\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (28.7124519348,46.0865354495), test loss: 53.114526701\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (163.395172119,179.859731246), test loss: 184.268946075\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (7.79892730713,45.7200937609), test loss: 61.929784584\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (288.25390625,179.733098287), test loss: 201.174133301\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (122.121887207,45.370620403), test loss: 63.0734681606\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (121.831489563,179.581748384), test loss: 181.258177948\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (52.5668563843,45.0172298821), test loss: 50.2871738434\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (117.072219849,179.437317914), test loss: 180.79074707\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (13.220908165,44.6686358626), test loss: 63.0881819248\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (130.32963562,179.312094594), test loss: 201.413909912\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (10.3784227371,44.3300815179), test loss: 61.6492799282\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (217.068634033,179.167271542), test loss: 181.024135208\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (13.804602623,43.9953543563), test loss: 49.2382915974\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (45.3944206238,179.023059653), test loss: 188.866309738\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (23.0572738647,43.664521729), test loss: 64.483175993\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (109.795051575,178.881194503), test loss: 193.703062439\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (11.8926210403,43.3378350165), test loss: 60.1409772873\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (157.92225647,178.733437524), test loss: 178.23081665\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (8.43795776367,43.0159320686), test loss: 51.0592607975\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (238.138916016,178.60683744), test loss: 187.540703583\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (90.3198394775,42.7061683673), test loss: 63.9238851547\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (138.13848877,178.458817191), test loss: 194.719963455\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (39.6603240967,42.392623867), test loss: 51.3702798843\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (132.83782959,178.317495508), test loss: 177.496946335\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (13.4260692596,42.0824514413), test loss: 52.425123024\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (149.773803711,178.190892669), test loss: 191.38981514\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (7.5944185257,41.7791249646), test loss: 63.3351697445\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (221.068344116,178.048787767), test loss: 189.386439896\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (11.8144159317,41.477975272), test loss: 52.0486087322\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (75.1370773315,177.907437475), test loss: 173.621759033\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (19.0503501892,41.1796556972), test loss: 54.0290185452\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (92.5821380615,177.766621894), test loss: 190.799137115\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (9.12086486816,40.8836356886), test loss: 64.2264564514\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (146.115783691,177.621213099), test loss: 186.333818817\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (8.42885875702,40.5905550016), test loss: 52.1738930702\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (233.431503296,177.495000203), test loss: 177.97409668\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (77.1800994873,40.3060500368), test loss: 56.0920752048\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (141.284881592,177.349526968), test loss: 191.433611298\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (32.0251426697,40.0182806999), test loss: 64.5510959625\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (167.634078979,177.211003131), test loss: 180.262919617\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (12.8761501312,39.7327053927), test loss: 52.506281805\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (149.232574463,177.083357241), test loss: 179.192546844\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (7.34834527969,39.451759636), test loss: 62.8119018078\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (227.072891235,176.944226891), test loss: 195.815168762\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (7.80642175674,39.1721851928), test loss: 64.6603775978\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (86.4724273682,176.804208588), test loss: 175.320966339\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (17.0049705505,38.8947415871), test loss: 50.9878516674\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (75.4300231934,176.66493876), test loss: 175.461907959\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (7.60108757019,38.618497135), test loss: 63.6528503656\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (137.62713623,176.522268128), test loss: 194.505589294\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (8.74130630493,38.3444992189), test loss: 62.6198474884\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (239.423706055,176.3962098), test loss: 176.39405632\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (53.7545700073,38.0765021071), test loss: 50.8866599083\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (160.876754761,176.253716139), test loss: 183.529689026\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (28.7876682281,37.8061295602), test loss: 64.2667324066\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (177.602813721,176.116501694), test loss: 187.932608795\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (8.02035427094,37.5372462984), test loss: 58.2920557022\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (137.928619385,175.98877915), test loss: 172.37780571\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (5.90231513977,37.2719276018), test loss: 52.2860972404\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (216.450653076,175.852212948), test loss: 182.738483047\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (4.15511369705,37.007533099), test loss: 64.6739071369\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (87.3549880981,175.713487123), test loss: 189.431868744\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (14.1295089722,36.7450265122), test loss: 53.7842986107\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (74.2622528076,175.576052814), test loss: 171.751707077\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (7.42433834076,36.4834643128), test loss: 54.5741219044\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (139.642364502,175.435821141), test loss: 186.632629395\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (14.4294338226,36.2238494798), test loss: 65.1203186512\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (236.503326416,175.309992821), test loss: 183.745400238\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (31.8484840393,35.9683737807), test loss: 51.785787487\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (201.429046631,175.170809578), test loss: 168.414649963\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (23.8967628479,35.7116097989), test loss: 56.7116589069\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (171.626693726,175.034203401), test loss: 186.214458084\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (6.29412889481,35.4560362665), test loss: 64.9241852283\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (132.377151489,174.906992011), test loss: 180.872167587\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (6.41117572784,35.2033062171), test loss: 53.1442960739\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (183.372512817,174.772150691), test loss: 172.127221298\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.10466480255,34.9513914281), test loss: 58.9596525431\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (94.0858840942,174.635211148), test loss: 185.820059967\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (8.12879180908,34.7007823986), test loss: 67.9104579926\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 2\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (296.793304443,inf), test loss: 211.191420364\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (402.496704102,inf), test loss: 269.647926331\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (174.547088623,191.123938042), test loss: 198.512982178\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (68.4837188721,124.433976135), test loss: 74.8814990044\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (80.7324295044,190.536385006), test loss: 212.88098793\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (66.4162750244,95.9830152564), test loss: 76.7485679626\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (116.798286438,190.077343933), test loss: 195.252454376\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (44.1422462463,86.1341857214), test loss: 65.5861195564\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (262.121337891,189.719974285), test loss: 194.731016159\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (70.8618011475,80.7745380843), test loss: 73.2357151985\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (118.712211609,189.403422299), test loss: 210.273511505\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (48.8709487915,77.3455887941), test loss: 72.2482847214\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (158.964630127,189.286817331), test loss: 195.337203598\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (74.3670501709,74.8532095002), test loss: 60.8143852234\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (282.919372559,189.047224909), test loss: 203.819490051\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (102.998664856,72.8426020235), test loss: 72.6413675308\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (141.829772949,188.859140804), test loss: 211.220199585\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (7.1937918663,71.1492957726), test loss: 64.0146530151\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (196.854431152,188.630903468), test loss: 189.14721756\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (45.5509185791,69.6967925612), test loss: 60.4176508427\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (78.844909668,188.444626311), test loss: 209.582858276\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (18.8317298889,68.4067562562), test loss: 69.9857772827\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (190.805114746,188.266262364), test loss: 204.373222733\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (31.836025238,67.2360290215), test loss: 57.2397632599\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (251.605163574,188.119481145), test loss: 191.336234665\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (105.556350708,66.1505900644), test loss: 58.2786728859\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (196.112243652,187.963598761), test loss: 207.729616928\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (6.95896244049,65.1472247862), test loss: 64.3670593262\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (205.782836914,187.804089594), test loss: 196.151805878\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (31.9719963074,64.1793706353), test loss: 52.6751534462\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (327.651428223,187.647480272), test loss: 192.383398819\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (93.4385147095,63.2791221007), test loss: 60.6438847542\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (217.944824219,187.51448401), test loss: 207.107695007\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (56.8644180298,62.4162053102), test loss: 59.6902023315\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (179.68927002,187.343826258), test loss: 190.221243668\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (23.3551750183,61.5867291791), test loss: 46.6722279549\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (158.326080322,187.18946145), test loss: 199.078399277\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (73.1070556641,60.792138593), test loss: 61.8651528358\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (143.582183838,187.014105043), test loss: 205.294535828\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (25.9295940399,60.0295002894), test loss: 51.9319169044\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (132.654098511,186.85439999), test loss: 190.650074768\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (39.603717804,59.3092995656), test loss: 50.4700469017\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (160.02734375,186.702014979), test loss: 204.823426056\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (20.1975402832,58.6199003286), test loss: 60.1570063591\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (301.685302734,186.573004721), test loss: 201.538694382\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (173.865570068,57.9777176881), test loss: 47.0080066204\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (161.344604492,186.414948079), test loss: 190.083383942\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (26.0435180664,57.3444432535), test loss: 50.4804875374\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (133.521789551,186.262485049), test loss: 202.542358398\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (26.9024200439,56.7364267332), test loss: 57.6044914722\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (216.326812744,186.121854929), test loss: 197.678936386\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (22.2314548492,56.1577401699), test loss: 46.5273846149\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (98.575958252,185.979013835), test loss: 188.807751465\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (59.255897522,55.6006509928), test loss: 49.2277573109\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (239.67678833,185.832798664), test loss: 204.94680748\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (19.8831119537,55.053371535), test loss: 56.3904445171\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (207.498443604,185.682073761), test loss: 187.687315369\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (34.3181762695,54.5251461425), test loss: 43.9956725121\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (174.827163696,185.518538326), test loss: 191.172239304\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (27.1401844025,54.0084234026), test loss: 57.3634919167\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (79.39503479,185.360006606), test loss: 202.042470169\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (38.8502159119,53.5121299359), test loss: 55.8860055923\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (354.19720459,185.23130095), test loss: 191.364904785\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (44.7017059326,53.0296291564), test loss: 45.7261586666\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (251.471694946,185.092675101), test loss: 196.571019745\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (79.5596160889,52.5734567453), test loss: 55.1847812653\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (175.25718689,184.936654273), test loss: 203.920703888\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (25.5969734192,52.1124333852), test loss: 45.6284526825\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (151.356704712,184.788393406), test loss: 180.165510559\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (11.39661026,51.6639770012), test loss: 46.8812355518\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (47.7575950623,184.643234411), test loss: 201.54258194\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (23.5064334869,51.2318684292), test loss: 54.1868470669\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (147.32093811,184.507744742), test loss: 195.839395905\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (46.1458969116,50.8103097119), test loss: 44.621407938\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (239.488494873,184.370476514), test loss: 185.102011108\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (31.8495788574,50.3895535706), test loss: 47.4139477253\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (53.0990905762,184.213388831), test loss: 200.319060135\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (22.5013694763,49.979245017), test loss: 54.7086270332\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (168.069274902,184.064002916), test loss: 185.748596573\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (21.8203964233,49.573878811), test loss: 44.110174942\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (277.949890137,183.917124682), test loss: 184.412436295\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (42.7555770874,49.179113121), test loss: 52.7473417759\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (147.869354248,183.780548062), test loss: 200.175942993\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (11.6105852127,48.7899793047), test loss: 52.0125396967\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (142.531280518,183.642168295), test loss: 184.401675415\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (23.0452823639,48.4185968803), test loss: 40.4169094563\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (129.13130188,183.490819734), test loss: 193.017154694\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (63.9285392761,48.0434238383), test loss: 53.3892234802\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (136.457931519,183.346175335), test loss: 200.441121292\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (91.1017837524,47.674698258), test loss: 44.5009531021\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (406.717224121,183.215729162), test loss: 180.379132843\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (53.9952392578,47.3129447533), test loss: 44.6258049488\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (133.136505127,183.068795881), test loss: 196.431033325\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (13.2517986298,46.9572890971), test loss: 53.39978652\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (145.252914429,182.933606128), test loss: 194.786177826\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (51.1387481689,46.6044509297), test loss: 43.4316831112\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (143.834014893,182.781190667), test loss: 184.281022263\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (16.2926807404,46.2554222327), test loss: 47.0898472786\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (160.48866272,182.636084835), test loss: 195.245330429\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (20.5378246307,45.9102646308), test loss: 54.1117526054\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (179.274810791,182.493177637), test loss: 185.329202271\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (23.6112632751,45.5710442654), test loss: 44.1251701593\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (125.058197021,182.355726914), test loss: 182.87177124\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (33.7389640808,45.2359177369), test loss: 52.5469359875\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (98.4185333252,182.217330306), test loss: 193.950548935\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (20.521320343,44.9106206346), test loss: 50.7697421551\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (217.320892334,182.074465118), test loss: 178.413191223\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (16.8835601807,44.582740282), test loss: 40.017074275\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (187.914550781,181.931669626), test loss: 186.789613342\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (20.0274276733,44.2586436296), test loss: 53.7744791031\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (238.366119385,181.802865433), test loss: 196.256984711\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (41.4532623291,43.9391375544), test loss: 50.158944273\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (237.321029663,181.658932654), test loss: 182.489126968\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (37.1983795166,43.6227774015), test loss: 44.5004706383\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (136.170639038,181.520976899), test loss: 189.110828781\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (8.88635158539,43.3067665637), test loss: 53.4513271809\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (137.299667358,181.373129483), test loss: 192.589088821\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (18.6008205414,42.9942252708), test loss: 43.3173290253\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (216.012756348,181.23403327), test loss: 175.568694687\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (27.2415275574,42.6838663621), test loss: 46.6367194653\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (199.540435791,181.092121786), test loss: 189.740603256\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (18.2588806152,42.3759516548), test loss: 52.0539977312\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (103.880569458,180.952835093), test loss: 189.363271332\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (34.6534614563,42.0720168498), test loss: 43.6675339699\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (126.483665466,180.816950998), test loss: 179.150754929\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (15.9223566055,41.7726198057), test loss: 47.4225441933\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (202.321899414,180.678898392), test loss: 194.923765945\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (14.8669891357,41.4719881622), test loss: 52.2550745964\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (329.585449219,180.542523453), test loss: 177.620695496\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (36.4482879639,41.174523849), test loss: 43.0572395563\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (214.076705933,180.410409089), test loss: 178.097278595\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (24.6385726929,40.8786617705), test loss: 52.8809894562\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (181.08581543,180.268044191), test loss: 192.949348831\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (15.7575273514,40.5852012004), test loss: 52.2524507999\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (94.3193054199,180.129388217), test loss: 177.230107498\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (14.0028610229,40.2923236697), test loss: 42.5835660458\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (208.052947998,179.98806805), test loss: 185.563895035\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.85622501373,40.0013185113), test loss: 56.0487346172\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (77.3160171509,179.847170096), test loss: 194.193434906\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (7.57375240326,39.712544855), test loss: 46.7043118954\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (138.713378906,179.709614612), test loss: 172.6966362\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (7.22695970535,39.425142804), test loss: 47.3490314007\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (364.663330078,179.578719223), test loss: 188.987773132\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (68.3599853516,39.1425541979), test loss: 54.8136966228\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (211.864151001,179.440112578), test loss: 185.857277298\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (6.43676805496,38.8593962932), test loss: 44.2021053553\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (145.655395508,179.302295327), test loss: 174.822208786\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (7.80703306198,38.5776353397), test loss: 49.6578631401\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (155.476623535,179.167492563), test loss: 189.451150513\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (12.805273056,38.2982786814), test loss: 54.1626610279\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (72.6652450562,179.034404416), test loss: 177.811094284\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (22.0587234497,38.0203390608), test loss: 45.2235421658\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (197.674957275,178.897725623), test loss: 175.547003937\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (5.83531570435,37.7436764136), test loss: 56.8742322445\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (171.730010986,178.760617749), test loss: 188.079220963\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (28.2937965393,37.4688873771), test loss: 53.8053501129\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (102.592803955,178.619734696), test loss: 173.246984482\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (14.1451015472,37.1944927258), test loss: 44.7130513191\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (119.645172119,178.481745186), test loss: 181.126867294\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (17.2071342468,36.9221302423), test loss: 59.8757605076\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 3\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (296.779418945,inf), test loss: 201.311745453\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (402.432128906,inf), test loss: 267.660272598\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (141.449996948,186.339577301), test loss: 181.196311188\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (41.0784988403,119.038715597), test loss: 70.4891854286\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (142.476272583,185.651058399), test loss: 191.371014023\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (207.596115112,91.7963182392), test loss: 76.9003182411\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (230.207717896,185.23974026), test loss: 195.105771446\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (108.23034668,82.1614205383), test loss: 76.313125515\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (195.667785645,184.874811829), test loss: 188.239997482\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (73.9243927002,77.0211682634), test loss: 66.6830727577\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (203.958374023,184.660927859), test loss: 181.938166046\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (46.0234909058,73.7479000051), test loss: 71.0292800903\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (163.128707886,184.393828752), test loss: 198.233035851\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (34.8943595886,71.318837458), test loss: 73.4612401962\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (139.564910889,184.212247471), test loss: 186.266370773\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (32.1087646484,69.4372889034), test loss: 61.5063426495\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (140.193740845,184.032178964), test loss: 184.34924736\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (32.6228637695,67.8289097348), test loss: 62.4890328407\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (359.821563721,183.873277196), test loss: 190.440906525\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (66.686088562,66.4703387754), test loss: 69.5200141907\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (209.746261597,183.679262563), test loss: 190.068456268\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (42.9062576294,65.2380591716), test loss: 57.3609679222\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (100.352111816,183.541088307), test loss: 175.73553772\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (55.024597168,64.1187802553), test loss: 57.4161128044\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (195.72177124,183.370477028), test loss: 188.985327339\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (23.7244796753,63.0521155178), test loss: 67.3093796253\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (167.523223877,183.187673746), test loss: 190.163191223\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (21.0267715454,62.0604603734), test loss: 58.1263044357\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (58.5806274414,183.031007651), test loss: 182.782808685\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (36.4033241272,61.1261242914), test loss: 54.0918591499\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (184.717803955,182.908273069), test loss: 184.705456543\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (65.2101898193,60.2362612104), test loss: 65.7721636772\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (46.3422966003,182.749912812), test loss: 192.615991592\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (39.5989723206,59.3805111187), test loss: 62.5912326813\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (145.882888794,182.573966989), test loss: 181.807859039\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (48.4351730347,58.5659452998), test loss: 49.9865777969\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (217.233917236,182.443620674), test loss: 180.580470657\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (13.9147062302,57.7938242234), test loss: 59.4482292175\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (256.334472656,182.300015769), test loss: 191.696462631\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (71.8149032593,57.0533571566), test loss: 61.3293622971\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (184.642120361,182.154514966), test loss: 183.973484421\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (75.0018997192,56.3475003709), test loss: 48.3999077797\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (166.815429688,181.975801415), test loss: 178.381170654\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (77.8570022583,55.667785572), test loss: 51.8974321842\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (111.464782715,181.847125781), test loss: 184.618156052\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (18.8862476349,55.0302965736), test loss: 59.6824189663\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (207.409301758,181.707294895), test loss: 190.894236374\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (23.2619113922,54.4092956207), test loss: 49.4107509613\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (147.215682983,181.567995154), test loss: 174.470039749\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (6.24782037735,53.8178348312), test loss: 49.2094231129\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (132.722442627,181.401444302), test loss: 183.357485199\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (26.1604881287,53.2439321453), test loss: 62.2426276684\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (141.793807983,181.272097157), test loss: 187.694555092\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (28.5728969574,52.7075234836), test loss: 55.6120420456\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (189.346633911,181.113987788), test loss: 182.057675934\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (9.51646518707,52.1742553161), test loss: 47.4450443745\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (247.19128418,180.992021218), test loss: 176.130489349\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (23.2872276306,51.6704467961), test loss: 58.2537258625\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (161.860916138,180.836196729), test loss: 192.03271904\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (23.0790958405,51.1737822129), test loss: 58.3653110981\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (279.836975098,180.693646505), test loss: 180.236093903\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (102.747413635,50.6987974567), test loss: 44.407110405\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (102.965667725,180.538015303), test loss: 176.275061989\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (39.8496780396,50.2385677336), test loss: 49.5477041245\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (196.269683838,180.40994884), test loss: 183.455257034\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (25.6479854584,49.7919386677), test loss: 56.9893541813\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (180.085250854,180.262278062), test loss: 187.163275146\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (22.6779613495,49.3479653462), test loss: 45.9361559868\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (97.831817627,180.120357051), test loss: 168.563128281\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (52.735004425,48.9214557863), test loss: 46.7223982334\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (185.722366333,179.976239263), test loss: 182.080478668\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (16.6778144836,48.5076762048), test loss: 58.1394926548\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (159.283508301,179.836455547), test loss: 185.735519791\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (19.6408481598,48.1074858771), test loss: 53.1825265646\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (115.581008911,179.694499285), test loss: 177.190006256\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (21.100402832,47.7063652759), test loss: 45.3466803074\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (174.619049072,179.558155073), test loss: 173.796347046\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (13.7680025101,47.3166864737), test loss: 56.6498155594\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (201.726425171,179.413578004), test loss: 185.127791595\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (12.7564725876,46.9401631417), test loss: 54.0579782963\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (69.4967575073,179.276578757), test loss: 173.676776886\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (51.050163269,46.5712753793), test loss: 41.9033658981\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (158.444702148,179.137350933), test loss: 173.022480011\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (24.3258781433,46.2026539439), test loss: 47.8250723362\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (156.430480957,178.990994577), test loss: 184.064248657\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (38.4173545837,45.8420695187), test loss: 55.6381337166\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (216.047622681,178.853296421), test loss: 179.410442352\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (36.8399162292,45.4928484362), test loss: 42.5396734238\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (217.297073364,178.72478116), test loss: 173.568495941\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (43.5533218384,45.1477699603), test loss: 46.02234025\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (108.105300903,178.57952974), test loss: 178.641263962\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (19.9001045227,44.8049780821), test loss: 55.6035711765\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (221.20223999,178.435340457), test loss: 183.183659554\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (20.2179355621,44.4661564774), test loss: 45.3970967293\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (123.696533203,178.300622985), test loss: 169.733009338\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (27.3633728027,44.137109373), test loss: 44.2571810246\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (262.751403809,178.166671034), test loss: 175.776052475\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (23.5627403259,43.8103070882), test loss: 56.6391429424\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (252.451568604,178.032350432), test loss: 181.272335815\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (24.8992004395,43.4882309545), test loss: 53.254618454\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (277.030578613,177.882715402), test loss: 174.951665497\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (31.7944355011,43.16675949), test loss: 43.7914646149\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (145.640228271,177.748697847), test loss: 171.721026993\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (18.8801574707,42.8539590977), test loss: 53.7877188206\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (107.176048279,177.611625019), test loss: 185.556916809\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (13.9158821106,42.5415969985), test loss: 53.4929509163\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (116.428604126,177.478488349), test loss: 175.10738678\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (16.1119422913,42.2335377333), test loss: 42.7411280632\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (90.5591888428,177.330540094), test loss: 168.475523376\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (18.6553287506,41.9259007079), test loss: 46.2556706429\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (114.125068665,177.202445351), test loss: 176.810502243\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (18.3658885956,41.6265474147), test loss: 53.8267821312\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (293.259216309,177.065020979), test loss: 180.643642044\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (55.7739562988,41.3261869201), test loss: 42.400335598\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (158.144592285,176.93349876), test loss: 161.856538963\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (41.9322357178,41.0294478455), test loss: 45.2619766712\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (59.5944442749,176.789001049), test loss: 175.97688427\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (6.56812524796,40.7322543087), test loss: 57.0090449333\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (237.867095947,176.65923429), test loss: 179.177364731\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (76.8134307861,40.442121713), test loss: 50.2028187275\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (210.435302734,176.5159538), test loss: 171.931354141\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (22.1529312134,40.1505497274), test loss: 43.25246315\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (236.753692627,176.388345802), test loss: 167.493276596\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (12.4521102905,39.8629182171), test loss: 55.6159941673\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (250.322814941,176.250504152), test loss: 183.046300507\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (45.914390564,39.5756556994), test loss: 54.1683359623\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (103.370727539,176.111483923), test loss: 170.629080963\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (38.857509613,39.2906978644), test loss: 41.2044579506\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (161.141036987,175.976289069), test loss: 167.352607346\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (48.2912750244,39.0092980076), test loss: 47.294179821\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (239.979309082,175.845102611), test loss: 175.882038116\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (24.638874054,38.729185492), test loss: 54.7213784695\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (187.536102295,175.70834364), test loss: 174.353410339\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (15.1674118042,38.4487724758), test loss: 42.6948207855\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (182.878860474,175.576323075), test loss: 161.962516594\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (15.6795310974,38.1709629528), test loss: 45.3647853851\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (166.821304321,175.438231303), test loss: 172.964439011\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (10.5343780518,37.8962441104), test loss: 56.8262331009\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (104.331985474,175.305842844), test loss: 176.277997208\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (19.6721191406,37.6231284794), test loss: 45.2985389233\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (130.908996582,175.172593622), test loss: 165.935816002\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (4.85124778748,37.349552789), test loss: 45.7161502361\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (320.714233398,175.039667164), test loss: 170.211508942\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (27.2400932312,37.0785153038), test loss: 56.9706893921\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (226.594573975,174.903149835), test loss: 176.63102951\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (11.3594293594,36.8101839793), test loss: 53.1330452442\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (121.530982971,174.775112377), test loss: 164.774685669\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (7.90408372879,36.542510606), test loss: 42.0901265621\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (172.617553711,174.640224455), test loss: 165.910836029\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (6.60493373871,36.2763803312), test loss: 56.8906068325\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (164.300537109,174.503625703), test loss: 177.58948555\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (8.11198425293,36.0109914244), test loss: 56.0513662338\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (61.6750411987,174.370228454), test loss: 168.794013977\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (14.2048244476,35.7491928816), test loss: 43.9246631145\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (174.528305054,174.2436069), test loss: 161.922747421\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (25.911315918,35.4878278347), test loss: 47.6508031368\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (98.8734054565,174.110554202), test loss: 171.296092224\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (6.95620250702,35.2281394207), test loss: 56.3689378738\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (153.165863037,173.972637788), test loss: 175.55897789\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (12.9142084122,34.9683030983), test loss: 44.7093193531\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 4\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (296.849365234,inf), test loss: 201.069592285\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (402.524414062,inf), test loss: 256.869826126\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (236.776275635,188.052525589), test loss: 182.667690086\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (81.5170288086,120.807409372), test loss: 64.0868458748\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (182.110305786,187.449075363), test loss: 202.063677597\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (24.5910758972,92.6368351603), test loss: 73.4024840832\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (151.974334717,186.951757009), test loss: 186.378045273\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (44.9465866089,82.9725200634), test loss: 60.7715150833\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (86.028213501,186.555693798), test loss: 177.604187012\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (45.9224243164,77.7569187691), test loss: 65.9296970367\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (151.229553223,186.323776932), test loss: 198.877880478\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (44.4122848511,74.44479524), test loss: 69.6883347511\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (216.929092407,186.163913809), test loss: 185.49278717\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (21.0181694031,72.078642794), test loss: 56.1973142624\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (209.371292114,185.991433763), test loss: 187.092123032\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (59.0467910767,70.2279314624), test loss: 65.7222634315\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (161.360473633,185.76374026), test loss: 202.294755173\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (60.7025680542,68.6699723549), test loss: 61.4555515289\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (108.360313416,185.556675735), test loss: 174.065714645\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (34.0305557251,67.3091274268), test loss: 54.6405997276\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (195.056396484,185.349456681), test loss: 192.578163338\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (20.2194442749,66.1107428884), test loss: 65.3988262177\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (137.100952148,185.164682887), test loss: 194.536837006\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (38.7323799133,65.0232730835), test loss: 55.2579116821\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (170.960144043,184.992939212), test loss: 177.549411774\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (109.322113037,64.0558883308), test loss: 53.5778402328\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (301.895324707,184.857593131), test loss: 197.79654274\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (56.6614685059,63.1175734623), test loss: 65.6463699341\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (254.545669556,184.710708699), test loss: 183.301751328\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (130.837051392,62.2649030169), test loss: 51.4597808838\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (225.414840698,184.548513255), test loss: 175.990392303\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (51.7692375183,61.4267042293), test loss: 60.7123912811\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (139.068786621,184.389223265), test loss: 196.855672836\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (47.8473625183,60.6378259534), test loss: 62.0224984169\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (318.623657227,184.249765969), test loss: 181.220499039\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (84.973815918,59.8971847118), test loss: 48.4393317223\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (145.184082031,184.112174742), test loss: 183.108678436\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (49.9327774048,59.1841559086), test loss: 60.5522888184\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (281.09362793,183.954206068), test loss: 196.978250885\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (67.0513687134,58.4980732547), test loss: 51.6488645077\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (123.10218811,183.79581171), test loss: 171.344975471\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (17.5344810486,57.82776698), test loss: 49.1573232174\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (103.19896698,183.621420794), test loss: 190.573773193\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (37.0327987671,57.1877903626), test loss: 62.3578554153\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (180.15852356,183.464259033), test loss: 190.553000641\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (39.6656837463,56.5695049209), test loss: 50.5001039505\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (117.504058838,183.313733608), test loss: 173.701773071\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (47.6787872314,55.9866887609), test loss: 52.9635734558\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (123.296401978,183.166284834), test loss: 195.71747818\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (12.9359350204,55.4187904275), test loss: 63.1651901245\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (281.225646973,183.028038001), test loss: 178.99831543\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (112.56098938,54.8783819727), test loss: 46.5484484196\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (241.993057251,182.881449609), test loss: 176.615852737\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (7.86463356018,54.3545316954), test loss: 60.5385120869\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (224.907165527,182.734562843), test loss: 191.507214355\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (11.4899597168,53.8425082195), test loss: 53.7913546324\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (201.86605835,182.586525018), test loss: 176.733163071\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (13.9186410904,53.3487800361), test loss: 46.4662673235\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (254.10736084,182.462522609), test loss: 182.149704742\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (47.5660629272,52.8723922357), test loss: 59.1572208643\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (180.092437744,182.305514907), test loss: 192.364093781\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (35.7381286621,52.4066570069), test loss: 49.2679009438\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (192.753234863,182.165273305), test loss: 168.635320282\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (24.6100521088,51.9469293676), test loss: 48.3381268501\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (129.851242065,182.007134518), test loss: 187.851487732\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (18.2762985229,51.4956761463), test loss: 62.2879636288\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (121.836914062,181.85031275), test loss: 183.982544708\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (25.064617157,51.0567741609), test loss: 50.2579703331\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (75.5506286621,181.708523509), test loss: 171.423843765\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (15.8267307281,50.6333957211), test loss: 56.9629406929\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (235.109970093,181.564398002), test loss: 189.73171196\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (23.0384941101,50.2184117256), test loss: 62.299044323\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (101.249923706,181.41815163), test loss: 176.179125214\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (56.0742263794,49.8111930781), test loss: 45.982554245\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (143.54586792,181.278267835), test loss: 175.395557022\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (30.4756851196,49.4195413522), test loss: 59.7177528381\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (254.163879395,181.136821796), test loss: 190.13707943\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (23.2653102875,49.0269626917), test loss: 51.8286158562\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (124.789085388,180.991543987), test loss: 173.056797409\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (24.8766784668,48.642547323), test loss: 46.5952715874\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (231.621124268,180.860482341), test loss: 182.843645287\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (64.2795791626,48.2660512358), test loss: 58.8184976101\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (100.711669922,180.71771992), test loss: 187.02301445\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (37.0291366577,47.8951103153), test loss: 48.7755977154\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (234.39956665,180.577625261), test loss: 168.141473389\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (13.018576622,47.5265223077), test loss: 48.8919620037\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (124.99899292,180.427389042), test loss: 185.233864975\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (25.7049179077,47.16293423), test loss: 61.6251100063\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (201.512207031,180.278957902), test loss: 177.557207108\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (6.53876686096,46.8060745535), test loss: 51.0223614216\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (261.539306641,180.141656155), test loss: 168.367447281\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (50.8836860657,46.4579773064), test loss: 57.1960483551\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (183.725616455,179.993608397), test loss: 184.896715927\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (38.1210479736,46.1132995873), test loss: 61.4484678745\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (178.269561768,179.855198019), test loss: 173.778300858\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (8.37930202484,45.772694031), test loss: 46.4356337547\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (136.524291992,179.715648673), test loss: 174.261081123\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (20.4666786194,45.444059193), test loss: 59.51448102\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (127.890670776,179.572689173), test loss: 187.904449844\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (51.3257637024,45.1128949116), test loss: 52.0000814915\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (121.802909851,179.433191098), test loss: 167.108040619\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (19.6492958069,44.7839525416), test loss: 47.5395068169\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (160.06439209,179.300464939), test loss: 178.915854645\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (9.74974060059,44.4619141603), test loss: 59.6595723152\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (106.761039734,179.165474376), test loss: 184.066246796\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (22.1734046936,44.14467779), test loss: 48.9605350971\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (156.721908569,179.021577594), test loss: 166.212532043\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (14.7759990692,43.8293402888), test loss: 50.0255581379\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (145.07409668,178.880399125), test loss: 183.450227165\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (12.8830337524,43.5156052769), test loss: 61.7967361927\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (130.29750061,178.733146767), test loss: 175.005067444\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (24.6576080322,43.2060257244), test loss: 52.4644417286\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (219.355911255,178.594437206), test loss: 165.172283554\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (44.8976364136,42.9001067892), test loss: 58.7557504654\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (201.968170166,178.45271456), test loss: 183.321966743\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (19.9197731018,42.5980446189), test loss: 62.5289847851\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (260.6378479,178.316929923), test loss: 171.028532028\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (43.8208694458,42.2985062418), test loss: 47.8678691864\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (201.038955688,178.180792641), test loss: 170.714598846\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (39.3286972046,42.0055070064), test loss: 60.816588974\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (189.12588501,178.041267591), test loss: 185.403524017\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (15.4051437378,41.708753441), test loss: 51.73499856\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (147.504302979,177.903157737), test loss: 163.033150101\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (7.43818664551,41.4151974581), test loss: 48.8351540565\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (298.858612061,177.769324172), test loss: 175.00434227\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (39.5551757812,41.1259676446), test loss: 60.5893611908\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (121.415672302,177.636052661), test loss: 180.745342255\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (15.3590097427,40.838906191), test loss: 50.1762588978\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (269.798400879,177.496446279), test loss: 163.29713707\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (27.7761993408,40.5537914945), test loss: 51.535484314\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (121.719535828,177.357763939), test loss: 180.260816956\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (14.3828887939,40.2687085782), test loss: 62.4541769981\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (123.287307739,177.213625963), test loss: 171.413009262\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (8.28649520874,39.9865735332), test loss: 53.0408455849\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (191.378738403,177.07560524), test loss: 160.831092453\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (8.83150959015,39.7060052132), test loss: 61.2988019943\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (92.4540939331,176.9363989), test loss: 180.445912552\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (16.0464553833,39.4287518482), test loss: 63.568554306\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (114.653427124,176.800658164), test loss: 170.218676758\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (13.6397686005,39.1518778773), test loss: 50.0220450878\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (262.448455811,176.667549307), test loss: 168.773756027\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (48.3447608948,38.8789813975), test loss: 61.9947831631\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (204.981781006,176.531235348), test loss: 182.774905777\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (8.14056777954,38.6063743756), test loss: 52.4784206867\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (217.518951416,176.39698958), test loss: 159.64938736\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (18.648525238,38.3349286132), test loss: 50.7447692871\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (185.954040527,176.259581049), test loss: 174.493434143\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (9.73286247253,38.0648551612), test loss: 62.7286550522\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (213.920623779,176.132817157), test loss: 176.805610847\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (31.6908588409,37.7969089154), test loss: 51.9087327957\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (150.162200928,175.992245733), test loss: 161.639113045\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (12.9739151001,37.5304299213), test loss: 54.263242054\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (178.371246338,175.859561168), test loss: 178.661925888\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (11.2769317627,37.2634989001), test loss: 66.8663181305\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (98.6312866211,175.71887013), test loss: 166.117877579\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (7.71905899048,36.9966787459), test loss: 55.4470600128\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (151.710067749,175.580262746), test loss: 157.859952736\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (21.964849472,36.7295916083), test loss: 64.2824082375\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (95.3457183838,175.446487872), test loss: 176.98008728\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (14.9746875763,36.4628632306), test loss: 66.2830183506\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 5\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (296.958892822,inf), test loss: 198.447028351\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (402.557861328,inf), test loss: 259.331174088\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (242.719314575,188.150486122), test loss: 177.345277023\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (144.655151367,122.60830261), test loss: 66.8438072205\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (154.047592163,187.410056164), test loss: 198.738465118\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (73.3025512695,94.7517293308), test loss: 66.8158902168\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (135.748291016,186.911926633), test loss: 178.563114929\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (36.0154190063,84.999475149), test loss: 57.8559240818\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (193.486541748,186.604189234), test loss: 192.89896965\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (35.447479248,79.8309902064), test loss: 66.8886512756\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (149.061691284,186.345755385), test loss: 187.9626194\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (37.3925323486,76.4630744037), test loss: 58.8454519272\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (129.934341431,186.127393796), test loss: 178.05945282\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (40.0031280518,73.9977130002), test loss: 64.7511289597\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (97.5055084229,185.944486621), test loss: 200.484543228\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (67.9223022461,72.0414822811), test loss: 61.2372922897\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (225.372299194,185.775494219), test loss: 176.917349243\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (57.4159393311,70.3854911097), test loss: 53.7977534294\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (166.16217041,185.584559792), test loss: 193.414826965\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (78.7476348877,68.9368925154), test loss: 63.7805426598\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (138.204711914,185.410636709), test loss: 185.124880981\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.07756328583,67.6321304765), test loss: 51.4432231903\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (142.957336426,185.248638135), test loss: 179.838895035\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (33.7322158813,66.4478310522), test loss: 60.1844525337\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (246.011413574,185.088452734), test loss: 195.065154648\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (50.5199584961,65.3374094386), test loss: 52.3965585709\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (247.375610352,184.917754232), test loss: 171.89079361\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (30.9454631805,64.294501706), test loss: 49.7698652267\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (182.194290161,184.746506572), test loss: 193.266506195\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (29.6082782745,63.3128028897), test loss: 62.2154166222\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (249.491027832,184.586025591), test loss: 183.375156021\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (71.0528106689,62.3817410633), test loss: 46.4990457058\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (121.048858643,184.424141952), test loss: 178.408408546\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (19.952337265,61.4918119313), test loss: 58.277196312\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (161.900817871,184.279576723), test loss: 194.159346008\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (35.6559906006,60.6520525885), test loss: 50.2879389286\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (87.3882827759,184.131455027), test loss: 172.333567047\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (61.9416656494,59.8507826117), test loss: 49.9546563148\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (209.830047607,183.98919981), test loss: 194.169610596\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (62.3791046143,59.0853340039), test loss: 59.0224733353\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (172.819549561,183.835054379), test loss: 180.861206055\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (48.5685691833,58.3558984204), test loss: 46.5756701469\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (335.96673584,183.684749624), test loss: 182.391247559\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (87.1819534302,57.661839636), test loss: 58.1488400459\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (97.9297790527,183.518510552), test loss: 190.316135788\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (49.4047088623,56.9941182477), test loss: 48.9945261478\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (223.855804443,183.377101071), test loss: 170.375700378\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (19.2098693848,56.3597121333), test loss: 55.5598155975\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (308.231323242,183.224531181), test loss: 193.642467499\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (89.0550613403,55.7544326018), test loss: 54.9086970329\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (94.8904190063,183.067871657), test loss: 173.092573166\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (14.9998664856,55.167229872), test loss: 47.7474345684\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (213.634963989,182.929206766), test loss: 183.480397797\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (20.9657764435,54.6076858092), test loss: 59.118857789\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (149.766082764,182.782103428), test loss: 182.499528122\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (114.198677063,54.0676879994), test loss: 47.8509944439\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (162.337219238,182.638599924), test loss: 168.881640244\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (19.1384887695,53.5391122727), test loss: 57.8545859575\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (194.157485962,182.496006626), test loss: 193.150421524\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (28.5699615479,53.0341663326), test loss: 49.5365458488\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (221.56098938,182.350622249), test loss: 171.238219452\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (26.646692276,52.5441536973), test loss: 47.8895113468\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (236.802276611,182.20139095), test loss: 186.339236832\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (68.0584259033,52.0679161829), test loss: 59.1078765869\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (52.4508361816,182.054603528), test loss: 178.798012924\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (38.0902175903,51.6024651362), test loss: 44.7886041164\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (192.045623779,181.918535434), test loss: 173.182942963\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (38.5885925293,51.1512431691), test loss: 57.3631223202\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (195.969772339,181.772901466), test loss: 189.421224213\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.64590930939,50.7109560112), test loss: 46.2815560818\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (101.653244019,181.628943675), test loss: 166.47226944\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (16.8127059937,50.28294198), test loss: 48.0365188122\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (146.327972412,181.491171682), test loss: 185.441868973\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (18.2315769196,49.864002695), test loss: 58.7127667427\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (235.773376465,181.350971034), test loss: 176.911705399\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (74.5567474365,49.4537795317), test loss: 44.4991345406\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (369.903747559,181.20573372), test loss: 173.194297981\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (163.678375244,49.0479568966), test loss: 56.7823455811\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (129.88293457,181.054374339), test loss: 187.068846512\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (32.7526435852,48.6469254598), test loss: 47.1848540545\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (97.2914886475,180.915665279), test loss: 168.22824173\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (23.5214538574,48.2586920701), test loss: 49.423513031\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (176.750823975,180.777430577), test loss: 187.48037529\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (14.2190704346,47.8792669916), test loss: 58.3916405678\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (288.106689453,180.635348343), test loss: 175.218809509\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (50.004486084,47.5067271226), test loss: 44.6292896748\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (160.5519104,180.489109637), test loss: 175.044855881\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (10.9214267731,47.1381721104), test loss: 56.2399097919\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (208.123123169,180.349500013), test loss: 186.360957718\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (20.092124939,46.7778057445), test loss: 48.2354463577\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (162.159469604,180.207528487), test loss: 164.950358963\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (19.1261863708,46.4222864775), test loss: 55.2549772263\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (242.740661621,180.067423237), test loss: 187.585710526\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (34.5033683777,46.071552843), test loss: 57.3796678543\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (89.7959976196,179.92556624), test loss: 168.507592773\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (26.1629600525,45.7257480424), test loss: 46.5604428768\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (78.1298446655,179.790462175), test loss: 175.849034119\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (11.9740562439,45.384776539), test loss: 55.6923323154\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (96.9423904419,179.655639481), test loss: 178.106197357\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (7.94471645355,45.0486008999), test loss: 47.7274324417\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (241.991790771,179.518809174), test loss: 162.62579155\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (45.9096412659,44.7168260837), test loss: 56.4873207569\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (144.074890137,179.375018765), test loss: 186.383411789\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (24.8375339508,44.3864383107), test loss: 49.7260091782\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (178.667861938,179.238076316), test loss: 163.741246223\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (23.6409263611,44.061384883), test loss: 47.5083339214\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (110.930923462,179.099029839), test loss: 179.225831604\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (21.4074077606,43.7400634314), test loss: 57.9987428665\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (161.709777832,178.963271466), test loss: 174.282028961\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (13.8939914703,43.4217738971), test loss: 45.7414417744\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (183.322341919,178.826156766), test loss: 165.881811523\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (9.85425186157,43.1062277845), test loss: 57.2755803585\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (93.092300415,178.687907163), test loss: 186.150602341\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (19.0963993073,42.7943167884), test loss: 47.4266412258\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (104.7554245,178.553608914), test loss: 163.230693054\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (12.6652908325,42.4844545541), test loss: 48.1197036982\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (224.619262695,178.41807119), test loss: 179.024076462\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (38.654914856,42.1775652525), test loss: 58.325056839\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (137.364013672,178.278743756), test loss: 170.968768311\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (15.7995758057,41.8721388495), test loss: 45.8272562027\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (136.763397217,178.143021029), test loss: 166.7437603\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (7.38644361496,41.5696512586), test loss: 56.9546364784\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (155.594573975,178.007404954), test loss: 181.93684082\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (37.9379119873,41.2698202677), test loss: 46.97851367\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (190.282592773,177.870371097), test loss: 161.474142838\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (19.6135826111,40.9712544733), test loss: 49.9554425716\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (219.046813965,177.732521983), test loss: 181.740563011\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (8.82656478882,40.6750039143), test loss: 59.206223774\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (175.430511475,177.59449223), test loss: 171.150073624\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (16.0518302917,40.3813070269), test loss: 45.2257034302\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (222.2840271,177.458026845), test loss: 169.745393372\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (26.3738002777,40.0892112579), test loss: 56.3810441971\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (101.853637695,177.320656296), test loss: 180.433035278\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (11.1883459091,39.798421307), test loss: 48.3448198795\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (114.256744385,177.186633966), test loss: 159.974649811\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (18.7381324768,39.5100980486), test loss: 55.6564918041\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (108.351486206,177.053283441), test loss: 180.620789719\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (32.0567092896,39.2230550144), test loss: 57.1575169563\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (201.40927124,176.919843032), test loss: 165.921856689\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (24.5630149841,38.9374716706), test loss: 48.0944033623\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (184.709472656,176.783886283), test loss: 170.906303787\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (23.3720283508,38.6542344935), test loss: 56.7454532623\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (355.273529053,176.648274694), test loss: 175.127960205\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (36.8286705017,38.3726654899), test loss: 48.5753112555\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (85.5176239014,176.508335165), test loss: 157.183796692\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (12.7762260437,38.0923768065), test loss: 58.5365451813\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (195.43649292,176.375479301), test loss: 178.762680435\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (8.80659008026,37.8148299933), test loss: 50.6023002625\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (284.207641602,176.240422042), test loss: 158.549856567\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (25.7455444336,37.5390128446), test loss: 49.5337828636\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (90.568572998,176.103237509), test loss: 173.017756271\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (7.12083911896,37.2646075664), test loss: 58.9184165001\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (162.37562561,175.971267819), test loss: 168.662478638\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (6.4673166275,36.9925508209), test loss: 49.9832283974\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (132.232406616,175.837076065), test loss: 159.932996941\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (21.0791568756,36.7220088784), test loss: 60.2297777176\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (105.930328369,175.703911514), test loss: 181.430701828\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (5.2552690506,36.4532669307), test loss: 50.304562521\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (183.526443481,175.572327499), test loss: 157.937602615\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (8.64208221436,36.1869319972), test loss: 51.2176721811\n",
      "run time for single CV loop: 2778.75273895\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 1\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (277.770751953,inf), test loss: 195.304706192\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (394.567871094,inf), test loss: 269.281389618\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (70.41015625,125.217446924), test loss: 67.2517824173\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (116.939407349,192.433445229), test loss: 152.983062744\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (116.550094604,84.5050367775), test loss: 45.8294146538\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (299.86227417,138.678612827), test loss: 84.4359285355\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (43.8662033081,69.6327201764), test loss: 37.3299725533\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (34.9345436096,113.679142138), test loss: 64.2131826401\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (22.539888382,62.0486061966), test loss: 44.8429298401\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (46.3954086304,100.967428311), test loss: 76.0051642418\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (44.9370422363,57.4417760525), test loss: 43.9633068085\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (80.4204101562,93.2289014469), test loss: 77.0619872093\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (52.9341011047,54.2561014717), test loss: 37.1491262436\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (42.0873641968,87.895302651), test loss: 62.3276805878\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (43.2676086426,51.8709955352), test loss: 44.3733007431\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (45.1344985962,83.9826584082), test loss: 76.8045290947\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (62.2627067566,50.0259804248), test loss: 40.4702526093\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (141.722808838,80.9688389728), test loss: 73.3752814293\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (29.6093997955,48.5248592815), test loss: 37.9511761189\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (38.7010993958,78.535401812), test loss: 63.1780944824\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (90.8375701904,47.2785099963), test loss: 43.7794506073\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (241.86505127,76.5362603813), test loss: 75.3925289154\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (38.2844734192,46.1987912649), test loss: 39.0197762012\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (33.0615768433,74.7839026181), test loss: 64.8678434372\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (21.0218048096,45.2493800533), test loss: 38.1303246975\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (37.8375473022,73.2587741587), test loss: 62.3415078163\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (39.8617172241,44.4175811623), test loss: 43.6689901829\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (72.1351776123,71.915348379), test loss: 73.4845341682\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (39.6011390686,43.6533061843), test loss: 36.0325844765\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (37.6059913635,70.6823149588), test loss: 60.1920329571\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (38.4195327759,42.9340083819), test loss: 37.5101108551\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (47.50050354,69.5499073453), test loss: 60.6999658585\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (52.8138198853,42.2694607069), test loss: 42.0633953094\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (117.851699829,68.5051538917), test loss: 70.6268601418\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (25.7427787781,41.6440775483), test loss: 34.1358395576\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (30.8356723785,67.5278400606), test loss: 57.4463378429\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (66.0203399658,41.0570105085), test loss: 38.0425477028\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (200.305053711,66.6196800389), test loss: 60.8250261307\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (34.0064048767,40.4936256816), test loss: 39.4248976231\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (30.7824935913,65.7319588105), test loss: 69.7716108322\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (21.5623092651,39.9543534852), test loss: 32.520235467\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (30.6094360352,64.8843573411), test loss: 56.2434151649\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (28.0100574493,39.4472175326), test loss: 38.2904929876\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (36.711605072,64.0788400352), test loss: 63.6232416153\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (29.1429595947,38.9572672405), test loss: 37.4016944408\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (35.5846672058,63.2953010738), test loss: 68.3645875931\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (29.8607196808,38.4756808518), test loss: 30.3988276482\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (44.164730072,62.5351045425), test loss: 53.0185914516\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (40.5808334351,38.0146447607), test loss: 38.9240491867\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (93.7642059326,61.8034911044), test loss: 66.6891954422\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (14.821896553,37.5706763596), test loss: 36.2784331322\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (15.8902606964,61.0985471367), test loss: 66.9411494732\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (53.0774345398,37.1464860564), test loss: 30.8493483543\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (177.932098389,60.4290804793), test loss: 51.2785937309\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (33.1726531982,36.7358734689), test loss: 39.5992180109\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (53.0223617554,59.7703945018), test loss: 68.7948592186\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (21.9969787598,36.3393960565), test loss: 34.1291848183\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (24.7795372009,59.1303477717), test loss: 64.0772507191\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (22.1916446686,35.9647530425), test loss: 33.6915287256\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (20.2107372284,58.5209737793), test loss: 54.1150974751\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (21.5935382843,35.6033233119), test loss: 39.6313999414\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (29.59765625,57.9287476103), test loss: 68.8538631916\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (24.1607894897,35.2475630113), test loss: 32.8906113148\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (35.7108688354,57.3531870309), test loss: 55.6131336689\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (32.3881530762,34.9057328492), test loss: 35.0146859169\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (70.9429702759,56.798882029), test loss: 55.77811656\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (13.1702537537,34.5745326193), test loss: 39.6486570835\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (12.8567008972,56.2639246948), test loss: 68.4888566971\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (46.2334403992,34.256818574), test loss: 31.8066294909\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (177.586425781,55.757297006), test loss: 53.7052335739\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (29.1043052673,33.9463725229), test loss: 35.5952293396\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (68.7149810791,55.25652982), test loss: 56.5781443834\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (20.211359024,33.6441776699), test loss: 38.8121592283\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (19.9789009094,54.7656699116), test loss: 68.0983296394\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (19.3594284058,33.3563430012), test loss: 31.5672723532\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (18.3420677185,54.297225678), test loss: 53.5264583349\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (15.3627986908,33.0760051823), test loss: 36.7396167517\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (22.7939567566,53.8388945116), test loss: 57.9159314632\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (21.8277397156,32.7979211975), test loss: 37.3560207367\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (33.6118164062,53.3904146641), test loss: 68.8814129353\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (19.9564971924,32.5278157453), test loss: 31.3709517956\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (34.8961029053,52.9544272412), test loss: 54.6455201626\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (11.9752626419,32.2632713733), test loss: 37.2147452116\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (8.6647644043,52.5298735287), test loss: 64.1212677002\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (43.540725708,32.0085052856), test loss: 35.0605385065\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (147.742660522,52.126346114), test loss: 65.5276222706\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (19.8832855225,31.7559780425), test loss: 29.680924964\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (60.7213020325,51.7213204945), test loss: 51.8682807446\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (18.9134178162,31.5096873684), test loss: 37.7668974876\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (14.8923625946,51.3231548987), test loss: 65.3681129456\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (12.1804265976,31.2730326973), test loss: 33.7245612621\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (11.7950153351,50.9397467694), test loss: 63.9232919931\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (13.87657547,31.0413028466), test loss: 29.8839581728\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (17.6770820618,50.5623730011), test loss: 50.5720569372\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.1694450378,30.810594642), test loss: 37.5042257071\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (29.7447147369,50.1912688424), test loss: 66.7401216507\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (11.816652298,30.5850417942), test loss: 32.3880543709\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (13.3261041641,49.8275468831), test loss: 62.0442350864\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (11.2716178894,30.3630358743), test loss: 32.1500142097\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (9.80536460876,49.4713345897), test loss: 52.8425061226\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (32.2079353333,30.1481540173), test loss: 37.5656378269\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (115.633872986,49.1305306409), test loss: 65.8816765308\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (13.1526069641,29.9344290228), test loss: 30.6597569942\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (47.8893890381,48.7867965279), test loss: 53.2689707756\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (19.7182750702,29.7256695825), test loss: 32.995868206\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (15.0247955322,48.4477244372), test loss: 54.0212399483\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (9.63218688965,29.5237690406), test loss: 37.4012490273\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (9.60286903381,48.1188061059), test loss: 65.519299984\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (15.7470169067,29.32520445), test loss: 30.3990252256\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (14.8123807907,47.7933520837), test loss: 53.4699930191\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.6106929779,29.1274113024), test loss: 33.6391871929\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (26.1020889282,47.4723993292), test loss: 55.176737833\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (11.94962883,28.9330195515), test loss: 36.3940285683\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (9.84250068665,47.1560097162), test loss: 66.1130297422\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (8.68342781067,28.7408309861), test loss: 30.475188303\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (10.8626031876,46.844521025), test loss: 53.864731741\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (26.9412574768,28.5539097156), test loss: 35.6499498129\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (106.217010498,46.5444437114), test loss: 56.5927942276\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (13.4528112411,28.3676864842), test loss: 35.6757783413\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (39.4452438354,46.2410416385), test loss: 66.8441837311\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.7801055908,28.184966856), test loss: 30.1240735054\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (14.4065628052,45.9408337526), test loss: 53.8077406406\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (10.935883522,28.0073181057), test loss: 36.1683886528\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (9.98389434814,45.6476355361), test loss: 63.9968836784\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (18.3239974976,27.8319763826), test loss: 34.0192624569\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (9.86149215698,45.3566156177), test loss: 65.5089568138\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (14.1304206848,27.6569177828), test loss: 28.6340342999\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (23.4896278381,45.0687520995), test loss: 51.3205300808\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (10.7048482895,27.4841111312), test loss: 36.4676370144\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (8.06225967407,44.7834554046), test loss: 64.666392374\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (5.17613840103,27.3127566618), test loss: 32.6955237389\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (12.0819311142,44.5015834283), test loss: 64.0076723099\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (21.8316860199,27.1450348258), test loss: 29.5025429249\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (81.0078887939,44.227943239), test loss: 50.4709125519\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (15.0637054443,26.9777562643), test loss: 36.1135229588\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (35.949344635,43.9509062794), test loss: 65.0963490009\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (18.8982315063,26.8129224262), test loss: 31.1642080784\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (9.35681724548,43.675863704), test loss: 58.9006595612\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (9.7417049408,26.6517609748), test loss: 31.5632860184\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (8.54186153412,43.4057304207), test loss: 52.0838303089\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (18.3386764526,26.4922053712), test loss: 35.8917157173\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (5.23542261124,43.1367221704), test loss: 64.9386776924\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.9241504669,26.3327514263), test loss: 30.0446321487\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (19.8007545471,42.8699702379), test loss: 53.5293302536\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (10.3268594742,26.1747011229), test loss: 32.4043552876\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (7.81088161469,42.6045306128), test loss: 53.9923985958\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (10.8148326874,26.0177619086), test loss: 36.3272591114\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (17.2898273468,42.3416267741), test loss: 65.5629683495\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (19.0648555756,25.8630449133), test loss: 29.7246485233\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (47.2773399353,42.0843685395), test loss: 52.0430735111\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (12.478263855,25.7087767927), test loss: 33.5556867599\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (31.2386302948,41.8241012395), test loss: 55.5347725868\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.5643701553,25.5562766963), test loss: 35.3456668139\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (7.41749811172,41.5650426402), test loss: 64.9452197075\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (8.46531486511,25.4065109712), test loss: 29.6652844906\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (9.59137916565,41.3094805286), test loss: 52.5362528086\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.7514486313,25.257862692), test loss: 34.8862969398\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (4.14260625839,41.0545263491), test loss: 56.9982572079\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.12642478943,25.1092135863), test loss: 35.0061160564\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (13.2273044586,40.8011734024), test loss: 67.0517972946\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 2\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (290.891326904,inf), test loss: 206.485419083\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (404.731140137,inf), test loss: 271.887214279\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (44.8747940063,127.997312806), test loss: 66.2097332954\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (146.675811768,196.58415331), test loss: 146.613644409\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (34.3112030029,86.9194193087), test loss: 43.8514192104\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (62.5709762573,142.273214464), test loss: 81.5589939117\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (36.2461280823,72.0491246993), test loss: 39.5251817703\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (44.822555542,117.527207407), test loss: 67.5503549576\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (61.3784370422,64.4243391576), test loss: 42.8903600216\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (76.0708084106,104.772943315), test loss: 75.3316468239\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (42.4271965027,59.7344742882), test loss: 40.4754318237\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (51.5141181946,97.0176645649), test loss: 74.8508043289\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (42.9458274841,56.5473880641), test loss: 38.3897870541\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (76.3427886963,91.7330854834), test loss: 64.238425827\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (124.935447693,54.171681541), test loss: 41.8961217403\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (111.114181519,87.8059671985), test loss: 75.973721981\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (6.6752986908,52.2832143958), test loss: 40.0574141502\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (9.12950801849,84.7494819914), test loss: 68.4071048737\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (39.2345199585,50.7546652508), test loss: 38.2078066826\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (57.2723655701,82.2950985613), test loss: 64.8798254013\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (26.3431835175,49.4777134467), test loss: 41.5614944935\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (22.9460792542,80.2534549938), test loss: 74.5224677086\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (29.0320701599,48.3720096866), test loss: 36.2740334988\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (37.5427780151,78.5111927227), test loss: 62.5216473579\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (70.7993850708,47.4002621222), test loss: 38.0420906067\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (113.050605774,76.9845845993), test loss: 63.6755224228\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (9.41022109985,46.5286163823), test loss: 38.107465601\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (10.7193078995,75.6340592681), test loss: 69.5458517075\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (40.6159820557,45.738497988), test loss: 32.9586296558\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (39.583694458,74.3914377904), test loss: 58.9429806709\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (69.5066299438,45.0099384667), test loss: 36.2240944862\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (101.575195312,73.2710097465), test loss: 66.5125294685\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (48.2416610718,44.3359056491), test loss: 33.5332057953\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (53.2420578003,72.2212507022), test loss: 65.5512606621\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (45.9857978821,43.7010462386), test loss: 29.7555862904\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (37.2717094421,71.2287012402), test loss: 52.9204247475\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (33.4837265015,43.0816140573), test loss: 35.5337192059\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (82.7577362061,70.2877570104), test loss: 67.4214262009\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (25.3791236877,42.4947626627), test loss: 31.7630859375\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (32.2302398682,69.3882074215), test loss: 57.2395205498\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (35.1488647461,41.9384839693), test loss: 31.6012645721\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (51.7025108337,68.5372626852), test loss: 55.6135342598\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (10.4537963867,41.3979435797), test loss: 34.585302496\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (24.8117790222,67.7168244232), test loss: 64.9907836914\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (57.7832984924,40.8859398385), test loss: 28.7808781385\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (189.29637146,66.9433785864), test loss: 51.5167229652\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (23.8066272736,40.3871030985), test loss: 31.9544130087\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (25.5767364502,66.176358115), test loss: 54.9917591095\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (27.0081481934,39.9101251313), test loss: 31.837868619\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (30.1136169434,65.4371560186), test loss: 61.0491260529\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (13.817237854,39.451105173), test loss: 27.1681755543\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (18.763584137,64.7318267123), test loss: 49.5573573112\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (38.6881523132,39.0154717392), test loss: 30.7371390343\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (58.6376495361,64.0513096838), test loss: 52.7801270962\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (20.3533649445,38.5935406968), test loss: 29.1709958553\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (26.308298111,63.38456663), test loss: 59.1381892681\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (27.5094738007,38.1802225514), test loss: 25.0302259684\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (38.5640602112,62.742604758), test loss: 46.4390250206\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (26.5097064972,37.7838589593), test loss: 31.0783327937\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (26.403799057,62.1179421806), test loss: 60.7851318836\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (16.0459442139,37.4037238062), test loss: 27.8074910283\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (53.1256866455,61.522880531), test loss: 57.9517088413\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (36.2942733765,37.0358846546), test loss: 26.6477887541\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (45.9128913879,60.9451375593), test loss: 48.7849470139\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (43.3845367432,36.6850150291), test loss: 29.480328685\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (87.4313278198,60.4016307513), test loss: 58.6573942661\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (35.6529388428,36.3423902807), test loss: 26.1951143205\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (31.5703353882,59.8577782157), test loss: 48.2298761368\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (6.42021799088,36.0109305954), test loss: 28.089071852\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (15.6093606949,59.3315744855), test loss: 49.8330420017\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (21.5586891174,35.6937097143), test loss: 29.5784573317\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (23.381187439,58.8279283354), test loss: 57.8656109333\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (44.6704788208,35.3900196606), test loss: 25.6763581097\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (48.2055168152,58.3394408405), test loss: 46.7075834274\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (10.4474420547,35.0905634284), test loss: 28.7122087359\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (35.1086540222,57.8555206343), test loss: 49.9624401569\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (16.5374584198,34.7969791286), test loss: 28.0154273927\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (27.1337394714,57.3855914122), test loss: 56.8656899929\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (26.023021698,34.5125272505), test loss: 24.2412226379\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (30.7733516693,56.9246281847), test loss: 45.3807306767\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (9.52729701996,34.2342499923), test loss: 28.3373666406\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (51.9537620544,56.4794330484), test loss: 55.2072512627\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (3.23199248314,33.9634089065), test loss: 25.4357202768\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (9.94782829285,56.0409303654), test loss: 54.195905304\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (12.9724378586,33.7024339933), test loss: 23.713113606\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (33.5770874023,55.6265390522), test loss: 42.2259088039\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (25.9845504761,33.4449774717), test loss: 28.1064004421\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (71.7835464478,55.2083128019), test loss: 55.977055645\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (21.5723419189,33.1924484876), test loss: 25.0015988588\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (116.476081848,54.7998767346), test loss: 46.183278203\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (46.2928962708,32.9495403154), test loss: 26.0999801397\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (72.798500061,54.40092616), test loss: 46.3448506832\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (11.6539077759,32.7114957508), test loss: 28.5238684177\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (14.5864295959,54.0094521097), test loss: 55.3018922329\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (17.0911884308,32.4766655259), test loss: 24.3800541759\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (51.5085601807,53.623084584), test loss: 44.7034943819\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (14.7769680023,32.2448874147), test loss: 27.8329333186\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (16.5543670654,53.2422910315), test loss: 48.1825276375\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.0678129196,32.0180767428), test loss: 27.6920534134\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (28.7758312225,52.867878476), test loss: 55.4709913969\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (17.401594162,31.7948076128), test loss: 23.8778972745\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (29.802778244,52.5021398054), test loss: 45.4849782705\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (8.83368587494,31.5758394448), test loss: 27.785987401\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (34.4065437317,52.1406881179), test loss: 54.0270346642\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (21.4312324524,31.3637756859), test loss: 24.6748357892\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (18.5471115112,51.7942857675), test loss: 52.160129118\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (17.2648925781,31.1529858743), test loss: 22.0959425926\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (20.8229942322,51.4443753041), test loss: 40.5403989792\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (25.8101158142,30.9456817464), test loss: 28.0970804811\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (22.0629234314,51.1003303921), test loss: 54.6323451996\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (30.2978019714,30.7448745848), test loss: 24.3362316966\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (43.8189239502,50.7629834672), test loss: 51.1029932022\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (44.9946899414,30.5475460807), test loss: 25.0633262157\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (44.2257766724,50.4294417286), test loss: 44.6981583595\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (15.4592676163,30.3503934847), test loss: 27.1661722898\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (8.17967605591,50.0974057696), test loss: 53.922340107\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (17.575252533,30.1563219462), test loss: 23.8001807213\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (21.8355770111,49.7702143501), test loss: 43.2621327877\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (18.8531188965,29.9653326871), test loss: 26.1976379395\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (32.6001739502,49.4468560294), test loss: 46.4676118851\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (16.8536777496,29.7760090329), test loss: 27.5007526398\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (20.7478675842,49.1274612516), test loss: 52.183509779\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (11.5824317932,29.5895501193), test loss: 24.143890357\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (45.8248901367,48.8124891893), test loss: 44.2437524796\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (6.12305545807,29.4074542991), test loss: 27.5229635954\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (23.4011898041,48.5055434676), test loss: 46.9779521465\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (14.1485385895,29.2263694562), test loss: 25.7852436543\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (13.7614812851,48.1955321314), test loss: 52.3450500488\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (46.1357803345,29.0483055344), test loss: 22.8344880581\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (54.9744262695,47.890817651), test loss: 42.4273862839\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (32.022151947,28.8730397701), test loss: 27.5391056538\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (30.5162220001,47.5879339868), test loss: 52.3508201838\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (27.1352882385,28.7003021075), test loss: 24.7004061103\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (14.6865329742,47.2874553145), test loss: 51.4460418224\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (9.73370456696,28.5269730065), test loss: 23.4577985764\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (18.5058116913,46.9883350896), test loss: 41.1927923679\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (8.93477725983,28.3559874017), test loss: 27.5546844959\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.92616271973,46.6911959318), test loss: 54.6931693077\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (11.6288003922,28.1870021321), test loss: 24.7265406489\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (9.71177387238,46.396846681), test loss: 45.2802003384\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (6.70823383331,28.0185485555), test loss: 25.6960148692\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (9.93268680573,46.1045399955), test loss: 45.8330771446\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (43.4876098633,27.8530376835), test loss: 27.821458149\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (117.940139771,45.818276767), test loss: 53.5883162022\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (12.6523857117,27.6884301307), test loss: 23.8595520496\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (6.26799821854,45.5305976217), test loss: 43.497778511\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (8.54790210724,27.5250654021), test loss: 27.9185079932\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (11.246512413,45.243630683), test loss: 47.6588484287\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (13.6697664261,27.3638694172), test loss: 26.897700572\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (19.3719959259,44.9601422784), test loss: 52.799846983\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (12.6470222473,27.2042341345), test loss: 23.5983814001\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (26.6827774048,44.6773213186), test loss: 43.4291575432\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (11.6930370331,27.0462473069), test loss: 28.4025794983\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (9.40745353699,44.3953203182), test loss: 54.4545768738\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (16.1523208618,26.8878660241), test loss: 24.3892476082\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (34.2491989136,44.1154645229), test loss: 51.3197292328\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (23.7069244385,26.7309174069), test loss: 22.8590607166\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (18.3236427307,43.8353441189), test loss: 41.4233654022\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.01734161377,26.5747037982), test loss: 29.0171609402\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (22.3871231079,43.5576354138), test loss: 56.7094421387\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 3\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (290.86038208,inf), test loss: 196.673010254\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (404.709228516,inf), test loss: 270.005249405\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (46.0955848694,125.148166807), test loss: 59.0057333469\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (63.875793457,188.48054575), test loss: 138.375101662\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (53.8476867676,84.9029118309), test loss: 43.0610255241\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (204.566635132,136.745844026), test loss: 81.1574491501\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (120.321090698,70.2925724862), test loss: 40.805421257\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (110.798202515,112.644436552), test loss: 78.2061691284\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (39.8451919556,62.7655225015), test loss: 38.3012625217\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (77.0346679688,100.323653061), test loss: 68.7245547295\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (16.4737586975,58.1464492346), test loss: 40.1952965736\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (48.70753479,92.8520174938), test loss: 73.1052886009\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (32.1025733948,54.9812221785), test loss: 40.2548366547\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (36.9509315491,87.7052924129), test loss: 76.2429784775\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (28.1468772888,52.6531305669), test loss: 35.9244260073\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (33.3257637024,83.956196887), test loss: 65.009894371\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (23.8692626953,50.8303697345), test loss: 38.2625525951\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (34.76512146,81.0108140601), test loss: 65.6066022873\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (52.0743331909,49.3468349104), test loss: 39.4553623199\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (73.3977203369,78.6697402043), test loss: 72.9646057129\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (43.2070503235,48.0950937251), test loss: 34.8204388142\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (48.2303504944,76.6953444057), test loss: 62.271761322\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (28.0633583069,47.0293696918), test loss: 35.9831791878\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (60.4512214661,75.0219350543), test loss: 61.9353272915\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (22.860666275,46.0922735045), test loss: 38.0641386986\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (25.0694503784,73.53730256), test loss: 71.3594895363\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (12.4223566055,45.2458585613), test loss: 35.4814412117\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (30.6300487518,72.2319625335), test loss: 64.1354200363\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (40.6112861633,44.4743608403), test loss: 33.7886898994\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (36.490070343,71.0473580506), test loss: 59.8944752693\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (42.6579971313,43.7721362892), test loss: 37.6184850216\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (76.0520095825,69.9650822537), test loss: 70.3546042442\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (28.4065418243,43.1204436753), test loss: 33.2103664398\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (49.8276863098,68.9502364336), test loss: 67.8489633083\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (21.8363933563,42.4993970822), test loss: 30.1421317577\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (57.8979606628,68.0032818579), test loss: 55.6961515903\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (26.5184173584,41.9073842675), test loss: 33.9775396824\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (16.0320129395,67.1077371368), test loss: 63.8567403793\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (68.5894851685,41.3486924473), test loss: 33.8711660385\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (95.1091842651,66.253099125), test loss: 66.3938724518\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (27.0439186096,40.8176419526), test loss: 28.6871557236\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (80.0483703613,65.4356258574), test loss: 53.6603956223\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (29.0798225403,40.300553854), test loss: 32.5553239346\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (103.392127991,64.6476682707), test loss: 56.1489731789\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (23.8730735779,39.8007600907), test loss: 33.8708600998\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (19.6514968872,63.8962937903), test loss: 63.1319681168\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (17.440076828,39.3180684289), test loss: 29.078121233\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (16.1554546356,63.15789379), test loss: 54.5509770393\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (9.60235214233,38.8627777242), test loss: 30.020606184\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.73447990417,62.4494743606), test loss: 52.9058724403\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (28.337469101,38.4157278943), test loss: 34.4623497486\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (33.070602417,61.7606611276), test loss: 65.4465720177\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (17.7272796631,37.9832996431), test loss: 28.5883376837\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (33.3037414551,61.1100763154), test loss: 59.5037250519\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (16.6298179626,37.5635821745), test loss: 26.9440954685\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (12.1012306213,60.4620894317), test loss: 51.0809429169\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (23.7655162811,37.1744557942), test loss: 31.9744657278\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (28.3518199921,59.8480730615), test loss: 60.8614922047\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (17.9723033905,36.7872429263), test loss: 29.4503054619\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (29.3054504395,59.2467471933), test loss: 60.9677684784\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (63.3405838013,36.4134970024), test loss: 25.026771903\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (122.604362488,58.6731421065), test loss: 47.3819265842\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (16.1750259399,36.0525710634), test loss: 30.7463650227\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (43.4746856689,58.1189311686), test loss: 52.6056384087\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (41.5934638977,35.7162138997), test loss: 30.4287445068\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (28.7246932983,57.5834854964), test loss: 60.0148712158\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (39.9948272705,35.3808073352), test loss: 25.9392709732\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (29.1461677551,57.0564364902), test loss: 48.9413425922\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (14.1215057373,35.0530021818), test loss: 28.8816573143\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (52.9153594971,56.5537575308), test loss: 49.8869267464\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (9.37556362152,34.7395853532), test loss: 32.0515592575\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (18.72813797,56.068280033), test loss: 61.5115710258\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (14.4523735046,34.4408380973), test loss: 26.6874084949\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (24.9072227478,55.6020687324), test loss: 56.0564266205\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (25.5478897095,34.147017556), test loss: 27.1149855375\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (19.8676376343,55.1379206327), test loss: 48.0081731319\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (3.53199768066,33.8574322378), test loss: 31.2953565359\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (13.5090370178,54.6911827864), test loss: 59.5406998158\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (8.67019844055,33.5780114069), test loss: 26.540895462\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (10.4922351837,54.261181686), test loss: 57.0110076427\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (35.2403259277,33.309603683), test loss: 23.2365971088\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (49.6218948364,53.8432955049), test loss: 44.0067533493\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (20.3377742767,33.0458330209), test loss: 29.3231233358\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (30.6339988708,53.4272217193), test loss: 50.2084022999\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (15.0164871216,32.7852196911), test loss: 28.6715579271\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (40.7816200256,53.0243886302), test loss: 58.5218673706\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (32.3335494995,32.5328658051), test loss: 23.8954143524\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (40.5587387085,52.6360318942), test loss: 44.9316842079\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (49.9970779419,32.2883983862), test loss: 28.5190038204\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (50.1128387451,52.2552003858), test loss: 48.4919646263\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (15.8618021011,32.049275048), test loss: 30.6617712259\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (22.1783561707,51.8774983703), test loss: 57.9645756245\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.4447383881,31.8114514305), test loss: 25.2054013968\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (24.1983833313,51.5085967085), test loss: 47.6277369976\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (7.93251132965,31.579530428), test loss: 26.4450670481\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (25.2387962341,51.1520098606), test loss: 46.5883608103\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.5863304138,31.3544200802), test loss: 30.9196674585\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (38.3063545227,50.8003769132), test loss: 59.1559945583\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (25.013420105,31.1364701981), test loss: 25.8141627073\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (25.5510215759,50.4539419512), test loss: 55.0443000793\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (10.6934108734,30.9173346528), test loss: 23.5333932161\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (43.3711395264,50.1123680562), test loss: 45.7735127449\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (8.23584270477,30.7033977902), test loss: 28.9405058861\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (30.2024917603,49.7819864225), test loss: 56.0127536058\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (11.1783027649,30.4941601696), test loss: 26.7523334265\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (16.2065315247,49.4530197354), test loss: 55.5509436131\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (11.0756855011,30.2919324958), test loss: 23.1010675907\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (19.6887798309,49.1300907557), test loss: 43.9274066448\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (10.4065275192,30.089033763), test loss: 27.8018874884\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (25.4752540588,48.810019092), test loss: 47.8516913414\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (12.9996776581,29.8896134375), test loss: 28.9364748597\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (24.4919204712,48.5017436808), test loss: 55.8451911926\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (53.2469406128,29.6941934352), test loss: 23.2506930828\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (76.2048492432,48.192733301), test loss: 44.3664160728\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (18.2088680267,29.5049051237), test loss: 26.3469481468\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (46.6669082642,47.8887391659), test loss: 46.4137753963\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (9.9378118515,29.3149726516), test loss: 30.1203350544\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (8.92370605469,47.5856312568), test loss: 58.591342926\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (30.681804657,29.1273719769), test loss: 24.5177762508\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (113.262161255,47.2935275891), test loss: 52.1848368883\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.5457630157,28.942146481), test loss: 24.0148077965\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (25.3426456451,46.9991040454), test loss: 44.2801737785\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (16.2730007172,28.7647399827), test loss: 29.2126359224\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (15.3176698685,46.7106295456), test loss: 56.3419739246\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (24.4816017151,28.5856800714), test loss: 26.0040236712\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (62.2988815308,46.4230457876), test loss: 55.2328145027\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (10.826379776,28.4067365207), test loss: 21.9268446207\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (49.2595405579,46.1391648351), test loss: 42.2165407181\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (12.6779270172,28.2321195305), test loss: 27.845887661\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (80.356880188,45.8613915678), test loss: 48.1251785755\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (36.1732330322,28.0628325786), test loss: 27.3785480499\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (30.4121837616,45.5846073964), test loss: 55.288106823\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (10.7999534607,27.8927266064), test loss: 22.8405272245\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (21.0198135376,45.3070825532), test loss: 43.2465947628\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (6.40333461761,27.722851918), test loss: 26.0362578869\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (17.2955226898,45.033802927), test loss: 45.7423138618\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (8.62178993225,27.5558673004), test loss: 29.3595813274\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (13.5241374969,44.7648572788), test loss: 57.3962723732\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (31.1672267914,27.3929322088), test loss: 24.036572361\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (30.0006713867,44.4983544218), test loss: 44.7723147869\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (15.5731143951,27.23027802), test loss: 24.9950047016\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (11.3391189575,44.2296793729), test loss: 45.1260522842\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (25.9100608826,27.0679543849), test loss: 29.6187371492\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (35.7635993958,43.9648556214), test loss: 57.2923228741\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (15.1458683014,26.9071381742), test loss: 24.8617495775\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (13.7769699097,43.7034684454), test loss: 53.3937497616\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (9.14980602264,26.7491065568), test loss: 21.9233740568\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (10.6272678375,43.4426837251), test loss: 41.3734222889\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (9.02139091492,26.5929995408), test loss: 28.3764500141\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (7.64554643631,43.1819829685), test loss: 56.3431282043\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (11.0619029999,26.4360151985), test loss: 27.1329539299\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (6.20990085602,42.9226393063), test loss: 56.0117472649\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.7391166687,26.2807573929), test loss: 22.7056622267\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (18.1255092621,42.66756468), test loss: 43.2610708714\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (19.3324127197,26.1276330475), test loss: 27.1228640556\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (31.95561409,42.4123891487), test loss: 46.4764586926\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (9.74604988098,25.9768737268), test loss: 28.6189509869\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (13.2545499802,42.157539495), test loss: 55.5283332348\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (11.8149166107,25.8246136096), test loss: 23.1302897453\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (16.8926811218,41.9032022613), test loss: 43.8450657845\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 4\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (290.872741699,inf), test loss: 196.485199738\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (404.749938965,inf), test loss: 259.039456177\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (71.8319549561,125.20000869), test loss: 58.2526278496\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (215.297744751,191.804260498), test loss: 126.929201698\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (14.9962377548,84.3402017765), test loss: 42.2274089813\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (27.1230449677,138.163715305), test loss: 77.4355632782\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (22.4631481171,69.5502826824), test loss: 36.6409370899\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (45.2121047974,113.780080507), test loss: 62.0464031696\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (33.0976905823,62.0036672318), test loss: 39.7812422752\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (46.0226669312,101.223404233), test loss: 67.0879755974\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (15.8681144714,57.3912946384), test loss: 39.3350740433\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (46.6250648499,93.5958987806), test loss: 71.368020916\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (22.1071281433,54.2398202176), test loss: 36.4312450886\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (22.4554252625,88.4426071081), test loss: 58.5400336266\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (43.0550079346,51.9395348583), test loss: 39.1584378242\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (58.9076385498,84.6698292383), test loss: 67.7167798996\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (70.6251983643,50.1445185896), test loss: 38.4334493637\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (68.737411499,81.7311939445), test loss: 64.4546262741\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (23.7947597504,48.6608280083), test loss: 36.0779931545\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (39.5055923462,79.348107449), test loss: 57.5658111572\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (17.7440795898,47.4298431771), test loss: 39.9950219154\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (27.0900535583,77.3788810406), test loss: 67.8965364456\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (53.1308441162,46.3822287259), test loss: 35.0084370136\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (44.0182266235,75.6997425331), test loss: 58.6443668365\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (39.307182312,45.4703610311), test loss: 35.6818848133\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (123.897171021,74.2767161496), test loss: 56.7245853901\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (38.9055328369,44.6470536441), test loss: 37.6043550491\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (55.3564682007,72.9767258651), test loss: 68.6004357815\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (51.4811172485,43.9080006493), test loss: 31.5413072586\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (135.463439941,71.8368517187), test loss: 55.4800783634\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (37.4903030396,43.22504559), test loss: 36.1836551666\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (54.0125427246,70.7643870443), test loss: 63.5176703453\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (37.3288536072,42.6002856702), test loss: 34.3823830128\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (47.2017326355,69.7839025258), test loss: 65.4653054237\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (64.2759552002,42.0141625209), test loss: 31.6924123287\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (88.5602645874,68.8845909466), test loss: 52.3360876083\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.1376876831,41.466595858), test loss: 35.0464195728\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (54.9600982666,68.0319111372), test loss: 62.9614933014\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (73.4504241943,40.9451525919), test loss: 32.0877645493\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (77.5388870239,67.2178612434), test loss: 55.2964436531\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (8.9304561615,40.4385787446), test loss: 31.4908361435\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (24.4481143951,66.4326230154), test loss: 51.8575881958\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (29.1123695374,39.9531759169), test loss: 35.6230397224\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (43.7961120605,65.6833106469), test loss: 63.8667328835\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (23.5094413757,39.4850349664), test loss: 29.79865942\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (50.7222976685,64.957668925), test loss: 52.1921002388\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (25.4380779266,39.0420077836), test loss: 31.5911188602\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (49.9311408997,64.2728489437), test loss: 54.2986632824\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (9.330534935,38.6072226526), test loss: 33.3335373402\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (13.7555198669,63.6030360361), test loss: 64.0980142593\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (56.3832435608,38.1905124224), test loss: 26.8996795654\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (127.244506836,62.9596950679), test loss: 48.2408011436\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (19.7303218842,37.7871487287), test loss: 33.610088253\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (9.25108242035,62.3352534604), test loss: 61.4132670403\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (15.5426492691,37.3987177833), test loss: 30.3934690952\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (15.580991745,61.7247194427), test loss: 54.880696106\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (21.9078178406,37.0217914682), test loss: 28.2185005188\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (18.4159431458,61.1371965194), test loss: 47.4928017139\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (33.4365119934,36.6640747841), test loss: 32.6755576611\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (59.1872177124,60.5700356191), test loss: 60.09801507\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (28.582736969,36.3154330907), test loss: 28.4253309727\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (34.8443489075,60.0162835612), test loss: 50.2529232025\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (6.2138671875,35.9755821398), test loss: 29.3061184883\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (28.2875270844,59.4738660753), test loss: 49.1117377281\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (23.3010845184,35.6419965867), test loss: 33.6024385929\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (21.8133010864,58.9442981553), test loss: 63.0051242828\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (27.6824512482,35.3203870336), test loss: 27.5780178547\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (23.8753204346,58.4311515888), test loss: 50.3852774143\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (22.0177364349,35.0111362799), test loss: 30.7015536785\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (18.7347717285,57.9401718354), test loss: 57.5929841518\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (18.5602836609,34.7075759006), test loss: 31.4078927994\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (26.9992675781,57.4608066462), test loss: 62.9343806744\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (12.0100221634,34.4119509745), test loss: 25.8037298679\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (50.5475769043,56.9912013559), test loss: 46.5103946209\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (12.7476119995,34.1268063858), test loss: 32.5292788029\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (37.8506584167,56.5440736678), test loss: 60.4663638115\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (29.4690551758,33.8475339547), test loss: 29.2019849777\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (26.1143951416,56.0972739624), test loss: 52.4459216118\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.7201900482,33.5738380536), test loss: 27.5251798153\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (22.8916511536,55.6630770617), test loss: 47.0577754974\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (64.4075622559,33.3115113003), test loss: 32.4815751553\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (75.3079605103,55.2397455259), test loss: 59.6984712124\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (29.7746582031,33.0534948309), test loss: 27.4029952049\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (36.6888504028,54.825341562), test loss: 49.3752470493\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (20.1970462799,32.8007298036), test loss: 29.645998764\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (17.250787735,54.4153407155), test loss: 49.4549875259\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (15.6638011932,32.5490482172), test loss: 32.5469460964\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (27.0458259583,54.0122301976), test loss: 62.4905786037\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (7.94593048096,32.3045453209), test loss: 27.3412212372\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (6.96331834793,53.6183994351), test loss: 51.2932233334\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (21.7019062042,32.0673205712), test loss: 30.5184214592\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (65.653137207,53.2368381693), test loss: 57.8475643158\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (18.0834159851,31.8318939155), test loss: 30.1556232929\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (45.6222190857,52.860190731), test loss: 62.410979557\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (4.71551656723,31.6014203004), test loss: 25.4723132133\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (6.06836128235,52.487992534), test loss: 46.5259814262\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.5639419556,31.376863311), test loss: 31.5636588573\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (28.7050170898,52.1317409987), test loss: 59.9784226894\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (22.0938873291,31.1552222649), test loss: 28.8103654861\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (60.0317230225,51.7729112799), test loss: 52.373643589\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (8.86593341827,30.9375391215), test loss: 27.3967182636\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (26.9317455292,51.4176934505), test loss: 47.515822649\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (10.3145618439,30.7256602396), test loss: 32.5549960136\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (9.16182327271,51.0723603913), test loss: 59.9226293087\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (15.2335958481,30.5190631299), test loss: 27.0457118511\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (26.0430793762,50.7328660939), test loss: 49.3926496267\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (22.6746139526,30.315356265), test loss: 29.7867431641\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (12.8457098007,50.3958733429), test loss: 49.8774805546\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.7331752777,30.1124056003), test loss: 32.2374299049\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (15.2975854874,50.0619231222), test loss: 62.1860593796\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (24.7602119446,29.9136807216), test loss: 27.5339371204\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (32.3896522522,49.7340022452), test loss: 52.1024172783\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (44.0752182007,29.7189797509), test loss: 31.065641737\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (58.1400260925,49.4114109842), test loss: 58.6560045719\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (9.84624671936,29.5257676025), test loss: 30.0703326702\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (33.7911834717,49.0943456729), test loss: 62.7912765503\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (30.5555019379,29.3362870013), test loss: 25.6287790298\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (47.49766922,48.7803303507), test loss: 47.0976174831\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (28.8957710266,29.1500254668), test loss: 31.5976791382\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (43.0206184387,48.4753702424), test loss: 60.3667850494\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.6657466888,28.9650428886), test loss: 27.9483155727\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (13.7729511261,48.1666067873), test loss: 51.5321106434\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (12.1173725128,28.7835966241), test loss: 27.3977984905\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (9.69816970825,47.8620296765), test loss: 48.0272497654\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (47.3101425171,28.6057116129), test loss: 32.646091032\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (60.3197784424,47.5641789807), test loss: 60.2460030079\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (20.2082595825,28.430553984), test loss: 27.2006530285\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (15.6911067963,47.2685089819), test loss: 50.0165582657\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (36.9575309753,28.2584065825), test loss: 30.041591692\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (33.9119033813,46.9759277744), test loss: 50.6375172138\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (14.6386184692,28.0861993085), test loss: 32.1723709583\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (15.1470518112,46.6840404026), test loss: 62.139167738\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (9.3732624054,27.9161187198), test loss: 27.163011837\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (9.52307128906,46.395784121), test loss: 51.7248816013\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (11.2129640579,27.7483344156), test loss: 31.9281711102\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (15.0895328522,46.1099774109), test loss: 60.3373829842\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (8.31608867645,27.5828530246), test loss: 30.25554533\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (19.2642192841,45.8285965227), test loss: 63.1386049271\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (11.8957710266,27.4182184556), test loss: 26.5527203083\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (18.1530303955,45.5481119262), test loss: 48.3542086601\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (24.1424865723,27.2558587961), test loss: 31.7420320034\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (80.4915161133,45.2726710091), test loss: 60.9318433762\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.5429239273,27.0952444528), test loss: 27.6365985394\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (8.62760162354,44.9975347094), test loss: 51.2791214466\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (15.0725908279,26.9361977861), test loss: 27.3535061359\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (20.0222110748,44.7232274305), test loss: 48.8407570839\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (17.8539009094,26.7787758896), test loss: 32.7643065929\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (10.7031641006,44.4514911021), test loss: 61.4665514946\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (20.022731781,26.6241047943), test loss: 27.3694398403\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (38.8906478882,44.1828160059), test loss: 50.3162903786\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (14.1117534637,26.470562807), test loss: 30.2938442707\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (14.025894165,43.9153769551), test loss: 51.4933597565\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (6.5400428772,26.3177856686), test loss: 32.5325473785\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (17.1660499573,43.6483922855), test loss: 64.2513134003\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (9.65267086029,26.1658300524), test loss: 26.8857148647\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (15.5387353897,43.3831467018), test loss: 51.5497632027\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (19.8382701874,26.0153598856), test loss: 32.107815218\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (24.4424343109,43.1191110972), test loss: 61.1449193478\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (16.2535381317,25.8662032305), test loss: 30.2126829147\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (20.7627563477,42.8572603612), test loss: 63.0363751411\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 5\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (290.884033203,inf), test loss: 193.814678192\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (404.71697998,inf), test loss: 261.576026917\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (106.274002075,126.089987808), test loss: 54.8672938347\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (262.961669922,193.190940884), test loss: 132.395246696\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (38.914100647,85.5228190064), test loss: 38.857858181\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (76.3960800171,140.324666602), test loss: 70.9024690628\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (34.2903442383,70.7743816442), test loss: 36.2187490463\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (37.411190033,115.934055484), test loss: 59.1376685143\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (27.0100765228,63.2531142094), test loss: 38.3867786407\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (38.4153594971,103.5069368), test loss: 68.1081401825\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (24.041847229,58.6295963718), test loss: 34.4279557228\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (40.610118866,95.8986194136), test loss: 61.0569306374\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (34.2763290405,55.4564607884), test loss: 36.6131588936\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (42.1210327148,90.7011354693), test loss: 66.5228525639\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (39.1557693481,53.107028376), test loss: 36.7567321301\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (77.4168319702,86.8764428692), test loss: 64.3539347649\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (35.8840332031,51.2729205879), test loss: 34.7425853729\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (60.7396392822,83.9004373006), test loss: 56.744036293\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (49.4117584229,49.7855710121), test loss: 35.6729288578\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (86.0124206543,81.4993976256), test loss: 66.3446153641\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (5.64354324341,48.5358392957), test loss: 31.63121562\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (8.50720024109,79.4942307626), test loss: 55.8665203094\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (13.3036804199,47.4691799016), test loss: 33.9304956436\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (38.8220481873,77.7921364447), test loss: 62.7979622841\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (25.6394615173,46.5291028686), test loss: 33.2629733086\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (65.677154541,76.2948849054), test loss: 57.5092206955\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (23.6103019714,45.6871734571), test loss: 31.7810294151\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (35.8891601562,74.9588236763), test loss: 53.1259975433\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (52.1949081421,44.9221998956), test loss: 33.5410333633\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (44.5646476746,73.7546357909), test loss: 65.4312752724\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (98.2255020142,44.2133150903), test loss: 29.4294406414\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (90.0356140137,72.64780517), test loss: 51.2284093857\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (14.378575325,43.5495350875), test loss: 32.0307530403\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (23.3514480591,71.6146369792), test loss: 60.4397428513\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (27.8185348511,42.936632504), test loss: 31.0737165451\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (31.4498023987,70.6538306704), test loss: 54.8247041225\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (42.0854644775,42.3545261227), test loss: 30.3536542416\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (63.0436630249,69.7436873923), test loss: 51.9122779846\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (49.6908340454,41.7993847736), test loss: 31.2189590454\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (59.9974937439,68.874056997), test loss: 62.0180657387\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (24.4522037506,41.2674427096), test loss: 28.9627701759\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (56.6739997864,68.0415965524), test loss: 49.15934515\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (82.102935791,40.7590948523), test loss: 32.307113266\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (97.8641357422,67.2413432853), test loss: 59.5396975517\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (39.405620575,40.2634750808), test loss: 28.0644164562\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (58.4098243713,66.4641317305), test loss: 51.1055895805\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (14.9737081528,39.7900610069), test loss: 29.1708972454\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (17.7649669647,65.718120628), test loss: 55.7305864334\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (64.7127685547,39.3355870809), test loss: 29.344085288\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (90.2842178345,64.9997026712), test loss: 56.3495688915\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (18.3967819214,38.8936424119), test loss: 28.5172578812\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (22.1677856445,64.298310952), test loss: 48.4714787483\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (20.8461093903,38.4713238432), test loss: 30.9775324821\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (22.1847572327,63.6249774491), test loss: 59.4440299511\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (26.42420578,38.064479414), test loss: 25.3933832169\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (144.903167725,62.9734793314), test loss: 48.05844841\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (13.2219638824,37.671916084), test loss: 30.3890236139\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (22.2038650513,62.3347716909), test loss: 57.9367476463\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (24.9155883789,37.2940587971), test loss: 27.623521924\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (25.7951927185,61.7259102208), test loss: 50.6597390175\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (17.5487213135,36.9293332485), test loss: 28.5608656168\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (27.1059455872,61.137028096), test loss: 48.4859170437\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (28.2312602997,36.5779882151), test loss: 30.1721971273\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (77.9537124634,60.5657164732), test loss: 59.9833303452\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (38.8838806152,36.2393582283), test loss: 24.7495159626\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (41.1921691895,60.0083894947), test loss: 45.7868039131\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (28.6233291626,35.9120201237), test loss: 29.9890441179\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (41.5213699341,59.4700346948), test loss: 58.3495529175\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (7.8640422821,35.5956665279), test loss: 26.1268160582\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (7.45329761505,58.9479648619), test loss: 47.3837071419\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (19.8937263489,35.2903608379), test loss: 28.4202968359\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (15.8443126678,58.44326569), test loss: 48.8954266071\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (15.5846157074,34.9932910447), test loss: 29.0353628397\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (26.1513404846,57.9527082953), test loss: 60.1658922434\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (40.9175796509,34.7050361034), test loss: 25.1469662428\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (82.8642730713,57.4749945003), test loss: 45.3909369946\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (82.9831237793,34.423304411), test loss: 29.5779042721\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (203.64616394,57.0055747741), test loss: 57.834647274\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (12.3029003143,34.1467865021), test loss: 26.3299810648\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (28.4652137756,56.5434714798), test loss: 48.475065136\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (8.25093078613,33.8805266725), test loss: 28.9328852892\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (22.9006080627,56.100255144), test loss: 50.2239475965\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (5.2166519165,33.6211465846), test loss: 28.6015036106\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (13.1775245667,55.6690768155), test loss: 59.8311424732\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (24.1689758301,33.3680726881), test loss: 25.8897846222\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (51.739112854,55.2479819348), test loss: 45.5280002356\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (5.2124414444,33.119444873), test loss: 29.9884694338\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (14.513258934,54.8338454992), test loss: 57.3550192118\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (17.9249229431,32.8774316607), test loss: 26.4095068932\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (23.2403450012,54.430485177), test loss: 49.5203312635\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (12.6484880447,32.640035889), test loss: 28.3563277245\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (27.3204650879,54.0345104934), test loss: 56.0754526615\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (8.41651535034,32.407644196), test loss: 27.8714021683\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (48.4566764832,53.6457828578), test loss: 58.5381557941\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (8.34268856049,32.1802699872), test loss: 27.0177165508\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (31.6316680908,53.2640687721), test loss: 47.2994778633\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (18.6560325623,31.9572619067), test loss: 29.1934298515\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (13.9616565704,52.8897761971), test loss: 56.8083465576\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (12.2161521912,31.7379997649), test loss: 24.9443633795\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (9.78424263,52.5227443911), test loss: 48.5896905422\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (15.9674968719,31.5231727498), test loss: 29.3123277903\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (60.8609237671,52.1619869677), test loss: 56.8159983635\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (43.4304275513,31.3117152196), test loss: 27.1165356398\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (32.5516929626,51.8038524508), test loss: 50.3756906033\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (11.8578977585,31.1029341277), test loss: 27.4054469585\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (29.4644393921,51.4536686092), test loss: 47.8817902565\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (22.1730823517,30.8991011136), test loss: 29.3153336048\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (19.5788612366,51.108951961), test loss: 58.817578125\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (19.3635063171,30.6980184946), test loss: 24.1194654465\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (20.3987846375,50.7694469191), test loss: 45.7752411366\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (13.0335378647,30.4997584529), test loss: 29.3420037746\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (11.8892269135,50.4340948861), test loss: 57.4963671207\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (16.6780948639,30.3050775603), test loss: 25.6227752209\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (23.3806724548,50.1041514839), test loss: 47.8354671955\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (17.9627571106,30.1130583908), test loss: 27.5091602802\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (18.3650588989,49.7778486746), test loss: 48.0415100098\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (19.7390556335,29.9237353066), test loss: 28.0106981277\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (49.0908927917,49.4560633657), test loss: 58.8960043907\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (13.0989122391,29.7369820672), test loss: 24.2519178391\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (19.1305656433,49.1369851358), test loss: 45.439792347\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.140583992,29.5529362957), test loss: 28.7650377274\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (6.807929039,48.8224709806), test loss: 56.8506160021\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (17.6782436371,29.3713622292), test loss: 25.457981348\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (44.4589271545,48.5119589399), test loss: 47.0747321844\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (6.85832166672,29.1919670822), test loss: 28.1167009354\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (27.7748985291,48.2034655513), test loss: 49.4451552868\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (15.981803894,29.0148540642), test loss: 27.6633763313\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (12.4322013855,47.8984173435), test loss: 58.6955935955\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (30.960483551,28.8397024189), test loss: 24.8938280106\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (16.0552787781,47.5968785373), test loss: 44.5543534517\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (41.0734863281,28.6660087603), test loss: 28.8385677338\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (33.1418075562,47.2978358244), test loss: 55.9287363529\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (10.2504062653,28.4937708108), test loss: 25.5814226627\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (12.4272661209,47.0007698322), test loss: 47.9097836494\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (29.0133628845,28.3243635348), test loss: 28.3557280064\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (27.8653297424,46.7068497855), test loss: 54.7080020905\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (20.0599822998,28.1557202889), test loss: 26.5400640249\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (36.6745452881,46.4143443825), test loss: 56.1807236671\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (32.5285072327,27.98919267), test loss: 26.4280375957\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (30.3176422119,46.1240238814), test loss: 46.5686966419\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.4431037903,27.8240253633), test loss: 29.050581646\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (25.6594944,45.8361046919), test loss: 55.8951840401\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (42.503616333,27.6608250384), test loss: 24.5959240913\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (53.6955871582,45.5501062345), test loss: 47.6900560856\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (15.2154321671,27.49801534), test loss: 29.4409885406\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (16.9762649536,45.2650388762), test loss: 56.758008194\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (8.25453948975,27.3374357506), test loss: 26.3184238434\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (11.3031320572,44.9829139693), test loss: 48.890240097\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (36.1136665344,27.1781930396), test loss: 26.8907490253\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (46.0782661438,44.7023652995), test loss: 47.6054913521\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (15.5683498383,27.0195165936), test loss: 28.7729599953\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (8.0103225708,44.4225880347), test loss: 57.3919219971\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (5.49622535706,26.8625309191), test loss: 24.8351822853\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (10.2590799332,44.1451368109), test loss: 47.9868288517\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (9.0953578949,26.7068672023), test loss: 29.4316978931\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (36.2211837769,43.868614783), test loss: 57.6466563225\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (7.06420326233,26.5522851968), test loss: 25.5978899479\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (8.38213443756,43.5934404997), test loss: 48.039356041\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (13.0086860657,26.3988324178), test loss: 27.579438448\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (12.3548231125,43.3204048419), test loss: 48.3892584324\n",
      "run time for single CV loop: 2906.49248695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:124: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:126: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Deign Net Architecutre and Run Caffe\n",
    "exp_name = 'Exp14_MC'\n",
    "cohort = 'ADNI1'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'HC'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "multi_label = False\n",
    "start_MC=1\n",
    "n_MC=1\n",
    "MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "start_fold = 1\n",
    "n_folds = 5\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "niter = 40000\n",
    "batch_size = 256\n",
    "\n",
    "pretrain = False\n",
    "multimodal_autoencode = False\n",
    "load_pretrained_weights = False\n",
    "\n",
    "if Clinical_Scale in ['BOTH','ADAS13_DX'] :\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "# dr: Dropout rate, lr: learning rate of each modality, tr: loss-weight for each task\n",
    "hype_configs = {\n",
    "                'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':10,'HC_CT_ff':25,'COMB_ff':10,\n",
    "                                       'ADAS_ff':10,'MMSE_ff':10,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}},\n",
    "    \n",
    "    \n",
    "                'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':10,\n",
    "                                       'ADAS_ff':10,'MMSE_ff':10,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}},\n",
    "                  \n",
    "    \n",
    "                }\n",
    "\n",
    "CV_perf_MC = {}\n",
    "for mc in MC_list:\n",
    "    CV_perf_hype = {}\n",
    "    for hype in hype_configs.keys():    \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "        tr = hype_configs[hype]['tr']\n",
    "        solver_configs = hype_configs[hype]['solver_conf']\n",
    "        \n",
    "#         if hype in ['hyp1','hyp2']:\n",
    "#             HC_snap = 20000 #20000 for ADNI2 5000 for ADNI1\n",
    "#             CT_snap = 20000 #20000 for ADNI2 5000 for ADNI1\n",
    "#             pre_hype = 'hyp2'\n",
    "#         else:\n",
    "#             print 'unknown hyp config'\n",
    "            \n",
    "#             print hype, pre_hype,HC_snap,CT_snap\n",
    "            \n",
    "        CV_perf = {}\n",
    "        start_time = time.time()\n",
    "        for fid in fid_list:            \n",
    "            print ''\n",
    "            print 'MC # {}, Hype # {}, Fold # {}'.format(mc, hype, fid)\n",
    "            snap_prefix = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}'.format(mc,fid,exp_name,hype,modality)\n",
    "\n",
    "            train_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/train_C688.txt'.format(mc,fid)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "\n",
    "            train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "            with open(train_filename_txt, 'w') as f:\n",
    "                    f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "                    \n",
    "                    \n",
    "            print 'Defining train net'\n",
    "\n",
    "            # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "            if pretrain:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_train.prototxt'.format(mc,fid)\n",
    "                with open(train_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(train_filename_txt, batch_size, node_sizes, modality)))            \n",
    "            else:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(train_net_path, 'w') as f:            \n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "                            \n",
    "            print 'Defining test net'\n",
    "\n",
    "            if pretrain:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_test.prototxt'.format(mc,fid)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(test_filename_txt, batch_size, node_sizes,modality)))\n",
    "            else:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            # Define Solver\n",
    "            print 'Defining solver'\n",
    "            solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "            with open(solver_path, 'w') as f:\n",
    "                f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, snap_prefix)))\n",
    "\n",
    "#             ### load the solver and create train and test nets\n",
    "            #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "            #solver = caffe.get_solver(solver_path)\n",
    "            solver = caffe.NesterovSolver(solver_path)\n",
    "            \n",
    "            if load_pretrained_weights:                    \n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_HC_CT_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                snap_path = baseline_dir + net_name.format(mc,fid,cohort,pre_hype,HC_snap,CT_snap)\n",
    "                print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "                solver.net.copy_from(snap_path)\n",
    "\n",
    "            #run caffe\n",
    "            results = run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode)\n",
    "            CV_perf[fid] = results\n",
    "            \n",
    "        print('run time for single CV loop: {}'.format(time.time() - start_time))\n",
    "\n",
    "        CV_perf_hype[hype] = CV_perf\n",
    "        \n",
    "CV_perf_MC[mc] = CV_perf_hype\n",
    "\n",
    "# pickleIt(CV_perf_MC, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Time for training \n",
    "#HC_CT fold 9,10, number of iterations: 40k, two hyper-params\n",
    "#start time: 14:47\n",
    "#end time: 15:31\n",
    "#runtime_mean = (13+31)/4\n",
    "#print runtime_mean\n",
    "\n",
    "# tx=670/4 #time for 10k iters\n",
    "# itx=4 # num of 10k iters\n",
    "# hx=2 #hyp choices\n",
    "# fx=5 #k-folds\n",
    "# mx=5 #mc-folds\n",
    "\n",
    "# num_hrs = (tx*itx*hx*fx*mx)/(3600.0)\n",
    "# print num_hrs\n",
    "\n",
    "# time_for_single_model = num_hrs*36/60\n",
    "# print time_for_single_model\n",
    "\n",
    "3*2904/5/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4f3259ea5db2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtrain_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCV_perf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mtest_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCV_perf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 6"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbcAAAJoCAYAAABVztwOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b432087d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "#niter = 30000\n",
    "test_interval = 500\n",
    "n_mc = len(CV_perf_MC.keys())\n",
    "\n",
    "for m, mc in enumerate(CV_perf_MC.keys()): \n",
    "    CV_perf_hype = CV_perf_MC[mc]\n",
    "    \n",
    "    for hype in CV_perf_hype.keys(): \n",
    "        CV_perf = CV_perf_hype[hype]\n",
    "        n_CV_configs = len(CV_perf)\n",
    "        pid = m*n_CV_configs + 1\n",
    "        \n",
    "        for f, fid in enumerate(fid_list):  \n",
    "            train_loss_list = CV_perf[fid]['train_loss']\n",
    "            test_loss_list = CV_perf[fid]['test_loss']\n",
    "            \n",
    "            for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "                #plt.figure()\n",
    "                ax1 = plt.subplot(n_mc,n_CV_configs,pid)            \n",
    "                ax1.plot(arange(niter), train_loss, label='train',linewidth='.5',alpha=0.25)\n",
    "                ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test_{}'.format(hype), linewidth='3')                            \n",
    "                ax1.set_xlabel('iteration')\n",
    "                ax1.set_ylabel('loss')\n",
    "                ax1.set_title('fid: {}, hyp: {}, Test loss: {:.2f}'.format(fid, hype, test_loss[-1]))\n",
    "                ax1.legend(loc=1)\n",
    "                #ax1.set_ylim(0,100)\n",
    "            pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp11'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'R_HC'\n",
    "batch_size = 256\n",
    "encoding_layer = 'code'\n",
    "weight_layers = 'code'\n",
    "multi_task = False\n",
    "\n",
    "if modality in ['L_HC', 'R_HC']:\n",
    "    snap_iter = 10000\n",
    "    \n",
    "    if modality == 'R_HC':\n",
    "        output_node_size = 16471\n",
    "    else: \n",
    "        output_node_size = 16086\n",
    "        \n",
    "    hype_configs = {\n",
    "                   'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':8000,'out':output_node_size},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "\n",
    "elif modality in ['CT']:  \n",
    "    snap_iter = 100000\n",
    "    \n",
    "    hype_configs = {\n",
    "                    'hyp1':{'node_sizes':{'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-4, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "elif modality in ['HC_CT']:  \n",
    "    snap_iter = 20000\n",
    "    hype_configs = {\n",
    "                 'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':500,'out_HC':16086,'out_CT':686},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "hype = 'hyp1'\n",
    "pretrain_preproc = 'ae_preproc_sparse_HC_10k_CT_100k_{}'.format(hype)\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "\n",
    "# Save either L or R HC encodings. Turn on CT + CS save only for one of these. \n",
    "# (Also decide if you want to copy CT raw scores or encodings)\n",
    "save_encodings = True\n",
    "save_scores = False\n",
    "CT_encodings = True\n",
    "\n",
    "for cohort in ['inner_train', 'inner_test','outer_test']:\n",
    "    data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "    test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)       \n",
    "    input_nodes = ['X_{}'.format(modality)]\n",
    "    #input_nodes = ['X_L_HC','X_CT']\n",
    "    net_file = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "    with open(net_file, 'w') as f:\n",
    "        f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "\n",
    "    test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "    with open(test_filename_txt, 'w') as f:\n",
    "        f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "    sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "    if modality == 'CT':\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "    else:\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "        \n",
    "    print 'Check Sparsity for model: {}'.format(model_file)\n",
    "    \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    \n",
    "    encodings_hdf_file = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,pretrain_preproc)\n",
    "    \n",
    "    if save_encodings:    \n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        input_data.create_dataset(input_nodes[0],data=encodings)           \n",
    "        input_data.close()\n",
    "        \n",
    "    if save_scores:\n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        if not CT_encodings: #copy raw scores for CT instead of encodings\n",
    "            CT_data = load_data(data_path, 'X_CT','no_preproc')                    \n",
    "            input_data.create_dataset('X_CT', data=CT_data)    \n",
    "            \n",
    "        adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "        mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "        input_data.create_dataset('y', data=adas_scores)           \n",
    "        input_data.create_dataset('y3', data=mmse_scores)    \n",
    "        input_data.create_dataset('dx_cat3', data=dx_labels)\n",
    "        input_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting TSNE embeddings \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "exp_name = 'Exp11'\n",
    "#preproc = 'no_preproc'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "Clinical_Scale = 'ADAS13_DX' \n",
    "batch_size = 256\n",
    "snap_interval = 2000\n",
    "snap_start = 2000\n",
    "encoding_layer = 'ff3'\n",
    "weight_layers = 'ff3'\n",
    "cohort = 'outer_test'\n",
    "multi_task = False\n",
    "\n",
    "modality = 'HC_CT'\n",
    "snap_end = 21000\n",
    "\n",
    "\n",
    "hype_configs = {\n",
    "            'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-2}},\n",
    "}\n",
    "\n",
    "hype = 'hyp1'\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "tr = hype_configs[hype]['tr']\n",
    "\n",
    "data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "test_net_path = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)       \n",
    "#input_node = 'X_{}'.format(modality)\n",
    "\n",
    "net_file = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)\n",
    "with open(net_file, 'w') as f:\n",
    "    #f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "\n",
    "test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "with open(test_filename_txt, 'w') as f:\n",
    "    f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "n_snaps = len(np.arange(snap_start,snap_end,snap_interval))\n",
    "tsne_encodings = {}\n",
    "net_weights = {}\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    print sn, snap_iter\n",
    "    model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    net_weights[snap_iter] = results['wt_dict']\n",
    "    print encodings.shape\n",
    "\n",
    "    adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "    mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "    dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "    tsne_model = TSNE(n_components=2, random_state=0, init='pca')\n",
    "    tsne_encodings[snap_iter] = tsne_model.fit_transform(encodings) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "plot_encodings = True\n",
    "plot_wts = False\n",
    "\n",
    "color_scores = dx_labels\n",
    "cm = plt.get_cmap('RdYlBu') \n",
    "marker_size = 100\n",
    "marker_size = (np.mod(color_scores+1,2)+1)*50\n",
    "\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    plt.subplot(np.ceil(n_snaps/2.0),2,sn+1)\n",
    "    if plot_encodings:\n",
    "        plt.scatter(tsne_encodings[snap_iter][:,0],tsne_encodings[snap_iter][:,1],c=color_scores,s=marker_size,cmap=cm)\n",
    "    if plot_wts:\n",
    "        plt.imshow(net_weights[snap_iter][weight_layers])\n",
    "        \n",
    "    plt.title(snap_iter)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp14_MC 1 HC\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.48040254884754818, 0.40524556866103156, 0.46782831290848215, 0.51374591340489695, 0.53767132513517069, 0.62413985547179862, 0.49784102499776833, 0.59451831698812252, 0.56073428416666526, 0.58223023706956945]\n",
      "ADAS mse: [61.884403002952972, 53.649215358747355, 70.931104932888772, 78.201980504815467, 55.429956722128857, 38.039327822850161, 69.123782288693462, 36.272566216682584, 67.23657262519977, 48.747316314477438], rmse: [7.8666640326731239, 7.3245624687586188, 8.4220606108534248, 8.8431883675977101, 7.4451297317191765, 6.1676030857092421, 8.3140713425308945, 6.0226710201274143, 8.1997910110685002, 6.9819278365274897]\n",
      "ADAS means: 0.526435738765, 57.9516225789, 7.55876695076\n",
      "\n",
      "MMSE corr: [0.46415085694142899, 0.40576250202494607, 0.4889730338466009, 0.55964464539561898, 0.50980084902761758, 0.54283564735493373, 0.48651387760719644, 0.56856553611036087, 0.58209642745468837, 0.59147705230987169]\n",
      "MMSE mse: [113.62805457873496, 88.591489973209178, 110.70480535986007, 104.75326174029337, 88.060892544702583, 79.274968537100008, 124.25611297545559, 52.911746199348791, 130.01082256235628, 57.27195837892247], rmse: [10.659646081307528, 9.4123052422458748, 10.521635108663485, 10.234904090429639, 9.3840765419247614, 8.903649169700028, 11.147022605855591, 7.2740460679974248, 11.402228841869308, 7.5678238866217331]\n",
      "MMSE means: 0.519982042807, 94.946411285, 9.65073376366\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 7, 2: 9, 3: 9, 4: 5, 5: 6, 6: 4, 7: 8, 8: 9, 9: 6, 10: 5}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.47772350700166416, 0.40524556866103156, 0.46782831290848215, 0.51529732562656205, 0.53767132513517069, 0.62413985547179862, 0.49569915099396317, 0.59451831698812252, 0.56073428416666526, 0.58799838629525547]\n",
      "ADAS mse: [61.339547767633441, 53.649215358747355, 70.931104932888772, 77.724201003926865, 55.429956722128857, 38.039327822850161, 68.880191029810106, 36.272566216682584, 67.23657262519977, 48.206077550883172], rmse: [7.831956828764663, 7.3245624687586188, 8.4220606108534248, 8.8161329960434962, 7.4451297317191765, 6.1676030857092421, 8.2994090771457998, 6.0226710201274143, 8.1997910110685002, 6.9430596678181571]\n",
      "ADAS means: 0.526685603325, 57.7708761031, 7.5472376498\n",
      "\n",
      "MMSE corr: [0.43324436108656145, 0.40576250202494607, 0.4889730338466009, 0.54403378044619743, 0.50980084902761758, 0.54283564735493373, 0.47671267973397846, 0.56856553611036087, 0.58209642745468837, 0.5944201224207607]\n",
      "MMSE mse: [116.04759642716108, 88.591489973209178, 110.70480535986007, 105.80140803240863, 88.060892544702583, 79.274968537100008, 124.89920036090531, 52.911746199348791, 130.01082256235628, 59.468475457823111], rmse: [10.772538996316564, 9.4123052422458748, 10.521635108663485, 10.285981140970881, 9.3840765419247614, 8.903649169700028, 11.175831081441116, 7.2740460679974248, 11.402228841869308, 7.7115806069717712]\n",
      "MMSE means: 0.514644493951, 95.5771405455, 9.68438727981\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 5, 2: 9, 3: 9, 4: 7, 5: 6, 6: 4, 7: 6, 8: 9, 9: 6, 10: 6}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [nan, nan, nan, 0.51374591340489695, nan, 0.62413985547179862, nan, nan, 0.56073428416666526, 0.58223023706956945]\n",
      "ADAS mse: [312.07413435256535, 302.7622339077887, 393.86748850269134, 78.201980504815467, 368.58084690401762, 38.039327822850161, 465.99720509366955, 274.08051686068711, 67.23657262519977, 48.747316314477438], rmse: [17.665620123634646, 17.400064192634137, 19.846095044181649, 8.8431883675977101, 19.198459492991034, 6.1676030857092421, 21.586968409058034, 16.555377279321878, 8.1997910110685002, 6.9819278365274897]\n",
      "ADAS means: nan, 234.958762289, 14.2445094843\n",
      "\n",
      "MMSE corr: [0.46937495244065336, 0.4081240696792045, 0.49045988217333153, 0.55964464539561898, 0.50874422680514364, 0.54283564735493373, 0.51873935131928528, 0.57226553670826075, 0.58209642745468837, 0.59147705230987169]\n",
      "MMSE mse: [112.24964451579548, 87.994889209707523, 109.71028969329288, 104.75326174029337, 88.047052146634741, 79.274968537100008, 115.74261644896997, 51.5706122437863, 130.01082256235628, 57.27195837892247], rmse: [10.594793273858414, 9.3805591096537277, 10.47426797887532, 10.234904090429639, 9.3833390723470469, 8.903649169700028, 10.758374247486001, 7.1812681501101387, 11.402228841869308, 7.5678238866217331]\n",
      "MMSE means: 0.524376179164, 93.6626115477, 9.5881207821\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 25, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 10, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 25}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 25, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 10, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 25}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 25, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 10, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 25}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 25, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 10, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 25}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 25, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 10, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 25}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 25, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 10, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 25}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 6, 2: 6, 3: 9, 4: 5, 5: 4, 6: 4, 7: 6, 8: 5, 9: 6, 10: 5}\n"
     ]
    }
   ],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "#exp_name = 'Exp11'\n",
    "#niter = 10000\n",
    "#modality = 'CT'\n",
    "start_fold = 1\n",
    "n_folds = 10\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "#preproc = 'no_preproc'\n",
    "#batch_size = 256\n",
    "snap_interval = 4000\n",
    "snap_start = 4000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "#Clinical_Scale = 'ADAS13'\n",
    "\n",
    "#MC_list = np.arange(1,11,1)\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "idx = 0  \n",
    "df_perf_dict_adas = {}\n",
    "df_perf_dict_mmse = {}\n",
    "df_perf_dict_adas_tuned = {}\n",
    "df_perf_dict_mmse_tuned = {}\n",
    "df_perf_dict_opt = {}\n",
    "\n",
    "for mc in MC_list:\n",
    "    print exp_name, mc, modality\n",
    "\n",
    "    for hype in hype_configs.keys():      \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "\n",
    "        for fid in fid_list:\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "            #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "            #with open(test_filename_txt, 'w') as f:\n",
    "            #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                    f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC']\n",
    "                elif modality == 'CT':\n",
    "                    f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_CT']\n",
    "                elif modality == 'HC_CT':\n",
    "                    f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "                elif modality == 'HC_CT_unified_hyp1':\n",
    "                    f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_HC_CT']\n",
    "                else:\n",
    "                    print 'Wrong modality'\n",
    "\n",
    "            #print 'Hype # {}, MC # {}, Fold # {}, Clinical_Scale {}'.format(hype, mc, fid, Clinical_Scale)\n",
    "            data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            if Clinical_Scale == 'ADAS13':\n",
    "                act_scores = load_data(data_path, 'adas_bl','no_preproc')\n",
    "            elif Clinical_Scale == 'MMSE': \n",
    "                act_scores = load_data(data_path, 'adas_m12','no_preproc')\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                act_scores_adas13 = load_data(data_path, 'adas_bl','no_preproc')\n",
    "                act_scores_mmse = load_data(data_path, 'adas_m12','no_preproc')\n",
    "            else:\n",
    "                print 'unknown clinical scale'\n",
    "\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort)\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            sub.call([\"cp\", baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort), baseline_dir + \n",
    "                      'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)])\n",
    "            #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "            #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "\n",
    "            if Clinical_Scale in ['ADAS13', 'MMSE']:                \n",
    "                multi_task = False\n",
    "                cs_list = ['opt']\n",
    "                iter_euLoss = []\n",
    "                iter_r = []        \n",
    "                iter_pred_scores = []\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = np.squeeze(results['X_out'])            \n",
    "                    iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                    iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "                fold_r[config_idx] = np.array(iter_r)\n",
    "                fold_act_scores[fid] = act_scores\n",
    "                fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                multi_task = True\n",
    "                cs_list = ['adas','mmse']\n",
    "                iter_euLoss_adas13 = []\n",
    "                iter_r_adas13 = []        \n",
    "                iter_pred_scores_adas13 = []\n",
    "                iter_euLoss_mmse = []\n",
    "                iter_r_mmse = []        \n",
    "                iter_pred_scores_mmse = []\n",
    "\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = results['X_out']   \n",
    "                    encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                    encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                    iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                    iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                    iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                    iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "                fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "                fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "                fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "                fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "                fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "                fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "                fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "\n",
    "    \n",
    "    if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "        fold_perf_dict = {'fold_r':fold_r,'fold_euLoss':fold_euLoss,'fold_act_scores':fold_act_scores,'fold_pred_scores':fold_pred_scores}\n",
    "    else: \n",
    "        fold_perf_dict = {'fold_r_adas13':fold_r_adas13,'fold_r_mmse':fold_r_mmse,'fold_euLoss_adas13':fold_euLoss_adas13,'fold_euLoss_mmse':fold_euLoss_mmse,\n",
    "                         'fold_act_scores_adas13':fold_act_scores_adas13,'fold_act_scores_mmse':fold_act_scores_mmse,\n",
    "                          'fold_pred_scores_adas13':fold_pred_scores_adas13,'fold_pred_scores_mmse':fold_pred_scores_mmse}\n",
    "        \n",
    "    opt_metric = 'euLoss' #euLoss or corr or both\n",
    "    #task_weights = {'ADAS':1,'MMSE':0} \n",
    "    save_multitask_results = False\n",
    "    CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/output/'\n",
    "    save_path = '{}{}_{}_NN_{}'.format(CV_model_dir,exp_name,mc,modality)\n",
    "    \n",
    "    # Create dictionaries of perfromance based on differnt task weights \n",
    "    for key, task_weights in {'adas':{'ADAS':1,'MMSE':0},'mmse':{'ADAS':0,'MMSE':1},'both':{'ADAS':1,'MMSE':1}}.items():\n",
    "        print 'Weights Tuned for: {}'.format(key)                              \n",
    "        results = compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path)  \n",
    "\n",
    "        # populate the perf dictionary for 10 MC x 10 folds\n",
    "        model_choice = 'APANN'    \n",
    "        for f, fid in enumerate(fid_list): \n",
    "            if key in ['adas','mmse']:\n",
    "                r_valid = results['{}_r'.format(key)][f]\n",
    "                MSE_valid = results['{}_mse'.format(key)][f]\n",
    "                RMSE_valid = results['{}_rmse'.format(key)][f]\n",
    "\n",
    "                if key == 'adas':\n",
    "                    df_perf_dict_adas_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                elif key == 'mmse':\n",
    "                    df_perf_dict_mmse_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                else: \n",
    "                    print 'unknown key'\n",
    "\n",
    "            elif key == 'both':                    \n",
    "                for cs in cs_list:            \n",
    "                    r_valid = results['{}_r'.format(cs)][f]\n",
    "                    MSE_valid = results['{}_mse'.format(cs)][f]\n",
    "                    RMSE_valid = results['{}_rmse'.format(cs)][f]\n",
    "\n",
    "                    if cs == 'adas':\n",
    "                        df_perf_dict_adas[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                    elif cs == 'mmse':\n",
    "                        df_perf_dict_mmse[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}                    \n",
    "                    else: \n",
    "                        print 'unknown key'\n",
    "\n",
    "\n",
    "            else: #single task dictionary\n",
    "                df_perf_dict_opt[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "\n",
    "            idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.2195444572928871"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbkAAAJoCAYAAABLB+y9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0LX+H+IGxrFAT9AxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02f945fad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.scatter(fold_act_scores_mmse[2], fold_pred_scores_mmse['hyp1_2'][3],s=50)\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "\n",
    "np.sqrt(85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results at: /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/output/\n"
     ]
    }
   ],
   "source": [
    "#Save df style dictionaly for seaborn plots\n",
    "cohort = 'ADNI1'\n",
    "update = 0\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_bl_up_{}.pkl'.format(exp_name, cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_m12_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse,df_perf_dict_path)\n",
    "\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_bl_tuned_up_{}.pkl'.format(exp_name,cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas_tuned,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_m12_tuned_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse_tuned,df_perf_dict_path)\n",
    "\n",
    "\n",
    "print 'saving results at: {}'.format(CV_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 25)\n",
    "n_rows = n_folds\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "plot_perf = False\n",
    "\n",
    "if multi_task:\n",
    "    euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "    corrs = [fold_r_adas13,fold_r_mmse]\n",
    "else:\n",
    "    euLosses = [fold_euLoss]\n",
    "    corrs = [fold_r]\n",
    "    \n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        \n",
    "        if plot_perf:\n",
    "            plt.figure(fid)\n",
    "            plt.subplot(n_rows,n_cols,1)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "            plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "            plt.legend()\n",
    "            plt.subplot(n_rows,n_cols,2)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "            plt.title('correlation, fid: {}'.format(fid))\n",
    "            plt.legend(loc=2)\n",
    "\n",
    "print preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path):\n",
    "    fold_r_dict = {}\n",
    "    fold_euLoss_dict = {}\n",
    "    fold_act_scores_dict = {}\n",
    "    fold_pred_scores_dict = {}\n",
    "\n",
    "    if multi_task:\n",
    "        fold_r_dict['ADAS'] = fold_perf_dict['fold_r_adas13']\n",
    "        fold_r_dict['MMSE'] = fold_perf_dict['fold_r_mmse']\n",
    "        fold_euLoss_dict['ADAS'] = fold_perf_dict['fold_euLoss_adas13']\n",
    "        fold_euLoss_dict['MMSE'] = fold_perf_dict['fold_euLoss_mmse']\n",
    "        fold_act_scores_dict['ADAS'] = fold_perf_dict['fold_act_scores_adas13']\n",
    "        fold_act_scores_dict['MMSE'] = fold_perf_dict['fold_act_scores_mmse']\n",
    "        fold_pred_scores_dict['ADAS'] = fold_perf_dict['fold_pred_scores_adas13']\n",
    "        fold_pred_scores_dict['MMSE'] = fold_perf_dict['fold_pred_scores_mmse']\n",
    "        \n",
    "        NN_results = computePerfMetrics(fold_r_dict, fold_euLoss_dict, fold_act_scores_dict, fold_pred_scores_dict, opt_metric, hype_configs, \n",
    "                                        Clinical_Scale,task_weights)\n",
    "        adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "        mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "        adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "        mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "        adas_rmse = NN_results['opt_ADAS']['CV_RMSE'].values()\n",
    "        mmse_rmse = NN_results['opt_MMSE']['CV_RMSE'].values()\n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['opt_ADAS']['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['opt_ADAS']['actual_CV_scores']\n",
    "        \n",
    "        print 'ADAS corr: {}'.format(adas_r)\n",
    "        print 'ADAS mse: {}, rmse: {}'.format(adas_mse,adas_rmse)\n",
    "        print 'ADAS means: {}, {}, {}'.format(np.mean(adas_r),np.mean(adas_mse),np.mean(adas_rmse))\n",
    "        print ''\n",
    "        print 'MMSE corr: {}'.format(mmse_r)\n",
    "        print 'MMSE mse: {}, rmse: {}'.format(mmse_mse,mmse_rmse)\n",
    "        print 'MMSE means: {}, {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse),np.mean(mmse_rmse))\n",
    "        print ''\n",
    "        print 'opt_hyp: {}'.format(opt_hyp)\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        \n",
    "        return {'adas_r':adas_r, 'adas_mse':adas_mse, 'adas_rmse':adas_rmse, 'mmse_r':mmse_r, 'mmse_mse':mmse_mse, 'mmse_rmse':mmse_rmse,\n",
    "               'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        NN_results = computeSingleTaskPerfMetrics(fold_perf_dict, opt_metric, hype_configs)\n",
    "        opt_r = NN_results['opt_r'].values()        \n",
    "        opt_mse = NN_results['opt_mse'].values()        \n",
    "        opt_rmse = NN_results['opt_rmse'].values()        \n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['actual_CV_scores']\n",
    "        print 'opt corr: {}'.format(opt_r)\n",
    "        print 'opt mse: {}'.format(opt_mse)\n",
    "        print 'means: {}, {}'.format(np.mean(opt_r),np.mean(opt_mse))\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        print ''\n",
    "    \n",
    "        return {'opt_r':opt_r, 'opt_mse':opt_mse, 'opt_rmse':opt_rmse,'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    \n",
    "\n",
    "\n",
    "    if save_multitask_results:\n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')        \n",
    "        save_path = save_path + '_' + st + '.pkl' \n",
    "        print 'saving results at: {}'.format(save_path)\n",
    "        output = open(save_path, 'wb')\n",
    "        pickle.dump(NN_results, output)\n",
    "        output.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = np.squeeze(results['predicted_CV_scores'][7])\n",
    "b = np.squeeze(results['actual_CV_scores'][7])\n",
    "\n",
    "print a , b                               \n",
    "print stats.pearsonr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    \n",
    "     # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "\n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    opt_rmse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "\n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    opt_rmse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "\n",
    "    print 'Clinical_Scale: {}'.format(Clinical_Scale)\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        print 'This function does not work for single clinical scale models. Use the code from the next cell'\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "\n",
    "        fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "        fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "        fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "        fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "\n",
    "        for fid in fid_hype_map.keys():\n",
    "            r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "            r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "            r_perf_array = np.array(fid_r_perf[fid])\n",
    "            euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "            euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "            euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "            if opt_metric == 'euLoss':\n",
    "                h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "            else:\n",
    "                h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "\n",
    "            opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "            opt_snap[fid] = snp\n",
    "\n",
    "            opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "            opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "            opt_rmse_adas[fid] = np.sqrt(2*euLoss_perf_array_adas[h,snp]) #RMSE\n",
    "            opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "            opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "            opt_rmse_mmse[fid] = np.sqrt(2*euLoss_perf_array_mmse[h,snp]) #RMSE\n",
    "\n",
    "            actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "            opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "            actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "            opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "        opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'CV_RMSE':opt_rmse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "        opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'CV_RMSE':opt_rmse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_snap':opt_snap, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "def computeSingleTaskPerfMetrics(fold_perf_dict,opt_metric,hype_configs):\n",
    "    corrs = fold_perf_dict['fold_r']\n",
    "    euLosses = fold_perf_dict['fold_euLoss']\n",
    "    act_scores = fold_perf_dict['fold_act_scores']\n",
    "    pred_scores = fold_perf_dict['fold_pred_scores']\n",
    "\n",
    "    \n",
    "    NN_multitask_results = {}\n",
    "   \n",
    "    snap_array = np.arange(snap_start,niter+1,snap_start)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = {}\n",
    "    opt_mse = {}\n",
    "    opt_rmse = {}\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "    actual_scores = {}\n",
    "    opt_pred_scores = {}\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "        # if want to find best hyp from mse values\n",
    "        if opt_metric == 'euLoss':\n",
    "            h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        else:\n",
    "            h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "            \n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        r = r_perf_array[h,snp]        \n",
    "        opt_r[fid] = r\n",
    "        opt_mse[fid] = 2*eu_loss\n",
    "        opt_rmse[fid] = np.sqrt(2*eu_loss)\n",
    "        #print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)        \n",
    "        opt_snap[fid] = snp\n",
    "        opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "        actual_scores[fid] = fold_act_scores[fid]\n",
    "        opt_pred_scores[fid] = fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp]\n",
    "\n",
    "    #print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r.values()), np.mean(opt_mse.values()))\n",
    "    #print opt_r\n",
    "    #print opt_mse\n",
    "    return {'opt_r':opt_r,'opt_mse':opt_mse,'opt_rmse':opt_rmse,'opt_hype':opt_hype,'opt_snap':opt_snap, 'actual_CV_scores':actual_scores,'predicted_CV_scores':opt_pred_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "\n",
    "model_file = 'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-05-06-17-14-49.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {4:0,5:1}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_ADAS13_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(5.64658243068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update fold performance for multitask\n",
    "print NN_results['opt_ADAS'].keys()\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "#model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl'\n",
    "model_file_soa = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-23-52.pkl'\n",
    "print 'current state of art: ' + model_file_soa\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file_soa,'rb'))\n",
    "\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "#load old file\n",
    "# model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-02-48.pkl'\n",
    "# print 'updated fold result file: ' + model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "idx_pairs = {1:1,3:3}\n",
    "CV_data = update_multifold_perf(boxplots_dir + model_file_soa, NN_results, idx_pairs)\n",
    "\n",
    "print \"\"\n",
    "print 'updated CV_data'\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "#print 'saving results at: {}'.format(save_name)\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(CV_data, output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():        \n",
    "        for idx in idx_pairs.keys():            \n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "\n",
    "    return CV_data\n",
    "\n",
    "def update_multifold_perf(saved_perf_file, new_perf_dict, idx_pairs):    \n",
    "    perf_metrics = ['CV_r', 'actual_CV_scores', 'CV_MSE', 'predicted_CV_scores']    \n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        if key == 'opt_hype':\n",
    "            for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                CV_data[key][idx_key] = new_perf_dict[key][idx_val]\n",
    "                \n",
    "        else:\n",
    "            for pm in perf_metrics:\n",
    "                for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                    CV_data[key][pm][idx_key] = new_perf_dict[key][pm][idx_val]\n",
    "    \n",
    "    return CV_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "model_files = ['Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl',             \n",
    "             'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-05-23-07-31-29.pkl']\n",
    "\n",
    "#'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-03-19-29-16_Fold9_10.pkl'\n",
    "\n",
    "Clinical_Scale = 'ADAS'\n",
    "perf_array = []\n",
    "for mf in model_files:\n",
    "    CV_data = pickle.load(open(boxplots_dir + mf,'rb'))['opt_{}'.format(Clinical_Scale)]        \n",
    "    perf_array.append(CV_data['CV_r'].values())\n",
    "\n",
    "print (np.max(np.array(perf_array),axis=0))\n",
    "print np.mean(np.max(np.array(perf_array),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "\n",
    "\n",
    "save_detailed_results = True\n",
    "multi_task = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results    \n",
    "    if not multi_task:\n",
    "        NN_results = {}\n",
    "        NN_results['CV_MSE'] = opt_mse\n",
    "        NN_results['CV_r'] = opt_r\n",
    "        NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "        NN_results['actual_CV_scores'] = actual_scores\n",
    "        NN_results['tid_snap_config_dict'] = opt_hype\n",
    "        print opt_r\n",
    "        print opt_mse\n",
    "        \n",
    "    else:\n",
    "        NN_results = NN_multitask_results\n",
    "        print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "        print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "        print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "        print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])\n",
    "        \n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_BOTH_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    for modality in modalities:\n",
    "        in_data = load_data(in_data_path, 'Fold_{}_{}'.format(fid,modality), preproc)         \n",
    "\n",
    "        # HDF5 is pretty efficient, but can be further compressed.\n",
    "        comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "        with h5py.File(out_data_path, 'a') as f:      \n",
    "            if modality == 'X_R_CT': #Fix the typo            \n",
    "                modality = 'X_CT'            \n",
    "\n",
    "            f.create_dataset('{}'.format(modality), data=in_data, **comp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold1_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold1_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold2_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold2_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold3_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold3_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold4_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold4_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold5_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold5_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold6_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold6_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold7_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold7_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold8_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold8_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold9_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold9_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold10_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp15_ADNI2_NN_OuterFold_MC_1_fold10_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp15_ADNI2_ADAS13_NN_valid_MC_1.h5\n"
     ]
    }
   ],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp15'\n",
    "exp_name_out = 'Exp15_MC'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "modalities = ['X_L_HC','X_R_HC','X_R_CT','adas_bl','adas_m12','dx']\n",
    "dataset = 'ADNI2'\n",
    "preproc = 'no_preproc' #binary labels\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/'\n",
    "\n",
    "for mc in np.arange(1,2,1):\n",
    "    for fid in np.arange(1,11,1):\n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_train_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_valid_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_ADAS13_NN_valid_MC_{}.h5'.format(exp_name,dataset,mc)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name_out)\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC: fid[1:4] 8000; fid[5,6] 6000, fid[7,8,9]: 8000 fid 10: 6000\n",
    "CT: fid[1:5] 12000 fid[6:10] 6000\n",
    "\n",
    "#spawn net\n",
    "ADNI_FF_train_hyp1_CT.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "cohort = 'ADNI2'\n",
    "modality = 'HC_CT'\n",
    "pretrain_snap_HC = 20000 #ADNI1: 5000\n",
    "pretrain_snap_CT = 20000 #ADNI1: 5000\n",
    "n_folds = 10\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/' \n",
    "#Custom snaps per fold\n",
    "#HC_iter = [8000,8000,8000,8000,6000,6000,8000,8000,8000,6000]\n",
    "#CT_iter = [12000,12000,12000,12000,12000,6000,6000,6000,6000,6000]\n",
    "#mc=1\n",
    "hc_hyp = 'hyp1'\n",
    "ct_hyp = 'hyp1'\n",
    "pretrain_hyp = 'hyp2' #This is hyp generated using optimal combination of hc and ct hyps. Prototxt file is saved with this hyp extension\n",
    "\n",
    "exp_name = 'Exp11_MC'\n",
    "\n",
    "for mc in np.arange(6,11,1):\n",
    "    for fid in np.arange(1,n_folds+1,1):\n",
    "        print 'fid: {}'.format(fid)\n",
    "        #for AE_branch in ['CT','R_HC','L_HC']:\n",
    "        for AE_branch in ['CT','HC']:\n",
    "            print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "            if AE_branch == 'L_HC':\n",
    "                params_FF = ['L_ff1', 'L_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'R_HC':\n",
    "                params_FF = ['R_ff1', 'R_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'HC':\n",
    "                params_FF = ['L_ff1','R_ff1']\n",
    "                AE_iter = pretrain_snap_HC\n",
    "                hyp = hc_hyp\n",
    "            elif AE_branch == 'CT':\n",
    "                #params_FF = ['ff1', 'ff2']\n",
    "                params_FF = ['ff1']\n",
    "                AE_iter = pretrain_snap_CT\n",
    "                hyp = ct_hyp\n",
    "                #fid for pretain is 1 because it's same definition for all the folds.\n",
    "                #Only use this during 1 of the modalities to avoid overwritting\n",
    "                print 'Spawning new net'\n",
    "                pretrain_net = caffe.Net(baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,pretrain_hyp,modality), caffe.TRAIN)\n",
    "            else:\n",
    "                print 'Wrong AE branch'\n",
    "\n",
    "            # conv_params = {name: (weights, biases)}\n",
    "            conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "            for conv in params_FF:\n",
    "                print 'target {} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "            # Review AE net params \n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hyp,AE_branch)\n",
    "            #model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "            model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hyp,AE_branch,AE_iter) \n",
    "\n",
    "            AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "            #params_AE = ['encoder1', 'code']\n",
    "            params_AE = params_FF #if you are using pretrained NN\n",
    "\n",
    "            # fc_params = {name: (weights, biases)}\n",
    "            fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "            for fc in params_AE:\n",
    "                print 'pretrained {} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "            #transplant net parameters\n",
    "            for pr, pr_conv in zip(params_AE, params_FF):\n",
    "                conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "                conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "            save_net = True\n",
    "            if save_net:\n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_{}_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                save_path = baseline_dir + net_name.format(mc,fid,cohort,pretrain_hyp,modality,pretrain_snap_HC,pretrain_snap_CT)\n",
    "                print \"Saving net to \" + save_path\n",
    "                pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_data['opt_ADAS']['CV_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Wxh = 0.5\n",
    "Whh = -1.0\n",
    "hb = -1.0\n",
    "x0 = 9\n",
    "x1 = 4\n",
    "x2 = -2\n",
    "\n",
    "h0 = 1/(1+exp(-Wxh*x0+hb))\n",
    "h1 = 1/(1+exp(h0*Whh - Wxh*x1 + hb))\n",
    "h2 = 1/(1+exp(h1*Whh - Wxh*x2 + hb))\n",
    "\n",
    "print h0,h1,h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
