{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import caffe\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(3)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "# Some defs to load data and extract encodings from trained net\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    #print net.blobs.items()[0]\n",
    "    #print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        #print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        #print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        #print net.blobs[input_node].shape\n",
    "        \n",
    "        data_layers.append(net.blobs[input_node])    \n",
    "        #print 'X_list shape: {}'.format(X_list[i].shape)\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "    \n",
    "     \n",
    "    net.reshape()            \n",
    "    #print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        #print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1, concat_param=dict(axis=1))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_CT_SpecCluster_dyn,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT_SpecCluster_dyn, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT_unified(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_HC_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_HC_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_HC_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_HC_CT,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_HC_CT, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=4, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas, n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT_SpecCluster_dyn, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "#     n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,n.ff1, concat_param=dict(axis=1))\n",
    "    #n.concat = L.Concat(n.X_L_HC,n.X_R_HC,n.X_CT, concat_param=dict(axis=1))\n",
    "    \n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=lr['COMB']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.dropC1 = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    n.dropC2 = L.Dropout(n.ff4, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2) #This is done automatically by caffe! \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff4, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        #n.ff2_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2ADAS13 = L.ReLU(n.ff2_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff4, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        #n.ff2_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2MMSE = L.ReLU(n.ff2_MMSE, in_place=True)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        n.ff1_DX = L.InnerProduct(n.ff3, num_output=node_sizes['DX_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1DX = L.ReLU(n.ff1_DX, in_place=True)\n",
    "        \n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_DX = L.InnerProduct(n.ff1_DX, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.DX_loss = L.SoftmaxWithLoss(n.output_DX, n.dx_cat3,loss_weight=tr['DX'])  \n",
    "    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_CT, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.dropC1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "        \n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_R_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_L_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))       \n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='HC_CT': #multimodal AE - experimental stage\n",
    "        HC_node_split = 0.8\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_L_HC = L.InnerProduct(n.X_L_HC, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.9*node_sizes['En1'])))        \n",
    "        n.NLinEn_L_HC = L.ReLU(n.encoder_L_HC, in_place=True)\n",
    "        \n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_CT = L.InnerProduct(n.X_CT, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.1*node_sizes['En1'])))        \n",
    "        n.NLinEn_CT = L.ReLU(n.encoder_CT, in_place=True)\n",
    "        \n",
    "        #Concat         \n",
    "        n.encoder1 = L.Concat(n.encoder_L_HC,n.encoder_CT, concat_param=dict(axis=1))\n",
    "        \n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers (or common encoder layers for multimodal) \n",
    "    \n",
    "#     n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.Sigmoid(n.encoder2, in_place=True)\n",
    "    #code layer\n",
    "#    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='xavier'))  \n",
    "    \n",
    "#     n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "    \n",
    "    #Decoder layers\n",
    "    if modality == 'CT':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)                    \n",
    "    \n",
    "    elif modality =='R_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    elif modality =='HC_CT':        \n",
    "        n.decoder_L_HC = L.InnerProduct(n.code, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_L_CT = L.ReLU(n.decoder_L_HC, in_place=True)\n",
    "        n.decoder_CT = L.InnerProduct(n.code, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_CT = L.ReLU(n.decoder_CT, in_place=True)\n",
    "        \n",
    "        n.output_L_HC = L.InnerProduct(n.decoder_L_HC, num_output=node_sizes['out_HC'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_HC = L.SigmoidCrossEntropyLoss(n.output_L_HC, n.X_L_HC)\n",
    "        \n",
    "        n.output_CT = L.InnerProduct(n.decoder_CT, num_output=node_sizes['out_CT'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_CT = L.EuclideanLoss(n.output_CT, n.X_CT)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    return n.to_proto()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode):\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 20\n",
    "    \n",
    "    if multimodal_autoencode:\n",
    "        train_loss_HC = zeros(niter)\n",
    "        test_loss_HC = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_loss_CT = zeros(niter)\n",
    "        test_loss_CT = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "        for it in range(niter):            \n",
    "            solver.step(1)  # SGD by Caffe \n",
    "            train_loss_HC[it] = solver.net.blobs['loss_HC'].data\n",
    "            train_loss_CT[it] = solver.net.blobs['loss_CT'].data\n",
    "            \n",
    "            if it % test_interval == 0:                        \n",
    "                t_loss_HC = 0\n",
    "                t_loss_CT = 0                                \n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()   \n",
    "                    t_loss_HC += solver.test_nets[0].blobs['loss_HC'].data\n",
    "                    t_loss_CT += solver.test_nets[0].blobs['loss_CT'].data\n",
    "                \n",
    "                test_loss_HC[it // test_interval] = t_loss_HC/(test_iter)\n",
    "                test_loss_CT[it // test_interval] = t_loss_CT/(test_iter)\n",
    "                print 'HC Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_HC[it], np.sum(train_loss_HC)/it, test_loss_HC[it // test_interval])\n",
    "                print 'CT Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_CT[it], np.sum(train_loss_CT)/it, test_loss_CT[it // test_interval])\n",
    "                \n",
    "        perf = {'train_loss':[train_loss_HC, train_loss_CT],'test_loss':[test_loss_HC,test_loss_CT]}\n",
    "    else: \n",
    "        \n",
    "        #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "        # losses will also be stored in the log\n",
    "        test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "        if not multi_task:\n",
    "            train_loss = zeros(niter)\n",
    "            test_loss = zeros(int(np.ceil(niter / test_interval)))  \n",
    "\n",
    "        else: \n",
    "            if multi_label:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_DX_loss = zeros(niter)\n",
    "                test_DX_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "            else:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_MMSE_loss = zeros(niter)\n",
    "                test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "\n",
    "        #output = zeros((niter, batch_size))\n",
    "        #solver.restore()\n",
    "        #the main solver loop\n",
    "        for it in range(niter):\n",
    "            #solver.net.forward()\n",
    "            solver.step(1)  # SGD by Caffe    \n",
    "            # store the train loss\n",
    "            if not multi_task:\n",
    "                train_loss[it] = solver.net.blobs['loss'].data        \n",
    "            else: \n",
    "                if multi_label:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_DX_loss[it] = solver.net.blobs['DX_loss'].data\n",
    "                else:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "            # store the output on the first test batch\n",
    "            # (start the forward pass at conv1 to avoid loading new data)\n",
    "            #solver.test_nets[0].forward()\n",
    "            #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "            # run a full test every so often\n",
    "            # (Caffe can also do this for us and write to a log, but we show here\n",
    "            #  how to do it directly in Python, where more complicated things are easier.)\n",
    "            if it % test_interval == 0:        \n",
    "                t_loss = 0\n",
    "                t_ADAS13_loss = 0\n",
    "                t_MMSE_loss = 0\n",
    "                t_DX_loss = 0\n",
    "                correct = 0\n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()                \n",
    "                    if not multi_task:\n",
    "                        t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                        if multi_label:\n",
    "                            correct += sum(solver.test_nets[0].blobs['output'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "                    else: \n",
    "                        if multi_label:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_DX_loss += solver.test_nets[0].blobs['DX_loss'].data\n",
    "                            correct += sum(solver.test_nets[0].blobs['output_DX'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "\n",
    "                        else:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "                if not multi_task:\n",
    "                    test_loss[it // test_interval] = t_loss/(test_iter)\n",
    "                    if multi_label:\n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                    print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                        it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval], test_acc[it // test_interval])\n",
    "                else:\n",
    "                    if multi_label:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_DX_loss[it // test_interval] = t_DX_loss/(test_iter) \n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'DX Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                            it, train_DX_loss[it], np.sum(train_DX_loss)/it, test_DX_loss[it // test_interval],test_acc[it // test_interval])\n",
    "                    else:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_MMSE_loss[it // test_interval] = t_MMSE_loss/(test_iter)             \n",
    "\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "        if not multi_task:\n",
    "            perf = {'train_loss':[train_loss],'test_loss':[test_loss],'test_acc':[test_acc]}\n",
    "        else:\n",
    "            if multi_label:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_DX_loss],'test_loss':[test_ADAS13_loss,test_DX_loss],'test_acc':[test_acc]}\n",
    "            else:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "                \n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,snap_prefix):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    \n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    #s.solver_type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 100000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 4000\n",
    "    s.snapshot_prefix = snap_prefix\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MC # 1, Hype # hyp5, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold1/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (288.431884766,inf), test loss: 179.976390076\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (323.582214355,inf), test loss: 366.6157547\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (68.953414917,77.2712170277), test loss: 45.7024087906\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.32393550873,71.2592615809), test loss: 3.03450455368\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (118.323638916,62.3687829332), test loss: 42.9915239811\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.62394046783,37.0832602123), test loss: 3.34076844454\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (43.2340087891,57.3741030738), test loss: 43.7268391609\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.88056516647,25.6847160834), test loss: 3.07741906643\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (39.1201667786,54.7763134308), test loss: 44.8770431519\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.73263883591,19.9754076325), test loss: 3.2593978554\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (29.5261249542,53.2422967943), test loss: 42.7216545582\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.82671451569,16.5482366043), test loss: 2.61747018397\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (96.5016937256,52.2047894483), test loss: 46.389526844\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (5.77820682526,14.2652716763), test loss: 3.39970276952\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (29.0952301025,51.3937218815), test loss: 41.156188345\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.6673579216,12.6333410478), test loss: 2.63350859582\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (34.2522621155,50.8050623949), test loss: 44.9093351841\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.0354681015,11.407364438), test loss: 3.10666913986\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (12.0286273956,50.2993933886), test loss: 41.2338630676\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.07654833794,10.4524762845), test loss: 3.04378267825\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (59.1813354492,49.8699244166), test loss: 45.577464962\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.25769901276,9.68412789063), test loss: 2.87076246738\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (27.0860729218,49.4738702472), test loss: 37.6163984776\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (5.09982681274,9.05416060814), test loss: 2.77585724592\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (47.9022598267,49.1525336947), test loss: 43.8772996664\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.14841413498,8.52623120671), test loss: 2.97142283916\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (15.1485061646,48.8758612377), test loss: 42.6283575535\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.36476159096,8.07924120321), test loss: 3.26250007451\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (34.084274292,48.6427381494), test loss: 39.8125259876\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.01781463623,7.6967886069), test loss: 2.62816470861\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (42.7416954041,48.4170745433), test loss: 46.1116236687\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.10544800758,7.36425520217), test loss: 3.17828494012\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (56.6125068665,48.2034352611), test loss: 42.3409368515\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.84728479385,7.07280171259), test loss: 2.64754744172\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (46.9117202759,47.9980153236), test loss: 42.2823113918\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.87749123573,6.81414176128), test loss: 3.06188485324\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (31.7734355927,47.7917074847), test loss: 40.7961431265\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.28676950932,6.58358257673), test loss: 2.87122020125\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (18.6987800598,47.5754749556), test loss: 45.7062093258\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.67682886124,6.37597016211), test loss: 2.82643584013\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (38.3260498047,47.4048269189), test loss: 37.7955540657\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.87758922577,6.18818053922), test loss: 2.65155049562\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (49.9150314331,47.2115276738), test loss: 41.9539269447\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.27476787567,6.01729382549), test loss: 2.81447764635\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (12.2173538208,47.0385882724), test loss: 39.0733419418\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.05114459991,5.86138541709), test loss: 3.01907529533\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (24.900177002,46.8584364461), test loss: 41.8821382523\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.36090779305,5.71850964632), test loss: 2.78358972967\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (22.2537994385,46.6757437877), test loss: 43.9661551476\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.03197717667,5.5874151306), test loss: 3.00457330048\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (33.6568946838,46.4922288219), test loss: 41.0314986706\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.24562215805,5.46540182955), test loss: 2.52158856988\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (78.4111785889,46.2994926394), test loss: 41.611354208\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.48238992691,5.35229188547), test loss: 3.01153321862\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (84.0664825439,46.0987028972), test loss: 38.0525135517\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.12353610992,5.24621123199), test loss: 2.76998621523\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (37.9200515747,45.8829318269), test loss: 41.9891551018\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.47520279884,5.14713693659), test loss: 2.81214468479\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (14.8809404373,45.6686460351), test loss: 35.2188164711\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.374934911728,5.05312439303), test loss: 2.72319548726\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (17.3243904114,45.4577417116), test loss: 39.3564739943\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.13863027096,4.96523454984), test loss: 2.69151544273\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (22.7316741943,45.2401573702), test loss: 36.3984885931\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.48315834999,4.88276689517), test loss: 2.90427448601\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (44.9084625244,45.0052138929), test loss: 37.8331394672\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.79022121429,4.8043049649), test loss: 2.74885470867\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (15.5704078674,44.7645013405), test loss: 37.9475875378\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.93202614784,4.72994999164), test loss: 2.88118274212\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (22.4200725555,44.5125084897), test loss: 35.4739195585\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.46613264084,4.65944298152), test loss: 2.36609309912\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (35.6308364868,44.2288355044), test loss: 37.4420601845\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.8336353302,4.59197266773), test loss: 3.03073857427\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (27.7516002655,43.9251389748), test loss: 33.939230299\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.99017596245,4.52729330911), test loss: 2.63878085911\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (54.4140586853,43.6261125076), test loss: 36.1183328629\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.66542673111,4.46526004737), test loss: 2.71980311871\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (16.492805481,43.303690338), test loss: 32.1404596806\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.47213363647,4.40587523057), test loss: 2.66138831377\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (12.9561347961,42.9837852545), test loss: 35.0087099552\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.862237870693,4.34881001682), test loss: 2.61498394907\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (10.9581785202,42.6423682681), test loss: 26.4268456936\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.36738967896,4.29404333509), test loss: 2.49533814788\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (53.1421661377,42.3001003101), test loss: 33.8003844738\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.97158169746,4.24179266031), test loss: 2.88184555173\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (40.0005645752,41.9510639158), test loss: 31.3998402119\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.35937547684,4.1915970043), test loss: 2.95922802389\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (26.684337616,41.5947447561), test loss: 29.0691741467\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.90280389786,4.14356150479), test loss: 2.56051041782\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (76.3674697876,41.2363390153), test loss: 31.9616862535\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.36761784554,4.09699698861), test loss: 2.96620723158\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (11.2060623169,40.8839720625), test loss: 31.0917336464\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.85395073891,4.05227796568), test loss: 2.62258092463\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (12.2732210159,40.5338808081), test loss: 31.9514825344\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.49451184273,4.00868175114), test loss: 2.82755894959\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (21.5787830353,40.1884065081), test loss: 30.4126561165\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.47966861725,3.96683112874), test loss: 2.88532624245\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (18.5758190155,39.8483605358), test loss: 33.4650141954\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.12538290024,3.92662356825), test loss: 2.72071376443\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (12.8896036148,39.5107611259), test loss: 28.0575847149\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.4010617733,3.88784504247), test loss: 2.75160770565\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (17.9730834961,39.1809351753), test loss: 32.4953330994\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.714019179344,3.85052985608), test loss: 2.85200644433\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (26.6672172546,38.8590861652), test loss: 29.8063274145\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.02180695534,3.81484249978), test loss: 3.03352802992\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (16.0751247406,38.5366760133), test loss: 31.0556953907\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.466380000114,3.77987690402), test loss: 2.83733641505\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (13.9844512939,38.2202764384), test loss: 31.388411665\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.47852993011,3.7460051023), test loss: 2.91396251321\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (39.086315155,37.9172767182), test loss: 32.1092919111\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.21534347534,3.71312652694), test loss: 2.6999800384\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (22.1443901062,37.6193682978), test loss: 30.0090922356\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.01403391361,3.68115617852), test loss: 2.69311933517\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (28.5855445862,37.3290233927), test loss: 32.051321125\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.78696298599,3.65040540075), test loss: 2.91283732951\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (7.41620159149,37.0432509372), test loss: 32.606617856\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.999633073807,3.62042049079), test loss: 2.7612039119\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (17.0431251526,36.7642574598), test loss: 29.8007155418\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.65965008736,3.59158543688), test loss: 2.84308534861\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (25.3951473236,36.4942365361), test loss: 32.3070950031\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.22062897682,3.56363061704), test loss: 2.85499970317\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (16.919752121,36.2251081683), test loss: 29.8968338966\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.343161642551,3.5363823035), test loss: 2.86301266849\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (48.3968811035,35.9625159248), test loss: 33.1436303616\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.69679880142,3.50988622409), test loss: 2.87328455299\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.831987381,35.7088065266), test loss: 31.3764425755\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.97796583176,3.48387387781), test loss: 2.93823818266\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (33.8968505859,35.4604646954), test loss: 31.3660486937\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.28954577446,3.45852589471), test loss: 2.56770298481\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (22.8723640442,35.2170764747), test loss: 30.8266609669\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.012570858,3.43385100723), test loss: 2.76926612258\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (7.77814292908,34.977680824), test loss: 32.7858108521\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.61792802811,3.40980932409), test loss: 2.86770389974\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (31.6864738464,34.7436775929), test loss: 30.2248131275\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.37858510017,3.38650966052), test loss: 2.65139190853\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (15.4135189056,34.5146125121), test loss: 31.5148505211\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.889703273773,3.36382544233), test loss: 2.93915291429\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (21.4152984619,34.2912346482), test loss: 32.8414563179\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.972997963428,3.34179625739), test loss: 2.61546359658\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (15.7972602844,34.0685518554), test loss: 34.4199284077\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.71183276176,3.32013833623), test loss: 2.93282865584\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (10.2286815643,33.8508103614), test loss: 32.5897024155\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.904885411263,3.29884166256), test loss: 2.89414232969\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (9.45696640015,33.6395368519), test loss: 29.4262060165\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.40258264542,3.27790432316), test loss: 2.7811399892\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (7.93292808533,33.4324571374), test loss: 32.3444066048\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.56705641747,3.25751094496), test loss: 2.49360746443\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (21.3079586029,33.2285144862), test loss: 30.7160135746\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.62477385998,3.23761524648), test loss: 2.84955458641\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (19.1542854309,33.0272296584), test loss: 31.3101122856\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.38873386383,3.21806001657), test loss: 2.51999666989\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (17.2926101685,32.8295945339), test loss: 32.1677008629\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.468803942204,3.19907318085), test loss: 2.75170262754\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (8.21559047699,32.6371958034), test loss: 32.1186686993\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.931054830551,3.18058569161), test loss: 2.93525813371\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (13.7978057861,32.4447142505), test loss: 32.7939487219\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.81389439106,3.16235743517), test loss: 2.63878897429\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (26.5411949158,32.2561524306), test loss: 27.8779283524\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.30202877522,3.1444401824), test loss: 2.57617482841\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (25.0803794861,32.0733075397), test loss: 32.2158012867\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.78985357285,3.12675930346), test loss: 2.79438230395\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (7.41859054565,31.8912196288), test loss: 29.1890636444\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.99943518639,3.10951366668), test loss: 2.88996104598\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (26.0644779205,31.7135293778), test loss: 30.2367947578\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.667560636997,3.09248239077), test loss: 2.67401177585\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (27.500328064,31.535866465), test loss: 29.5764644623\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.47936224937,3.07574321755), test loss: 2.75331694186\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (10.1398200989,31.3629992749), test loss: 34.2189578056\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.604890227318,3.05949445863), test loss: 2.74776816219\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (8.24054527283,31.1925168378), test loss: 29.3397236586\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.679751455784,3.04351744443), test loss: 2.63473802954\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (14.5113658905,31.0232767182), test loss: 35.0804461002\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.57582449913,3.02795367582), test loss: 2.98954825401\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (14.8239955902,30.8565075989), test loss: 32.6665456772\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.00814652443,3.01252807928), test loss: 2.57822422981\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (5.17575502396,30.6925820051), test loss: 30.1238294601\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.10159635544,2.99733461303), test loss: 2.67826976776\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (11.9957962036,30.5319786337), test loss: 31.8148597956\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.8780040741,2.9822193936), test loss: 2.71258718967\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (19.0879631042,30.373284445), test loss: 28.2955205917\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (5.24655532837,2.96748088569), test loss: 2.82559599429\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (17.1802597046,30.215271997), test loss: 33.5729545832\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.32857370377,2.95291440511), test loss: 2.83297158182\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (21.4972000122,30.059717478), test loss: 32.966558671\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.7624335289,2.93861287411), test loss: 2.87861642241\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (14.0259361267,29.9063689462), test loss: 33.5896348476\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.866973280907,2.92460822344), test loss: 2.60606345236\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (15.8043451309,29.7565387395), test loss: 30.0195328951\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.11721897125,2.91097720233), test loss: 2.64807235003\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (8.6041021347,29.6050391078), test loss: 36.0287575245\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.275018393993,2.89743230216), test loss: 2.91772790551\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.7960395813,29.4570636367), test loss: 31.3016923428\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.49459528923,2.8841012298), test loss: 2.62658643126\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (36.7773475647,29.3123384288), test loss: 30.6348731995\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.62772607803,2.87081984613), test loss: 2.83580287099\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (20.7513046265,29.168679656), test loss: 32.0006744385\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.11436641216,2.85779711669), test loss: 2.62639309466\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (29.0552520752,29.0264054366), test loss: 28.2340296507\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.21165847778,2.84498143095), test loss: 2.67710005939\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (5.69545555115,28.8842998593), test loss: 32.9272758484\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.877192258835,2.83224560796), test loss: 2.76851653755\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (7.88383769989,28.7450721632), test loss: 28.2220673561\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.38871836662,2.81986672849), test loss: 2.70850266218\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (8.36645030975,28.6085020828), test loss: 33.885422039\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.62587583065,2.80769849164), test loss: 2.62197479159\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (11.6783094406,28.4708786847), test loss: 33.234145546\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.47974678874,2.79567514212), test loss: 2.82555007041\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (20.6208305359,28.336578543), test loss: 33.7632338047\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.74703741074,2.78386696595), test loss: 2.81985087693\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (6.64324092865,28.2036677534), test loss: 29.4610150337\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.66745901108,2.77204792933), test loss: 2.52296285927\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (21.5050907135,28.0725782488), test loss: 32.6584510803\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.88943862915,2.76043290338), test loss: 2.88593125641\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (23.3478870392,27.9423669527), test loss: 31.1392745256\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.96075356007,2.74896007226), test loss: 2.50389935672\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (9.24691390991,27.8123981731), test loss: 28.6765949249\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.95406985283,2.73761144849), test loss: 2.69617608041\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (17.7658309937,27.6842255384), test loss: 33.4835868835\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.774959266186,2.7264554089), test loss: 2.87065337002\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (8.86553287506,27.557875388), test loss: 29.1689297915\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.665634512901,2.7155410442), test loss: 2.75291511416\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (17.0357875824,27.4332956526), test loss: 37.8024745941\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.28842702508,2.70479171772), test loss: 3.08591650724\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (8.96893501282,27.3080086516), test loss: 29.9523385048\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.16165590286,2.69415335679), test loss: 2.77873365879\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (8.07977390289,27.1852959502), test loss: 34.4852831364\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.932657897472,2.68359667991), test loss: 2.72973840237\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (10.8059711456,27.0639588797), test loss: 30.4281466722\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.34022974968,2.6730771079), test loss: 2.62623934746\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (12.7572793961,26.9438839157), test loss: 33.4301264763\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.55080223083,2.66274170725), test loss: 2.93956615031\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (17.0796909332,26.8242874796), test loss: 32.4853121758\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.25764989853,2.65252590299), test loss: 2.54963796139\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (17.3372879028,26.7049903147), test loss: 30.4376405716\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.27736759186,2.64240411356), test loss: 2.74480406642\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (14.9277896881,26.5876182583), test loss: 32.7757076025\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.65793466568,2.63247318685), test loss: 2.71648311317\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (11.7461929321,26.472065656), test loss: 28.6222825408\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.492728948593,2.62273843689), test loss: 2.8285852313\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.77171754837,26.3554606856), test loss: 34.9142422676\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.844887018204,2.6130624208), test loss: 2.91470948905\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (10.9445056915,26.2418697786), test loss: 29.2507407665\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.798256337643,2.60351764329), test loss: 2.69892585874\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (17.7789382935,26.1291472956), test loss: 36.2219684124\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.18322682381,2.59396499932), test loss: 2.80620933175\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (5.04730987549,26.0164734818), test loss: 29.9826303959\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (0.868085026741,2.58458193252), test loss: 2.54585794806\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (9.21946144104,25.9055352152), test loss: 34.7565724373\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.19464755058,2.57527898234), test loss: 2.98702968955\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (22.5333576202,25.7940557427), test loss: 32.6287162781\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.75956332684,2.56603338868), test loss: 2.62952560782\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (5.05756950378,25.684259583), test loss: 32.1580369949\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.06130182743,2.55696132732), test loss: 2.82651446611\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (5.75940418243,25.5759731961), test loss: 32.7470059872\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.998603999615,2.54801658583), test loss: 2.71961912215\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (5.54446125031,25.4677670504), test loss: 28.3181488991\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.76394283772,2.53924993314), test loss: 2.65641929507\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (3.60043644905,25.3606128546), test loss: 34.7279294491\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.890781283379,2.53050793145), test loss: 2.83526240289\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (4.26219654083,25.2548079373), test loss: 30.8767482758\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.81109666824,2.52182817777), test loss: 2.76361221969\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (7.64725112915,25.1498589887), test loss: 35.331555891\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.3353574276,2.51315356202), test loss: 2.64659383297\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (13.1070861816,25.0455015416), test loss: 31.4907141209\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (4.5102853775,2.50463782458), test loss: 2.70948104262\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (15.2339057922,24.9413773909), test loss: 36.9260460377\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (3.16563463211,2.49615964679), test loss: 3.00433917642\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (13.4199323654,24.8377562612), test loss: 32.1391582489\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.88851499557,2.48776341795), test loss: 2.71581301689\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (13.0161390305,24.7354356839), test loss: 33.2981507301\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.973375499249,2.4794813823), test loss: 2.94224237204\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (7.90205669403,24.634666272), test loss: 32.6751074791\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.693449020386,2.47141245581), test loss: 2.52234040201\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.26500368118,24.5328408211), test loss: 30.4302013874\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.419425070286,2.46334715934), test loss: 2.71964778602\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.82125473022,24.4332098761), test loss: 33.8785590887\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.924747228622,2.45536354203), test loss: 2.84071798027\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (21.0019569397,24.3344152705), test loss: 28.7494094849\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.610208928585,2.4473465343), test loss: 2.66023201197\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (11.3195285797,24.2358122847), test loss: 35.2077430248\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.79519402981,2.43947299165), test loss: 2.61177410483\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (14.3635482788,24.1379255291), test loss: 31.4390280485\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.61862373352,2.43163697707), test loss: 2.77266793698\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (5.88589859009,24.0399325094), test loss: 34.9245056629\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.52803266048,2.42385500499), test loss: 2.69401338696\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (9.62539482117,23.9430366013), test loss: 32.167677784\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.29794836044,2.41619950722), test loss: 2.71623377502\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.03532648087,23.8477347457), test loss: 34.2680077553\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.295781373978,2.4086508661), test loss: 2.99030100405\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (4.78426122665,23.7519163381), test loss: 36.7249708414\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.507350325584,2.40120313034), test loss: 2.66624959409\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (8.93169116974,23.6576138678), test loss: 29.7521794558\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.95939791203,2.39379178223), test loss: 2.689223364\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (3.80975151062,23.5640337901), test loss: 33.8864113331\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.67119836807,2.38638564139), test loss: 2.86393575072\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (10.8998012543,23.4710975737), test loss: 29.0038132906\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.37157344818,2.37905180381), test loss: 2.79331667274\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (12.7268638611,23.3782081922), test loss: 35.6696297169\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.684835195541,2.37174661047), test loss: 2.90255845785\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (6.25053119659,23.2857953061), test loss: 30.5197168112\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.35769629478,2.36452301368), test loss: 2.79448239803\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (7.68773841858,23.1939536562), test loss: 39.7094880104\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.582926213741,2.35739011526), test loss: 2.83454866409\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (4.19862270355,23.1029664469), test loss: 32.1265645742\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.730219721794,2.35033218506), test loss: 2.71436936855\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (9.88939476013,23.0131516386), test loss: 40.6908429146\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.252635657787,2.34340242098), test loss: 3.24999409914\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (8.87428092957,22.9228436482), test loss: 36.1443708897\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.10555410385,2.33650605387), test loss: 2.5937523216\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (15.1178398132,22.8340704739), test loss: 32.4889796734\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.831497907639,2.32960312892), test loss: 2.81002121568\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (6.2823638916,22.7456442216), test loss: 34.0643863678\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.62612056732,2.32272806561), test loss: 2.76964908242\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (7.90791702271,22.6576463997), test loss: 32.0002188683\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.473636567593,2.3159104606), test loss: 2.96342115998\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (11.3266992569,22.5701118236), test loss: 36.8107497215\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.372402250767,2.30914467926), test loss: 2.88997677565\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (9.99903583527,22.4827896481), test loss: 32.2655241728\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.286226511,2.30245505111), test loss: 2.73477934152\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (9.49655342102,22.3961235567), test loss: 37.7044746399\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.22807943821,2.29582216335), test loss: 2.76834056079\n",
      "\n",
      "MC # 1, Hype # hyp5, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold2/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (338.597442627,inf), test loss: 182.188418579\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (301.675170898,inf), test loss: 363.701426697\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (20.3669509888,57.665085166), test loss: 46.3310787201\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.851340770721,71.4570913391), test loss: 3.51101327538\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (64.9421615601,49.958436203), test loss: 38.0762601614\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.33064651489,37.2726691219), test loss: 3.35513575375\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (21.9364547729,47.2823131428), test loss: 42.3240701199\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.80805826187,25.8713717324), test loss: 3.37503852248\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.5149383545,45.6098009546), test loss: 36.1785662174\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.3591337204,20.1616143838), test loss: 3.57038314342\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (47.0115585327,44.5558407194), test loss: 38.8492208004\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.94738030434,16.7340674001), test loss: 2.74847611487\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (10.2262210846,43.5872780976), test loss: 39.586978054\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.43791604042,14.4289113232), test loss: 3.79704750776\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (54.4160270691,42.7722830218), test loss: 37.120072937\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (5.92367315292,12.7778947726), test loss: 2.43270686567\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (37.831111908,42.0098764306), test loss: 39.4890592098\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.804179430008,11.5303312613), test loss: 3.6235624969\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (16.6761608124,41.2800978329), test loss: 33.1344304562\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.61647701263,10.5583692877), test loss: 2.76120082736\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (37.3316078186,40.5564929055), test loss: 41.9099488735\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.03805971146,9.77718145065), test loss: 3.53456829488\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (45.4020996094,39.9005049251), test loss: 29.8392162323\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.9887983799,9.1355462814), test loss: 2.48744246066\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (13.6283988953,39.2337437789), test loss: 38.5702597141\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.98297071457,8.59646203537), test loss: 3.45619194508\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (38.2326889038,38.6032569237), test loss: 29.6202209234\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.24572002888,8.14103916646), test loss: 3.29486682415\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (40.0353469849,37.9594308481), test loss: 37.6833402634\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.12697505951,7.74816736282), test loss: 3.2659715116\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (11.6838607788,37.3350073257), test loss: 30.6438928604\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.79850769043,7.40634117973), test loss: 3.4639354229\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (25.0539665222,36.7290660082), test loss: 32.0190332651\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.6339097023,7.10581546885), test loss: 2.5141753912\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (25.1819438934,36.1210487355), test loss: 34.8384864092\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.80299735069,6.83821980102), test loss: 3.73753667474\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (31.9372634888,35.5310602014), test loss: 30.4186672211\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.86160612106,6.59962108621), test loss: 2.37821692824\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (27.7846126556,34.9734613123), test loss: 35.8149040699\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.79089307785,6.38414925037), test loss: 3.52950870395\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (20.3241424561,34.451545465), test loss: 26.7621492147\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.5091971159,6.18929674081), test loss: 2.47902697027\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (21.8866024017,33.9427770225), test loss: 37.2973670006\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.61350011826,6.0125191239), test loss: 3.46202651262\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (14.9556808472,33.4554200835), test loss: 24.2160259962\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.85062789917,5.85167058266), test loss: 2.44868790805\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (7.20168066025,32.9974839905), test loss: 36.2598567963\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.852080106735,5.70376514298), test loss: 3.32935531139\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (65.2389068604,32.558479577), test loss: 26.660360384\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.58296394348,5.56770462511), test loss: 3.29947695136\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (8.31785583496,32.1266694132), test loss: 34.5500461578\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.59564304352,5.44135064332), test loss: 2.94278390408\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (12.8199033737,31.730770887), test loss: 29.5214758873\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.56455183029,5.32454647554), test loss: 3.50831566155\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (14.7554750443,31.352888333), test loss: 30.2486549854\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.58415985107,5.21464310043), test loss: 2.37426892221\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (11.5032958984,30.9965938011), test loss: 33.7676739693\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.794327497482,5.11276449852), test loss: 3.59814873934\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (57.3520126343,30.6495700057), test loss: 27.2980111599\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.3536324501,5.01724850512), test loss: 2.37697051764\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (18.542263031,30.3214719013), test loss: 34.8540522099\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.98211538792,4.92873462613), test loss: 3.46247375607\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (24.1244430542,30.0111457497), test loss: 26.2715156317\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.60531258583,4.8447821693), test loss: 2.63472985625\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (23.993309021,29.7062687782), test loss: 35.7036694288\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.559814572334,4.76578107484), test loss: 3.36291671991\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (51.0619049072,29.4132825871), test loss: 27.2318769455\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.91370105743,4.6911498449), test loss: 2.98895475566\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (12.9594917297,29.1385503316), test loss: 34.4325961351\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.59538936615,4.62045704343), test loss: 3.21272261739\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (11.0432748795,28.8731144531), test loss: 26.9048129559\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.52962136269,4.55292567748), test loss: 3.29765182734\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (18.5188083649,28.6209912684), test loss: 29.1410689831\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.34342265129,4.48928391011), test loss: 2.65059626997\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (5.84014892578,28.3679056716), test loss: 31.5833713055\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.01962125301,4.4283844797), test loss: 3.54558569491\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (10.3630867004,28.1324785713), test loss: 29.7152721882\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.4000736475,4.37138469295), test loss: 2.20251652002\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (22.2441139221,27.9054552039), test loss: 31.866553688\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.24413514137,4.31648536806), test loss: 3.47807065845\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (7.48370838165,27.6789059305), test loss: 28.5367493629\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.39449334145,4.26412600548), test loss: 2.40195002556\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (20.011636734,27.4601319757), test loss: 34.8707267284\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.43260669708,4.2137318382), test loss: 3.42685723901\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (25.1090660095,27.2533118808), test loss: 27.4445629835\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.24003577232,4.16559875221), test loss: 2.48599929512\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (23.3539142609,27.0519232921), test loss: 35.4333256245\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.734234392643,4.11900033884), test loss: 3.24411126971\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (57.2106666565,26.8555873164), test loss: 26.7856386662\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.57751822472,4.07491208902), test loss: 3.08519627452\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (18.4907913208,26.6594647856), test loss: 35.2524706125\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.20411634445,4.03212273976), test loss: 3.12728326023\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.0537776947,26.473801791), test loss: 26.752182889\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (4.43655014038,3.99147109765), test loss: 3.38498285711\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (21.8074169159,26.2948062145), test loss: 31.0060288429\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.71998095512,3.95222751494), test loss: 2.55451897681\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.5497913361,26.1126209082), test loss: 30.1368838787\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.70084404945,3.91414615004), test loss: 3.57169340253\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (4.88379764557,25.9376241087), test loss: 29.43634305\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.36831915379,3.87737542258), test loss: 2.19155535102\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (29.5684242249,25.768722082), test loss: 32.8043490887\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (5.62987518311,3.8417509183), test loss: 3.3336448431\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (11.8570785522,25.6056256681), test loss: 26.5843419075\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.943867087364,3.80730770566), test loss: 2.39113949239\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (13.2215633392,25.4418464061), test loss: 35.7313782692\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.77151703835,3.77402826772), test loss: 3.19418180585\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (18.4981155396,25.2815659349), test loss: 24.8008647442\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.21514046192,3.74202677373), test loss: 2.38304386735\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (6.40210533142,25.1274745299), test loss: 35.7622105837\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.80470085144,3.71106936868), test loss: 3.15372452438\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (53.1498603821,24.975066008), test loss: 25.1888893127\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.49484443665,3.6810741537), test loss: 3.08924271613\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (26.4378395081,24.8229758937), test loss: 34.7708711624\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.51590931416,3.65183587772), test loss: 3.10357083678\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (7.12163448334,24.6761741496), test loss: 26.5555845261\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.3477332592,3.62352046642), test loss: 3.31135617048\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (7.73754024506,24.5327525469), test loss: 31.4741354942\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.77542114258,3.59555609703), test loss: 2.40327442884\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (3.95301222801,24.3926179859), test loss: 32.3161166668\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.370025515556,3.56874630993), test loss: 3.37732024193\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (37.7532653809,24.2518467844), test loss: 29.023380518\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.91900897026,3.54244566547), test loss: 2.29808043242\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (18.0556735992,24.114139807), test loss: 34.1748283863\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.65705657005,3.51726811501), test loss: 3.21887198091\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.6041965485,23.9813811242), test loss: 26.0314852715\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.12298750877,3.49259077877), test loss: 2.42927332222\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.6986818314,23.8463714199), test loss: 36.2012488365\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.561191320419,3.46851911952), test loss: 3.11265589297\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (21.2306556702,23.7156933286), test loss: 23.9379874706\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.81361842155,3.44510680735), test loss: 2.39344816506\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (6.36109256744,23.5882350542), test loss: 34.9943335533\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.32611274719,3.4222099956), test loss: 3.04364348948\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.1428766251,23.4621280664), test loss: 25.0122422934\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.13492918015,3.39960581453), test loss: 3.00053093433\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (18.1658153534,23.3385615603), test loss: 29.4538606644\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.848840475082,3.37779561384), test loss: 2.56442076862\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (12.401763916,23.2131504744), test loss: 28.7785628796\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.441897124052,3.35626935758), test loss: 3.30141331553\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (9.35898399353,23.0918619356), test loss: 31.2841952562\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.80516421795,3.33563280508), test loss: 2.35531262457\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (17.9447097778,22.9742450861), test loss: 34.029901886\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.903406560421,3.31532888499), test loss: 3.2927713722\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (8.19434547424,22.8541235528), test loss: 28.6069367886\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.01832532883,3.2954584799), test loss: 2.33971256018\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (24.4998283386,22.7376207842), test loss: 33.4361465931\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.13387107849,3.27592233242), test loss: 3.0854331404\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (10.9383792877,22.623839268), test loss: 26.0420483589\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.637313485146,3.25685367256), test loss: 2.41275700629\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (20.9676074982,22.5111695939), test loss: 35.6077311993\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.980888426304,3.23797997295), test loss: 3.04571362734\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (35.7905349731,22.3990070689), test loss: 26.1221535921\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.93721985817,3.21975034208), test loss: 2.76085829437\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.4722232819,22.2865540226), test loss: 35.5895658493\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.794243872166,3.20174016338), test loss: 3.03312593102\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (9.58283996582,22.1768059664), test loss: 25.5169993043\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.72311353683,3.18423065609), test loss: 3.09212382734\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (13.6878643036,22.0705425665), test loss: 31.7193638325\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.0029027462,3.16713183247), test loss: 2.59025540948\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (15.4574899673,21.9616252735), test loss: 30.2154907942\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.89224815369,3.15028468087), test loss: 3.35029600263\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (6.43573999405,21.8555896759), test loss: 30.9829945087\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.906874775887,3.13359490459), test loss: 2.22681542188\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (11.578294754,21.7519688033), test loss: 33.0829412699\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (4.40660142899,3.11735650416), test loss: 3.1752202332\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (7.49848461151,21.6494211906), test loss: 28.6374136209\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.16441845894,3.10125896768), test loss: 2.38872937262\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (15.1473760605,21.5458760304), test loss: 37.3822706223\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.83883810043,3.08551103891), test loss: 3.21516343355\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (17.6298656464,21.4437900348), test loss: 26.2815671444\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.936807155609,3.07013383651), test loss: 2.30098020732\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (4.64307117462,21.3434575168), test loss: 36.1245895863\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.623386085033,3.05499140535), test loss: 3.02364081889\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (36.9117126465,21.2443433159), test loss: 27.4846592426\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.27638101578,3.04023996844), test loss: 2.94014077783\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (24.4910850525,21.1454394929), test loss: 35.8700244904\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.42259979248,3.0256241953), test loss: 3.00719357431\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (7.83174037933,21.0476409248), test loss: 26.9699234009\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.43946146965,3.01122962361), test loss: 3.13402331173\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (5.06736421585,20.9517722374), test loss: 33.5244700432\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.39802074432,2.9969619645), test loss: 2.4432077691\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (4.94961357117,20.8567537991), test loss: 32.4887026548\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.421541035175,2.98305959706), test loss: 3.26131273508\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (15.5995101929,20.7605590736), test loss: 31.7232856989\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.30653429031,2.96926318312), test loss: 2.26182071269\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (17.7105484009,20.6661298706), test loss: 35.0577235222\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.99211072922,2.95588287717), test loss: 3.16487644911\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (6.12125349045,20.5733598291), test loss: 27.0162671089\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.858837366104,2.94257464158), test loss: 2.36797788739\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.3757362366,20.4795719033), test loss: 37.1544877529\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.10632872581,2.92955912944), test loss: 2.94814427197\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (5.85623550415,20.3885300402), test loss: 25.4156356335\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.83547091484,2.91677001365), test loss: 2.35613410175\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (5.96439647675,20.2976859595), test loss: 36.4308036327\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.93015515804,2.90406442448), test loss: 2.98493105769\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (18.6911735535,20.2077505781), test loss: 24.9391343594\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.51058554649,2.89148640514), test loss: 2.82932653427\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (17.2374820709,20.1186105088), test loss: 34.383989203\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.717279672623,2.87920268895), test loss: 2.70956795514\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (10.6788768768,20.0278698988), test loss: 28.0834063053\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.923750042915,2.86693807813), test loss: 3.11248997748\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (5.62886428833,19.9393633905), test loss: 33.3226437092\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.93481492996,2.85508148035), test loss: 2.38171284199\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (15.7495384216,19.8527445392), test loss: 36.5108104229\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.05800354481,2.84329103864), test loss: 3.25926763117\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (5.81673908234,19.7640266344), test loss: 30.7814630747\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.28099811077,2.83168217017), test loss: 2.4184200868\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (11.0747289658,19.6783473749), test loss: 35.6750611305\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.30124127865,2.82022351585), test loss: 3.0827572763\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (8.30980396271,19.5927497034), test loss: 27.7030292273\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.747053980827,2.80883975512), test loss: 2.42061665654\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (15.8098831177,19.507428184), test loss: 37.5149966955\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.96837079525,2.79758227839), test loss: 3.02245198786\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (25.7894096375,19.4226135175), test loss: 28.4613920212\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.91376137733,2.78658472712), test loss: 2.67695073932\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (13.0294733047,19.3371089789), test loss: 36.3553931713\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.853524267673,2.77561527234), test loss: 2.9701280266\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (10.5196142197,19.2529676287), test loss: 27.0025571346\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.01657581329,2.76487879128), test loss: 2.87668247223\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (6.63415241241,19.1705315493), test loss: 32.7738825321\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.600317299366,2.75429103089), test loss: 2.68858847022\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.1387739182,19.0864027882), test loss: 31.4151837826\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.35003423691,2.74384245897), test loss: 3.20720800459\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (6.57498550415,19.0046130734), test loss: 33.8301268816\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.00831854343,2.73343828068), test loss: 2.2552277118\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (6.5967669487,18.9232682308), test loss: 34.5088885307\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (3.78385710716,2.72319963093), test loss: 3.16183809638\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (2.83936357498,18.8422236221), test loss: 32.5461182833\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.04546427727,2.71300664889), test loss: 2.39039815366\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (15.5388374329,18.7613029521), test loss: 36.8144320965\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (4.43908262253,2.70303310832), test loss: 3.14063946903\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (8.92126560211,18.6803257003), test loss: 29.7615559101\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.80860054493,2.69310025907), test loss: 2.45943653286\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.90122509003,18.6003774149), test loss: 38.022129631\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.61343550682,2.68331890885), test loss: 2.95613277555\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (32.0638198853,18.5216991594), test loss: 32.3046033859\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.75188493729,2.67374228528), test loss: 2.99618486166\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (9.95244503021,18.4419809903), test loss: 36.5643219948\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.752620697021,2.66417315121), test loss: 2.99890552312\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.38336896896,18.3636765619), test loss: 27.909624362\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.78662699461,2.65474081164), test loss: 3.11725937724\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (7.39347219467,18.2864629671), test loss: 35.6445800781\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.21528482437,2.64534026934), test loss: 2.63700554967\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (7.60066270828,18.2089068756), test loss: 32.143562603\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.529909729958,2.63610172382), test loss: 3.30074709654\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (5.06091785431,18.1313865758), test loss: 34.1051185846\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.05063414574,2.62693209186), test loss: 2.33629912138\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (11.9881973267,18.0545108057), test loss: 35.9676873207\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.20798683167,2.61793631089), test loss: 3.12386699617\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (7.47775554657,17.9783273968), test loss: 29.7014615297\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.20717644691,2.6089402252), test loss: 2.42440768778\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (10.0809135437,17.9024129056), test loss: 38.7251735687\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.93414324522,2.60018735158), test loss: 2.94224457443\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (4.73330593109,17.8272791412), test loss: 27.9827677011\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.652976989746,2.59145854912), test loss: 2.39271950126\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (5.86705970764,17.7524677422), test loss: 37.9470267534\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.66011202335,2.582790559), test loss: 2.93680612743\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (14.9225988388,17.6787103587), test loss: 26.0161804438\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.944188058376,2.57420717363), test loss: 2.80251024216\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (9.43574142456,17.6046960673), test loss: 37.731766057\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.470635056496,2.56573770302), test loss: 3.0280516386\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (5.58514595032,17.5304511159), test loss: 28.6749803782\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.58854317665,2.55730186817), test loss: 3.09368087947\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (12.0219163895,17.4574290975), test loss: 36.1675733328\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.3394882679,2.54904765565), test loss: 2.54364956915\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (4.81632041931,17.3848463937), test loss: 36.9657800674\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.14746618271,2.54079970177), test loss: 3.26578609049\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (6.12571716309,17.3122298442), test loss: 33.755755496\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.19477570057,2.53273767311), test loss: 2.44898871779\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (4.29028654099,17.2408581666), test loss: 37.2347942829\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.04470205307,2.52466655527), test loss: 3.08404876292\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (9.31134033203,17.1694873608), test loss: 29.9873159885\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.758850932121,2.51664998362), test loss: 2.46475713849\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (6.01982736588,17.098957628), test loss: 38.9482863426\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.856917798519,2.50875565489), test loss: 2.93961797357\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (10.229139328,17.0283649974), test loss: 28.1316086769\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.19919383526,2.5009360729), test loss: 2.39175151587\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (13.5185070038,16.957731281), test loss: 38.6310205936\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.963849067688,2.49316806969), test loss: 2.96218528152\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (6.00168418884,16.8879521521), test loss: 28.4672111511\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.449481070042,2.48550523601), test loss: 2.79757413268\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (4.8946018219,16.8187410403), test loss: 34.1281396389\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.933889985085,2.47791000386), test loss: 2.70244140625\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (7.95505428314,16.7493890547), test loss: 31.2056287289\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.68246102333,2.47046002329), test loss: 3.15107711852\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (7.13979578018,16.6814000947), test loss: 36.7250278234\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (3.34094643593,2.46300337015), test loss: 2.50537508726\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.65188646317,16.6133338035), test loss: 36.5748254776\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (3.11725592613,2.45558307252), test loss: 3.1175642848\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (3.76806735992,16.5460107212), test loss: 33.9521357536\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.13005161285,2.44826457946), test loss: 2.46798447222\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (13.0257472992,16.4787753044), test loss: 36.7260862827\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (4.34199237823,2.44102706447), test loss: 2.97465822399\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (3.86421608925,16.4115658794), test loss: 31.5437152147\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.8395190239,2.43382935785), test loss: 2.51989137828\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (6.00586891174,16.345325117), test loss: 39.4376833916\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.5255740881,2.42670079748), test loss: 2.94235990942\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (22.5018768311,16.2794929523), test loss: 33.4142391205\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.4901099205,2.41970188587), test loss: 2.91565292776\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.52377748489,16.2135659362), test loss: 38.0477032423\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.20315623283,2.4127240974), test loss: 3.01747139096\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.38276720047,16.1487294498), test loss: 28.4135046005\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.687193512917,2.40580110921), test loss: 2.98785358071\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (4.44641017914,16.0842309892), test loss: 36.1606608391\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.19727039337,2.39889577288), test loss: 2.75464408994\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (8.31658172607,16.0200080457), test loss: 33.1149214745\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.663580298424,2.39211783187), test loss: 3.23756586015\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.19889736176,15.9559603577), test loss: 35.8164168596\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.3898332119,2.38534373657), test loss: 2.43665180206\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (9.03928565979,15.8921596442), test loss: 36.2718954325\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.51044344902,2.37867454262), test loss: 3.09098460078\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (5.78565740585,15.8292285125), test loss: 31.7847281694\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.78443837166,2.37202989208), test loss: 2.52307828069\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (8.55540847778,15.7663810912), test loss: 41.2997706413\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.16306853294,2.36551812458), test loss: 3.14843369126\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.45289993286,15.7040496708), test loss: 29.7461389542\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.22912430763,2.35902209092), test loss: 2.40743128508\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (3.79877138138,15.6424421949), test loss: 38.6094620705\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.60673093796,2.35256383332), test loss: 2.9252696678\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (9.81001663208,15.58125243), test loss: 27.5203969955\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.932144403458,2.3461431146), test loss: 2.72130222321\n",
      "\n",
      "MC # 1, Hype # hyp5, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold3/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (273.776794434,inf), test loss: 185.148607635\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (316.40020752,inf), test loss: 354.422521973\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (28.3488063812,85.4497170658), test loss: 40.4019563198\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.68009090424,56.2134999956), test loss: 3.27620196939\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (108.973068237,65.5604227223), test loss: 35.3171450138\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.56950926781,29.6745591927), test loss: 3.52970778346\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (40.6930847168,58.7856078765), test loss: 39.6546882153\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.96806430817,20.8239180574), test loss: 3.20295236707\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (9.73181724548,55.1147620027), test loss: 38.3274232864\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.76732110977,16.3860914721), test loss: 3.24710337222\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (39.4906806946,52.8721789482), test loss: 41.5393680096\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (8.49784374237,13.7241648251), test loss: 2.79242228866\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (37.8895950317,51.2781578016), test loss: 40.1851989746\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (8.49797344208,11.9482209234), test loss: 3.57363727689\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (50.4322547913,50.0076294697), test loss: 37.7527070045\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.11968922615,10.6764692688), test loss: 2.80372906923\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (38.4171333313,48.9711320601), test loss: 39.7570614815\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.38570690155,9.71987036531), test loss: 3.26894872785\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (25.4883270264,48.0713150807), test loss: 35.1895319939\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.15225028992,8.97130577108), test loss: 3.08841846585\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (27.6835422516,47.2523084748), test loss: 36.0690862179\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.67695963383,8.36853917993), test loss: 3.09160556495\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (34.4834976196,46.4825928004), test loss: 31.4277168751\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.87440109253,7.87280762152), test loss: 3.27095316648\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (41.1195068359,45.8036082826), test loss: 33.2659971714\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.15628528595,7.4557903167), test loss: 3.15452990532\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (53.5821037292,45.129094729), test loss: 32.6012565136\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (4.34907197952,7.09892424989), test loss: 3.43018063307\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (51.0534286499,44.5083710769), test loss: 35.868104744\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.19772672653,6.7939157409), test loss: 2.72442353368\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (35.0193862915,43.8747274566), test loss: 34.0174968719\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.76151251793,6.5265992625), test loss: 3.46556585133\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (54.009349823,43.2521339239), test loss: 32.3673338413\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.58687782288,6.29256768684), test loss: 2.55664687455\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (48.0505218506,42.6287633917), test loss: 33.775222683\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.76226329803,6.08367498917), test loss: 3.17684028149\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (23.1242179871,41.9982996164), test loss: 29.7080113888\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.07168674469,5.89705194383), test loss: 2.91162883341\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (56.2193984985,41.3697481281), test loss: 30.8366350651\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.00068879128,5.72827831183), test loss: 2.99777296484\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (22.1709594727,40.7609735616), test loss: 25.9549632549\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.7222058773,5.57561491955), test loss: 3.10847285986\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (8.98402309418,40.1561755334), test loss: 28.5235968351\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (5.31204509735,5.43491675401), test loss: 3.11791698337\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (26.5163860321,39.5707716322), test loss: 25.4127987385\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (8.47570610046,5.30678961198), test loss: 3.43648068607\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (21.6968727112,38.9971071798), test loss: 28.9123952389\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (6.19493818283,5.18924329803), test loss: 2.7458987087\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (23.7032299042,38.4357630886), test loss: 29.0284855127\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.01899266243,5.0807711823), test loss: 3.3789283216\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (16.2070560455,37.8916620966), test loss: 30.4782201529\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.59559416771,4.98072031572), test loss: 2.58396897018\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (15.906460762,37.3684793077), test loss: 28.9002192974\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.07865762711,4.88792883003), test loss: 3.13978613019\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (15.7834396362,36.8574190652), test loss: 28.0668443203\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.29467916489,4.8009506936), test loss: 2.82690775096\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (21.1481819153,36.3620639525), test loss: 29.9019270182\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.57513809204,4.71953777144), test loss: 3.14996316731\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (26.4929733276,35.8992294541), test loss: 24.3003602266\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.86468040943,4.6428408519), test loss: 2.7756586805\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (65.4721984863,35.4484265649), test loss: 27.8800785065\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.38707685471,4.56998406747), test loss: 3.2385731101\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (24.498374939,35.0183989629), test loss: 24.4060713291\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.0261797905,4.50239341491), test loss: 3.33510790467\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (17.5397586823,34.5993635594), test loss: 28.2502208948\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.03683233261,4.4380740464), test loss: 2.75975095928\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (37.9942474365,34.2041507666), test loss: 28.6170786858\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.37946510315,4.37805983379), test loss: 3.33637921065\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (21.0068016052,33.8238169627), test loss: 28.5993906021\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.18488025665,4.32093540628), test loss: 2.48232133985\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (22.3815803528,33.4543579047), test loss: 29.8054488659\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.46507716179,4.26678605086), test loss: 3.13989375532\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (23.852180481,33.0970654353), test loss: 29.4493604183\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.13853406906,4.21487518662), test loss: 2.71938551217\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (16.6282730103,32.7586426314), test loss: 29.9513745308\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.44721031189,4.16548658738), test loss: 3.1367361933\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (15.2937936783,32.4319465484), test loss: 28.0118006468\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (5.01167631149,4.11752060251), test loss: 2.87023437023\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (21.9381332397,32.1176172928), test loss: 28.3245192289\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (7.25634384155,4.07211161082), test loss: 3.17662602663\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (18.448802948,31.8124222352), test loss: 25.065513134\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.1679558754,4.02864347195), test loss: 3.31980315447\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (14.1183757782,31.518138534), test loss: 29.1756607056\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.63967490196,3.98707475176), test loss: 3.0520134002\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (12.8878946304,31.2345527085), test loss: 27.2771526814\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.98626208305,3.94746609663), test loss: 3.1836188525\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (9.39402008057,30.9635615231), test loss: 30.0251191616\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.83229291439,3.90953903859), test loss: 2.55399707556\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (12.0260095596,30.6963827288), test loss: 30.5975872755\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.22460758686,3.87276425391), test loss: 3.22513560951\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (22.9502182007,30.4369537603), test loss: 28.3365055561\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.86034107208,3.8372012395), test loss: 2.57808109075\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (31.1316165924,30.1910599863), test loss: 33.1657227278\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.35418891907,3.80272132928), test loss: 3.27039009333\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (58.7274703979,29.9490561995), test loss: 27.6046621323\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.46717596054,3.76902930971), test loss: 2.77828704119\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (18.4518375397,29.7136957496), test loss: 28.6480400085\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.77235388756,3.73705440272), test loss: 3.06531682163\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (13.9164600372,29.4824333466), test loss: 25.6247514248\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.43282413483,3.70581990227), test loss: 3.14439211488\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (26.5858364105,29.2615865344), test loss: 29.2189686775\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.92637944221,3.67610825201), test loss: 3.10224287808\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (16.0161056519,29.0466766772), test loss: 26.9513110876\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.5007519722,3.64721140181), test loss: 3.26562058628\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (20.3811225891,28.8350538167), test loss: 31.124865818\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.87556791306,3.61928883522), test loss: 2.58407828808\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (17.3343410492,28.6278250439), test loss: 28.9973011971\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.857471227646,3.59197934584), test loss: 3.35123281181\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (12.4675750732,28.4282085861), test loss: 29.2794804335\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.85232424736,3.56543319546), test loss: 2.3899911195\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (14.5166015625,28.2331546297), test loss: 32.1483335495\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.91632580757,3.53914496417), test loss: 3.12266277671\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (20.0794391632,28.0421474885), test loss: 28.3936320782\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (6.32219648361,3.5139261963), test loss: 2.75971736312\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.3141326904,27.8534952371), test loss: 30.4582363129\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.38521504402,3.48929946864), test loss: 3.04843400717\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (12.1456737518,27.669856699), test loss: 26.2246682644\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.72129356861,3.46547628902), test loss: 2.96620534062\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (12.2932939529,27.4908018127), test loss: 30.2458767414\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.45604705811,3.44239380369), test loss: 3.05022827983\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (7.02240085602,27.3174920415), test loss: 26.2122294903\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.77092993259,3.42000734581), test loss: 3.20567258894\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (9.40306949615,27.1441928126), test loss: 30.3443599463\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.12090790272,3.39801727665), test loss: 2.67172693312\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (21.4193267822,26.9747494087), test loss: 29.6006210327\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.90325093269,3.37643252063), test loss: 3.28150501847\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (36.2256317139,26.8117136735), test loss: 31.4173710823\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.62650203705,3.35518135673), test loss: 2.42630990744\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (48.8930702209,26.6496440451), test loss: 30.8459148407\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.1529340744,3.33418221257), test loss: 3.13224457502\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (21.5407505035,26.4896921222), test loss: 29.7842241764\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.53499186039,3.31401873713), test loss: 2.6448371321\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.0224370956,26.3313638335), test loss: 32.7341200829\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.90944743156,3.29408033518), test loss: 3.08895935416\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (19.7663249969,26.1783079816), test loss: 25.4688054562\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.5826728344,3.27489952543), test loss: 2.5877466023\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (15.2019300461,26.0284338985), test loss: 30.687586689\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.23091411591,3.25606560459), test loss: 3.00648640692\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (17.126121521,25.879527635), test loss: 26.4521090508\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.34946870804,3.2376918469), test loss: 3.12839611173\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (14.6001834869,25.732805564), test loss: 30.7841832161\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.81476855278,3.21956344622), test loss: 2.70377799422\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (8.70820808411,25.5896659164), test loss: 30.1996700764\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.16494977474,3.20173194515), test loss: 3.22913762033\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (11.1130619049,25.4489637421), test loss: 31.3099704742\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.01416945457,3.18396106948), test loss: 2.3071685642\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (18.0755958557,25.3097567628), test loss: 32.6859535217\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (5.32986545563,3.16678952371), test loss: 3.16570354104\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (15.9024295807,25.1711385204), test loss: 32.0103672981\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.97179675102,3.14983836762), test loss: 2.54934410155\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (9.89119625092,25.0354247107), test loss: 33.8969013214\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.80017209053,3.13333277519), test loss: 3.0742474556\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (11.5470733643,24.9022123777), test loss: 28.3265622616\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.14472401142,3.11719144384), test loss: 2.67832406759\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (5.50874471664,24.7723680333), test loss: 31.6648406982\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.50581133366,3.10146449099), test loss: 2.97846567035\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (6.20520401001,24.6415192536), test loss: 26.5165406227\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.804372191429,3.08592281337), test loss: 3.07876954973\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (19.2432384491,24.5136018971), test loss: 32.229712534\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.13895320892,3.07058850295), test loss: 2.95845048726\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (37.3601493835,24.3888423286), test loss: 29.8461845636\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.43429088593,3.05534771937), test loss: 3.12551056147\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (41.9958610535,24.2645346928), test loss: 33.0714447021\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.81230580807,3.04029775644), test loss: 2.39881716669\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (20.5339546204,24.1408182632), test loss: 33.3736745834\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.977043747902,3.02570524084), test loss: 3.21917578578\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (8.67401123047,24.0181599111), test loss: 31.0162490368\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.64334630966,3.01122057409), test loss: 2.46640830934\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (16.2998180389,23.8984379064), test loss: 36.0108584881\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.49883556366,2.99717504363), test loss: 3.19535030127\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (12.7198600769,23.780609162), test loss: 28.5100056648\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.64754486084,2.98331784689), test loss: 2.67659626305\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (14.9203882217,23.663422105), test loss: 32.7217763901\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.28310751915,2.96978068894), test loss: 2.98342768848\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (11.3967342377,23.547589635), test loss: 26.7426010132\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.784612298012,2.95636750325), test loss: 2.96883111298\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (6.33139848709,23.4335209729), test loss: 32.8163304806\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.08645486832,2.94306395577), test loss: 3.03284530193\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (8.81694221497,23.3210257385), test loss: 29.1401516914\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.46534490585,2.92978116889), test loss: 3.14477117062\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (15.9778766632,23.2090100954), test loss: 34.1330839634\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (4.55369520187,2.91689942456), test loss: 2.55586702526\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.395362854,23.0972426273), test loss: 31.1056575298\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.47776722908,2.90409771444), test loss: 3.25146576762\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (9.50674629211,22.9874720752), test loss: 32.352504921\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.48023462296,2.89158378424), test loss: 2.31267406791\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (10.139954567,22.8790543701), test loss: 35.6452293873\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.12508440018,2.87927019777), test loss: 3.16821016669\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (4.65813493729,22.7727516667), test loss: 29.2782045841\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.852960109711,2.86722621105), test loss: 2.62560819238\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (3.76860785484,22.6655393873), test loss: 34.1827143908\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.456637203693,2.85532380833), test loss: 2.97327265441\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (14.4735746384,22.5606431123), test loss: 27.3846691132\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (3.25741791725,2.84353943548), test loss: 2.8401833415\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (29.2015132904,22.4574894157), test loss: 34.4562468052\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.83118081093,2.8317352165), test loss: 3.06145666987\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (35.8894882202,22.3545828124), test loss: 27.8067107677\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.6205496788,2.82009609455), test loss: 3.11536687016\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (18.0789070129,22.2515929796), test loss: 33.6982813358\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.732900321484,2.80872265463), test loss: 2.66869663596\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (7.93057155609,22.1494347766), test loss: 31.5261716843\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.14401125908,2.79741968061), test loss: 3.23782483935\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (12.6249055862,22.0490308333), test loss: 34.0816544533\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.49619948864,2.78640298566), test loss: 2.36734500527\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (10.8312978745,21.9501034086), test loss: 34.1673062325\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.13810133934,2.77548473203), test loss: 3.09334181249\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (13.7328157425,21.8514588472), test loss: 31.3615678787\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.62254905701,2.76481001336), test loss: 2.62588695735\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (7.44194412231,21.7537363851), test loss: 36.4245222092\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.733808994293,2.75420723927), test loss: 3.17218900919\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.53097677231,21.6570286216), test loss: 26.7046782017\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.74228715897,2.74364422102), test loss: 2.53510512412\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (7.29412078857,21.5613920639), test loss: 34.3328485489\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.0747628212,2.73308249215), test loss: 3.02374887466\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (12.0470838547,21.4659479815), test loss: 27.6988799572\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.83317255974,2.72282559458), test loss: 3.15346851945\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (8.317237854,21.3705871849), test loss: 34.4198563099\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.03233361244,2.71256246698), test loss: 2.70784424543\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (9.30199241638,21.2766787355), test loss: 33.0091034412\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.07056713104,2.70252519356), test loss: 3.21715803742\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (9.62106323242,21.183647125), test loss: 33.8073600769\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.09645318985,2.69261211409), test loss: 2.31905044317\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (4.26371383667,21.0918429993), test loss: 35.3418349266\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.895938396454,2.6828827439), test loss: 3.12986130416\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (3.10412120819,20.9996672234), test loss: 34.2591545105\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.685833096504,2.67327475705), test loss: 2.59883659184\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (9.15423965454,20.9090359886), test loss: 37.1569809914\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.28877162933,2.66371687596), test loss: 3.11807588488\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (21.004442215,20.8195175098), test loss: 29.1397527695\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.49259114265,2.6541111787), test loss: 2.65900823474\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (29.6151123047,20.7302928485), test loss: 35.3063114166\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.59814345837,2.64465774803), test loss: 2.98301045224\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (15.9798212051,20.6407652068), test loss: 27.8107843876\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.36612343788,2.63536966098), test loss: 3.07580702603\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (7.76694202423,20.5519426353), test loss: 36.0707194805\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.409030258656,2.62611602269), test loss: 3.06303936243\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (9.89029502869,20.4641265829), test loss: 31.0927521706\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.45456290245,2.61707858544), test loss: 3.09876696169\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (9.4058008194,20.3775554454), test loss: 35.770685339\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.63652443886,2.60808591922), test loss: 2.50913038254\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (12.4614868164,20.2912251208), test loss: 36.2834996223\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.88038396835,2.59930546327), test loss: 3.26278030574\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (4.99111652374,20.2055079445), test loss: 33.7307905674\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.710267901421,2.59054785568), test loss: 2.60183741152\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (4.39515018463,20.1204485115), test loss: 39.9311727524\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.71322131157,2.5818014375), test loss: 3.32006656528\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (6.75352954865,20.0361473919), test loss: 30.0247614861\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.41905546188,2.57304159779), test loss: 2.6558553353\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (9.42675018311,19.951991995), test loss: 36.7353257656\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (3.2165722847,2.56454666175), test loss: 3.07412580252\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (6.90883827209,19.8680423173), test loss: 28.5616357803\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.84186053276,2.55600310874), test loss: 2.9482681483\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (9.57392311096,19.7849846303), test loss: 36.2043123245\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.910436451435,2.54762676352), test loss: 3.0780247122\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.07789897919,19.7025346687), test loss: 30.0146124363\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.10388529301,2.53933734553), test loss: 3.14543586075\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (4.07853078842,19.6209840702), test loss: 36.6638689995\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.833064317703,2.53118997399), test loss: 2.60526801124\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (2.20013141632,19.5393066795), test loss: 32.444071269\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.890716254711,2.52315352965), test loss: 3.26520339251\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (8.58621406555,19.4589561922), test loss: 34.232454443\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.26803350449,2.51511807886), test loss: 2.35223632604\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (15.6503572464,19.3790610539), test loss: 38.919562006\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.13677799702,2.50702019862), test loss: 3.26851550043\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (23.9667015076,19.2996038315), test loss: 30.8096042633\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.69374108315,2.49907423738), test loss: 2.59689132422\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (13.5190219879,19.2198978375), test loss: 36.8903997898\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.58326840401,2.49124511034), test loss: 3.01199921966\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.84736299515,19.1407433708), test loss: 29.1827775002\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.370715081692,2.48342787978), test loss: 2.8528518118\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (9.21005725861,19.0624402252), test loss: 36.7331073761\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.45436620712,2.47577687687), test loss: 3.10696373135\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.19777679443,18.9849047921), test loss: 29.3913573503\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.11670386791,2.4681282135), test loss: 3.15343176723\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (11.8251247406,18.9077744025), test loss: 36.5928666115\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.36537504196,2.46068467098), test loss: 2.77042105049\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (4.01588821411,18.8310508177), test loss: 32.9410779953\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.720829665661,2.4532443054), test loss: 3.30992140174\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (4.61773777008,18.7548364752), test loss: 36.1905865669\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.61744165421,2.44578965956), test loss: 2.52409967184\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (6.25692939758,18.6793917961), test loss: 36.0841061592\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.4076385498,2.4383347509), test loss: 3.14201098382\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (8.09415531158,18.6039312223), test loss: 33.3130696297\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.65014386177,2.43108246421), test loss: 2.73180723488\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (6.11760377884,18.5287660202), test loss: 39.2341106415\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.63120293617,2.42378579273), test loss: 3.15850574076\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (11.4150676727,18.4542710708), test loss: 28.3264539242\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.06326341629,2.41662359449), test loss: 2.54335079193\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (8.14493179321,18.3801741227), test loss: 36.9990070343\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.19533824921,2.40951289427), test loss: 3.10316852778\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (3.74610686302,18.3070063803), test loss: 28.3099474907\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.728516042233,2.40253086565), test loss: 3.05425547063\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (1.94838190079,18.2337074052), test loss: 36.7657683372\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.08556008339,2.39563762805), test loss: 2.84690287113\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.62284612656,18.161556287), test loss: 33.7299450874\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (3.14568161964,2.38872600827), test loss: 3.20322691202\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (10.7131500244,18.0896272486), test loss: 35.8117605209\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.733002066612,2.38175301401), test loss: 2.37734671235\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (19.2233200073,18.0182257926), test loss: 37.1307683945\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.61963415146,2.37492876807), test loss: 3.14087972045\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (11.1284360886,17.9467238017), test loss: 36.3182641983\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (3.91869902611,2.3681871696), test loss: 2.65541053116\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (5.87771987915,17.8756208945), test loss: 39.1649328709\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.593015313148,2.36144418396), test loss: 3.14409276769\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (9.83164024353,17.8052638507), test loss: 30.7264428139\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.45544779301,2.3548348583), test loss: 2.6682382524\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.08357477188,17.7354826899), test loss: 37.7502041817\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.680090427399,2.34821566457), test loss: 3.07412510663\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (11.2177877426,17.6661835947), test loss: 29.3819218874\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.8049788475,2.34177864784), test loss: 3.10820978284\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.7215719223,17.5972203037), test loss: 38.3230729103\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.728130698204,2.3353376317), test loss: 3.14864699244\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (7.41877555847,17.5287100882), test loss: 33.414428997\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.43938159943,2.32887544808), test loss: 3.2732485652\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.633893013,17.4608518749), test loss: 37.4281035423\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.52735364437,2.32242226567), test loss: 2.53739350736\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (8.25949001312,17.3931419774), test loss: 37.7112543106\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.86196553707,2.31612561898), test loss: 3.30245366991\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (5.50371456146,17.3256001065), test loss: 35.6311525822\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.52326893806,2.30978798997), test loss: 2.59134990424\n",
      "\n",
      "MC # 1, Hype # hyp5, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold4/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (353.173614502,inf), test loss: 204.859576225\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (386.216308594,inf), test loss: 447.426261902\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (29.2013607025,125.245154951), test loss: 49.5196827888\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.83788394928,95.6362519912), test loss: 3.13667520285\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (126.287696838,87.0420327852), test loss: 40.7013312817\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.65376234055,49.3065136043), test loss: 3.45634334087\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (37.0784034729,73.9256951718), test loss: 46.529199791\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.43867826462,33.8775582256), test loss: 3.30065329075\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (34.8904876709,67.2907508365), test loss: 44.8586084366\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.1862142086,26.1638620953), test loss: 3.40904259682\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (38.9502220154,63.326030383), test loss: 43.9809101105\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.988046765327,21.5412491818), test loss: 2.76919637918\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (61.111618042,60.6636433841), test loss: 45.9844305038\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.36407113075,18.4632897949), test loss: 3.52087161243\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (28.3074226379,58.7451515998), test loss: 37.6602970123\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.27767443657,16.2694449979), test loss: 2.56501708031\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (31.8762264252,57.2800137207), test loss: 47.3006697655\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.09951865673,14.6199074231), test loss: 3.30192627311\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (80.1810684204,56.1229097782), test loss: 37.4280332088\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.26186943054,13.3403340415), test loss: 2.9030464232\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (60.9086303711,55.1452471228), test loss: 45.7413127422\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.74562323093,12.314223379), test loss: 2.97814775109\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (31.049331665,54.3118717905), test loss: 40.703117466\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.99976110458,11.4770957537), test loss: 3.19087619781\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (35.9853630066,53.6237890155), test loss: 43.3858703136\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (7.46044063568,10.7780660094), test loss: 3.29838672876\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (36.0677680969,53.0555745561), test loss: 42.8253046513\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.06673002243,10.1881568088), test loss: 3.43549415171\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (21.6533374786,52.5438897252), test loss: 41.4914003372\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.47465097904,9.68727150636), test loss: 2.69825057089\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (42.890663147,52.0658127082), test loss: 45.4547288895\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.60086989403,9.25257546425), test loss: 3.46884794235\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (28.312335968,51.6350195772), test loss: 41.6905543327\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.67847728729,8.87250202208), test loss: 2.77164352536\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (23.3239936829,51.2351925596), test loss: 43.3895913601\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.86767745018,8.5353733742), test loss: 3.26607863903\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (46.9356231689,50.8563947267), test loss: 37.2091663361\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.90182995796,8.23585216545), test loss: 2.90247192383\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (47.6793785095,50.4888430608), test loss: 46.7717230797\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.77472114563,7.96748778708), test loss: 3.23203514218\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (39.6249046326,50.1636465289), test loss: 33.9187161922\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.54227280617,7.72559244024), test loss: 2.63989557922\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (55.4424438477,49.8445358209), test loss: 43.1000705242\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (5.32854175568,7.50677292018), test loss: 3.26412447095\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (84.1524200439,49.5639895826), test loss: 38.9545186996\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.83654737473,7.30905827766), test loss: 3.17343117595\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (16.465139389,49.2782306345), test loss: 38.9258404255\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.82767760754,7.12759492292), test loss: 2.69984441996\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (20.0606784821,49.0031823768), test loss: 45.5490834236\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.58766150475,6.96212602042), test loss: 3.32375394106\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (32.7205276489,48.707444432), test loss: 39.5139933348\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.855434298515,6.80848231572), test loss: 2.6908516556\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (26.478471756,48.4096164726), test loss: 43.3134337425\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.593082427979,6.66718128074), test loss: 3.27903105915\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (61.5712852478,48.1061375588), test loss: 35.8897126675\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.20726466179,6.53556439101), test loss: 2.82362980843\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (32.8595504761,47.8204910851), test loss: 44.0385292053\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.50581049919,6.41301879014), test loss: 3.20271505713\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (37.5361366272,47.5239871001), test loss: 33.4412015915\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.53078866005,6.29840787541), test loss: 2.74818863273\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.1206951141,47.2378325777), test loss: 40.54105196\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.94054818153,6.19159220905), test loss: 3.08216789365\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (68.5634613037,46.9431347809), test loss: 34.4416356325\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.57711219788,6.091017351), test loss: 3.15811242461\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (19.0586299896,46.6480342079), test loss: 39.7995460987\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.44299197197,5.99699513737), test loss: 3.09846710563\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (41.7057495117,46.3422487987), test loss: 38.4442378521\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (5.00598526001,5.90709076109), test loss: 3.23636667728\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (88.2565383911,46.025391544), test loss: 35.7157166004\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.97750020027,5.82178022359), test loss: 2.5778717339\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (137.228820801,45.700856591), test loss: 39.3490763664\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.33454704285,5.74052804745), test loss: 3.29859861732\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (24.5493621826,45.3671268331), test loss: 31.731444931\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.02167367935,5.66291956257), test loss: 2.49657981694\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (25.1448631287,45.0305404416), test loss: 40.3307219505\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.62808179855,5.5880315376), test loss: 3.1310280323\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (18.4836730957,44.6883075934), test loss: 28.6267412186\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.58322811127,5.51680564563), test loss: 2.70281751454\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (33.8085517883,44.3380623578), test loss: 38.1930659294\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.53042984009,5.44834537809), test loss: 2.99722655416\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (34.3171920776,43.9758834965), test loss: 31.1367026806\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.776434123516,5.38213971129), test loss: 2.98942924142\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (22.4058971405,43.6097888575), test loss: 35.1318261147\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.6063349247,5.31755458534), test loss: 3.06558848023\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (24.2159786224,43.2418040261), test loss: 32.6839625835\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.38493347168,5.25529498825), test loss: 3.21670064926\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (45.1282501221,42.8649858419), test loss: 29.3241536617\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (6.33638191223,5.19458299069), test loss: 2.43597996831\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (6.3167257309,42.4872206176), test loss: 36.2803838253\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.87364625931,5.13562803408), test loss: 3.25712867975\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (24.6921348572,42.1172116414), test loss: 29.9988746166\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.65594601631,5.07840038106), test loss: 2.47808127701\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (16.4586429596,41.7522125558), test loss: 34.8093540668\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.08267450333,5.02294612084), test loss: 3.20879798234\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (42.1738433838,41.3946385921), test loss: 28.4188965321\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.57554006577,4.96973325691), test loss: 2.65691757202\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (8.40853595734,41.0370219213), test loss: 36.6603518724\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.51926410198,4.91803189182), test loss: 3.0759106487\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (30.553062439,40.688762648), test loss: 24.2478952885\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.36588335037,4.8681084678), test loss: 2.54698797315\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (25.8784637451,40.345866533), test loss: 34.9410120249\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.34862422943,4.81945212037), test loss: 3.0074403882\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (33.0555419922,40.0055567725), test loss: 30.2955877781\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.79657053947,4.77236359781), test loss: 3.1295166105\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (20.3538093567,39.6694807615), test loss: 28.2485392094\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.01570892334,4.726468845), test loss: 2.50328627974\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (36.2039337158,39.3491221638), test loss: 33.5434050322\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.34539914131,4.68223612386), test loss: 3.23333473206\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (4.21406078339,39.0298621469), test loss: 29.9285616875\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.94084632397,4.63914342475), test loss: 2.52398842424\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (15.184841156,38.7220768635), test loss: 35.5538161993\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.806513190269,4.59764123918), test loss: 3.18671964407\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (16.0781059265,38.4157764458), test loss: 28.2925630093\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.9207072258,4.55740935989), test loss: 2.57975705266\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (38.110874176,38.1215746546), test loss: 36.1453278065\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.3119263649,4.51872613497), test loss: 3.15941635072\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (14.9218130112,37.8327991288), test loss: 28.3796747208\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.37353157997,4.48090043452), test loss: 2.72091828734\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (39.7069015503,37.5479141188), test loss: 33.9775745869\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (4.36448526382,4.44426397382), test loss: 2.97774036229\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (51.2444381714,37.2677165749), test loss: 30.4264421463\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.50770306587,4.40843471946), test loss: 3.36443642229\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (16.8202781677,36.9965553063), test loss: 35.3156172276\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.89644956589,4.37373880942), test loss: 3.19122801721\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.8671398163,36.7315175532), test loss: 31.2658285856\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.961223840714,4.33963122948), test loss: 3.24540878832\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (5.48028659821,36.4713591113), test loss: 31.1404268742\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.340954184532,4.30675459574), test loss: 2.64517517686\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (17.8913612366,36.2161810669), test loss: 34.0045171261\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.3984670639,4.27478166251), test loss: 3.31602948308\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (21.651309967,35.9669837666), test loss: 30.5208039522\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.18456184864,4.24395095443), test loss: 2.39807514101\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.9243154526,35.7233380777), test loss: 36.6693027973\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.844193100929,4.21371941977), test loss: 3.13707967103\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (23.8296642303,35.4856521689), test loss: 28.8426441193\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.822009205818,4.18437260071), test loss: 2.79418463856\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (16.9110984802,35.2475117649), test loss: 35.9499773264\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.79766917229,4.15549225591), test loss: 3.02813256681\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (17.4089641571,35.015316867), test loss: 28.2353554249\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (0.958165705204,4.12732900738), test loss: 3.03974163085\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (25.6535491943,34.7899188622), test loss: 33.8173093081\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (5.77381372452,4.09971239425), test loss: 3.15686783493\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (7.24211978912,34.5669517291), test loss: 31.2569245577\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.12846755981,4.07275151804), test loss: 3.26661068499\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (9.46043395996,34.3492245455), test loss: 29.3466745853\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.71290707588,4.04670599674), test loss: 2.52637845278\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (18.5298614502,34.1339813426), test loss: 34.6948147535\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.5808968544,4.0211125422), test loss: 3.32465308309\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (7.84018993378,33.922776503), test loss: 30.510550499\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.50764095783,3.99630627436), test loss: 2.53496259451\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (10.220785141,33.7177241604), test loss: 33.8744771004\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.84207129478,3.97200460493), test loss: 3.08175917268\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (25.9056510925,33.511017527), test loss: 30.3292843342\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.91947007179,3.94803772139), test loss: 2.69711967856\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (13.9089412689,33.3085196085), test loss: 36.0491391182\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.09847044945,3.92452912567), test loss: 3.01283729374\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (35.2870101929,33.1123055403), test loss: 25.7513669968\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.25225639343,3.90151873171), test loss: 2.62977673337\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.73964309692,32.9164519444), test loss: 34.6595567465\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.36220526695,3.87887643431), test loss: 3.11491557658\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (36.4897994995,32.7260069217), test loss: 29.6539502859\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.85140419006,3.85690619905), test loss: 3.12299883217\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (8.16009807587,32.5348732519), test loss: 33.8235441208\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.50666213036,3.83526574607), test loss: 2.71964801252\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.5664424896,32.3493385715), test loss: 32.7777108669\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.96310937405,3.81433149116), test loss: 3.30422590673\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.44549655914,32.1667914292), test loss: 31.8540629864\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.590708196163,3.79361110862), test loss: 2.58666710407\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (18.9103431702,31.9846184672), test loss: 34.9542404175\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.633902668953,3.77332325018), test loss: 3.19807313383\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (20.0105361938,31.804644301), test loss: 29.5932296276\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.7077589035,3.7532520232), test loss: 2.56214342713\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (7.51551151276,31.629729925), test loss: 36.6996536136\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.43982481956,3.73360196314), test loss: 3.10305774808\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (42.6960258484,31.4565157941), test loss: 27.5212828636\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.43670415878,3.71423094533), test loss: 2.68241009712\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.1236047745,31.2844707587), test loss: 34.5466285467\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.43459701538,3.69526786949), test loss: 2.88485498428\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (32.824924469,31.1147614367), test loss: 30.906123209\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.27315306664,3.67665645777), test loss: 3.40142701566\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (13.2959794998,30.9472851674), test loss: 35.1234185815\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.792339921,3.65852294528), test loss: 3.16203536689\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.9450426102,30.7830151381), test loss: 30.8827000856\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.87241148949,3.64067331405), test loss: 3.17278245091\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (42.0182495117,30.6201278921), test loss: 32.6971198082\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.77860319614,3.62310076428), test loss: 2.68633945659\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (3.84134697914,30.4579488194), test loss: 32.6209858656\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.13730454445,3.60571809619), test loss: 3.30132164657\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (8.04514598846,30.2989767799), test loss: 30.698938179\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.53995943069,3.58866329107), test loss: 2.42421668172\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (8.19046783447,30.1423268371), test loss: 36.1035806894\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.70928239822,3.57165562062), test loss: 3.0742559135\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (17.3471317291,29.987232251), test loss: 28.3978832245\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (5.01893520355,3.55510958522), test loss: 2.7383800298\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (9.77880859375,29.8332404349), test loss: 35.9189228892\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.71102142334,3.53875922688), test loss: 3.02212068439\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (19.6155548096,29.6809577767), test loss: 26.0326014519\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.07365465164,3.52283728178), test loss: 2.70005861521\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (12.2073402405,29.5311553018), test loss: 35.2863759995\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.2930393219,3.50710948543), test loss: 3.05670530796\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (28.7932624817,29.3840596415), test loss: 30.0091131091\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (3.71196269989,3.49174638996), test loss: 3.22070299983\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (12.356004715,29.2358215137), test loss: 31.5843828201\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.87327742577,3.47643146684), test loss: 2.54184653312\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (4.1263794899,29.0902575207), test loss: 33.9342433214\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.999965786934,3.46125856336), test loss: 3.20626851618\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (10.4441814423,28.9477012418), test loss: 31.7216096401\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.37874829769,3.44636463825), test loss: 2.50025100708\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (13.1088590622,28.8050521119), test loss: 35.0008579969\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.79705047607,3.43157896482), test loss: 3.01263234615\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (33.9164199829,28.665138755), test loss: 32.8901206493\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.43434548378,3.41717740217), test loss: 2.61505101174\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (12.7615308762,28.52537611), test loss: 37.3643265724\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.47740697861,3.40291029338), test loss: 2.95708143413\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (27.5856132507,28.3878097502), test loss: 27.1366071463\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.42665052414,3.38898260479), test loss: 2.67398707271\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (15.684885025,28.2525554901), test loss: 35.621419692\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.94609951973,3.37519818583), test loss: 2.95988468379\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (8.01884460449,28.116101699), test loss: 29.0438701272\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.19291353226,3.36155594313), test loss: 3.05331252813\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (9.66193389893,27.9829964553), test loss: 36.1259789109\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.16198801994,3.34803643138), test loss: 2.99301423132\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (12.4154052734,27.8512236917), test loss: 32.6174347162\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.959856212139,3.33464309198), test loss: 3.15750711262\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.50030326843,27.7202818437), test loss: 33.7979119778\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.95145320892,3.32150515521), test loss: 2.53597460687\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (11.6750946045,27.5907773509), test loss: 36.3132911682\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.07896542549,3.30851293756), test loss: 3.13569762111\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (12.3669700623,27.4609017278), test loss: 31.5498100758\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.03056812286,3.29567330223), test loss: 2.57151685059\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (17.4876556396,27.3341345565), test loss: 38.2808475018\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.45246171951,3.28313889111), test loss: 3.1043687731\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (10.7587594986,27.2080564511), test loss: 28.7315880299\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.886040210724,3.27065989524), test loss: 2.66558194458\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (17.0946121216,27.0830731674), test loss: 35.4863932133\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.93407201767,3.25844378805), test loss: 2.86692167073\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (9.37058162689,26.9591229535), test loss: 31.0654661179\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.41833090782,3.24623604425), test loss: 3.17527477443\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (9.83957576752,26.8367052861), test loss: 36.3210073948\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (5.92728805542,3.23422427993), test loss: 3.1265317589\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (6.2163476944,26.7158361146), test loss: 30.8678070784\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.512736082077,3.22220746389), test loss: 3.09929618239\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (3.99559307098,26.5946995855), test loss: 34.10205369\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.378266304731,3.21045119682), test loss: 2.69129770398\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (11.1415662766,26.475136122), test loss: 32.78273139\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.89422488213,3.19881904933), test loss: 3.21687114239\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (19.459072113,26.3561349324), test loss: 37.0494878292\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.38375902176,3.18740711396), test loss: 2.48290970773\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (9.77539825439,26.2388137244), test loss: 37.1991265535\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.89453125,3.17611740569), test loss: 3.0232370019\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (11.278219223,26.1231000641), test loss: 33.1441445351\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.555938661098,3.16497461719), test loss: 2.94676919729\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (6.08237409592,26.0066982313), test loss: 38.4719524384\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (3.00679278374,3.15389549515), test loss: 3.01384870857\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (13.6448698044,25.8923533633), test loss: 26.8299145699\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.997143507,3.14291047075), test loss: 2.40564786643\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (18.0350208282,25.7789818698), test loss: 36.5794679642\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (3.56884431839,3.13196659744), test loss: 2.97542766631\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (3.15306711197,25.666276581), test loss: 31.3261282444\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.54046082497,3.12119810455), test loss: 3.04114548266\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (4.08890914917,25.5542638109), test loss: 33.8107498884\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.36118173599,3.11058505717), test loss: 2.62509570271\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (16.2060375214,25.4429887814), test loss: 34.4649955153\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.757505714893,3.10008946294), test loss: 3.1405140385\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (10.1052017212,25.3329594969), test loss: 33.8305762291\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (3.75133633614,3.08973946934), test loss: 2.53048672676\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (4.93112277985,25.2241055063), test loss: 37.4024285793\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.535144925117,3.07950870102), test loss: 2.96879654378\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (9.2088470459,25.1152465896), test loss: 34.1061666965\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.25394523144,3.06935363091), test loss: 2.64808642864\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (6.37950515747,25.0076155503), test loss: 38.0077516556\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.22538769245,3.05924860907), test loss: 2.89827573597\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (19.5006656647,24.9017258243), test loss: 28.1326462269\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.62954926491,3.04921816855), test loss: 2.60261330605\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (9.52443027496,24.7954081615), test loss: 36.7060717106\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.40664052963,3.03926227249), test loss: 2.8843790859\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (10.7355222702,24.6903899688), test loss: 32.0186836123\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (3.25811100006,3.02951558798), test loss: 2.94373991787\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (3.7376909256,24.5856431065), test loss: 38.0266901016\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.1930103302,3.01976810051), test loss: 3.06154251695\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.09432220459,24.4815768372), test loss: 31.6946104765\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.888274669647,3.01024582865), test loss: 3.16016240418\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (2.90777730942,24.37942304), test loss: 36.3454603672\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.12974262238,3.00076193871), test loss: 2.56696261913\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (8.9171628952,24.2766489189), test loss: 35.3025653362\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.72434961796,2.99139523734), test loss: 3.10876186192\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (7.40179252625,24.1755562115), test loss: 32.0058176041\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.40522480011,2.98203532725), test loss: 2.46363856941\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.01671981812,24.0750570259), test loss: 39.2590659142\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.38459014893,2.9727415801), test loss: 3.0768625617\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (26.1029415131,23.9753119759), test loss: 29.4889957905\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.13185119629,2.96356117235), test loss: 2.7063116312\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.66423511505,23.8759620256), test loss: 37.0853294611\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.08206367493,2.95445692792), test loss: 2.80519925952\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (9.58456993103,23.7766276757), test loss: 32.2646888733\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.72026371956,2.9454579261), test loss: 3.12380321175\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (6.01438045502,23.6788217688), test loss: 37.9344545603\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.01368284225,2.93656562259), test loss: 3.11336362362\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (7.73055076599,23.5813438569), test loss: 31.6572657466\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.62022459507,2.92772481989), test loss: 3.08113922775\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (8.4449262619,23.4848227436), test loss: 36.108110857\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.27377283573,2.91905713661), test loss: 2.636425367\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (4.08149290085,23.3889920068), test loss: 33.8450663328\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.848201036453,2.91035707108), test loss: 3.15423891991\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (6.93856811523,23.2935951882), test loss: 36.3502710819\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.64225745201,2.90177293394), test loss: 2.5648624897\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.9583902359,23.1995105736), test loss: 37.1964541197\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (3.38798999786,2.89312452435), test loss: 2.96166571081\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (12.1319675446,23.1050855452), test loss: 32.1961502552\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (3.6264193058,2.88466026253), test loss: 2.77467962801\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (14.3035697937,23.0117932434), test loss: 40.245415926\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.29065394402,2.87621891477), test loss: 3.07197878063\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (13.3192100525,22.9185037544), test loss: 28.8786476612\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.95221352577,2.86791905848), test loss: 2.42004254833\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (10.1167764664,22.8260975798), test loss: 37.9856798172\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.66115629673,2.85967778848), test loss: 2.96515896916\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (6.48346614838,22.7350587405), test loss: 30.0069298506\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.045596838,2.85154391995), test loss: 3.09722599685\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (4.5873003006,22.6434031834), test loss: 35.9665331841\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.46779417992,2.8434702152), test loss: 2.66904640794\n",
      "\n",
      "MC # 1, Hype # hyp5, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold5/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (304.201599121,inf), test loss: 160.533025742\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (299.430419922,inf), test loss: 365.594003296\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (66.203994751,56.4611973677), test loss: 46.4100928307\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (6.4464225769,57.6067187874), test loss: 3.77905551791\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (37.6131095886,50.7760474524), test loss: 38.9960054874\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (6.17667102814,30.7596677327), test loss: 3.83577543497\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (45.4795150757,48.9531082354), test loss: 44.3834290504\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.41458511353,21.8186400288), test loss: 3.77500967383\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.9579238892,47.9803547242), test loss: 42.427304554\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.72172307968,17.3382991073), test loss: 3.9569051981\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (14.2017707825,47.3978363808), test loss: 44.1368155003\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.879352867603,14.6518595221), test loss: 3.08652396798\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (17.9836082458,46.9860199022), test loss: 43.6094724655\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (4.45122909546,12.8688231734), test loss: 4.00045816898\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (29.0959815979,46.664035152), test loss: 41.6414593697\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.84744882584,11.5954460778), test loss: 3.28968503475\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (50.575176239,46.4160768242), test loss: 45.1648234367\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.03403449059,10.6366058343), test loss: 3.77157924771\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (98.4787902832,46.2098054), test loss: 37.893474412\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.1856045723,9.89404763409), test loss: 3.36576815248\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (75.4392700195,46.0060284355), test loss: 46.7004673958\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.7305816412,9.29581007863), test loss: 3.70992915034\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (41.1937294006,45.8260923066), test loss: 34.6758664608\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.72714042664,8.81053988553), test loss: 3.03477801681\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (12.1412143707,45.6818852456), test loss: 45.8403238773\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.79417604208,8.40039958244), test loss: 3.80498364568\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (45.5633621216,45.5712807123), test loss: 39.022450304\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.02805328369,8.05594039485), test loss: 3.78568001091\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (20.0784358978,45.4696933786), test loss: 41.4989665031\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.97263598442,7.76250492818), test loss: 3.26661822796\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (31.1395797729,45.3504424248), test loss: 45.0449687958\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.947578132153,7.50668511177), test loss: 3.82636989355\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (39.0189094543,45.251868711), test loss: 43.2349904537\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (11.6133365631,7.28311159671), test loss: 3.1689563334\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (65.0184631348,45.166026351), test loss: 41.9850887299\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.97539281845,7.08555183463), test loss: 3.77960629463\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (35.3900299072,45.0558904922), test loss: 38.122724247\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.18586707115,6.90837903736), test loss: 3.29591132998\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (48.5257110596,44.9505476856), test loss: 45.3901787758\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.04082560539,6.75040343686), test loss: 3.69932927489\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (40.2782592773,44.8627217047), test loss: 36.6827241421\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.89239835739,6.60654359269), test loss: 3.01997442842\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (31.3653297424,44.7813855833), test loss: 44.3911390305\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.65177690983,6.47558315031), test loss: 3.86656851768\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (21.2811393738,44.7071815285), test loss: 37.0118535042\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.81603360176,6.35803896792), test loss: 3.63433770537\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (93.0578384399,44.6293400589), test loss: 45.458518672\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.56317687035,6.24980703037), test loss: 3.65096381307\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (60.4273529053,44.5439173226), test loss: 41.75442276\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.39814662933,6.14956060702), test loss: 3.56274647713\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (62.646156311,44.4680706525), test loss: 42.1800095558\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.23468613625,6.05642829618), test loss: 3.12987456918\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (40.1677131653,44.37428155), test loss: 42.7391791344\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.8370051384,5.96974764535), test loss: 3.74490849376\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (37.4282455444,44.2792003598), test loss: 37.9117094994\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.18880367279,5.88923873858), test loss: 3.18461339474\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (55.8206596375,44.2012177757), test loss: 43.1939688683\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.99590969086,5.81405957459), test loss: 3.60477032065\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (11.245259285,44.1078196985), test loss: 35.619903326\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (5.30729913712,5.7426538902), test loss: 2.98431779146\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (16.9866790771,44.0334159274), test loss: 44.1416473389\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.30831170082,5.67600636254), test loss: 3.65822075605\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (32.8340530396,43.9411820881), test loss: 37.4803339958\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.4656471014,5.6135967864), test loss: 3.21258402467\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (102.114814758,43.8517637053), test loss: 42.087772131\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (6.1393828392,5.5543273292), test loss: 3.53538911343\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (62.256729126,43.7584643506), test loss: 38.842567873\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (8.28740882874,5.49740295998), test loss: 3.75920010805\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (62.7125473022,43.6525852133), test loss: 40.0471604109\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (6.10716819763,5.44288609319), test loss: 3.02816866636\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (33.0206756592,43.5428751309), test loss: 41.781808567\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (5.33615493774,5.39092530495), test loss: 3.71288079023\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (40.0894966125,43.4402015248), test loss: 39.7065377235\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (4.57374191284,5.34137135795), test loss: 2.94963496327\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (21.1417312622,43.3307672428), test loss: 40.4788395882\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.22435569763,5.29235557153), test loss: 3.5108476758\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (11.2358388901,43.2193317427), test loss: 34.2874930859\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.721088409424,5.24529941316), test loss: 2.85929987431\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.517206192,43.1019834193), test loss: 43.1538157463\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.01004838943,5.20046580673), test loss: 3.50090663433\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (24.1944580078,42.9780193794), test loss: 30.5373068333\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.56731534004,5.15668641848), test loss: 2.62304465473\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (38.5425109863,42.8489399862), test loss: 41.1669898033\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.87311398983,5.11335313654), test loss: 3.35407960415\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (90.0152053833,42.7131952189), test loss: 34.4858781815\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.21262931824,5.07162742066), test loss: 3.41635748148\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (61.9399452209,42.5635683288), test loss: 35.3965894938\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.80162632465,5.0297499539), test loss: 2.80114843249\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (31.9754333496,42.4080320119), test loss: 39.773792696\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.11749362946,4.98934950513), test loss: 3.40687077641\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (6.69438791275,42.2493786622), test loss: 36.9320653915\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.715622067451,4.94811063544), test loss: 2.56710216999\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (37.0805053711,42.08725342), test loss: 38.6137158394\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.01969981194,4.90815539333), test loss: 3.50471673608\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.2517223358,41.9164225451), test loss: 32.292207408\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.95394635201,4.86903908945), test loss: 2.58036826253\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (27.9705905914,41.7311440565), test loss: 38.922722435\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.11842536926,4.82988068469), test loss: 3.33009563684\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (26.0039634705,41.539115829), test loss: 29.6230086803\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (8.31324768066,4.79125328436), test loss: 2.59327061772\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (37.1245193481,41.3389642823), test loss: 37.3780319452\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.96463060379,4.75298105719), test loss: 3.16234657764\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (21.682094574,41.1163733705), test loss: 30.7489752769\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.51240968704,4.71424898323), test loss: 3.14212900996\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (22.8426837921,40.8710534138), test loss: 36.134655571\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.16436600685,4.6742830151), test loss: 3.18980275691\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.0777139664,40.6190915011), test loss: 31.3754152775\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.47472405434,4.6341404878), test loss: 3.35200119317\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (19.8811302185,40.3593678381), test loss: 32.2809801579\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.953434109688,4.59433276896), test loss: 2.51171520948\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (12.6377506256,40.0950514422), test loss: 34.0795948982\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.19649684429,4.55627543225), test loss: 3.42229874134\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (43.9444046021,39.8225914722), test loss: 28.3291329861\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.32630503178,4.51873448836), test loss: 2.50185815096\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (21.8737163544,39.5500478334), test loss: 35.4198306084\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.4160990715,4.48252970583), test loss: 3.46969572306\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (39.5084533691,39.2793230585), test loss: 27.3740428925\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.71633541584,4.4472846853), test loss: 2.71187865585\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (30.765504837,39.0031979494), test loss: 34.8054820538\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.983522415161,4.41267465729), test loss: 3.29818466902\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (25.3352603912,38.7298421408), test loss: 30.6206834316\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.97816538811,4.37904926942), test loss: 3.31163957119\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (20.8268737793,38.465254469), test loss: 34.7933623791\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.2836265564,4.34626357237), test loss: 3.42135438919\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (11.8001413345,38.2003698582), test loss: 30.0266812801\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (4.5038318634,4.31394540223), test loss: 3.52938581705\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (19.208442688,37.9425777282), test loss: 30.2380983829\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.853043913841,4.28253780749), test loss: 2.63834329545\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (24.4341545105,37.6832761356), test loss: 32.8319602489\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.2773771286,4.25194377808), test loss: 3.55596320033\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (59.7411117554,37.432220842), test loss: 31.7544477701\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.31162452698,4.2223872696), test loss: 2.54684170485\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (20.9958496094,37.1846991159), test loss: 32.0009161949\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.2241590023,4.19349835724), test loss: 3.32025267482\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (49.5195808411,36.9389642926), test loss: 30.2373136997\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.85324192047,4.16515562864), test loss: 2.72389010787\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (21.400970459,36.6963657428), test loss: 35.5056087017\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.29384279251,4.13736350221), test loss: 3.3536884129\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (13.2016973495,36.4631815771), test loss: 25.9217143059\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.15607666969,4.1104062671), test loss: 2.56130699813\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (10.3568563461,36.2333191608), test loss: 35.4863528252\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.701671659946,4.08338549663), test loss: 3.44932531118\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (5.45517921448,36.0070594687), test loss: 29.5701835155\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.442100197077,4.05723148772), test loss: 3.49647929072\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (8.43500041962,35.7833585072), test loss: 31.2506869078\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.34284114838,4.03176845614), test loss: 2.80474865437\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (21.9596233368,35.564084823), test loss: 33.9258522511\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.00480794907,4.00689494463), test loss: 3.4658321321\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (11.1765680313,35.3495885155), test loss: 32.479733181\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.537515699863,3.98261721246), test loss: 2.5212977767\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (50.5087852478,35.1398677277), test loss: 34.0667831421\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.63395452499,3.95898292574), test loss: 3.33889755607\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (13.6397342682,34.9291670123), test loss: 33.1904235363\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.38542222977,3.93547127009), test loss: 2.65136160851\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (19.5358715057,34.7252661725), test loss: 34.2366371632\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.33707761765,3.91268940839), test loss: 3.30945501029\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (6.58036327362,34.5260240783), test loss: 28.4356626511\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.93047118187,3.88979888659), test loss: 2.71962230504\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (15.532661438,34.3297050617), test loss: 35.2198845387\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.840904057026,3.86753241699), test loss: 3.23234174252\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (18.5857925415,34.1355893504), test loss: 30.489657259\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.09382748604,3.84586004553), test loss: 3.42593848407\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (25.1938114166,33.9438169546), test loss: 36.8253596306\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.47423374653,3.82445102829), test loss: 3.31017320752\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.1276607513,33.7556851579), test loss: 30.939484334\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (3.77846670151,3.80365425689), test loss: 3.47240258455\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (8.81086158752,33.5731730934), test loss: 32.6640226841\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.583157479763,3.78334638882), test loss: 2.56015121639\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (14.784787178,33.388509538), test loss: 32.6044810534\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.962458491325,3.76312913758), test loss: 3.39241012335\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (12.5761709213,33.2084150658), test loss: 30.7434516907\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.35632741451,3.74335919278), test loss: 2.43699284792\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (6.22534418106,33.033481378), test loss: 34.8065541983\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.65305566788,3.72379157453), test loss: 3.3304998517\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (23.5088424683,32.8602645263), test loss: 28.6141026974\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.729743540287,3.70430216234), test loss: 2.65923128724\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (12.2452125549,32.6895839796), test loss: 35.5395840168\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.20606029034,3.68551750943), test loss: 3.28001033664\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (27.7203197479,32.5187900596), test loss: 26.153650856\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.21139359474,3.66680144639), test loss: 2.84404276609\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.82474708557,32.3519515392), test loss: 35.8107654333\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.86441135406,3.64861271042), test loss: 3.31178078651\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (22.3299541473,32.1887430977), test loss: 29.3406290531\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.628870010376,3.63080708641), test loss: 3.33316063881\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (29.7297515869,32.0247713179), test loss: 31.3143315315\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.934340238571,3.61309049796), test loss: 2.67061316073\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (14.3887224197,31.8637045939), test loss: 31.6653019667\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.10405158997,3.59568165178), test loss: 3.38175981641\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (12.4580898285,31.7074568129), test loss: 31.7912703037\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.10515165329,3.57850187389), test loss: 2.43837867081\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.6501436234,31.5512343518), test loss: 32.2595288277\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.25061941147,3.56142277256), test loss: 3.17825356126\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (21.133846283,31.3977960209), test loss: 29.198797226\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.831018447876,3.5446303524), test loss: 2.55649463534\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (21.221244812,31.2438420825), test loss: 35.5220188379\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.69989681244,3.52811206305), test loss: 3.19764464796\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (38.5157012939,31.0940192175), test loss: 27.0194676638\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.28707408905,3.51199855644), test loss: 2.62773934603\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (16.1569633484,30.9459637999), test loss: 35.0987600803\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.25577306747,3.49613543195), test loss: 3.22399727106\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (44.6084022522,30.7988333325), test loss: 29.415490675\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (3.42568802834,3.48049447154), test loss: 3.32364723384\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (18.9184703827,30.6524782488), test loss: 36.4784664154\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (3.63639116287,3.4649849998), test loss: 3.22370209694\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (7.22704267502,30.5103136691), test loss: 31.4878796101\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.90179419518,3.44978647156), test loss: 3.31593955457\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (6.74282503128,30.3693662859), test loss: 32.889057827\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.620673418045,3.43443028896), test loss: 2.52948422134\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (5.57520389557,30.2292181414), test loss: 33.2579913616\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.577427089214,3.41944010532), test loss: 3.22443363369\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (8.45557594299,30.0898747236), test loss: 30.5127042294\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.82328820229,3.40470092704), test loss: 2.48780909181\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (16.8202190399,29.9522745888), test loss: 35.9082959652\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.4007153511,3.39019581606), test loss: 3.20507058501\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.12272548676,29.8167866788), test loss: 28.2224287033\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.557925581932,3.37591139078), test loss: 2.59124791026\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (37.9566345215,29.6837333609), test loss: 36.2522598743\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.22170329094,3.36195230843), test loss: 3.21131655127\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (9.04707527161,29.5491067631), test loss: 32.987915802\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (3.40674448013,3.34797853098), test loss: 3.14489828944\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (17.5231437683,29.4181075732), test loss: 35.6640730858\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.07292985916,3.33428978914), test loss: 3.23782276511\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (5.49862194061,29.2888902698), test loss: 29.4962317944\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.845414042473,3.3204641182), test loss: 3.27199684083\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (12.5557441711,29.1604232642), test loss: 33.1202988148\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.557267904282,3.30692968655), test loss: 2.58923519254\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (17.5153598785,29.0325319729), test loss: 32.3173615456\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.90635919571,3.29363824089), test loss: 3.26653984785\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (16.608663559,28.9055844755), test loss: 33.9534255266\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.00811111927,3.28044340981), test loss: 2.40097583979\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (10.478679657,28.780274193), test loss: 33.9094468117\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (3.15074443817,3.26749030636), test loss: 3.13619521856\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (7.59970569611,28.6578959746), test loss: 28.967881918\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.614011287689,3.25481064716), test loss: 2.61287503242\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (12.7783956528,28.5336312692), test loss: 36.5706979752\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.922361850739,3.24215086157), test loss: 3.17137817144\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (10.2155809402,28.4119358162), test loss: 26.2205741882\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.38375115395,3.22964700775), test loss: 2.44597733617\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.65340614319,28.2926472942), test loss: 35.6003749371\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.45116865635,3.21721680887), test loss: 3.19063480198\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (23.248380661,28.1736784627), test loss: 28.1374538898\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.696480512619,3.20479932462), test loss: 3.08219021857\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (13.8458986282,28.0555306087), test loss: 31.7980998039\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.33439159393,3.1927052792), test loss: 2.66795572639\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (21.6437950134,27.9373954589), test loss: 32.2811486721\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (0.990141034126,3.18064930324), test loss: 3.22118658423\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (5.31959056854,27.8208883605), test loss: 32.6305146456\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.62878394127,3.16880205286), test loss: 2.39629941583\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (14.1857719421,27.7065484966), test loss: 32.5919573784\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.680361568928,3.15719425007), test loss: 3.03646671772\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (27.997045517,27.5915798614), test loss: 34.347908783\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.939502358437,3.14562140202), test loss: 2.69224776924\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (11.1689634323,27.4780884154), test loss: 34.7966688633\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.66365623474,3.1341632375), test loss: 3.05347574353\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.88231086731,27.3668159661), test loss: 27.7775486469\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.07512903214,3.12279020428), test loss: 2.58893325925\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.82780265808,27.2554569368), test loss: 35.3846504688\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.36066317558,3.11147715884), test loss: 3.09463841617\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (20.4735221863,27.1450266949), test loss: 27.8178860664\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.726345717907,3.10027403213), test loss: 2.91049215794\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (14.882191658,27.0344654522), test loss: 36.7443533897\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.12486767769,3.08922553957), test loss: 3.1885866046\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (26.5519924164,26.9258576295), test loss: 29.2697992802\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.53671789169,3.0783456856), test loss: 3.17608410418\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (13.8309659958,26.8181673248), test loss: 34.5363142967\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.7825319767,3.06759878762), test loss: 2.57720791698\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (33.8322753906,26.7112183842), test loss: 33.6498514652\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.72636222839,3.05700348336), test loss: 3.17409855127\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (14.1446485519,26.6043567003), test loss: 31.4517343998\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.78132867813,3.04644539359), test loss: 2.42988676429\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (4.17964029312,26.4996353917), test loss: 35.2157188416\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.72530996799,3.0360287675), test loss: 3.16867073476\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (4.16977787018,26.3956409105), test loss: 28.9790937424\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.625553250313,3.02550884987), test loss: 2.58500344157\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (5.89298677444,26.2916098785), test loss: 35.4303625822\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.620943546295,3.01517968462), test loss: 3.01618036628\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (7.15342950821,26.1877605675), test loss: 30.9294240952\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.39564847946,3.0049752575), test loss: 3.03776827455\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (12.5728988647,26.0847638479), test loss: 37.6571764946\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.88749909401,2.9948735793), test loss: 3.19134497941\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (5.86662387848,25.9827024712), test loss: 30.0358295918\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.596391618252,2.98487450743), test loss: 3.14578603208\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (30.0988693237,25.8823604995), test loss: 37.193444109\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.01567578316,2.97509735017), test loss: 3.04042623341\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (6.89600849152,25.7807973121), test loss: 31.1718769312\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (3.0919251442,2.96530019387), test loss: 3.20186467469\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (13.6005964279,25.681310588), test loss: 34.6170131683\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.9695379734,2.95563213892), test loss: 2.46833382845\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (4.02325391769,25.5828318178), test loss: 33.0252963543\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.703776955605,2.94586416692), test loss: 3.00122173131\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (9.37026882172,25.484478122), test loss: 29.3211053371\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.457002907991,2.93627209379), test loss: 2.48076793253\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (12.7282791138,25.3863147144), test loss: 36.4914637089\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.61437821388,2.92680839696), test loss: 3.04367952347\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (11.660697937,25.2887614305), test loss: 26.4996012211\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.738184928894,2.91739287376), test loss: 2.31557682157\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (10.5453252792,25.1919440948), test loss: 36.2335528374\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.85266494751,2.90807887151), test loss: 3.13823549747\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (6.05910634995,25.096928273), test loss: 28.3084214211\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.657466173172,2.89895341394), test loss: 3.01213020086\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (10.2262706757,25.0007568261), test loss: 34.2185225964\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.941354095936,2.88985834535), test loss: 2.7416121304\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.67693805695,24.9061418917), test loss: 30.4563575745\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.28453552723,2.88080885959), test loss: 3.08553698659\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (4.55040931702,24.8128318361), test loss: 34.8424134254\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.18648517132,2.87179631677), test loss: 2.51001642942\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (19.3430194855,24.7195340644), test loss: 34.1869708061\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.795437574387,2.86280529785), test loss: 3.01072828174\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (13.4513130188,24.6265780689), test loss: 32.7460526943\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.34848797321,2.85397688113), test loss: 2.49732153118\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (15.6294851303,24.5337323401), test loss: 37.1907362938\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.84959936142,2.84518831964), test loss: 3.11102613807\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (3.60189628601,24.441632555), test loss: 29.6157829285\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.65205073357,2.83648009867), test loss: 2.64227325916\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.66155338287,24.350890261), test loss: 36.1864708424\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.720972299576,2.82793693934), test loss: 3.03998278677\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (23.4230003357,24.2599257028), test loss: 31.4303201199\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.920730948448,2.8194313135), test loss: 3.09130958021\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (9.52853870392,24.1697425466), test loss: 38.450620532\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.46700501442,2.81096500367), test loss: 3.24019863009\n",
      "run time for single CV loop: 7066.17712998\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold1/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (288.431884766,inf), test loss: 180.427759933\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (323.582214355,inf), test loss: 367.310520935\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (74.2677154541,95.5305961809), test loss: 45.5775005817\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.90538215637,102.515060668), test loss: 2.90462768078\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (118.950775146,71.6887104068), test loss: 43.3677398205\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.77175235748,52.6300955846), test loss: 3.02476063371\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (42.2669792175,63.5990036497), test loss: 43.8856614113\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.49302768707,36.0090519886), test loss: 2.9678553015\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (39.0383834839,59.4570681429), test loss: 45.1155105591\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.57111001015,27.6951202977), test loss: 3.12844342589\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (30.3784484863,57.0006958683), test loss: 42.9452403069\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.71526145935,22.7088015744), test loss: 2.51144976616\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (97.4323730469,55.3522520192), test loss: 46.5392148972\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (5.65751075745,19.3881262717), test loss: 3.21338676214\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (29.086265564,54.1106036332), test loss: 41.4771870613\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.633164823055,17.0168910398), test loss: 2.62082841694\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (33.5687141418,53.2030713515), test loss: 45.0634097576\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.68994009495,15.238131897), test loss: 3.03749678731\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (12.01603508,52.4528012185), test loss: 41.6192359209\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.97556948662,13.8561967645), test loss: 2.96562051773\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (60.5862464905,51.8309346824), test loss: 45.7803634167\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.53080701828,12.7496015081), test loss: 2.83199463487\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (27.1694736481,51.2810139736), test loss: 37.9322022915\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.89973735809,11.8437699916), test loss: 2.72355697751\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (48.0924568176,50.8339600133), test loss: 44.1369065762\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.31115949154,11.086912934), test loss: 2.98615971208\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (15.176361084,50.4538789777), test loss: 43.0459394932\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.59982395172,10.447201509), test loss: 3.24420907199\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (34.861000061,50.1346328816), test loss: 40.1986443996\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.22077655792,9.8997556354), test loss: 2.64387898147\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (42.4847755432,49.8373217215), test loss: 46.4802734375\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.0491591692,9.42523764697), test loss: 3.14626916051\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (57.467792511,49.5645606946), test loss: 42.8444905281\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.96086287498,9.01024184852), test loss: 2.70517802238\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (46.8568267822,49.3100319691), test loss: 42.7302702904\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.9710162878,8.64333555519), test loss: 3.03412774205\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (32.1273727417,49.0638763866), test loss: 41.3885789633\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.37612867355,8.31728547375), test loss: 2.89315653145\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (19.5976104736,48.8159268435), test loss: 46.3410886765\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.81485700607,8.02489253975), test loss: 2.8605951786\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (38.9225502014,48.6214254117), test loss: 38.5101523876\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.09557151794,7.76168751433), test loss: 2.6802118063\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (50.5430831909,48.4112736531), test loss: 42.4719272614\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.31445598602,7.52292108122), test loss: 2.89329094291\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (12.588010788,48.2291602211), test loss: 39.9731396914\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.90778684616,7.30566891034), test loss: 3.07598971426\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (25.6135025024,48.0463700537), test loss: 42.6808905602\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.41121149063,7.10706919629), test loss: 2.84699074626\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (22.1798057556,47.8680956659), test loss: 44.9676701546\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.82913255692,6.92546953474), test loss: 3.05371721387\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (35.3884124756,47.6957890976), test loss: 42.1733090401\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.31688904762,6.75748777132), test loss: 2.61358155608\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (79.4039077759,47.5205495604), test loss: 42.8414142132\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.19536972046,6.60227022217), test loss: 3.08063545823\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (86.474571228,47.3442940735), test loss: 39.5197140694\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.34056615829,6.45759887211), test loss: 2.81907711625\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (41.3909988403,47.1596458905), test loss: 43.6347698212\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.75072717667,6.32311218683), test loss: 2.88709200025\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (16.5487327576,46.9827297725), test loss: 37.0745048046\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.404296040535,6.19641740189), test loss: 2.77927805781\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (19.4622650146,46.8172016771), test loss: 41.0689850807\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.17805433273,6.07837846049), test loss: 2.78405404091\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (24.0904064178,46.6520163038), test loss: 38.477231288\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.33364582062,5.96777951941), test loss: 2.95174828172\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (49.8686828613,46.476471836), test loss: 39.7834730625\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (6.07649040222,5.86328254429), test loss: 2.83354514837\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (19.5079421997,46.3035002124), test loss: 40.5274306297\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.84586000443,5.76458092826), test loss: 2.99030846655\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (25.1050109863,46.1258413587), test loss: 38.6241085052\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.18206501007,5.6711412858), test loss: 2.39519042373\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (47.1009674072,45.9392138318), test loss: 40.7067061424\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.36846542358,5.58230713707), test loss: 3.05335022211\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (31.0765037537,45.7414597964), test loss: 37.638891983\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.59649538994,5.49758526864), test loss: 2.70143621564\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (63.0404243469,45.5557939639), test loss: 40.2231301785\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.01427030563,5.4167189267), test loss: 2.76721503139\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (21.0827922821,45.349964428), test loss: 36.198585844\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.73225593567,5.33953461878), test loss: 2.64576890469\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (14.5685825348,45.1560600824), test loss: 39.3576431513\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.776966869831,5.26570801052), test loss: 2.66634520292\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (11.4476718903,44.9381304897), test loss: 30.4085751295\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.79363489151,5.19484448843), test loss: 2.51005303264\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (72.8244400024,44.7121589293), test loss: 36.6175075054\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.2810986042,5.12712276554), test loss: 2.78269802332\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (58.9925041199,44.4741005236), test loss: 35.5408162594\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.691593647,5.06170049489), test loss: 3.0089284271\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (27.1102161407,44.2233413784), test loss: 32.3280543327\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.41636586189,4.99884919248), test loss: 2.47234131992\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (97.3589401245,43.9633894335), test loss: 36.3912137985\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.76696920395,4.93794408384), test loss: 2.96353128254\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (16.3362464905,43.699566152), test loss: 33.7087640285\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.42136621475,4.879210548), test loss: 2.5548891753\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (13.9086322784,43.4263554964), test loss: 35.1760356903\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.822255373,4.82201311325), test loss: 2.8104914248\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (28.7554740906,43.1483562357), test loss: 31.7756774664\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.67659091949,4.76690801869), test loss: 2.72901913822\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (23.2751502991,42.8622258467), test loss: 34.9831110954\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.01140069962,4.71370104608), test loss: 2.60426263809\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (11.3313827515,42.5645602511), test loss: 28.3677357197\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.3277810812,4.66227406226), test loss: 2.66139819622\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (19.8721427917,42.2649683024), test loss: 31.9083569527\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.724216639996,4.61253882882), test loss: 2.75558688641\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (20.6419773102,41.9586731317), test loss: 29.8951004267\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.03789567947,4.56470480471), test loss: 2.99228990376\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (17.3040332794,41.6432247048), test loss: 29.9432235241\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.80768430233,4.51795455374), test loss: 2.71901116967\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.7835493088,41.3232026952), test loss: 32.3955635548\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.88754153252,4.47261557497), test loss: 2.93933752477\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (57.1243438721,41.0068617173), test loss: 30.5326717138\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.51801896095,4.4285502926), test loss: 2.60999873281\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (18.8262233734,40.6843673837), test loss: 30.0462750435\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.979248821735,4.3858194573), test loss: 2.77539277077\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (27.2036361694,40.3632822084), test loss: 30.4243653297\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.61348748207,4.34468171586), test loss: 2.8507185936\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (10.3970432281,40.0416566447), test loss: 31.6233285189\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.926844060421,4.30455166979), test loss: 2.7791234225\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (22.4007453918,39.7247772078), test loss: 28.0787471294\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.64576876163,4.26594907636), test loss: 2.82634547651\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (21.00025177,39.4141872871), test loss: 30.9720147848\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.10215425491,4.22858667119), test loss: 2.81936689019\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (15.7283678055,39.1039026613), test loss: 28.266085577\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.357051640749,4.19222040537), test loss: 2.89449827075\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (44.6241226196,38.7997377472), test loss: 31.5614293337\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.77273035049,4.15695343748), test loss: 2.8780854404\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.8188486099,38.5051904921), test loss: 31.2942076683\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.20171260834,4.12248246901), test loss: 3.02199207544\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (31.2686691284,38.216299508), test loss: 30.4216564894\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.30473721027,4.08899158276), test loss: 2.58007435203\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (22.2119445801,37.934091149), test loss: 30.5321479321\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.07166576385,4.0565150943), test loss: 2.87283910513\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (9.0186662674,37.6567928885), test loss: 31.480614233\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.21466708183,4.02498433562), test loss: 2.86387270391\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (31.2764053345,37.3861107593), test loss: 28.9179124117\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.44006109238,3.99446489394), test loss: 2.67848673463\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (19.5104064941,37.1225186543), test loss: 30.4288613558\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.916048765182,3.96492950494), test loss: 2.99037757814\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (18.4905490875,36.8654324175), test loss: 31.492345047\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.12582933903,3.93628165244), test loss: 2.68892855048\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (18.8400325775,36.6114438033), test loss: 31.2923811197\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.83179831505,3.90820663783), test loss: 2.98270979226\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (10.6556816101,36.3637178111), test loss: 32.1411752224\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.06330931187,3.8808129513), test loss: 2.99755144715\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (9.6379070282,36.1246414051), test loss: 29.1527062416\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.81225848198,3.853984061), test loss: 2.94278437495\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (10.5580701828,35.8920406051), test loss: 31.424533987\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.454241395,3.8279448098), test loss: 2.58146478534\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (20.1785240173,35.6641806219), test loss: 30.8096491814\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.38675689697,3.80268303052), test loss: 2.99533852339\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (18.9100914001,35.4404870028), test loss: 30.8980979443\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.86546468735,3.77793423284), test loss: 2.61480861306\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (19.6928958893,35.2228194709), test loss: 30.9951843023\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.298302173615,3.75400561344), test loss: 2.86851790249\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (9.70158100128,35.0116204195), test loss: 33.1136814117\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.30884242058,3.73082674741), test loss: 3.03822599947\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (14.9230651855,34.8021787796), test loss: 32.5382504463\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.28043150902,3.70799124679), test loss: 2.76159967035\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (27.3243904114,34.5979838726), test loss: 28.1441284418\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.48453366756,3.6856571926), test loss: 2.72461211979\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (25.657541275,34.401967332), test loss: 32.6382730484\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.00727558136,3.66378948466), test loss: 2.92087726593\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (6.41617631912,34.2079725846), test loss: 29.9829890251\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.60245418549,3.64245313852), test loss: 3.10384442806\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (24.4296073914,34.0201959483), test loss: 30.0417803049\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.825254559517,3.62157218906), test loss: 2.75858403742\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (29.4842758179,33.8337998277), test loss: 30.2936367989\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.51808595657,3.60113188591), test loss: 2.92358039618\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (11.1209869385,33.6534033757), test loss: 34.1729133129\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.843867480755,3.58135482491), test loss: 2.81618937701\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.5235004425,33.477018607), test loss: 29.7222735882\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.887982428074,3.56204580821), test loss: 2.77629769146\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (15.0149784088,33.3028798238), test loss: 35.0073731899\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.4567937851,3.54325730044), test loss: 3.03695357591\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (17.7065505981,33.1323453641), test loss: 32.5164021254\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.32606124878,3.52467261069), test loss: 2.69802412242\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (5.75490570068,32.9660974918), test loss: 30.3695013046\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.65481758118,3.50651450236), test loss: 2.81690900326\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (16.4255123138,32.8045365976), test loss: 32.5223716021\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (3.94133520126,3.48849748821), test loss: 2.85220200419\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (22.6672267914,32.6461695872), test loss: 29.5270708561\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (6.22020816803,3.47101260871), test loss: 3.06009023339\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (19.0605640411,32.4898540499), test loss: 34.0317151785\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (4.02811050415,3.45386545094), test loss: 2.96567872167\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (27.6817054749,32.3364965865), test loss: 32.0339557171\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.13959407806,3.43704240385), test loss: 2.99282854795\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (18.0113105774,32.1866820344), test loss: 33.0301403522\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.12681484222,3.42070029656), test loss: 2.68318642825\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (17.9616012573,32.0411331162), test loss: 30.7029462337\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.5314218998,3.40484424746), test loss: 2.82855252922\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.5980072021,31.8947678213), test loss: 35.3559358597\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.315050899982,3.38905132279), test loss: 2.96610670984\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (14.0922908783,31.7520707649), test loss: 31.3776713848\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.00714921951,3.37358161487), test loss: 2.76363007277\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (45.9402084351,31.6147014218), test loss: 30.1724718094\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.09882974625,3.35831317898), test loss: 2.9203532204\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (21.2546691895,31.4790842037), test loss: 32.4704123974\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.22585487366,3.34331154056), test loss: 2.77283734679\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (36.5098953247,31.3459404685), test loss: 29.408150053\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.1839363575,3.32869244154), test loss: 2.94616533965\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (8.92442512512,31.2134063113), test loss: 33.4403532743\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.49405622482,3.31420733985), test loss: 2.9147995621\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (9.78721237183,31.0845211868), test loss: 28.9846770525\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.07720506191,3.3001800962), test loss: 2.92371724248\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (16.1476402283,30.9591415792), test loss: 33.2366262436\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.548732280731,3.28648450564), test loss: 2.65496692806\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (17.5441970825,30.8329326773), test loss: 31.9588926792\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.605219364166,3.27292881228), test loss: 2.93193409443\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (33.5983390808,30.710125463), test loss: 32.7867723942\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (4.02302503586,3.25965958767), test loss: 2.83613804877\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (8.32882976532,30.5902698173), test loss: 29.2343205929\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.039965868,3.24644282776), test loss: 2.65145664811\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (21.3529930115,30.4729001902), test loss: 31.9676800728\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.20552492142,3.23348126645), test loss: 2.91312709153\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (24.9237670898,30.3571899129), test loss: 31.0145603895\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.0867497921,3.22074399629), test loss: 2.63387508392\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.0568561554,30.242419233), test loss: 27.0309374094\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.93315458298,3.20822444552), test loss: 2.70189555138\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (24.062330246,30.1295812939), test loss: 32.6924833298\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.14554238319,3.19595308412), test loss: 2.94518628716\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (12.8936138153,30.0191957777), test loss: 28.7854902744\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.897125840187,3.18400740912), test loss: 2.9460889101\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (24.365354538,29.9111195545), test loss: 33.6394970894\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.369258642197,3.17229333905), test loss: 2.92846701443\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (12.9348125458,29.8024378582), test loss: 29.3824366093\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.97259807587,3.16065968402), test loss: 2.90950272381\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.08663845062,29.6961458124), test loss: 33.4218234777\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.26063942909,3.14916914573), test loss: 2.75638145804\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (14.9000749588,29.5928743393), test loss: 30.4456076622\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (4.41833353043,3.13777328234), test loss: 2.74478018284\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (24.3035430908,29.49149379), test loss: 31.6353651047\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.6623415947,3.12660711339), test loss: 2.90864228606\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (20.9893226624,29.3909009825), test loss: 32.3197519064\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.2546248436,3.11564785107), test loss: 2.67904876173\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (18.7832736969,29.2907357763), test loss: 29.8897171497\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.50593137741,3.10478635025), test loss: 2.82977493405\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (24.6006145477,29.1930496376), test loss: 32.4414073944\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.837930560112,3.09422063376), test loss: 2.82703973055\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (13.8660497665,29.0974525161), test loss: 29.871533823\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.517625808716,3.08391561028), test loss: 3.04738826454\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (14.1636867523,29.0007943246), test loss: 31.9873914719\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.36832582951,3.07363976581), test loss: 2.86139111519\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (19.4327983856,28.906467978), test loss: 29.6812974453\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.03564405441,3.06349089138), test loss: 2.8504304871\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (22.645154953,28.8151132501), test loss: 34.0926637411\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.45174241066,3.0534517105), test loss: 2.71843446791\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (4.84075593948,28.7236292786), test loss: 29.2581250191\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.17340898514,3.04357153815), test loss: 2.63235064149\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (13.053442955,28.6345394365), test loss: 33.0971661091\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.29850244522,3.03382120475), test loss: 2.94082633555\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (34.8031311035,28.5448694107), test loss: 31.3236616611\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.11375021935,3.02416496922), test loss: 2.72275931239\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (10.0626468658,28.4573049721), test loss: 30.7514969349\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.12820696831,3.01475999596), test loss: 2.86687698662\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (5.02803611755,28.3714280345), test loss: 31.8851601601\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.04276323318,3.00553400497), test loss: 2.79746764898\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.29292011261,28.2856436219), test loss: 29.5450514793\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (3.08777308464,2.99650742119), test loss: 2.8757581383\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (4.9728603363,28.2008193404), test loss: 33.4545993328\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.760915756226,2.98746504866), test loss: 2.87597307116\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (3.33789968491,28.1177248094), test loss: 31.01684618\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.51752948761,2.978582999), test loss: 2.95807362497\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (11.1677770615,28.0364267907), test loss: 32.4964615822\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.28064250946,2.96966657323), test loss: 2.58074667454\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (20.1386604309,27.9560638811), test loss: 30.5790622234\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (5.9751739502,2.96097691611), test loss: 2.77859865427\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (20.6351051331,27.8760539334), test loss: 33.6853539944\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (4.86294269562,2.9523873194), test loss: 2.89432583451\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (24.9806632996,27.7965498281), test loss: 28.8456322908\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.43360900879,2.94385160545), test loss: 2.65517364144\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (17.0122203827,27.7186782093), test loss: 31.5636457443\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.23112082481,2.9355555049), test loss: 2.98681841642\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (16.4903144836,27.6428153441), test loss: 31.0509909153\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.02347147465,2.92748512502), test loss: 2.58711574823\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (5.94776248932,27.5650763272), test loss: 31.308564496\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.631224632263,2.9193586595), test loss: 2.88325456977\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (10.0308990479,27.489477073), test loss: 32.6453296185\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.00679159164,2.91134985933), test loss: 2.89744447172\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (45.1327781677,27.4163477887), test loss: 28.7879304171\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.36452960968,2.90338501713), test loss: 2.81302788258\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (16.4045848846,27.3434312281), test loss: 32.0235211134\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.29941558838,2.89552403258), test loss: 2.53278492689\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (25.1209831238,27.271370564), test loss: 30.5361311436\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.43274831772,2.8877886263), test loss: 2.8625353694\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (9.4170589447,27.1989505335), test loss: 31.7268406868\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.05698013306,2.8800862855), test loss: 2.60367974639\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (14.6814002991,27.1282116585), test loss: 30.139648509\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.21175599098,2.87258314399), test loss: 2.78194314092\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (12.807302475,27.0592979149), test loss: 32.7197000504\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.384126067162,2.86524351479), test loss: 2.97619481683\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (10.3548088074,26.9890428701), test loss: 32.2864946842\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.630442142487,2.85793570938), test loss: 2.68623899817\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (24.5394649506,26.9205082879), test loss: 27.8619463921\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (3.36047840118,2.85073671766), test loss: 2.65418452024\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.35480880737,26.8533758713), test loss: 31.9968228579\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (3.74176073074,2.84354361013), test loss: 2.81412038207\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (9.0442905426,26.7873072621), test loss: 29.1852241039\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.56378090382,2.8363969411), test loss: 2.93509183526\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (15.055059433,26.7215165357), test loss: 30.4495340824\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.02693355083,2.82934976938), test loss: 2.70658236742\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (8.59460735321,26.65603027), test loss: 29.1760402203\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.9125649929,2.82240002219), test loss: 2.82428056002\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (11.2276935577,26.5911305565), test loss: 34.5795354366\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.02214574814,2.81554492349), test loss: 2.73274595141\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.99444055557,26.5273853944), test loss: 29.4568103313\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.01850223541,2.80885246458), test loss: 2.72086241245\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (16.2763175964,26.4649414828), test loss: 36.4508223057\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.39339569211,2.80227527861), test loss: 3.07748602629\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (12.9179620743,26.4013211788), test loss: 32.2366488934\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.25678360462,2.79570175732), test loss: 2.63891041875\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (20.2623500824,26.3392233667), test loss: 30.1187261581\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.08480381966,2.78915544566), test loss: 2.7478252247\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (17.0715045929,26.2785762189), test loss: 31.7141548157\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (5.11006259918,2.78266528735), test loss: 2.74108899832\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (20.9579792023,26.2185293107), test loss: 29.9340520144\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.77361750603,2.7762170263), test loss: 2.94381112307\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (19.6912155151,26.1587396385), test loss: 33.8770076036\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.739768028259,2.76989138317), test loss: 2.85383887291\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (12.4162931442,26.0988530344), test loss: 31.2919043541\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.01420211792,2.76358887766), test loss: 2.84054707587\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (21.5101203918,26.0403163391), test loss: 33.7081999302\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.84536266327,2.75743335941), test loss: 2.64828520715\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold2/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (338.597442627,inf), test loss: 184.135882187\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (301.675170898,inf), test loss: 364.692315674\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (21.4429702759,70.8285921211), test loss: 46.8481045723\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.972562670708,124.617617457), test loss: 3.39591273367\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (66.1876678467,56.9463659964), test loss: 38.5277825594\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.40106534958,63.8609274989), test loss: 3.09270614088\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (22.9320220947,52.2903877834), test loss: 43.2105862617\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.74007201195,43.6055890185), test loss: 3.28949934244\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (23.1015586853,49.7065099177), test loss: 37.2038361311\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.15950679779,33.4734910902), test loss: 3.4186403662\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (49.522983551,48.1802148106), test loss: 40.2965368748\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.90489256382,27.396218238), test loss: 2.71783913374\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (10.5567674637,46.9609093657), test loss: 40.9666926384\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.27359604836,23.3413697229), test loss: 3.67823069096\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (61.7082901001,46.0307020112), test loss: 39.4214921951\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (7.00775337219,20.4475920442), test loss: 2.54500553608\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (39.6322555542,45.2359097615), test loss: 40.7788326263\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.668167412281,18.2706582538), test loss: 3.52699279487\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (17.4317264557,44.5337637298), test loss: 35.2285626888\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.42405414581,16.5788212381), test loss: 2.72094684243\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (41.5289802551,43.8729152899), test loss: 43.7632267952\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.75586795807,15.2225883674), test loss: 3.50076871216\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (54.2935409546,43.3057647516), test loss: 32.960833168\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.67642903328,14.1088964829), test loss: 2.51059408188\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (13.2345924377,42.7418692334), test loss: 41.0112598181\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.42327880859,13.1723298083), test loss: 3.31762952805\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (37.458656311,42.2286935525), test loss: 31.9359538555\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.20896148682,12.3789741745), test loss: 3.04138784409\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (52.5116729736,41.6997777805), test loss: 40.3192202091\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.04028367996,11.6948556995), test loss: 3.08092903495\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (13.8148870468,41.1822442548), test loss: 33.5930857658\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (5.55729436874,11.0978028317), test loss: 3.31089217663\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (22.3290119171,40.6727598576), test loss: 35.4534461021\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.62445640564,10.5738365665), test loss: 2.49587705433\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (24.120464325,40.150867349), test loss: 36.9104486942\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.09657478333,10.1090697211), test loss: 3.54537444711\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (40.3789749146,39.624746758), test loss: 32.972385025\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.82022941113,9.6945773491), test loss: 2.35533159375\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (32.1474075317,39.1087981919), test loss: 36.9978322029\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.68572998047,9.32123961254), test loss: 3.36700745821\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (28.4200782776,38.6116159645), test loss: 28.4993398666\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.64876031876,8.98421035148), test loss: 2.41074059904\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (33.8190765381,38.1072910596), test loss: 37.46545403\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.93004751205,8.67778927899), test loss: 3.18828218579\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (20.4369735718,37.5980142171), test loss: 24.7789329529\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.29299163818,8.39855827759), test loss: 2.3584513545\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (12.2353076935,37.1012859408), test loss: 35.6890490294\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.887124955654,8.14174196412), test loss: 3.12947580814\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (71.1245269775,36.6046488606), test loss: 26.5105098486\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.64770150185,7.90558278915), test loss: 3.0682805568\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (11.5234470367,36.1030193462), test loss: 34.1687307835\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.53301382065,7.68677126051), test loss: 2.78718284667\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (12.2190036774,35.6255879094), test loss: 29.380257225\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.78557443619,7.48407283574), test loss: 3.27046578825\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (13.7972660065,35.1574728281), test loss: 30.0326919079\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.9787787199,7.29446788474), test loss: 2.31619656086\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (8.32598495483,34.7078677469), test loss: 33.3572300196\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.692056536674,7.11828532945), test loss: 3.40161160827\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (54.9372673035,34.2647979012), test loss: 26.4726968765\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.85791349411,6.95321593347), test loss: 2.32525105774\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.1297416687,33.8376949166), test loss: 34.0480901718\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.09892892838,6.79958262038), test loss: 3.2838945806\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (26.5163993835,33.4297127143), test loss: 25.7262514353\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.89863061905,6.65453204393), test loss: 2.50605678856\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (22.4442405701,33.0287150972), test loss: 34.4986091137\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.454608023167,6.51819386441), test loss: 3.17288299799\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (47.5552368164,32.6435199823), test loss: 26.5442486763\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.35402154922,6.3897481687), test loss: 2.77292227\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (13.0032062531,32.2814874559), test loss: 33.0947957516\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.49096918106,6.26835812035), test loss: 3.08897630572\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (10.3133354187,31.9321787055), test loss: 26.5366584778\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.54077863693,6.15308754389), test loss: 3.15480179489\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (19.1522293091,31.602362954), test loss: 28.4259501696\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.36943984032,6.04436230317), test loss: 2.59586875439\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (6.07187128067,31.2760860479), test loss: 31.4832598925\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.877785086632,5.94081658571), test loss: 3.41596487164\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (11.4985904694,30.9714367207), test loss: 29.417028904\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.5276799202,5.84340343591), test loss: 2.15979833305\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (26.3627052307,30.6810403098), test loss: 31.2172523022\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.39603948593,5.75024932166), test loss: 3.35045812726\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (8.42911148071,30.3956345832), test loss: 28.3400960922\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.33977842331,5.66162647796), test loss: 2.3632861495\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (20.0047187805,30.1220323277), test loss: 34.30486269\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.32849574089,5.57687559837), test loss: 3.3428683579\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (29.8181381226,29.8666542873), test loss: 26.954817462\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.37452602386,5.49613409076), test loss: 2.43466081023\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (21.6437721252,29.6202145422), test loss: 35.0605888009\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.711597681046,5.4185195349), test loss: 3.16542982459\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (55.8762168884,29.383543007), test loss: 27.1202213287\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.40417432785,5.34490279918), test loss: 3.01011828035\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (19.3194999695,29.1512687513), test loss: 34.4624980688\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.37798130512,5.27399624049), test loss: 3.07185374498\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.0058994293,28.932960788), test loss: 26.8941865444\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (4.64723396301,5.20649606262), test loss: 3.33517412245\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (21.4704914093,28.7250947039), test loss: 31.0917090416\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.83676588535,5.14166125868), test loss: 2.53447431028\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.9413890839,28.5173446555), test loss: 30.4529059887\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.75018322468,5.07913658906), test loss: 3.51399841011\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (6.27398300171,28.3190805466), test loss: 29.5164399147\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.39916312695,5.01906408873), test loss: 2.18311958909\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (28.4984455109,28.131298392), test loss: 32.9173642635\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (5.28558254242,4.96117337499), test loss: 3.31321369708\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (15.1961431503,27.9524940367), test loss: 26.7689176798\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.08306908607,4.90542426835), test loss: 2.40078080297\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (14.3318328857,27.7750831618), test loss: 35.9522349119\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.90300416946,4.85172896602), test loss: 3.180233109\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (16.9712791443,27.6040728954), test loss: 24.8746198177\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.17115986347,4.80018758016), test loss: 2.39240254164\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (9.19523620605,27.4414862864), test loss: 35.3790407419\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.833607673645,4.75048882381), test loss: 3.13695583642\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (54.0739898682,27.2828385991), test loss: 25.8246441364\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.36297106743,4.7025440899), test loss: 3.10532462895\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (33.451675415,27.1259812639), test loss: 34.633411622\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.68704378605,4.65605588892), test loss: 3.11234780252\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (7.01686096191,26.976479728), test loss: 26.9290254593\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.56121277809,4.61118279021), test loss: 3.32485108972\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (8.70034599304,26.8329795826), test loss: 31.0526937008\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.01087903976,4.56735494296), test loss: 2.40054572225\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (3.45865368843,26.6949311163), test loss: 32.8286186218\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.366049289703,4.52527510701), test loss: 3.38658574522\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (46.1025543213,26.5572934264), test loss: 28.2192916393\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.90773630142,4.4842656031), test loss: 2.29917275608\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.797208786,26.4244839598), test loss: 34.0694799423\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.57202577591,4.44501081826), test loss: 3.27917702496\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (17.4843387604,26.2980456332), test loss: 26.7017727852\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.39348244667,4.40673936471), test loss: 2.47329581678\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (15.7887439728,26.1706712515), test loss: 36.082895565\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.582348108292,4.36957761398), test loss: 3.19231217504\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (29.6455287933,26.0479656196), test loss: 24.5259652853\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.32341265678,4.33359465356), test loss: 2.43678073883\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (8.95003986359,25.9308332118), test loss: 35.295055604\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.57151651382,4.29854633983), test loss: 3.1179484427\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.9859218597,25.816606343), test loss: 26.2256645203\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.13730335236,4.26427386383), test loss: 3.08726902902\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (20.0432872772,25.7065598714), test loss: 28.7156422377\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.06007385254,4.23120330412), test loss: 2.59356560558\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (16.2721977234,25.5943316053), test loss: 29.9245135307\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.565963566303,4.19876703293), test loss: 3.38289501965\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (11.4215373993,25.4887896348), test loss: 30.6789468765\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.76363909245,4.16773852242), test loss: 2.36248319149\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (27.462184906,25.3873159918), test loss: 32.2933109283\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.20024800301,4.13736921343), test loss: 3.30064209998\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (11.75344944,25.2838198377), test loss: 28.2425453901\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.1560959816,4.10773798936), test loss: 2.38239518106\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (23.278087616,25.1837407186), test loss: 33.2276512146\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.28039121628,4.07882127862), test loss: 3.19459025264\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (19.7414398193,25.0891772126), test loss: 26.1559892178\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.983083128929,4.05072346855), test loss: 2.47751024961\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (20.8093643188,24.9961290597), test loss: 35.3549345493\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.974610984325,4.023098663), test loss: 3.17522019446\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (48.9906272888,24.9047356024), test loss: 27.5171686172\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.66176843643,3.99647985955), test loss: 2.91173650324\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.6969833374,24.8125636198), test loss: 34.6259546995\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.918492913246,3.97026885142), test loss: 3.13402295709\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.3038883209,24.7253294161), test loss: 26.3828783989\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.33390724659,3.9449940137), test loss: 3.23054450452\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (18.3886833191,24.6415413076), test loss: 30.5305600643\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.20360743999,3.92034101574), test loss: 2.60922756493\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (25.5478668213,24.5550570749), test loss: 30.4972147942\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.36582565308,3.8961304832), test loss: 3.47504321635\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (7.92462205887,24.4713607181), test loss: 29.6265138149\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.10474944115,3.87239640072), test loss: 2.23278028667\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (18.5009651184,24.3918906501), test loss: 31.9508635044\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (5.04577159882,3.84930630283), test loss: 3.26474166811\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (11.292766571,24.3146327427), test loss: 27.1668127537\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.33878755569,3.82659193478), test loss: 2.4263142854\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (19.1766242981,24.236376789), test loss: 36.3385395527\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.24086856842,3.80446775955), test loss: 3.35196837783\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (17.4688682556,24.1598247217), test loss: 25.4170601606\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.932064056396,3.78289959), test loss: 2.35940599144\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (9.76311397552,24.0863120582), test loss: 34.8615647793\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.774576544762,3.76186480576), test loss: 3.12983202934\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (50.3458099365,24.0139251817), test loss: 27.1161808968\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.53324079514,3.74135905164), test loss: 3.06239284873\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (58.3708152771,23.9412572328), test loss: 34.4210355043\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.01055526733,3.72114804907), test loss: 3.10756144226\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (5.1167140007,23.8702681331), test loss: 26.735521543\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.71367990971,3.70139358363), test loss: 3.26786892414\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (8.26563453674,23.8024004614), test loss: 31.1445522547\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.55756187439,3.6818690016), test loss: 2.41663487852\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (4.66335487366,23.7361643716), test loss: 32.371565783\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.343848913908,3.6628981844), test loss: 3.37186307013\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (27.7784633636,23.6687958579), test loss: 29.2034803152\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (4.23184251785,3.64420886712), test loss: 2.21651069224\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (20.1519432068,23.6032585421), test loss: 33.9959364653\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.89017343521,3.62610987128), test loss: 3.29717121124\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (7.52291584015,23.540224127), test loss: 26.16157763\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.977727293968,3.60827405091), test loss: 2.43013205528\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (17.7038764954,23.4757334102), test loss: 35.9030686855\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.67440259457,3.59082366067), test loss: 3.10450504422\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (16.1224689484,23.4132841466), test loss: 24.3375957489\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.33512783051,3.57374700021), test loss: 2.41245987415\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (9.92097568512,23.353030959), test loss: 35.1571266651\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.33092355728,3.55691311175), test loss: 3.10858693123\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (20.2712440491,23.2939847349), test loss: 25.3922021866\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.76038479805,3.54028750946), test loss: 2.99663059115\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (21.4200019836,23.2362639836), test loss: 33.2904129267\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.01136922836,3.52411282886), test loss: 2.7663225174\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (15.2221364975,23.1760000465), test loss: 28.2533228874\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.2311052084,3.50805856999), test loss: 3.25931893289\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (7.99253082275,23.11951005), test loss: 30.3735929012\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.53423523903,3.4926238613), test loss: 2.35466564\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (34.8315582275,23.0649988885), test loss: 33.4423686981\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.18139672279,3.47737793431), test loss: 3.31653214097\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (13.109003067,23.0078229992), test loss: 27.8073687315\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.70282483101,3.46235187863), test loss: 2.37372491062\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (18.2285461426,22.9529452936), test loss: 33.5542156696\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.60587060452,3.44757629399), test loss: 3.22269886136\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (15.2871570587,22.9004933174), test loss: 26.1424249649\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.13709175587,3.43307055924), test loss: 2.46953926384\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (21.8873710632,22.8484223551), test loss: 35.3059719563\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.95056283474,3.41868412933), test loss: 3.17224571109\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (47.1121063232,22.7967636621), test loss: 27.3027217865\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.92964076996,3.40475651296), test loss: 2.81831509471\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (25.8561401367,22.7439570743), test loss: 34.2050290585\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.3947353363,3.39087624549), test loss: 3.06027806699\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (26.1708431244,22.6938853565), test loss: 25.9488600731\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.54703199863,3.37742550893), test loss: 3.04264736474\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (11.3605422974,22.6452067972), test loss: 29.5156732559\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.580249667168,3.36421692436), test loss: 2.64987223148\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (19.9094238281,22.5943632992), test loss: 30.3477818966\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.37875938416,3.35114748201), test loss: 3.35889052451\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (6.95475721359,22.5452238768), test loss: 30.0451395035\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.980912387371,3.33822557333), test loss: 2.18213356584\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (16.5736503601,22.4982724134), test loss: 31.3715363503\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (5.29401779175,3.32558436072), test loss: 3.24912433922\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (4.37444496155,22.4522285575), test loss: 28.3342012644\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.23885953426,3.31300834189), test loss: 2.33465881944\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (22.4166469574,22.4052613376), test loss: 34.0349475384\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (5.87228488922,3.30077703569), test loss: 3.28023465872\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (12.5702419281,22.3585853055), test loss: 26.8335100651\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.651933789253,3.28864350408), test loss: 2.45631958246\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (8.35072517395,22.313783354), test loss: 34.8951966524\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.80094718933,3.27682447668), test loss: 3.089161852\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (54.288066864,22.2699375134), test loss: 27.6664694786\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (3.94346642494,3.26523625768), test loss: 2.97955325246\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (50.1181564331,22.224264373), test loss: 33.7559253216\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.48376274109,3.25369090373), test loss: 3.05189100206\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.24387168884,22.1802686869), test loss: 26.253345418\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.816106319427,3.24237768115), test loss: 3.24719462097\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (11.7066669464,22.1380238965), test loss: 31.5725824833\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.17107725143,3.23110526238), test loss: 2.5569892168\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (9.0710067749,22.0963447416), test loss: 30.5478101254\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.403518676758,3.2200913966), test loss: 3.40599374771\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (10.889957428,22.0535543078), test loss: 29.7256302834\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (3.02046465874,3.20919002698), test loss: 2.19389537573\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (17.8020896912,22.0116863424), test loss: 33.1026839256\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.11521804333,3.19856873864), test loss: 3.22010752261\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (6.33711338043,21.9713299192), test loss: 26.6353181362\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.31514930725,3.18804556676), test loss: 2.41046457738\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (14.7974643707,21.9297044814), test loss: 35.4989290237\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.81638288498,3.17771748608), test loss: 3.07444339991\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (5.74260139465,21.889274842), test loss: 24.4722055912\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.566983103752,3.16753728067), test loss: 2.3829749614\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (8.61163806915,21.850056097), test loss: 34.7952738285\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.75462734699,3.15745211974), test loss: 3.02144595087\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (28.2402954102,21.8117410071), test loss: 24.854876709\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.27332234383,3.1474454147), test loss: 2.93549715877\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (16.1147403717,21.7735795713), test loss: 34.219338274\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.667970657349,3.13764402062), test loss: 3.03848372698\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (14.7530202866,21.7335355803), test loss: 26.9003333092\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.10324001312,3.12788060254), test loss: 3.21254408956\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (14.3583440781,21.6960134848), test loss: 31.1906135559\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.74425888062,3.11844483284), test loss: 2.42259746194\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (22.4859371185,21.6594937112), test loss: 32.6697187424\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.30991220474,3.10908055611), test loss: 3.31933549047\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (13.2966175079,21.6210246922), test loss: 28.5278959751\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.62446260452,3.09983104116), test loss: 2.28807796836\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (15.3174438477,21.5841319628), test loss: 33.6762201309\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.47467112541,3.090665848), test loss: 3.18242153227\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (20.6914520264,21.5485974844), test loss: 26.1961918831\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.16396021843,3.08162678919), test loss: 2.41074599624\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (10.1469917297,21.5132108361), test loss: 35.3724142551\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.04217994213,3.07263885261), test loss: 3.07290645838\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (14.8488206863,21.4778182532), test loss: 24.1603975773\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.30710887909,3.0638903083), test loss: 2.36303706467\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (35.8324012756,21.4415764805), test loss: 35.2120576143\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.61172986031,3.05512647165), test loss: 2.99278824031\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (28.220085144,21.4069313255), test loss: 25.6206399202\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.725829958916,3.04661321793), test loss: 2.94802911282\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (10.3614149094,21.3729869026), test loss: 28.9091954708\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.08966434002,3.0382182776), test loss: 2.57419085503\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (19.180147171,21.3375041069), test loss: 28.8464612007\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.93039059639,3.02988034984), test loss: 3.2886356771\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (9.77485179901,21.3033017022), test loss: 30.7632156372\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.72378849983,3.02161844876), test loss: 2.34898183048\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (18.5961456299,21.2703811114), test loss: 32.3943002701\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (4.92885780334,3.01345747167), test loss: 3.1739033103\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (6.77476358414,21.2379822095), test loss: 27.9108726501\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.48363292217,3.00534564879), test loss: 2.2972230494\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (28.6121940613,21.2047295523), test loss: 32.7496938229\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (6.17462921143,2.99741184343), test loss: 3.08069312871\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (9.91561508179,21.1713602185), test loss: 26.0091965675\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.77069735527,2.98950812244), test loss: 2.4368917048\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (6.332903862,21.1394291772), test loss: 34.7060337067\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.18575072289,2.98178478125), test loss: 3.02712923586\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (49.8776779175,21.1079520084), test loss: 27.591542387\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (3.91862249374,2.9741922169), test loss: 2.80960629582\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (37.8648223877,21.0751855232), test loss: 33.8026957035\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.75865983963,2.96659874055), test loss: 3.03053033352\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (6.70076274872,21.0434996073), test loss: 25.6531876206\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.370273649693,2.95912732054), test loss: 3.09221815765\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (11.4571113586,21.0129069137), test loss: 30.7765625715\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.38296508789,2.95165770943), test loss: 2.57415393591\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (14.6622009277,20.9827485697), test loss: 30.504976058\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.457894086838,2.94433509274), test loss: 3.33689748049\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (10.9649085999,20.951463698), test loss: 29.8966219187\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (3.90003800392,2.93706927986), test loss: 2.23148651123\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (14.8421697617,20.9206618804), test loss: 32.1168884754\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.65692734718,2.92994855823), test loss: 3.14539999664\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (8.09295368195,20.8911322734), test loss: 26.9925912857\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.24159193039,2.92289793178), test loss: 2.40782849491\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (29.8371295929,20.8605051733), test loss: 36.0886319637\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.04470825195,2.91593188913), test loss: 3.22074238658\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (7.03362083435,20.8305648982), test loss: 24.7475752831\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.991896271706,2.90906108591), test loss: 2.31261095703\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (7.61894321442,20.8015153175), test loss: 34.2712128639\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.74691784382,2.90223134185), test loss: 2.97619293481\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (30.4098911285,20.7730867192), test loss: 25.443522644\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.49384522438,2.89541680639), test loss: 2.8377943337\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold3/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (273.776794434,inf), test loss: 184.919059372\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (316.40020752,inf), test loss: 356.333236694\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (29.7353343964,103.764591643), test loss: 40.6982983589\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.68866825104,102.649048156), test loss: 3.11369553506\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (108.299346924,75.016294729), test loss: 35.4847635746\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.99494457245,52.9219267288), test loss: 3.2425401926\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (41.2453308105,65.2311991181), test loss: 40.1724964619\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.65363311768,36.341392482), test loss: 3.06874165237\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (9.48920440674,60.1856906995), test loss: 39.087840271\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.83026695251,28.0364387473), test loss: 3.10283652544\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (41.0208053589,57.1753715447), test loss: 42.6447431087\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (7.85509967804,23.0545023152), test loss: 2.65258820951\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (40.6907348633,55.1097413747), test loss: 40.9142585278\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (8.95123004913,19.7304504888), test loss: 3.37117401659\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (52.3096504211,53.5383494579), test loss: 39.4414636612\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.93594872952,17.353061721), test loss: 2.77543713152\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (40.7751235962,52.3144508635), test loss: 40.9552713156\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.59347343445,15.5679599495), test loss: 3.16314944327\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (27.5550270081,51.3043805002), test loss: 37.0001339912\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.21371889114,14.1771722049), test loss: 2.87698071003\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (30.3380947113,50.4336299191), test loss: 38.0965416431\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.01561951637,13.0617132034), test loss: 2.95617319942\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (37.1305541992,49.657446262), test loss: 33.293791151\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.00974798203,12.1472043312), test loss: 3.00273671746\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (47.6057434082,49.0099717733), test loss: 35.6870063782\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.76440358162,11.3809243204), test loss: 3.07655813992\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (53.1032485962,48.3937046489), test loss: 35.740067625\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.93966817856,10.7283318715), test loss: 3.24513365626\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (58.1595115662,47.8644898385), test loss: 39.4568956375\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.9947412014,10.1689923585), test loss: 2.58622021973\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (43.4692077637,47.3425425741), test loss: 37.3454772472\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.43364024162,9.68109575055), test loss: 3.29902450442\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (59.0565109253,46.845928799), test loss: 36.7353346825\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.56057405472,9.2537760393), test loss: 2.50410032868\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (55.6043663025,46.3364404064), test loss: 36.5388672829\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.71649456024,8.87420222479), test loss: 3.06948343813\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (25.1525325775,45.8103785464), test loss: 33.3005823135\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.29919719696,8.53515515477), test loss: 2.70908020735\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (67.2282333374,45.2879166131), test loss: 34.504242897\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.53214168549,8.22933883045), test loss: 2.84755732119\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (24.6897583008,44.785396737), test loss: 29.1272379875\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.36883497238,7.95307786351), test loss: 2.82141761184\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (6.70606899261,44.2774198763), test loss: 32.036597538\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.13900899887,7.70007712741), test loss: 2.99059637785\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (33.0578155518,43.7839432551), test loss: 28.8963581562\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (7.75050258636,7.46966661682), test loss: 3.17911009789\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (25.9367218018,43.2895921425), test loss: 32.6896396637\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (6.56893634796,7.25795208088), test loss: 2.61840887368\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (30.618183136,42.7868885355), test loss: 32.1726436377\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.74055671692,7.0628066509), test loss: 3.18870540857\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (21.6453781128,42.2844999502), test loss: 33.0319599152\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.65784287453,6.8824314905), test loss: 2.46113632321\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (16.8839626312,41.7802986318), test loss: 30.5938963413\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.92020726204,6.71496252077), test loss: 3.01711600423\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (18.2186279297,41.2700614721), test loss: 28.9830135107\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.33159053326,6.55867647628), test loss: 2.61736550927\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (23.2874412537,40.7558178583), test loss: 31.496688509\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.5799510479,6.41263899287), test loss: 3.00390761197\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (31.9766159058,40.258773725), test loss: 24.7611050129\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.05022644997,6.27548691553), test loss: 2.58920973241\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (54.1120567322,39.756985638), test loss: 27.9723015308\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.75853133202,6.14623257142), test loss: 3.08234006464\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (30.8090286255,39.2679672576), test loss: 23.8999922752\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.61483168602,6.02562867274), test loss: 3.14093357623\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (22.5787506104,38.7791159513), test loss: 27.9351249933\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.65994381905,5.91161859185), test loss: 2.67685607225\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (39.1893692017,38.3034005257), test loss: 29.0215240002\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.05494165421,5.80485147276), test loss: 3.22298414707\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (27.4810199738,37.8367789468), test loss: 28.2423604012\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.07299232483,5.70376045451), test loss: 2.38566683829\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (19.8487873077,37.3760045091), test loss: 28.875578022\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.73691344261,5.60831694502), test loss: 3.09098182917\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (26.6161670685,36.9254381941), test loss: 28.3025591373\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.15713143349,5.5176070105), test loss: 2.65883841813\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (13.8663063049,36.4955005808), test loss: 29.0813655376\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.2761387825,5.43164964582), test loss: 3.10784949958\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (13.8029222488,36.0790191473), test loss: 26.8924525738\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.86688899994,5.34935582034), test loss: 2.80427548885\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (23.5996017456,35.6789364933), test loss: 27.5638443708\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (7.30830907822,5.27154069432), test loss: 3.14416818023\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (18.5824966431,35.292198925), test loss: 24.026826334\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.38191175461,5.19739019612), test loss: 3.28530792892\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (14.0355014801,34.9206784465), test loss: 28.0207845092\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.87267637253,5.12690280611), test loss: 3.07509189844\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (14.4025154114,34.5646381147), test loss: 26.5392409801\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.84814214706,5.05999225081), test loss: 3.19485260546\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (9.94221305847,34.2249736685), test loss: 29.3904787064\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.81673026085,4.99609081878), test loss: 2.52118375897\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (12.8790311813,33.8946584868), test loss: 29.8835922003\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.08096563816,4.93479957945), test loss: 3.23045440018\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (23.5643806458,33.5759685002), test loss: 27.8225037813\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.46003484726,4.87593385803), test loss: 2.57275385559\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (32.842830658,33.2760079075), test loss: 31.7148654461\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.26648235321,4.81943436028), test loss: 3.27675817609\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (58.6137428284,32.9840653651), test loss: 27.2616402626\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.38122963905,4.76482349176), test loss: 2.76754582822\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (20.5105285645,32.7028750901), test loss: 27.8620041609\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.5260103941,4.71289646211), test loss: 3.06090517938\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (16.4856643677,32.4297776038), test loss: 25.0853980541\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.58776569366,4.66269815556), test loss: 3.16744653285\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (26.2373924255,32.1703693129), test loss: 28.3024380684\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.68551433086,4.61495645953), test loss: 3.14109545648\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (17.7408561707,31.9207645977), test loss: 26.2285936713\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.41401815414,4.56888738208), test loss: 3.32034761012\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (18.9874229431,31.6775738354), test loss: 30.2240015745\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.42079424858,4.52457509996), test loss: 2.58875963688\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (17.8709011078,31.4416072329), test loss: 28.240000391\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.01056957245,4.48162100374), test loss: 3.34576604366\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.6924285889,31.2168400136), test loss: 28.6928716421\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.77985453606,4.44016029491), test loss: 2.41765874773\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (16.3881378174,30.9991195909), test loss: 31.3694545269\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.35346031189,4.39968828898), test loss: 3.16518583596\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (22.1996440887,30.7883532691), test loss: 27.7154375553\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (6.61921691895,4.36088073685), test loss: 2.72375552356\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (21.2480888367,30.5825007454), test loss: 29.1877770901\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.56282138824,4.3232380956), test loss: 3.03621733785\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (14.5765266418,30.3839981245), test loss: 25.4812625408\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.85255742073,4.2869972215), test loss: 3.01857747138\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (14.9515810013,30.19220823), test loss: 28.8876308441\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.52424931526,4.25210762186), test loss: 3.10568593144\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (7.81818389893,30.0080992995), test loss: 25.7244995117\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.73029732704,4.21834298158), test loss: 3.28746157289\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (10.6590576172,29.8264608623), test loss: 28.9625007629\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.11676883698,4.18547361167), test loss: 2.70133603215\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (24.6576194763,29.6496337475), test loss: 28.5009538651\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.34402084351,4.15346420349), test loss: 3.31429766417\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (40.3723716736,29.4821069949), test loss: 30.300975132\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.94634723663,4.1223317502), test loss: 2.45278112292\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (54.8106079102,29.3169498161), test loss: 29.2638171434\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.46518301964,4.09181920299), test loss: 3.1455420047\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (25.6620140076,29.1558036436), test loss: 28.8341449261\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.41459262371,4.06255190589), test loss: 2.64991408288\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.3592834473,28.9974117911), test loss: 30.7389866829\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.2331135273,4.03387449732), test loss: 3.08881930113\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (18.7354316711,28.8458795074), test loss: 24.8761543274\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.37495326996,4.0063759003), test loss: 2.64617903531\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (17.360086441,28.6991677013), test loss: 28.8271696091\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.26705884933,3.97958175885), test loss: 3.06221970916\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (17.7963466644,28.5542745713), test loss: 25.9975193501\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.05891156197,3.95354275635), test loss: 3.25670961142\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (18.0624008179,28.4123940382), test loss: 29.0011501789\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.01644647121,3.92804602648), test loss: 2.72679238617\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (9.68359184265,28.2761692694), test loss: 28.8616876602\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.21370744705,3.90316235011), test loss: 3.27893662453\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (14.1827325821,28.1431697431), test loss: 29.7233365059\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.93689036369,3.87862227678), test loss: 2.30742968172\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (22.237537384,28.013199585), test loss: 30.0544950724\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (6.00837039948,3.85492290934), test loss: 3.14212011099\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (19.848449707,27.8847438588), test loss: 30.300647068\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.40106725693,3.83171583865), test loss: 2.5763969183\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.8866233826,27.7600504864), test loss: 30.7728806973\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.80354011059,3.80921202049), test loss: 3.07567056417\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.5536699295,27.6387051385), test loss: 27.6377036095\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.2639746666,3.78739208018), test loss: 2.74256532192\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (5.74763965607,27.5215829698), test loss: 29.04183321\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.48231446743,3.76613939766), test loss: 3.02825141847\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (7.2018699646,27.4045752095), test loss: 25.8023871422\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.05454659462,3.74527021271), test loss: 3.2133120209\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (24.9243793488,27.2901696438), test loss: 29.5917070627\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.4039452076,3.72480156525), test loss: 3.01312094629\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (48.158241272,27.1812605096), test loss: 27.7726037979\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (3.41775989532,3.70471609248), test loss: 3.1731485486\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (52.4729423523,27.072948247), test loss: 30.8899579048\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.27407264709,3.68489613784), test loss: 2.41644884944\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (25.272064209,26.9662359347), test loss: 30.9340193033\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.983064591885,3.66577197908), test loss: 3.22058510184\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (12.8438186646,26.8606930966), test loss: 28.8411267996\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.97243773937,3.64689916135), test loss: 2.48427716047\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (13.7755470276,26.7591974036), test loss: 32.7073049068\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.21396696568,3.62871802278), test loss: 3.21793187857\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (14.4537086487,26.6605032733), test loss: 27.5744822502\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.68441534042,3.61089281888), test loss: 2.73366511613\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (19.7272796631,26.5622732964), test loss: 28.8924990654\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.20740938187,3.59349521949), test loss: 3.00229942799\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (18.3908252716,26.4655019444), test loss: 25.5792316437\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.06207215786,3.57635163293), test loss: 3.07651085258\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (8.75702571869,26.3721332506), test loss: 29.2569635391\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.22731161118,3.55949246803), test loss: 3.06966833174\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (11.8751106262,26.2805219281), test loss: 27.0254405737\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.58091282845,3.54275538341), test loss: 3.22704546154\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (23.1071815491,26.1904577763), test loss: 30.9187521458\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (5.54466056824,3.52653761894), test loss: 2.50573883802\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (18.3030395508,26.1007288969), test loss: 29.1090740681\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.062312603,3.51055143975), test loss: 3.28850976229\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (13.6522350311,26.0132912005), test loss: 29.3260816097\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.35941779613,3.49498416629), test loss: 2.33307804316\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (12.7945919037,25.9278045218), test loss: 32.2114630699\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.26576220989,3.47983419893), test loss: 3.15084645748\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (4.97516155243,25.8450627421), test loss: 28.1382860184\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.827668428421,3.46500307452), test loss: 2.65564127266\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (4.54275131226,25.7616055181), test loss: 30.1590171576\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.95097476244,3.45037803955), test loss: 2.97167247981\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (20.384141922,25.6798464254), test loss: 25.6588859081\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.45377826691,3.43596547641), test loss: 2.93823418617\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (48.516960144,25.6018729678), test loss: 29.468632555\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.35543346405,3.42172908801), test loss: 3.04512582719\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (50.9864044189,25.5238961928), test loss: 25.830867672\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.22052788734,3.40762375434), test loss: 3.22479136586\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (24.9551124573,25.4464898351), test loss: 29.9208755016\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.805623173714,3.39395697919), test loss: 2.62089693844\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (11.6884965897,25.3695241205), test loss: 29.204515028\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.36481499672,3.3804015385), test loss: 3.26541290283\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (11.5631923676,25.2953376997), test loss: 30.7164818764\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.22206509113,3.36732501538), test loss: 2.3734297514\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (11.1603755951,25.223007938), test loss: 30.2740103483\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.03708982468,3.35443334399), test loss: 3.05977013409\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (25.0287952423,25.1506469667), test loss: 29.5321734428\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.93520689011,3.341832428), test loss: 2.62307627201\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (15.1696376801,25.079005158), test loss: 31.0911831856\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.993646383286,3.32935813544), test loss: 3.06968528926\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (9.45235061646,25.0097783001), test loss: 24.866890192\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.88528060913,3.31702977546), test loss: 2.5707187742\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (9.13175106049,24.9415380489), test loss: 29.8202231884\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (3.24766492844,3.30472363312), test loss: 2.97489433885\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (21.9330673218,24.874181913), test loss: 25.5263385057\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (5.16396522522,3.29279113189), test loss: 3.20721620619\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (15.941901207,24.8066951821), test loss: 29.9608531237\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (3.00761365891,3.28095543286), test loss: 2.64874509871\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (13.3985567093,24.7407696962), test loss: 29.5222126484\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.901638507843,3.26941396536), test loss: 3.23320878446\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (12.8417901993,24.6761774279), test loss: 29.5024900436\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.23129594326,3.25815896151), test loss: 2.26649391651\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (4.90825986862,24.6134195304), test loss: 31.2676642179\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.827035069466,3.24710519643), test loss: 3.09970567226\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (4.52427387238,24.5497707972), test loss: 30.7433154106\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.1891169548,3.23616874545), test loss: 2.54759255722\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (13.0271606445,24.4872165872), test loss: 31.1175506115\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.48070955276,3.22535051951), test loss: 3.05255364478\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (45.2286529541,24.4276323909), test loss: 27.3321855068\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (3.29730367661,3.21462025371), test loss: 2.66866807938\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (48.4272727966,24.3677803041), test loss: 29.8275779963\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.38351345062,3.20396447746), test loss: 2.95192325413\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (25.2415771484,24.3080102333), test loss: 25.5178592205\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.37383174896,3.19360888319), test loss: 3.14248652458\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (11.898639679,24.2484170747), test loss: 30.6232298374\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.531405568123,3.18328909644), test loss: 2.96232746542\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (9.76701164246,24.1907288285), test loss: 28.1327679634\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.18259394169,3.1733397865), test loss: 3.12614738345\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (10.3877449036,24.1345125244), test loss: 30.7936113358\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.65792155266,3.16349239897), test loss: 2.43677361161\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (30.3267745972,24.0780332692), test loss: 32.1131590128\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (4.6881775856,3.15386215329), test loss: 3.2459153235\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (10.5739440918,24.0218931219), test loss: 28.7384898424\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.843709826469,3.14429056115), test loss: 2.45347201079\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (8.9029083252,23.9676461009), test loss: 33.4210796833\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.85363435745,3.13479772475), test loss: 3.23155648708\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (7.80930328369,23.9139166699), test loss: 27.5462986946\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.24964666367,3.12526843262), test loss: 2.6459838286\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (20.7869300842,23.8607609874), test loss: 29.4520143509\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (4.77398490906,3.11605781747), test loss: 2.96765022427\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (14.7836694717,23.8073148415), test loss: 25.598607111\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (3.05283594131,3.1068637982), test loss: 2.99809316695\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (14.4782381058,23.7549681273), test loss: 30.2880579948\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.711134314537,3.09788934816), test loss: 3.00256137252\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (13.6722068787,23.703587362), test loss: 27.1710393906\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.22832274437,3.08912299), test loss: 3.19571185112\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (5.22783756256,23.6535458397), test loss: 31.0940587521\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.783608317375,3.08049649041), test loss: 2.49863945544\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (4.74734115601,23.6025695586), test loss: 29.5439032078\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.56270301342,3.07194458272), test loss: 3.28403231502\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (12.1199140549,23.5525070202), test loss: 29.2297071457\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.59595251083,3.06345812678), test loss: 2.28315795511\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (45.3047943115,23.5046524146), test loss: 33.0514529943\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.015822649,3.05500736192), test loss: 3.17705360651\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (46.077709198,23.4565094195), test loss: 27.5787573814\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.81468439102,3.04661074062), test loss: 2.56962671131\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (26.3044872284,23.4082243607), test loss: 30.3197009325\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.26623177528,3.03843441896), test loss: 2.93156241179\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (11.2665166855,23.3599607352), test loss: 25.8522286892\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.456248402596,3.03025942751), test loss: 2.86160642505\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (10.0639362335,23.31313648), test loss: 30.1210556507\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.24017238617,3.02237239571), test loss: 2.99494634867\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (10.8711566925,23.2674824045), test loss: 26.2825588703\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.348341465,3.01454388516), test loss: 3.19304294288\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (35.6517562866,23.2214695628), test loss: 30.3445022583\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (4.25438976288,3.00688585172), test loss: 2.62084046155\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (5.93213558197,23.1756072642), test loss: 29.1562286377\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.70322906971,2.99926556977), test loss: 3.25875945687\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (9.74584960938,23.1313024467), test loss: 30.6929930687\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.76682376862,2.99167683116), test loss: 2.34873146564\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.45343160629,23.0872946544), test loss: 30.4640644312\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.99137830734,2.98403324119), test loss: 3.04698269069\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (19.861000061,23.0436154298), test loss: 28.6537116051\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (4.06507635117,2.97664564914), test loss: 2.58215018213\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (13.5746355057,22.9995769765), test loss: 32.0117842197\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.82705020905,2.96925319327), test loss: 3.05234471858\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (18.5591926575,22.9564258102), test loss: 24.8915729284\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.824186444283,2.96203480065), test loss: 2.51652922034\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (15.1305208206,22.913937698), test loss: 30.1614948273\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.29184269905,2.95496252533), test loss: 2.96478500366\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.58108043671,22.872562573), test loss: 26.0763516665\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.732506513596,2.94800270733), test loss: 3.14982841015\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.81538391113,22.8301819625), test loss: 30.0551584005\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.95011997223,2.94108672611), test loss: 2.65201486498\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (9.7776889801,22.7886227952), test loss: 29.4423951387\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.68822932243,2.93421173962), test loss: 3.21714461446\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (43.8506355286,22.7487958702), test loss: 29.6610749722\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.33486890793,2.92733793378), test loss: 2.23291336\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (45.8231086731,22.7087568619), test loss: 31.5875466347\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.98417568207,2.9205163746), test loss: 3.09381097555\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (28.0673160553,22.668378456), test loss: 30.5392772436\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (3.16866731644,2.91385575313), test loss: 2.499263978\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (10.0224456787,22.6278972735), test loss: 32.2059307098\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.674538195133,2.90717750946), test loss: 3.04664865732\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (13.1958827972,22.588623538), test loss: 27.4240420341\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.43389928341,2.90073386838), test loss: 2.63527225256\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (10.888212204,22.5503281023), test loss: 30.3870487452\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.01670205593,2.89432463332), test loss: 2.95071895719\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (41.3245773315,22.5116406557), test loss: 26.5642419815\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (3.69354844093,2.88805173825), test loss: 3.13890566826\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.43253421783,22.4730035435), test loss: 31.0381374836\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.672652244568,2.88180764555), test loss: 2.96112868786\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (14.798169136,22.4355986433), test loss: 28.7549787045\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.63296234608,2.87556265756), test loss: 3.14102931917\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (7.63588047028,22.3984347489), test loss: 31.9081264496\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.01701426506,2.86926542505), test loss: 2.39138717204\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (21.2587223053,22.3614744912), test loss: 32.2098732471\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.93152475357,2.86316638241), test loss: 3.25578167737\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (12.6110010147,22.324078084), test loss: 29.3951362133\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.72852659225,2.85706918247), test loss: 2.4095791012\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold4/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (353.173614502,inf), test loss: 206.417950821\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (386.216308594,inf), test loss: 452.973568726\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (47.8889007568,151.948157894), test loss: 77.4493086815\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.54314684868,151.99944799), test loss: 2.88483605683\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (138.179901123,105.665431444), test loss: 43.9477152348\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.37759828568,77.3638367337), test loss: 3.08547133803\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (37.938079834,86.6796313801), test loss: 47.317859745\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.73714983463,52.4706407138), test loss: 3.02927792668\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (35.9398498535,77.0625771537), test loss: 45.8454252243\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.751297295094,40.021587508), test loss: 3.10963425636\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (39.6594161987,71.3152369169), test loss: 44.7275968075\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.650181889534,32.5557651632), test loss: 2.53565470874\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (60.5214233398,67.466276813), test loss: 46.7175339699\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.10504341125,27.5808418239), test loss: 3.1837880373\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (28.9975452423,64.703712157), test loss: 38.4762320518\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.75240910053,24.0307245096), test loss: 2.37304000258\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (32.3059768677,62.6073338553), test loss: 48.0406941891\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.0831515789,21.3651498843), test loss: 3.06806858182\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (82.257270813,60.9632413034), test loss: 38.4106678963\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.16860723495,19.2938391617), test loss: 2.64048900008\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (61.6093559265,59.59791398), test loss: 46.4789424896\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.69953203201,17.6349456611), test loss: 2.77346664667\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (32.1911048889,58.4497201646), test loss: 41.70062747\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.61331009865,16.279218809), test loss: 2.96495652795\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (36.5196762085,57.5026275593), test loss: 44.2181818485\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.84894943237,15.1484399414), test loss: 3.07163507342\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (36.8147659302,56.7202290812), test loss: 43.8822328091\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.82040297985,14.1923490454), test loss: 3.17043510675\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (21.9895896912,56.046641683), test loss: 42.6165325642\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.38955974579,13.3751431552), test loss: 2.50480034947\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (44.6454582214,55.4067898842), test loss: 46.35275774\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.20739030838,12.6705319068), test loss: 3.24249046445\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (28.1324062347,54.8323196687), test loss: 42.6202153444\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.46168518066,12.0560847826), test loss: 2.58128682375\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (23.6354255676,54.3079338132), test loss: 44.1378214836\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.70102119446,11.5134214651), test loss: 3.08799166679\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (47.4177932739,53.8214493248), test loss: 38.2437944889\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.90381526947,11.0319017026), test loss: 2.67021406293\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (49.5074920654,53.361167751), test loss: 47.7677016735\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.53769004345,10.6011067806), test loss: 3.05735586882\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (41.9166870117,52.9564890288), test loss: 35.1727449417\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.11531352997,10.2136524508), test loss: 2.48575630784\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (56.244468689,52.56943245), test loss: 44.0216349602\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (5.39037656784,9.86344092276), test loss: 3.05427001119\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (88.4545898438,52.232123926), test loss: 40.2548521996\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.19418334961,9.54627927803), test loss: 3.02136824131\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (16.5114135742,51.8989404758), test loss: 40.2834233761\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.90043234825,9.25670411481), test loss: 2.56527672112\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (21.1183452606,51.5929665407), test loss: 47.2206672668\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.57705879211,8.99217622476), test loss: 3.1721529901\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (36.6086349487,51.2894567684), test loss: 41.4204848051\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.768721103668,8.74740217753), test loss: 2.5128444165\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (25.5748310089,50.9937830774), test loss: 44.9279593468\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.541798889637,8.52245677216), test loss: 3.16164911985\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (65.9810180664,50.7006745228), test loss: 38.0261890888\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.26184153557,8.31405575774), test loss: 2.6708345294\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (32.5193939209,50.4339131505), test loss: 46.0334364414\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.19363927841,8.12064724817), test loss: 3.09706233144\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (39.0105514526,50.1640323074), test loss: 36.3802552223\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.28361940384,7.94057612056), test loss: 2.63123138845\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (21.8846950531,49.9137233175), test loss: 42.6410605192\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.99239206314,7.77310803149), test loss: 2.97323993444\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (73.7368392944,49.6626095069), test loss: 37.1053919792\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.53179252148,7.61636889257), test loss: 3.07171880603\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (22.848739624,49.4198100458), test loss: 42.5666858673\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.59368014336,7.47026844507), test loss: 3.01596409678\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (52.157333374,49.1735663242), test loss: 41.163707304\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.66076898575,7.33216961628), test loss: 3.16436020732\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (96.681022644,48.9240563686), test loss: 39.4389625549\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.69194936752,7.20221792503), test loss: 2.52702342868\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (153.65423584,48.6740618273), test loss: 42.328423357\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.9037361145,7.07965409413), test loss: 3.25024982095\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (33.4336357117,48.4201103726), test loss: 35.3385642052\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.07124638557,6.96377001645), test loss: 2.46780908108\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (37.3843688965,48.1686240724), test loss: 43.2183932781\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.43665277958,6.853574344), test loss: 3.11675698161\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (22.8342628479,47.9223099486), test loss: 32.8087663651\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.69676923752,6.74975217397), test loss: 2.62167547643\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (44.2964630127,47.6765941212), test loss: 40.9623490334\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.87307739258,6.65124346755), test loss: 2.92006896734\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (43.6328849792,47.4214456818), test loss: 34.7864732265\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.953278541565,6.55763552027), test loss: 2.9324850291\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (26.905752182,47.1639694486), test loss: 38.207409811\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.52292633057,6.46788139964), test loss: 2.97700533271\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (19.0944480896,46.9011965154), test loss: 36.5322799921\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.22770404816,6.38244555679), test loss: 3.18813605905\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (52.9030075073,46.6265959803), test loss: 34.3210236073\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (6.40065050125,6.30034964065), test loss: 2.44509536028\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (18.2377147675,46.3413513484), test loss: 39.3131036282\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.74616622925,6.22141203509), test loss: 3.19426315427\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (40.5653457642,46.0548757856), test loss: 33.7842828989\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.26799035072,6.14529869401), test loss: 2.49343525767\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (13.3274478912,45.7662800968), test loss: 37.3592915058\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.79620242119,6.0721845448), test loss: 3.1878629148\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (26.5180473328,45.4751925731), test loss: 30.2562046051\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.77641665936,6.00202638227), test loss: 2.54912914038\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.1544113159,45.1727903677), test loss: 38.8738394022\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.58526861668,5.93431083974), test loss: 2.97196565866\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (39.0311889648,44.8665716389), test loss: 25.9908447742\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.71587264538,5.86874991104), test loss: 2.46344507337\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (33.0583839417,44.5516616682), test loss: 35.1100899935\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.14095616341,5.80475264855), test loss: 2.80266545415\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (35.3825645447,44.2283283945), test loss: 30.505708003\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.85702443123,5.74289234083), test loss: 2.91845681667\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (22.7762832642,43.8960435686), test loss: 29.3926320314\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.4846830368,5.68253705631), test loss: 2.40422915518\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (45.3988189697,43.5671810831), test loss: 34.2818302155\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.92374539375,5.62392748164), test loss: 3.02273435295\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (6.73173427582,43.2248478171), test loss: 29.8787050962\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.50513744354,5.56678229374), test loss: 2.37557780743\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (13.3910064697,42.8827207811), test loss: 35.1540910959\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.568854928017,5.51130917702), test loss: 3.07687551379\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (11.2355461121,42.5337172758), test loss: 27.0312077045\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.98653590679,5.45725449596), test loss: 2.4449447453\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (40.5413894653,42.1915477208), test loss: 35.4745414495\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.73889350891,5.40483001013), test loss: 2.94966005981\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (22.9294509888,41.8495823809), test loss: 27.1447775841\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.6727771759,5.35321152002), test loss: 2.53995683789\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (39.4705810547,41.5076988192), test loss: 32.7568996668\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (4.89496517181,5.30309431767), test loss: 2.72058989257\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (53.0689163208,41.1679062274), test loss: 29.230919981\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.63109731674,5.25410953179), test loss: 3.05105761886\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.4384346008,40.8364274456), test loss: 33.5386616945\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.92886173725,5.206421617), test loss: 2.94894740283\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.0532131195,40.5113306935), test loss: 30.696837616\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.46600079536,5.15962712409), test loss: 3.04470426738\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (5.60907506943,40.1926206069), test loss: 30.4172758579\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.440950334072,5.11420182814), test loss: 2.48922626972\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (14.9552488327,39.8806448371), test loss: 33.5574007511\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.36878371239,5.06995430783), test loss: 3.14366618395\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (20.9623794556,39.5763480869), test loss: 28.3324581623\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.04726994038,5.02711982869), test loss: 2.29737948552\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (12.2618627548,39.2799666942), test loss: 35.7483844995\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.04256820679,4.98509393222), test loss: 2.97428657413\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (23.5633563995,38.9917754689), test loss: 27.3516052246\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.867077708244,4.94425426494), test loss: 2.58357236832\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (19.5238056183,38.7057605295), test loss: 34.8469981194\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.01300525665,4.90428559049), test loss: 2.8394403249\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (18.8042354584,38.4278575994), test loss: 28.1667459011\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.10830307007,4.86533569296), test loss: 2.87323023379\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (24.7474708557,38.15954071), test loss: 33.2329504728\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (5.75954627991,4.82727982145), test loss: 3.05556951761\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (8.93363380432,37.8966893747), test loss: 30.8211742878\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.84798657894,4.79015262762), test loss: 3.15629319847\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (9.53687095642,37.6411962457), test loss: 28.7423562765\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.55159509182,4.75422310468), test loss: 2.44342588335\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (19.7848358154,37.3908942513), test loss: 34.3585645914\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.79500770569,4.7190973143), test loss: 3.18616656661\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.91176700592,37.1472127188), test loss: 30.3000555515\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.79917621613,4.68499587804), test loss: 2.45439616144\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (10.4046182632,36.9117593063), test loss: 33.4502851963\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.87260878086,4.65165831332), test loss: 2.99247384965\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (26.6112861633,36.6773493678), test loss: 29.8535761833\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.86390829086,4.61898504234), test loss: 2.63305824101\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (13.2486743927,36.4485250673), test loss: 35.3127547741\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.15296363831,4.58702022705), test loss: 2.93221666962\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (35.4375991821,36.2289194411), test loss: 25.7098332405\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.18212509155,4.55583059103), test loss: 2.54361428171\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.91557693481,36.011545462), test loss: 34.3902642965\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.66671895981,4.52523730883), test loss: 3.04243237972\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (41.6793518066,35.8015555465), test loss: 30.0023275971\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.87063264847,4.49556467126), test loss: 3.04404822886\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (8.87205219269,35.5925881834), test loss: 33.4905548811\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.33757889271,4.46646377767), test loss: 2.66765045226\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (11.9066781998,35.3911661863), test loss: 33.2289750218\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.10770344734,4.4383302834), test loss: 3.19209938347\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (8.82836437225,35.1947393872), test loss: 30.9338640213\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.660448074341,4.41059166831), test loss: 2.53331487998\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (21.5210113525,34.9996423446), test loss: 35.0376051903\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.529348254204,4.38351576407), test loss: 3.13442846835\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (24.9495048523,34.8082951514), test loss: 28.5340893984\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.78510522842,4.35691055464), test loss: 2.54028528184\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (8.0565032959,34.623757395), test loss: 35.6321759224\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.37159156799,4.33088321811), test loss: 3.04449468851\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (42.9597816467,34.4425265366), test loss: 27.5077659607\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.42520260811,4.30534799544), test loss: 2.60001287311\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.0145530701,34.264199861), test loss: 33.618816936\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.45889568329,4.28038061495), test loss: 2.84249253869\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (40.9566879272,34.0890416145), test loss: 30.9862305164\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.32744884491,4.25597949631), test loss: 3.27893615067\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (14.9041366577,33.9178072182), test loss: 34.3220225573\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.85861992836,4.23223289997), test loss: 3.11738887131\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (11.4000654221,33.7509108932), test loss: 30.711578393\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.85205507278,4.20890702573), test loss: 3.11957616359\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (47.654964447,33.586513601), test loss: 31.2169472694\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.83642506599,4.1860372513), test loss: 2.59205034971\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (3.55599832535,33.4235076245), test loss: 32.7110618114\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.3562002182,4.16351703221), test loss: 3.24596312046\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (7.37496423721,33.265151999), test loss: 29.02115798\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.62564015388,4.14149740571), test loss: 2.40429879874\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.40282917023,33.1107193691), test loss: 35.2561392546\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.62929534912,4.11966995884), test loss: 3.02713083625\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (22.1252269745,32.9585470044), test loss: 27.6972719193\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (5.3425989151,4.09843684622), test loss: 2.65671124756\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (12.7457647324,32.8089053001), test loss: 34.8145861506\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.98064756393,4.077556498), test loss: 2.96552423686\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (18.8270359039,32.661390609), test loss: 25.499023056\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.08338308334,4.05723571742), test loss: 2.66961407438\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (13.254357338,32.5178208934), test loss: 34.4705725551\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.43232083321,4.03727526789), test loss: 3.00686426461\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (33.2511367798,32.3779453335), test loss: 30.5355190158\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (3.91479825974,4.01777507803), test loss: 3.19991582781\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (16.4657230377,32.2369154722), test loss: 29.269986093\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (3.06680107117,3.99846731791), test loss: 2.45364313126\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (6.58870697021,32.0992359706), test loss: 33.3935139179\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.18017828465,3.97944484162), test loss: 3.1802893579\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.82170772552,31.9657706877), test loss: 29.8560401917\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.79984974861,3.96082540719), test loss: 2.42998302281\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (15.9110116959,31.8337138854), test loss: 33.8245064497\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (3.01563549042,3.94241198378), test loss: 2.99677914381\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (48.061000824,31.7047522121), test loss: 30.5265550613\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.92006266117,3.92449283143), test loss: 2.61831668019\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (12.6626682281,31.5761559471), test loss: 35.5697586536\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.55647945404,3.90684268224), test loss: 2.9216610983\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (34.599571228,31.4515934844), test loss: 26.288397646\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.00273370743,3.8896296875), test loss: 2.57743867934\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (19.3920936584,31.3290448637), test loss: 34.0817956924\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.75852060318,3.87267010113), test loss: 2.96312074363\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (16.0995292664,31.2062811944), test loss: 29.4640159369\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.39809179306,3.85593082169), test loss: 3.0678014636\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (17.9840278625,31.0861371727), test loss: 34.5238854885\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.37636888027,3.83942854078), test loss: 2.96107344925\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (16.5997314453,30.9697147836), test loss: 32.3477588058\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.23179733753,3.82318359865), test loss: 3.15248527974\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (10.5551862717,30.854062128), test loss: 31.5039504528\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (3.45590305328,3.80723413718), test loss: 2.50526144207\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (16.8792324066,30.7407124831), test loss: 35.0285014868\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.36605083942,3.79153332798), test loss: 3.11906470656\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (17.7304973602,30.6270983597), test loss: 29.0382033825\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.5498187542,3.77606776533), test loss: 2.5185958162\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (17.8044281006,30.5175668897), test loss: 36.3965354443\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.70791232586,3.76105150118), test loss: 3.10019250214\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (13.2219057083,30.4096587274), test loss: 27.2579600811\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.06458055973,3.74613375509), test loss: 2.66696467549\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (24.3611373901,30.3020741527), test loss: 33.4088308215\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (3.608543396,3.73154483838), test loss: 2.8437586993\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (24.0042457581,30.1958637359), test loss: 31.006807375\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.55168962479,3.71706856621), test loss: 3.24475822747\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (17.1617393494,30.0924160204), test loss: 34.4186213732\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (6.23022842407,3.70285377977), test loss: 3.11319961697\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (11.6009845734,29.9907839189), test loss: 30.7317099929\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.79406952858,3.68872037399), test loss: 3.11236079037\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (4.30862140656,29.8898488825), test loss: 30.9904768467\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.391556859016,3.67488548229), test loss: 2.59067666233\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (11.6196231842,29.7901399025), test loss: 32.3520806551\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.91983425617,3.66127669423), test loss: 3.23732435852\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (28.7827529907,29.6919866847), test loss: 31.2583644867\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.50876665115,3.64798353349), test loss: 2.49405338764\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (16.1125888824,29.5958707823), test loss: 34.920424521\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.20371246338,3.63485536134), test loss: 3.03368130326\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (24.3567867279,29.5015344308), test loss: 29.7201850414\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.717592060566,3.62195465646), test loss: 2.7353537336\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (16.2846508026,29.4063368429), test loss: 35.5220311403\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.58640766144,3.60912724528), test loss: 2.9644171685\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (21.3880157471,29.3130866309), test loss: 24.7579277992\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.84507346153,3.59651134499), test loss: 2.43101230413\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (27.7724990845,29.2223825615), test loss: 34.6590211153\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (3.89569830894,3.5839782817), test loss: 2.9768167913\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (6.7611041069,29.1321413715), test loss: 30.8275889158\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.80538344383,3.5716693791), test loss: 3.14081649184\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (7.51669168472,29.0432933141), test loss: 29.6246963739\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.61434471607,3.55960459535), test loss: 2.50484884381\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (16.2350597382,28.9551041417), test loss: 34.1578283072\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.05132877827,3.54767685214), test loss: 3.16203240305\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (16.5586090088,28.8687111395), test loss: 30.1006109238\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (3.89133191109,3.53601403479), test loss: 2.42807933241\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (4.91352176666,28.7842900013), test loss: 34.1338556767\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.579770684242,3.52449321145), test loss: 2.9832803607\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (10.9320011139,28.6985565204), test loss: 30.5238004684\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.22076761723,3.51306529787), test loss: 2.6124012202\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (7.26847267151,28.6146183336), test loss: 34.6436362743\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.35904383659,3.50175720917), test loss: 2.89865159094\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (44.5439758301,28.5334591717), test loss: 26.5239226341\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.14465093613,3.49061616366), test loss: 2.56731985062\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (12.5149211884,28.4518941759), test loss: 34.1360872507\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.84971380234,3.47954394734), test loss: 2.90730800927\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (25.6607780457,28.3722805495), test loss: 30.0710448027\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.52341127396,3.46873768111), test loss: 3.06773895621\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (5.16156196594,28.2920176204), test loss: 35.5595341563\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.969954490662,3.45798933747), test loss: 3.04483710527\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (8.33903121948,28.2141796279), test loss: 31.0003468513\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.10690379143,3.44754629179), test loss: 3.1476952821\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (12.4697666168,28.1377736028), test loss: 31.780612278\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.11301267147,3.43716140027), test loss: 2.51222551689\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (17.3893070221,28.0604100782), test loss: 33.4991782665\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.06279301643,3.42689389963), test loss: 3.12729629278\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (13.2876548767,27.9842145278), test loss: 27.9866313934\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.13504767418,3.4166903462), test loss: 2.43736289144\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.01853656769,27.9103083304), test loss: 36.0399179459\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.07964897156,3.4066167355), test loss: 3.0799726069\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (39.696559906,27.8369902864), test loss: 27.4530850887\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.40357136726,3.39663737362), test loss: 2.67304337919\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (15.1237802505,27.7640876076), test loss: 34.0669118404\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.48895764351,3.38678184471), test loss: 2.79946079403\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (23.1533966064,27.691355535), test loss: 31.1048752785\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (3.5949010849,3.37705108932), test loss: 3.1299929902\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (14.1638202667,27.6202344292), test loss: 34.4732482433\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.25548565388,3.36752851746), test loss: 3.10868236721\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (8.35362625122,27.5501982093), test loss: 30.4424176335\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.82325541973,3.35807761861), test loss: 3.13158863485\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (14.8927307129,27.480229601), test loss: 30.785934186\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.76029610634,3.34879812625), test loss: 2.54921046793\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.43506765366,27.4110104056), test loss: 32.3001422405\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.841486990452,3.33954537939), test loss: 3.21084403545\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (11.5358839035,27.3429192689), test loss: 31.7138103962\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.91973042488,3.3304393496), test loss: 2.52164627239\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (8.18856048584,27.2760488705), test loss: 33.5881759167\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (4.43309593201,3.32130209106), test loss: 2.95312964767\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (21.0754528046,27.2094979888), test loss: 28.7378487587\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (4.49231481552,3.3123609269), test loss: 2.69555189908\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (21.5310401917,27.1435789188), test loss: 35.686178267\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.11092376709,3.30350631495), test loss: 3.01970637143\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (16.3211307526,27.0778450872), test loss: 25.4256010056\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.33885931969,3.29482085719), test loss: 2.41492175311\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.73283195496,27.0135734493), test loss: 34.3281087399\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.88229608536,3.28625646059), test loss: 2.9187742427\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (29.9383335114,26.950700831), test loss: 29.6473084688\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (4.10739278793,3.27784028298), test loss: 3.13317724168\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (8.70714950562,26.8863251441), test loss: 29.9590025663\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.42350745201,3.26940286296), test loss: 2.54617921412\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold5/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (304.201599121,inf), test loss: 161.875683594\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (299.430419922,inf), test loss: 366.476121521\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (65.1860961914,66.2485761204), test loss: 45.6069519997\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (7.07734060287,97.6090625184), test loss: 3.77318983674\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (36.7980804443,55.0684454303), test loss: 38.2348508358\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (6.05239582062,50.87674329), test loss: 3.70492142439\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (44.4203796387,51.4468773972), test loss: 43.7174012184\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.97657966614,35.298575737), test loss: 3.76346098781\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.4208240509,49.5974501233), test loss: 41.760558033\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.73127663136,27.4989336722), test loss: 4.00007035732\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (13.9026041031,48.5037167908), test loss: 43.5300528049\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.849098742008,22.8204394269), test loss: 3.13495157361\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (17.5160675049,47.7603000037), test loss: 42.9811536789\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (4.5698800087,19.708241452), test loss: 4.03991631269\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (28.9719696045,47.2114925167), test loss: 41.127545929\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.89983701706,17.4847445518), test loss: 3.37707589269\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (51.3319244385,46.8004115331), test loss: 44.5239081383\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.961760342121,15.8125453067), test loss: 3.89354203343\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (98.03515625,46.4714818807), test loss: 37.5656688213\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.05704450607,14.5159216725), test loss: 3.3929191947\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (73.1412506104,46.1744041846), test loss: 46.2303291321\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.66687202454,13.4747030199), test loss: 3.70277248621\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (40.7369270325,45.9247788684), test loss: 34.3258015633\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (5.20150470734,12.6261425317), test loss: 3.08051602244\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (11.819814682,45.7261710935), test loss: 45.4585255623\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.814279139042,11.9134094374), test loss: 3.8588755846\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (45.3321990967,45.5743477921), test loss: 38.6922225952\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.87035989761,11.3135031868), test loss: 3.8067001462\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (19.9580612183,45.4410430693), test loss: 41.2616092443\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.9076166153,10.800843005), test loss: 3.33236209154\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (31.5890331268,45.2983423674), test loss: 44.629587841\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.858679413795,10.3549771214), test loss: 3.90019822717\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (38.1514663696,45.182687542), test loss: 42.8943865299\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (12.6450681686,9.96501341596), test loss: 3.31283314824\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (64.4516067505,45.0842891323), test loss: 41.6444924355\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.43137741089,9.62085064786), test loss: 3.92921792269\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.8782348633,44.9657866836), test loss: 38.0967115402\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.58017587662,9.31377295548), test loss: 3.34935879707\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (48.6078681946,44.8564621161), test loss: 45.2043158531\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.30353236198,9.03949476986), test loss: 3.83624880314\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (40.1276512146,44.7673486055), test loss: 36.7694775343\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.49227809906,8.79117680537), test loss: 3.11279885769\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (31.0582237244,44.687308796), test loss: 44.3004609585\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.6639983654,8.56641838381), test loss: 3.93485623598\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (20.9483337402,44.6169575189), test loss: 36.9908355236\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.02167034149,8.36363052893), test loss: 3.69561452866\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (92.1000976562,44.5456158764), test loss: 45.4685543537\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.55580496788,8.1784710049), test loss: 3.74865353107\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (58.8284912109,44.4695526667), test loss: 41.6354897499\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (6.28031539917,8.00769162009), test loss: 3.71849178076\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (62.9559288025,44.4055804918), test loss: 42.3806610107\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.49206709862,7.84986391357), test loss: 3.2749525249\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (40.9209594727,44.3260131976), test loss: 42.7142611504\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.87156176567,7.70387718474), test loss: 3.92226687074\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (37.4769973755,44.2475415135), test loss: 38.2782473087\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.97152662277,7.568708891), test loss: 3.3555441916\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (55.5285453796,44.1887650741), test loss: 43.2707592964\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.34202861786,7.44289661794), test loss: 3.83342562914\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (11.8287582397,44.1164329624), test loss: 36.259978056\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (4.98224449158,7.32481382024), test loss: 3.12542995214\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (16.8773155212,44.0658990445), test loss: 44.5187853336\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.5857925415,7.21492378614), test loss: 3.8316627264\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (32.8913040161,43.999940124), test loss: 37.9833711147\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.38449513912,7.11241905875), test loss: 3.35693204999\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (101.90738678,43.9391728363), test loss: 42.6281270981\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (6.85786914825,7.0158336431), test loss: 3.67917035222\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (62.2340164185,43.8774180685), test loss: 39.3566789627\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (9.27617454529,6.92409728866), test loss: 3.96556692123\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (64.556060791,43.8059379966), test loss: 41.0326680422\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (6.85731649399,6.83731590129), test loss: 3.23737475276\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (34.7394180298,43.7336593166), test loss: 42.5705845833\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (5.55740785599,6.75542071597), test loss: 3.91544702649\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (41.0036239624,43.6716698698), test loss: 41.1267403603\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (5.09359312057,6.67787433738), test loss: 3.20797029138\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (20.3586807251,43.6061310548), test loss: 41.1048000336\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.58618831635,6.60304230436), test loss: 3.82064138055\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (12.5822000504,43.5425680126), test loss: 35.9031470299\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.7894769907,6.53202882621), test loss: 3.07841088176\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (16.4759788513,43.4767842639), test loss: 44.4452582359\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.34536075592,6.46511976099), test loss: 3.71915646791\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (27.2182693481,43.4082366678), test loss: 32.3768836021\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.63067865372,6.40096566699), test loss: 2.88651736975\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (46.6683654785,43.3395077155), test loss: 42.7356347084\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.927078783512,6.33865857903), test loss: 3.55603482127\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (92.1374664307,43.2681937604), test loss: 36.1363396168\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.53795337677,6.27963951749), test loss: 3.67742802501\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (65.8218231201,43.1881466425), test loss: 38.110002327\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.59807395935,6.2219762518), test loss: 3.08345112205\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (37.4165496826,43.1066159354), test loss: 41.9416215897\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.85877227783,6.16712609255), test loss: 3.65594286919\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (9.72739601135,43.0257263921), test loss: 40.1837335825\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.733167886734,6.11269465725), test loss: 2.97024846673\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (42.5555763245,42.9466946363), test loss: 40.7692109585\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.47727966309,6.0609749135), test loss: 3.85044581294\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (19.2584953308,42.8645291652), test loss: 35.2438148975\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.77546715736,6.01119372258), test loss: 2.8808920145\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (30.2018489838,42.7728200822), test loss: 41.7033008099\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.866513609886,5.96236546259), test loss: 3.62113082409\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (31.7765274048,42.681677997), test loss: 33.4448439121\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (10.9714431763,5.91490951667), test loss: 2.91731533408\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (53.0871887207,42.5891594198), test loss: 40.9435947418\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.6143643856,5.86868722106), test loss: 3.37636131048\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (29.1636695862,42.4836205493), test loss: 33.7285053015\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.92144441605,5.82315595155), test loss: 3.2605558157\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (39.4608306885,42.3745593819), test loss: 40.2850146294\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.92572569847,5.77873927924), test loss: 3.32712684274\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (29.3283157349,42.2660101773), test loss: 36.038593626\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.71155834198,5.73472924374), test loss: 3.47230706811\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (25.9245147705,42.1542235911), test loss: 37.9645571709\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.23999166489,5.69151505152), test loss: 2.71344403625\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.8760089874,42.0392197027), test loss: 38.158882618\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.6958322525,5.64958700002), test loss: 3.59747212529\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (74.2345428467,41.9164160373), test loss: 32.7765909672\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.62590694427,5.60815766862), test loss: 2.71264637411\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (41.9553642273,41.7866795149), test loss: 38.7235791206\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (4.63453006744,5.5670749573), test loss: 3.41901687384\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (52.5368385315,41.6542145344), test loss: 30.6675848484\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.82474637032,5.52631077503), test loss: 2.71351422071\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (36.7098312378,41.5088166474), test loss: 38.7886794329\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.49611520767,5.4858513824), test loss: 3.1622790277\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (29.0955505371,41.3570973262), test loss: 31.9466160297\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.80920791626,5.44584648819), test loss: 2.94427850246\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (38.9468040466,41.205935622), test loss: 36.4115928411\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.17867279053,5.40609101985), test loss: 3.18153194785\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (8.93071842194,41.0432480415), test loss: 31.8163722038\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.80021810532,5.36641468398), test loss: 3.26888766289\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.3954811096,40.880640934), test loss: 32.2301496983\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.936234176159,5.32719042314), test loss: 2.55317483246\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (23.9278488159,40.7046592782), test loss: 34.7686960697\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.84466004372,5.28837788096), test loss: 3.34533191323\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (74.7218017578,40.5241885651), test loss: 32.4623391628\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (4.29876232147,5.24990326086), test loss: 2.37525799572\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (35.7485351562,40.3339977412), test loss: 32.3863479137\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (5.28899002075,5.21150525077), test loss: 3.18189789653\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (55.8262443542,40.1334363149), test loss: 28.3667344093\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (5.33832073212,5.17332527529), test loss: 2.41415148377\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (22.4696388245,39.9239222837), test loss: 35.0394694567\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.42526102066,5.13542925623), test loss: 3.03862202764\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (20.409942627,39.713490108), test loss: 24.5983688831\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.92323732376,5.09794978377), test loss: 2.28023785949\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.6472015381,39.4978504989), test loss: 33.4826832771\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.820742428303,5.06020785838), test loss: 3.10437923074\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (5.24740886688,39.2783180247), test loss: 26.8080194235\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.303532332182,5.02303628079), test loss: 3.02533093095\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (10.3773155212,39.0544997984), test loss: 29.4734690189\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.45426774025,4.9864440325), test loss: 2.55966866314\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (19.7098045349,38.8287649867), test loss: 32.1709866643\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.6737883091,4.95027462277), test loss: 3.19344397485\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (18.2057151794,38.6029394903), test loss: 29.9731679916\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.483718931675,4.91451840063), test loss: 2.39247472882\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (55.9397506714,38.3767867881), test loss: 32.088478756\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.46130371094,4.87950041795), test loss: 3.15017127395\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (23.4945964813,38.1474214359), test loss: 28.8760853291\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.42713427544,4.84470949514), test loss: 2.45622355342\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (21.9546203613,37.9215219913), test loss: 32.6068039894\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.26170253754,4.81079258316), test loss: 3.23958700299\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (6.3857421875,37.6993418074), test loss: 26.2735330582\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.02210509777,4.77698249912), test loss: 2.58955820054\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (21.0305595398,37.480136937), test loss: 32.9007047772\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.885169863701,4.74409054793), test loss: 3.17022124529\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (16.9573669434,37.2622772203), test loss: 28.3593800783\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.72793972492,4.71201204277), test loss: 3.20774041116\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (28.1867713928,37.0463765386), test loss: 33.5499970913\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.32323157787,4.68046832276), test loss: 3.24209563434\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (11.8606433868,36.8348694225), test loss: 29.0825130939\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (4.28502416611,4.64981527364), test loss: 3.36835798025\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (10.6999053955,36.6283635994), test loss: 31.5050351381\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.713976681232,4.61986566524), test loss: 2.55570879877\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (15.6489286423,36.4211627813), test loss: 31.9071835995\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.750358581543,4.59033635722), test loss: 3.36925368905\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (15.716085434,36.2188886596), test loss: 29.5344985962\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.2194532156,4.56152745346), test loss: 2.46785328984\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (6.72096633911,36.0226829434), test loss: 33.724125576\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.08501815796,4.53318613615), test loss: 3.40891418457\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (22.8301696777,35.8301345652), test loss: 27.8392724752\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.714046120644,4.50519449658), test loss: 2.72544143796\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (12.5021514893,35.6408207884), test loss: 34.2779978275\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.27343165874,4.47815987664), test loss: 3.36681754589\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (32.6920013428,35.4523048083), test loss: 25.7281996727\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.38836693764,4.4514145989), test loss: 2.87667361796\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (12.5823345184,35.2693873454), test loss: 34.8852760315\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.15819144249,4.42550808364), test loss: 3.36915675402\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (28.328868866,35.0907129671), test loss: 29.2590733051\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.614354848862,4.40014639297), test loss: 3.32254689336\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (29.3940219879,34.9124013593), test loss: 30.4044819355\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.954782426357,4.37507650813), test loss: 2.74530603886\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (18.8064956665,34.7382586216), test loss: 31.6424731493\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.95489025116,4.35060098665), test loss: 3.39915254116\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (14.7417449951,34.5698168557), test loss: 31.352773571\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.10238337517,4.32651728299), test loss: 2.47275694907\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (10.5561962128,34.403175393), test loss: 31.9853981495\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.36391615868,4.30276178514), test loss: 3.30801243186\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (19.2974853516,34.2405192307), test loss: 29.4264019489\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.875448465347,4.27949763121), test loss: 2.68714469969\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (25.87940979,34.0777199499), test loss: 34.6453703642\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.53975152969,4.25667936165), test loss: 3.31355502605\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (42.967540741,33.9206386139), test loss: 27.306447053\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.36977672577,4.23450496351), test loss: 2.68274344206\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (17.0826339722,33.7662966586), test loss: 34.7678785324\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.44393610954,4.2127510855), test loss: 3.30701322258\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (52.0469932556,33.6133235149), test loss: 29.649206543\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (4.12387657166,4.19134965416), test loss: 3.32116754949\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (19.712594986,33.4624387477), test loss: 36.2306334972\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (4.04043531418,4.17030454363), test loss: 3.2943244338\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (8.29752349854,33.3165676943), test loss: 32.0216155529\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.83897793293,4.14969123193), test loss: 3.35391844511\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (7.41025972366,33.1734041068), test loss: 32.7117687225\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.664566099644,4.12907622301), test loss: 2.58885420561\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (5.51259326935,33.0322699345), test loss: 33.5284861922\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.481962144375,4.10902386966), test loss: 3.32064300179\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (9.83319282532,32.8922306627), test loss: 29.8346460342\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (3.47690725327,4.08934720917), test loss: 2.5203009665\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (21.8776016235,32.754813647), test loss: 34.6057375908\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.5719742775,4.07006184608), test loss: 3.35525326133\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (11.9151287079,32.6208532566), test loss: 28.859933424\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.567503094673,4.05117487637), test loss: 2.7208517611\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (47.4021949768,32.4893643513), test loss: 34.4699818134\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.37838983536,4.0326933778), test loss: 3.24238861203\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (13.5389099121,32.3574893799), test loss: 32.3334144115\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (3.05232286453,4.01434206002), test loss: 3.17580382228\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (19.4007720947,32.229497422), test loss: 35.6426120281\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.83149576187,3.99642459561), test loss: 3.34272341132\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (7.0625667572,32.1045972684), test loss: 30.5376417875\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.04225826263,3.97844827967), test loss: 3.38328980505\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (14.0741205215,31.9817144742), test loss: 32.5781308174\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.690527141094,3.9609295995), test loss: 2.66128500104\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (16.941034317,31.8595938897), test loss: 32.5976690769\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.74412512779,3.94375869794), test loss: 3.36228886247\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (26.2781772614,31.7389809056), test loss: 32.4314743638\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.28722345829,3.92678055053), test loss: 2.48436559439\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (11.6636867523,31.6211015309), test loss: 33.7629039288\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (3.64986634254,3.91024561833), test loss: 3.27509213686\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (7.6672129631,31.5062786633), test loss: 30.7537045956\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.577819347382,3.89400161388), test loss: 2.70566122532\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (15.6087226868,31.3900976636), test loss: 36.3440615654\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.7105935812,3.8778477707), test loss: 3.29175627828\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (15.2831802368,31.2769327564), test loss: 26.6898523808\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.15043199062,3.86198690453), test loss: 2.50045734346\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (6.8279042244,31.1669521463), test loss: 35.1363747597\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.93088316917,3.84625843717), test loss: 3.23935953081\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (23.2454528809,31.0585782292), test loss: 29.0968028545\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.753685712814,3.83061309263), test loss: 3.19713282883\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.4776039124,30.9513722975), test loss: 30.7767445803\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.23720347881,3.81544313009), test loss: 2.73633388281\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (29.5558414459,30.8437858951), test loss: 32.6673932076\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.37187671661,3.80032387254), test loss: 3.31451960802\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (11.9873418808,30.7394835455), test loss: 31.7578883171\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.17297172546,3.78563978263), test loss: 2.36472878456\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (25.0707073212,30.6372260748), test loss: 32.6851471663\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.622230887413,3.77120040646), test loss: 3.18250966072\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (30.8163833618,30.5343504201), test loss: 33.6821910858\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.984093368053,3.75683441653), test loss: 2.64275529981\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (17.9818000793,30.4337580115), test loss: 34.0875946999\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.5812754631,3.74274828293), test loss: 3.18843997121\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (14.397652626,30.3360850857), test loss: 28.0230294228\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.99225246906,3.72878464562), test loss: 2.61872057617\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (9.9786863327,30.2389431363), test loss: 34.9279052496\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.80551147461,3.71493853754), test loss: 3.214752689\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (19.8465194702,30.1436088532), test loss: 28.8473700285\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.907796859741,3.7013086537), test loss: 3.06820306182\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (25.9245204926,30.0473068164), test loss: 36.2394304037\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.16346502304,3.68785602835), test loss: 3.27687133551\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (38.6278343201,29.9543850686), test loss: 30.2368371964\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.18098640442,3.67474555386), test loss: 3.31977157295\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (16.7654151917,29.8626617969), test loss: 33.7202174187\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.25215673447,3.66181449954), test loss: 2.59744100571\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (50.2691802979,29.7710967344), test loss: 33.6088690758\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (3.91338086128,3.64903881898), test loss: 3.28439973593\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (19.2549381256,29.680421676), test loss: 30.7773979902\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.71615815163,3.63641420454), test loss: 2.45489482284\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (7.36567735672,29.5924906739), test loss: 34.5906173468\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.90432357788,3.62399578605), test loss: 3.3107357502\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (6.75117683411,29.5058266363), test loss: 28.8146359444\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.618446588516,3.61147035164), test loss: 2.62238927186\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (5.72676563263,29.4199062659), test loss: 34.0765261889\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.510319232941,3.59926725583), test loss: 3.14690693617\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (10.2137355804,29.3339895619), test loss: 30.8886604786\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (3.35837340355,3.58722265241), test loss: 3.09744543135\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (21.0748176575,29.2493410709), test loss: 35.7090816617\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.38223838806,3.57537615249), test loss: 3.28043220639\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (11.2561388016,29.1666309931), test loss: 29.7952565193\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.568815469742,3.56372300711), test loss: 3.31787997186\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (46.720489502,29.0850847796), test loss: 33.199568224\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.3848490715,3.55228940626), test loss: 2.81537230015\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (12.385723114,29.0026204574), test loss: 31.3456820726\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (3.02427482605,3.54087756176), test loss: 3.35394709408\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (19.3748168945,28.9225630866), test loss: 33.4036144733\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.77317643166,3.52969659562), test loss: 2.46193697453\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (7.15813875198,28.8441074356), test loss: 32.1206933975\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.985837638378,3.51838687435), test loss: 3.17440742254\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (12.696559906,28.7665766939), test loss: 29.4867249966\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.602849245071,3.50735986986), test loss: 2.60394282639\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (17.2228775024,28.6890088394), test loss: 35.1583364487\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.70042395592,3.49650197117), test loss: 3.2056863308\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (23.2168903351,28.6120025629), test loss: 26.172903204\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.10813677311,3.4857122811), test loss: 2.4269274354\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (11.8367671967,28.5366588854), test loss: 35.23067801\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (3.55341529846,3.47518144933), test loss: 3.22962005734\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (7.72558784485,28.4630648953), test loss: 29.444143033\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.570045650005,3.46480941443), test loss: 3.18217084706\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (15.2139730453,28.3879178802), test loss: 31.8700844288\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.70606648922,3.45446060783), test loss: 2.72064583004\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (14.4852209091,28.3148072125), test loss: 31.632735467\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.17089521885,3.44428322727), test loss: 3.24815918803\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (6.8733754158,28.2435362739), test loss: 32.175302124\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.85636913776,3.43414508027), test loss: 2.38628322184\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (23.5144939423,28.1730308996), test loss: 32.9707435608\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.749841988087,3.42401797438), test loss: 3.15736064017\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (12.8708410263,28.1029342372), test loss: 31.4244421482\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.22721600533,3.41418076143), test loss: 2.54839820862\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (28.0093898773,28.0320950263), test loss: 34.8672021151\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.35417795181,3.40432748817), test loss: 3.2489528954\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (10.7522373199,27.9634216728), test loss: 29.7482204437\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.13076114655,3.39473962986), test loss: 2.67727546096\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (23.5469512939,27.8959007247), test loss: 34.5332003474\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.626960635185,3.38528927847), test loss: 3.13208062649\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (31.1386947632,27.8274832421), test loss: 31.2414999962\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.0026050806,3.37584681722), test loss: 3.17507169545\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (16.631187439,27.7605841003), test loss: 36.0935748816\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.2897977829,3.36656619659), test loss: 3.24263421595\n",
      "run time for single CV loop: 7068.35223389\n",
      "\n",
      "MC # 2, Hype # hyp5, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold1/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (372.580444336,inf), test loss: 212.463696289\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (272.157623291,inf), test loss: 333.577511597\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (49.3959197998,94.3059145346), test loss: 43.1923662663\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.77330470085,69.6125007408), test loss: 3.1776374042\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (50.6453857422,70.8144000335), test loss: 35.6979866982\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.68154907227,36.3902728916), test loss: 2.99546362758\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (71.5373077393,62.9589779186), test loss: 42.3193995953\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.730946064,25.3039136555), test loss: 3.37020010352\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (35.4897537231,59.1247357481), test loss: 39.5481573105\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.778846204281,19.7667633156), test loss: 3.27143836915\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (25.3087711334,56.7577216942), test loss: 41.963455677\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.39361786842,16.450119219), test loss: 3.28208975792\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (59.0084495544,55.093639114), test loss: 41.5009637833\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.11570322514,14.2336424875), test loss: 3.71342980862\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (43.6911773682,53.8521647404), test loss: 39.5623440742\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.93416643143,12.650500917), test loss: 2.95589077473\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (13.6186103821,52.892341926), test loss: 42.2326936245\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.07464838028,11.4626190633), test loss: 3.42089707255\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (37.2864990234,52.1099992122), test loss: 41.5525981903\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.43386852741,10.5339153393), test loss: 2.84717923999\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (56.8066558838,51.5445036628), test loss: 41.9293161392\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.35567760468,9.79591058047), test loss: 3.65292721987\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (37.8947143555,51.0188227465), test loss: 41.0420331955\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.22320449352,9.19135779972), test loss: 2.75681836605\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (33.0592308044,50.5604381364), test loss: 41.7279858589\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.24188137054,8.68556030833), test loss: 3.3816134572\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (30.3459453583,50.1099275893), test loss: 41.056605196\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.17369675636,8.25591107782), test loss: 3.13710771799\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (67.3171386719,49.7498489192), test loss: 42.687980938\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.00335025787,7.88665791445), test loss: 3.35776362121\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (27.8571643829,49.3983912793), test loss: 39.0448055267\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.19122052193,7.56479857599), test loss: 2.97017776072\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (72.8717041016,49.1010201377), test loss: 42.7350651264\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.46632313728,7.28444278036), test loss: 3.2026317358\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (50.0716094971,48.8053435198), test loss: 36.4323255062\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.3206911087,7.03741127698), test loss: 2.97433632314\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (109.272628784,48.5199459132), test loss: 40.2956967831\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (6.75801324844,6.81580275351), test loss: 3.02546780705\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (16.1698436737,48.2059754546), test loss: 32.8356406212\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.28678655624,6.61603573452), test loss: 2.94723923206\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (58.3214111328,47.9388148328), test loss: 39.7337165833\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.89531517029,6.43559232986), test loss: 3.18820809722\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (36.3043518066,47.6857899298), test loss: 37.0441697598\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.67000436783,6.27068459403), test loss: 3.29728509188\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (27.8691291809,47.4307353018), test loss: 40.8699375868\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.39671587944,6.12226587982), test loss: 3.02662614882\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (27.8886108398,47.171825398), test loss: 39.9071102619\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.77696740627,5.98518518487), test loss: 3.44992296398\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (35.4133110046,46.891999475), test loss: 36.4580741405\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.71147489548,5.85848136987), test loss: 2.73792843223\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (33.2104454041,46.6093754588), test loss: 41.2597909927\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.00063705444,5.74151877103), test loss: 3.41113993227\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (44.2628860474,46.3329213087), test loss: 36.7458684206\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.27992868423,5.63142047314), test loss: 2.61906264126\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (23.1876678467,46.064683655), test loss: 38.556594038\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.10033559799,5.52928892015), test loss: 3.40775114298\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (42.8491859436,45.779122314), test loss: 34.9519961834\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.28805494308,5.43463788896), test loss: 2.62736522257\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (46.5591201782,45.4723255376), test loss: 36.4911342144\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (5.1182308197,5.34530673349), test loss: 3.13875016272\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (40.9483604431,45.1055406615), test loss: 33.877504015\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.68195295334,5.26095319855), test loss: 2.8844227612\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (16.9347953796,44.723358893), test loss: 37.2278325081\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.13822770119,5.18083559467), test loss: 3.13590065241\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (37.9497451782,44.3416175724), test loss: 30.4914852858\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.43457984924,5.10379455414), test loss: 2.77782562375\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (26.1343727112,43.9521647962), test loss: 34.7572405815\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.36152029037,5.03118508346), test loss: 3.03351152539\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (21.1732635498,43.5512368124), test loss: 25.8459077835\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.2348818779,4.9622460693), test loss: 2.60850501657\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (29.2030448914,43.1378801257), test loss: 32.745959568\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.693543434143,4.89574125276), test loss: 3.01662333012\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (21.2408390045,42.7094714529), test loss: 30.9048015594\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.98356842995,4.83244235834), test loss: 3.02542913854\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (13.4620780945,42.2699626982), test loss: 32.0598651171\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.14702224731,4.77150657651), test loss: 2.97700712085\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (18.2893333435,41.8277796715), test loss: 30.1189816475\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (5.63145828247,4.71210147489), test loss: 3.0914450258\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (21.1768016815,41.3819347313), test loss: 26.874266386\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.70465970039,4.65575843477), test loss: 2.70680040121\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (23.6642837524,40.9314374138), test loss: 30.0844893456\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.23616695404,4.60160863248), test loss: 3.10976708978\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (17.7732181549,40.4795658218), test loss: 28.2203520298\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.17134189606,4.54940454615), test loss: 2.57397123724\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (29.2663173676,40.0263858041), test loss: 31.2154974461\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.40791285038,4.4987416517), test loss: 3.33621668816\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (7.99618530273,39.582639307), test loss: 27.7268464088\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.2940158844,4.44981163992), test loss: 2.48376871347\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (15.0778617859,39.1440743495), test loss: 31.7580127478\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.81726551056,4.40213766243), test loss: 3.11872245073\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (19.1916828156,38.7156752131), test loss: 27.8948982716\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.59753739834,4.35655877191), test loss: 2.67207756341\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (12.1510629654,38.2962137466), test loss: 32.9811738968\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.72419345379,4.31284011832), test loss: 3.08747130632\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (10.4515275955,37.8854435584), test loss: 29.2728686571\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.13055539131,4.27042460595), test loss: 2.82650483251\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (16.3418979645,37.4744191419), test loss: 31.600004673\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.7431012392,4.22911819643), test loss: 3.00012921691\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (45.7710494995,37.0817138964), test loss: 27.0854946852\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.39822101593,4.18918828034), test loss: 2.88008222282\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (20.505235672,36.6936099185), test loss: 28.8295583963\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (7.56156873703,4.15003787941), test loss: 2.82953144312\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (17.8420276642,36.3132064234), test loss: 23.8818470478\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.73238039017,4.11235559041), test loss: 2.64293997884\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (13.5085821152,35.9445502977), test loss: 29.9817578793\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.64286422729,4.07620329244), test loss: 2.91199232638\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.7631044388,35.5833070787), test loss: 28.8811452627\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.59304904938,4.04103402566), test loss: 2.99695058465\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (10.5047616959,35.2268087065), test loss: 30.4781102657\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.811606824398,4.0065517018), test loss: 2.8912558943\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (16.9331378937,34.8827442549), test loss: 27.5629321098\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (5.26458644867,3.97306357427), test loss: 3.06044911444\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (27.0776252747,34.5455087646), test loss: 26.4506524563\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.40340280533,3.94029023771), test loss: 2.63680802286\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (5.75592374802,34.2129746789), test loss: 31.6854906559\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.561824560165,3.90865497985), test loss: 3.15172857344\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (5.96909999847,33.8904080192), test loss: 29.6652123928\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.85768699646,3.87805508874), test loss: 2.53562268019\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (17.3918037415,33.5731578571), test loss: 29.8245975018\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.48292273283,3.84827415833), test loss: 3.07827690244\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (8.49401855469,33.261913216), test loss: 29.8294178486\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.8194270134,3.81911025163), test loss: 2.53113456517\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (8.19297122955,32.9603781596), test loss: 30.508967936\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.19727063179,3.79028942128), test loss: 3.00492055118\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (10.8165302277,32.6622862233), test loss: 31.7595362663\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.12735629082,3.76236197383), test loss: 2.71982537359\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (10.7136669159,32.3695944442), test loss: 33.7401348352\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.42834937572,3.73520424017), test loss: 3.02899475694\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (13.4154968262,32.0850056037), test loss: 28.5505226612\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.18429374695,3.70878910347), test loss: 2.89203805923\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (19.7969970703,31.8038027453), test loss: 30.7019057393\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.8787651062,3.68312439566), test loss: 2.79885493219\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (8.90786552429,31.5288718877), test loss: 26.0087263584\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.22389268875,3.65782719196), test loss: 2.73003665805\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (31.3495635986,31.2608942796), test loss: 30.0114076614\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.41316127777,3.63272829372), test loss: 2.72892839164\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (37.4442138672,30.9951348537), test loss: 29.5479979992\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.59226942062,3.60840708486), test loss: 2.93260324299\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (19.4523200989,30.7347596796), test loss: 32.2893654823\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.60420763493,3.58474138464), test loss: 2.86754990518\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (14.8716220856,30.4801717879), test loss: 28.8519160986\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.815181493759,3.56161665587), test loss: 3.10249831378\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.9266386032,30.228173292), test loss: 31.811663866\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.89802575111,3.53900448515), test loss: 2.85242053717\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (12.3011665344,29.9828109136), test loss: 28.7277224064\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (4.90677642822,3.51673968931), test loss: 3.03924611658\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (8.73303794861,29.7407139356), test loss: 29.6319282532\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.79164123535,3.49456684893), test loss: 2.61073697507\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (15.1805953979,29.5020990277), test loss: 30.1306261539\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.42889475822,3.47312614009), test loss: 3.09099183679\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (15.5745859146,29.2679053255), test loss: 31.1413516998\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.30708265305,3.45218041702), test loss: 2.40015647709\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (7.08915281296,29.0393654687), test loss: 32.6237095356\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.684824585915,3.43173669431), test loss: 2.99830794036\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (2.42323827744,28.8119747848), test loss: 30.7778523445\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.960024297237,3.41159103113), test loss: 2.60696060359\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (14.9709300995,28.5913413415), test loss: 32.7424728632\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.6356856823,3.39165483641), test loss: 2.91759490371\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (4.04346179962,28.3721197891), test loss: 31.9235200882\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.24887120724,3.37202812258), test loss: 2.75784172714\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (7.95933628082,28.1562227433), test loss: 32.1624795198\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (3.18372106552,3.35285362582), test loss: 2.85401422381\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (16.1649169922,27.9448705667), test loss: 29.780354166\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.49554514885,3.33415601529), test loss: 2.77934383452\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (27.3756504059,27.7375463468), test loss: 31.6816603661\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.2597489357,3.31586400425), test loss: 2.67858688831\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (14.1759529114,27.5321761339), test loss: 26.2129814148\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.5465464592,3.29778205933), test loss: 2.6553018108\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (7.65542840958,27.331501699), test loss: 30.4275504827\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.579737305641,3.2798915746), test loss: 2.74477633238\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (10.872045517,27.1331059089), test loss: 29.1798062801\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.77899885178,3.26216317376), test loss: 2.80902013779\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (6.21411848068,26.93648237), test loss: 31.6440703869\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.8913449049,3.24493240951), test loss: 2.80542708337\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (10.909040451,26.7444821927), test loss: 28.1556553364\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.21860647202,3.2280776607), test loss: 2.85596054494\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (19.4850997925,26.5552310924), test loss: 30.5721514225\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.50312852859,3.2115542021), test loss: 2.76531918496\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (10.0292768478,26.369376041), test loss: 28.9692791939\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.08223116398,3.19524432785), test loss: 2.91061461121\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (3.67357683182,26.186501896), test loss: 33.2532944202\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.3122344017,3.17898904563), test loss: 2.6042414844\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.4903020859,26.0054889853), test loss: 29.955275631\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.23381185532,3.16305180086), test loss: 3.02578880191\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (13.1720848083,25.8265298906), test loss: 34.2800720692\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.54282164574,3.14744840146), test loss: 2.48967851102\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (7.71794223785,25.65093278), test loss: 33.257651329\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.00878977776,3.132174693), test loss: 2.91595537066\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (18.8222312927,25.4778657498), test loss: 35.7834141254\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.79200732708,3.11715090212), test loss: 2.84321020544\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (12.0059928894,25.3082118398), test loss: 34.8695507526\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.03018593788,3.10230756439), test loss: 3.00933217108\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (4.11690044403,25.1410892683), test loss: 31.1240711689\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.68231475353,3.08749925961), test loss: 2.80159929842\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (15.2868652344,24.9747180341), test loss: 32.810008812\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.91467642784,3.07302893843), test loss: 2.64168683887\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (12.2169618607,24.8109606394), test loss: 30.1909674644\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.23146152496,3.05880475556), test loss: 2.70873105228\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (4.82645177841,24.649712863), test loss: 32.3610244751\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.346112221479,3.04482825408), test loss: 2.70900202468\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (6.34496879578,24.4905842217), test loss: 29.4337634087\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.31531047821,3.03117743613), test loss: 2.7935443759\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (4.25640964508,24.3346714307), test loss: 31.183748436\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.65529155731,3.01761976398), test loss: 2.79512548149\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (2.33165788651,24.1805005204), test loss: 30.1665172577\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.93188035488,3.00405103973), test loss: 2.91273254752\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (6.30364322662,24.0272205878), test loss: 32.8813196182\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.51624298096,2.99079761116), test loss: 2.75530328304\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (7.9788942337,23.8765967163), test loss: 29.3600866795\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.07533144951,2.97775741373), test loss: 2.95351115465\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.56735897064,23.7283463948), test loss: 32.211490345\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.619865655899,2.96500378337), test loss: 2.77081078142\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (4.21807193756,23.5817172707), test loss: 30.5022808552\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (3.04361534119,2.95244171825), test loss: 2.97312377393\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (4.24523878098,23.4380916931), test loss: 33.9324478626\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.35745859146,2.93990866232), test loss: 2.58883831054\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (13.6545200348,23.2955343037), test loss: 33.3764616013\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.736825823784,2.92750048265), test loss: 2.95094096959\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (21.9746551514,23.1542419043), test loss: 33.1091806412\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.03861904144,2.91532110774), test loss: 2.54374421835\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (9.70234680176,23.0150093824), test loss: 33.204771471\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.97864818573,2.9033766535), test loss: 2.82349975109\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (6.29110717773,22.8781732479), test loss: 37.8944659233\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.14354515076,2.89165350032), test loss: 2.92197960913\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (6.52887201309,22.7426881149), test loss: 34.2549909115\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.87456822395,2.88003446515), test loss: 2.844546628\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (6.05113363266,22.6095998016), test loss: 33.6117842674\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.643529057503,2.86847165389), test loss: 2.86266593933\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (8.81874465942,22.4774796273), test loss: 32.7049705505\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.62271666527,2.85706662131), test loss: 2.57828870416\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (8.09945487976,22.3464209062), test loss: 28.5998887062\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.60760855675,2.84579394709), test loss: 2.6280948624\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.08851528168,22.2176407194), test loss: 34.3151576996\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.903572261333,2.83476186169), test loss: 2.69873980284\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (14.016626358,22.0904515324), test loss: 30.5294883728\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.782323360443,2.82392847452), test loss: 2.77109524608\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (3.35397100449,21.9651295147), test loss: 32.8843506336\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.39912450314,2.81314551117), test loss: 2.80763398409\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.44233179092,21.8413458804), test loss: 29.1918772221\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (4.04099702835,2.80248140388), test loss: 2.85010006428\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.93295001984,21.7186277032), test loss: 32.7775833607\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.612187981606,2.79183524486), test loss: 2.76271839142\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (1.72852790356,21.596962212), test loss: 30.4986870289\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.71115279198,2.78141514942), test loss: 2.8867010355\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (9.08941268921,21.4771888116), test loss: 34.7382155895\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.73828065395,2.77116705165), test loss: 2.56879682392\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (6.39495563507,21.3588635135), test loss: 30.2992825985\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.75612026453,2.76111348192), test loss: 2.94749395251\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (9.06463623047,21.2425588076), test loss: 34.2465013027\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (3.6628537178,2.75112196162), test loss: 2.51722134203\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (5.51876831055,21.1273258151), test loss: 34.4332199097\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.73896312714,2.74109940475), test loss: 2.89284524322\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (16.0566864014,21.0128783018), test loss: 35.2110724926\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.69033241272,2.73125811639), test loss: 2.71276843399\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (5.26081514359,20.8996718638), test loss: 36.7059211731\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.89444041252,2.72155720129), test loss: 2.93944439888\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.72761535645,20.7880367478), test loss: 32.3139861107\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.23757171631,2.71198000855), test loss: 2.80322173759\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (4.2689704895,20.6777338741), test loss: 36.0757248402\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.88152110577,2.70264730418), test loss: 2.73316065967\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (7.1407699585,20.5692139763), test loss: 31.1427339077\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.91481661797,2.69328522028), test loss: 2.75328619704\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (4.16350603104,20.4617795138), test loss: 32.3657289028\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.47434210777,2.6839310621), test loss: 2.5391566962\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (6.73672676086,20.3547675125), test loss: 29.1860899448\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.789385557175,2.67472196394), test loss: 2.63107352257\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (4.30146026611,20.2493100243), test loss: 34.4392699242\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.722057819366,2.66567483446), test loss: 2.80474605262\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (2.66203451157,20.1450638907), test loss: 31.1084584951\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.1788700819,2.65676534881), test loss: 2.81880087256\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (5.51782512665,20.0420810918), test loss: 33.488923645\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.95054626465,2.64801990783), test loss: 2.77124656439\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (5.94678211212,19.940628125), test loss: 29.398085475\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.7724391222,2.63921950088), test loss: 2.98459042311\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (12.2595901489,19.8400655265), test loss: 32.5297684193\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.29610872269,2.63048462225), test loss: 2.73145737946\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (4.82936000824,19.739925848), test loss: 31.0758915186\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.16640591621,2.62187070997), test loss: 2.95056750774\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (9.28163909912,19.6412836531), test loss: 34.7125053883\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.13081884384,2.61336832369), test loss: 2.58835859299\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (4.03286552429,19.5438785942), test loss: 31.9345695019\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.808348417282,2.60501406065), test loss: 2.88712292314\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (4.61880111694,19.4474195166), test loss: 35.2242056847\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.987317204475,2.59676001028), test loss: 2.58072817624\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (8.89828300476,19.3523747535), test loss: 34.8643135905\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.94561481476,2.58847631299), test loss: 2.86084300578\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (6.81582927704,19.2579297797), test loss: 37.3206376553\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.868507683277,2.58030865774), test loss: 2.74888574779\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.05909442902,19.1641074467), test loss: 35.4467455626\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.80973625183,2.57221724825), test loss: 2.87492153645\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (4.10181999207,19.0716712363), test loss: 32.4122330666\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.52735424042,2.56424415537), test loss: 2.81425961256\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (3.54269981384,18.9804342306), test loss: 38.1982159376\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.173009797931,2.55641678391), test loss: 2.75058889389\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (2.5964384079,18.8899951336), test loss: 29.9419538975\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.872818231583,2.54861450049), test loss: 2.62237612605\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (4.03229522705,18.8006176875), test loss: 32.846213758\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.789102435112,2.5408355352), test loss: 2.62639543712\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (8.99279022217,18.7120672255), test loss: 30.5008714676\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.95382285118,2.53311635692), test loss: 2.75783148706\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.56692171097,18.6238674038), test loss: 33.7518232822\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.10419845581,2.52550288473), test loss: 2.79967911839\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (8.68026351929,18.5371162595), test loss: 30.4756538391\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.90695261955,2.51798458329), test loss: 2.87002946734\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (4.03618812561,18.4511814973), test loss: 34.5659536839\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.39324045181,2.5106265057), test loss: 2.80824626088\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.11526298523,18.3663536443), test loss: 30.3339605808\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.847270727158,2.50324901304), test loss: 2.85566944778\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (2.74446296692,18.2822839965), test loss: 33.8529875278\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.686893463135,2.49587417055), test loss: 2.59765693247\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (6.07567691803,18.1988832413), test loss: 31.2701538563\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.05434167385,2.48862764504), test loss: 3.01950812638\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.93590021133,18.1160307686), test loss: 36.1598293304\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.870890200138,2.48143663627), test loss: 2.44208067954\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.97553348541,18.0342120741), test loss: 37.3201108694\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.502677321434,2.47433227385), test loss: 2.95790908337\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (3.50799489021,17.9533720556), test loss: 36.4390397549\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.15159130096,2.46739035487), test loss: 2.79846396148\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (4.11327981949,17.8734812062), test loss: 35.2006016016\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.850073575974,2.46039361546), test loss: 2.88611683547\n",
      "\n",
      "MC # 2, Hype # hyp5, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold2/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (324.519104004,inf), test loss: 182.829498672\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (246.64680481,inf), test loss: 303.035157013\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (18.4088230133,66.6796644325), test loss: 44.2019428253\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.04774475098,29.2717369248), test loss: 3.46752566993\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (29.554775238,56.1270873239), test loss: 37.5379008293\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.32379817963,16.2970866539), test loss: 2.86969003081\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (54.4851722717,52.6044831707), test loss: 42.6020172119\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.95717144012,11.9553115211), test loss: 3.67007667422\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (19.4601745605,50.8559384782), test loss: 41.0945552349\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.4385509491,9.78696040092), test loss: 3.5420417577\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (7.53586530685,49.6923821937), test loss: 40.5255879879\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.59940767288,8.48107592138), test loss: 3.55466997623\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (39.2473869324,48.8618758735), test loss: 43.3264282227\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.12097072601,7.60011586484), test loss: 3.9011965394\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (45.943862915,48.2211770437), test loss: 38.4618865013\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.67309141159,6.97203941633), test loss: 2.8817648977\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (48.7357254028,47.646286027), test loss: 46.2432953358\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.79385924339,6.49380205451), test loss: 3.81250071526\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (15.3607540131,47.2072781811), test loss: 40.2626809597\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.87147140503,6.11585260545), test loss: 2.7285204649\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (34.859413147,46.8714454497), test loss: 44.2790128708\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.14909946918,5.81305715328), test loss: 4.00968176126\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (43.4730758667,46.5333162092), test loss: 38.6540953159\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.26874351501,5.56466401817), test loss: 2.72804263234\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (23.3557319641,46.2080699369), test loss: 42.5581206799\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.989782452583,5.35537253332), test loss: 3.67810122967\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (41.9461479187,45.9014906368), test loss: 37.2930771351\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.5499958992,5.17997042667), test loss: 3.02724202275\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (37.0177879333,45.5782001666), test loss: 43.2014340878\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.79204940796,5.02604931296), test loss: 3.50151975751\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (23.2785453796,45.2936247662), test loss: 35.9311078072\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.24899244308,4.88767904893), test loss: 2.61656828523\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (16.8374977112,45.036354454), test loss: 41.8461418629\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.06537485123,4.76694923351), test loss: 3.63468255997\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (17.989566803,44.7616771899), test loss: 33.4870430946\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.76588070393,4.6580757275), test loss: 2.48745209277\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (32.5424919128,44.4803927742), test loss: 39.6615938187\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.27903342247,4.5590538896), test loss: 3.35356809199\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (55.5075531006,44.1577775554), test loss: 36.6375335217\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (5.9766292572,4.47015791531), test loss: 3.4192391932\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (39.1584014893,43.8096216288), test loss: 36.5063642979\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.47031784058,4.3866904716), test loss: 3.34176360965\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (51.7195587158,43.479678472), test loss: 36.7200418949\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (5.93020439148,4.30800160323), test loss: 3.4275005728\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (30.0345096588,43.1551446809), test loss: 32.8524191618\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.44473552704,4.23649563058), test loss: 2.68078416586\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (18.4879169464,42.8081508955), test loss: 39.930100441\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.89250123501,4.16886407489), test loss: 3.51739395857\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (46.3685302734,42.4502970011), test loss: 34.9495437384\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.9057264328,4.10504213872), test loss: 2.59765384793\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (26.365814209,42.0767993551), test loss: 39.0826114655\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.72281241417,4.04544035949), test loss: 3.71303353906\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (26.7285900116,41.6871374765), test loss: 33.9806625843\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (6.9530620575,3.98847936646), test loss: 2.4153288275\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (21.8212013245,41.2985423964), test loss: 36.9190334797\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.13463878632,3.93371766307), test loss: 3.62875190973\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (37.0073051453,40.9024152257), test loss: 29.1307935715\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (7.16971540451,3.88435984765), test loss: 2.45082293451\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (30.25025177,40.4843346955), test loss: 37.9530148506\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.56245958805,3.83726747742), test loss: 3.57636370659\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (27.1360206604,40.0558165611), test loss: 28.6860302925\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.52316141129,3.79214001774), test loss: 2.72909723818\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (28.3852043152,39.6165026336), test loss: 36.7569879055\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.69413328171,3.74911397336), test loss: 3.41273193657\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (13.8476409912,39.1696808524), test loss: 26.1288718224\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.17232990265,3.70711710847), test loss: 2.47351096272\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (22.2726287842,38.7308983643), test loss: 33.6138757944\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.16155648232,3.66619220205), test loss: 3.25303398073\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (21.7187156677,38.2934475874), test loss: 30.14039011\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.79771447182,3.62740946318), test loss: 3.15842212737\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (41.9502487183,37.8518925804), test loss: 31.9532742977\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.16286087036,3.59023930756), test loss: 3.42461801171\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (15.9652643204,37.4143193891), test loss: 30.1555917978\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (0.877088844776,3.55418394048), test loss: 3.30636017025\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (31.5962924957,36.9749166026), test loss: 26.9331439972\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.92215418816,3.51933454277), test loss: 2.60267580599\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (10.1878213882,36.5424059442), test loss: 31.409876442\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.13454246521,3.48502284261), test loss: 3.43447006494\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (11.113038063,36.1221184818), test loss: 29.7464621067\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.82970547676,3.45153109176), test loss: 2.42970821857\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (42.3235206604,35.7095894646), test loss: 32.530510664\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.31377458572,3.41925771939), test loss: 3.66700552702\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (14.5410919189,35.3021704358), test loss: 28.0253285408\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.835587501526,3.38839677719), test loss: 2.26620837897\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (11.4152412415,34.9078760325), test loss: 33.3577881813\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.76234447956,3.35856296109), test loss: 3.48082271516\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (63.7913246155,34.514866492), test loss: 27.2590529442\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.04243087769,3.32945504769), test loss: 2.46350506693\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (18.347202301,34.1342099638), test loss: 35.7449633121\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.94656443596,3.30079807827), test loss: 3.61792235374\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (13.5650157928,33.7649860442), test loss: 28.7206384659\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.85274779797,3.27295658554), test loss: 2.58005834818\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (9.1556968689,33.4033931551), test loss: 34.8099545956\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.57068705559,3.24609134657), test loss: 3.47220470011\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (8.83030033112,33.0509154513), test loss: 25.316994381\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.37055873871,3.22035101955), test loss: 2.46213216633\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (10.4390659332,32.7101539728), test loss: 32.8692332983\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.994793176651,3.19540903263), test loss: 3.2734646678\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (5.48794269562,32.3690267773), test loss: 30.441995573\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.890692412853,3.17097890331), test loss: 3.21743245274\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (19.1730575562,32.0418698036), test loss: 32.1654254198\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.2161192894,3.14686122357), test loss: 3.4201813817\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (6.04219150543,31.7223834747), test loss: 30.619584322\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.98257887363,3.12337717915), test loss: 3.20567875803\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (12.0815963745,31.4088369252), test loss: 33.2655126572\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.61356401443,3.1005777431), test loss: 3.15843882561\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (3.80895924568,31.1031780989), test loss: 30.0436792374\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.68480372429,3.07858651111), test loss: 3.50824204981\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (20.3353176117,30.8070254703), test loss: 29.1581230164\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.03585863113,3.05726155214), test loss: 2.49971594512\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (21.787815094,30.5108660188), test loss: 32.0212327719\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.47870111465,3.03627383449), test loss: 3.64074221253\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (14.9260349274,30.2249422016), test loss: 28.9468287468\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.18935966492,3.0153967163), test loss: 2.29088413864\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (8.15406036377,29.9456296693), test loss: 35.6386105061\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.94294822216,2.9949895364), test loss: 3.46718027294\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (36.0987281799,29.6700554974), test loss: 27.8021621227\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.0438117981,2.97503551506), test loss: 2.34388689399\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (25.8561668396,29.4016906426), test loss: 35.5908102036\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.80723047256,2.95582967922), test loss: 3.47085269094\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.9676742554,29.1402773324), test loss: 28.2770507813\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.66286730766,2.9371522818), test loss: 2.64973429143\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (16.1970310211,28.8794085655), test loss: 34.6803451538\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.302118420601,2.91866115878), test loss: 3.43601026237\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (24.3348007202,28.6271936017), test loss: 27.4388889313\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.24753165245,2.90028932551), test loss: 2.55792626888\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (11.8536615372,28.3797520712), test loss: 34.6955910683\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.36156749725,2.88226708707), test loss: 3.20935328156\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (24.7996482849,28.1343714601), test loss: 26.0035511732\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.47841525078,2.86453088369), test loss: 2.82425220609\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (21.100479126,27.8958146972), test loss: 35.8292755842\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.14583826065,2.84741091516), test loss: 3.19383912385\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.6549291611,27.6630055253), test loss: 31.438364315\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.331018298864,2.83077599504), test loss: 3.38000902534\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (8.47206687927,27.4311339878), test loss: 34.4024199486\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.39770507812,2.81434594101), test loss: 3.37667083293\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (7.9126739502,27.2057425349), test loss: 29.7805176735\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.612295031548,2.79785253151), test loss: 3.36297504455\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (12.9834117889,26.9846610909), test loss: 30.3933790207\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.85562324524,2.78173115834), test loss: 2.59416614473\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (10.7183914185,26.7646970627), test loss: 34.3857540131\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.85403418541,2.7658131877), test loss: 3.40501614511\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (7.43993282318,26.5508293538), test loss: 31.8260035992\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.49998426437,2.75042508907), test loss: 2.38821955025\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (29.7912101746,26.341640967), test loss: 36.4479416609\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.82432687283,2.73545089652), test loss: 3.61623381078\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (20.8294944763,26.133762869), test loss: 28.658138895\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.993309021,2.72058228104), test loss: 2.29294256121\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.85937690735,25.9309694966), test loss: 35.6308044195\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (4.2772026062,2.70568853715), test loss: 3.39820561111\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (12.9361209869,25.7316222724), test loss: 29.350082922\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.59308981895,2.69102677198), test loss: 2.5789060086\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (8.57928085327,25.5334402831), test loss: 36.33070755\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.842875361443,2.67663150931), test loss: 3.29559784532\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (8.79830551147,25.3404385182), test loss: 28.3565328121\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.13619589806,2.66264650936), test loss: 2.50639759675\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (25.9618339539,25.1506950993), test loss: 35.2563859463\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.24279987812,2.64901856086), test loss: 3.22213574946\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.1630897522,24.9633938842), test loss: 25.7229501724\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.02727937698,2.63549480718), test loss: 2.35311445594\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (9.34207344055,24.7804207444), test loss: 34.6355607271\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.50061273575,2.62192543029), test loss: 3.16204740703\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (3.00749397278,24.5999812595), test loss: 29.207531786\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.394640922546,2.60866644412), test loss: 2.86801203787\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (5.69419956207,24.4209116864), test loss: 36.0086315155\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.6186965704,2.59559748881), test loss: 3.2349953115\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (11.7943687439,24.2460283431), test loss: 30.1399812937\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.24658536911,2.58281515089), test loss: 3.23289880455\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.4684028625,24.0736252543), test loss: 30.9884676695\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.12940573692,2.57039966579), test loss: 2.58955582976\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (7.28066253662,23.9041125382), test loss: 32.9212132692\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.546185791492,2.55803623733), test loss: 3.34397794902\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (8.91914463043,23.7378042729), test loss: 33.1011033058\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.78954958916,2.54564176512), test loss: 2.44668942541\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (10.9225320816,23.5740375169), test loss: 35.3569430351\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.31199264526,2.53358447863), test loss: 3.42601307034\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (4.41098165512,23.4112328183), test loss: 31.8784945965\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.854067444801,2.52160292206), test loss: 2.30195448995\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (10.9864883423,23.2519560648), test loss: 35.7602386475\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.9251486063,2.50991074804), test loss: 3.33300452679\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (20.9654197693,23.0949760074), test loss: 31.7651984215\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.10789084435,2.49853694009), test loss: 2.62004924119\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (16.5927505493,22.9405727135), test loss: 37.068556118\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.78526139259,2.48720846475), test loss: 3.31615356058\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (3.88709378242,22.7885694258), test loss: 29.6773692131\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.652413666248,2.47581827036), test loss: 2.53358733058\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (14.2959680557,22.6389669731), test loss: 36.9072210789\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.36414361,2.46477211663), test loss: 3.178477557\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (17.1151504517,22.4904289027), test loss: 26.8872587204\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.905758857727,2.45377072299), test loss: 2.32070014924\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (3.3197259903,22.3446650131), test loss: 37.6792835712\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.456301629543,2.44299954022), test loss: 3.12594342232\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (4.93097019196,22.2009337065), test loss: 29.6070489883\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.809760570526,2.43254083084), test loss: 2.90556943119\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (4.98996257782,22.0596647265), test loss: 35.8203705311\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.388954460621,2.42212041333), test loss: 3.21590319276\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (9.33319091797,21.9204868866), test loss: 30.2079577923\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.20994651318,2.41164434383), test loss: 3.05754950345\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (6.35953187943,21.7829369488), test loss: 32.9138313532\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.30659663677,2.40146810435), test loss: 2.62415477633\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (6.05552482605,21.6466971932), test loss: 36.2513165474\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.62714838982,2.39132967726), test loss: 3.30641826093\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (4.86674404144,21.5130404021), test loss: 36.5165429115\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.772234618664,2.38138500538), test loss: 2.4247005403\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (6.26200151443,21.3810429213), test loss: 35.1041891336\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.227690547705,2.37172223558), test loss: 3.40190100074\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (8.95614814758,21.2511675875), test loss: 33.8012124062\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.76687759161,2.36209092465), test loss: 2.35338099897\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (5.59113407135,21.1229079155), test loss: 36.9754673004\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.28381204605,2.35236702309), test loss: 3.33787393272\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (6.63753032684,20.9960613727), test loss: 34.2299566269\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.702707648277,2.34293477108), test loss: 2.5083814621\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (10.4311332703,20.8704642699), test loss: 40.5672224045\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.36969900131,2.33353043401), test loss: 3.46335302293\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (5.71641588211,20.7471123983), test loss: 31.0026695251\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.49757289886,2.32431175376), test loss: 2.52412741184\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (4.65755081177,20.6252244974), test loss: 38.6711767197\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.34639525414,2.31535150484), test loss: 3.23798376918\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (5.35072898865,20.5052388292), test loss: 27.1598926544\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.09940600395,2.30639439318), test loss: 2.32788278759\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (3.52441978455,20.3866572454), test loss: 37.238384366\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.718113780022,2.29736318462), test loss: 3.07872287333\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (9.65474796295,20.269183098), test loss: 32.6175563812\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.89032030106,2.28860466038), test loss: 2.89821046591\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (8.72207641602,20.1527655803), test loss: 38.4022265196\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.457384288311,2.27985816386), test loss: 3.2927337721\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (6.8562002182,20.0385812812), test loss: 31.9859471083\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.19041538239,2.27126566028), test loss: 3.02469641566\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (14.9768295288,19.9257001054), test loss: 33.7653372765\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.916766285896,2.26292944926), test loss: 2.67105481327\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (12.5453023911,19.8144573983), test loss: 32.8267853737\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.69434094429,2.25459421339), test loss: 3.17520466447\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.35513877869,19.7044848358), test loss: 35.5385744095\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.867240548134,2.24620851012), test loss: 2.49099114239\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (13.6466474533,19.5953241839), test loss: 35.3213137627\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (3.71358823776,2.23802050947), test loss: 3.37690144181\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (5.05057525635,19.4872117507), test loss: 39.9961203575\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.60749161243,2.22988187403), test loss: 2.44552576542\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.76679325104,19.3810349231), test loss: 37.8158494473\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.68927693367,2.22184338291), test loss: 3.3319475174\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (4.30656003952,19.276019768), test loss: 32.7580014706\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.510940611362,2.2140794725), test loss: 2.47302522659\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (8.80936813354,19.1725776561), test loss: 40.0472403526\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.990697622299,2.20629192636), test loss: 3.40626401305\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (12.7847900391,19.0703567591), test loss: 30.0034757137\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.39616072178,2.19845902573), test loss: 2.53281985223\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (4.59412574768,18.9686433387), test loss: 41.4805346966\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.638260662556,2.19075456345), test loss: 3.30814324319\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (6.88192510605,18.8680799515), test loss: 32.4519405365\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.75721311569,2.1831717054), test loss: 2.59162694216\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (6.28762626648,18.769062689), test loss: 37.5558450699\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.21634340286,2.17566106047), test loss: 3.03092941642\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (4.39550352097,18.6711821886), test loss: 32.5041474342\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.11331784725,2.1684014327), test loss: 3.0034871906\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.84970188141,18.5746468809), test loss: 37.0963181496\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.74805092812,2.16109184454), test loss: 3.17180850804\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.92704772949,18.4791731398), test loss: 30.4106820583\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.951100945473,2.1537444619), test loss: 2.91801603436\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (17.2140579224,18.3843923081), test loss: 34.9682433128\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.23274934292,2.14653340265), test loss: 2.66417243332\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (5.79207038879,18.2904762342), test loss: 36.9559811831\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.416525304317,2.13939948456), test loss: 3.29630057514\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (3.65807938576,18.1979265768), test loss: 35.4217803478\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.416277647018,2.13235496197), test loss: 2.51240517944\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (6.49653244019,18.1064358027), test loss: 34.3868624687\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.684084177017,2.12554435626), test loss: 3.35373139381\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (5.33233356476,18.016116623), test loss: 33.9540580273\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.37555861473,2.11866728459), test loss: 2.32434971631\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (1.56893873215,17.9267798892), test loss: 38.8275107384\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.37816512585,2.11177144791), test loss: 3.34188654423\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (4.36795139313,17.8379319528), test loss: 31.1532177448\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.474088788033,2.10498857173), test loss: 2.30519604385\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (8.12497425079,17.7501975059), test loss: 42.1187119722\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.13584423065,2.09828880135), test loss: 3.4130374223\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (0.731917619705,17.66350315), test loss: 30.3652973175\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.158763170242,2.091657882), test loss: 2.49782342613\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (2.89482784271,17.5777507729), test loss: 39.838990283\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.995044648647,2.08523234242), test loss: 3.20312003195\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (3.57108259201,17.4930502358), test loss: 28.8730852365\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.39953660965,2.07875636942), test loss: 2.47728730738\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (4.44215488434,17.4093552382), test loss: 38.8709737778\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.59866774082,2.07225515028), test loss: 3.00278458297\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (4.46059179306,17.3259291556), test loss: 32.0370205879\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.4131937027,2.06586872307), test loss: 2.95161929727\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (7.39734601974,17.2435405834), test loss: 37.6780832291\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.74621200562,2.05953753809), test loss: 3.1605261296\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (2.97935771942,17.1622705795), test loss: 31.1529150486\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.11542129517,2.05329015391), test loss: 2.97274682522\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (7.01570224762,17.0816898137), test loss: 39.4083621502\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.05353605747,2.04720382154), test loss: 3.18712919354\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (4.38734292984,17.0021113847), test loss: 31.8296088934\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.42144620419,2.04106666546), test loss: 3.25836057961\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (10.1311407089,16.9234155903), test loss: 34.9196187973\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.925457239151,2.03494106547), test loss: 2.53789203763\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (3.53704166412,16.8449947124), test loss: 34.9758647442\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.01909935474,2.02889658509), test loss: 3.31944104433\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (3.66679024696,16.7674475561), test loss: 35.3150145531\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.835325241089,2.02289952503), test loss: 2.29219787717\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.63305211067,16.6910474152), test loss: 39.807281971\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.603304743767,2.01699546074), test loss: 3.44473538399\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (5.08756113052,16.615091598), test loss: 32.0751396179\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.16820347309,2.01122589801), test loss: 2.34163538814\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.51405906677,16.5401276617), test loss: 39.9166517735\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.54381036758,2.00539963561), test loss: 3.3825106889\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (5.62101554871,16.4658894715), test loss: 30.5176704884\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.21813321114,1.99960405536), test loss: 2.47821219414\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (7.00771999359,16.392039779), test loss: 39.7106015682\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.74899840355,1.99386792109), test loss: 3.1926504612\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (7.15901279449,16.3189099889), test loss: 30.4866119862\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.06566262245,1.98817654784), test loss: 2.47558543682\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.93373656273,16.2468232995), test loss: 38.4191234112\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.10944700241,1.98257379549), test loss: 3.07213691473\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (4.5135974884,16.1751864301), test loss: 28.8638112545\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.23255228996,1.977090278), test loss: 2.62544929981\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (1.89204525948,16.1044805959), test loss: 38.1768185139\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.67433285713,1.97152934531), test loss: 3.09887184501\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (6.93919754028,16.0344390856), test loss: 32.5140996933\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.73472058773,1.96605633415), test loss: 2.93822453618\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (3.72301483154,15.9647038918), test loss: 39.1390380859\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.41537439823,1.96059476778), test loss: 3.19583960176\n",
      "\n",
      "MC # 2, Hype # hyp5, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold3/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (304.554748535,inf), test loss: 153.403495598\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (266.356384277,inf), test loss: 313.821763611\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (114.750640869,59.7349586992), test loss: 46.4036746502\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (7.64040756226,28.9950896813), test loss: 3.26876393557\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (42.2224731445,53.0989634032), test loss: 37.9758136272\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.54707312584,16.1415707741), test loss: 3.44308438301\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (61.0651626587,50.7374004386), test loss: 47.2822822332\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.59923553467,11.8621466855), test loss: 3.34948358834\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (53.5780029297,49.5924751878), test loss: 43.6208403587\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.97228574753,9.71726313357), test loss: 3.38206214905\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (23.9265117645,48.7685926971), test loss: 43.6295027733\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (6.74903535843,8.43242645473), test loss: 2.79224687815\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (39.921043396,48.2762804825), test loss: 45.5627892494\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.92634081841,7.58740796076), test loss: 3.74969055057\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (18.1072673798,47.8341503018), test loss: 36.4829607487\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.91666030884,6.98550281154), test loss: 2.73401379585\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (32.8845825195,47.4823107123), test loss: 46.4095609665\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.6850643158,6.53732418133), test loss: 3.33731583953\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (42.8532562256,47.2023652101), test loss: 33.7421711445\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (7.26969385147,6.18607321209), test loss: 2.73364722133\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (20.7609119415,46.9356324647), test loss: 45.1934598923\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.58179283142,5.90428064476), test loss: 3.31277863383\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (35.1969451904,46.6852683362), test loss: 37.6157306671\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.38066768646,5.67187736988), test loss: 3.34992246628\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (102.707633972,46.4559936452), test loss: 41.0358880997\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.49985170364,5.47686627796), test loss: 2.95147001743\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (42.6058654785,46.2632665008), test loss: 44.2831977367\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.64651322365,5.31121186924), test loss: 3.41859454215\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (54.6574554443,46.0603071286), test loss: 41.8752949238\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.84201061726,5.16509962273), test loss: 2.80185986757\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (19.6150970459,45.8914366517), test loss: 43.5050741196\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.27137804031,5.03848217179), test loss: 3.47605843544\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (30.061416626,45.7402563418), test loss: 35.7442601204\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.64713954926,4.92733294202), test loss: 2.66595570743\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (26.1320800781,45.578131812), test loss: 45.5298066616\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.48878145218,4.82709954568), test loss: 3.24864937067\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (30.8649616241,45.4183251623), test loss: 31.3300120354\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.84691643715,4.73709585989), test loss: 2.55048533678\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (52.2417526245,45.2621189205), test loss: 43.7440232754\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.11478424072,4.65577768571), test loss: 3.19135940075\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (93.5718917847,45.0846872454), test loss: 36.8535452366\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.06184864044,4.58307458146), test loss: 3.2451002717\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (28.3982582092,44.8956637224), test loss: 38.3324994802\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.82980060577,4.51563462539), test loss: 2.79970671535\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (28.2045021057,44.6961495438), test loss: 42.7943229198\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.35114622116,4.45367291467), test loss: 3.3557934165\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (27.6078510284,44.5076991584), test loss: 39.4682878017\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.38598752022,4.39370758538), test loss: 2.72390339375\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (17.233417511,44.2815429939), test loss: 42.0978831291\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.62208938599,4.33855330879), test loss: 3.2106428802\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (28.3817634583,44.066304071), test loss: 32.0232642651\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.32855153084,4.28752776235), test loss: 2.58817924261\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (18.3153209686,43.8288830962), test loss: 42.7507295609\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.30633449554,4.23864538049), test loss: 3.08179425001\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (34.6750564575,43.5846501262), test loss: 27.5023354053\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.880076885223,4.19285130059), test loss: 2.39180099368\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (27.2458934784,43.3290323383), test loss: 40.9840387821\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.2881436348,4.14863647571), test loss: 3.10832309425\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (25.6670322418,43.0580691136), test loss: 35.1914579391\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.819958627224,4.10614724784), test loss: 3.28355163038\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (34.5697555542,42.7695997547), test loss: 34.3870480537\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.38176226616,4.06489179463), test loss: 2.48720663786\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (45.0962333679,42.4699718728), test loss: 38.242909193\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.3394356966,4.02472020656), test loss: 3.26987430751\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (30.8087425232,42.1655865608), test loss: 32.430086565\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.63624763489,3.98593969373), test loss: 2.41868588924\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (19.7708816528,41.8424373352), test loss: 40.1194236755\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.63185548782,3.9470698016), test loss: 3.04305252433\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (27.0941829681,41.5154896976), test loss: 27.7605377674\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.80185437202,3.90961169406), test loss: 2.48301426768\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (31.6325340271,41.182790119), test loss: 38.3743041515\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.33128428459,3.87337486608), test loss: 3.00976675153\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (10.7930450439,40.8349804976), test loss: 30.3161278009\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.75552606583,3.83741167977), test loss: 2.88471179307\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (76.7186737061,40.4848759674), test loss: 36.1673812866\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.37285327911,3.80263905507), test loss: 3.04218317866\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (15.9960184097,40.1243776308), test loss: 32.6215584278\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.0800139904,3.76806414379), test loss: 3.17710031867\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (42.1135406494,39.758307663), test loss: 31.6853575468\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.29049563408,3.73449219262), test loss: 2.42944129407\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (23.6905899048,39.3847417861), test loss: 36.6173413277\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.04979765415,3.70122697719), test loss: 3.30212062895\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (19.3519134521,39.0092900187), test loss: 28.1596525192\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.11919975281,3.66890202692), test loss: 2.38332892358\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (31.0821399689,38.6410369032), test loss: 37.8592418194\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.63425993919,3.63676309701), test loss: 3.08080918193\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (12.499464035,38.2679868995), test loss: 25.7696670055\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.14660692215,3.60538704489), test loss: 2.54147773683\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (16.2489871979,37.9028867469), test loss: 35.706588459\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.29548895359,3.57490520534), test loss: 3.08307855427\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (23.8192806244,37.5375942838), test loss: 29.5003473759\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.64544630051,3.54516927417), test loss: 3.15708317757\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (26.1472682953,37.1787337548), test loss: 36.0593596935\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.90339446068,3.51643199374), test loss: 3.11841414273\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (13.4442939758,36.8229392152), test loss: 30.8601977348\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.17936348915,3.48847560996), test loss: 3.1755276829\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (23.3015670776,36.475521119), test loss: 31.1585628986\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.42233836651,3.46114896639), test loss: 2.55874330848\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (19.8608818054,36.1279783657), test loss: 35.5443612099\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.526061415672,3.43408800486), test loss: 3.190236637\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (7.32735729218,35.7886984297), test loss: 28.4821795464\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.545966506004,3.40765919624), test loss: 2.57407694161\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (9.37833786011,35.4576584647), test loss: 35.8966506004\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.30534863472,3.38189605715), test loss: 3.0397921145\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (6.88007164001,35.1314614027), test loss: 27.3724706173\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.36779761314,3.35611299342), test loss: 2.68412068784\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (5.44068193436,34.8115060141), test loss: 36.0106476068\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.19008779526,3.33116907598), test loss: 2.98479098678\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (12.1873683929,34.4996559227), test loss: 29.4387172222\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.39214134216,3.30704788793), test loss: 3.21763690114\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (4.26121520996,34.1908730379), test loss: 37.0143884659\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.963499844074,3.28317656288), test loss: 3.14893257022\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (34.0034446716,33.8916207277), test loss: 32.8152231693\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.41369843483,3.26028241124), test loss: 3.30752507448\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (9.84388542175,33.5977207404), test loss: 31.9492099762\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.68750667572,3.23781520923), test loss: 2.49055676758\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (20.9873714447,33.3095480127), test loss: 38.1450030804\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.756985783577,3.21588254298), test loss: 3.21196628809\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.1215991974,33.0244880501), test loss: 31.2060464382\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.25276589394,3.194395067), test loss: 2.56796373427\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.7277154922,32.7454032628), test loss: 36.0916409492\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.22427296638,3.17331748438), test loss: 2.96858096719\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (18.5702781677,32.4752601234), test loss: 26.0742054462\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.23271453381,3.15243081568), test loss: 2.55294157118\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (33.7036705017,32.2077934616), test loss: 36.4829596996\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.94491887093,3.13199820097), test loss: 3.09091992676\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (3.91462373734,31.9451732488), test loss: 29.2564176559\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.443437814713,3.11205640873), test loss: 3.16203558892\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (30.3840026855,31.6868869018), test loss: 36.4924234867\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.68800520897,3.09251721485), test loss: 2.98173071146\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (18.8820133209,31.4339028997), test loss: 35.6669714928\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.19146299362,3.07346061278), test loss: 3.28783801794\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (17.2365207672,31.1855490685), test loss: 32.3777876377\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.48190534115,3.05489787696), test loss: 2.4509138152\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (17.0817451477,30.9428451204), test loss: 35.6711293221\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.45966386795,3.0367107145), test loss: 3.05853189528\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (11.2103595734,30.7014162256), test loss: 31.1298225403\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.666232764721,3.01880138317), test loss: 2.67886053324\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (19.6012496948,30.4659874617), test loss: 36.9580869436\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.83880937099,3.00128890824), test loss: 2.99584953934\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (7.44095134735,30.2342895296), test loss: 24.705133462\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.67890453339,2.98408976684), test loss: 2.43106685877\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.5029649734,30.0063285449), test loss: 37.1264960289\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.80382061005,2.96676523382), test loss: 3.07725359201\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (11.4243364334,29.7817761583), test loss: 30.1127112865\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.29456329346,2.94997275031), test loss: 3.11136563122\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (5.31996059418,29.5608010735), test loss: 32.3641425133\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.811733186245,2.93357127367), test loss: 2.69429571331\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (7.65376853943,29.3420100142), test loss: 33.7727030754\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.337307751179,2.9172851055), test loss: 3.21589584649\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (11.2484064102,29.1287720119), test loss: 33.7987231731\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.650573790073,2.90158383182), test loss: 2.53028596044\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (9.42269134521,28.9181157365), test loss: 35.7473634243\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.3532397747,2.88608418852), test loss: 3.06037059128\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (6.9491648674,28.7113199214), test loss: 32.3365316868\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.482116580009,2.87088107085), test loss: 2.83542869091\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (6.767827034,28.505519953), test loss: 38.4335438728\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.81479072571,2.85591450724), test loss: 3.00085599124\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (6.46783828735,28.3042818979), test loss: 24.2938912392\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.857559502125,2.84115705965), test loss: 2.38339842707\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (17.7509803772,28.1070050276), test loss: 37.5769134521\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.15190696716,2.82643503403), test loss: 3.07146654427\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (23.0546989441,27.9117030952), test loss: 30.0181378365\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.49203109741,2.81199207198), test loss: 3.23371210545\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (9.27654647827,27.7186485366), test loss: 34.058391571\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.576836705208,2.79781345664), test loss: 2.65140806139\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (16.7760925293,27.5281526321), test loss: 33.5216026783\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.53835773468,2.78386009642), test loss: 3.19751980603\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (10.0106182098,27.3408932099), test loss: 35.1683657646\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.498369693756,2.77014923466), test loss: 2.53415405154\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (16.3396129608,27.1561736394), test loss: 37.7144266605\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.589589118958,2.75672234316), test loss: 3.04056540728\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (10.9670648575,26.975324377), test loss: 28.206637907\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.430979907513,2.74355912713), test loss: 2.57010332346\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (6.91431093216,26.7948612341), test loss: 38.8421158314\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.8682487607,2.73055426156), test loss: 2.95439041108\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (17.3845481873,26.6188535279), test loss: 27.9578466177\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.81878900528,2.71780102602), test loss: 2.80865876973\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (6.75999307632,26.4443020509), test loss: 37.8347015381\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.84443044662,2.70516747439), test loss: 3.05748987645\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (15.5999088287,26.2729290555), test loss: 30.0687423229\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (4.3727016449,2.69249623099), test loss: 3.07045106888\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (21.7498321533,26.1033048496), test loss: 35.6867153168\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.05697488785,2.68009010202), test loss: 2.61778676957\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (6.84476041794,25.9356750395), test loss: 36.7451758385\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.31091141701,2.66798840199), test loss: 3.20557800233\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (5.98587703705,25.7697164615), test loss: 33.0628150463\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.55569791794,2.65589697336), test loss: 2.38538832664\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (7.87774515152,25.6070368569), test loss: 38.928726697\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.599004030228,2.64414646689), test loss: 3.11000140905\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (7.48268556595,25.4466462126), test loss: 28.5212272644\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.01759123802,2.63257668973), test loss: 2.51902744174\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (9.86713600159,25.2883882994), test loss: 40.4384260178\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.22160315514,2.62119733483), test loss: 2.95536305904\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (5.78012418747,25.1311513181), test loss: 29.6652475834\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.08826851845,2.60997958484), test loss: 2.90684336126\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (11.5831184387,24.9770286082), test loss: 38.5147975445\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.21427118778,2.59887997527), test loss: 3.02548463047\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (8.22945022583,24.8250263521), test loss: 29.74352777\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.65315103531,2.58774948782), test loss: 3.01864496768\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (12.4141521454,24.6747884286), test loss: 36.6996686935\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.93121600151,2.57685980742), test loss: 2.61744847298\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (10.2796621323,24.5257341415), test loss: 38.1662343025\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.61352300644,2.5661332305), test loss: 3.17105367184\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (7.5779042244,24.378403599), test loss: 32.0665633202\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.23535585403,2.5555303934), test loss: 2.49344913661\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.27369499207,24.233295996), test loss: 38.9155359268\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.37072873116,2.54507452423), test loss: 3.04845016301\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (12.2399110794,24.090099796), test loss: 27.4650794983\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.48495292664,2.53482591952), test loss: 2.62767104357\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.51024961472,23.949046312), test loss: 39.1201884985\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.11190271378,2.52472998623), test loss: 2.83291123956\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (14.6470413208,23.8090110643), test loss: 28.4121321201\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.05483675003,2.51478011542), test loss: 2.99841010273\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (6.39025783539,23.6713188934), test loss: 39.6004449844\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.64917850494,2.50497472019), test loss: 3.06049669906\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (7.2883682251,23.5350312407), test loss: 32.4806294918\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.03533959389,2.4952121788), test loss: 3.04338636398\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (8.19962120056,23.4006945048), test loss: 36.9720667839\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (3.47308826447,2.48546341248), test loss: 2.65528113544\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (15.7733335495,23.2677366838), test loss: 38.3483388901\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.817136526108,2.47586613852), test loss: 3.07290605307\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (12.8827505112,23.1360048849), test loss: 33.2863903046\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (3.70650720596,2.46649406015), test loss: 2.49480810314\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (8.97788619995,23.0054392497), test loss: 38.168787241\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.88946294785,2.45709863982), test loss: 2.86333354115\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (8.24544334412,22.8770788959), test loss: 27.8201509953\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.683586716652,2.4479398983), test loss: 2.51916716024\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (10.1811389923,22.7501472056), test loss: 40.2320070028\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.599328517914,2.43889250931), test loss: 2.94140897989\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (15.8342685699,22.6251833664), test loss: 28.8815336704\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.5639244318,2.43001079907), test loss: 2.95708178282\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (8.68369483948,22.5004607903), test loss: 40.584966135\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.32649821043,2.42122552865), test loss: 3.09959303141\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (15.8873844147,22.3782976862), test loss: 34.9382473469\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.56059479713,2.41250112648), test loss: 3.06211702228\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (4.54260444641,22.2570068766), test loss: 36.4584997654\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.31684422493,2.40372854932), test loss: 2.47882593274\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (9.87974834442,22.1374565312), test loss: 38.5491663456\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.73301208019,2.39515911431), test loss: 2.97873442471\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (10.0600299835,22.0184731301), test loss: 32.2844157696\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.74925994873,2.38669193078), test loss: 2.64368596375\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (9.31597900391,21.9007521755), test loss: 40.3848968029\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.51152920723,2.3783171602), test loss: 2.89178647101\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (5.85782194138,21.7844378766), test loss: 27.5372899532\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.57258439064,2.36998731289), test loss: 2.47344168425\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (6.74392223358,21.6693253745), test loss: 39.8276386261\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.34021365643,2.36182271361), test loss: 2.98221718073\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (4.27768802643,21.5559849219), test loss: 29.1967368603\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.07512354851,2.35376204551), test loss: 2.91862409711\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (13.171295166,21.4433476206), test loss: 36.85813694\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.97593736649,2.34584835527), test loss: 2.74914468527\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (3.47795796394,21.3322930091), test loss: 34.5193516493\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.1992483139,2.337974331), test loss: 3.09283251166\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (4.63980436325,21.2221639527), test loss: 37.0008824825\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.17883777618,2.33016477361), test loss: 2.54499635994\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (5.71535158157,21.1135909401), test loss: 40.6882988453\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.79181218147,2.32231638573), test loss: 3.05519047678\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (7.71482658386,21.0059037362), test loss: 29.1814414501\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.42389893532,2.31460402933), test loss: 2.51267236173\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (9.92239761353,20.89914116), test loss: 40.9983347416\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.30038070679,2.30704125612), test loss: 2.95215093344\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (13.4932231903,20.7932592723), test loss: 26.2518972874\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.15378642082,2.29945069642), test loss: 2.3851670891\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (9.6269493103,20.6887488459), test loss: 40.4274783611\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.32410562038,2.29203897664), test loss: 2.99079476893\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (7.07786846161,20.5853933192), test loss: 30.4404637337\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.48277208209,2.28467982025), test loss: 2.89478810728\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (14.1074562073,20.4834786163), test loss: 37.1747963905\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.07084727287,2.27747745627), test loss: 2.82509533167\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (5.59054660797,20.3818676681), test loss: 34.2240199566\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.998325705528,2.27033560495), test loss: 3.09170886874\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (8.75650119781,20.281995882), test loss: 38.1311092854\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.817071378231,2.26322295841), test loss: 2.55649993122\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (3.0814499855,20.1828944065), test loss: 40.1560889482\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.15983843803,2.25607379356), test loss: 3.04791307449\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (1.90602397919,20.084948222), test loss: 29.6487614155\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.792003035545,2.249065455), test loss: 2.56484115422\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (7.53031539917,19.9876018871), test loss: 41.5186837196\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.75825595856,2.24216761191), test loss: 2.87467095852\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (8.69785022736,19.891105275), test loss: 27.9950860023\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.67771315575,2.23527405626), test loss: 2.65662798584\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (6.66485023499,19.7956658686), test loss: 40.0504996777\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.49383950233,2.2284451564), test loss: 3.00567475557\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (3.67334651947,19.701124872), test loss: 30.901021719\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.05569422245,2.22172878678), test loss: 3.01830741465\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (3.02473187447,19.6078593515), test loss: 37.7825771332\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.395175755024,2.2150903935), test loss: 2.67454019189\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (4.92112922668,19.5153259481), test loss: 37.7880343437\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.86000549793,2.20857691644), test loss: 3.17625067532\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (4.66732692719,19.4238264467), test loss: 35.6366668701\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.953497171402,2.2020776226), test loss: 2.48478459492\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (8.86808776855,19.3331930061), test loss: 40.7784396172\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.75103068352,2.19560929102), test loss: 3.09595141411\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (4.5462808609,19.2434687771), test loss: 30.0635326862\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.18269610405,2.18912837334), test loss: 2.66737835407\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (5.68272256851,19.1545749552), test loss: 40.6341757298\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.56204223633,2.18274232873), test loss: 2.85855021179\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (6.07266330719,19.0663105154), test loss: 31.1388563633\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.22684121132,2.17646210747), test loss: 2.83264332414\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (10.4707508087,18.9787581545), test loss: 40.0374399185\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.53580737114,2.17016213158), test loss: 3.01124348342\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (11.3940391541,18.8922750455), test loss: 32.6873174667\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.66114616394,2.16398417276), test loss: 2.98159924448\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (3.69719028473,18.8064850968), test loss: 40.1835262299\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.643720328808,2.15784747599), test loss: 2.63723663539\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.19911670685,18.7219551209), test loss: 41.3360989571\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.867459356785,2.15182655327), test loss: 3.20526851416\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.39008712769,18.637647051), test loss: 34.4557987213\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.42263674736,2.14587541064), test loss: 2.58304379582\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.03882408142,18.5546514124), test loss: 41.2552414894\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.55081152916,2.1399052276), test loss: 3.0576628983\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (2.19729304314,18.4721480137), test loss: 28.8653805733\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.84516739845,2.13392724895), test loss: 2.5994476527\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (1.375903368,18.3906732519), test loss: 41.2447487354\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.50689804554,2.12804114335), test loss: 2.80501253083\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.51180744171,18.3095900644), test loss: 30.0277391911\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.85685896873,2.12225366592), test loss: 2.88732670248\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (3.56797218323,18.2291645868), test loss: 41.7746554375\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.10890722275,2.11645020234), test loss: 3.06155757904\n",
      "\n",
      "MC # 2, Hype # hyp5, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold4/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (296.07421875,inf), test loss: 181.764491272\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (336.269836426,inf), test loss: 376.617066956\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (32.5001831055,76.6188444948), test loss: 49.0849772453\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.49500632286,53.2683683029), test loss: 3.81923549473\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (14.6329336166,61.7787829318), test loss: 35.5688168764\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.6742515564,28.1805510061), test loss: 3.09525697827\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (40.8344039917,56.8892005574), test loss: 46.9180153847\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.47922420502,19.8089710653), test loss: 3.65812423527\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (28.5433559418,54.2327783787), test loss: 39.1772964716\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.54609799385,15.6089000832), test loss: 3.66953831911\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (17.9966812134,52.6413117773), test loss: 42.3039307594\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.61106514931,13.0914002974), test loss: 3.32655608058\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (43.6054725647,51.4324841733), test loss: 43.3635311127\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.67074775696,11.4045424863), test loss: 3.71231685877\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (38.0133895874,50.5321015902), test loss: 43.3285258532\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.70331001282,10.1944470853), test loss: 2.89641063213\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (162.169311523,49.7660067922), test loss: 44.0206555367\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (7.2636218071,9.28407898242), test loss: 3.63868440688\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (26.7983474731,49.0523550074), test loss: 38.6387841702\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.14574694633,8.57085822812), test loss: 3.14845326543\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (26.7753543854,48.4582361507), test loss: 45.4248595238\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.2269949913,7.99996008168), test loss: 3.48494616151\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (116.428123474,47.8901485592), test loss: 36.4957672358\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.60148620605,7.52752686539), test loss: 2.86553323865\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (76.1035079956,47.3951971618), test loss: 45.5112798691\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.21585822105,7.13341683109), test loss: 3.64661374688\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (25.7289123535,46.8752155397), test loss: 31.3202922821\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.42547130585,6.79771657573), test loss: 2.77149638534\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (50.7463874817,46.3696847929), test loss: 42.2828756332\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.4245262146,6.50899861834), test loss: 3.57767995298\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (15.7373189926,45.8659495435), test loss: 34.4028721094\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.54905593395,6.25533979495), test loss: 3.50996623635\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (53.1202011108,45.3434438607), test loss: 37.8764326334\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (6.18708086014,6.03183792325), test loss: 3.08461522758\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (23.6349868774,44.8010408636), test loss: 37.381084919\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.63047432899,5.83256396718), test loss: 3.63204834461\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.3141403198,44.2310219068), test loss: 35.4379058838\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.477466583252,5.65447913292), test loss: 2.79123394489\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (17.7371025085,43.6366399824), test loss: 37.2762901783\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (0.644896745682,5.4951561411), test loss: 3.69963108003\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (15.601348877,43.0165857613), test loss: 31.6153120995\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.31223678589,5.35031291167), test loss: 2.83114493489\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (23.6596240997,42.3832951102), test loss: 38.1175642967\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.08378100395,5.21874417481), test loss: 3.66567553282\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (25.2426681519,41.7343331725), test loss: 29.1749933243\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.903082430363,5.09616973488), test loss: 3.10885505676\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (37.7542648315,41.0688636047), test loss: 35.873301363\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.48523843288,4.98357299629), test loss: 3.36174398065\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (22.9517784119,40.3984960757), test loss: 24.9479056358\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.66139602661,4.87840113264), test loss: 2.78436171412\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (84.4767227173,39.7419558259), test loss: 35.0450868368\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.89620923996,4.78100816771), test loss: 3.47789683938\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (4.75811004639,39.0880722952), test loss: 27.8657975197\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.46976613998,4.68892580462), test loss: 3.48232589364\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (21.8609676361,38.4526838468), test loss: 34.3603918314\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.3369910717,4.60390543237), test loss: 3.28980302811\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (13.8682489395,37.8294059823), test loss: 28.7805683136\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.529577076435,4.52356668804), test loss: 3.54316877723\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (8.93798446655,37.2317570413), test loss: 29.8037066936\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.834160089493,4.44823575299), test loss: 2.69759363234\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (13.4650821686,36.6472483521), test loss: 30.9548432112\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.89035344124,4.37718796617), test loss: 3.52234634757\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (46.2032852173,36.0840043189), test loss: 29.502523613\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.78230714798,4.30948676234), test loss: 2.7476497829\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (13.7287445068,35.5465266484), test loss: 34.5865336418\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.13562273979,4.24553298119), test loss: 3.45322454572\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (9.67457580566,35.0263581756), test loss: 27.0030229568\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.41713142395,4.18446423477), test loss: 2.74673917443\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (26.0772171021,34.5291237901), test loss: 33.647498703\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.55885350704,4.12658623725), test loss: 3.23881682456\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (33.7392654419,34.0446259857), test loss: 25.1359713554\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.31554961205,4.07137465538), test loss: 2.74271845669\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (31.0242042542,33.5806597491), test loss: 35.356785965\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.30431103706,4.01924012388), test loss: 3.39814620614\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (22.7383155823,33.1365730316), test loss: 27.9575232744\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.89604759216,3.96936699534), test loss: 3.4804788053\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (19.4587287903,32.6994171139), test loss: 34.2980172634\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.51292085648,3.92119107876), test loss: 3.47464545071\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.4449396133,32.2815707534), test loss: 28.0154096603\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.496624022722,3.87513238497), test loss: 3.49762714505\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (5.80771446228,31.8810767128), test loss: 29.618068552\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.25743818283,3.83041603473), test loss: 2.53330224901\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (25.7906608582,31.4933034026), test loss: 31.3696563721\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.48605298996,3.78770460627), test loss: 3.4817404747\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (38.5948410034,31.1132493901), test loss: 29.793374157\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.76951897144,3.74646307712), test loss: 2.61110399812\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (38.4565620422,30.7479748969), test loss: 34.8185365677\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.50438261032,3.70736991527), test loss: 3.43031769097\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (13.9135532379,30.395210993), test loss: 27.8374717474\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.919411063194,3.66926808754), test loss: 2.73074747175\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (7.56086158752,30.0473910984), test loss: 33.7761518002\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.74776637554,3.63260821499), test loss: 3.06906865835\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.2409858704,29.7128179533), test loss: 25.4587560892\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.34548377991,3.59688563547), test loss: 2.66974670291\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (7.95680952072,29.3889185802), test loss: 34.309639883\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.3062492609,3.56235087651), test loss: 3.36179338396\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (8.08448696136,29.0745715482), test loss: 28.22486763\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.922956049442,3.52872092134), test loss: 3.35404099226\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (9.30984687805,28.765860546), test loss: 34.7562537432\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (5.4137377739,3.4964418447), test loss: 3.4949903056\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (3.64140176773,28.4657499423), test loss: 29.3686138153\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.78469097614,3.46504575359), test loss: 3.36446160972\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (11.9096260071,28.1744811223), test loss: 30.85957551\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.14322328568,3.43466078768), test loss: 2.51137802899\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (18.2740440369,27.8890469594), test loss: 30.9544651985\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.62322330475,3.40524496206), test loss: 3.36949640214\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (4.15053892136,27.6104603162), test loss: 30.8688860416\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.648398339748,3.3764199509), test loss: 2.74211352393\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (3.23033285141,27.3408301636), test loss: 34.9714398861\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.22081267834,3.34837499676), test loss: 3.29908287227\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (7.12610721588,27.0769682152), test loss: 28.602452755\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.34360790253,3.32108691061), test loss: 2.59241392612\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (9.63992595673,26.8182912189), test loss: 34.6978946805\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.11459803581,3.29452993869), test loss: 2.90127373934\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (15.9088859558,26.5638662245), test loss: 26.9003129959\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.44432938099,3.2686188669), test loss: 2.58424606025\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (11.2977180481,26.3170170905), test loss: 34.4823455095\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.69568562508,3.24359965475), test loss: 3.18633000851\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (19.3098430634,26.0768011166), test loss: 28.9339756012\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.05620718002,3.21920005421), test loss: 3.35185462236\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (5.76209163666,25.8383961529), test loss: 35.7380679369\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.76180052757,3.1953074055), test loss: 3.33346972764\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (8.96215820312,25.6070900087), test loss: 28.3752566099\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.75042045116,3.17192459308), test loss: 3.31551475525\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (4.59510231018,25.3819574639), test loss: 31.8111395836\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.00516915321,3.14887040141), test loss: 2.44198464155\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.5526380539,25.1603310208), test loss: 30.8566890478\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.994626283646,3.12649415576), test loss: 3.28317134976\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (13.7915658951,24.9412172031), test loss: 33.1725440025\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.81563901901,3.10468567081), test loss: 2.56681385338\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (11.911655426,24.7278910702), test loss: 35.2925538301\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.56371295452,3.08355315978), test loss: 3.24625254571\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (8.19969177246,24.52058734), test loss: 29.7481682777\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.745381057262,3.06277886406), test loss: 2.66975226998\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (11.3772659302,24.3143243158), test loss: 35.3935244322\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.12483310699,3.04259098091), test loss: 2.84374863952\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (9.50483512878,24.1141308856), test loss: 26.6642913818\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.16140985489,3.02262062694), test loss: 2.55810994804\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (6.97933864594,23.9181874955), test loss: 35.5028478384\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.70513176918,3.00309933261), test loss: 3.11701243222\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (10.4296131134,23.7256279096), test loss: 27.825017035\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.75845909119,2.98390066208), test loss: 3.08403444886\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.08111763,23.5351235942), test loss: 36.0686119318\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.16255068779,2.96517469054), test loss: 3.29454721212\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (12.3880844116,23.3488205765), test loss: 31.1446195602\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.878753066063,2.94692824158), test loss: 3.24735612869\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (7.76845550537,23.166063089), test loss: 33.4882602692\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.75641155243,2.92900834909), test loss: 2.4505105257\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (19.7998752594,22.9861436688), test loss: 30.7511547565\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.30414390564,2.91157117245), test loss: 3.27100043148\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (15.6074285507,22.8101845535), test loss: 34.0859311104\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.60704421997,2.89435835561), test loss: 2.63426550627\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.7186574936,22.6374292467), test loss: 36.0512693882\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.05938959122,2.87738216829), test loss: 3.2469073981\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (8.33066558838,22.4676956436), test loss: 31.3935941219\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.3726401329,2.86076481559), test loss: 2.59135754704\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (22.1495914459,22.3002009007), test loss: 36.6142419815\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.2594575882,2.84456908221), test loss: 2.87826365829\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.7660779953,22.1346426288), test loss: 27.8429028034\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.4608591795,2.8285624213), test loss: 2.52428022325\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (8.10429382324,21.9731503336), test loss: 36.1849501133\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.8259049654,2.81290741023), test loss: 3.04609680176\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (7.30711174011,21.8148681537), test loss: 32.9515980959\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.345457285643,2.79770109567), test loss: 3.4205224514\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (5.78395843506,21.6575227815), test loss: 36.9368188381\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.53145158291,2.78260799789), test loss: 3.28157340884\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (3.38325500488,21.5041092682), test loss: 29.4183745384\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.4552731514,2.76783807091), test loss: 3.20929814577\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (4.92650270462,21.3535330375), test loss: 34.0537929058\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.19885396957,2.75311621983), test loss: 2.45989120603\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (7.45589923859,21.2044617974), test loss: 31.1504559755\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.63824236393,2.7387462386), test loss: 3.24201882482\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.59883737564,21.0569406624), test loss: 34.4471960068\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.536112606525,2.72466227055), test loss: 2.38979058266\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (3.46297693253,20.9122378325), test loss: 36.6417807579\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.97414171696,2.71091292565), test loss: 3.19204211235\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (9.36424732208,20.7707266349), test loss: 32.3494253159\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.450160831213,2.69738254016), test loss: 2.67281716466\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (6.09660482407,20.6296459236), test loss: 36.8058646202\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.38494575024,2.68411036415), test loss: 2.87948642373\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (6.76396465302,20.4923382739), test loss: 28.6267726421\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.41534328461,2.67095898453), test loss: 2.53512580097\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (10.3847675323,20.3566993848), test loss: 36.270681572\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.38825130463,2.65791577712), test loss: 2.91820608377\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (11.9565601349,20.2231479878), test loss: 30.0248735905\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.7635178566,2.64510761374), test loss: 3.12275116146\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (9.39350414276,20.090358939), test loss: 37.570284462\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.434341609478,2.63260749912), test loss: 3.2126046896\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (10.9033727646,19.9601821877), test loss: 29.9260303974\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.65234327316,2.62030512074), test loss: 3.22381337881\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (6.31668424606,19.8322428189), test loss: 34.7728230238\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.85486125946,2.60815298481), test loss: 2.50564247072\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (5.76780414581,19.7053032407), test loss: 31.8361361265\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.542772710323,2.59630177883), test loss: 3.23499155045\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (3.60739398003,19.5812409632), test loss: 34.5575041294\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.45407140255,2.58450096249), test loss: 2.37096441388\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (6.52253293991,19.4585414913), test loss: 37.6683512926\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.620844244957,2.57282433915), test loss: 3.22327933311\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (12.053563118,19.3377629698), test loss: 33.8488336086\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (3.4925236702,2.5613536505), test loss: 2.64175031036\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (8.51887607574,19.2179899644), test loss: 37.7272649765\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.84438669682,2.55014664629), test loss: 2.90254618526\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.95505619049,19.099858646), test loss: 30.1866028309\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.685054719448,2.53895303376), test loss: 2.52291341051\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (5.93308448792,18.9835199349), test loss: 37.4586856365\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.964597821236,2.52800139275), test loss: 2.88828888535\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (15.6453018188,18.8691843646), test loss: 35.8796662331\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.0651601553,2.51732615303), test loss: 3.52087197602\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (2.8668794632,18.7556488441), test loss: 38.2329635143\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.27442848682,2.50668103303), test loss: 3.23829749823\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (8.08569908142,18.6442541219), test loss: 30.8783372402\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (4.57180643082,2.49617541991), test loss: 3.36979363561\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (8.0138835907,18.5344951732), test loss: 35.811191988\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.831101834774,2.48571521408), test loss: 2.65234701782\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.15885925293,18.4255933967), test loss: 32.6728420258\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.704572677612,2.47553018501), test loss: 3.24200487435\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (3.80538153648,18.31773553), test loss: 36.109138298\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.5861979723,2.46539425229), test loss: 2.39373223633\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (3.7164850235,18.2118880991), test loss: 37.044052434\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.14980530739,2.45551663378), test loss: 3.19250796139\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (4.47856378555,18.1074832504), test loss: 35.2966855049\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.99426388741,2.44571941537), test loss: 2.67827028632\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (7.43513774872,18.0037447125), test loss: 38.8618567467\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.781613767147,2.43610982535), test loss: 2.98514014184\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (15.5354156494,17.9022285894), test loss: 30.4165605068\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.82236123085,2.42648579412), test loss: 2.53671923131\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (3.30925035477,17.8015679141), test loss: 38.378158617\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.688644170761,2.41694895576), test loss: 2.96431157142\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.34949111938,17.7024020959), test loss: 31.9444952965\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.662259757519,2.4076276529), test loss: 3.14281588793\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (5.9062333107,17.6036644678), test loss: 39.330245018\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.94406604767,2.39844587905), test loss: 3.25572029352\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.56131839752,17.5064416834), test loss: 30.2382111073\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.33815813065,2.38937108762), test loss: 3.24499958754\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.69881439209,17.4106782272), test loss: 35.5676674604\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.13789105415,2.38036980331), test loss: 2.69472426921\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (4.841837883,17.3157774218), test loss: 33.0310544014\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.81700444221,2.37161447935), test loss: 3.25470549464\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (3.99039888382,17.2225151027), test loss: 35.6687161446\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.492696791887,2.36282046925), test loss: 2.4090061754\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.95561408997,17.1300803569), test loss: 38.344566226\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.827848255634,2.35410070054), test loss: 3.23551406264\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (5.75051164627,17.0389025401), test loss: 37.3166216373\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.39309206605,2.34550492496), test loss: 2.77132157683\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (5.32570648193,16.948366732), test loss: 40.7589546442\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.2126969099,2.33708282979), test loss: 3.11753082722\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (4.92781543732,16.8591168561), test loss: 33.9366501808\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.01954698563,2.32871491574), test loss: 2.72054524124\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (7.33413362503,16.7707961255), test loss: 39.1373351336\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.80234801769,2.32039937639), test loss: 2.922010573\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (12.7384567261,16.683666437), test loss: 33.8044121265\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.855603933334,2.31231861408), test loss: 3.15112618804\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (4.73533916473,16.5975051648), test loss: 39.4451151371\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.05696725845,2.30424179667), test loss: 3.22766298652\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (4.3589220047,16.5123053954), test loss: 30.546147418\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.17993307114,2.29617286771), test loss: 3.21034930944\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (10.9158468246,16.4284734996), test loss: 36.3091193676\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.34048295021,2.28827796767), test loss: 2.68564759791\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (3.96486568451,16.3450382337), test loss: 33.8743998051\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.37479281425,2.28047007338), test loss: 3.24174512327\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (3.46158099174,16.2623776293), test loss: 36.7070595264\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.64600533247,2.27270892963), test loss: 2.39633413553\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (5.155854702,16.1808876109), test loss: 37.8481763363\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.03330636024,2.26507253452), test loss: 3.21773906648\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (5.01059818268,16.1005927747), test loss: 37.4654301167\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.900555789471,2.25753410373), test loss: 2.74947078824\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (3.92475032806,16.0205364365), test loss: 39.3038323164\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.986131072044,2.25013212094), test loss: 3.2307928592\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (4.02763462067,15.9419360247), test loss: 31.390012598\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.875201940536,2.24266537202), test loss: 2.64462370872\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (2.84623003006,15.8642685278), test loss: 39.640947628\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.61331641674,2.23528615972), test loss: 2.94384373724\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (4.94759464264,15.7872217416), test loss: 34.0534011841\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.236078560352,2.22800527096), test loss: 3.11134285927\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (9.32699775696,15.7107828931), test loss: 39.1452628613\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.26307296753,2.22084051033), test loss: 3.21218467355\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (8.08837890625,15.6351881668), test loss: 30.8317874908\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.12161636353,2.21373619889), test loss: 3.29335814416\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (1.98570251465,15.5605924556), test loss: 36.2494729996\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.397420495749,2.20669445932), test loss: 2.75865370631\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (10.1498088837,15.4866891996), test loss: 33.7434234619\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.966429710388,2.19981517462), test loss: 3.24374015927\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (3.71885299683,15.4137739004), test loss: 36.4343030453\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.4979736805,2.19289329208), test loss: 2.40521561503\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (3.41964769363,15.3415130779), test loss: 38.4059765816\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.950458884239,2.18605127327), test loss: 3.25928869247\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (3.88854718208,15.2700910635), test loss: 36.426363945\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.20776665211,2.17925912248), test loss: 2.70181055665\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (4.93332099915,15.1990724183), test loss: 39.1554813862\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.69570493698,2.17261254849), test loss: 3.14748040438\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.99518108368,15.1288977997), test loss: 32.8941449642\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.4202080965,2.16594438083), test loss: 2.6815087527\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (5.44668769836,15.0595105398), test loss: 41.3333918095\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.74595528841,2.15936117167), test loss: 2.98978520632\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (3.8469145298,14.990788066), test loss: 33.013758707\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.2997597456,2.15295622957), test loss: 3.03508425057\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (2.77804017067,14.9228940322), test loss: 39.4808440685\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.74966311455,2.14649400472), test loss: 3.26353386045\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.02192306519,14.8556349066), test loss: 31.4503780603\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.26418733597,2.14007997279), test loss: 3.26915231645\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (1.94012594223,14.7891782105), test loss: 36.1274128914\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.563405990601,2.13374374548), test loss: 2.66767315269\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (5.73943328857,14.7232641151), test loss: 34.8547219276\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.936743676662,2.1275143732), test loss: 3.26542323828\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (4.51836299896,14.6577678117), test loss: 37.0209412575\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.58493471146,2.12127359978), test loss: 2.34834995568\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (3.49725914001,14.5930461928), test loss: 39.0465349913\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.288647115231,2.11513296267), test loss: 3.23922367692\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (10.6223983765,14.5292590568), test loss: 38.6532138348\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.841260433197,2.1090723083), test loss: 2.79124448597\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.33681750298,14.4655820705), test loss: 39.2263452053\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.14051413536,2.10308372254), test loss: 3.13029161692\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (4.16508436203,14.4029858246), test loss: 31.5509253025\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.865605831146,2.09707990066), test loss: 2.66935394034\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (2.33651542664,14.3410446651), test loss: 40.5306471348\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.18272435665,2.09109164161), test loss: 2.98545101285\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (3.88056850433,14.279470722), test loss: 33.0671210766\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.03969168663,2.08522737009), test loss: 3.05148593187\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (8.09086608887,14.2182705547), test loss: 39.835791707\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.771305561066,2.07937919294), test loss: 3.17407073677\n",
      "\n",
      "MC # 2, Hype # hyp5, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold5/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (384.482147217,inf), test loss: 216.319611359\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (341.257598877,inf), test loss: 405.680145264\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (49.0977745056,174.997797563), test loss: 48.4025504112\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.92289304733,180.358240162), test loss: 3.69853327274\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (171.404632568,112.04063971), test loss: 43.1897044182\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.37553596497,91.8217972546), test loss: 3.75184585452\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (57.4091949463,90.5966707013), test loss: 44.3675187588\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.05434131622,62.2985130786), test loss: 3.49774095416\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (46.4546012878,79.8617190964), test loss: 45.8887644768\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.33994948864,47.523636389), test loss: 3.77696683407\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (45.6757011414,73.4351415213), test loss: 42.675476408\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.5748257637,38.6567114024), test loss: 3.08871402144\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (51.3576316833,69.195338532), test loss: 49.3779676437\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.46765565872,32.7492730492), test loss: 4.08083072305\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (19.1034259796,66.0670053301), test loss: 39.2586168051\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.1934158802,28.5257428649), test loss: 2.95068919659\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (7.69381999969,63.7106954917), test loss: 47.6601406574\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.68998670578,25.3637976375), test loss: 3.77956718802\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (23.5917892456,61.8602630612), test loss: 35.6809835434\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.96951794624,22.89863126), test loss: 3.08283794522\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (25.7403621674,60.3579852029), test loss: 42.7005253792\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.58050584793,20.9282794839), test loss: 3.46883585453\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (36.0973358154,59.0957903806), test loss: 42.3518088341\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.8006503582,19.312508778), test loss: 3.64667485356\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (12.1977844238,57.9937461147), test loss: 44.3300306082\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.95769882202,17.9629890928), test loss: 3.44290421903\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (33.978843689,57.1020502744), test loss: 46.9226125717\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (6.76939344406,16.8206072007), test loss: 3.67274723947\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (7.46396112442,56.2812028411), test loss: 41.7740646362\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.39197254181,15.8385511756), test loss: 2.92118133008\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (26.2698135376,55.5801665195), test loss: 46.9187512398\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.97641336918,14.9869099181), test loss: 4.04264284968\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (45.1726531982,54.9501544347), test loss: 36.8434031487\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (0.939935982227,14.2425161952), test loss: 2.91550580263\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (33.1677589417,54.3637566683), test loss: 46.5473065376\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.553367495537,13.5841669278), test loss: 3.71483951211\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (28.3433876038,53.8209274536), test loss: 34.948790741\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.44866478443,12.9991447115), test loss: 2.79410965443\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (59.6087722778,53.3230194697), test loss: 40.8544371605\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.93105530739,12.4739217232), test loss: 3.43680268526\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (96.5606842041,52.8413509735), test loss: 41.0516746759\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.65549135208,12.0006741172), test loss: 3.65393659472\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (92.0799865723,52.3884739822), test loss: 42.9279281139\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.48094511032,11.5715599733), test loss: 3.4505487442\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (26.4431190491,51.9390690777), test loss: 45.7657493114\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.06203269958,11.1806654066), test loss: 3.78144695759\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (175.997940063,51.5278541143), test loss: 39.042111063\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (6.17502689362,10.822199735), test loss: 2.64281782508\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (19.1277542114,51.095233481), test loss: 44.1508460045\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.87652873993,10.4925938196), test loss: 3.7864405334\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (23.5655593872,50.6930807557), test loss: 34.8555991411\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.79446053505,10.1889542999), test loss: 2.82176656425\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (16.9499893188,50.2751565222), test loss: 41.8993221283\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (4.12148427963,9.90741799673), test loss: 3.54929866195\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (25.4876861572,49.8627963419), test loss: 32.3171047688\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.9283156395,9.64701430142), test loss: 2.66564372182\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (34.7473907471,49.4477691302), test loss: 37.8104475021\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (7.01562404633,9.40313184668), test loss: 3.31928303838\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (16.1676635742,49.0263800549), test loss: 37.9104408264\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.0165207386,9.17540887436), test loss: 3.47684898973\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (35.3043785095,48.5944039757), test loss: 32.634982276\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.55587530136,8.96140671863), test loss: 2.77231027484\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (81.2755432129,48.1483640111), test loss: 41.8508298874\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.35105776787,8.7602386001), test loss: 3.79287163019\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (31.5461425781,47.707157793), test loss: 33.0680726051\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.42042374611,8.57057697127), test loss: 2.59954611361\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (23.377243042,47.2481852605), test loss: 40.5171597958\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.91452431679,8.39075107895), test loss: 3.55880489349\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (8.53074645996,46.7858062921), test loss: 29.2750228882\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.38473701477,8.22053207039), test loss: 2.8078201741\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (30.1554527283,46.31574128), test loss: 36.4071593046\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.63640975952,8.05975844522), test loss: 3.43742639422\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (12.1744680405,45.8295887167), test loss: 26.7401711464\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.3487251997,7.90640124022), test loss: 2.68444721699\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (18.3101024628,45.3376319157), test loss: 33.1332891941\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.16739082336,7.76097642821), test loss: 3.28625097871\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (12.3791294098,44.8434870701), test loss: 33.4403679848\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.83143424988,7.62213956956), test loss: 3.37658469826\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (68.5169525146,44.3422331538), test loss: 29.3004778385\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (5.06159591675,7.48972959649), test loss: 2.64384606481\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (38.3525161743,43.8349842829), test loss: 34.4971167088\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.78586268425,7.36280020879), test loss: 3.63398332596\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (11.4725484848,43.3293352133), test loss: 30.0966534615\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.53048944473,7.24149471267), test loss: 2.60335013121\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (20.3996562958,42.8356077374), test loss: 36.4398377657\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.44661223888,7.12500107634), test loss: 3.56643443704\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (12.2495613098,42.3420819841), test loss: 26.9105836153\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.43309044838,7.01336549888), test loss: 2.78069573343\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (21.8742752075,41.8607043618), test loss: 33.4296686649\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.05971622467,6.90641399828), test loss: 3.40709093213\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (30.9405899048,41.3810980807), test loss: 25.8525195599\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.29723882675,6.80356682258), test loss: 3.14265654236\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.9583625793,40.9113104417), test loss: 33.3090681553\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.763337016106,6.70527369499), test loss: 3.25607713908\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (10.8609333038,40.4513159051), test loss: 31.3620224714\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.77670812607,6.61063142512), test loss: 3.36659409106\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.2688922882,40.0029230609), test loss: 30.2506402493\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.01873016357,6.51956518341), test loss: 2.70153569728\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (7.8653345108,39.5593350636), test loss: 34.0733653665\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.06067705154,6.4316936274), test loss: 3.45975387692\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (6.51286458969,39.1255746574), test loss: 30.4493882895\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.684474468231,6.34683704876), test loss: 2.76196377575\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (12.3808689117,38.7058091993), test loss: 35.3113298416\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.13975214958,6.26510906938), test loss: 3.39717380702\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (5.84516620636,38.2947443239), test loss: 27.8283466339\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.19612240791,6.18587127143), test loss: 2.77945066988\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (6.1859292984,37.8924269724), test loss: 35.4960860729\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.29723846912,6.10931935022), test loss: 3.35160480738\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (12.7696676254,37.5005169467), test loss: 30.5977961302\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.42105150223,6.03569627047), test loss: 3.12710135579\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (7.35314321518,37.113454591), test loss: 35.2024163127\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.6264257431,5.96412531772), test loss: 3.27934152782\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (41.5547561646,36.7381484067), test loss: 30.1411052227\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.79634714127,5.89529952752), test loss: 3.30898491144\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (10.1376619339,36.3710914766), test loss: 33.7883677006\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.59667396545,5.82847878686), test loss: 2.68544100523\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (20.7520942688,36.0118860962), test loss: 37.0333413601\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.32631039619,5.76366097729), test loss: 3.3795918107\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (18.2401657104,35.6574305569), test loss: 33.5137143135\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.987110376358,5.70065136215), test loss: 2.75531941354\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (6.22673034668,35.3115470615), test loss: 37.2062027216\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.43328034878,5.63948088975), test loss: 3.24242474139\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (14.7657337189,34.9763068695), test loss: 28.0439689159\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.901110291481,5.57994733162), test loss: 2.76636794806\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (31.7537937164,34.6459249008), test loss: 35.8303785324\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.23133707047,5.52212745363), test loss: 3.29622052759\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (6.33877706528,34.3221411075), test loss: 30.1258521557\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.406342923641,5.46593021008), test loss: 3.19407106638\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (27.2313575745,34.0034742379), test loss: 37.0349582195\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.72015452385,5.41120017205), test loss: 3.39064260125\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (18.9397220612,33.6913569099), test loss: 34.9375006199\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.12484300137,5.35817904282), test loss: 3.33588971794\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.8450088501,33.3859176698), test loss: 35.8357789993\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.25695705414,5.30653775875), test loss: 2.64795931131\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (12.5546274185,33.0876312066), test loss: 35.8060556889\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.64646410942,5.25627506496), test loss: 3.26954425573\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (12.3544836044,32.7925623522), test loss: 34.0165590763\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.86249983311,5.20719528419), test loss: 2.80715855807\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (9.27100849152,32.5034904207), test loss: 39.0399971247\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.63115358353,5.1592678287), test loss: 3.30722665638\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (4.9213681221,32.2210914605), test loss: 28.3442577839\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.18254375458,5.112606285), test loss: 2.57017327547\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (5.77454471588,31.9438789213), test loss: 36.5789957047\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.22518789768,5.06678988746), test loss: 3.2847683996\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (9.88334274292,31.6713054614), test loss: 30.6680487156\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.71740889549,5.02220939621), test loss: 3.18005354404\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (6.26325750351,31.4036689887), test loss: 38.6048775673\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.55398201942,4.97880513862), test loss: 3.22698206455\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (6.13971853256,31.138971197), test loss: 34.4374804974\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.606413543224,4.93617281261), test loss: 3.32309128642\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (12.1034326553,30.8807104087), test loss: 35.1448762655\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.907662391663,4.89487180393), test loss: 2.5905462116\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (8.52042007446,30.6271985552), test loss: 40.5041805267\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.813792645931,4.85440324885), test loss: 3.28242937326\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (9.02399253845,30.3782437558), test loss: 38.8833099365\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.641261875629,4.81484137338), test loss: 2.9417227298\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (14.0501651764,30.1320507525), test loss: 39.7050190449\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.32763290405,4.77614303218), test loss: 3.14165810049\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (10.3265857697,29.8911319464), test loss: 28.5455313206\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.23266410828,4.73814347477), test loss: 2.57093307078\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (9.89004516602,29.6552215098), test loss: 38.1244996548\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.01442670822,4.70091829585), test loss: 3.29567822814\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (16.1257553101,29.4229383921), test loss: 28.8142615557\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.25178134441,4.66452361278), test loss: 3.00645063072\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (8.36174869537,29.1937012545), test loss: 35.2738986969\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.375225067139,4.62889271086), test loss: 2.91062655002\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (6.96851444244,28.9679372253), test loss: 33.2736205101\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.02224779129,4.59396345716), test loss: 3.2582041502\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (10.8147678375,28.7460638033), test loss: 36.1853621483\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.716414690018,4.55984398718), test loss: 2.58988047242\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (9.30633449554,28.5280569333), test loss: 43.4434453487\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.491496026516,4.52642823646), test loss: 3.29990904033\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (9.7040348053,28.3148082322), test loss: 30.485184288\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.551026642323,4.49371578265), test loss: 2.75260289907\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (10.5467882156,28.1034168581), test loss: 39.5841570616\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.360850870609,4.4616117845), test loss: 3.05487269461\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (11.464931488,27.8963420129), test loss: 27.9455466747\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.95368099213,4.43011839426), test loss: 2.60324320793\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (9.44676589966,27.6925845463), test loss: 38.6084854841\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.52875292301,4.39919021046), test loss: 3.2281343773\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (12.9658355713,27.4925081576), test loss: 30.2533171177\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.31820940971,4.36875884103), test loss: 2.99864943326\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (15.2140827179,27.2951240695), test loss: 37.3154417038\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.830064594746,4.33889573549), test loss: 2.79210882485\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (13.9478187561,27.1005791329), test loss: 36.3201711655\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.95765519142,4.3097253325), test loss: 3.17383722663\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (5.64247226715,26.9081974178), test loss: 35.8119919777\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.51488780975,4.28093361487), test loss: 2.62136951685\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (13.2069301605,26.719866233), test loss: 40.8460293293\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.31482708454,4.25285490229), test loss: 3.3167296052\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (4.76319408417,26.5343764534), test loss: 32.5686790466\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.59773504734,4.22523032344), test loss: 2.74235423058\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (10.4035081863,26.3522961787), test loss: 42.0558804035\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.27602148056,4.19817196862), test loss: 3.12293351591\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (4.16312789917,26.1716768632), test loss: 29.8009518623\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.430839270353,4.17152099706), test loss: 2.97437899113\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (14.7946062088,25.9950832156), test loss: 39.0773444176\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.68465971947,4.14529769313), test loss: 3.15360153019\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (4.91417312622,25.8206013226), test loss: 31.4958100796\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.54987084866,4.11943510578), test loss: 3.06277570128\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (11.5058269501,25.6492119136), test loss: 38.0869331837\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.68589496613,4.0941407689), test loss: 2.69528963268\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (15.9566650391,25.4793918814), test loss: 38.7858942509\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.82564628124,4.0692064382), test loss: 3.19596644044\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (11.7006187439,25.3118864715), test loss: 35.9018076658\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (4.49547338486,4.04471922947), test loss: 2.75239089876\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (7.58840703964,25.1467839666), test loss: 42.2458180904\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.50458049774,4.02062027001), test loss: 3.19765392393\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.3119907379,24.9842247216), test loss: 30.9179585457\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.17621445656,3.99697830025), test loss: 2.74979066253\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (3.73487758636,24.8245829738), test loss: 40.0363799095\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.1259329319,3.97373436314), test loss: 3.11649236679\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (9.23310661316,24.6664175803), test loss: 30.8683431625\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.35298717022,3.95087125843), test loss: 2.84101984501\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.56170082092,24.5109150688), test loss: 40.6376921654\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.99405241013,3.92835533906), test loss: 3.19081151187\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.62988376617,24.35745666), test loss: 31.857363987\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.97558665276,3.90617002602), test loss: 3.0368832469\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (8.00979137421,24.2064756102), test loss: 39.238448\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.68732309341,3.88425247224), test loss: 2.74287916422\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (5.26303958893,24.0570365996), test loss: 40.1668995142\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.26309025288,3.86269408506), test loss: 3.13778981566\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (9.99395751953,23.9096758177), test loss: 38.0502195835\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.45515680313,3.84156261326), test loss: 2.81113087684\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (11.9463863373,23.763595054), test loss: 42.1577285767\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.897508323193,3.82059321834), test loss: 3.07130164802\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (12.9245281219,23.6201176092), test loss: 29.3577764034\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.5407961607,3.80011745078), test loss: 2.68734706938\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (2.64570093155,23.4784191891), test loss: 43.7372759342\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.682308375835,3.77987210529), test loss: 3.13878010064\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (15.150226593,23.3392180932), test loss: 31.129418087\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.05683267117,3.76002298869), test loss: 2.94971467257\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (3.93653011322,23.2007758142), test loss: 42.3907546997\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.28838706017,3.7404218185), test loss: 3.26708753705\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (4.18054771423,23.0650391035), test loss: 33.9696205139\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.705912351608,3.72105052834), test loss: 3.13668715656\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (6.12197589874,22.9307047946), test loss: 38.5946233749\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.901036560535,3.7018964276), test loss: 2.69380944371\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.6831870079,22.7984012144), test loss: 40.5496863365\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.58324623108,3.68311854478), test loss: 3.1258526206\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (11.1960306168,22.6671919834), test loss: 36.9083857059\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.19988203049,3.6646054792), test loss: 2.94307866693\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (8.12203121185,22.5375054072), test loss: 43.9177636623\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.30380249023,3.6462914512), test loss: 3.22685753405\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.81776428223,22.4094627711), test loss: 31.1895171642\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.21442699432,3.62823488715), test loss: 2.5783172369\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (3.44921445847,22.2830086343), test loss: 40.7694693089\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.75405585766,3.61049019632), test loss: 3.18144675791\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (3.35072755814,22.1586175827), test loss: 31.1702819824\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.473371595144,3.59297906632), test loss: 3.00396431386\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (9.83016586304,22.0354262736), test loss: 42.7162325382\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.86394035816,3.57575709824), test loss: 3.12393991649\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (3.03715658188,21.9138778053), test loss: 34.8771476746\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.14727258682,3.55872223942), test loss: 3.18326213956\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.22466087341,21.7937949687), test loss: 38.8968619347\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.884754359722,3.54189790192), test loss: 2.57638769448\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (3.57037878036,21.6753697236), test loss: 42.6294829607\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.05090618134,3.52525096192), test loss: 3.2036178112\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (6.22404146194,21.5580951734), test loss: 33.5917620659\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.31828820705,3.50883458507), test loss: 2.85286088884\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (7.52358913422,21.4421365396), test loss: 42.7885367393\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.954927444458,3.49269426127), test loss: 3.01148570254\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (8.69589614868,21.3270747484), test loss: 30.8439685822\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.29730260372,3.4766456088), test loss: 2.70781702101\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (13.7043333054,21.2139456572), test loss: 42.8348354816\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.58565223217,3.46092469247), test loss: 3.20227818489\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (5.07229232788,21.1018267392), test loss: 33.7618904114\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.403124630451,3.44535893704), test loss: 2.9409525156\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (4.64738178253,20.9915567227), test loss: 39.9922911644\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.751384139061,3.43005893013), test loss: 2.98167244792\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (9.90632343292,20.8819385393), test loss: 33.8460505486\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.564877450466,3.41494060286), test loss: 3.14368447959\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (2.98802185059,20.7740059994), test loss: 38.7874986172\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.764080107212,3.39995157344), test loss: 2.65990346968\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (4.33941173553,20.6671647566), test loss: 42.7013261795\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.757853508,3.38510154808), test loss: 3.26780948639\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (4.17948532104,20.5617332993), test loss: 31.917624855\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.507122755051,3.37050368817), test loss: 2.79920087457\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (3.44149184227,20.4571150433), test loss: 41.4542895317\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.502584517002,3.35608834028), test loss: 2.93475975394\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (6.49413967133,20.3534370387), test loss: 29.5440246105\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.52274179459,3.34179954898), test loss: 2.77311721146\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (5.70869636536,20.2511421048), test loss: 43.5262449741\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.59769976139,3.32769381454), test loss: 3.14312133491\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (4.29103183746,20.149805676), test loss: 31.1840842247\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.44627070427,3.3137797663), test loss: 2.96902657449\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (5.24946260452,20.049968889), test loss: 40.3085000992\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.951520621777,3.30004191282), test loss: 2.845554021\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (3.26666736603,19.9510219593), test loss: 37.4763960361\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.30380451679,3.28650780901), test loss: 3.14035810828\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (6.39578390121,19.8532246402), test loss: 38.610760355\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.798914670944,3.27308084712), test loss: 2.74737852812\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (13.723493576,19.7566049167), test loss: 43.0810404301\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.994286417961,3.25980576005), test loss: 3.25439302325\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (4.30740070343,19.6609042296), test loss: 32.8420843124\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.10931110382,3.24662856317), test loss: 2.89839570224\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (9.94205474854,19.566279135), test loss: 42.0874094963\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.45123815536,3.2336651771), test loss: 2.98325368166\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (5.79437828064,19.4724206791), test loss: 32.2537797213\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.588052868843,3.22082655349), test loss: 2.97661885917\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (10.8610677719,19.3793618608), test loss: 42.4622910976\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.19484066963,3.20807157739), test loss: 3.17036964595\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.65698194504,19.2875535554), test loss: 35.6588709831\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.631714820862,3.19554178521), test loss: 3.0442948103\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (7.22519779205,19.196559771), test loss: 41.4054726601\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.800595283508,3.18312282254), test loss: 2.73580431342\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (3.31042838097,19.1068504271), test loss: 41.1148668766\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.57272553444,3.17090544045), test loss: 3.19334610701\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (2.39891338348,19.0176903943), test loss: 39.3016957521\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.46635770798,3.15880925355), test loss: 2.87897681892\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.27618694305,18.9297953792), test loss: 42.4407235622\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.71377253532,3.14679385975), test loss: 3.16414867342\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (3.85720825195,18.8426161328), test loss: 30.9777257919\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.74267458916,3.13487189193), test loss: 2.81834052801\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.65336704254,18.7565429436), test loss: 41.7317591667\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.51809477806,3.12312453763), test loss: 3.0420637399\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.80303812027,18.6710379533), test loss: 32.3812113285\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.43634468317,3.11149962303), test loss: 2.82807187438\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (7.16172504425,18.5862338541), test loss: 42.9511710644\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.75846326351,3.0999776151), test loss: 3.20631173849\n",
      "run time for single CV loop: 7076.54307795\n",
      "\n",
      "MC # 2, Hype # hyp4, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold1/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (372.580444336,inf), test loss: 212.771631241\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (272.157623291,inf), test loss: 334.275019836\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (46.8810768127,120.606959458), test loss: 42.9528451443\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.95812892914,113.880949077), test loss: 3.11469427943\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (49.8987045288,83.8848098152), test loss: 35.6269237041\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.90744042397,58.5619769265), test loss: 2.85841178894\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (71.0119094849,71.5160970887), test loss: 42.0980511427\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.22318458557,40.1096659182), test loss: 3.25549689531\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (34.5670776367,65.4380902702), test loss: 39.5694489002\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.735875070095,30.8885552855), test loss: 3.1441459775\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (25.2128200531,61.7415404098), test loss: 41.9127987623\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.31175518036,25.3590213529), test loss: 3.19799457192\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (59.1416625977,59.2064808462), test loss: 41.4455783367\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.16667795181,21.6656847486), test loss: 3.49577471614\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (43.8181037903,57.3562778773), test loss: 39.6396584511\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.82137870789,19.0265701812), test loss: 2.8482016623\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (13.6073894501,55.954097981), test loss: 42.3894915104\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.98209881783,17.045791762), test loss: 3.32321969867\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (37.0985870361,54.8373863094), test loss: 41.7929267883\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.35866975784,15.5010507939), test loss: 2.75228148103\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (57.7066497803,54.0142984399), test loss: 42.0448659897\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.5009727478,14.2696893632), test loss: 3.55593512654\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (38.409614563,53.2870958259), test loss: 41.4104290485\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.11810421944,13.261549379), test loss: 2.72179521322\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (34.2057113647,52.670648898), test loss: 41.9258356094\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.31023478508,12.4198101097), test loss: 3.33943922222\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (31.5147590637,52.0966485351), test loss: 41.6400261879\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.28934431076,11.7063276566), test loss: 3.03494051099\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (67.7317504883,51.6410178475), test loss: 42.8808042526\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.20752739906,11.0940296231), test loss: 3.349439013\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (28.2761459351,51.2164015759), test loss: 39.9723022938\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.17639493942,10.5622908459), test loss: 2.92787002027\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (74.3211975098,50.8662019434), test loss: 43.3346024513\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.33922493458,10.0984802751), test loss: 3.12606053948\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (51.0230369568,50.5348288812), test loss: 37.4046580315\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.46655225754,9.68991994777), test loss: 2.94940853417\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (111.911819458,50.2045446372), test loss: 40.846043396\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (6.94034671783,9.3289625905), test loss: 3.0483074069\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (16.3546886444,49.8443660497), test loss: 33.5111021042\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.42244505882,9.00748340916), test loss: 2.90682459325\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (61.3229827881,49.5461433388), test loss: 40.3317300797\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.19678926468,8.71779498427), test loss: 3.21875228882\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (37.328453064,49.2760224979), test loss: 38.0496409416\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.14696931839,8.4549021226), test loss: 3.30259546936\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (29.5277061462,49.0177639278), test loss: 42.0565496445\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.36709284782,8.21778025302), test loss: 3.05324083567\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (26.4529590607,48.7711345874), test loss: 40.953364706\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.79008328915,7.99987230118), test loss: 3.50925353765\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (39.4454650879,48.5157320385), test loss: 38.4368271351\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.67893433571,7.79958334902), test loss: 2.75447591543\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (36.2750549316,48.270571933), test loss: 42.6140007019\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.1816649437,7.61547249333), test loss: 3.45070028305\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (51.3202018738,48.0420084227), test loss: 39.4756058931\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.16015815735,7.44376774033), test loss: 2.69687075615\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (27.2924079895,47.8359320178), test loss: 40.0390072346\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.57187581062,7.28532321839), test loss: 3.50376706719\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (50.4800186157,47.6244211538), test loss: 38.452822876\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.97625732422,7.13886864467), test loss: 2.72962220907\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (58.834312439,47.4194503196), test loss: 39.0458065987\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (5.33645868301,7.00122508826), test loss: 3.21409662664\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (46.8989143372,47.2031607787), test loss: 38.142691493\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.86688816547,6.87233721395), test loss: 2.93397522271\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (22.185213089,46.9862512912), test loss: 41.0038469791\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.50779104233,6.75140723438), test loss: 3.23684717417\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (42.5593681335,46.7808784393), test loss: 36.3454958916\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.89815115929,6.63676170652), test loss: 2.84955563843\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (30.9972667694,46.5833606354), test loss: 39.4464338303\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (5.27711963654,6.52953304882), test loss: 3.11286836267\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (30.7675666809,46.3847758383), test loss: 32.3720738888\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.85759425163,6.42865872894), test loss: 2.68544696867\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (44.7920761108,46.1801910034), test loss: 37.6055110455\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.02615380287,6.33223858339), test loss: 2.99569180906\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (31.1851806641,45.9685864485), test loss: 35.3883304119\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.76770353317,6.24125961093), test loss: 3.07586819232\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (19.497095108,45.7525561379), test loss: 37.9164153099\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.29654359818,6.15427691436), test loss: 3.03323334455\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (27.0590267181,45.535267555), test loss: 36.3145916462\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (5.37521219254,6.07053337546), test loss: 3.16613471508\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (22.5566825867,45.3275268406), test loss: 34.6853118658\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.65843582153,5.99142569718), test loss: 2.69868899286\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (37.0967483521,45.1115629613), test loss: 36.6918321609\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.53069400787,5.91588385103), test loss: 3.17656380534\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (20.6147270203,44.8836395445), test loss: 35.7590587378\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.71857500076,5.84301285977), test loss: 2.5630882889\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (58.9491653442,44.6477677374), test loss: 37.1146701336\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.52518868446,5.77287683231), test loss: 3.4095189333\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (18.9364910126,44.4084274641), test loss: 34.3701337337\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.52437829971,5.70529827858), test loss: 2.52895055711\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (11.5696659088,44.1584417795), test loss: 36.5905690193\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.2297949791,5.63995841989), test loss: 3.30192514062\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (27.2533988953,43.9122921368), test loss: 33.6832016945\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.70204937458,5.57743054812), test loss: 2.72399470806\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (40.098274231,43.6560512809), test loss: 36.3756005526\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.04144024849,5.51737483178), test loss: 3.1173682332\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (53.0890960693,43.390119429), test loss: 31.8504452229\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.62730741501,5.45864199418), test loss: 2.80929647684\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (25.9030570984,43.1035749624), test loss: 35.7335144997\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.95356893539,5.40150351472), test loss: 2.98502811193\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (73.6459121704,42.8234516635), test loss: 29.8978088379\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (5.36683559418,5.34601706636), test loss: 2.76324379444\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (24.0808448792,42.5279322083), test loss: 32.0970118523\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (7.28835058212,5.29175842836), test loss: 2.86918148398\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (20.3200855255,42.2230459419), test loss: 25.4371030331\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.54275214672,5.23921275515), test loss: 2.52992226481\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (6.91194677353,41.9011150123), test loss: 30.2965620995\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (5.83467102051,5.18809293138), test loss: 2.801207304\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (17.5111351013,41.5647963845), test loss: 28.8126274586\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.12451481819,5.13780404839), test loss: 2.92570569515\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (12.2064304352,41.2180647775), test loss: 30.6729180574\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.50682592392,5.08850919869), test loss: 2.78288813233\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (23.0459041595,40.8667247713), test loss: 28.0117184162\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.94470357895,5.04020118422), test loss: 2.97502325028\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (22.0086765289,40.5003652081), test loss: 26.2904802799\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.03404521942,4.99343448297), test loss: 2.48959884048\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (14.334608078,40.1275475439), test loss: 30.9084865212\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.934555053711,4.94793452852), test loss: 3.0905279845\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (11.3212099075,39.7507782584), test loss: 28.2907624245\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.04322016239,4.90323117145), test loss: 2.46160211563\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (14.1939506531,39.366953074), test loss: 28.6502107501\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.581917047501,4.85953701185), test loss: 3.09677702188\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (6.99542140961,38.9842304615), test loss: 28.7271314621\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.86314153671,4.81657131763), test loss: 2.45167167187\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (5.2774772644,38.6073565933), test loss: 28.8552437067\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.59444999695,4.773936651), test loss: 3.00553224683\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (16.1874198914,38.233547446), test loss: 29.7298979759\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.40443885326,4.73239222848), test loss: 2.65143318176\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.9474830627,37.8642231652), test loss: 30.7768939614\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.98570501804,4.69183328288), test loss: 2.97808296084\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (18.7220916748,37.5029777135), test loss: 27.5617684364\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.71945905685,4.65210411779), test loss: 2.84154619724\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (16.1596355438,37.1453330703), test loss: 29.1372169256\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.74524354935,4.61336731505), test loss: 2.71562291235\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (8.37414550781,36.7955319155), test loss: 25.5075031281\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.5646841526,4.57534723352), test loss: 2.651638785\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (30.4108314514,36.4542251796), test loss: 28.8835484982\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.61460769176,4.53781679375), test loss: 2.69748415053\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (37.7235832214,36.1176775132), test loss: 28.8496284008\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.55930042267,4.50135823912), test loss: 2.8914973259\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (18.9645786285,35.7888748933), test loss: 30.0205475807\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.60383844376,4.46586599929), test loss: 2.83274496347\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (21.7431030273,35.4680303123), test loss: 28.1589662075\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.05616080761,4.43115560895), test loss: 3.06911805421\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (11.6463108063,35.1521774841), test loss: 31.0855666637\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.10423612595,4.39729576486), test loss: 2.83379633427\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (12.9758644104,34.8448101688), test loss: 27.821523571\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (5.14033222198,4.36413031776), test loss: 3.01694309115\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (9.24866104126,34.5428467035), test loss: 29.3646988392\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.00932884216,4.33135540127), test loss: 2.56382502019\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (12.7083845139,34.2468825751), test loss: 29.6425996304\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.38123416901,4.29960626367), test loss: 3.09037430286\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.201581955,33.9575897055), test loss: 30.547082901\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.41817188263,4.26863802046), test loss: 2.40672252476\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (3.84290122986,33.6756508972), test loss: 32.1134027958\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.72687214613,4.23842311554), test loss: 3.03168032765\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (2.86547040939,33.3970552413), test loss: 29.8962589264\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.08924341202,4.20881724824), test loss: 2.64378481954\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (17.8404693604,33.126740919), test loss: 31.732330513\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.73938798904,4.17968516986), test loss: 2.94603104591\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.25708770752,32.8595042335), test loss: 30.6734519958\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.20944547653,4.15111145035), test loss: 2.78884415627\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.2371244431,32.5977801067), test loss: 31.2126094818\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (3.43646287918,4.12325704552), test loss: 2.89328130186\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (11.7627038956,32.3423218399), test loss: 29.3840907574\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.74070310593,4.09611560908), test loss: 2.79996173382\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (31.8040733337,32.0925316578), test loss: 28.7719914436\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.43866562843,4.06960356938), test loss: 2.66820195019\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (18.3154067993,31.8460604458), test loss: 26.8056495667\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.80907583237,4.04354358127), test loss: 2.67502923459\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (9.56312656403,31.6058410324), test loss: 29.8508589864\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.581612706184,4.01788640801), test loss: 2.78409034461\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (10.9206819534,31.369424441), test loss: 29.5611781597\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.85701286793,3.99258394026), test loss: 2.87685017884\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (6.37540626526,31.1365272088), test loss: 31.6747644424\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.0050034523,3.96800515464), test loss: 2.85193461925\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (12.7049789429,30.9096833307), test loss: 28.4048222065\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.31003451347,3.94399337961), test loss: 2.92717719972\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (23.3809242249,30.6869136282), test loss: 29.5864929676\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.66120481491,3.92052119133), test loss: 2.76123178005\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (8.55076980591,30.468346598), test loss: 28.2009885311\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.18953466415,3.89741521089), test loss: 2.96844407767\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (4.0221862793,30.2543303549), test loss: 31.6936428547\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.62555384636,3.87458054533), test loss: 2.61064539254\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.7889537811,30.0431942368), test loss: 30.1819963217\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.7234480381,3.85220278176), test loss: 3.07144690454\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (14.4994840622,29.835464719), test loss: 32.7492177486\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.55519807339,3.83033551012), test loss: 2.52077917159\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (9.6967792511,29.6323056363), test loss: 31.8411419868\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.03369903564,3.80898950428), test loss: 2.98487899601\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (18.6355991364,29.4325259827), test loss: 33.7949923992\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.86207473278,3.78803698856), test loss: 2.84896539152\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (12.5727710724,29.2368684746), test loss: 33.3599073887\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.27532076836,3.76741775783), test loss: 3.08989477754\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (5.5966553688,29.0450107734), test loss: 30.3735368729\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.72768735886,3.74697191559), test loss: 2.8418097645\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (16.2899971008,28.854836989), test loss: 32.1450585365\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.61800312996,3.72699893759), test loss: 2.72757818699\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (13.7017126083,28.6683621694), test loss: 28.8389299393\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.26368570328,3.70742442855), test loss: 2.77327697575\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (5.22332668304,28.4856598908), test loss: 30.4888686657\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.366062432528,3.68825026862), test loss: 2.76108103096\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (6.34890604019,28.3056241737), test loss: 30.2342024326\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.19235777855,3.66951490095), test loss: 2.9067679286\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (5.48767137527,28.1295609589), test loss: 30.6617152929\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.86539888382,3.65101076413), test loss: 2.85330741704\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (3.41045451164,27.9562901201), test loss: 30.3549679756\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.45852255821,3.63258856992), test loss: 3.01735870242\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (6.97373104095,27.7843420201), test loss: 32.5141350031\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.5722322464,3.61460743768), test loss: 2.82065128535\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (8.38645172119,27.6161813509), test loss: 29.6230594635\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.04081988335,3.59695621736), test loss: 3.03583888412\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (7.84091281891,27.4512510582), test loss: 31.0681089878\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.776213824749,3.57971484879), test loss: 2.77175528854\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (7.24626111984,27.2882360053), test loss: 30.3850845337\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.9853053093,3.56275939668), test loss: 3.07847498059\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (4.67904567719,27.1289112621), test loss: 32.4290943146\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.37298727036,3.54593872518), test loss: 2.60350863934\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (18.0090007782,26.971353262), test loss: 33.3216627598\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.663555145264,3.52931263777), test loss: 3.03696078062\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (26.9381904602,26.8156409376), test loss: 31.6309749126\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.01914048195,3.51302725036), test loss: 2.56556004584\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (10.9999885559,26.6627562232), test loss: 32.5727990627\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.57569801807,3.49709214538), test loss: 2.91022068858\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (9.68580913544,26.5130797273), test loss: 37.2121067524\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.778018713,3.4814693094), test loss: 2.92666722834\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (12.9646530151,26.3648349737), test loss: 32.588247776\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.47899031639,3.46602825532), test loss: 2.93162730634\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (12.5186719894,26.2198527445), test loss: 31.9746147633\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.724094510078,3.45073880249), test loss: 2.85838580728\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (10.144323349,26.0760210679), test loss: 31.3230935574\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.75695884228,3.43565814903), test loss: 2.63853019178\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (11.411485672,25.9338419916), test loss: 27.9107629299\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.88203108311,3.42079529805), test loss: 2.66602978855\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (14.5844421387,25.7948689051), test loss: 32.7876841545\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.01153492928,3.40630458563), test loss: 2.77937984169\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (23.0243282318,25.6577009585), test loss: 31.2502871275\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.00646984577,3.3920593098), test loss: 2.91315986514\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (4.15458536148,25.5226119705), test loss: 32.2529113293\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.88938689232,3.37796323427), test loss: 2.87150042057\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (8.53758811951,25.3898602012), test loss: 29.1310549259\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (4.91992473602,3.36405409506), test loss: 2.91515482068\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (8.12006950378,25.2583912116), test loss: 30.859351778\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.683593213558,3.35020909987), test loss: 2.70718269497\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (2.02416491508,25.1281878802), test loss: 30.5877307892\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.76377630234,3.33665957285), test loss: 2.98617300391\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.7085108757,25.0006047031), test loss: 32.9467023849\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.76359200478,3.32340016058), test loss: 2.59845885038\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (7.15915966034,24.8746976956), test loss: 30.2034656525\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.677935540676,3.31036448772), test loss: 3.05845859051\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (14.0576858521,24.7512317596), test loss: 33.0924098492\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (3.30824065208,3.29748771527), test loss: 2.55333953947\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.23208236694,24.6292575085), test loss: 33.7228138924\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (3.2923374176,3.28462080705), test loss: 2.96230023503\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (17.3400840759,24.5081433773), test loss: 33.3732982635\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.897358179092,3.27198947552), test loss: 2.71018391997\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.62452030182,24.3885258916), test loss: 35.9480610371\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.89462471008,3.25956659413), test loss: 2.99570066929\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.97480201721,24.2711845435), test loss: 31.0306693554\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.67319905758,3.24736183155), test loss: 2.86901918352\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (5.81310462952,24.1551773116), test loss: 34.4174705505\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.00039339066,3.23542661984), test loss: 2.83304052353\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (12.4639596939,24.0414110001), test loss: 30.5748744965\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.73417532444,3.22354839036), test loss: 2.84506722391\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (7.06505537033,23.9290617834), test loss: 31.1853559971\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.73210632801,3.21169056814), test loss: 2.59536474347\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (7.86717987061,23.8170383937), test loss: 28.3702435493\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.774805486202,3.20005429217), test loss: 2.6732121408\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (5.29229068756,23.7068325854), test loss: 32.7016248703\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.798765420914,3.18862547351), test loss: 2.82148804069\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (5.03676366806,23.5984949608), test loss: 30.4485890865\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.49746656418,3.17741066973), test loss: 2.94855361283\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (7.83210325241,23.491258374), test loss: 32.7892243385\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.30863881111,3.16638519748), test loss: 2.83975319862\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.13069725037,23.3860727164), test loss: 29.0572532177\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.890974640846,3.15538982172), test loss: 3.03635279536\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (25.3208656311,23.2820090877), test loss: 31.1933195591\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.52606701851,3.14443726008), test loss: 2.69571113884\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (8.28901100159,23.1780909337), test loss: 31.692736268\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.78683423996,3.13370606771), test loss: 3.05206825137\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (12.9151649475,23.0761218019), test loss: 33.2932627201\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.44520115852,3.12312281734), test loss: 2.58131453991\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (4.18477725983,22.9759091052), test loss: 32.0347430706\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.811439096928,3.11274527427), test loss: 3.00338481069\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (8.00604057312,22.8764517534), test loss: 34.2352155685\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.00433325768,3.10250257003), test loss: 2.58241361976\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (12.4168367386,22.7789723698), test loss: 33.902604866\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.15080952644,3.09227915362), test loss: 2.95551190376\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (12.2956600189,22.6818118043), test loss: 35.8070207596\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.10644865036,3.08217351539), test loss: 2.78478999138\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (8.36514663696,22.5855868958), test loss: 34.7722565651\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (3.66187930107,3.07220716957), test loss: 2.98396024704\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.25360488892,22.490926198), test loss: 31.6279919624\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.91521894932,3.06242584431), test loss: 2.84727660418\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.56294250488,22.3978785124), test loss: 34.3946159363\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.304852902889,3.05280579068), test loss: 2.7557949692\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (2.89719104767,22.3054652966), test loss: 29.3163673878\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.809779405594,3.04327500466), test loss: 2.68962005973\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (5.85864019394,22.2145932902), test loss: 31.8690870762\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.00432789326,3.03378045082), test loss: 2.7080046773\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (10.927854538,22.124296538), test loss: 30.7654049873\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.23318099976,3.02434231162), test loss: 2.85588238835\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (6.81900501251,22.0344464828), test loss: 33.4208879948\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.46747756004,3.01508118984), test loss: 2.83586846888\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (14.2690124512,21.9465039826), test loss: 30.2026590824\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.5688958168,3.00598571977), test loss: 2.97595720887\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (7.03230667114,21.8594105175), test loss: 34.1677737713\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.6430785656,2.99703978841), test loss: 2.88154428005\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.60173892975,21.7737381177), test loss: 30.0744170189\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.14956569672,2.98814536879), test loss: 2.96777893901\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (3.93436145782,21.6888534032), test loss: 32.7698104858\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.900873780251,2.9792524325), test loss: 2.5944521904\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (11.1001968384,21.6045039323), test loss: 32.4345583916\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.2797832489,2.97050459644), test loss: 3.10671274066\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (12.229347229,21.5209145949), test loss: 34.3755394936\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.36687290668,2.96186745623), test loss: 2.44885042906\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.02026939392,21.4385823894), test loss: 35.3767477512\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.635063588619,2.95337462443), test loss: 3.04000385404\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.4948797226,21.357231209), test loss: 35.1127661705\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.71357631683,2.94504148434), test loss: 2.758748281\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (4.8039932251,21.2771997652), test loss: 34.9237649441\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.07818257809,2.93671275218), test loss: 2.98188436627\n",
      "\n",
      "MC # 2, Hype # hyp4, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold2/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (324.519104004,inf), test loss: 183.196348953\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (246.64680481,inf), test loss: 304.990975189\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (16.9623298645,82.1308189707), test loss: 43.988803196\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.04860520363,51.554298653), test loss: 3.33866807222\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (29.5591888428,63.848763032), test loss: 37.6722206116\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.16949462891,27.4418840387), test loss: 2.70264615417\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (55.4111366272,57.8082005952), test loss: 42.8467193604\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.58238697052,19.3855993563), test loss: 3.48668738604\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (19.8730983734,54.8448056825), test loss: 41.7369362354\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.68107008934,15.3650097663), test loss: 3.32826452255\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (8.12637710571,52.9852805302), test loss: 41.0147191525\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.74613428116,12.9546242121), test loss: 3.39466905594\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (39.876701355,51.71761601), test loss: 43.666002655\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.33150148392,11.345259368), test loss: 3.61615577936\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (47.6756820679,50.7867191933), test loss: 39.2839185238\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.3713593483,10.2007096781), test loss: 2.7808565557\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (50.5011558533,50.0164632859), test loss: 46.8765788078\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.13156700134,9.3389359576), test loss: 3.65954391956\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (16.3195056915,49.4426621503), test loss: 41.4229799271\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.55716657639,8.66580278014), test loss: 2.73935682774\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (34.9031906128,49.0192201582), test loss: 45.1101949692\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.11388134956,8.12956151027), test loss: 3.81206123233\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (45.6062011719,48.6324336411), test loss: 40.1494706631\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.39398670197,7.69093335778), test loss: 2.78278885782\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (25.2727813721,48.289112697), test loss: 43.5151157379\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.12382721901,7.32365413507), test loss: 3.55530147254\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (42.6200752258,47.9879717823), test loss: 39.0340573788\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.84055757523,7.01493615378), test loss: 2.91459674835\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (39.3329429626,47.6896499223), test loss: 44.6490887642\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (5.07769918442,6.74834619566), test loss: 3.46508899927\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (26.9755496979,47.4502327936), test loss: 38.5636726379\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.84784030914,6.51482199159), test loss: 2.69854955673\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (17.1604537964,47.255228526), test loss: 43.397431469\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.97687530518,6.31258472055), test loss: 3.40595926046\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (18.7417736053,47.0575620662), test loss: 36.4742283344\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.68367791176,6.13314467541), test loss: 2.67040287852\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.5847892761,46.8702778219), test loss: 42.2607867718\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.94724154472,5.97280192247), test loss: 3.28015395403\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (63.0979309082,46.690396069), test loss: 40.1432849884\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (7.12974214554,5.83014766624), test loss: 3.36635841727\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (43.6595420837,46.5020790646), test loss: 39.8564543247\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.9723534584,5.70014003828), test loss: 3.33562210798\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (60.4601249695,46.34453446), test loss: 41.4927858353\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (6.73438405991,5.5809294316), test loss: 3.44123385847\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (33.3267631531,46.2103377475), test loss: 37.8898735046\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.7918176651,5.47422831053), test loss: 2.71362293661\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (22.7421112061,46.0661923082), test loss: 45.2468060493\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.96340119839,5.37586554654), test loss: 3.50426432192\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (51.6290206909,45.9224400092), test loss: 40.2781652927\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.28624951839,5.28492029545), test loss: 2.64892859459\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (35.4209594727,45.778285146), test loss: 44.6107548714\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.89939570427,5.20167609558), test loss: 3.69954024553\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (32.2997283936,45.6270471489), test loss: 40.5892801285\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (7.56014633179,5.1240889696), test loss: 2.61015972197\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (32.0285682678,45.4918531544), test loss: 42.5127959251\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.60339403152,5.05054324241), test loss: 3.65184804201\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (53.2427024841,45.3741480633), test loss: 37.2350468636\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (9.49810314178,4.98379436849), test loss: 2.62885825634\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (50.2999649048,45.2444635547), test loss: 43.619697094\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.62173366547,4.92084568426), test loss: 3.4379910171\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (41.4720573425,45.109454928), test loss: 36.8124202728\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.16126799583,4.86122442579), test loss: 2.86155725121\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (33.9155883789,44.9742135352), test loss: 44.1890286922\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.52790021896,4.80544848684), test loss: 3.31078019738\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (23.3673019409,44.8298598252), test loss: 35.1741928101\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.16434764862,4.75235064002), test loss: 2.60383687317\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (29.0598068237,44.6946266118), test loss: 40.3804936886\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.12813949585,4.70157104271), test loss: 3.21526341438\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (36.7035522461,44.5681425211), test loss: 38.560555172\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.97837591171,4.65436240838), test loss: 3.0448869884\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (71.5502471924,44.4216762333), test loss: 37.528173399\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.15082836151,4.61041873255), test loss: 3.23922060132\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (26.6613388062,44.2594291644), test loss: 38.5874704361\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.82357847691,4.56807629796), test loss: 3.33374063373\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (65.0745925903,44.0906898933), test loss: 35.435148859\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.93833947182,4.52793149941), test loss: 2.62734571099\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (26.9209060669,43.9123107431), test loss: 39.6140410423\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.47604966164,4.48893302944), test loss: 3.29814368188\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (11.1132612228,43.7356122999), test loss: 36.2476926804\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.42059755325,4.45126885811), test loss: 2.4691767931\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (96.5248565674,43.56132167), test loss: 41.3571275234\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (5.42812633514,4.41517539946), test loss: 3.59894145131\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (24.7487888336,43.3730332967), test loss: 36.1709733009\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.938861489296,4.38065543221), test loss: 2.42616975307\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (21.7733306885,43.1796818633), test loss: 40.0177173138\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.40408849716,4.3465961427), test loss: 3.3780067265\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (123.262680054,42.970418526), test loss: 31.1952970505\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.95322704315,4.31340742928), test loss: 2.46990204453\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (31.9835700989,42.7529571378), test loss: 39.7013525486\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.2310769558,4.28054370939), test loss: 3.32581275105\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (35.0414428711,42.531333533), test loss: 31.8102171898\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.0746679306,4.24819379817), test loss: 2.51818626821\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (14.6191978455,42.3028245411), test loss: 38.5526515484\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.43347883224,4.21662406746), test loss: 3.20645938516\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (34.641418457,42.0651281872), test loss: 30.9478302956\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.32544708252,4.18618570312), test loss: 2.42848107815\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (37.7481651306,41.8169321629), test loss: 34.8399814844\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.8030154705,4.15563534148), test loss: 2.94429931343\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (21.0875358582,41.5503957782), test loss: 32.1245923519\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.621539056301,4.12548156264), test loss: 2.93184876144\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (60.7164001465,41.2788185097), test loss: 32.5453324318\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.91393375397,4.09573851541), test loss: 3.11541493535\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (4.14715766907,40.9906763397), test loss: 30.9745759487\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.56511735916,4.06695484353), test loss: 3.06913950145\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (10.590379715,40.6897495916), test loss: 32.6526146173\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.40762662888,4.03900987228), test loss: 2.89223659337\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (20.3282318115,40.3773106135), test loss: 31.7171891212\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.92499041557,4.01180232909), test loss: 3.24655801058\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (23.1079368591,40.0565419278), test loss: 28.6778273106\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.18207621574,3.98434322952), test loss: 2.30952563286\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (32.4491081238,39.7221264065), test loss: 33.5700156212\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.02627277374,3.95663507992), test loss: 3.35637590587\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (25.4828701019,39.3853521322), test loss: 28.6840208292\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.04718589783,3.92849039469), test loss: 2.1981283322\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (18.7433261871,39.0459965304), test loss: 33.1363609314\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.4932243824,3.90054628879), test loss: 3.28598652482\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (43.1112136841,38.700409033), test loss: 25.1632882357\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.76999676228,3.87273756925), test loss: 2.14844427556\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (26.6130027771,38.3519743587), test loss: 33.2957614422\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (4.03611564636,3.84562536024), test loss: 3.16760280728\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (12.3544216156,38.0037131582), test loss: 25.6493993282\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.39627289772,3.81873937145), test loss: 2.3903962791\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (30.5510826111,37.6507984469), test loss: 32.2711202145\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.9810090065,3.79201775757), test loss: 3.12488921285\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (20.1287155151,37.3033326058), test loss: 26.0866723537\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.67679309845,3.76548325266), test loss: 2.38851935863\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.8053750992,36.9593536171), test loss: 31.0086680412\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.62292075157,3.73933511074), test loss: 3.05214509666\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (22.4306049347,36.6159162493), test loss: 24.2215423584\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.21136283875,3.71350260819), test loss: 2.59541995227\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (18.9633674622,36.2780412128), test loss: 31.7075570107\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.93994230032,3.68845596954), test loss: 3.09631611407\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (17.1506156921,35.9462354224), test loss: 27.4185134888\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.717266082764,3.6639923006), test loss: 3.07213026583\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (9.58348846436,35.6156541376), test loss: 31.7580354929\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.30139732361,3.63986814762), test loss: 3.19213389456\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (10.2240524292,35.2940451967), test loss: 28.4292912483\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.503871560097,3.61593530171), test loss: 3.23068016022\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (15.6465492249,34.978534215), test loss: 29.1499256849\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (4.85857629776,3.59250762532), test loss: 2.50015696883\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (8.84756946564,34.6661986845), test loss: 32.7182889938\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.00497770309,3.56951056914), test loss: 3.34636288211\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (6.52219390869,34.3618385465), test loss: 30.8438943863\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.13777637482,3.54730385347), test loss: 2.3556333214\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (35.0012245178,34.0644843549), test loss: 33.5273275375\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.20365166664,3.52566954099), test loss: 3.44108842611\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (22.8513679504,33.7704418808), test loss: 27.5741122723\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.17462038994,3.50432755425), test loss: 2.23934615254\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.13578605652,33.4847870765), test loss: 33.7287905693\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (5.18519878387,3.48322819015), test loss: 3.3156447202\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.0242595673,33.2050589043), test loss: 28.7754621029\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.83988130093,3.46243913942), test loss: 2.50092148632\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.7237005234,32.9290044177), test loss: 33.8701414108\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.20131587982,3.44218285207), test loss: 3.23725111783\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (7.26629495621,32.6604620127), test loss: 27.5377837658\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.5222568512,3.42258732193), test loss: 2.49059922472\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (25.398525238,32.3973633184), test loss: 32.6627002954\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.47852993011,3.4035086495), test loss: 3.15841822624\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.4319400787,32.1384095908), test loss: 25.2329746723\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.46821641922,3.38463211604), test loss: 2.30754840225\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.1589384079,31.8863785448), test loss: 33.3726120472\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (3.94775724411,3.36592213188), test loss: 3.18975287974\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (2.79911327362,31.6389868534), test loss: 28.9134605885\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.214996293187,3.34760579225), test loss: 2.93052794933\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (5.42959690094,31.394687765), test loss: 34.2322864532\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.54505968094,3.32965812058), test loss: 3.31004769206\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (13.9340782166,31.1567125124), test loss: 29.4878006697\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.41804170609,3.31224797997), test loss: 3.26857696772\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (12.0214672089,30.9228646232), test loss: 29.6928457499\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.89093112946,3.29534306887), test loss: 2.55802499652\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.8845920563,30.6931764041), test loss: 32.3362445354\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.0209685564,3.27854541286), test loss: 3.35120353699\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (10.5914897919,30.4688322928), test loss: 32.1085999012\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.0889248848,3.26187106612), test loss: 2.45197283328\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (9.81357479095,30.2487125556), test loss: 34.3211904049\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.38695669174,3.24567244273), test loss: 3.39811588526\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (5.15629816055,30.0308405723), test loss: 31.4519878864\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.03788912296,3.22966184562), test loss: 2.33093090355\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (7.1900396347,29.818262533), test loss: 34.2629581928\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.79673838615,3.21414637004), test loss: 3.28632844165\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (26.0562973022,29.6095809193), test loss: 31.3768531799\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.01392149925,3.19906022962), test loss: 2.5783809498\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (17.6731395721,29.4045144771), test loss: 34.9872807026\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.32971358299,3.18407690776), test loss: 3.29845390916\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (3.65644335747,29.2038681281), test loss: 28.9746788025\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.929104506969,3.16912758605), test loss: 2.4982052803\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (15.5359649658,29.0068711729), test loss: 35.143377018\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.91934907436,3.15462843078), test loss: 3.18527396172\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (22.7239742279,28.811888978), test loss: 26.6604323864\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.969106972218,3.14026945033), test loss: 2.28551516831\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (7.42068624496,28.6213349614), test loss: 35.0186682701\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.63196760416,3.12631787185), test loss: 3.17502159476\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (6.2136759758,28.4338310404), test loss: 29.5041825294\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.44714999199,3.1127372045), test loss: 2.97028193027\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (4.10437488556,28.2500471452), test loss: 34.569923532\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.662009358406,3.09926537197), test loss: 3.27815880477\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (13.1601352692,28.0700921663), test loss: 29.6510481358\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.74808049202,3.08580582283), test loss: 3.1021688886\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (6.64069461823,27.8927108289), test loss: 31.0311181545\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.01466298103,3.07270216442), test loss: 2.64067666531\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (7.92643165588,27.7171537587), test loss: 33.7946541309\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.82981073856,3.05973854826), test loss: 3.29469250143\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (8.07850074768,27.5457691297), test loss: 33.5248372078\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.901899755001,3.04712171021), test loss: 2.45603559166\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (7.68786382675,27.3766810214), test loss: 33.940154314\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.168484553695,3.03482296561), test loss: 3.38726622164\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (10.3012332916,27.2108492575), test loss: 32.4501929283\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.02127742767,3.02263324365), test loss: 2.37355788648\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (6.02446556091,27.0480070602), test loss: 35.329097271\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.99285018444,3.01038528804), test loss: 3.28059979379\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (11.6827373505,26.8876100623), test loss: 32.6728903294\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.22821557522,2.99850752601), test loss: 2.48668296188\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (12.2883090973,26.7286593088), test loss: 38.3133646488\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.11894607544,2.98669953538), test loss: 3.40288897455\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (9.71366119385,26.5734624577), test loss: 29.2892438412\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.46633148193,2.97523387811), test loss: 2.53081448078\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (5.45212268829,26.4203591842), test loss: 36.3565696239\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.42758131027,2.96404822447), test loss: 3.20617603064\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (7.42747163773,26.2701341496), test loss: 26.527852726\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.9132976532,2.95291483078), test loss: 2.26907254606\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (4.31328630447,26.122564852), test loss: 35.1578426361\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.908518254757,2.94171625194), test loss: 3.09174352288\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (15.6146812439,25.976889087), test loss: 30.6954150915\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.2648460865,2.93087656294), test loss: 2.95779113173\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (8.21739959717,25.8324824062), test loss: 36.0632053852\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.677846908569,2.92005777078), test loss: 3.30902482569\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (6.85930347443,25.6914743139), test loss: 30.0536124945\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.96488523483,2.90955280186), test loss: 3.09083930552\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (19.2211837769,25.5523455909), test loss: 31.1745834112\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.864393591881,2.89931805858), test loss: 2.60247793794\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (13.5986776352,25.4156469963), test loss: 32.1045044899\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.35055446625,2.88912428984), test loss: 3.1901689887\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.10780096054,25.2812794592), test loss: 33.8818286657\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.14450120926,2.87886780523), test loss: 2.46185772121\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (19.8052425385,25.1482980308), test loss: 33.9799784184\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (3.38940811157,2.86889501531), test loss: 3.38402954489\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (13.5402040482,25.0166509894), test loss: 35.1716356039\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.47804951668,2.85897740281), test loss: 2.36995522231\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.79827213287,24.8878997963), test loss: 35.9276671171\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.839832782745,2.84928043359), test loss: 3.27748104632\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (4.5236582756,24.760633274), test loss: 31.9419534683\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.873703598976,2.83990304977), test loss: 2.45631597042\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (8.43758392334,24.6358008866), test loss: 37.7260106564\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.19385683537,2.83051225047), test loss: 3.40514339209\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (16.7788009644,24.5130138791), test loss: 29.7289821148\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.52321171761,2.82106840756), test loss: 2.51061858684\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (4.87004184723,24.3910494608), test loss: 38.4988217115\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.61036002636,2.81183901734), test loss: 3.28323305249\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (9.68095588684,24.2705316891), test loss: 28.7451979637\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.68929338455,2.80270270247), test loss: 2.44319868162\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.84668731689,24.1525567133), test loss: 35.0466133833\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.51095592976,2.79377322851), test loss: 3.02572775185\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (5.92468833923,24.0358417166), test loss: 32.069931221\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.87123656273,2.78512000063), test loss: 2.96516646445\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (12.3457641602,23.9212153405), test loss: 34.9003695488\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.65379953384,2.77643614008), test loss: 3.20055515468\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (11.2842216492,23.8083118169), test loss: 29.8722455978\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.28177881241,2.76769249497), test loss: 2.96708317995\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (29.1943473816,23.6964023985), test loss: 32.5254735231\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.55777263641,2.75916497033), test loss: 2.63209340125\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (9.86785888672,23.5854905465), test loss: 32.6558471203\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.793226659298,2.75067988824), test loss: 3.21238741875\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (5.86024284363,23.4768819909), test loss: 32.9391953468\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.566485643387,2.74239809036), test loss: 2.48965470344\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (9.0003862381,23.3694131928), test loss: 33.4822309256\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.931824922562,2.73436393566), test loss: 3.34321611524\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (5.02673912048,23.2637267259), test loss: 32.4410469532\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.3191421032,2.72630320149), test loss: 2.26938166022\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (2.81162858009,23.1596544008), test loss: 37.1985896111\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.35046052933,2.71817580829), test loss: 3.28148081899\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (7.28357601166,23.0561385173), test loss: 30.0145580292\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.536214709282,2.7102323576), test loss: 2.25488889664\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (14.8358678818,22.9540343732), test loss: 38.8251734257\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.2236032486,2.70236469988), test loss: 3.33493198603\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (1.50732183456,22.8537456305), test loss: 29.6286993504\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.227352529764,2.6946638921), test loss: 2.49476718605\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (5.32638120651,22.7543611316), test loss: 37.182550478\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.916332483292,2.68718016741), test loss: 3.19516248405\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (6.36203432083,22.6566841584), test loss: 28.2918579817\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.19309353828,2.67967788184), test loss: 2.3978763938\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (5.86533498764,22.5604424515), test loss: 36.3299819231\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.67124176025,2.67209372781), test loss: 2.96549386382\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (9.04585266113,22.4645308515), test loss: 31.5270507336\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.45847797394,2.66471644203), test loss: 2.90634236336\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (10.9654407501,22.3698454234), test loss: 35.2857919455\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.78786075115,2.65736134975), test loss: 3.13425095081\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.45219326019,22.2771052464), test loss: 29.6921300411\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.882068991661,2.65020620356), test loss: 2.98375099599\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (11.0576562881,22.1849132387), test loss: 36.367854023\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.9734044075,2.64321580239), test loss: 3.17006850541\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (6.16750526428,22.0942890003), test loss: 31.0710863113\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.97570753098,2.63617957981), test loss: 3.22089981437\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (19.8094291687,22.0049596706), test loss: 32.9182257652\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.82629191875,2.62910064774), test loss: 2.50090647638\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (6.36721038818,21.9158642275), test loss: 34.6467627287\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.35842347145,2.62221753458), test loss: 3.30440178216\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (6.58378982544,21.82784488), test loss: 33.3049770832\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.06125175953,2.61532756374), test loss: 2.24785512686\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (10.174577713,21.7417618467), test loss: 37.7998147488\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.733156323433,2.60864232971), test loss: 3.36014883816\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (5.95060443878,21.6559521532), test loss: 31.0702350616\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.6559394598,2.60208463607), test loss: 2.27064813972\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.16970729828,21.5716843314), test loss: 38.2361029625\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (5.84230661392,2.59548807135), test loss: 3.31637083888\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (8.85383701324,21.4883667427), test loss: 30.0720495701\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.79874157906,2.5888509073), test loss: 2.47031773329\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (9.50836849213,21.4055721561), test loss: 37.2342133045\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (4.86663532257,2.58238929607), test loss: 3.16110068262\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (7.71447610855,21.3235766896), test loss: 29.9630841255\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.07369089127,2.57594260188), test loss: 2.45908198357\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (5.69652748108,21.2433053894), test loss: 35.6359633684\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.46163725853,2.56967481445), test loss: 3.00315394104\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (7.06275749207,21.1633429785), test loss: 28.1577594757\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.99458146095,2.56351586012), test loss: 2.50703263879\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (2.50758862495,21.0847374438), test loss: 35.6721461773\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.10716104507,2.55727525567), test loss: 3.03701579869\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (15.4044008255,21.0069436507), test loss: 31.0074180126\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.785229802132,2.55109684537), test loss: 2.93043791354\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (5.71774959564,20.9296081018), test loss: 36.0762014389\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.4793946743,2.54499636713), test loss: 3.15074414611\n",
      "\n",
      "MC # 2, Hype # hyp4, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold3/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (304.554748535,inf), test loss: 154.068838501\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (266.356384277,inf), test loss: 315.707489014\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (123.618904114,71.6702683764), test loss: 46.6381626129\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (7.36851930618,48.5873026726), test loss: 3.15799205303\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (41.9266967773,59.1119475451), test loss: 38.0756891727\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.092274189,25.9093176506), test loss: 3.24189445972\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (60.4710159302,54.6654059569), test loss: 47.1615772247\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.61763763428,18.3558097939), test loss: 3.25513371527\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (53.1514282227,52.4753323412), test loss: 43.5493860722\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.22419643402,14.5738258743), test loss: 3.25073951781\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (24.2135448456,51.0463290071), test loss: 43.7382185936\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.57327795029,12.3030320113), test loss: 2.68198444247\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (39.8323745728,50.1858485909), test loss: 45.6379230976\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.20684576035,10.7915758437), test loss: 3.49116065502\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (18.3240833282,49.499422766), test loss: 36.7773124695\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.88764286041,9.70723266401), test loss: 2.63422664404\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (32.8774337769,48.9714994884), test loss: 46.6831318855\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.74223470688,8.89481395102), test loss: 3.1570486486\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (43.2761230469,48.5589807969), test loss: 34.0604014874\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (6.49965715408,8.2606463171), test loss: 2.60732151866\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (20.7644557953,48.1896617963), test loss: 45.503771019\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.7184574604,7.7529148893), test loss: 3.1534861058\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (35.2809066772,47.8579394068), test loss: 37.8380060196\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.94044232368,7.33600358255), test loss: 3.20854722857\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (102.69506073,47.5635837936), test loss: 41.5233632565\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.4668469429,6.98756217567), test loss: 2.79369561672\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (42.4106750488,47.3182129159), test loss: 44.5778011322\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.31869244576,6.69250982854), test loss: 3.2988386035\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (56.1401290894,47.0716027202), test loss: 42.4795623302\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.74654352665,6.43641838293), test loss: 2.68566792011\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (20.2671871185,46.8666956619), test loss: 43.6328982353\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.18127977848,6.21525616117), test loss: 3.33553774953\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (30.09516716,46.6858046764), test loss: 36.2578824759\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.51045298576,6.02132758198), test loss: 2.60425245166\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (26.3622894287,46.5002988324), test loss: 46.0212492943\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.66499376297,5.84858723115), test loss: 3.09387232661\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (30.8366088867,46.3226386036), test loss: 31.9844819546\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.73870897293,5.69455511349), test loss: 2.50348553658\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (52.2729873657,46.1576614221), test loss: 44.371556139\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.92043352127,5.55573910227), test loss: 3.09234185219\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (92.8103256226,45.9850182214), test loss: 37.3670332909\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.6771645546,5.43041840862), test loss: 3.17941015363\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (28.6537895203,45.8081350912), test loss: 39.6025210381\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.4684715271,5.31550024782), test loss: 2.70230584443\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (30.008600235,45.6274819573), test loss: 43.6326507092\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.87972021103,5.2105079005), test loss: 3.29958413839\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (29.4298191071,45.4643103044), test loss: 41.0525071144\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.20873439312,5.11262568056), test loss: 2.66436689198\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (19.6554412842,45.2833080568), test loss: 42.9232853889\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.39449691772,5.0224657054), test loss: 3.16842244864\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (28.9816646576,45.1273643649), test loss: 33.9553789616\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.50443840027,4.93935464175), test loss: 2.552697438\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (19.431886673,44.9557979545), test loss: 44.4538605213\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.48362588882,4.86138868715), test loss: 3.01263801157\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (36.855506897,44.7834510464), test loss: 29.8743793964\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.843126535416,4.78917455989), test loss: 2.38139624298\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (27.64295578,44.6080384396), test loss: 43.1061366796\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.04931020737,4.72113118412), test loss: 3.07003690004\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (26.4674835205,44.4227482858), test loss: 36.9390352249\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.971278786659,4.65714577707), test loss: 3.23489044309\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (35.6921310425,44.2261003945), test loss: 37.3759456158\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.3349198103,4.59649067395), test loss: 2.4365534544\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (52.0049858093,44.0229588882), test loss: 40.3084527016\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.33927750587,4.53881937761), test loss: 3.20560728908\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (36.0761032104,43.8195081534), test loss: 35.8775583267\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.48008561134,4.4841753063), test loss: 2.43350200057\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (31.9831352234,43.5998735281), test loss: 41.9772894859\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.84348499775,4.43103771328), test loss: 3.05484502912\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (32.5066070557,43.381435931), test loss: 30.9901369333\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.41649210453,4.38088169014), test loss: 2.47274720669\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (23.3275184631,43.1571301924), test loss: 41.3579004288\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.91257691383,4.33302831807), test loss: 2.93894930184\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (12.0329179764,42.9135397376), test loss: 32.1541094542\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.0000936985,4.28704210707), test loss: 2.78857194483\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (79.7145080566,42.6543327715), test loss: 38.1210143089\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.45775222778,4.2439704404), test loss: 2.94977136254\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (23.9249382019,42.3795668315), test loss: 33.8425480843\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.12610864639,4.20192613276), test loss: 3.0676158309\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (27.0847682953,42.0923626132), test loss: 34.767150116\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.05244600773,4.16134994114), test loss: 2.35098113418\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (25.0154819489,41.7905752896), test loss: 37.6090022087\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.24345469475,4.12144562122), test loss: 3.13177720904\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (21.9807853699,41.4766458419), test loss: 29.3379601479\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.34262657166,4.08253450417), test loss: 2.30485913455\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (37.716255188,41.1591480885), test loss: 37.9921743393\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.77616500854,4.04385567348), test loss: 2.92945823669\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (7.09034824371,40.8227716743), test loss: 26.7905018568\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.36524677277,4.00608175376), test loss: 2.40867428184\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (9.52921867371,40.4856013737), test loss: 36.4339150906\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.02006113529,3.969181863), test loss: 2.86235970259\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (30.5277366638,40.1348131873), test loss: 27.606756258\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.06726789474,3.93278153507), test loss: 2.82043076158\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (28.2743644714,39.7755159839), test loss: 34.7986050129\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.48103165627,3.89719955982), test loss: 2.88727445006\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (4.72734212875,39.4084318666), test loss: 29.4239561081\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.04037880898,3.86234132436), test loss: 2.93590070009\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (20.5300006866,39.0360861966), test loss: 31.2565199852\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.2062369585,3.82794833057), test loss: 2.40368830562\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (14.6111869812,38.6551699471), test loss: 34.1147780418\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.436802238226,3.79414553217), test loss: 3.02112131119\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (9.43907165527,38.2727916755), test loss: 27.1905835152\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.591791152954,3.76099506383), test loss: 2.43088018596\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (6.51032114029,37.8910923934), test loss: 33.8913817883\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.43744778633,3.72859100382), test loss: 2.85999953151\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (6.15443277359,37.5079215673), test loss: 25.4266323566\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.60251176357,3.6962891908), test loss: 2.51178992987\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (8.64333152771,37.1267962619), test loss: 33.6683104038\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.15206897259,3.6650550059), test loss: 2.83743134439\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.8502244949,36.7496086842), test loss: 25.9944509029\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.49537062645,3.63465060301), test loss: 3.0334928602\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (5.53075695038,36.3745551185), test loss: 34.6118415356\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.0522210598,3.60474521152), test loss: 3.00826285779\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (32.3043746948,36.0068647106), test loss: 30.563315773\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.36918139458,3.57593060632), test loss: 3.11144752502\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (9.32107162476,35.6443904033), test loss: 31.6469884396\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.95439887047,3.54773736475), test loss: 2.43482030034\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (19.9412441254,35.2883019961), test loss: 34.6387348652\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.828010439873,3.52028314539), test loss: 3.04055920243\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (7.05548763275,34.9367642044), test loss: 29.7200502872\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.05708217621,3.49348626094), test loss: 2.5195483312\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (12.2221221924,34.5936115309), test loss: 34.4952963829\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.18902826309,3.4672155476), test loss: 2.86443827152\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (17.1765117645,34.2600292182), test loss: 25.076815176\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.27020561695,3.44120834463), test loss: 2.49848977029\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (27.6678028107,33.9321109696), test loss: 35.4960634947\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.67799794674,3.41586461773), test loss: 3.00996710658\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (4.10767221451,33.6112964309), test loss: 27.1056689739\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.461225777864,3.39121110165), test loss: 3.08679119945\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (30.9654006958,33.2975321634), test loss: 35.7900220633\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.86483573914,3.3671096471), test loss: 2.91237096786\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (19.055973053,32.9913819), test loss: 32.982439661\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.04980158806,3.34369769402), test loss: 3.16020349562\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (12.3571500778,32.6921415489), test loss: 32.9765235901\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.38279521465,3.32099806798), test loss: 2.47363073826\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (16.0592269897,32.4012050442), test loss: 35.1953919888\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.28990626335,3.2988452742), test loss: 3.01907108128\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (13.2786026001,32.1142936355), test loss: 29.6861561537\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.568404018879,3.27715831863), test loss: 2.60809922218\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (18.4590187073,31.8360217408), test loss: 36.5404592276\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.0936191082,3.25606998725), test loss: 2.94468248785\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (6.24198436737,31.5634997392), test loss: 25.218165493\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.90053367615,3.23543525926), test loss: 2.47492358685\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (9.97270202637,31.2976677921), test loss: 37.0477916718\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.73504173756,3.21487622513), test loss: 3.05845081806\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (10.4339923859,31.0369569919), test loss: 29.1594599247\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.1988503933,3.1950222709), test loss: 3.16144424379\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (5.3530960083,30.7818536309), test loss: 32.8955826283\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.836950659752,3.17565788027), test loss: 2.71614813209\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.05642795563,30.5317635155), test loss: 33.5903914928\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.336263209581,3.15659329025), test loss: 3.19498545229\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (9.46716594696,30.2883584402), test loss: 34.6732352734\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.749246239662,3.13823004822), test loss: 2.5566849038\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (7.83771944046,30.049864913), test loss: 35.8845930576\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.43066978455,3.12023231395), test loss: 3.07344622612\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (7.56784296036,29.8169341755), test loss: 30.2388541698\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.625179886818,3.10263225076), test loss: 2.68629369438\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (6.07278490067,29.5871886593), test loss: 38.4021386385\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.81111621857,3.08540857061), test loss: 3.01102098078\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (4.7927236557,29.3634442559), test loss: 25.3740091324\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.992414295673,3.06849455915), test loss: 2.47234773785\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (16.892906189,29.1451208016), test loss: 37.8853557587\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.93181920052,3.05174506467), test loss: 3.1212224409\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (19.6833496094,28.9304711148), test loss: 30.1755959988\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.40893089771,3.03539376365), test loss: 3.31673339605\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (8.76294708252,28.7193786313), test loss: 34.3559750557\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.501999378204,3.01939433354), test loss: 2.64380648881\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (19.2128887177,28.5123741049), test loss: 33.7769284248\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.73299121857,3.00370891518), test loss: 3.21707206368\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (10.7644510269,28.3100077017), test loss: 35.0332919598\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.603555142879,2.988363757), test loss: 2.60070323795\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (14.0510520935,28.1112431848), test loss: 37.9540035725\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.347759604454,2.97342243015), test loss: 3.09043869078\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (15.0514554977,27.9174861185), test loss: 29.4388892651\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.52529078722,2.95880177982), test loss: 2.65242066979\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (6.00374507904,27.7255559315), test loss: 38.6486961842\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.961230993271,2.9444317631), test loss: 2.98574377894\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (20.2578544617,27.5391370913), test loss: 29.0193595409\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.98930954933,2.93038115557), test loss: 2.92969001085\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (5.28546047211,27.3552484255), test loss: 38.36025002\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.1173157692,2.91653060971), test loss: 3.14952562004\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (13.1547384262,27.1756021136), test loss: 30.5770782709\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (4.38024663925,2.90270054928), test loss: 3.19178931713\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (20.4267425537,26.9986365728), test loss: 36.0247169018\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.992095589638,2.88920288929), test loss: 2.6574225992\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (8.80640411377,26.8245587855), test loss: 37.1169030666\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.44795942307,2.8760745299), test loss: 3.29417422414\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (5.4643034935,26.6533232028), test loss: 33.7106272221\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.78808391094,2.86300505483), test loss: 2.46167761385\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (7.96657943726,26.4860770797), test loss: 39.0313642263\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.601238131523,2.8503663332), test loss: 3.17892895341\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.0078315735,26.3219316071), test loss: 28.8124840736\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.15582132339,2.83796522163), test loss: 2.59490757883\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (9.86727809906,26.1608178465), test loss: 38.9117004633\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.47765159607,2.82580638971), test loss: 2.96321209669\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (6.19879150391,26.0014533872), test loss: 31.0221810341\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.20208728313,2.8138436938), test loss: 3.06401637495\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (10.960278511,25.8459228718), test loss: 38.8338619709\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.22404742241,2.8020476492), test loss: 3.13938138187\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (9.7091999054,25.6932372437), test loss: 30.4979204178\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.71800518036,2.79029387005), test loss: 3.1504691571\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (14.5212783813,25.5430477766), test loss: 36.8539036751\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.83502101898,2.77879481561), test loss: 2.64682382047\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.30588722229,25.3944145866), test loss: 38.1934734106\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.75255513191,2.76750954158), test loss: 3.26018871963\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (6.25721263885,25.2483124602), test loss: 32.1548079491\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.39827752113,2.75638448221), test loss: 2.52119192183\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.39441108704,25.105046276), test loss: 39.0316664934\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.39710104465,2.74545365297), test loss: 3.11382635534\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (13.2098369598,24.9641165489), test loss: 28.3488457203\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.80232715607,2.73480300683), test loss: 2.66768606007\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (7.4881439209,24.8260222256), test loss: 38.7800918579\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.14503383636,2.72432646473), test loss: 2.89184676111\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (16.2909164429,24.6891969115), test loss: 29.5876919985\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.21589136124,2.71401522189), test loss: 3.16508613229\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (7.83423900604,24.5555025903), test loss: 39.6804358959\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.75563240051,2.70389499335), test loss: 3.14977064729\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (7.98289680481,24.4235213067), test loss: 33.2435564518\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.889926195145,2.69387174335), test loss: 3.18246414363\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.0985679626,24.2941338641), test loss: 36.982623291\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (4.0682554245,2.68385937321), test loss: 2.66883924007\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (16.7020874023,24.1663292099), test loss: 38.7526189804\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.927022695541,2.67402897444), test loss: 3.17674394846\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (16.3934783936,24.040161988), test loss: 33.7854121208\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (3.71083855629,2.66445704079), test loss: 2.57255013436\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (11.2692737579,23.9156172748), test loss: 38.4605599642\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.21256542206,2.65488972222), test loss: 2.96997300088\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (9.02770805359,23.7936118571), test loss: 28.5114119053\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.707469582558,2.6456113053), test loss: 2.61939615309\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (13.5814418793,23.673625314), test loss: 39.6543909073\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.638760089874,2.63649340597), test loss: 3.01807969511\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (18.2235355377,23.5556692425), test loss: 29.9517497301\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.11180496216,2.62754991414), test loss: 3.10409123302\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (8.65295410156,23.4383519991), test loss: 40.3361633062\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.379081726074,2.61870888937), test loss: 3.17670586109\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (17.6375083923,23.3239501563), test loss: 35.5252049446\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.81466281414,2.60997993478), test loss: 3.2161282599\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (4.78445529938,23.2108660983), test loss: 36.6916423321\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.38557481766,2.60121668328), test loss: 2.52247155011\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (11.638838768,23.0996992913), test loss: 38.5956726313\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.87839770317,2.59267358372), test loss: 3.09658549726\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (9.49434566498,22.9892775449), test loss: 32.831838131\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.79231905937,2.5842483806), test loss: 2.68211527467\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (8.31659507751,22.8804037452), test loss: 39.6613619804\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (4.5666847229,2.57596568379), test loss: 2.9927157104\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (7.67957496643,22.7733378645), test loss: 27.8852202415\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.74091911316,2.56772639116), test loss: 2.55392411351\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (8.4407043457,22.6676921925), test loss: 39.8966908216\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.60439682007,2.55971697959), test loss: 3.06608225852\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (7.75325870514,22.5640834055), test loss: 30.0748281479\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.22213304043,2.55182127075), test loss: 3.10726203024\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (18.7506160736,22.4611596677), test loss: 36.6713208199\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.12405228615,2.54407322658), test loss: 2.7694370389\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (2.84735441208,22.360282535), test loss: 35.1313580275\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.22602128983,2.5364073783), test loss: 3.23027862161\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (8.52274990082,22.2605296001), test loss: 37.0873418808\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.01762509346,2.52882847612), test loss: 2.60527704209\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (10.8220701218,22.1625311509), test loss: 39.3554642677\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (3.76808214188,2.52121380273), test loss: 3.11761221588\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (8.65655231476,22.0654677566), test loss: 30.0044010162\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.70685386658,2.5137221693), test loss: 2.60607500821\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (14.4593009949,21.9694194698), test loss: 41.033082819\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.79499483109,2.5064268244), test loss: 3.08173386753\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (17.9601650238,21.8744140256), test loss: 26.9051546574\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.21176791191,2.49909826685), test loss: 2.45226593018\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (11.9236412048,21.7809824772), test loss: 40.1902835608\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.52681577206,2.49199846906), test loss: 3.07872886658\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (8.76439189911,21.6889922615), test loss: 29.9149341822\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.510118424892,2.48498625856), test loss: 3.06787217557\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (17.996925354,21.5984534004), test loss: 37.3316019535\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.35489153862,2.47812249879), test loss: 2.86303680539\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (7.29522132874,21.5081340341), test loss: 34.6893380165\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.22576141357,2.47132559323), test loss: 3.23389779031\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (13.6488142014,21.4200172092), test loss: 37.7452986717\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.0411028862,2.46458300991), test loss: 2.62526255399\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (3.91859459877,21.3326154485), test loss: 40.2315324068\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.22835969925,2.45781489597), test loss: 3.15221259892\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (3.43799424171,21.2465632406), test loss: 30.0683832645\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.955938994884,2.45118028418), test loss: 2.61121578813\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (10.2223415375,21.1609660438), test loss: 40.8651724815\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.79548883438,2.44468780286), test loss: 2.97689083964\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (10.8068656921,21.076336541), test loss: 27.559482193\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (3.81112766266,2.43820930103), test loss: 2.646399948\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (11.6564369202,20.99293006), test loss: 39.8868921757\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.88581752777,2.43179116359), test loss: 3.10138067305\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (3.41718912125,20.9104284912), test loss: 31.4490854263\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.882669568062,2.42553141289), test loss: 3.17326425314\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (5.36659049988,20.8294369338), test loss: 37.5250573158\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.475978434086,2.41936110156), test loss: 2.71438587457\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (7.26260566711,20.748876395), test loss: 37.9307403088\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.40650987625,2.41331423852), test loss: 3.30500042439\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (8.31275558472,20.6698510651), test loss: 35.5106154442\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.13512957096,2.4072998942), test loss: 2.49597304463\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (15.1177711487,20.5915670187), test loss: 40.804244113\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.88097465038,2.40134215335), test loss: 3.21537209451\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (10.1075592041,20.5143373883), test loss: 30.2974405766\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.70963692665,2.39535388839), test loss: 2.65485147834\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (7.3257188797,20.4378108021), test loss: 40.1596949339\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.69376146793,2.3894472197), test loss: 2.9441317454\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (8.90741252899,20.3619126176), test loss: 31.4162525177\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.74851644039,2.38368693092), test loss: 2.96987279952\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (15.0741500854,20.2867662615), test loss: 39.8869006634\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.65121150017,2.3778988055), test loss: 3.10197577775\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (17.3580131531,20.2127908945), test loss: 31.6595687389\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.13750290871,2.37227150376), test loss: 3.09401294887\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (3.90754079819,20.1396193891), test loss: 38.9299212456\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.812916994095,2.36670245369), test loss: 2.66476472318\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (8.52427196503,20.0676838116), test loss: 40.9781230688\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.01248669624,2.36124811154), test loss: 3.33307663798\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.94425964355,19.9957349121), test loss: 34.2350326061\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.93527376652,2.3558579969), test loss: 2.55853058696\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (7.19953155518,19.9255356582), test loss: 40.844627285\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.723204612732,2.35047244489), test loss: 3.17035437822\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (3.65364360809,19.8556380069), test loss: 29.4937565327\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (3.03006362915,2.34506593261), test loss: 2.68026115745\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (2.49561357498,19.7867888154), test loss: 40.4931535721\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.762636005878,2.33975127468), test loss: 2.90092378184\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (7.15954494476,19.7182054092), test loss: 30.1650690556\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (4.54884529114,2.33456554455), test loss: 3.02695606649\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (5.37774324417,19.6502223511), test loss: 40.6293456793\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.65807330608,2.3293576707), test loss: 3.11747116148\n",
      "\n",
      "MC # 2, Hype # hyp4, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold4/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (296.07421875,inf), test loss: 183.45008316\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (336.269836426,inf), test loss: 379.812153625\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (29.1205596924,93.1003686562), test loss: 49.8115135193\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.28945469856,83.3745387439), test loss: 3.71015084982\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (15.6598558426,70.592277771), test loss: 36.1490925312\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.64150309563,43.2061138886), test loss: 2.84566581249\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (41.694442749,63.1742668845), test loss: 47.5530118942\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.4300994873,29.808571294), test loss: 3.58014405668\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (29.4383277893,59.315206141), test loss: 40.4665712833\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.528175354,23.0950316792), test loss: 3.52320979834\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (18.4660377502,57.060683942), test loss: 43.4211486816\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.40521359444,19.073846074), test loss: 3.10265743136\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (44.8375511169,55.4509049342), test loss: 44.9403810978\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.26302886009,16.3887477239), test loss: 3.58728310466\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (37.9670639038,54.3079665467), test loss: 44.9743485451\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.6731262207,14.4703506527), test loss: 2.93199365735\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (166.354156494,53.3998730057), test loss: 45.6543534756\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (7.30272102356,13.0323274514), test loss: 3.62922740877\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (31.3303031921,52.6175446948), test loss: 40.6215735912\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.15425443649,11.9112701416), test loss: 3.08064749837\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (29.7050476074,52.0100864922), test loss: 47.5927569866\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.6830124855,11.0159304635), test loss: 3.5065848738\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (126.8540802,51.4699659159), test loss: 39.4469355583\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.14261722565,10.2801202145), test loss: 2.89899795949\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (85.5616073608,51.0361755074), test loss: 48.1190386295\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.83961248398,9.66833021304), test loss: 3.58799474537\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (28.6915130615,50.6025336427), test loss: 34.4938082695\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.5296831131,9.14960877661), test loss: 2.74816457629\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (56.3241424561,50.2129515531), test loss: 45.5792141676\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.05676364899,8.70483436378), test loss: 3.46821242571\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (20.1563987732,49.8547240945), test loss: 38.1474940777\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.65788674355,8.31707613195), test loss: 3.43142139763\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (59.2225723267,49.5064204465), test loss: 42.5884826183\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (6.40087461472,7.97785331588), test loss: 3.08122153282\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (32.2331123352,49.1595838205), test loss: 42.1498783588\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.34265041351,7.67745719497), test loss: 3.50204371214\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (53.7196388245,48.8410688697), test loss: 42.372616601\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.588569641113,7.40861800281), test loss: 2.82955379784\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (29.6324157715,48.5517234109), test loss: 43.0395370483\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.18596363068,7.16853055848), test loss: 3.66696878672\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (19.0389308929,48.2579554502), test loss: 39.1178375721\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.65237903595,6.95156235555), test loss: 2.91544765234\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (41.6534576416,47.9626380468), test loss: 44.4585769653\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.86231899261,6.75456040042), test loss: 3.41786746681\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (41.6816940308,47.657296684), test loss: 36.603839159\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.888038635254,6.57175987063), test loss: 3.03491875827\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (48.9420547485,47.3464226422), test loss: 44.4506418228\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.67657113075,6.40491978191), test loss: 3.27960266769\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (32.1625785828,47.0238559297), test loss: 31.9764968872\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.44190835953,6.2502538824), test loss: 2.74303823709\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (153.889709473,46.7164526922), test loss: 42.2608370781\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (5.69324493408,6.10723761292), test loss: 3.34541341066\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (12.3081111908,46.3910221597), test loss: 33.8891145706\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.07515025139,5.97347088262), test loss: 3.28506363332\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (22.4619369507,46.0584201869), test loss: 40.5718050718\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.81924414635,5.84981566391), test loss: 3.16590694189\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (18.9259643555,45.6961413474), test loss: 35.5394169807\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.456594794989,5.73322558423), test loss: 3.363070485\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (30.3802967072,45.33504629), test loss: 37.1512473106\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.816297471523,5.62318235763), test loss: 2.7357831955\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (22.8055534363,44.9509957769), test loss: 37.0540504456\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.2197227478,5.5196451461), test loss: 3.40474933684\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (86.1910934448,44.5584031978), test loss: 34.7436526299\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (5.3126411438,5.42138374006), test loss: 2.81081437171\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (21.0148944855,44.1602538642), test loss: 38.8295613766\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.35528349876,5.32821808745), test loss: 3.28017634153\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (11.0577440262,43.7422531982), test loss: 30.4690915823\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.38975286484,5.2393305871), test loss: 2.69305357337\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (59.0529212952,43.3275852131), test loss: 37.9932331085\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.3484351635,5.15497352529), test loss: 3.15850064456\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (62.438331604,42.8914586518), test loss: 26.8108080864\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.06730818748,5.0743883292), test loss: 2.66934527755\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (33.4361343384,42.4440107554), test loss: 35.1363639116\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.04107165337,4.99758387537), test loss: 3.26640450954\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (37.0247955322,41.9906530534), test loss: 27.5195373058\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.31827783585,4.92390701632), test loss: 3.27782259583\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (18.8776569366,41.5190030715), test loss: 33.6459543228\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.89124655724,4.85309918019), test loss: 3.34147208631\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (19.3780536652,41.0418114911), test loss: 28.2479968548\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.759127378464,4.7853905921), test loss: 3.33261577785\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (6.70550394058,40.5621172719), test loss: 29.910844183\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.1184926033,4.71990669488), test loss: 2.54852313995\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (21.2236919403,40.0736648952), test loss: 30.5268802643\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.58613395691,4.6571537725), test loss: 3.42040555477\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (48.2159194946,39.5700010958), test loss: 28.4687980175\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.56418585777,4.5964465207), test loss: 2.67807204127\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (43.5370025635,39.0668815506), test loss: 33.083129549\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.4870262146,4.5386351165), test loss: 3.40238046646\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (13.8773651123,38.5683191607), test loss: 26.4028247833\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.3155632019,4.48259933721), test loss: 2.79640961736\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (10.8393421173,38.0714001488), test loss: 31.8404706478\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.83379769325,4.42895792198), test loss: 3.08292029798\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (14.2203826904,37.5851346484), test loss: 24.3159095764\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.40203428268,4.37709888311), test loss: 2.76848336458\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (8.09700870514,37.111331388), test loss: 32.1765459061\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.38738811016,4.32731827786), test loss: 3.43938906789\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.703127861,36.649045202), test loss: 26.1253807545\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.08068919182,4.27917630463), test loss: 3.48175095916\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (9.98593139648,36.1955487037), test loss: 32.7371259212\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (6.16079092026,4.23323315853), test loss: 3.55577866733\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (4.08803844452,35.7525882712), test loss: 26.491760087\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.58196878433,4.18899350499), test loss: 3.4326879859\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (12.0048828125,35.3243108241), test loss: 30.1474828482\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.17862331867,4.14646312424), test loss: 2.65550500602\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (14.9774227142,34.9059686353), test loss: 29.2742144823\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.69397640228,4.10549240054), test loss: 3.42362187505\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (4.27642250061,34.5000047955), test loss: 30.3995536327\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.56472414732,4.06571691527), test loss: 2.85510961264\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (3.89813137054,34.1079375901), test loss: 33.4602900267\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.37650251389,4.02725189785), test loss: 3.38754201233\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (6.50551176071,33.7267167563), test loss: 28.5069403648\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.64828109741,3.98997136078), test loss: 2.7570697993\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (11.0984096527,33.3557911096), test loss: 32.7588551283\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.02819919586,3.95392522755), test loss: 3.02233604491\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (16.0352363586,32.9941509662), test loss: 26.3507836342\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.32079315186,3.91895075509), test loss: 2.74373512864\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (11.5534706116,32.6444907775), test loss: 33.7149304867\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.6259739399,3.88532796819), test loss: 3.32706977725\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (16.4475498199,32.305702502), test loss: 28.4921114922\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.22789096832,3.8526121064), test loss: 3.49695318937\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (6.71658611298,31.9730841513), test loss: 34.4283032179\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.50827598572,3.82077062062), test loss: 3.47462739274\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (7.17171859741,31.6512362394), test loss: 27.6827811003\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.08053946495,3.78988681523), test loss: 3.4197049886\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (5.12073230743,31.3390454914), test loss: 31.1873986721\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.25080406666,3.75953910231), test loss: 2.53537766039\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.9294195175,31.0338085139), test loss: 30.0589250803\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.3419611454,3.73016085216), test loss: 3.39654383361\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (14.9397506714,30.7347280418), test loss: 32.5857394695\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.00226759911,3.70166111208), test loss: 2.71617642641\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (12.1065788269,30.4449011148), test loss: 34.2049103379\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.6290640831,3.67418562099), test loss: 3.37157701254\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.7993021011,30.1637748749), test loss: 30.2992289543\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.911945521832,3.6472408987), test loss: 2.78123401105\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (11.1337375641,29.8869164755), test loss: 33.4866520643\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.50281882286,3.62109039163), test loss: 2.95226745307\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (9.04562187195,29.6185483126), test loss: 27.1475229263\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.01806461811,3.59542015576), test loss: 2.7170617342\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (10.9683933258,29.3570228293), test loss: 34.0542742014\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (4.44619417191,3.57040679038), test loss: 3.24889582098\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (8.71170425415,29.1014813651), test loss: 28.1448417664\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.94855546951,3.54587865256), test loss: 3.36629918814\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (16.9916858673,28.8509279053), test loss: 34.8090193033\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.76802158356,3.52202288145), test loss: 3.40953694582\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (13.8736629486,28.6065501616), test loss: 29.0158302307\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.0011241436,3.49889048738), test loss: 3.36344719231\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (6.89033174515,28.3686077895), test loss: 31.7801652431\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.39492797852,3.47626582807), test loss: 2.51046865284\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (22.7442150116,28.1353668085), test loss: 30.0020091057\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.73658323288,3.45418910419), test loss: 3.37385227829\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.9182033539,27.9083598058), test loss: 32.9579688072\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (4.24143314362,3.43258482816), test loss: 2.65112692714\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.4036779404,27.6861360642), test loss: 35.0321580887\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.48022830486,3.41132041739), test loss: 3.38198669553\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.3118305206,27.4685069235), test loss: 30.803399086\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.64107608795,3.39048562026), test loss: 2.73184305429\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (30.3195133209,27.2545059162), test loss: 34.906221962\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.01140832901,3.37029120134), test loss: 3.01959429681\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (11.503534317,27.0445194203), test loss: 27.9355693817\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.04539060593,3.35038829313), test loss: 2.66893165708\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (8.39338493347,26.8402084364), test loss: 34.8627041578\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.93855774403,3.33105360298), test loss: 3.18469696045\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (12.1398077011,26.6408646603), test loss: 31.0097520351\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.55116301775,3.31220036887), test loss: 3.39182861149\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (8.8610200882,26.4442259249), test loss: 35.7471690655\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.41403341293,3.29361283404), test loss: 3.36699746251\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (3.58829140663,26.252643943), test loss: 29.5996780872\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (4.16296720505,3.27549026657), test loss: 3.37368424684\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (3.84029173851,26.0656385522), test loss: 32.519793272\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.75387001038,3.25746168956), test loss: 2.47246387005\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (6.99501895905,25.8812551289), test loss: 31.298425436\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.67802298069,3.23989126875), test loss: 3.36960034072\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.56040382385,25.6997497695), test loss: 33.3453692913\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.513848245144,3.22268104534), test loss: 2.46915438771\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (5.25348091125,25.5229614078), test loss: 35.955642271\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.53582859039,3.2060391961), test loss: 3.32273288965\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.2766199112,25.3506263433), test loss: 33.1560830593\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.358796566725,3.18962398097), test loss: 2.77113748789\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (5.97933530807,25.1797554088), test loss: 35.5438095093\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.31532120705,3.17354414726), test loss: 2.9945255816\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (7.24376773834,25.0139060198), test loss: 28.9595016479\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.78623008728,3.15769828868), test loss: 2.63519937992\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (9.69867420197,24.850740434), test loss: 34.9330560446\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.24552178383,3.14201741828), test loss: 3.02459855676\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (13.4807062149,24.6908241489), test loss: 30.4218276501\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.20385980606,3.1266354562), test loss: 3.37545123994\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (12.6829395294,24.5327304793), test loss: 36.1764640331\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.446688175201,3.11164271188), test loss: 3.30394880772\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.7043571472,24.3780787782), test loss: 29.7299336195\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.53373599052,3.09701111511), test loss: 3.34784994125\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (6.47897338867,24.2268296816), test loss: 32.9561753273\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.989673376083,3.08257627599), test loss: 2.5070002377\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (5.24735116959,24.07745103), test loss: 31.8603057623\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.467528313398,3.06850224756), test loss: 3.34888520241\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (3.59637403488,23.9319737921), test loss: 33.7512498856\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.25594902039,3.05455761243), test loss: 2.41728696376\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (7.56678581238,23.7887595469), test loss: 36.5609192371\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.660270810127,3.04081736986), test loss: 3.33704921603\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (21.0876922607,23.6482395965), test loss: 33.0479948997\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.691603899,3.02727657902), test loss: 2.71059698164\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (8.22938919067,23.5089684445), test loss: 36.1853465319\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.46277165413,3.0141004421), test loss: 3.00059021413\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.29878616333,23.3725613453), test loss: 29.6781055927\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.642962098122,3.0010131579), test loss: 2.61022414267\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (7.37286472321,23.2388297428), test loss: 35.7302378178\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.54435896873,2.98827732702), test loss: 2.98736192584\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (25.0626239777,23.1077763973), test loss: 33.0061496735\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.87584555149,2.97582962062), test loss: 3.38554912806\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (2.88405990601,22.9782068881), test loss: 36.428809309\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.57671535015,2.96346388064), test loss: 3.28563599288\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (12.3411922455,22.8514943841), test loss: 30.3355841398\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (5.21456623077,2.95130923874), test loss: 3.39853785038\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (10.7663135529,22.7271503677), test loss: 33.3598547459\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.11083889008,2.93921505013), test loss: 2.62576399595\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (10.3817443848,22.6039433238), test loss: 32.7070868015\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.15612924099,2.92743660089), test loss: 3.31607499123\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (3.70559740067,22.4824417311), test loss: 34.1789940834\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.62164473534,2.91577336057), test loss: 2.41025034934\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (3.44729876518,22.3634890005), test loss: 35.9081803799\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.32475328445,2.90447776722), test loss: 3.27461438775\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (6.55169296265,22.2470601384), test loss: 35.39793396\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.38827013969,2.89330394743), test loss: 2.6971528694\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (9.89757823944,22.1313213314), test loss: 36.9237101316\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.05498719215,2.88231132097), test loss: 3.02244047374\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (18.9440155029,22.0186685833), test loss: 30.5471969604\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.2314350605,2.87139226807), test loss: 2.60589675978\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (3.98018288612,21.9072154998), test loss: 36.0866702557\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.19005119801,2.86056226664), test loss: 2.95334891975\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (8.78000736237,21.7977018337), test loss: 31.6093970299\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.430409640074,2.8499511215), test loss: 3.30208463669\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (8.38307857513,21.6890394429), test loss: 36.6294708014\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (3.90204906464,2.83956734873), test loss: 3.24649321288\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (8.30510902405,21.5822987073), test loss: 29.8608048916\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.38636183739,2.82936054109), test loss: 3.28486537337\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (8.43882751465,21.4777212936), test loss: 33.4999882221\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.17795836926,2.81926394885), test loss: 2.65555874035\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (4.32627439499,21.3741738294), test loss: 32.6393130779\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.46455097198,2.80943552487), test loss: 3.30899730623\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (7.84560441971,21.2730672179), test loss: 34.4087162971\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.691722810268,2.79959858883), test loss: 2.39770497233\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (16.5188522339,21.172982623), test loss: 36.4158624291\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.63467383385,2.78990821608), test loss: 3.2864194721\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.1932172775,21.0743622236), test loss: 33.9866315842\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.14628696442,2.78029942722), test loss: 2.63127306402\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (10.9407920837,20.9766832828), test loss: 37.5012711287\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.07507443428,2.77095796071), test loss: 3.09248995781\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (10.3744716644,20.8805150691), test loss: 31.2319233418\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.13450813293,2.76165888862), test loss: 2.65178861916\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (9.93667888641,20.7860558896), test loss: 36.7258621931\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.42540884018,2.75253039643), test loss: 2.9561793983\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (19.5996589661,20.6928237132), test loss: 33.247594738\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.28318214417,2.74360909176), test loss: 3.16515324265\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (7.86252784729,20.6011148708), test loss: 36.9532959938\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.83254146576,2.73474698234), test loss: 3.22013656348\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (7.26699924469,20.5106210733), test loss: 30.1626111865\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.40124845505,2.72591866041), test loss: 3.27017871141\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (18.4493904114,20.42179762), test loss: 33.612656188\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.00827646255,2.71724819758), test loss: 2.60952162445\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (8.26668167114,20.3334177501), test loss: 32.9484893799\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.99156236649,2.70870868217), test loss: 3.24920166582\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (4.87702894211,20.2460846649), test loss: 34.7324198246\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.591709494591,2.70024025482), test loss: 2.36993341446\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (5.69851589203,20.1602546763), test loss: 36.2150073528\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.64435374737,2.6919937064), test loss: 3.24343061596\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (7.43551826477,20.0760901951), test loss: 36.2686178207\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.1524027586,2.68383623376), test loss: 2.68356172442\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (6.16682338715,19.9920750599), test loss: 36.1305906534\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.14621591568,2.67581623351), test loss: 3.16679838151\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (9.73433685303,19.9101479213), test loss: 31.2007082939\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.986945807934,2.6677911759), test loss: 2.63782627881\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (4.62207317352,19.8291583332), test loss: 36.9541534662\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.22540092468,2.65982142681), test loss: 2.8963652432\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (14.9898948669,19.7490793783), test loss: 32.6567069054\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.818878173828,2.6520022142), test loss: 3.15138776451\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (21.1155509949,19.6694818671), test loss: 36.5467159271\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.17820763588,2.64428944453), test loss: 3.17139380574\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (12.4557523727,19.5910553127), test loss: 30.0800633669\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.39721727371,2.63675046416), test loss: 3.25768643171\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (2.72724986076,19.5140901043), test loss: 33.9034850597\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.293135046959,2.62925431824), test loss: 2.64028394222\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (18.039981842,19.4377389808), test loss: 32.6823804855\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.07801377773,2.62193180391), test loss: 3.24392392039\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (14.6124477386,19.3630063222), test loss: 35.1701109409\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.68534564972,2.61460445104), test loss: 2.37576726675\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.47616291046,19.2886944845), test loss: 36.2020627499\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.25759148598,2.60735089849), test loss: 3.25683439672\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (7.1878900528,19.2154767294), test loss: 34.6311523914\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.4553912878,2.60014942545), test loss: 2.59721190333\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (8.39231204987,19.1427202077), test loss: 36.3194046974\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (3.9756231308,2.59313233406), test loss: 3.10571843982\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (9.40124797821,19.0709167404), test loss: 31.4977870464\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.8067252636,2.58612683683), test loss: 2.64322812706\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (9.83701229095,19.000293799), test loss: 37.8396697521\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.6230224967,2.57923229158), test loss: 2.94222483039\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (4.75464344025,18.9302208633), test loss: 32.7135032177\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.77882575989,2.57252078595), test loss: 3.09493648708\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (3.57347154617,18.861554703), test loss: 36.5291919231\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.60138607025,2.56578612026), test loss: 3.15909869075\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.84582519531,18.7934352301), test loss: 30.656547451\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.56485581398,2.55909785175), test loss: 3.26516509652\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (2.69622612,18.726292961), test loss: 33.8603847027\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.52676200867,2.55249714312), test loss: 2.56630375981\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.00113964081,18.6595483095), test loss: 33.219229269\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.19082593918,2.54600194946), test loss: 3.22655678988\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (13.9514436722,18.593421509), test loss: 35.2620092869\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.10199594498,2.53954546787), test loss: 2.29780627489\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (8.59819030762,18.5282710102), test loss: 36.7589251995\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.343193143606,2.53322758794), test loss: 3.24069673419\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (19.2250671387,18.4642541649), test loss: 37.2699272156\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.26537013054,2.52698564774), test loss: 2.63416614532\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.98267889023,18.4002368863), test loss: 36.3320708513\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.53690743446,2.52083050646), test loss: 3.08427758217\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (11.4880924225,18.3376853568), test loss: 31.6420400143\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.599581539631,2.51466373848), test loss: 2.65043388754\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (4.75048589706,18.2757792214), test loss: 38.1347610712\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.66331291199,2.50849782634), test loss: 2.90286266208\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (9.30760192871,18.2142606158), test loss: 32.5425719738\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.27834296227,2.50247436397), test loss: 3.07289390564\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (15.3302993774,18.1530562935), test loss: 37.0687206984\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.02584314346,2.49651625022), test loss: 3.07246146202\n",
      "\n",
      "MC # 2, Hype # hyp4, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold5/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (384.482147217,inf), test loss: 217.066049957\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (341.257598877,inf), test loss: 409.339399719\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (129.752304077,196.100346924), test loss: 102.740728188\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.80841994286,230.74612269), test loss: 4.10221987367\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (170.554489136,129.672485612), test loss: 43.9816648245\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.76922798157,117.007757039), test loss: 3.45778468847\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (56.9811553955,102.203324681), test loss: 44.0986763954\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.86964190006,79.0929665024), test loss: 3.50720280409\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (44.7700500488,88.4406761458), test loss: 45.785573101\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.77136576176,60.1312815185), test loss: 3.66878287196\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (44.8749008179,80.2027510633), test loss: 42.4520203352\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.22408485413,48.7561100402), test loss: 3.03120898604\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (51.5718765259,74.7599566714), test loss: 49.1144599915\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.61011898518,41.1771845477), test loss: 3.89910550117\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (19.0364151001,70.7796027075), test loss: 39.2439974546\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.17668509483,35.761658159), test loss: 2.98076976538\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (7.95157384872,67.7938604703), test loss: 47.5473683357\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.08246088028,31.7069676044), test loss: 3.7160728395\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (23.6455421448,65.4614775002), test loss: 35.6212447643\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.91521167755,28.5488732253), test loss: 3.06009458303\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (26.0541343689,63.5803884197), test loss: 42.7178891659\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.88578557968,26.0254229335), test loss: 3.49543981552\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (36.6388587952,62.0154242172), test loss: 42.275160265\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.66206216812,23.9600739054), test loss: 3.4785712719\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (12.1279830933,60.6688236437), test loss: 44.4377792835\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.09284472466,22.2388105701), test loss: 3.44464812279\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (33.6300811768,59.577678628), test loss: 47.1308990479\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (7.26614522934,20.7838874471), test loss: 3.65642654896\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (7.32468414307,58.5940481121), test loss: 42.1823280573\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.31588315964,19.5349995445), test loss: 3.02688390315\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (26.8216075897,57.7613193153), test loss: 47.1591163158\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.26454114914,18.453706192), test loss: 3.98466146588\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (44.2772941589,57.0252045048), test loss: 37.4056136131\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (0.801186323166,17.5094220395), test loss: 2.9723731041\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (34.6555747986,56.3547981425), test loss: 46.991895771\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.416871190071,16.6760886217), test loss: 3.73948372006\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (29.4602069855,55.7474998435), test loss: 35.6710228443\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.45073306561,15.9365380913), test loss: 2.85706611574\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (60.2485160828,55.2027326213), test loss: 41.712676692\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (5.12813949585,15.2741051287), test loss: 3.56952713728\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (98.310836792,54.6907844387), test loss: 41.7513082743\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.38559675217,14.6784187664), test loss: 3.68276746869\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (94.1151885986,54.2233455096), test loss: 44.2281315327\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.80572390556,14.1397148465), test loss: 3.49803891182\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (27.9411487579,53.7734814307), test loss: 46.9921453476\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.38282155991,13.6501821796), test loss: 3.82306193113\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (185.78515625,53.3764918166), test loss: 40.9024588585\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (8.53606128693,13.2028739194), test loss: 2.84141635895\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (21.0715866089,52.9852113755), test loss: 45.9242409706\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.49720001221,12.7927348586), test loss: 3.93027674556\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (26.3126487732,52.6441072845), test loss: 37.2065607071\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.93429839611,12.4163573075), test loss: 3.00180674791\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (20.8325424194,52.3013283675), test loss: 43.8991786242\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (4.42196273804,12.0687714743), test loss: 3.65893193483\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (29.8920688629,51.9796314415), test loss: 35.5139094353\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (4.06719398499,11.7486102085), test loss: 2.83619528711\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (40.6235351562,51.6718373764), test loss: 41.3234034061\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (8.8007068634,11.4500627784), test loss: 3.54768992662\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (19.4481086731,51.3729959447), test loss: 41.4666679382\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.88586330414,11.1725517123), test loss: 3.59265485406\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (38.6642074585,51.0812111013), test loss: 37.21437006\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.95338487625,10.9134030318), test loss: 2.97999199629\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (94.3388671875,50.7895817761), test loss: 46.6262372971\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (4.24929141998,10.6710275764), test loss: 3.89959731698\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (35.7951087952,50.5209193656), test loss: 38.4392099857\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.56556844711,10.4440164282), test loss: 2.88432622254\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (27.7701644897,50.2462668834), test loss: 45.7860833645\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.21427130699,10.2298155769), test loss: 3.90323022604\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (19.0030250549,49.9876138132), test loss: 34.9844193459\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.52141487598,10.0285825455), test loss: 3.08466077447\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (24.1037979126,49.7334748634), test loss: 41.5603000641\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.0467145443,9.83950534159), test loss: 3.59646163583\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (13.8328609467,49.4756198757), test loss: 33.3561594009\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.57923150063,9.66053211404), test loss: 2.90438227654\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (32.5086402893,49.2193607517), test loss: 39.3338639259\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.998227059841,9.49145787632), test loss: 3.48833488524\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (25.4316825867,48.9668132335), test loss: 40.5135482788\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.51890563965,9.33070883429), test loss: 3.62981796265\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (105.388916016,48.7106344758), test loss: 36.7552475214\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (7.32456302643,9.17826050532), test loss: 2.92967628837\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (60.9009132385,48.4467945743), test loss: 42.5386909008\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.48506557941,9.03259433992), test loss: 3.84331464767\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (16.197517395,48.1751026149), test loss: 35.9341197968\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (4.58976602554,8.89367083875), test loss: 2.88128842115\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (42.0154495239,47.9115666626), test loss: 42.0689284325\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.11299610138,8.76032353124), test loss: 3.77488347888\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (15.1322507858,47.6328127801), test loss: 32.1422283649\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.33983802795,8.63269782765), test loss: 3.01054421961\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (19.5843009949,47.3622796957), test loss: 38.037022543\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.57795739174,8.51065426508), test loss: 3.56144877672\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (42.5376701355,47.0769663175), test loss: 30.4753233433\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.49385309219,8.39330827987), test loss: 3.12140058875\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (19.4073429108,46.7848029276), test loss: 37.0358800888\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.65311050415,8.28122433855), test loss: 3.35108838975\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (12.4953403473,46.4863537239), test loss: 36.9087050915\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (9.51263999939,8.17285300885), test loss: 3.51109645963\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (24.2382011414,46.1796093333), test loss: 33.4830245018\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.39396476746,8.06806195394), test loss: 2.82993148863\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (13.3611431122,45.8605062888), test loss: 39.1538189411\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.25368022919,7.96690399821), test loss: 3.68523259163\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (18.530040741,45.5299995425), test loss: 32.0617054701\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.37122523785,7.86880896732), test loss: 2.94846750796\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (21.1950073242,45.1976979053), test loss: 37.0984370232\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.55001592636,7.77391198158), test loss: 3.56440939903\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (20.5679626465,44.8522382454), test loss: 27.8878620148\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.82931816578,7.68121181546), test loss: 2.85031130612\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (20.3779659271,44.5003494532), test loss: 32.9392571449\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.67218112946,7.59129008074), test loss: 3.50338674188\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (13.8480272293,44.1401612896), test loss: 30.7372168064\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.78199541569,7.50407746698), test loss: 3.13624687791\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (8.62533378601,43.765587361), test loss: 32.5103686571\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.85226118565,7.41888016116), test loss: 3.27189091444\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (60.5938224792,43.3851508053), test loss: 31.5899448395\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (4.40804862976,7.33616717062), test loss: 3.36912467778\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (17.8273601532,42.9979393551), test loss: 30.4432103157\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.70292568207,7.25494519118), test loss: 2.71545228958\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (26.5060939789,42.6063891146), test loss: 34.523257494\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.34407651424,7.17576364564), test loss: 3.46413248777\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (21.9085922241,42.2089836664), test loss: 28.8865743637\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.18648219109,7.09814530407), test loss: 2.74538602084\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (9.08948707581,41.8113371223), test loss: 34.1868432522\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.43041503429,7.02241504431), test loss: 3.33883961737\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.6913795471,41.4186291399), test loss: 24.5388616562\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.15835690498,6.94813223372), test loss: 2.71798439324\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (37.4702453613,41.0249352175), test loss: 31.5719208717\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.45858049393,6.87554088195), test loss: 3.39488199949\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (6.40809059143,40.6351068811), test loss: 28.6795323849\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.365222036839,6.80464863267), test loss: 3.24258919358\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (36.7906799316,40.2453282249), test loss: 33.8966501474\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.58169269562,6.73511004838), test loss: 3.38588535041\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (24.4027519226,39.8592570927), test loss: 31.7492032528\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.948692202568,6.66739592124), test loss: 3.41089224964\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.867931366,39.4798210319), test loss: 32.0844974041\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.751262784,6.60124015587), test loss: 2.69709443301\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (15.9364347458,39.1082138575), test loss: 33.40652318\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.04196858406,6.53659423458), test loss: 3.43818334937\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (10.4412460327,38.7401730737), test loss: 31.4941870689\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.46791601181,6.47342444078), test loss: 2.80713953078\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (12.8036231995,38.3802738695), test loss: 36.4368624926\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.1255004406,6.41177937989), test loss: 3.44980661273\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (5.46396970749,38.0287596089), test loss: 26.9609176636\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.90659809113,6.35165368087), test loss: 2.60487343073\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (5.90337848663,37.6844196608), test loss: 34.3771492004\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.70338332653,6.29269405059), test loss: 3.5200528264\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (9.73212432861,37.3467317876), test loss: 29.567644453\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.47560882568,6.23525381565), test loss: 3.41969624013\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (7.26887607574,37.0157314856), test loss: 36.5345887184\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.99942374229,6.17934390562), test loss: 3.45797985792\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (11.0034103394,36.6893140858), test loss: 33.012327528\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.746219813824,6.1243991499), test loss: 3.52098888755\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.4762325287,36.3711091203), test loss: 33.9805872798\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.11950337887,6.07114676507), test loss: 2.62335209399\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (12.2126064301,36.0595504084), test loss: 36.4338501692\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.02034330368,6.01894821298), test loss: 3.47987286448\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (17.1009635925,35.7544688249), test loss: 33.7487675667\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.31595754623,5.96795052941), test loss: 2.86259813905\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (16.286687851,35.4539162795), test loss: 36.9568058729\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (4.42259407043,5.91808064846), test loss: 3.35474700928\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.32843208313,35.1604142687), test loss: 28.1956455708\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.17810940742,5.86921304283), test loss: 2.66705820858\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.3291873932,34.874407975), test loss: 36.6237746716\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.40913271904,5.82143919956), test loss: 3.59354021549\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (14.7704334259,34.5933053182), test loss: 29.0076038122\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.51388025284,5.77468845789), test loss: 3.24784252346\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (8.0004119873,34.317198817), test loss: 33.7532549858\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.353917270899,5.72899600645), test loss: 3.06466557086\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (6.84275341034,34.0459371907), test loss: 32.2866914749\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.45507383347,5.68431153107), test loss: 3.51513059139\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.293504715,33.7800588901), test loss: 35.1185573578\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.696554660797,5.64071236516), test loss: 2.69242007434\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.5791454315,33.5193413108), test loss: 39.2660485268\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.352467268705,5.59808311619), test loss: 3.55492652059\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (13.0594997406,33.2646662122), test loss: 30.6023623466\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.604670643806,5.55633699965), test loss: 2.87001528293\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (13.0561132431,33.0129751349), test loss: 37.3138482094\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.417399734259,5.51536785769), test loss: 3.34304205477\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (19.1370258331,32.7671726458), test loss: 27.6833557606\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (3.25207519531,5.47528280213), test loss: 2.74562302828\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (7.1244392395,32.5259080414), test loss: 37.6049710751\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.87512230873,5.4359497104), test loss: 3.54542289972\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (13.0870809555,32.2895660081), test loss: 30.4370229721\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.74524736404,5.39731848091), test loss: 3.29119024873\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (16.2335262299,32.0571207546), test loss: 35.7864546776\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.905489146709,5.35940076016), test loss: 2.96792140603\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (19.0487289429,31.8286931263), test loss: 34.218759346\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (4.38796806335,5.32243116818), test loss: 3.44163761139\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (5.02977371216,31.6029151865), test loss: 34.7526088238\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.73416495323,5.28598272063), test loss: 2.73208447844\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.7560329437,31.3826963161), test loss: 39.003690505\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.34133172035,5.25052899919), test loss: 3.58557114601\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (8.23269176483,31.1660979926), test loss: 31.02679286\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.823597311974,5.21566325355), test loss: 2.85816225708\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (13.297539711,30.953744571), test loss: 39.3267133236\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.89989829063,5.18149711715), test loss: 3.43446879089\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (5.48080205917,30.7436874172), test loss: 29.9797421455\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.982648968697,5.14786273102), test loss: 3.25726108849\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (14.1726961136,30.5384651224), test loss: 37.6867077827\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.30867528915,5.11483248636), test loss: 3.45947132409\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (4.04362297058,30.3368994912), test loss: 31.448267436\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.74265921116,5.08233525568), test loss: 3.33814966381\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (13.4127349854,30.1388794958), test loss: 36.6470872879\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.84483718872,5.0504843023), test loss: 2.81882498562\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (15.7163171768,29.9433694542), test loss: 37.4360821009\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.33033180237,5.01917272046), test loss: 3.46155855656\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (11.2626533508,29.7507835599), test loss: 34.4363919258\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (5.52993583679,4.98847554092), test loss: 2.86138885468\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (8.83814048767,29.5614618416), test loss: 40.2091982126\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.47477924824,4.95828484369), test loss: 3.44625101089\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.7759227753,29.3753761524), test loss: 30.7677383423\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.6802406311,4.92875292013), test loss: 2.80680281073\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.81920433044,29.1929450658), test loss: 38.4966497421\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.58029532433,4.89968763275), test loss: 3.45157440901\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (9.86948013306,29.0124277369), test loss: 31.3008203506\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.88067555428,4.87108340635), test loss: 3.11516809762\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (10.2013902664,28.8356449643), test loss: 39.2801303148\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.10177612305,4.84298466578), test loss: 3.4878975004\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (9.99710273743,28.6617065324), test loss: 31.1474387169\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.00044393539,4.8153020852), test loss: 3.28332901299\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (9.00386619568,28.4907350313), test loss: 37.8232094288\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (3.48050546646,4.78802311373), test loss: 2.8321718663\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (6.05586338043,28.3221484857), test loss: 38.6750126839\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.31905639172,4.7611366096), test loss: 3.39144781232\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (12.9946374893,28.1561685162), test loss: 36.5113330841\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.7885863781,4.73486696776), test loss: 2.91461550444\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (15.00411129,27.9916423371), test loss: 40.666831255\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.22331690788,4.70880810882), test loss: 3.33131651878\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (12.5881080627,27.8306395281), test loss: 29.1643117905\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.33462321758,4.68342824098), test loss: 2.76698539406\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (5.90412807465,27.6720242183), test loss: 40.239922905\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.663764238358,4.65837089884), test loss: 3.42998259217\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (17.2951316833,27.5162994354), test loss: 31.8882762909\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.26012325287,4.63376514719), test loss: 3.20111707747\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.01972293854,27.3616858836), test loss: 41.0154837132\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.35806834698,4.60946150363), test loss: 3.56093194038\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (7.16956281662,27.2105700596), test loss: 33.516476202\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.11691069603,4.58549765733), test loss: 3.37020445168\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (5.64625644684,27.0614866339), test loss: 37.3453937531\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.99894785881,4.56184571915), test loss: 2.79884785414\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (4.73413705826,26.9147926637), test loss: 39.1133032322\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.32925653458,4.53862557233), test loss: 3.36408113241\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (12.1685276031,26.769693655), test loss: 35.6765202045\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (4.66089200974,4.51576797351), test loss: 2.99387204349\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (13.8382644653,26.626431724), test loss: 42.1435265303\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.27146816254,4.49320566184), test loss: 3.50512533784\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (9.57170009613,26.4852198322), test loss: 30.6802936077\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.38263821602,4.47095216656), test loss: 2.62561389208\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (5.53038024902,26.3460576472), test loss: 40.0144402504\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.88680303097,4.44917346813), test loss: 3.46397193074\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (6.65190029144,26.2095293227), test loss: 30.472912097\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.402531117201,4.42765533014), test loss: 3.2489133954\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (16.4015960693,26.0742558732), test loss: 41.4178473473\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.38767933846,4.40647488047), test loss: 3.30435948074\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (2.64243960381,25.9413062179), test loss: 34.9198014259\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.30957174301,4.38558091474), test loss: 3.43132022619\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.26250648499,25.8104213772), test loss: 38.1296764374\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.04057025909,4.36494512544), test loss: 2.66687936932\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.08355808258,25.6813881189), test loss: 41.3474345446\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.95931482315,4.34457968406), test loss: 3.41621437669\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (6.25648641586,25.5539091276), test loss: 33.4005292416\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.63868021965,4.32443727766), test loss: 2.88592428863\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (9.78323554993,25.4281629176), test loss: 41.5326683521\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.087854743,4.30471253407), test loss: 3.27779844105\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (15.1822032928,25.3032582559), test loss: 30.4304600239\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.0492413044,4.28509985742), test loss: 2.70022538006\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (17.4200820923,25.1808666184), test loss: 40.6071331501\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.76950228214,4.26594708708), test loss: 3.48599313498\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (4.75404167175,25.0598916183), test loss: 31.6843127728\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.726791858673,4.24701111831), test loss: 3.16321211755\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.39764404297,24.9410328082), test loss: 38.8543660641\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.679965794086,4.2283743622), test loss: 3.08584352583\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (14.930644989,24.823006787), test loss: 34.1565698624\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.12324857712,4.20995320161), test loss: 3.39158530831\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (4.28425264359,24.7071986191), test loss: 37.7589818478\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.45653879642,4.19174116288), test loss: 2.70176480561\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (5.81211185455,24.5928298563), test loss: 41.6964422703\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.32179641724,4.17371604312), test loss: 3.52876217663\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (6.08244419098,24.4801251621), test loss: 32.0799864769\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.08280014992,4.15597439883), test loss: 2.84274006784\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (5.24171304703,24.3684141133), test loss: 40.3161538601\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.484708666801,4.1384928745), test loss: 3.20818408132\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (10.8691701889,24.2579140795), test loss: 29.1999516964\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.22480630875,4.12119403781), test loss: 2.75753457546\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (8.16893768311,24.1488488007), test loss: 41.3042271137\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.60861194134,4.10410257266), test loss: 3.37463726997\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.4704914093,24.0411180372), test loss: 32.2271542549\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.66312921047,4.08732729144), test loss: 3.16661325991\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (7.57605743408,23.9353272528), test loss: 38.8040378571\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.0353167057,4.07074353712), test loss: 2.94962753952\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (4.19625854492,23.8302865373), test loss: 37.0243654251\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.96058893204,4.0543977525), test loss: 3.34522331953\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (10.3128070831,23.7270432382), test loss: 37.2612300396\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.635934948921,4.0382105444), test loss: 2.79180939198\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (17.1948642731,23.6252280114), test loss: 41.887835598\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.67609190941,4.02222211341), test loss: 3.47804521918\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.57267856598,23.5244508487), test loss: 32.6932352066\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.28811240196,4.00638174513), test loss: 2.85199277699\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (20.0779342651,23.4250143217), test loss: 41.3780565023\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (3.48898720741,3.99074818251), test loss: 3.26964671016\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (5.30006217957,23.3263817729), test loss: 32.1721755743\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.58905595541,3.9753357504), test loss: 3.15116681457\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (22.4751033783,23.2286755027), test loss: 40.0924165249\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.48584294319,3.96001736725), test loss: 3.37617643476\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.0789308548,23.1323640258), test loss: 34.9989126682\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.20530962944,3.94502982512), test loss: 3.23602681458\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (8.60498809814,23.0372695624), test loss: 39.8570204258\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.28186368942,3.93018767891), test loss: 2.77491016984\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.58089828491,22.9436341754), test loss: 40.314251256\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.90857863426,3.91557031782), test loss: 3.38714007139\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (3.27435159683,22.8505059808), test loss: 37.5378556728\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.09515285492,3.9010970888), test loss: 2.83383104056\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.80778932571,22.7591003083), test loss: 41.6117368221\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.20769023895,3.88675864965), test loss: 3.38266308308\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (6.86129045486,22.6686113647), test loss: 31.565613699\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.77846193314,3.87253236047), test loss: 2.80116022527\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (6.33265972137,22.5793223627), test loss: 40.8782275677\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.75234746933,3.8585154664), test loss: 3.30641012788\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (7.91606712341,22.4906714772), test loss: 32.4129558563\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.580518722534,3.84467368779), test loss: 2.99672859907\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (10.2858505249,22.4028832045), test loss: 41.4967814445\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.50138449669,3.8309664898), test loss: 3.45514672697\n",
      "run time for single CV loop: 7066.01472402\n",
      "\n",
      "MC # 3, Hype # hyp5, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold1/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (269.10043335,inf), test loss: 153.94179306\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (326.558807373,inf), test loss: 372.875187683\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (49.9546203613,63.330143199), test loss: 48.3592759132\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.28427314758,83.1240089486), test loss: 3.67877578139\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (51.0531997681,54.6489038401), test loss: 37.8432846546\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.80468022823,43.256353333), test loss: 3.05244016051\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (19.9484062195,51.6824482714), test loss: 46.7564163208\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.48916816711,29.9674458834), test loss: 3.67510303855\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.3782444,50.2640961766), test loss: 42.0597340584\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.93911886215,23.3284720902), test loss: 3.74200080037\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (29.927488327,49.2839454515), test loss: 48.9521872997\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.01855230331,19.3503608421), test loss: 3.52102853656\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (12.3563251495,48.5715685236), test loss: 46.0198588371\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.904064297676,16.7069197238), test loss: 3.79462803602\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (37.1605072021,47.9914336747), test loss: 45.1163099766\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.602080345154,14.8126604323), test loss: 3.17299170494\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (60.2240638733,47.5131174192), test loss: 46.1994973183\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.84306049347,13.3979292375), test loss: 3.92987943888\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (44.2799301147,47.0474217168), test loss: 42.2260012627\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.82788300514,12.2964878087), test loss: 3.036679703\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (43.3384552002,46.6651179542), test loss: 47.1043585777\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.9200258255,11.4137556767), test loss: 3.84957272112\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (26.816532135,46.3682531209), test loss: 39.2600764275\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (5.65356445312,10.694641368), test loss: 3.07446961999\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (18.4324111938,46.0624705395), test loss: 46.0259880066\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.90167450905,10.0959761336), test loss: 3.64527657628\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (114.251533508,45.7654357353), test loss: 36.1535571575\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (6.79311800003,9.59228050754), test loss: 2.95654509068\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (50.7140464783,45.4667498508), test loss: 44.1481544018\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.39922475815,9.15758078942), test loss: 3.52705642581\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (48.1190872192,45.1688869439), test loss: 38.9692978382\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.86615729332,8.78320872806), test loss: 3.64127128124\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (30.7573184967,44.8447301188), test loss: 43.513862896\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.40046453476,8.45478724553), test loss: 3.58670965731\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (28.2643089294,44.5403100204), test loss: 40.5099460602\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.42508602142,8.16358057072), test loss: 3.75941657424\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (33.9848670959,44.2639838017), test loss: 40.3910001755\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.02975082397,7.90635944068), test loss: 3.07489272952\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (58.9097938538,43.9656017457), test loss: 42.8117885113\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.73892307281,7.67542150207), test loss: 3.85571359694\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (18.0268802643,43.6509994872), test loss: 40.40202775\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.09564125538,7.46351734282), test loss: 3.00551435351\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (26.6043510437,43.3363114676), test loss: 40.8996752739\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.55441570282,7.26917178694), test loss: 3.73922168016\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (39.1933708191,43.0080637119), test loss: 36.6243884087\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.89880263805,7.09219638877), test loss: 3.08530705273\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (36.4590682983,42.6540893524), test loss: 42.3869698524\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.78168141842,6.92896585047), test loss: 3.61175327897\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (8.29493999481,42.3048938579), test loss: 32.8564087391\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.28099060059,6.77733761187), test loss: 2.97511116564\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (20.6452732086,41.9655270979), test loss: 41.0833264351\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.36669778824,6.63744529969), test loss: 3.74651504755\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (33.0961608887,41.6029635441), test loss: 28.2366570473\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.687570333481,6.50670071828), test loss: 2.84567633271\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (42.0009384155,41.23456577), test loss: 39.7391648293\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (6.35790300369,6.38496288078), test loss: 3.50789068937\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (11.6668434143,40.8616139259), test loss: 34.8375879765\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.78716063499,6.2692615193), test loss: 3.63674805164\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (30.0217704773,40.4768127765), test loss: 34.0170740128\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.47352409363,6.16024486233), test loss: 3.00929406285\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (21.679681778,40.0847393298), test loss: 37.2347962856\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.68662667274,6.05686447191), test loss: 3.6202848196\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (24.2328224182,39.7009452874), test loss: 35.0105401993\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (4.62002515793,5.95760276779), test loss: 2.72688774467\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (9.59854888916,39.3250961216), test loss: 37.360970211\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.30305862427,5.86369734012), test loss: 3.70137873888\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (26.1444416046,38.9397332331), test loss: 31.9118771553\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.85045921803,5.77391789757), test loss: 2.70149430633\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (47.3979568481,38.5608276084), test loss: 38.7052110672\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.02219486237,5.68837869854), test loss: 3.59718188643\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (22.214553833,38.1899723764), test loss: 31.7163451195\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.04751610756,5.60618014462), test loss: 2.97865823507\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (32.5781326294,37.8157295344), test loss: 38.7783620596\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.07669115067,5.52703304264), test loss: 3.26628302634\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (17.5333786011,37.4480471283), test loss: 28.1203569412\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (5.03195571899,5.45135790431), test loss: 2.74323357344\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (37.2539329529,37.0950531944), test loss: 37.758810997\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.0232591629,5.3778045467), test loss: 3.38178472519\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (25.2526988983,36.7542199496), test loss: 31.6370675802\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.89945673943,5.30788094945), test loss: 3.44400753081\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (14.6729316711,36.411154981), test loss: 38.4956164122\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.29383432865,5.24040299274), test loss: 3.46254027486\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (28.1963043213,36.0797773293), test loss: 33.2063976049\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.15430164337,5.1759385502), test loss: 3.55277479291\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (4.66345119476,35.761762531), test loss: 34.2578429937\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.877824187279,5.11390626048), test loss: 2.66363227069\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (98.6080551147,35.4434397977), test loss: 34.6583790779\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.26764392853,5.0540535857), test loss: 3.73310091496\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (17.6768569946,35.1366106273), test loss: 35.0604454994\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.2817401886,4.99642112353), test loss: 2.67103784978\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (18.4910774231,34.8402997504), test loss: 34.9285376787\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.4608745575,4.94047091501), test loss: 3.54207656085\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (46.4097175598,34.5554492098), test loss: 33.1931130409\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.17462921143,4.88692133798), test loss: 2.73760218024\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (28.5147647858,34.2714930245), test loss: 38.4831609249\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.95607960224,4.83511920919), test loss: 3.57486261129\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (7.20985794067,33.9971682398), test loss: 29.4410607576\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.9560046196,4.78539781481), test loss: 2.85642825663\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (33.9406051636,33.7351671035), test loss: 37.1694075584\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.07144331932,4.73756462406), test loss: 3.41372105479\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (3.82916259766,33.4698850868), test loss: 31.5174688816\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.750792264938,4.69098211503), test loss: 3.26974605918\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (8.02943897247,33.2189183684), test loss: 39.1201691151\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.93405199051,4.6459936422), test loss: 3.49937389493\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (12.3817577362,32.9743383009), test loss: 32.4690738201\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.69161605835,4.60203451824), test loss: 3.53902243078\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.7993812561,32.7369277128), test loss: 32.7803085804\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.43338608742,4.55986897391), test loss: 2.97265491784\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (20.0700683594,32.5006829497), test loss: 35.9097713232\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.70866990089,4.51881674168), test loss: 3.58185995519\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (13.8679084778,32.2718401822), test loss: 35.1171658993\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.18895673752,4.4789667306), test loss: 2.69665682018\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (49.1072540283,32.0495597037), test loss: 34.9229536057\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (4.01767730713,4.44033688886), test loss: 3.46345359385\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (44.7340126038,31.827700957), test loss: 33.092275095\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.45882177353,4.40239151108), test loss: 2.65676716566\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (3.48789644241,31.6146760775), test loss: 37.0828380585\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.50156712532,4.36550130763), test loss: 3.53951000273\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.3400306702,31.4066509272), test loss: 31.9691961765\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.28808999062,4.32922378245), test loss: 2.91189058125\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (18.5599403381,31.2024872309), test loss: 37.9296862364\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.44187712669,4.29434266697), test loss: 3.35218540132\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (17.9803848267,31.0004384509), test loss: 29.3671451092\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.16303956509,4.26017743925), test loss: 2.69744152874\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (7.24695682526,30.8044900632), test loss: 37.9596580982\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.10226726532,4.22719296156), test loss: 3.40048024356\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (20.0543136597,30.6121455592), test loss: 31.7847046137\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.66032385826,4.19513731328), test loss: 3.45491128266\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (20.3315525055,30.4222207866), test loss: 38.7347939968\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.00234532356,4.16372393695), test loss: 3.46451691091\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (26.7754821777,30.2395811999), test loss: 31.7540976763\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.72386002541,4.1330698541), test loss: 3.44904073477\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (25.0211200714,30.058645088), test loss: 36.025378418\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.56029891968,4.10289493021), test loss: 2.67968963683\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (21.8982486725,29.8805908236), test loss: 34.501180172\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.21554327011,4.07376292472), test loss: 3.55285714567\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (21.9035224915,29.7044502515), test loss: 35.8147583961\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.04163146019,4.04509476835), test loss: 2.58654214591\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (10.1399326324,29.5336587733), test loss: 35.3163971901\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.98346233368,4.01741005906), test loss: 3.40145607293\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (17.0066299438,29.3640714386), test loss: 33.6679420471\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.66659390926,3.99031161262), test loss: 2.70859067589\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (19.7727737427,29.197146914), test loss: 37.6712647438\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.2679207325,3.96373973872), test loss: 3.41056911051\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.0051708221,29.0359651269), test loss: 29.3443952084\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.16631639004,3.93773140218), test loss: 2.73230928481\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.5535202026,28.8762781416), test loss: 37.4245948315\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.55233931541,3.91209041114), test loss: 3.2458381176\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (23.4467144012,28.7183107813), test loss: 32.5144073486\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (4.89479255676,3.88718098542), test loss: 3.22200728357\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (9.25928688049,28.5615389199), test loss: 38.121313858\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.28023827076,3.86270061547), test loss: 3.36639089584\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.5920686722,28.4095065966), test loss: 30.5550395727\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.52535319328,3.8388938089), test loss: 3.23662373424\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (19.4353370667,28.2579434462), test loss: 33.5991541386\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.449725359678,3.81562556364), test loss: 2.92214152515\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.0500402451,28.1089942213), test loss: 34.2796137333\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.640650987625,3.79269742318), test loss: 3.43799959719\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (48.5004501343,27.9639447332), test loss: 34.7546858311\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.32034897804,3.77021832931), test loss: 2.47851703167\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (5.67095851898,27.8203226237), test loss: 35.6015417576\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.25672292709,3.74795335672), test loss: 3.27186469883\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (23.4437122345,27.6776381813), test loss: 34.1519565105\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.95815682411,3.72619508166), test loss: 2.59339636862\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (12.787817955,27.5366813056), test loss: 35.8453445196\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (3.17387223244,3.70501821791), test loss: 3.31404046714\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (15.4318437576,27.3989608794), test loss: 32.0039188862\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.31088471413,3.68416306599), test loss: 2.74593012333\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (28.8607063293,27.2615308179), test loss: 37.8485960484\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.1627215147,3.6638569508), test loss: 3.24414002895\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (32.299156189,27.1268279143), test loss: 28.9167304516\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.06219816208,3.64375826541), test loss: 2.56873753518\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (10.2422637939,26.9941711539), test loss: 37.0638399124\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.02129268646,3.62400031224), test loss: 3.28503333926\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (15.1310710907,26.8641331866), test loss: 31.3019990921\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.98674476147,3.60444324777), test loss: 3.1781568855\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.0734462738,26.7334707852), test loss: 38.1483982563\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.29958105087,3.58518096459), test loss: 3.34136269093\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (13.8929100037,26.6047245169), test loss: 30.934744215\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.16389214993,3.56643682633), test loss: 3.25859664679\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (10.4662351608,26.479095602), test loss: 36.7678339958\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.569933176041,3.54801827158), test loss: 2.70965134203\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (7.53670406342,26.3527420279), test loss: 34.1615610838\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.989506602287,3.53001073418), test loss: 3.3916991502\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (15.8189544678,26.2290798773), test loss: 35.6319607735\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.51892662048,3.51214516319), test loss: 2.49466180205\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (22.5783157349,26.1076875698), test loss: 35.3276776314\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.97271585464,3.49443571095), test loss: 3.28248491287\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (12.3090724945,25.9879640084), test loss: 33.2391191483\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.37809681892,3.47698844586), test loss: 2.69306994975\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (12.362156868,25.8673822195), test loss: 38.0691707373\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.02406001091,3.45981692629), test loss: 3.29978922158\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (30.4834842682,25.7486908953), test loss: 31.1812239647\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.05438065529,3.44302693048), test loss: 2.71037697196\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (20.2125778198,25.6326182053), test loss: 37.415597868\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.605871081352,3.42647974411), test loss: 3.23473278135\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (22.2756309509,25.5162352206), test loss: 32.2160488129\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.29644906521,3.41028413269), test loss: 3.07365566194\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (4.76897144318,25.401721284), test loss: 37.7870129347\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.674031317234,3.39416305019), test loss: 3.28668500483\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (4.70218658447,25.2888965718), test loss: 31.5590707779\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.698365092278,3.37815983196), test loss: 3.24414308369\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (4.13277864456,25.177573703), test loss: 35.1549460888\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.402075171471,3.3624631327), test loss: 2.85021430254\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (29.729227066,25.0658595779), test loss: 33.2531509399\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.08121061325,3.34692612123), test loss: 3.33409177363\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (11.4624452591,24.9551563328), test loss: 36.4195588112\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.26883363724,3.33174894456), test loss: 2.51804433167\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.8082914352,24.8473414941), test loss: 34.8771555901\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.962083995342,3.31677026482), test loss: 3.20106216073\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (7.70325565338,24.7387343887), test loss: 35.9132152081\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.387787818909,3.30203152847), test loss: 2.64279735982\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (19.7757434845,24.6324734519), test loss: 36.0954405785\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.07611298561,3.28741817524), test loss: 3.24722621441\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.21071624756,24.5271742115), test loss: 32.471524477\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.13393354416,3.27286960839), test loss: 2.7197108984\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (22.8973503113,24.4229064251), test loss: 37.2851643562\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (3.14694523811,3.25859954468), test loss: 3.13540514112\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (24.1416320801,24.3177058828), test loss: 29.7478811741\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.37981009483,3.2443649489), test loss: 2.55616272986\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (4.921043396,24.2145575698), test loss: 37.7395309448\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.88764762878,3.2305343721), test loss: 3.24866844416\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (4.76469326019,24.1135121874), test loss: 32.1358307838\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.68979358673,3.21690616365), test loss: 3.05286677182\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.55046081543,24.0115362036), test loss: 39.6691525936\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.24607920647,3.20347942973), test loss: 3.34040952325\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (20.4584293365,23.9116765185), test loss: 33.3669258118\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.09941053391,3.19013504185), test loss: 3.19899157584\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (4.49335813522,23.8128628585), test loss: 37.6618000984\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.869213461876,3.17679084282), test loss: 2.70708017945\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (4.91552257538,23.714262015), test loss: 34.9142943382\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.70164465904,3.16377160403), test loss: 3.29803333431\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (12.729142189,23.615501478), test loss: 35.6458345413\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.30208301544,3.1508048705), test loss: 2.42331126928\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (18.5172748566,23.5184843222), test loss: 36.1594610214\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.36801147461,3.13813474359), test loss: 3.26039716005\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (13.8347473145,23.4231407226), test loss: 32.9216536999\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.914297819138,3.12565243331), test loss: 2.68240999579\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (7.65155696869,23.326947914), test loss: 39.9297347069\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.82154679298,3.11335886063), test loss: 3.1766936779\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.30621337891,23.2324326568), test loss: 31.2550132751\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.7213845253,3.10112241349), test loss: 2.55585420579\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (18.4125518799,23.1390930275), test loss: 38.2668091536\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.03860807419,3.08890945275), test loss: 3.16142854989\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (18.4554252625,23.0458853256), test loss: 32.4919826508\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.36978816986,3.07694076154), test loss: 3.02427085936\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (3.73694181442,22.952387651), test loss: 38.3135511398\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.39442658424,3.0650198488), test loss: 3.22617240101\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (7.91588687897,22.8603391155), test loss: 32.0387271881\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.24807977676,3.0533650735), test loss: 3.13407512903\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (7.80414104462,22.7697363754), test loss: 38.2353230953\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.542441606522,3.04185699503), test loss: 3.05928019285\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (16.4012050629,22.6787960954), test loss: 33.3225991964\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.14720928669,3.03053079497), test loss: 3.26282687187\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.04177284241,22.5893489029), test loss: 38.0715966702\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.6320707798,3.01923606028), test loss: 2.58321676254\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (12.6008481979,22.5004125783), test loss: 35.4282198191\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.56405282021,3.00802985644), test loss: 3.13225834668\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (16.5311832428,22.4115537308), test loss: 37.9010881424\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.2279047966,2.99697157073), test loss: 2.68033853322\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (14.9788265228,22.323029121), test loss: 36.666362524\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.9894708395,2.98596503676), test loss: 3.10551165342\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (11.0068759918,22.2356753454), test loss: 33.5820058346\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.83010566235,2.97518942127), test loss: 2.72762668133\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (16.3895111084,22.1494352953), test loss: 42.5566636324\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.177544891834,2.96460850291), test loss: 3.33925609589\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (2.87282085419,22.0627657551), test loss: 29.5066971779\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.878221035004,2.95410972153), test loss: 2.49975760728\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.93299674988,21.9775263096), test loss: 38.6698626995\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.93821167946,2.94361609292), test loss: 3.17524029016\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (6.78949356079,21.8927807249), test loss: 31.9866498709\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.39708852768,2.9332576996), test loss: 2.99941455573\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.14434719086,21.8080958807), test loss: 39.7795912743\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.41193008423,2.92299430696), test loss: 3.06191580296\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (10.6201534271,21.7235555047), test loss: 36.2062732697\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.39593458176,2.91281400983), test loss: 3.25767718405\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (7.30542182922,21.6400730755), test loss: 39.0530150414\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.539972424507,2.90278718453), test loss: 2.69512346759\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (19.1031341553,21.5575077125), test loss: 37.190109396\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.97794628143,2.89296665198), test loss: 3.27617051601\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (6.40517997742,21.4751750619), test loss: 36.9513991356\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.09089398384,2.88321253567), test loss: 2.4884131372\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (4.19301891327,21.3935504815), test loss: 37.5478437424\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.80456137657,2.87346687727), test loss: 3.27858928442\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.82339572906,21.3124518897), test loss: 34.1491446972\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.34922552109,2.86381169551), test loss: 2.71479901373\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (11.0688333511,21.2311990358), test loss: 39.0349748611\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.29531764984,2.85423856518), test loss: 2.97475273609\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (11.1259670258,21.15047861), test loss: 35.0170578957\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.599445343018,2.8447591389), test loss: 2.69274830818\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.23951816559,21.0705710584), test loss: 39.2502402067\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.746579170227,2.83542849426), test loss: 3.11428322196\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.6313085556,20.9911234753), test loss: 32.5135143757\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.91247868538,2.8262600285), test loss: 2.94021394849\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (8.22184944153,20.9125740954), test loss: 40.0395474911\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.82267868519,2.81714296079), test loss: 3.22581568807\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (13.2147731781,20.8343848665), test loss: 31.8142143726\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.35829377174,2.8080149304), test loss: 3.14743266404\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (22.5027580261,20.7564492931), test loss: 39.6662210464\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.50153827667,2.79902703708), test loss: 2.9266661942\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (8.85498046875,20.6784054136), test loss: 33.9891483784\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.44414567947,2.79007806355), test loss: 3.24929190874\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (10.8626403809,20.6011353452), test loss: 42.3107269287\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.31217455864,2.78120492292), test loss: 2.73416594863\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (6.7020611763,20.524499851), test loss: 37.0141371012\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.05144119263,2.77247793691), test loss: 3.13060594797\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (9.60874462128,20.448211535), test loss: 38.9814400673\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.715596556664,2.76387696909), test loss: 2.72997413278\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (5.38909387589,20.3727078911), test loss: 39.1384413004\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.57874727249,2.75531392267), test loss: 3.15862722099\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (5.28528308868,20.2974681402), test loss: 34.9544929504\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.602606415749,2.74676052365), test loss: 2.83339097947\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.52118062973,20.2227127112), test loss: 40.0376124859\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.693613767624,2.73831810594), test loss: 3.06328359172\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (9.53156280518,20.1480418396), test loss: 31.3342079163\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (3.11355161667,2.72993003314), test loss: 2.71811130345\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.50424861908,20.0736798667), test loss: 41.861401701\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.688062787056,2.72160290113), test loss: 3.24252792895\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (6.11585617065,20.0001157496), test loss: 32.7902722836\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.35869431496,2.71336854922), test loss: 3.02861579657\n",
      "\n",
      "MC # 3, Hype # hyp5, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold2/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (291.586273193,inf), test loss: 186.951126099\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (335.177215576,inf), test loss: 377.763659668\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (38.3705711365,77.1039356718), test loss: 46.0667672157\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.8885474205,53.235239772), test loss: 3.33662343025\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (32.2245941162,62.2301527843), test loss: 37.2940555096\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.26907145977,28.1958225736), test loss: 3.51456897259\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (57.2329368591,57.3077884982), test loss: 45.4483419418\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.51136279106,19.8575130491), test loss: 3.46371151209\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (33.3982772827,54.7065947683), test loss: 42.3941954613\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.760620832443,15.6787417043), test loss: 3.50518950522\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (18.9399013519,53.1960386116), test loss: 39.8239064693\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.70137608051,13.1798724939), test loss: 3.03350096345\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (62.8405151367,52.0858677514), test loss: 46.5057202816\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.57850909233,11.5116085772), test loss: 3.57960498333\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (50.9133834839,51.2833572431), test loss: 42.8461297512\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.680901944637,10.3154165428), test loss: 3.01780065298\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (32.9795722961,50.6004264495), test loss: 46.1194783211\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.90851259232,9.41755383704), test loss: 3.57212385535\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (41.1718673706,50.0364310757), test loss: 37.8152657986\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.05067062378,8.71613038729), test loss: 3.2956384033\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (31.1754760742,49.6054825446), test loss: 48.0345869064\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.71565651894,8.15539687797), test loss: 3.4670094192\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (40.8506698608,49.1976947153), test loss: 37.6483003616\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (5.95505189896,7.69329309992), test loss: 2.95983360112\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (89.2109146118,48.8894483516), test loss: 44.9244345665\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.67851352692,7.30962446732), test loss: 3.58164137006\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (72.5020141602,48.5803204744), test loss: 33.2400951385\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.49879980087,6.98400908241), test loss: 2.75254062414\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (43.7062149048,48.2948065175), test loss: 43.7316223145\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (9.41742897034,6.70339639441), test loss: 3.34512525201\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (92.8329849243,48.0364955919), test loss: 40.1686180115\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.58109903336,6.45870149224), test loss: 3.40949605107\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (161.602554321,47.7630370393), test loss: 40.6067982435\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.02161407471,6.24308125022), test loss: 2.8764629662\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (43.2394256592,47.5022839128), test loss: 42.5582583427\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.08655929565,6.05291084847), test loss: 3.35051666498\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (45.1369132996,47.2613657043), test loss: 38.9595394135\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.84547376633,5.88123191742), test loss: 2.78817979693\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (29.5161247253,47.0454397229), test loss: 43.9047921181\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.35907173157,5.72778789017), test loss: 3.58882744908\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (52.199760437,46.8159503911), test loss: 34.572449398\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.93989419937,5.58941016006), test loss: 2.74217477441\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (52.0346374512,46.5878188264), test loss: 45.0070914268\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.94743108749,5.46504860985), test loss: 3.479679811\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (24.6167449951,46.3549139262), test loss: 35.2011511803\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.57164239883,5.34969663692), test loss: 3.05436028242\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (56.072845459,46.112266437), test loss: 42.6118699074\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (6.61126708984,5.24371590753), test loss: 3.01108625233\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (17.7636032104,45.8534424325), test loss: 30.7145160675\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.97774386406,5.14495925304), test loss: 2.7592640996\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (45.1177406311,45.5992408591), test loss: 39.5467896461\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.812280535698,5.0523737979), test loss: 3.14896613955\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (25.2362957001,45.3390882794), test loss: 35.2708085537\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.19258749485,4.96560650759), test loss: 3.21488763392\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (65.9117965698,45.0589461842), test loss: 39.0881263256\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.57021105289,4.88406464814), test loss: 3.20566191971\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (82.6558685303,44.7634374655), test loss: 36.8232583761\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.65154838562,4.80728833318), test loss: 3.23287057877\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (33.4740104675,44.4532309073), test loss: 33.8702144146\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.422570049763,4.73318724432), test loss: 2.71926880777\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (30.460319519,44.1300948828), test loss: 39.1986217737\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.66716122627,4.6634462852), test loss: 3.3875149399\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (25.5622997284,43.7921132853), test loss: 31.9848009348\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.61552596092,4.59670152837), test loss: 2.67259547114\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (29.0686759949,43.4553735713), test loss: 40.7463767529\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.4838745594,4.5331335632), test loss: 3.29013566375\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (12.1580610275,43.1187743322), test loss: 29.7033401728\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.71473836899,4.47187109634), test loss: 2.72607126832\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (21.6601409912,42.7763967222), test loss: 38.6356338024\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.08878087997,4.41399954567), test loss: 3.32333998978\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (15.3553791046,42.4232606984), test loss: 25.5286824703\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.760079622269,4.35893066242), test loss: 2.47285466045\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (22.2258796692,42.068311529), test loss: 34.5395846605\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.98561000824,4.30567393791), test loss: 3.12841615528\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (27.31798172,41.7035041106), test loss: 31.2511712074\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.441239535809,4.25471458725), test loss: 3.25384459198\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (30.7660083771,41.334557769), test loss: 34.5187648535\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.50362610817,4.20570491321), test loss: 3.16100520939\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (39.3732452393,40.9780055223), test loss: 32.3358742952\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.55711579323,4.15869979095), test loss: 3.29348777384\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (18.9770488739,40.6157161835), test loss: 29.8672233343\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.44077348709,4.11282640789), test loss: 2.64208624959\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (25.1455249786,40.2632783982), test loss: 34.2367110014\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.798210024834,4.06953149718), test loss: 3.44190404713\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (18.6658973694,39.9097106099), test loss: 30.0464570761\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.895581841469,4.0277419813), test loss: 2.56593041718\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (12.4773101807,39.5640187055), test loss: 35.9947630405\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.83891749382,3.98747556906), test loss: 3.26203913093\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (65.8897705078,39.2231026337), test loss: 28.4222581387\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.7840719223,3.94889134332), test loss: 2.77565333396\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (61.5332565308,38.8838186294), test loss: 35.5170809746\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.37970304489,3.91139427388), test loss: 3.26867725849\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (9.5321521759,38.5559611868), test loss: 25.0793214798\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.02273368835,3.87521166454), test loss: 2.6733769238\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (12.5785808563,38.2346228554), test loss: 33.749948287\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.4285428524,3.83999611946), test loss: 3.24961978793\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (23.30493927,37.923359891), test loss: 30.4771845341\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.32491827011,3.80618261854), test loss: 3.26448121071\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (18.2208480835,37.6135560148), test loss: 34.020160079\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.471330881119,3.77355375551), test loss: 3.30870911479\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (16.8733787537,37.3136978864), test loss: 31.8460880518\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.09920358658,3.74231159679), test loss: 3.31091028452\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (34.1933670044,37.0252169095), test loss: 29.791257906\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.61327648163,3.71202926503), test loss: 2.65728383064\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (29.1665477753,36.7364292746), test loss: 33.5265806675\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.66823554039,3.68231626786), test loss: 3.51305929124\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (7.31076955795,36.4558996785), test loss: 30.574580431\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.54530835152,3.65370128059), test loss: 2.61894289553\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (9.39774417877,36.1861974122), test loss: 33.5751081944\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.64903879166,3.62543337915), test loss: 3.23019444048\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (18.3671245575,35.9224194504), test loss: 30.0164579868\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.77198410034,3.59819403907), test loss: 2.70538492799\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (17.96352005,35.6608068104), test loss: 34.7855538845\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.74636173248,3.5718478993), test loss: 3.21008038819\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (9.49766349792,35.4095758836), test loss: 26.4057320595\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.05980229378,3.54656368376), test loss: 2.69874085784\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (19.0097579956,35.1661563303), test loss: 33.5876689911\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.44859004021,3.52183791139), test loss: 3.19538634121\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (14.514093399,34.9230063613), test loss: 30.6353019714\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.45228022337,3.49760739655), test loss: 3.19268931746\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (6.99108839035,34.6849765884), test loss: 33.6784741163\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.03653645515,3.47396916304), test loss: 3.24699305594\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (7.22705984116,34.4545810072), test loss: 31.9934995174\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.741703510284,3.45070055063), test loss: 3.18811738342\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.9602575302,34.2298801839), test loss: 28.8138902307\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.0723233223,3.42797082643), test loss: 2.5926397711\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (8.01988792419,34.0072152908), test loss: 33.0814729214\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.49622559547,3.4059684406), test loss: 3.35688262582\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (13.8136882782,33.7902301651), test loss: 30.3371827841\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.23803186417,3.38458420927), test loss: 2.55250088871\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (9.736369133,33.5801431105), test loss: 33.2176762342\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.255105644464,3.36358892379), test loss: 3.16075160801\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (32.9374809265,33.3709251834), test loss: 30.6189908028\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.13931894302,3.34318498139), test loss: 2.64504300058\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (27.9808444977,33.165798441), test loss: 33.7368912935\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.25379419327,3.32310602483), test loss: 3.08188795745\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (6.60575008392,32.9667160012), test loss: 26.8710555553\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.55026459694,3.30355495022), test loss: 2.68762292862\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (20.7809524536,32.771826381), test loss: 33.4155701637\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.77485084534,3.28409905155), test loss: 3.0598228991\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (22.6596336365,32.5789596158), test loss: 30.1190858841\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.51484847069,3.26535413409), test loss: 3.19312108159\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.5193605423,32.3888373722), test loss: 33.5088035345\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.76990425587,3.24704185881), test loss: 3.20130211115\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (19.4097118378,32.2044507087), test loss: 29.8877311707\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.41102838516,3.22915337403), test loss: 3.02069055438\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (23.2776012421,32.0210517352), test loss: 28.0260440588\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.04140281677,3.21160463713), test loss: 2.6267003417\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (32.8545684814,31.8410779854), test loss: 33.4557612062\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.47036314011,3.19442422336), test loss: 3.3384196043\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (24.5148735046,31.6667206211), test loss: 29.857277441\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.69656813145,3.17750023657), test loss: 2.44749232829\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (46.101776123,31.4937873105), test loss: 34.2826794386\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (4.34395885468,3.16076524597), test loss: 3.13671818972\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (42.550491333,31.3238195854), test loss: 29.3706990719\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.49081277847,3.14450117977), test loss: 2.60331662297\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (19.5800704956,31.1548331441), test loss: 35.0664645672\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.66603684425,3.12851485209), test loss: 3.15049729645\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.74223136902,30.9903340259), test loss: 27.6180272579\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.32876038551,3.11293592519), test loss: 2.62673560679\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (42.2662124634,30.8297083096), test loss: 34.552395916\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.43961715698,3.09775038917), test loss: 3.13673046231\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (5.42708206177,30.6673295175), test loss: 25.130701828\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.23877239227,3.08268407486), test loss: 2.6864138037\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (18.1948890686,30.5100381286), test loss: 33.7456391335\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (5.07099533081,3.06796634077), test loss: 3.13261019588\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (33.0044021606,30.3555280268), test loss: 29.6923613548\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.46499252319,3.05325090398), test loss: 2.93567794859\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (18.2791786194,30.2023699761), test loss: 28.8699159503\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.10777211189,3.038912108), test loss: 2.52683970034\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (8.83734226227,30.0494493396), test loss: 32.0531688929\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.31967997551,3.02481982533), test loss: 3.18947533667\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (21.2383480072,29.9009916677), test loss: 30.5143852711\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.22820544243,3.01115600421), test loss: 2.59859321713\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (11.8257856369,29.7559378319), test loss: 35.0841936111\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.8420612216,2.99766804357), test loss: 3.17809411883\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.5346670151,29.6097217142), test loss: 29.739950943\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.41024518013,2.98436607609), test loss: 2.61060273349\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (19.7958908081,29.4671456544), test loss: 36.2313301086\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.499653398991,2.9712665128), test loss: 3.2237883538\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (7.25608444214,29.3271456211), test loss: 29.3298107147\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.31946349144,2.95821584363), test loss: 2.62222624123\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (32.9128646851,29.1882964208), test loss: 33.8608090639\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.32225990295,2.9454695248), test loss: 3.02666217983\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (31.1161613464,29.0488689872), test loss: 25.2176783085\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.93252706528,2.93289864819), test loss: 2.48732866645\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (12.8579816818,28.9126059337), test loss: 33.562552762\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.83106827736,2.92070979855), test loss: 3.1290075466\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (19.3400096893,28.7801025895), test loss: 29.6048570395\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.27188777924,2.90864290371), test loss: 2.97858833075\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (18.3835067749,28.6462258093), test loss: 34.7807596684\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.868895888329,2.89675862764), test loss: 2.98035179973\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (16.1848373413,28.5151498528), test loss: 31.4287905455\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (3.01207637787,2.88504479802), test loss: 3.08974545896\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (27.3294334412,28.3866251159), test loss: 32.4185432911\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (5.21414804459,2.87341168811), test loss: 2.55696208477\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (21.0301742554,28.2593815198), test loss: 33.7652763367\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (5.47490167618,2.86195848058), test loss: 3.17393911481\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (30.6112670898,28.1319631631), test loss: 30.13781147\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.28405976295,2.85063156656), test loss: 2.41735391617\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (15.1686563492,28.0059335044), test loss: 36.47886343\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.20245790482,2.83955872396), test loss: 3.20424553156\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (11.5652246475,27.8828983479), test loss: 28.7203937054\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.05879211426,2.82860197159), test loss: 2.60577758253\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (8.2172088623,27.7593567671), test loss: 35.4417910576\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.56096863747,2.81791998558), test loss: 3.0363576442\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (11.8810119629,27.6381689188), test loss: 25.2411769867\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.27928233147,2.80727594969), test loss: 2.44575451463\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (40.489944458,27.5187766792), test loss: 33.966798234\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.55517053604,2.79682356169), test loss: 3.05071429014\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (16.0072746277,27.3997968224), test loss: 30.3506290436\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.780494093895,2.78633350687), test loss: 2.94184948206\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (23.7747993469,27.2817385055), test loss: 36.2666614532\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.67801880836,2.7761658966), test loss: 3.31324214339\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (10.623884201,27.1641868638), test loss: 32.3013321877\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.19428527355,2.76604166956), test loss: 3.09277501106\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (14.6046924591,27.0493780057), test loss: 33.2791819572\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.752003550529,2.75609248211), test loss: 2.63504559398\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (20.4420394897,26.9351400441), test loss: 32.874288559\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (3.13685488701,2.74632580102), test loss: 3.15136123896\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (5.69620275497,26.822386166), test loss: 33.1744273186\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.784407019615,2.7366504436), test loss: 2.52531545088\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (4.03606748581,26.7114481856), test loss: 36.4251016855\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.8864159584,2.7270545781), test loss: 3.14726713598\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (12.3895568848,26.6013280631), test loss: 30.8774828911\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.03093910217,2.71751805705), test loss: 2.61531732678\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (12.6632080078,26.491294575), test loss: 36.5446145535\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.17448854446,2.70812980507), test loss: 3.08816846907\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (15.790104866,26.3813657919), test loss: 27.162582159\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.89574921131,2.69884658566), test loss: 2.41325650066\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (13.5286417007,26.2733311488), test loss: 34.2213028789\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.60343551636,2.68970799118), test loss: 3.00356493294\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (7.28482532501,26.1672236392), test loss: 31.7755020618\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.127815365791,2.68075884822), test loss: 3.09257735908\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (15.0162258148,26.0605981396), test loss: 35.644153595\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.766667425632,2.67188497356), test loss: 3.08974228352\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (5.60069131851,25.9559146019), test loss: 32.4854344726\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.34782326221,2.66310562023), test loss: 3.08695233166\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (21.7250289917,25.8528711286), test loss: 33.5516684532\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.5905213356,2.6542887335), test loss: 2.62897269875\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (12.4353551865,25.7495654626), test loss: 34.018874383\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.00220775604,2.64560987497), test loss: 3.19306778908\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (9.04965019226,25.6464814988), test loss: 33.4021337986\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.28420448303,2.63702664698), test loss: 2.51223540604\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (4.06229734421,25.5453689665), test loss: 37.4080623627\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.60466265678,2.62862728845), test loss: 3.19552372992\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (5.64059495926,25.445927721), test loss: 32.6307948112\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.40644204617,2.62029472092), test loss: 2.68130787909\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (8.921667099,25.3454051994), test loss: 37.174819684\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.42635512352,2.61208300318), test loss: 3.00341836214\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (7.04325294495,25.246693733), test loss: 27.9096724987\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.842696905136,2.60389914488), test loss: 2.5243901521\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (4.93692874908,25.1489441887), test loss: 36.0341333866\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.50984311104,2.59573137318), test loss: 2.98278183043\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (4.75029754639,25.0516225281), test loss: 31.2662750244\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.638904035091,2.58768933811), test loss: 3.00059721768\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (26.4584255219,24.9543434277), test loss: 37.4290637255\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.49457192421,2.57974586427), test loss: 3.23510617018\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (25.3751220703,24.8578887255), test loss: 32.1233085871\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.45461380482,2.57192349814), test loss: 3.11686190367\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.87866783142,24.7633038519), test loss: 33.5606307745\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.834345817566,2.56413061479), test loss: 2.68878087401\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (5.82410621643,24.6684173946), test loss: 34.7453442097\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.68533790112,2.55649525941), test loss: 3.23282592595\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (6.7270898819,24.5752728096), test loss: 33.7805879593\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.883497714996,2.54887792468), test loss: 2.42699651867\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (9.94918251038,24.4829555062), test loss: 36.1444403172\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (4.51140880585,2.54132699163), test loss: 3.07331037521\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (4.91782188416,24.3909347809), test loss: 33.9434841156\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.890065312386,2.53376379457), test loss: 2.62225108147\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (7.7533493042,24.2986127683), test loss: 37.3709639072\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.95808684826,2.52629968983), test loss: 2.93517668843\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (17.3583297729,24.206846805), test loss: 30.4793216228\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.5603852272,2.51894073681), test loss: 2.59510501772\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.86292409897,24.116368196), test loss: 36.1964118481\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.752542257309,2.51160724306), test loss: 2.94640700221\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (12.2293186188,24.0262367284), test loss: 32.2666482449\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.392711043358,2.50447120837), test loss: 3.02273542285\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (6.23212051392,23.9371908997), test loss: 36.3357745051\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.28198027611,2.49733565195), test loss: 3.09273020253\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.34205675125,23.8487726818), test loss: 31.7115260124\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.461209386587,2.49024080686), test loss: 2.84490152895\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (7.81364154816,23.7607615557), test loss: 32.6489043951\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.76855254173,2.48315756217), test loss: 2.6390946418\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (9.49559497833,23.6729947971), test loss: 35.5935618162\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.98427182436,2.47618583886), test loss: 3.15963068008\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (6.99746417999,23.5854997543), test loss: 35.7318549633\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.600763201714,2.46923708462), test loss: 2.45020565689\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.39255714417,23.4993391591), test loss: 37.2077137113\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.720079958439,2.46233213944), test loss: 3.03964594603\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (14.2604646683,23.4137718789), test loss: 34.496490097\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.811158180237,2.45558172176), test loss: 2.69559429884\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (6.62430143356,23.3285566452), test loss: 38.8971277475\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.75973415375,2.44885971833), test loss: 3.13435003012\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (8.40533733368,23.2439001443), test loss: 30.3911940098\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.81223785877,2.44214282062), test loss: 2.68587577045\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (2.41587352753,23.159616687), test loss: 36.3398054123\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.02973806858,2.4354425761), test loss: 2.82057418823\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (14.8903846741,23.0755404009), test loss: 30.933362627\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.488336861134,2.42883644212), test loss: 2.9994363904\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (17.233789444,22.9915546856), test loss: 37.6919240952\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.06117260456,2.42227866638), test loss: 3.13265731335\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (13.4680862427,22.9084698187), test loss: 31.8243671894\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.85023403168,2.41580447035), test loss: 2.94193398952\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (11.1058607101,22.8265906679), test loss: 34.1497088432\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.37639725208,2.40937517098), test loss: 2.767789644\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.39482498169,22.7443312465), test loss: 34.5662415981\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.273711413145,2.40299829328), test loss: 3.13650349975\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (9.66786956787,22.6633168414), test loss: 35.2531238079\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.18752670288,2.39668822838), test loss: 2.67308503836\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.14745640755,22.5831274932), test loss: 39.2328236103\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.482174545527,2.39029924522), test loss: 3.13098611236\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (10.2800045013,22.5025682654), test loss: 35.0862230301\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.535948038101,2.38400966615), test loss: 2.83170236349\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.02148151398,22.4219053136), test loss: 40.7886721134\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.53241288662,2.37776461692), test loss: 3.11673723757\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.58150911331,22.3419166118), test loss: 32.5658573151\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.97432696819,2.37161001277), test loss: 2.82119257152\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (9.3232011795,22.2631567167), test loss: 37.0038191438\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.533440768719,2.36546270035), test loss: 2.8851067394\n",
      "\n",
      "MC # 3, Hype # hyp5, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold3/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (306.968475342,inf), test loss: 171.212651062\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (290.084991455,inf), test loss: 349.768161011\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (33.1508407593,70.769221909), test loss: 46.8635285378\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.1351852417,61.8684846462), test loss: 3.34502800405\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (29.7114028931,57.4581471059), test loss: 36.1517464638\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.51047515869,32.5927380082), test loss: 2.98605617285\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (46.9723892212,52.8305740879), test loss: 44.1543246269\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.94369339943,22.8059250443), test loss: 3.33111447692\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (20.2980308533,50.3860798458), test loss: 38.516587472\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.47289228439,17.8979406912), test loss: 3.2846411258\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (40.9075241089,48.7155913089), test loss: 43.4675443888\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.88908910751,14.9521738925), test loss: 3.33107432127\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (23.6175918579,47.500449287), test loss: 42.1016505241\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.86879134178,12.9840994447), test loss: 3.30758471787\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (23.1746330261,46.5105299503), test loss: 39.2924595356\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.96067142487,11.5694908944), test loss: 2.89765094817\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (27.7944393158,45.5766962399), test loss: 43.7908220291\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.976805686951,10.5051503166), test loss: 3.58121410012\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (22.1019020081,44.7733123736), test loss: 38.1794809103\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.37456488609,9.67603531656), test loss: 2.83564242125\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (26.2540988922,44.0101158594), test loss: 41.1904509544\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.52847290039,9.00447673305), test loss: 3.6114535749\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (27.8916378021,43.3283694908), test loss: 32.9267028809\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.60798311234,8.45608134269), test loss: 2.91914302409\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (17.2276916504,42.6408418663), test loss: 40.9115505457\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.87894034386,7.99503439547), test loss: 3.3031899631\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (30.0795650482,41.9698731097), test loss: 32.7652694702\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.91105270386,7.60067162683), test loss: 3.2934594363\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (38.1566047668,41.2997774939), test loss: 38.2408885002\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.54129076004,7.26141060854), test loss: 3.13024483919\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (29.1904563904,40.6296241786), test loss: 27.5350755692\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.78577423096,6.96535977314), test loss: 2.72184497416\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (10.1646652222,39.9948603282), test loss: 35.5175897121\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (6.09661865234,6.70201103109), test loss: 3.13362393081\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (22.3046607971,39.3805423973), test loss: 30.7795659542\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.28322410583,6.46656353307), test loss: 3.15924072266\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (10.7706565857,38.7593593698), test loss: 34.13976264\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.40941774845,6.25728972996), test loss: 3.14363088608\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (11.0187263489,38.1621387741), test loss: 32.8911533356\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (5.12329864502,6.06912097291), test loss: 3.14035282135\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (45.5145149231,37.5806486124), test loss: 28.9488395214\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.6118516922,5.89805759301), test loss: 2.68842374384\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (68.8630371094,36.991951882), test loss: 34.3573956013\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (5.90886497498,5.74190978138), test loss: 3.35038410425\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (25.081413269,36.4415356211), test loss: 29.6220366001\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.03475141525,5.59880582954), test loss: 2.69969963431\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (26.6877784729,35.9041767282), test loss: 33.9640077114\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.98684692383,5.46556125584), test loss: 3.28285763264\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (27.5499305725,35.3937440949), test loss: 27.2011681795\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (6.30397701263,5.34423091161), test loss: 2.78247606158\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (13.6027231216,34.8995154409), test loss: 34.3198450089\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.53283452988,5.23184582028), test loss: 3.17241919041\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (10.9511919022,34.4307376131), test loss: 27.7106870174\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.637325167656,5.12727896539), test loss: 2.98756094277\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (11.7934465408,33.9747211091), test loss: 32.8286438942\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.87107753754,5.03010504816), test loss: 3.10565908551\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (24.7331600189,33.5382031021), test loss: 24.2691351175\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.96776986122,4.93942399441), test loss: 2.61821860671\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (13.0100059509,33.1300110033), test loss: 31.9200737953\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.96676564217,4.85294661756), test loss: 3.05122705698\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (16.2190856934,32.7369907133), test loss: 29.6059543848\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.770863294601,4.77176157912), test loss: 2.92971879989\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (13.0596160889,32.3533770863), test loss: 31.701269865\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.06725502014,4.69589004699), test loss: 3.03285199404\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (10.5658588409,31.9916704654), test loss: 30.7008603096\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.51304388046,4.62480413483), test loss: 3.06603840217\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (21.7649173737,31.6485334057), test loss: 27.9354460001\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.51275920868,4.55768474836), test loss: 2.79281007946\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (17.583568573,31.3064620733), test loss: 32.3763462543\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.74460327625,4.49350833952), test loss: 3.21863797605\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (19.9268245697,30.9890813477), test loss: 27.8370798826\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.73197317123,4.4328011755), test loss: 2.53950371444\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (15.6762151718,30.67849576), test loss: 34.0786587715\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.97037553787,4.37416095059), test loss: 3.02714310884\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (27.261592865,30.3815463504), test loss: 27.2324651718\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.73171424866,4.3189209714), test loss: 2.66235301346\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (15.7781524658,30.0946561276), test loss: 33.9576053858\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.602053165436,4.26659200306), test loss: 3.12806477547\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (26.307384491,29.8224646829), test loss: 27.7461133003\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.62811887264,4.21662434587), test loss: 2.89851422906\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (26.8894042969,29.5539749623), test loss: 31.7510072708\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.26711559296,4.16869820238), test loss: 3.00680322945\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (19.4450149536,29.2932813921), test loss: 24.8817689896\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.29860877991,4.12296272057), test loss: 2.51531323344\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (18.6208381653,29.0475864917), test loss: 30.6058373928\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.82939386368,4.07818135539), test loss: 2.92583432794\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (18.7085227966,28.8059838836), test loss: 29.9159369946\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.47711396217,4.03539516882), test loss: 2.87078230679\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (13.0560426712,28.5677942933), test loss: 31.3240950584\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.02122962475,3.99446268843), test loss: 2.96297970414\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (12.6614265442,28.3406581483), test loss: 29.8985253096\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.24470257759,3.95545783338), test loss: 2.98994522393\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (54.0646286011,28.1218217643), test loss: 27.5475788116\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.88270664215,3.9179121673), test loss: 2.68270489573\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.7557964325,27.9014630337), test loss: 32.1530518055\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.770634293556,3.88146824795), test loss: 3.12494085729\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (20.604719162,27.6934971334), test loss: 28.4908593178\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.18083035946,3.84622075998), test loss: 2.48226114511\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (8.45468044281,27.4890064333), test loss: 33.7999246597\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.57248663902,3.81165267785), test loss: 3.0055406183\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (27.8209686279,27.2894703286), test loss: 27.2969213009\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.80175757408,3.77856248241), test loss: 2.50570014268\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (37.2750167847,27.0944464408), test loss: 33.5451426029\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.50807905197,3.74690829477), test loss: 3.0533391118\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (22.338054657,26.9065416883), test loss: 27.9771759033\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.746427893639,3.71610442556), test loss: 2.79211856872\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (5.75009536743,26.7187638617), test loss: 31.9512074947\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.638879477978,3.68619869852), test loss: 2.90582164526\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (8.06867599487,26.5364978485), test loss: 25.5413795471\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.708505690098,3.65742197306), test loss: 2.51284691095\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (16.1051597595,26.3624846654), test loss: 30.1608457565\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.81946611404,3.62875916385), test loss: 2.82387668192\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (20.7964115143,26.188369401), test loss: 29.0120499134\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.81645190716,3.60110107393), test loss: 2.77141156793\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.5023498535,26.0148713137), test loss: 30.4034780502\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.44731068611,3.57432778183), test loss: 2.87790517807\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (5.04052686691,25.8478093389), test loss: 29.8011813164\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.79723149538,3.54851308919), test loss: 2.79257740378\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (8.78609085083,25.6846320391), test loss: 27.5003214836\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.941806674,3.52345779091), test loss: 2.73400016427\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (15.2071914673,25.5228495725), test loss: 31.8313180923\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.8175675869,3.49889914349), test loss: 3.02295137048\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (26.2698440552,25.3672470633), test loss: 29.1073018551\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.88825511932,3.47492304893), test loss: 2.5935607627\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (9.79480552673,25.2125628632), test loss: 33.2412874699\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.00813281536,3.45115302091), test loss: 2.9507948041\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.1219301224,25.0589128813), test loss: 29.0304838181\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.11104774475,3.42810242462), test loss: 2.50125048459\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (14.7479372025,24.9081927687), test loss: 32.8084960938\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.10587596893,3.40592366681), test loss: 2.8357904613\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (15.5171833038,24.7629474763), test loss: 27.6895244122\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.77791762352,3.38428878216), test loss: 2.68023459017\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (12.2810726166,24.6173516997), test loss: 32.2930489063\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.675236105919,3.36308969815), test loss: 2.79502983838\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (10.8561925888,24.4753595601), test loss: 26.4114019871\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.66766786575,3.34248854334), test loss: 2.55340847671\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (34.5125656128,24.3373465232), test loss: 30.4225393295\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.66815233231,3.32173930289), test loss: 2.71755549014\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (13.9047803879,24.1981260563), test loss: 29.0911112785\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.95511293411,3.30169329993), test loss: 2.7008274585\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (9.86975479126,24.0597913914), test loss: 30.67649436\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.61285340786,3.28206690168), test loss: 2.83089685738\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (11.8881969452,23.9266389623), test loss: 29.4072561741\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.94973325729,3.26308280213), test loss: 2.73351646811\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (22.0808448792,23.7959286404), test loss: 28.4545327187\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.10261440277,3.24452695443), test loss: 2.6873069495\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.5899124146,23.6651110912), test loss: 30.4762018919\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.615425527096,3.22626194886), test loss: 2.86990828514\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (11.4739589691,23.5372621893), test loss: 29.8390693188\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.84845733643,3.20821181256), test loss: 2.55823016167\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (10.8034200668,23.411002822), test loss: 31.0456993818\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (5.47366428375,3.19028586), test loss: 2.93861221969\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (38.4413986206,23.285120037), test loss: 29.8479658127\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.86283683777,3.17280665095), test loss: 2.47596711814\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (7.64463233948,23.1611758514), test loss: 32.2536916256\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.87030053139,3.15597071115), test loss: 2.78181999624\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (7.7325425148,23.0409725999), test loss: 28.8815659523\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.03823757172,3.13938012359), test loss: 2.61493538022\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.3289899826,22.920190948), test loss: 32.5628682137\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.63934063911,3.12309346063), test loss: 2.73779817522\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (6.51559972763,22.8014973324), test loss: 26.203546381\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.64826095104,3.10707655399), test loss: 2.53774250299\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (5.92837810516,22.6851295267), test loss: 31.3935976505\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.82251262665,3.09100493366), test loss: 2.55235012174\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (14.126490593,22.5692304759), test loss: 28.3036558628\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.24025309086,3.07533792224), test loss: 2.62233521342\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.1313085556,22.4532849976), test loss: 31.8037660837\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.66614592075,3.06002585418), test loss: 2.79003511965\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (13.9748935699,22.3404492619), test loss: 28.2762758017\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (3.50155448914,3.04504729438), test loss: 2.69207839072\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (17.4477806091,22.2285551503), test loss: 28.9620475769\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.69051706791,3.03033749309), test loss: 2.63472541124\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (25.464969635,22.1177435108), test loss: 29.6293627739\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.27657270432,3.01586514517), test loss: 2.71808495224\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (6.26412010193,22.0087946109), test loss: 31.4053462982\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (3.69231653214,3.00147697454), test loss: 2.53715004027\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (7.81976556778,21.9010880098), test loss: 31.1563169956\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.63581848145,2.98714857665), test loss: 2.89869506955\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (18.0435180664,21.792530346), test loss: 31.1288915634\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.11977243423,2.97315836758), test loss: 2.55404265523\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (4.37132024765,21.6860175071), test loss: 32.0222872257\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.94097948074,2.95954065313), test loss: 2.78556092978\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (4.64003038406,21.5817949088), test loss: 29.9270475864\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.77449464798,2.94615334479), test loss: 2.52507939786\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (10.3889074326,21.4771627025), test loss: 33.4785453081\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.1839594841,2.93295198845), test loss: 2.71947248876\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (11.9786424637,21.3746652496), test loss: 27.608451438\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (5.01002597809,2.91993238099), test loss: 2.58919693232\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (34.1104812622,21.2740152901), test loss: 31.9397025108\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.76638650894,2.90688612144), test loss: 2.59443499148\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (27.8764133453,21.1721427403), test loss: 25.8482221365\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.11001610756,2.89409327158), test loss: 2.39073287845\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (17.7344112396,21.070663552), test loss: 30.5715896606\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.64166021347,2.88150386862), test loss: 2.7336874783\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (4.77173566818,20.9714698478), test loss: 28.0653713703\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.29939448833,2.86911562162), test loss: 2.64704992771\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (14.7419109344,20.8735882962), test loss: 32.0271595478\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.415816366673,2.8570750953), test loss: 2.80450628102\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (9.67110061646,20.7763691447), test loss: 29.0682013988\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.856642365456,2.84512956795), test loss: 2.6775233686\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (4.5567407608,20.6806567537), test loss: 32.9821097851\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.859048247337,2.8331260702), test loss: 2.50537368655\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (6.1801366806,20.5852688491), test loss: 30.0733294964\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.516483187675,2.82128558741), test loss: 2.83688346744\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (18.4438285828,20.4886723385), test loss: 32.2125506878\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.36023211479,2.80963675569), test loss: 2.53669388294\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (14.0705194473,20.3941698911), test loss: 31.6292345524\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.99601697922,2.79827202383), test loss: 2.76403033137\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (9.84191036224,20.3017611019), test loss: 31.4262374401\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.923566520214,2.78711050586), test loss: 2.53866835237\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (10.9829053879,20.2092674842), test loss: 33.3282128811\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.777094841003,2.77608207354), test loss: 2.68008174598\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (5.71310567856,20.1175738564), test loss: 28.2176813126\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.94860005379,2.76511622612), test loss: 2.50489774942\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (7.80429410934,20.0265779863), test loss: 31.2030334711\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.36635780334,2.75416317987), test loss: 2.60142594278\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.15678024292,19.9355965737), test loss: 27.9999297857\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.01409578323,2.74338163303), test loss: 2.50446466208\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (7.95258569717,19.8454977073), test loss: 32.9417842388\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.0404804945,2.7327745641), test loss: 2.79117182642\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.4506454468,19.7568396019), test loss: 27.991327095\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.71929550171,2.72233633973), test loss: 2.65402676463\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (8.05037117004,19.6688932303), test loss: 31.9415205002\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.4126367569,2.71213273581), test loss: 2.76793959141\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (12.0303344727,19.581343036), test loss: 29.2304115295\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.26776492596,2.70193874771), test loss: 2.71297735572\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (3.11820936203,19.4946868477), test loss: 32.9063301086\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.934515237808,2.69173325018), test loss: 2.53237992823\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (15.695971489,19.4081164097), test loss: 30.4675815582\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.17741560936,2.68166617133), test loss: 2.84130219221\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (6.99259614944,19.3210834246), test loss: 32.3678852558\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.707496166229,2.67170696528), test loss: 2.41045868099\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (9.54842662811,19.2361046839), test loss: 32.7652092934\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.516712844372,2.66203265171), test loss: 2.73258511424\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (9.07810020447,19.1522601562), test loss: 32.3386388779\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.7126942873,2.65252398882), test loss: 2.59229976535\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (5.03761863708,19.067849265), test loss: 33.7627692223\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.07647228241,2.6430558315), test loss: 2.78254264891\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (4.84394836426,18.9845717468), test loss: 29.1124980927\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.06471943855,2.63357975993), test loss: 2.48993077278\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (8.93573188782,18.9023263585), test loss: 31.9955412865\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.67160964012,2.62420313939), test loss: 2.55551744998\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (9.18897247314,18.8199983712), test loss: 28.116718483\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.14733719826,2.61496608167), test loss: 2.4864664942\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (15.1623926163,18.7380591344), test loss: 32.0375218868\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.41338157654,2.60579282039), test loss: 2.73196834028\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (4.53665781021,18.6568024863), test loss: 29.2674456596\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.412287712097,2.59671960672), test loss: 2.67414010763\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (20.231552124,18.5764459237), test loss: 32.718555212\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.16998815536,2.58789224531), test loss: 2.79886924624\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (4.10700798035,18.4960658459), test loss: 29.3924646854\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.685531556606,2.57908135093), test loss: 2.65387403667\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (5.3545088768,18.4169371242), test loss: 32.2993049622\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (3.11543560028,2.57025639078), test loss: 2.57202145457\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.38461208344,18.3380076796), test loss: 31.2474643469\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.775215744972,2.56155376943), test loss: 2.79601445496\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (3.77031254768,18.2587087894), test loss: 33.3426930904\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.16373467445,2.55295365014), test loss: 2.4591845125\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (7.48824691772,18.1805440646), test loss: 33.432990694\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.08641338348,2.54451716747), test loss: 2.76179004908\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (4.17910575867,18.1036736437), test loss: 33.3929443836\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.231012821198,2.53620117057), test loss: 2.61327145398\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (10.1710739136,18.0268081246), test loss: 34.6781910419\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.62804293633,2.52796958656), test loss: 2.81484226584\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (5.54053449631,17.950912868), test loss: 29.8340457439\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.11227703094,2.51968733724), test loss: 2.49381229877\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (15.431306839,17.8755193511), test loss: 33.3110049248\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.75637602806,2.51149425854), test loss: 2.63567750752\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.86606693268,17.799669111), test loss: 27.4685237885\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.10284733772,2.50336442754), test loss: 2.39888368845\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (9.44666194916,17.7242155917), test loss: 31.561143589\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.27846813202,2.49532770243), test loss: 2.69801083207\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (1.40607631207,17.6494454974), test loss: 29.6840363979\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.371730357409,2.48740579221), test loss: 2.41730210483\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (5.940762043,17.5755035109), test loss: 32.5020324945\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.283618927,2.47966929107), test loss: 2.80176187754\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (18.3120689392,17.5023306621), test loss: 29.0452979565\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.895180881023,2.47197068765), test loss: 2.6365632087\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (2.22502398491,17.429737111), test loss: 33.3029314518\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.46326851845,2.46422569049), test loss: 2.77258631289\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (9.30556678772,17.3569355874), test loss: 31.3604878426\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.956835150719,2.4566005359), test loss: 2.72696115375\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (4.50220298767,17.284459864), test loss: 34.1621778488\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.545390963554,2.44899587797), test loss: 2.44083779454\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (6.84021711349,17.2130497495), test loss: 33.9034832478\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.540099024773,2.44154227158), test loss: 2.77453200519\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.25043106079,17.1428018465), test loss: 34.1709758759\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.61607897282,2.43427258049), test loss: 2.69509873539\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (2.01594877243,17.0722341848), test loss: 35.4653501511\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.12522625923,2.42695075204), test loss: 2.85756499767\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.64020586014,17.0021683211), test loss: 29.9585042\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.1437381506,2.41959436169), test loss: 2.52121765018\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (6.11134052277,16.9324778255), test loss: 35.5679440975\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.44059574604,2.41233821434), test loss: 2.65190443695\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (12.3694505692,16.862669381), test loss: 28.2139056683\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.91176879406,2.40514999441), test loss: 2.35777793229\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (4.15776777267,16.7933786197), test loss: 32.4672325611\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.476345062256,2.39808413507), test loss: 2.67829373479\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.27799773216,16.7251735397), test loss: 30.2900185585\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.0050855875,2.39111815068), test loss: 2.46619285643\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (9.63805866241,16.6574728097), test loss: 32.9660345554\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.573320329189,2.38428411958), test loss: 2.84253773689\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.71907997131,16.5899477118), test loss: 31.1693407536\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.8069473505,2.3774615019), test loss: 2.66936376989\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (3.50433182716,16.5235080731), test loss: 33.3076524734\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.859579384327,2.37055104957), test loss: 2.74335414171\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (7.96648979187,16.4570223458), test loss: 32.670256424\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.715320825577,2.36380480499), test loss: 2.74513139129\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (5.64139080048,16.3910360555), test loss: 36.0741475105\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.35535812378,2.35706683017), test loss: 2.50049458146\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (11.246295929,16.3254085589), test loss: 34.7815345526\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.76890146732,2.35040885436), test loss: 2.87475084662\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (10.3809528351,16.2605180833), test loss: 32.388242054\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.63651996851,2.34387781023), test loss: 2.5242143631\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.50666999817,16.1955879819), test loss: 35.3026002645\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.12712180614,2.33734448387), test loss: 2.88114120364\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (3.14359617233,16.1309826106), test loss: 30.7702286243\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.466516822577,2.33080305462), test loss: 2.56195284724\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (4.64432430267,16.0669984161), test loss: 36.5683337212\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.32681775093,2.32436072969), test loss: 2.72566373348\n",
      "\n",
      "MC # 3, Hype # hyp5, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold4/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (424.076416016,inf), test loss: 238.386824036\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (331.441467285,inf), test loss: 398.943850708\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (64.1702270508,99.3206074333), test loss: 44.6413099289\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.15526103973,73.1885914544), test loss: 3.40317259282\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (102.512237549,72.1836701021), test loss: 38.3576776028\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.59185743332,38.2432550055), test loss: 3.47993923426\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (29.3217639923,63.0723831654), test loss: 42.8338421822\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (5.22893238068,26.5858066014), test loss: 3.43822114468\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (70.992477417,58.2906486905), test loss: 39.5590812683\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.98195576668,20.7447182829), test loss: 3.66076357663\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (27.8089466095,55.4178348076), test loss: 39.6975170135\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.63065397739,17.2416771756), test loss: 2.81080508232\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (41.6693458557,53.3706213864), test loss: 44.6084537029\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.11830210686,14.9100782254), test loss: 3.71364824176\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (60.3503723145,51.838412402), test loss: 40.9768497467\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.25800561905,13.2469133171), test loss: 2.7972037375\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (29.5550632477,50.5995701362), test loss: 39.9581534386\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.00605630875,11.9930490771), test loss: 3.28054650128\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (43.2085571289,49.5620336768), test loss: 34.336288929\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.639300525188,11.0198616224), test loss: 2.91877374351\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (38.8305664062,48.6372154425), test loss: 42.2106852055\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.58874607086,10.2394755825), test loss: 3.17459304333\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (16.6139278412,47.8683963194), test loss: 31.1963194847\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.24042892456,9.59948117476), test loss: 2.72563201189\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (17.7270088196,47.1169307269), test loss: 39.8037993431\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (5.1757774353,9.06382199563), test loss: 3.46550546885\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (14.185760498,46.4750622308), test loss: 34.3014179707\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.96264487505,8.60968483708), test loss: 3.45787558556\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (43.7691192627,45.8284067655), test loss: 39.0859319687\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.63239097595,8.22027738579), test loss: 3.34769801199\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (14.294046402,45.210711502), test loss: 36.7443774223\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.3412797451,7.88237228462), test loss: 3.33967306018\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (33.4832954407,44.6070761206), test loss: 35.7873804092\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.79989910126,7.58289932236), test loss: 2.85175253153\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (38.2585601807,43.9992449473), test loss: 38.5143958569\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.62280511856,7.31766715387), test loss: 3.40142108798\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (16.3352222443,43.3886424291), test loss: 30.6424956322\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.650357604027,7.07930709056), test loss: 2.69029143304\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (59.8566894531,42.8090825517), test loss: 38.0635053158\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.08348083496,6.86433285435), test loss: 3.23139475882\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (37.2124328613,42.2078790728), test loss: 28.5632180691\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (6.21115350723,6.66762007031), test loss: 2.65213953555\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (25.2011642456,41.6333590128), test loss: 36.6862387657\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.15659332275,6.48892468178), test loss: 3.37413065434\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (12.662779808,41.0400227445), test loss: 24.8451124191\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.32314550877,6.32453949727), test loss: 2.79903543591\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (18.7813987732,40.4515467151), test loss: 35.0741138458\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (4.92668390274,6.17308911598), test loss: 3.30367688835\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (43.3712387085,39.8672877943), test loss: 31.6681058407\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.78879737854,6.03115081878), test loss: 3.53982616663\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (33.3650512695,39.2854729429), test loss: 28.8308740139\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.36017823219,5.89930453648), test loss: 2.58911499828\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (25.6006374359,38.7149295544), test loss: 35.1344631672\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.83148312569,5.77580251205), test loss: 3.33898311853\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (22.8072090149,38.1721593688), test loss: 31.1524553776\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.87926077843,5.65955535723), test loss: 2.43800302148\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (19.1506996155,37.6385915105), test loss: 34.3706022978\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.17501282692,5.54924041385), test loss: 3.3948081255\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (12.041893959,37.1326852063), test loss: 29.3697040558\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.49114608765,5.44626888494), test loss: 2.54743900597\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (14.0869054794,36.6350795903), test loss: 34.9738384247\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.416655600071,5.34861821668), test loss: 3.16843115389\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (47.2571029663,36.1618257303), test loss: 26.3398189545\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.99167919159,5.25720607154), test loss: 2.61499368548\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (17.973285675,35.7060603264), test loss: 34.4375607967\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.12487101555,5.17058662656), test loss: 3.23137517571\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (12.2420711517,35.262822401), test loss: 29.9820827961\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.34442448616,5.088269058), test loss: 3.28786440492\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (10.5386915207,34.8362979691), test loss: 33.7680477619\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.2148501873,5.01017245391), test loss: 3.23483754396\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (41.3637962341,34.4358553933), test loss: 30.69484694\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.8917183876,4.93593690739), test loss: 3.27522509396\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (20.9204235077,34.0476256588), test loss: 31.0950133801\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.1049053669,4.86463201525), test loss: 2.38359240592\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (48.9928741455,33.6779317588), test loss: 34.8165215254\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.82047128677,4.79771656688), test loss: 3.35573640466\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (7.68165683746,33.3154491426), test loss: 30.4082268238\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.2288724184,4.73377254901), test loss: 2.41497339308\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (36.7242584229,32.9722622043), test loss: 36.38085742\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.77159130573,4.67323194779), test loss: 3.1598829627\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (9.32093524933,32.6431064632), test loss: 28.5659050226\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.83686113358,4.61524635472), test loss: 2.63092077971\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (14.1121044159,32.3203144886), test loss: 35.124777174\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.48470878601,4.55948559392), test loss: 3.11399348378\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (7.75611591339,32.0087758265), test loss: 25.7441235065\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.18383979797,4.50614656352), test loss: 2.48850566745\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (6.28873729706,31.7132192082), test loss: 35.100829649\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.69721364975,4.45468792852), test loss: 3.24220802486\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (14.5139446259,31.426433794), test loss: 30.7067676783\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.21032047272,4.40473590879), test loss: 3.35003823936\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (12.6141471863,31.1488645556), test loss: 29.119555378\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.50814342499,4.35759026431), test loss: 2.63971000016\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (17.1750335693,30.8782357541), test loss: 33.9917856693\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.91619729996,4.31202878683), test loss: 3.25529978573\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (13.0489759445,30.6171776726), test loss: 31.8232820511\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.62473940849,4.26849276561), test loss: 2.36422640681\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (20.1028594971,30.3679604507), test loss: 34.4755766869\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.68208861351,4.22664412626), test loss: 3.13738832474\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (25.4364204407,30.1195550842), test loss: 32.4216510534\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.76095294952,4.18598692853), test loss: 2.4781845957\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (16.456615448,29.8794656604), test loss: 35.0410450459\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.41467237473,4.14656570823), test loss: 3.04734972119\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (23.1009349823,29.6488598123), test loss: 27.7203326225\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (6.26064825058,4.10823910939), test loss: 2.67800942659\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (8.23898029327,29.4236067415), test loss: 36.3155707121\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.47353768349,4.071062189), test loss: 3.02479327023\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (27.4300804138,29.2044704108), test loss: 30.9889086723\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.95218086243,4.03552706353), test loss: 3.45873425305\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (23.1042175293,28.9890743067), test loss: 35.7600394249\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.26587820053,4.00094249948), test loss: 3.27696098685\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (12.0247220993,28.7804592635), test loss: 30.1730280161\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.993132472038,3.96778313509), test loss: 3.1823731184\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (29.0424194336,28.5797015004), test loss: 32.1754719973\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (4.70433855057,3.93571802974), test loss: 2.36406565011\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (14.6380472183,28.3780784022), test loss: 32.841315937\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.96439087391,3.90418290439), test loss: 3.24981839061\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (9.88513660431,28.1835817355), test loss: 32.7268968582\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.09843075275,3.87371899055), test loss: 2.47207981944\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (6.88638496399,27.9941381782), test loss: 35.6982356548\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.40815865993,3.8436062743), test loss: 3.12792363465\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.3453941345,27.8093448242), test loss: 28.4177326679\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.03831374645,3.81449753356), test loss: 2.52760755122\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (14.5402021408,27.6267767609), test loss: 36.3675513744\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (5.76508522034,3.78640741182), test loss: 3.02665244937\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.6885509491,27.4478548405), test loss: 25.7498008251\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.699441552162,3.75905243764), test loss: 2.44865394682\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (8.30815505981,27.2741703569), test loss: 36.4166827202\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.74082249403,3.73254632497), test loss: 3.18211903721\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (17.5735912323,27.1047064765), test loss: 31.6868779182\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.561612308025,3.7068444452), test loss: 3.31315500736\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (13.3997621536,26.9354213466), test loss: 35.8290083408\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.02333831787,3.68158120461), test loss: 2.98316507041\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.018579483,26.7705487691), test loss: 32.1501229525\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.47605967522,3.65701031544), test loss: 3.16889185607\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (7.39265632629,26.6098786531), test loss: 32.5326985836\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.68945407867,3.63255430851), test loss: 2.35001904219\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (16.1422348022,26.4516290178), test loss: 34.7577215433\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (5.10543394089,3.60886293187), test loss: 3.21424277425\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (26.5746173859,26.2948554308), test loss: 31.0504155636\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.5637755394,3.58573519909), test loss: 2.45535972714\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (1.87062323093,26.1407393095), test loss: 35.8896130085\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.85565567017,3.56342046869), test loss: 3.04977232963\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.7484283447,25.9908096061), test loss: 28.3298341274\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.80389559269,3.54161366322), test loss: 2.5529749006\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (45.3236694336,25.8436906219), test loss: 38.0030360222\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (4.15372562408,3.52040690112), test loss: 3.02104040384\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (24.3236656189,25.6963841676), test loss: 31.2414873123\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.3527816534,3.49934600886), test loss: 3.04812347293\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.54668045044,25.5529167845), test loss: 35.4571033955\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.12247467041,3.47890576412), test loss: 3.18904025853\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (9.42753791809,25.41237529), test loss: 29.2326143384\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.57064962387,3.45846040354), test loss: 3.10243684947\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.1782159805,25.2729728301), test loss: 32.3299782276\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.82527184486,3.43851700816), test loss: 2.5765857935\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.0379104614,25.1345008447), test loss: 32.3936484098\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.28682041168,3.41916741672), test loss: 3.1533443898\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (18.5270690918,24.9992055273), test loss: 35.9522397995\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.862688183784,3.40039367028), test loss: 2.48568131775\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.42958450317,24.8662668075), test loss: 33.7133120537\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.92477273941,3.38194071362), test loss: 2.96705373675\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.8042497635,24.7342972472), test loss: 30.2022590637\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.12693786621,3.36385681149), test loss: 2.51874547303\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (21.002948761,24.6043975687), test loss: 36.1035008907\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.72700548172,3.34606209605), test loss: 2.86570839137\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (5.98572921753,24.4765215326), test loss: 27.725895071\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.93118071556,3.32853402035), test loss: 2.52251956463\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.41437101364,24.3511298937), test loss: 36.2881910801\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.20534658432,3.31111929068), test loss: 3.03915449679\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (18.1736888885,24.226037592), test loss: 29.6199571133\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.44289255142,3.29408430279), test loss: 3.24268306494\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (33.1116905212,24.101552473), test loss: 36.5522961855\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.22965621948,3.27740331281), test loss: 3.17491007298\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (11.3004760742,23.979810063), test loss: 29.624586916\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.04566049576,3.26124984972), test loss: 3.03388012648\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (13.0104236603,23.8597171289), test loss: 34.2735692024\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.94180834293,3.24530163475), test loss: 2.38037840426\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (16.4648361206,23.7405318811), test loss: 33.9274324894\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.75247764587,3.22972290708), test loss: 3.1175847888\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (3.33574414253,23.6226824097), test loss: 31.5330323219\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.624037265778,3.21424565961), test loss: 2.41780734211\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (4.98980712891,23.5070663096), test loss: 36.3525281906\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.67237901688,3.19897863468), test loss: 3.0688485831\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (19.1554450989,23.3929568389), test loss: 29.8765402317\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.34460234642,3.18387148318), test loss: 2.57138217688\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (10.6031942368,23.2786024142), test loss: 36.4402423024\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.51723456383,3.16899041589), test loss: 2.92084771991\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (19.1921272278,23.1651287541), test loss: 25.500042963\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.4123955965,3.15445399972), test loss: 2.56736813188\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (22.5041790009,23.0541100655), test loss: 36.7493081093\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.875415802,3.14032873662), test loss: 3.11641693711\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.09580898285,22.9441208365), test loss: 29.5946906567\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.837923526764,3.12628972886), test loss: 3.14939536303\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (14.3347892761,22.8347128664), test loss: 33.1104865313\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.887374699116,3.11254749784), test loss: 2.54960443527\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (8.60429000854,22.7268580688), test loss: 32.0004849911\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.30741381645,3.09895400186), test loss: 3.0300949052\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (5.18279504776,22.6205142545), test loss: 35.2434320927\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.94743657112,3.08547558763), test loss: 2.43993845284\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (31.5675964355,22.5151535042), test loss: 34.5753807664\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.08758831024,3.07214130953), test loss: 2.95658560395\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (7.92623090744,22.4100139864), test loss: 32.8812585831\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.16090393066,3.05899154503), test loss: 2.52078849971\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (4.49660968781,22.3046839732), test loss: 36.7154723883\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.58546304703,3.04610031207), test loss: 2.85455383062\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (15.5128574371,22.2019212259), test loss: 28.0670190811\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.97198748589,3.03352500701), test loss: 2.51713914275\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (6.38975429535,22.1005999747), test loss: 38.2913432121\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.381747066975,3.02104461877), test loss: 3.00907924771\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (15.4592638016,21.9989139806), test loss: 29.2493989468\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.323463916779,3.00881735195), test loss: 3.03662633151\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.87724161148,21.8989085592), test loss: 37.1837962151\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.22158515453,2.9966486887), test loss: 3.11567742527\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (10.1065921783,21.8003881172), test loss: 28.8096966982\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.88831305504,2.98457803706), test loss: 2.93567051291\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (3.64100575447,21.7016603837), test loss: 35.9340497971\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.52766168118,2.97267469537), test loss: 2.3838549614\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.88902902603,21.6037163051), test loss: 34.3850958347\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.46271538734,2.9609022225), test loss: 3.01771982908\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (5.21409225464,21.5059813344), test loss: 34.5660353661\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.26859307289,2.94929737947), test loss: 2.37419962287\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (5.67458057404,21.4098528547), test loss: 36.8213619709\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.63865053654,2.93800032578), test loss: 3.02809245884\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (7.75535297394,21.3149727112), test loss: 29.2497157097\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.00110805035,2.92680125373), test loss: 2.4952306509\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (22.5560035706,21.2197047008), test loss: 40.0096090317\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.07035207748,2.9157576631), test loss: 2.92650181651\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (18.0507411957,21.126519591), test loss: 27.040775156\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.20079779625,2.90479823384), test loss: 2.38900081813\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.1754655838,21.0337348384), test loss: 37.7698077202\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.462946414948,2.89386489853), test loss: 3.02970110774\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (19.7545700073,20.9411507736), test loss: 28.8279500961\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (3.46187472343,2.88309322301), test loss: 3.08349804878\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (12.639339447,20.8493826479), test loss: 34.7727398396\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.1494538784,2.87245512511), test loss: 2.64727133065\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (4.06335401535,20.7574252693), test loss: 31.7618216038\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.584329843521,2.86194978389), test loss: 2.9789767921\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (5.26666975021,20.6666939514), test loss: 36.2598618507\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.878215193748,2.85167499794), test loss: 2.48099267781\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (7.74489450455,20.5774927399), test loss: 35.2204904079\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.6040790081,2.8415414826), test loss: 2.93161228001\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (4.97853088379,20.4875642132), test loss: 35.4113977194\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.5359865427,2.83150099935), test loss: 2.58992702067\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.9030656815,20.3994642081), test loss: 37.1905617714\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.839027762413,2.82151232908), test loss: 2.84527280629\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (18.0419960022,20.3118591778), test loss: 28.9349298\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (0.760918796062,2.81156570932), test loss: 2.5791821003\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (6.5430765152,20.2246897854), test loss: 38.529517746\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.71657991409,2.80171709095), test loss: 2.78810495138\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (18.4286346436,20.1378828963), test loss: 29.9409423113\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.76388168335,2.79204252068), test loss: 2.96217825413\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (17.7537155151,20.0510741662), test loss: 38.7380494952\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.35442137718,2.78243580367), test loss: 3.14705214202\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (8.85883331299,19.9655115265), test loss: 29.2008626461\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.603450357914,2.77301036057), test loss: 2.89402175844\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (4.51529407501,19.880760889), test loss: 36.7182820797\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.48207354546,2.76371509678), test loss: 2.49484843761\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (11.94348526,19.7957827261), test loss: 33.3550175667\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.591934740543,2.7545211457), test loss: 3.06505083442\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (9.81188678741,19.7124370082), test loss: 37.3764419079\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.5731651783,2.74537059487), test loss: 2.54268454909\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (7.49189186096,19.6293928749), test loss: 37.0572219372\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.39506936073,2.73621391222), test loss: 2.97590663135\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (8.85601902008,19.546534536), test loss: 30.1901787758\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.73065781593,2.72715306295), test loss: 2.48240741491\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (9.60313034058,19.4640943971), test loss: 39.5539050102\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.2110850811,2.71823806652), test loss: 2.78171131015\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (11.3815002441,19.3823203304), test loss: 27.3788822651\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.22581791878,2.70941491711), test loss: 2.31578090638\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (9.26153755188,19.3009295949), test loss: 39.0958856583\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.71773600578,2.7007311913), test loss: 3.02118756771\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (8.11429309845,19.2206573719), test loss: 28.9821237087\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.656970143318,2.69214255574), test loss: 2.9781516999\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (8.49393939972,19.1401822037), test loss: 40.0626679897\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.04501116276,2.68367056043), test loss: 3.07639397383\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (5.61399173737,19.061025904), test loss: 31.4478232622\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.793378293514,2.67517574199), test loss: 2.92340822518\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (7.03065919876,18.9819766225), test loss: 37.6569347858\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (3.38543176651,2.66674144411), test loss: 2.46023714244\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (7.12060928345,18.9035867642), test loss: 38.3327805996\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.853905975819,2.65834718397), test loss: 3.01132190824\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (8.89425086975,18.8252672223), test loss: 35.9697580338\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.55672359467,2.65012779653), test loss: 2.65911190808\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (7.25326061249,18.7472652919), test loss: 37.8636891842\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.02871394157,2.64194439317), test loss: 2.95364834219\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (10.9220790863,18.6702702119), test loss: 29.9866275311\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.751517474651,2.63386356742), test loss: 2.54968319535\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (14.9705505371,18.5940464219), test loss: 39.5235176563\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.73730826378,2.62595198427), test loss: 2.77817110717\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (4.67520236969,18.5174215145), test loss: 30.9504566669\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.2798500061,2.61807883027), test loss: 2.84183202386\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (17.7417106628,18.4421593309), test loss: 39.6286639214\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.932303845882,2.61021006527), test loss: 3.1040469408\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (8.35871124268,18.3673597516), test loss: 29.463505578\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.55209994316,2.60232882894), test loss: 2.92859608531\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (4.2424659729,18.2927169572), test loss: 37.5962758064\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.1808180809,2.59457210344), test loss: 2.654070279\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (10.5971698761,18.2181623981), test loss: 32.306583643\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (3.46968650818,2.58692285207), test loss: 2.94863651097\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (6.30393075943,18.1444080274), test loss: 41.233355999\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.77172398567,2.57933168857), test loss: 2.61763720959\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (10.8448104858,18.0709819897), test loss: 36.2332332611\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.91331768036,2.57180041783), test loss: 2.87902393341\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (2.85201048851,17.998385771), test loss: 36.672306776\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.178285092115,2.56442693001), test loss: 2.88918854296\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (2.70335435867,17.9258874091), test loss: 40.4740247726\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.41109108925,2.55710595446), test loss: 2.82014151812\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (8.08171653748,17.854346131), test loss: 28.2533604622\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.15430235863,2.54977160055), test loss: 2.39722318351\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (2.66864609718,17.7830219571), test loss: 40.2898976326\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.593600690365,2.54238864141), test loss: 2.94866482168\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (5.78307533264,17.7121735318), test loss: 29.7470872402\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.46879589558,2.53518220912), test loss: 2.91277233064\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.3922958374,17.6415910517), test loss: 41.5342319012\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.63713634014,2.52801287997), test loss: 3.10064162165\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (10.6048278809,17.5713878625), test loss: 30.081193161\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.85654389858,2.52094130728), test loss: 2.84491771162\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.32956695557,17.5015923344), test loss: 40.020710659\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.688601911068,2.51391508893), test loss: 2.46356637031\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (14.5841360092,17.432889441), test loss: 38.4130239964\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.987893879414,2.50704587648), test loss: 3.21658804417\n",
      "\n",
      "MC # 3, Hype # hyp5, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold5/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (166.366851807,inf), test loss: 144.456360817\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (355.396362305,inf), test loss: 378.354934692\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (30.6081466675,58.5334925375), test loss: 43.6323641777\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.49493098259,46.8647097479), test loss: 3.39911119342\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (35.3648414612,52.3172837706), test loss: 37.0227543354\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.24322748184,25.1890969704), test loss: 3.13965304494\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (44.4541893005,50.2137500257), test loss: 42.2323444843\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.80998325348,17.9433986369), test loss: 3.70535550714\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (27.3421878815,49.2227310653), test loss: 39.2699991703\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.36196208,14.3238897982), test loss: 3.84833966792\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (28.2263870239,48.5016679162), test loss: 39.8692514896\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.693577051163,12.1523700294), test loss: 3.636413908\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (62.1087493896,47.9839616855), test loss: 41.2872826576\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (5.78413391113,10.701285629), test loss: 3.9329059124\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (52.2933197021,47.5162360018), test loss: 36.7066693306\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.7017083168,9.66123440796), test loss: 2.80891669989\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (45.0640487671,47.088539921), test loss: 42.0704532385\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.790231108665,8.87730445429), test loss: 3.76431737542\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (51.0787277222,46.7528647364), test loss: 38.3765708923\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.5397939682,8.2584149494), test loss: 2.74093442261\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (23.3389034271,46.5001165439), test loss: 40.7105839252\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (5.32368469238,7.76265432012), test loss: 4.05639617443\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (46.6414718628,46.2386471399), test loss: 37.7280851364\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.25022876263,7.35614140936), test loss: 2.73065617681\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (57.0790252686,45.9844377816), test loss: 38.9016018867\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.36828041077,7.01665552363), test loss: 3.60234160423\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (47.1624221802,45.7120539198), test loss: 38.4288825989\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.48917031288,6.72775311142), test loss: 2.96904636025\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (32.8406181335,45.4341847413), test loss: 40.7383525848\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.56825840473,6.47813739147), test loss: 3.36413916349\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (29.017999649,45.1848489039), test loss: 36.7899633408\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.23435354233,6.25870870261), test loss: 2.8437823534\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (20.6185092926,44.970502152), test loss: 40.4242108822\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (5.74196243286,6.06773679229), test loss: 3.65819582343\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (58.6305770874,44.7402651931), test loss: 33.4662361145\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.28529953957,5.89834737125), test loss: 2.77791054845\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (33.2090148926,44.5081801693), test loss: 38.4513718128\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.32266235352,5.74704444889), test loss: 3.17287066579\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (28.4687156677,44.2547885521), test loss: 36.1696567535\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.19944858551,5.61023785526), test loss: 3.45658134818\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (44.4517059326,43.9903588213), test loss: 36.4822722435\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.06999254227,5.48522118778), test loss: 3.21543336511\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (48.2952194214,43.7370759502), test loss: 36.5579818726\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (5.7407989502,5.36965411064), test loss: 3.51905061007\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (39.0412101746,43.4962270703), test loss: 32.3565615177\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.06900167465,5.26434107802), test loss: 2.78187040985\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (22.1107254028,43.2375832124), test loss: 37.7818371773\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.939936757088,5.16695664897), test loss: 3.46355897784\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (15.3310146332,42.9739763629), test loss: 33.4273953438\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.42817115784,5.0768435928), test loss: 2.45343544781\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (34.7831497192,42.6882104982), test loss: 36.0469024062\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.37500739098,4.99211684389), test loss: 3.4962897718\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (44.7484817505,42.389109539), test loss: 31.9531607628\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.02246165276,4.91198639007), test loss: 2.46077613235\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (35.8344726562,42.0935524432), test loss: 34.8507506847\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (4.97492408752,4.83543205468), test loss: 3.42768701315\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (54.5357513428,41.8038490757), test loss: 33.0853598118\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.34239912033,4.76363816719), test loss: 2.66854250431\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (28.0599822998,41.4952090974), test loss: 35.3629253864\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.98169493675,4.69555750552), test loss: 3.25270733833\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (32.585899353,41.1822346934), test loss: 30.5849703789\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.86048841476,4.63112383121), test loss: 2.58783177137\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (41.853263855,40.8476292385), test loss: 34.9398153782\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.8587667942,4.56895919), test loss: 2.8510436058\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (22.7128601074,40.5042376252), test loss: 27.3033028364\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.76221239567,4.50910351216), test loss: 2.53595290482\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (33.8326835632,40.1651603068), test loss: 32.9542811871\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.63237559795,4.45084881958), test loss: 2.98521946669\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (39.3704452515,39.8297260214), test loss: 30.0280252934\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.35940694809,4.39561689217), test loss: 3.33023108244\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (20.5704860687,39.4826900253), test loss: 31.8075080633\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.58249044418,4.34253246132), test loss: 3.0732178688\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (33.2260437012,39.1379965556), test loss: 30.1849635601\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (4.53769254684,4.29192386583), test loss: 3.28586637676\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (14.1829633713,38.780574775), test loss: 26.0933089495\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.30213952065,4.2429464607), test loss: 2.39390736222\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (10.8749446869,38.4237675355), test loss: 31.48977952\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.51141834259,4.19533349784), test loss: 3.31094634235\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (16.9958553314,38.0782187446), test loss: 27.845680809\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.928606629372,4.1488719846), test loss: 2.24961938858\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (20.2289676666,37.74002805), test loss: 30.8167040825\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.38504934311,4.10465901641), test loss: 3.36573343277\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (23.3982334137,37.400657296), test loss: 27.2816927195\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (4.1566028595,4.06205634453), test loss: 2.2545878917\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (21.1880874634,37.0706002992), test loss: 30.1015740633\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.99384069443,4.02138349618), test loss: 3.23130964637\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (23.214471817,36.7385941728), test loss: 29.9311558366\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.80991363525,3.98199329549), test loss: 2.4796888113\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (2.12791252136,36.4115872342), test loss: 30.9119452477\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.56269049644,3.9437078595), test loss: 3.06627542078\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (19.2971343994,36.099358424), test loss: 27.2745844364\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.93868112564,3.90621522575), test loss: 2.51771549433\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (21.1019363403,35.7942923772), test loss: 32.6630038261\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.14874482155,3.87041182917), test loss: 2.89842282832\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (22.5167045593,35.4900946368), test loss: 25.7733847618\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.46343135834,3.83594573806), test loss: 2.50904276669\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.742099762,35.1960984078), test loss: 32.0628243446\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.1496014595,3.80305618309), test loss: 2.95668987036\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (25.9598846436,34.9027191248), test loss: 28.8871920586\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.25430262089,3.77106758273), test loss: 3.2907897383\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.4403295517,34.6150385821), test loss: 31.2754847765\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.86080765724,3.73982935964), test loss: 3.03299631476\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (10.7648620605,34.3407264787), test loss: 28.4522554874\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.88739085197,3.70910920533), test loss: 3.27577998042\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (20.3000736237,34.0739276699), test loss: 25.7843087196\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.20158648491,3.6796309511), test loss: 2.43705528527\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (18.8378372192,33.8094866891), test loss: 30.2837574482\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.53526449203,3.65111787046), test loss: 3.28171721101\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.2883262634,33.5549847018), test loss: 27.8419027805\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.981588125229,3.6238711117), test loss: 2.22239245474\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (24.8144454956,33.3022657113), test loss: 30.1118921757\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.728291332722,3.59723526841), test loss: 3.2801174134\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (12.1100082397,33.0544097038), test loss: 27.8430685043\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.44902944565,3.57117787025), test loss: 2.19711868167\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (15.9747543335,32.818103171), test loss: 29.3572714567\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (4.44167470932,3.54548877132), test loss: 3.15185630322\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.5869312286,32.5870933728), test loss: 29.7317962646\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.737987041473,3.52060867975), test loss: 2.44435885251\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (15.0444526672,32.3577014864), test loss: 30.9171474934\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.09789037704,3.49654183301), test loss: 3.03155636489\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.6786670685,32.1366531486), test loss: 27.4063980579\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.726316869259,3.47344312206), test loss: 2.50280271471\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.2995910645,31.9170614394), test loss: 34.0695231199\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.325571417809,3.45082029047), test loss: 2.82011098564\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (10.690032959,31.70093182), test loss: 26.0284829617\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.07880711555,3.42861806275), test loss: 2.46353330016\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (10.8829345703,31.4940823827), test loss: 32.054280448\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.31137418747,3.40666581738), test loss: 2.90579418689\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (8.81272315979,31.2912862434), test loss: 28.882590723\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.53455150127,3.38528461547), test loss: 3.18188521415\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (24.4012985229,31.089651772), test loss: 30.8068596363\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.73683333397,3.36453717646), test loss: 3.02278444767\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.3799209595,30.8944334535), test loss: 28.1945436478\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.21702408791,3.34456719937), test loss: 3.24021146894\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (14.3318834305,30.7001150057), test loss: 28.2711437702\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.528590559959,3.32495906973), test loss: 2.4668800503\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (9.78166484833,30.5087238203), test loss: 29.4357703686\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.62230575085,3.30566926905), test loss: 3.20709062815\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (10.4521083832,30.3250948864), test loss: 28.3769274712\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.67125415802,3.28652940292), test loss: 2.19031674862\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (4.05421161652,30.1439204055), test loss: 30.0894487858\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.83217310905,3.26786192803), test loss: 3.21249623895\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (32.610496521,29.9641445208), test loss: 27.9722014427\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.37889766693,3.24967661316), test loss: 2.21826583743\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (10.3113832474,29.7887463058), test loss: 29.6194021225\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.01038408279,3.23211308731), test loss: 3.06755691767\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (14.1943778992,29.6140523236), test loss: 32.0984211683\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.841772794724,3.21484108365), test loss: 2.48114594519\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (9.41538238525,29.4422980233), test loss: 30.7358452082\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.05979752541,3.19780309443), test loss: 2.98544340432\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (6.87284851074,29.2764158115), test loss: 27.4415155888\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.0863724947,3.18086755153), test loss: 2.47248018384\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (8.04637336731,29.1124091584), test loss: 32.5219526052\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.01266598701,3.16430768272), test loss: 2.77034915984\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (31.4650993347,28.9495905164), test loss: 26.0199091673\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.77694535255,3.14811201162), test loss: 2.45002175272\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (9.35023117065,28.7901868357), test loss: 31.9789077759\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.886643469334,3.13243926189), test loss: 2.83952513635\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (15.2679710388,28.6317233551), test loss: 28.485854578\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.27699756622,3.11704480339), test loss: 3.08416001946\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.8375682831,28.4756511363), test loss: 30.6930012226\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.833982229233,3.10180981713), test loss: 2.93249217272\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (8.32438564301,28.324103652), test loss: 27.3932177067\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.1895788908,3.0866456839), test loss: 3.13664429784\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (9.72615909576,28.174384558), test loss: 26.8353172779\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.81012320518,3.07177746994), test loss: 2.37934239507\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (13.7011947632,28.0252362038), test loss: 29.4880120277\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.798528909683,3.05721171522), test loss: 3.12017312646\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (8.47026538849,27.8792160309), test loss: 29.2360603333\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.80277967453,3.04312000886), test loss: 2.22794221342\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (34.404045105,27.7342688734), test loss: 29.1207026005\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.01768064499,3.02920679188), test loss: 3.17971621752\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (22.8288269043,27.5908582913), test loss: 28.1122380733\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.73672449589,3.01545649303), test loss: 2.12004660964\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (6.93917179108,27.4512663463), test loss: 29.8431372404\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.920613229275,3.00170132474), test loss: 3.00906378627\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (15.5897932053,27.3133731946), test loss: 30.559395504\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (3.38076877594,2.98827326509), test loss: 2.45595818162\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (3.07296204567,27.1755697656), test loss: 31.317609787\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.832622289658,2.97500995673), test loss: 2.96637535989\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (10.6763591766,27.0407242147), test loss: 27.9054837227\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.17565155029,2.96218261924), test loss: 2.49564684033\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (36.2754936218,26.9069539107), test loss: 34.9507960796\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.41809630394,2.94953628254), test loss: 2.82999901474\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (25.9787731171,26.7743851754), test loss: 26.2665027857\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.86262381077,2.93698745286), test loss: 2.39343478233\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (6.51043367386,26.6447753827), test loss: 31.9041276455\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.864030361176,2.92442436803), test loss: 2.80337753892\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (29.7104949951,26.517083484), test loss: 28.0634517193\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.24648189545,2.91217459644), test loss: 2.96366614997\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (2.59728193283,26.3887101769), test loss: 30.792018795\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.85397040844,2.90001827995), test loss: 2.89132426679\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (12.4054946899,26.2634453408), test loss: 26.730169487\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.27509307861,2.88825179246), test loss: 3.03803077042\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (34.678062439,26.13903583), test loss: 29.2046383142\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.95224452019,2.87666615322), test loss: 2.43968001008\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (24.240020752,26.0153720325), test loss: 28.9847319126\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.71535265446,2.86511929105), test loss: 3.07005292177\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (2.80904197693,25.8944086712), test loss: 30.0359514713\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.308787167072,2.8535528689), test loss: 2.23555683643\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (27.461643219,25.7749259878), test loss: 28.8197035789\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.5345723629,2.84227635707), test loss: 3.10451500118\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (3.76427507401,25.654785959), test loss: 28.5641971827\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.23444843292,2.83106339025), test loss: 2.14002246261\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (9.07198524475,25.5374510917), test loss: 30.1832234383\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.483397603035,2.82018270262), test loss: 2.94718858898\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (18.0545806885,25.4206117747), test loss: 32.2517507792\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.43834042549,2.80947927096), test loss: 2.51273632348\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.2726955414,25.3052454028), test loss: 31.6393994331\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.08099400997,2.79879441515), test loss: 2.92441089749\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (19.2690048218,25.1919587168), test loss: 28.3818050385\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (3.1275677681,2.7881440627), test loss: 2.4667679131\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (17.8948783875,25.0792237875), test loss: 33.4996220589\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.02365541458,2.77762787978), test loss: 2.73562790453\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (6.23751163483,24.9666304108), test loss: 26.9557049751\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.43436849117,2.76724605855), test loss: 2.39444447309\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (9.65963459015,24.8561016964), test loss: 32.5163490057\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.994991898537,2.75713476117), test loss: 2.74271301925\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.0934104919,24.7463716887), test loss: 28.8270393848\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.29324197769,2.74718771274), test loss: 2.9571100384\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.4643135071,24.6379232852), test loss: 31.6961487293\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.930843532085,2.73725604856), test loss: 2.85854440033\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (22.8888988495,24.5308066216), test loss: 26.6770615101\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (4.13130521774,2.72735139902), test loss: 2.981118536\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (5.16649436951,24.4246872657), test loss: 29.5577587128\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.616675496101,2.71754961201), test loss: 2.51058161855\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (6.89245796204,24.3183181938), test loss: 30.1226704121\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.44943153858,2.70789403846), test loss: 3.05307952762\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (11.2904567719,24.2139260866), test loss: 31.2529500961\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.937227964401,2.69843851251), test loss: 2.29737817943\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (6.92043352127,24.1102114448), test loss: 29.0987999916\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.17956495285,2.68916955468), test loss: 3.06332909465\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (9.34184360504,24.0074989375), test loss: 29.1832098007\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.15897607803,2.67989400468), test loss: 2.1522557497\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (11.184135437,23.9062701622), test loss: 31.1761599064\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (4.57056236267,2.67065396361), test loss: 2.93569411039\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (3.8504986763,23.8056213848), test loss: 32.4965087891\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.362248599529,2.66147010449), test loss: 2.54697186351\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (7.83345413208,23.7047699948), test loss: 32.4769834518\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.29830360413,2.65242577142), test loss: 2.92929841578\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (15.1895246506,23.6058019649), test loss: 28.9574073315\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.44928896427,2.64357462986), test loss: 2.53616580963\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (11.7325706482,23.5072457632), test loss: 34.1021371365\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.3678920269,2.6348777072), test loss: 2.68460222483\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (9.62014198303,23.4097580258), test loss: 27.8068640947\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.36857688427,2.6261779153), test loss: 2.42863933146\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (11.8854494095,23.3135058348), test loss: 33.7608066082\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (3.23130345345,2.61750293823), test loss: 2.66991461515\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (12.8702325821,23.2176930011), test loss: 28.8287667036\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.345841288567,2.60887368901), test loss: 2.92618170083\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (11.07492733,23.1217158517), test loss: 32.3794253826\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.462880760431,2.60036710171), test loss: 2.86984567642\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (14.9216918945,23.0274032167), test loss: 27.3018536568\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.33983302116,2.59203616353), test loss: 2.99966414571\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (33.195930481,22.9338055536), test loss: 34.6941665888\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.60327720642,2.58385937773), test loss: 2.7020064041\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.12827110291,22.8404774797), test loss: 29.1108614922\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.89098119736,2.57565555919), test loss: 3.06874490976\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (12.4826374054,22.7485806372), test loss: 32.425512886\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.10426425934,2.56747118616), test loss: 2.35640686601\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (13.3575754166,22.6570451868), test loss: 29.8057551861\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.07734632492,2.55933262069), test loss: 3.1048534587\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (11.2678461075,22.5653378605), test loss: 29.9729056358\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.34183132648,2.55130915215), test loss: 2.20612502545\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (12.7195997238,22.4752120222), test loss: 31.658478117\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.09898138046,2.54344387406), test loss: 2.9506432116\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (33.4234275818,22.3854953116), test loss: 36.723244524\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.66687965393,2.5356930863), test loss: 2.73513514996\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (8.24523925781,22.2962919796), test loss: 33.0453924656\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.03326177597,2.52794530922), test loss: 2.98188555837\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (13.3033027649,22.2083054548), test loss: 30.3323080778\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.03196752071,2.52019937787), test loss: 2.60145421922\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.38160991669,22.1204255466), test loss: 34.309895134\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.34873652458,2.51249327216), test loss: 2.68937748373\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (7.61778497696,22.0326305056), test loss: 28.6840394974\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.98960185051,2.50491033405), test loss: 2.46841785312\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (10.5575065613,21.9459673962), test loss: 33.760180378\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.22116804123,2.49744634702), test loss: 2.66403771937\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (17.7219009399,21.8600596972), test loss: 33.1966389894\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.01857161522,2.49010214044), test loss: 3.19191659689\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.10760211945,21.7745222641), test loss: 33.8546611786\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.73392486572,2.48275745465), test loss: 2.91206449866\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (6.52512359619,21.6896954169), test loss: 27.5665085316\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.911247491837,2.47538535476), test loss: 3.02286930084\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (10.3181629181,21.6054683213), test loss: 34.3062133312\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.80755889416,2.46809159856), test loss: 2.69877307117\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (8.56139469147,21.5210508157), test loss: 30.0394682884\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.25029850006,2.46089516671), test loss: 3.08017031401\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.19088888168,21.4376627435), test loss: 32.5906727314\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.05215215683,2.45378554927), test loss: 2.42910026312\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (25.9837112427,21.3553147837), test loss: 34.2121994972\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.41062664986,2.44683444582), test loss: 3.32490180135\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (9.14175701141,21.2726009838), test loss: 31.2410794258\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.77699911594,2.43984086472), test loss: 2.24603954554\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (6.02189922333,21.1910667461), test loss: 33.1332017422\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.716451704502,2.43282544484), test loss: 2.98484948277\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (9.23386573792,21.1100194391), test loss: 35.0125322819\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (3.19675827026,2.42592813552), test loss: 2.6426710695\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (14.4025087357,21.0286814191), test loss: 33.64957757\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.78553879261,2.41903519969), test loss: 3.02413502336\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (8.31608581543,20.9483581116), test loss: 30.5549564838\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.409547477961,2.41227376815), test loss: 2.58264299035\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (22.7793903351,20.8688094882), test loss: 42.7515872955\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.96801304817,2.40564110545), test loss: 3.07882640362\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (10.704533577,20.7892391318), test loss: 29.4912445545\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.79984498024,2.39898819852), test loss: 2.52302742004\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.07715320587,20.7106354009), test loss: 35.2695922852\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.889905333519,2.39229493718), test loss: 2.69102053046\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.92910814285,20.6322573573), test loss: 30.0329640388\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (3.14946746826,2.38571646805), test loss: 3.02244639099\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (14.7674007416,20.5540146245), test loss: 34.9961072206\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.721719682217,2.37913611955), test loss: 2.97923569083\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (7.79227638245,20.4764360947), test loss: 29.5790847301\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.352545320988,2.37268107925), test loss: 3.01814468503\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.45087051392,20.3996894005), test loss: 41.6843861103\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.854079842567,2.36634522842), test loss: 3.22737608254\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (8.05100440979,20.3229483049), test loss: 30.1505204916\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.27775716782,2.35998959549), test loss: 3.16468383372\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (7.08412361145,20.2470603452), test loss: 34.0844573736\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.73250246048,2.35359394215), test loss: 2.47484517992\n",
      "run time for single CV loop: 7087.40909505\n",
      "\n",
      "MC # 3, Hype # hyp4, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold1/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (269.10043335,inf), test loss: 154.282530022\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (326.558807373,inf), test loss: 373.315306091\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (50.7369689941,74.480282239), test loss: 48.620279789\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.49689650536,146.167616844), test loss: 3.57899935246\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (51.1495857239,60.2948819184), test loss: 38.1500692368\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.48467898369,74.7965056866), test loss: 2.88895405531\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (20.4993171692,55.5573161332), test loss: 47.1013325691\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.5564622879,51.0053191219), test loss: 3.55367513895\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.1244735718,53.2957068238), test loss: 42.7080146313\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.80986452103,39.1132677805), test loss: 3.55657728314\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (29.4129638672,51.8344208672), test loss: 49.5844902039\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.98714709282,31.9789396283), test loss: 3.44345756173\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (11.9546985626,50.8209191573), test loss: 46.7011845589\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.948911726475,27.2298597217), test loss: 3.63800207973\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (38.0679779053,50.0424208114), test loss: 45.7790209293\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.66416144371,23.8315089117), test loss: 3.06130206287\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (62.9655838013,49.4323791016), test loss: 46.7750512123\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.60143852234,21.2879547393), test loss: 3.79213527739\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (46.0597724915,48.8778896988), test loss: 43.1098049641\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.69959235191,19.3078446944), test loss: 2.96940302253\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (45.1387367249,48.4379832105), test loss: 47.7883236885\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.60101413727,17.7215611702), test loss: 3.77774939239\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (28.1014900208,48.1095797308), test loss: 40.5992275238\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (5.68756961823,16.4266028183), test loss: 2.9903549552\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (19.777256012,47.7907049478), test loss: 46.898186779\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.70952582359,15.3472197457), test loss: 3.53628953099\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (116.897315979,47.4960443828), test loss: 37.6954433441\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (6.66941261292,14.4363754017), test loss: 2.88999716043\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (52.0193481445,47.2147121347), test loss: 45.40189991\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.44650685787,13.6523619441), test loss: 3.45597060919\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (51.0100021362,46.9522300255), test loss: 40.2229756355\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.8876042366,12.9748948151), test loss: 3.50927945971\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (36.135345459,46.6764321074), test loss: 44.9696774483\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.19309186935,12.3810567301), test loss: 3.52021608949\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (33.8714294434,46.4301505714), test loss: 42.4261440754\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.2948153019,11.8555181954), test loss: 3.68258689642\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (38.1075210571,46.2245527321), test loss: 42.9475873947\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.96059083939,11.3899323585), test loss: 3.01749706864\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (66.0029449463,46.0054780085), test loss: 45.3666009903\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.81568074226,10.9729703302), test loss: 3.81699821949\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (21.2969055176,45.7786209161), test loss: 43.3506323338\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.08413946629,10.5984602985), test loss: 3.02911596894\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (29.6536750793,45.5628519934), test loss: 42.9057552814\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.90064620972,10.2578202553), test loss: 3.7314604342\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (43.2693939209,45.3444220813), test loss: 39.3741752148\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.1914358139,9.94843190052), test loss: 3.04881924391\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (38.8124580383,45.1058265901), test loss: 44.9840779305\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.07077383995,9.66491110312), test loss: 3.63512117267\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (5.75804758072,44.8791852032), test loss: 36.454685688\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.96427512169,9.4036382136), test loss: 2.96798394024\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (21.4733066559,44.6723980463), test loss: 43.7051940918\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.5457701683,9.16364752322), test loss: 3.71287239194\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (31.4171104431,44.4457368943), test loss: 32.363450861\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.653747797012,8.94114502759), test loss: 2.85831029415\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (56.9963684082,44.2153463353), test loss: 42.7071275234\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (6.93674993515,8.73514094198), test loss: 3.51851524711\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (16.2272701263,43.9823180788), test loss: 37.5536957741\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.08642721176,8.54168609801), test loss: 3.59945918024\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (36.2700004578,43.7384546437), test loss: 38.0655672073\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.68466758728,8.36111162582), test loss: 3.059183532\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (25.4995040894,43.4804501582), test loss: 40.3803048134\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.12613630295,8.19154475333), test loss: 3.56232033968\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (42.7862434387,43.2254353359), test loss: 39.0535935402\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (5.10858535767,8.03096209483), test loss: 2.81755091548\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (20.0019187927,42.9732972555), test loss: 39.9945953369\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.55379414558,7.87999285287), test loss: 3.67690517902\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (37.6250953674,42.7020226864), test loss: 35.1057547569\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.8992061615,7.73695577428), test loss: 2.81188146472\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (45.4538116455,42.4236527944), test loss: 40.8242769241\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.45561504364,7.60129860835), test loss: 3.54222620875\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (25.6870384216,42.1371846821), test loss: 33.5784864426\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.39475154877,7.47095272488), test loss: 2.95501337051\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (43.1697731018,41.8338396344), test loss: 39.2839350462\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.65965676308,7.34620842004), test loss: 3.19352046549\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (25.9325714111,41.5196249132), test loss: 29.4366651535\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (5.34327983856,7.22711587906), test loss: 2.65768621564\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (38.8946800232,41.2069705754), test loss: 37.339969492\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.27808785439,7.112167476), test loss: 3.31719127893\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (23.262304306,40.8961438134), test loss: 31.2216349363\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.91509568691,7.00256969459), test loss: 3.21234630644\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (18.6616802216,40.5695973984), test loss: 36.8006320477\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.43655014038,6.89699048223), test loss: 3.30988655388\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (23.1024513245,40.2400073449), test loss: 32.9132358074\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.74504995346,6.79562153528), test loss: 3.40166195929\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (12.4476127625,39.9126966231), test loss: 33.47262609\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.01292300224,6.69781230951), test loss: 2.60518030524\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (113.937705994,39.5731081194), test loss: 34.876804018\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.29917788506,6.60341441759), test loss: 3.55543307066\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (19.3289718628,39.2345462202), test loss: 33.8808883667\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.28592276573,6.51227318231), test loss: 2.60210329592\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (15.7764453888,38.8987958316), test loss: 33.7515453577\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.04506468773,6.42392346804), test loss: 3.4260145098\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (51.1408081055,38.5696473104), test loss: 31.4031217337\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (4.27497959137,6.33895221259), test loss: 2.63177633584\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (33.0821075439,38.2356643432), test loss: 36.4124335051\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.31827974319,6.25674178072), test loss: 3.42510228157\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (9.90760231018,37.9066677096), test loss: 28.0554741144\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (4.47061777115,6.17744954707), test loss: 2.75312509835\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (28.615901947,37.5863485855), test loss: 34.7733280182\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.9352876544,6.10098972594), test loss: 3.24829168618\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (5.30384063721,37.2609354765), test loss: 29.5876585007\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.879988670349,6.02682461191), test loss: 3.10835479498\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (8.54288291931,36.9491523859), test loss: 36.1572340488\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.8829126358,5.95516688734), test loss: 3.40429678559\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (10.481836319,36.6439750254), test loss: 30.5428202152\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.38546609879,5.88554407246), test loss: 3.40603315681\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.4625329971,36.3469897207), test loss: 31.2423523903\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.53688120842,5.81854874806), test loss: 2.93359074593\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.0875682831,36.0527666053), test loss: 34.2415679455\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.50773048401,5.75369597481), test loss: 3.47895519137\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (14.8690128326,35.7683883621), test loss: 33.7695954323\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.2999382019,5.69110235026), test loss: 2.68991623521\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (52.670173645,35.4923570367), test loss: 33.7785516024\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (4.15352058411,5.63076168624), test loss: 3.40268847942\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (52.454574585,35.2190123257), test loss: 31.7444299221\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.41352581978,5.57206976035), test loss: 2.69713378549\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (3.2859017849,34.9568766425), test loss: 35.9412581682\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.51092839241,5.51527803557), test loss: 3.52735865116\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.0170154572,34.7024857135), test loss: 31.5640504837\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.4096827507,5.45989109003), test loss: 2.93253668547\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.1265449524,34.4544784291), test loss: 36.845448494\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.34763336182,5.40662725597), test loss: 3.35188755393\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (22.4768066406,34.2112176016), test loss: 28.8195011139\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.18393230438,5.35482689501), test loss: 2.73265111893\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (9.97717666626,33.9763962596), test loss: 36.494914937\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.04040050507,5.30486163531), test loss: 3.43091906011\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (19.4516048431,33.74755988), test loss: 31.1903600931\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.11126804352,5.2564404881), test loss: 3.50589891821\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (21.2182025909,33.5228952614), test loss: 37.771901989\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.03030920029,5.209292666), test loss: 3.52568178177\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (26.3297042847,33.3087024024), test loss: 31.5089140415\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.56459665298,5.16348027039), test loss: 3.49859363735\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (24.0614070892,33.0989423469), test loss: 35.5676053286\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.51984524727,5.11870151699), test loss: 2.77029474974\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (22.6101036072,32.8941117878), test loss: 34.5431535721\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.19860553741,5.07552071018), test loss: 3.58935876489\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (23.1232757568,32.6929226092), test loss: 35.5226387501\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.989667832851,5.03332727846), test loss: 2.70200512409\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (12.3303508759,32.4997377742), test loss: 34.9509278774\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.07388305664,4.99261915118), test loss: 3.47034555078\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (16.0769367218,32.3096292441), test loss: 34.0508512497\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.02499628067,4.9529178016), test loss: 2.849669227\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (22.4596862793,32.1234931224), test loss: 37.2844053268\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.55927681923,4.91424789091), test loss: 3.48401291072\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.2707099915,31.9458710546), test loss: 29.6812574863\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.28038394451,4.87655401295), test loss: 2.87084307075\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (17.0332679749,31.7718178373), test loss: 36.4784916878\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.47256350517,4.83959546524), test loss: 3.37715376616\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (20.9999160767,31.6008394787), test loss: 33.1396898985\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (4.96595144272,4.80380040003), test loss: 3.39412586987\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (9.64711666107,31.4327875458), test loss: 37.6662466049\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.30882167816,4.76884394683), test loss: 3.53984429538\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.0220422745,31.2712159683), test loss: 31.0742684603\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.79018878937,4.73493775658), test loss: 3.41115282476\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (21.3221721649,31.1115897545), test loss: 33.4319106102\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.432919979095,4.70183756596), test loss: 3.04544871747\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (18.9742889404,30.9550876642), test loss: 34.2524396896\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.79544043541,4.66945819019), test loss: 3.56236339211\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (54.3149108887,30.8052099244), test loss: 34.5906767368\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.5611987114,4.63789008144), test loss: 2.59187286049\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (10.1315002441,30.6584426492), test loss: 34.889482975\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.35378968716,4.60677859259), test loss: 3.39622479677\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (29.5531864166,30.5135040294), test loss: 33.7747098923\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.2612361908,4.57650585707), test loss: 2.75461179614\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (14.604511261,30.3709929796), test loss: 35.3575981617\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.96023058891,4.54710899281), test loss: 3.46299980581\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (16.5370006561,30.2338284671), test loss: 33.0428698063\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.54268121719,4.51834295228), test loss: 2.94586673975\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (31.6987609863,30.0975457473), test loss: 37.2186596394\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.13714003563,4.49030867004), test loss: 3.38417933881\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (35.1368370056,29.9639308469), test loss: 29.7430379868\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.09735250473,4.46278952463), test loss: 2.75336658061\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (10.3366422653,29.8349700978), test loss: 37.2840456486\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.45902752876,4.43586123121), test loss: 3.46331300139\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (13.2866039276,29.7100229816), test loss: 31.7958590984\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.09624552727,4.40931106234), test loss: 3.43394839168\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.200966835,29.5849330784), test loss: 38.4885380745\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.01164531708,4.38333562573), test loss: 3.54799214005\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (15.2084693909,29.4624910273), test loss: 31.5628099442\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.0261259079,4.35814060588), test loss: 3.4492920965\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (17.0845756531,29.3442813853), test loss: 35.87155056\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.51591861248,4.33346312233), test loss: 2.84257147312\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (11.1799955368,29.2259677169), test loss: 34.2829498291\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.29746830463,4.3093559962), test loss: 3.57196981013\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (19.5420970917,29.1101419935), test loss: 34.8519486427\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.66473054886,4.2856240026), test loss: 2.68426854014\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (20.3091201782,28.9987457907), test loss: 35.4446741104\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (5.31828308105,4.2622523845), test loss: 3.43478024602\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (13.9279317856,28.8902021303), test loss: 33.4006364822\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.78917264938,4.23928517725), test loss: 2.84934690595\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (15.201376915,28.7811556584), test loss: 37.4128182411\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.61709547043,4.21685014574), test loss: 3.45520488173\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (43.0856056213,28.6742702965), test loss: 31.0481133938\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.81206035614,4.19498908909), test loss: 2.86215256155\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (30.3306121826,28.5709759195), test loss: 36.1262079954\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.757813811302,4.1735334071), test loss: 3.40350922048\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (35.9156723022,28.4676894895), test loss: 33.251858902\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.98943042755,4.15251260424), test loss: 3.34851626158\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (7.47204589844,28.3657638679), test loss: 37.9637990713\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.742538928986,4.13178388104), test loss: 3.50429296494\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (7.09949636459,28.2680358149), test loss: 31.8109542608\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.646420061588,4.11131478976), test loss: 3.45052163005\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (5.59889030457,28.1726339965), test loss: 34.2610551596\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.276756078005,4.09126275482), test loss: 2.92206085026\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (39.3563308716,28.0762440767), test loss: 34.0738590717\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.35837221146,4.07155523698), test loss: 3.51199786961\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (13.3564090729,27.9817287997), test loss: 35.4670292377\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.16960835457,4.05239735522), test loss: 2.63118824959\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.8503446579,27.8908273023), test loss: 33.8296308041\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.04509663582,4.03357648727), test loss: 3.35214703977\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (10.7526283264,27.7991803077), test loss: 34.7046127796\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.445492476225,4.01504905313), test loss: 2.76896604896\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (24.2494754791,27.7095520553), test loss: 34.8068425179\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.29904031754,3.99680773421), test loss: 3.39418771267\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (17.2892951965,27.6228653179), test loss: 32.2804776192\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (4.59764003754,3.97873349015), test loss: 2.86720482707\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (27.0597782135,27.5381391998), test loss: 36.2510292053\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (3.66473722458,3.96103714095), test loss: 3.34203097522\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (41.9577636719,27.4516025013), test loss: 29.3674831867\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.85941004753,3.94352466061), test loss: 2.68676992059\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (7.61370849609,27.367950844), test loss: 37.3874034882\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.49971723557,3.92657654294), test loss: 3.43014269769\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (6.62597942352,27.2868868665), test loss: 31.1597737551\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.95652699471,3.90989306577), test loss: 3.34222967923\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (15.7555408478,27.2048082195), test loss: 38.9795538425\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.42372262478,3.89342418371), test loss: 3.51351535916\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (23.6275024414,27.1246577359), test loss: 32.6519754887\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.15400576591,3.87721838734), test loss: 3.38030988872\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.49141979218,27.0472043626), test loss: 35.884150219\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.78840184212,3.86105536987), test loss: 2.77258260846\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (6.27752971649,26.9708304759), test loss: 34.3312873363\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.03519630432,3.84529945046), test loss: 3.44676729143\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (17.1475601196,26.8930617414), test loss: 33.7363487244\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.28231549263,3.82969670092), test loss: 2.53274160028\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (28.59724617,26.8183750871), test loss: 35.4562517166\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (3.38905477524,3.81455901531), test loss: 3.38817428648\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (16.3667831421,26.7455063477), test loss: 33.143803072\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.56741023064,3.79965392549), test loss: 2.82032999694\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (12.7738981247,26.6713343242), test loss: 37.4325053692\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.85448598862,3.7849102233), test loss: 3.31852298379\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (6.52566814423,26.5989122128), test loss: 29.9772823095\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (4.49106407166,3.77038606778), test loss: 2.68553800285\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (22.4644432068,26.5292429394), test loss: 36.9510262966\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.25722718239,3.75588646329), test loss: 3.39976770431\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (23.0890426636,26.4602240628), test loss: 32.5530532837\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.60754704475,3.7417353846), test loss: 3.36112430692\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (6.5020532608,26.3896093951), test loss: 37.7201906681\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.99344009161,3.72769641057), test loss: 3.44719942808\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (24.3756141663,26.3216673851), test loss: 31.5249591351\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.10323238373,3.71406045372), test loss: 3.38818616271\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (7.32966852188,26.2554712612), test loss: 34.8134794712\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.08569312096,3.70060613809), test loss: 2.97424231172\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (60.6089019775,26.1884135908), test loss: 32.8133045435\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.77330541611,3.68733166472), test loss: 3.47564706206\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (11.6334161758,26.1228471515), test loss: 35.6850544691\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.69642353058,3.67418548168), test loss: 2.59892221391\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (14.3207826614,26.0590359904), test loss: 33.4823637962\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.37635111809,3.66115312966), test loss: 3.27830763459\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (33.5731163025,25.99607275), test loss: 34.940128231\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.42499375343,3.64834692536), test loss: 2.73403164744\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (24.1836338043,25.9319403993), test loss: 34.3156420708\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.26945161819,3.63565239708), test loss: 3.25528410673\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (9.34516143799,25.8697563408), test loss: 31.6321445465\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.43091058731,3.62329235073), test loss: 2.83638537526\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (25.6423339844,25.809642083), test loss: 36.1148591995\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.395246207714,3.6111249595), test loss: 3.32879892886\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (2.5895075798,25.7475157015), test loss: 27.4447123051\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.761762857437,3.59906282754), test loss: 2.60372495949\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (7.09986114502,25.688011152), test loss: 37.0515272856\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.13837242126,3.58711267137), test loss: 3.33843397498\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (8.947681427,25.6296536839), test loss: 31.3176463604\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.97776317596,3.57526506326), test loss: 3.28278635144\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (12.86444664,25.5716414182), test loss: 38.0508934021\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.46101200581,3.56362286467), test loss: 3.20663842559\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (16.1579933167,25.5126697692), test loss: 34.1274638653\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.9581515789,3.55206413149), test loss: 3.39250396788\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (13.1417331696,25.455687474), test loss: 35.5094620228\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.05874156952,3.54079013956), test loss: 2.70325580835\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (40.3163528442,25.3997466704), test loss: 34.413158226\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.30426168442,3.52970609947), test loss: 3.37740244269\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (37.636138916,25.3432593161), test loss: 33.5625175476\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.37057995796,3.51869121183), test loss: 2.49852704108\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (4.07985973358,25.2884188528), test loss: 35.4913855314\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.8133122921,3.50778813548), test loss: 3.41101535559\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (7.56636571884,25.2345783081), test loss: 32.0607450962\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.99449396133,3.49693382865), test loss: 2.75003561974\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (19.9623260498,25.18059849), test loss: 36.1942278147\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.54426002502,3.48629738652), test loss: 3.18150170445\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (17.5246925354,25.1263822212), test loss: 29.6738533497\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.1041508913,3.47572063168), test loss: 2.6414044708\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (9.440117836,25.0737667671), test loss: 36.6149414062\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.10831093788,3.4654040105), test loss: 3.3179924354\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (17.3883552551,25.021483575), test loss: 30.7744619131\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (3.21977972984,3.45522530444), test loss: 3.21486677229\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (16.3433017731,24.9693619959), test loss: 37.654014492\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.06406497955,3.44514192041), test loss: 3.39731904864\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (22.9537506104,24.9190565268), test loss: 31.0012639999\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.82560276985,3.43511265524), test loss: 3.31547625363\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (23.6012535095,24.8689683123), test loss: 35.6346683979\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.50292229652,3.42516437067), test loss: 2.88211452663\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (22.1521434784,24.8187151976), test loss: 32.6592597723\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.97612726688,3.4153966796), test loss: 3.4146012485\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (20.5034313202,24.7683313636), test loss: 36.1212620258\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.969475507736,3.40565821941), test loss: 2.62998780608\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (12.2321395874,24.7197332235), test loss: 33.3617444038\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.13723301888,3.39618226695), test loss: 3.23818288445\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (17.0289516449,24.670740653), test loss: 34.8134313583\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.6133055687,3.38678347816), test loss: 2.70493912995\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (18.4277496338,24.6223447109), test loss: 35.7005107403\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.91137945652,3.37748994321), test loss: 3.31107190251\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (14.0207061768,24.5756004512), test loss: 30.6931305647\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.01754403114,3.36826346755), test loss: 2.76590694636\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (14.5484790802,24.5290468903), test loss: 35.8170545578\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.21047604084,3.35908517438), test loss: 3.20061710775\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (21.331829071,24.4822223311), test loss: 27.5129529953\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (4.37647342682,3.35005505433), test loss: 2.77097538114\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (9.8946685791,24.4353289934), test loss: 37.0941523314\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.12366294861,3.34109272076), test loss: 3.31423462629\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (8.36339950562,24.3900523219), test loss: 30.7589164972\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (3.38337039948,3.33231598476), test loss: 3.27901388705\n",
      "\n",
      "MC # 3, Hype # hyp4, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold2/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (291.586273193,inf), test loss: 188.122465134\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (335.177215576,inf), test loss: 379.62592926\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (34.0415611267,95.4917539911), test loss: 45.385615921\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.00754165649,91.5466388432), test loss: 3.21052037477\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (31.8975715637,71.110817749), test loss: 36.6236439228\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.77723193169,47.4141481937), test loss: 3.26848666072\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (56.5350227356,62.9045293233), test loss: 44.8063633442\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.78435397148,32.6925042374), test loss: 3.38111560941\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (31.8765449524,58.6614110584), test loss: 41.8834314346\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.801908075809,25.3209586172), test loss: 3.37362520695\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (18.6583309174,56.1704303625), test loss: 39.2533543587\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.62447106838,20.9060906173), test loss: 2.87257491946\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (61.0905914307,54.4113598574), test loss: 45.9477837563\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.5918558836,17.9611105468), test loss: 3.48202259243\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (50.5085525513,53.1507237992), test loss: 42.1932324886\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.623287916183,15.8532507457), test loss: 3.00749541223\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (33.0916213989,52.1281366528), test loss: 45.5778455734\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.95418691635,14.2720222301), test loss: 3.50301145911\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (40.4542655945,51.3010504702), test loss: 37.503374958\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.33078086376,13.0385413957), test loss: 3.16191054285\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (30.1181163788,50.6605607927), test loss: 47.5198519707\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.87580513954,12.0514685265), test loss: 3.4392038703\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (41.0070343018,50.0815753848), test loss: 37.3787597895\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (5.1656293869,11.2408008457), test loss: 2.93567022979\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (88.3932495117,49.6303171502), test loss: 44.3139457226\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (7.14368867874,10.5664839799), test loss: 3.3986861825\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (70.8080215454,49.2009005748), test loss: 32.8487085819\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.17054724693,9.99483602651), test loss: 2.75634056926\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (41.9530143738,48.8106620287), test loss: 43.1434740543\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (10.1367492676,9.50319404981), test loss: 3.27022393346\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (89.229598999,48.4546705973), test loss: 39.5860807419\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.66483354568,9.07652472067), test loss: 3.37096576095\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (155.900482178,48.0943796558), test loss: 40.0494960785\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.21569728851,8.70168580166), test loss: 2.82873671651\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (41.9863128662,47.7571027033), test loss: 41.9206581116\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.09373950958,8.37040226082), test loss: 3.29837651849\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (44.2549972534,47.4467095416), test loss: 38.41282866\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.8749127388,8.07321020776), test loss: 2.80736650825\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (29.2464332581,47.165215342), test loss: 43.1212059021\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.41786670685,7.807184813), test loss: 3.525132972\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (51.1463699341,46.8760407914), test loss: 34.1293404579\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.85399961472,7.56706069765), test loss: 2.76488673687\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (50.5822525024,46.5973652817), test loss: 44.3815755367\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (5.2032957077,7.34917103438), test loss: 3.3843842566\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (24.4138450623,46.3221778519), test loss: 34.921527338\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.8061029911,7.14822923251), test loss: 3.00447874665\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (55.8254737854,46.0423156664), test loss: 42.0374476433\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (6.58080291748,6.96425198249), test loss: 2.9607196629\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (16.8642044067,45.7520635642), test loss: 30.3990351677\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.14404678345,6.79398766989), test loss: 2.70841932893\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (46.5900268555,45.4765347502), test loss: 39.2825752974\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.84640532732,6.63565508682), test loss: 3.13282394409\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (25.2162475586,45.2130553388), test loss: 35.1429691792\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.26542568207,6.48864629673), test loss: 3.16105873287\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (65.8531799316,44.9429119882), test loss: 39.2237471104\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.37745630741,6.35198789266), test loss: 3.13228573203\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (82.725402832,44.6713688255), test loss: 37.1550240993\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.92664933205,6.22457607726), test loss: 3.16009644568\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (33.9252624512,44.3911121211), test loss: 34.4679305077\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.412360221148,6.10331230703), test loss: 2.70679466128\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (31.18699646,44.1027764468), test loss: 39.3643880129\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.60675144196,5.98974606107), test loss: 3.31675652266\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (27.3399715424,43.792857994), test loss: 32.1353761673\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.61869394779,5.88209088106), test loss: 2.66028125882\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (32.2961387634,43.4812338039), test loss: 40.8966370583\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.45726633072,5.78002237151), test loss: 3.27212350965\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (12.8007631302,43.1723228759), test loss: 30.5032824755\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.54763841629,5.68266916535), test loss: 2.69542199373\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (22.2017631531,42.8589502475), test loss: 38.477785635\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.88325190544,5.59055814794), test loss: 3.20478117168\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (15.6619968414,42.5339484051), test loss: 26.3647032022\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.684109985828,5.50309060382), test loss: 2.45901578367\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (25.3953266144,42.2065161459), test loss: 34.3638413429\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.03374838829,5.41898098109), test loss: 3.02040401474\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (23.4688148499,41.8672207685), test loss: 30.5511586666\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.41040033102,5.33871961901), test loss: 3.17924206257\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (27.0493583679,41.5208910959), test loss: 33.8889965773\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.51685285568,5.26182165484), test loss: 3.04602076411\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (40.5790023804,41.1829598942), test loss: 32.1747229576\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.60035526752,5.18818028052), test loss: 3.20857146084\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (18.0973472595,40.8345665579), test loss: 29.7775382757\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.37906002998,5.11695473181), test loss: 2.60348264128\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (24.0817527771,40.4923691222), test loss: 33.818951869\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.763780236244,5.04935776888), test loss: 3.34564070553\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (15.7808122635,40.1434796054), test loss: 29.260281682\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.781831622124,4.98440155556), test loss: 2.53726349622\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (14.6388864517,39.7991394233), test loss: 35.155796957\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.930634140968,4.92193672812), test loss: 3.17786127925\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (67.8212814331,39.453824368), test loss: 27.4243836403\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.43585062027,4.8620716844), test loss: 2.75088710189\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (62.0631942749,39.1068168101), test loss: 34.6881045818\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.5699801445,4.80423069279), test loss: 3.14437653422\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (8.6053352356,38.7681265928), test loss: 23.9423318386\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.08682107925,4.74850340359), test loss: 2.59075947404\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (11.7569255829,38.4338137853), test loss: 32.0074307442\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.69593477249,4.69455471211), test loss: 3.20319020748\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (23.75207901,38.1086202077), test loss: 28.6577855587\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.00504469872,4.64280837209), test loss: 3.19879880846\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (20.9045963287,37.7841187565), test loss: 32.0941291094\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.432443737984,4.59305355123), test loss: 3.24465464056\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (16.3415336609,37.4688885497), test loss: 30.1997735023\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.0146021843,4.54538046162), test loss: 3.27781885564\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (28.8832092285,37.164519197), test loss: 28.4990963936\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.17658233643,4.49933109436), test loss: 2.62819152474\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (30.3037147522,36.8610205057), test loss: 32.5410239458\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.65727186203,4.45464180083), test loss: 3.39454296231\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (7.281894207,36.5658181184), test loss: 29.5064631939\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.74079155922,4.41167301947), test loss: 2.59886749983\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (9.12408542633,36.2824182432), test loss: 32.7764939785\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.87214016914,4.36964619385), test loss: 3.19988006949\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (18.9246520996,36.006004559), test loss: 28.8837347031\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.66255199909,4.3292223444), test loss: 2.6963311553\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (16.8671379089,35.7332274181), test loss: 33.9172799706\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.8029794693,4.29032930829), test loss: 3.16513017714\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (9.74310684204,35.4718932114), test loss: 25.5516880512\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.55148839951,4.25299739734), test loss: 2.65462457538\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (17.4955501556,35.2197536748), test loss: 32.8203611374\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.43904495239,4.21670247438), test loss: 3.18056791425\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (13.0096673965,34.9699046259), test loss: 29.757084322\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.574013710022,4.18144225401), test loss: 3.22628009766\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (7.31985378265,34.7272072225), test loss: 32.9019206047\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.25728702545,4.14726443924), test loss: 3.27396453917\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (8.04738903046,34.4941756944), test loss: 30.5007553816\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.958949506283,4.11394667421), test loss: 3.19595642984\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (16.3567276001,34.2682699987), test loss: 28.3515132785\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.44611930847,4.08157373297), test loss: 2.62595200837\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (7.49309158325,34.0460637103), test loss: 32.6740503311\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.39826011658,4.05041273986), test loss: 3.34339240789\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (13.7434415817,33.830543562), test loss: 29.9916561842\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.39409160614,4.02027041835), test loss: 2.54702738225\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (10.5982446671,33.6232701815), test loss: 32.9096868634\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.348328649998,3.99085456512), test loss: 3.16296086907\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (33.5876579285,33.4183619577), test loss: 29.7796182632\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.89669442177,3.9623489563), test loss: 2.7079741776\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (30.0564117432,33.2185068216), test loss: 33.2893105626\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.13512825966,3.93450652321), test loss: 3.08127216995\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (6.30041885376,33.026356866), test loss: 26.8482484818\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.62125635147,3.90750927292), test loss: 2.72941333503\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (23.1796112061,32.8400407472), test loss: 32.8709365964\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.03634405136,3.88084810658), test loss: 3.15026018023\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (21.265958786,32.6566779929), test loss: 30.0998375893\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.44545102119,3.85519819787), test loss: 3.26872935593\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.3507108688,32.4772408909), test loss: 33.4076711655\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.67503237724,3.83026705975), test loss: 3.30112151504\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (18.8402481079,32.3046402997), test loss: 29.62870754\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.54701375961,3.80602950266), test loss: 3.13116178513\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (22.7083320618,32.1340017688), test loss: 27.8441907883\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.30790090561,3.78230970227), test loss: 2.68702004254\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (34.4732933044,31.9674457594), test loss: 33.3489044547\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.75737094879,3.75920929545), test loss: 3.33670967221\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (25.3977355957,31.8079682781), test loss: 29.7721110821\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.92220687866,3.73661031715), test loss: 2.48128907084\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (52.6456336975,31.6506767487), test loss: 33.8579857945\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (4.95132541656,3.71435582147), test loss: 3.17817423344\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (47.7786407471,31.4969868114), test loss: 29.020138073\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.10931253433,3.6928051167), test loss: 2.69056838155\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (16.7794570923,31.3450499264), test loss: 34.5076723099\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.01691460609,3.67173515729), test loss: 3.17854290605\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (10.5030632019,31.1983889938), test loss: 27.5947529316\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.36674165726,3.65128649249), test loss: 2.71371587515\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (50.6340332031,31.0568792813), test loss: 32.7372665882\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.7940890789,3.63138312723), test loss: 3.12620815933\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (5.08971500397,30.913700123), test loss: 25.2206792355\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.32569158077,3.61172608043), test loss: 2.75251509845\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (21.2934684753,30.7767423419), test loss: 33.9476173162\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (5.46311903,3.59260308058), test loss: 3.23123621643\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (33.2926063538,30.6432934324), test loss: 29.8871906996\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.60510706902,3.57360900348), test loss: 3.06095300317\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (20.2966480255,30.5119576304), test loss: 28.3923753738\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.35574662685,3.55514475166), test loss: 2.62484008074\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (10.6335048676,30.3810443475), test loss: 32.1095512152\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.75448465347,3.53709204438), test loss: 3.22442846894\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (23.614030838,30.2553002449), test loss: 30.0781061649\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.60950636864,3.51964725917), test loss: 2.62075468153\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (11.3209095001,30.1335252908), test loss: 34.5824499846\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.773725271225,3.5025034426), test loss: 3.21552639306\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.2217311859,30.0106996949), test loss: 29.03588202\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.48091101646,3.4856072717), test loss: 2.68730671853\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (21.6720371246,29.8915562036), test loss: 35.6264705181\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.622603237629,3.46909344321), test loss: 3.25458982289\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (11.3423976898,29.7763290099), test loss: 28.3018542767\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.30423545837,3.45272684765), test loss: 2.66992055923\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (37.3183746338,29.6629307421), test loss: 32.8146895885\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (4.16505527496,3.43676526298), test loss: 3.0702024132\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (35.1329536438,29.5490683515), test loss: 25.0835550308\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.05382323265,3.42110437337), test loss: 2.53825275302\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (16.1302757263,29.438932235), test loss: 33.4926740408\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.76387870312,3.40597165448), test loss: 3.21714348495\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (25.1443405151,29.3329637488), test loss: 29.8440010071\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.52380371094,3.39107652817), test loss: 3.1423599124\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (21.7058677673,29.2256577523), test loss: 34.4474503994\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.983451604843,3.37638118332), test loss: 3.08668538332\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (19.392168045,29.1208242757), test loss: 30.6093575001\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.43726587296,3.36198224318), test loss: 3.15066094697\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (22.487165451,29.0197006293), test loss: 31.2076553345\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (5.38605833054,3.34777957281), test loss: 2.60369058847\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (23.3400230408,28.9205412538), test loss: 33.3148356915\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (5.97476291656,3.33379899774), test loss: 3.20982000828\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (26.644903183,28.8213589383), test loss: 29.3614520073\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.05111169815,3.32007508888), test loss: 2.49497398734\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (13.7854032516,28.7238951671), test loss: 35.473420763\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.15875720978,3.30673712556), test loss: 3.24473650455\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (14.4054946899,28.6300091785), test loss: 28.0466191292\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.23016786575,3.29359476028), test loss: 2.69379742444\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (7.0988368988,28.5353499273), test loss: 33.5902972698\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (3.168612957,3.28074975288), test loss: 3.06495854855\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (18.6401023865,28.4427987775), test loss: 25.0192139626\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.32279515266,3.26801487921), test loss: 2.52563190609\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (59.5273399353,28.3535630637), test loss: 33.2156818271\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.36103129387,3.25560343423), test loss: 3.14327095747\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (15.9767007828,28.2645478354), test loss: 30.3264415979\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.772205650806,3.24314107193), test loss: 3.11322785318\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (30.3950042725,28.1768422302), test loss: 34.1960240602\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (3.29148745537,3.23112629769), test loss: 3.30142788887\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (16.0692901611,28.0896251034), test loss: 31.2378862739\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.927075028419,3.21921632019), test loss: 3.1646460861\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (16.6019992828,28.00558362), test loss: 31.1382920742\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.796889364719,3.20761327376), test loss: 2.66628163904\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (28.0863304138,27.9217893036), test loss: 31.8622313023\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (4.22309207916,3.19619579043), test loss: 3.19603453577\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (8.18775081635,27.8388362928), test loss: 31.2055724621\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.561688303947,3.1849086272), test loss: 2.56612279862\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (3.36621904373,27.7588695595), test loss: 35.0393742323\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.75250339508,3.1738000352), test loss: 3.19675434828\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (18.4145698547,27.6797613436), test loss: 29.1724179506\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.77924990654,3.16278050344), test loss: 2.64570956528\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (21.3196868896,27.6011723303), test loss: 34.4035917401\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.98855686188,3.15198027305), test loss: 3.16735889316\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (16.0826892853,27.5223575109), test loss: 25.7823266029\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.84966897964,3.14134794546), test loss: 2.47073367983\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (17.4807395935,27.4461723484), test loss: 33.1682359695\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.72383630276,3.13097489414), test loss: 3.10920533836\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (7.82682323456,27.3720706696), test loss: 30.7904381752\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.303218603134,3.12079708634), test loss: 3.17321045995\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (25.7866191864,27.2965352113), test loss: 34.0779451132\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.34254312515,3.11069546861), test loss: 3.16606350094\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (10.3684101105,27.2234827145), test loss: 31.1507820845\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.33442699909,3.10078366186), test loss: 3.18237356544\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (27.2041873932,27.1524955001), test loss: 31.2198206902\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.82549190521,3.09086024221), test loss: 2.61161051095\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (14.0579185486,27.0813915669), test loss: 32.772802949\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.20312821865,3.08111821897), test loss: 3.24928210378\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (9.42850494385,27.0098287839), test loss: 31.4019097328\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.970913171768,3.07153612546), test loss: 2.53346585035\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (10.4093055725,26.9410922665), test loss: 33.929793787\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.27185153961,3.06226358702), test loss: 3.13267940283\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (10.935587883,26.8739956122), test loss: 30.2451849461\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.8190112114,3.05307007866), test loss: 2.68830373883\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (17.0135326385,26.8053636339), test loss: 34.5385370493\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.10338163376,3.04395434776), test loss: 3.0677393198\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.8689365387,26.7385241542), test loss: 26.1729796886\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.959407091141,3.03496904354), test loss: 2.55397840738\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (7.13502407074,26.6735686897), test loss: 33.5978995323\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.7797652483,3.0260341513), test loss: 3.07878834009\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (4.42704391479,26.6092256866), test loss: 30.1241203785\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.607204079628,3.01724062976), test loss: 3.08499833345\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (45.0961685181,26.5446895823), test loss: 34.0008515835\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.47403025627,3.00860924018), test loss: 3.21430765986\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (40.6215896606,26.4810325729), test loss: 30.41401999\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.53109121323,3.00017329911), test loss: 3.18258518577\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (19.9719619751,26.4196147241), test loss: 30.0063190222\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.915140330791,2.99181045846), test loss: 2.66277505457\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (10.7795639038,26.3568743027), test loss: 32.5807581902\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.12025594711,2.98356222788), test loss: 3.28967872262\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (7.2453751564,26.2956195007), test loss: 30.5609601498\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.0999161005,2.97540239376), test loss: 2.44347286522\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (18.9849319458,26.2362837104), test loss: 32.6365679145\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (6.13573741913,2.96738887401), test loss: 3.07093744278\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.2009768486,26.1774092379), test loss: 30.2655601501\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.05369710922,2.95932712433), test loss: 2.57876935601\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (10.3717374802,26.1182999625), test loss: 33.3286022186\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.96912670135,2.95145761194), test loss: 2.98261241913\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (27.1502628326,26.0597022699), test loss: 26.380745697\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.97124409676,2.94374311309), test loss: 2.58782823831\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (8.48445510864,26.0028784761), test loss: 33.3810261011\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.81992661953,2.93610290547), test loss: 3.08923950493\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (21.6509170532,25.9454101084), test loss: 30.0370178223\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.465970724821,2.92861120293), test loss: 3.09442879111\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (7.36692142487,25.8888919655), test loss: 33.7819792509\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.89558291435,2.92117394371), test loss: 3.15692081153\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (15.6213283539,25.8344171037), test loss: 29.5575931787\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.809199631214,2.91384830883), test loss: 2.88992103636\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (11.5021829605,25.7795739969), test loss: 29.3077098966\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.20058679581,2.90648793657), test loss: 2.57318693101\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (13.9228649139,25.7253028821), test loss: 32.4014021158\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.10200166702,2.89933772866), test loss: 3.19190811813\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (21.4357681274,25.6710198424), test loss: 30.4403332233\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.23456442356,2.89221924678), test loss: 2.36265170276\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (6.95230674744,25.6183999844), test loss: 33.1929922342\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.04517900944,2.88524329649), test loss: 3.0579403162\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (27.6515312195,25.5659689155), test loss: 30.3096292019\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.68300104141,2.87838769457), test loss: 2.60515434444\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (31.2216663361,25.5136723651), test loss: 33.833826685\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.12449741364,2.8715764249), test loss: 3.09515905231\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (8.96514892578,25.4625572692), test loss: 27.3872491837\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.01420044899,2.86483056552), test loss: 2.67019836307\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (8.25233078003,25.4121172428), test loss: 32.6980155587\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.76131510735,2.8581161596), test loss: 2.91338768601\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (20.6514816284,25.3619413054), test loss: 27.8257256031\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.594642877579,2.85150243377), test loss: 2.99702756405\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (35.4657974243,25.3111948133), test loss: 34.5131141186\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.44904053211,2.84497006653), test loss: 3.14063674808\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (18.9034881592,25.2617713893), test loss: 29.3960243821\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.86696457863,2.83859157386), test loss: 2.91796683669\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (31.0003662109,25.2139138107), test loss: 29.1357150555\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.58250522614,2.83228539159), test loss: 2.66964439154\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (10.320766449,25.1646211), test loss: 31.4186640739\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.449362963438,2.82597259346), test loss: 3.17617357075\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (8.15568447113,25.1167324917), test loss: 30.5530439377\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (3.08680725098,2.81981517712), test loss: 2.60871309638\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.16356611252,25.0702210238), test loss: 34.3937069893\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.587315440178,2.81356569246), test loss: 3.10341261625\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (17.1895160675,25.0234718064), test loss: 29.7365659714\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.14773821831,2.80745736841), test loss: 2.61410174966\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (9.71473026276,24.9757939144), test loss: 35.6429316044\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.79295313358,2.80141154368), test loss: 3.14132383764\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (8.17625999451,24.9298834917), test loss: 27.989133215\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.61499595642,2.79554267685), test loss: 2.63678729236\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (15.300034523,24.8851616889), test loss: 32.676856184\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.591694056988,2.78970182751), test loss: 2.97544118166\n",
      "\n",
      "MC # 3, Hype # hyp4, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold3/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (306.968475342,inf), test loss: 171.58586235\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (290.084991455,inf), test loss: 350.777362061\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (31.4130058289,86.0982727213), test loss: 47.4782164574\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.05587530136,93.6612003096), test loss: 3.28842879534\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (30.95753479,65.802764008), test loss: 36.9540416241\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.61367809772,48.5279848169), test loss: 2.88412533998\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (51.722038269,59.0238931077), test loss: 45.8005016327\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.14731383324,33.4663560208), test loss: 3.37111656666\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.958026886,55.6510735128), test loss: 40.9201117039\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.74602913857,25.9333415692), test loss: 3.27090224624\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (45.3397293091,53.5357995611), test loss: 46.2238887787\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.43362832069,21.4166581286), test loss: 3.33402273059\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (27.3388385773,52.1156996439), test loss: 44.5098362446\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.98314809799,18.4046525244), test loss: 3.41634586453\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (27.5280418396,51.0602815804), test loss: 43.061578846\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.5298602581,16.2489105961), test loss: 2.92950653434\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (32.8666305542,50.1778010789), test loss: 47.2552658081\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.998679876328,14.630624156), test loss: 3.62494564056\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (26.9301071167,49.5146408926), test loss: 43.1346797228\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.62024354935,13.3710616332), test loss: 2.92743532062\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (29.8355464935,48.938659876), test loss: 45.3818299294\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.281940341,12.358852854), test loss: 3.68065869212\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (32.5750350952,48.4968196755), test loss: 38.797074604\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.81140136719,11.5339402316), test loss: 2.88557308316\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (18.9893913269,48.0804839209), test loss: 46.5491823196\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.08002543449,10.8435976072), test loss: 3.39455218315\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (37.602432251,47.7097665385), test loss: 38.6809774876\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.99182772636,10.2568834743), test loss: 3.21476404071\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (40.8948287964,47.3631534948), test loss: 45.7369672298\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.90394985676,9.75496524581), test loss: 3.1365855515\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (52.4500617981,47.02720254), test loss: 34.9934420586\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.69067907333,9.3193906292), test loss: 2.76280273199\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (13.0507888794,46.7323223264), test loss: 43.387959671\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.60850763321,8.93453506963), test loss: 3.25625835657\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (40.3678092957,46.4759244863), test loss: 38.3146373034\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.3189432621,8.59377373015), test loss: 3.14232158661\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (14.9476089478,46.2102118516), test loss: 43.5491696835\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.83167600632,8.29122407142), test loss: 3.18339205682\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (19.6668205261,45.9673293617), test loss: 41.8773406982\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (7.4122543335,8.01923858164), test loss: 3.2782825768\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (81.6539993286,45.7264759549), test loss: 39.3521852493\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.45160388947,7.77326290168), test loss: 2.71975573301\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (106.652145386,45.4654660603), test loss: 43.5543016434\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (6.78932285309,7.54933356893), test loss: 3.4040160805\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (37.7285995483,45.2302567493), test loss: 40.1122871876\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.67952823639,7.34437889131), test loss: 2.76307590604\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (17.4843177795,44.9865977471), test loss: 43.0637251854\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.903901577,7.15501346382), test loss: 3.49097337723\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (58.4567604065,44.7653189251), test loss: 36.6924514294\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (9.16478729248,6.98174538376), test loss: 2.74282949567\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (21.7438163757,44.5324843279), test loss: 43.1627565384\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.80843484402,6.82076645341), test loss: 3.19680939317\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (32.8369903564,44.2966334925), test loss: 35.2421050549\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.586090147495,6.67031775607), test loss: 2.91971563697\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (44.8296318054,44.0530877519), test loss: 41.8249796867\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.67546153069,6.53067938496), test loss: 2.96359024644\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (37.5008087158,43.7984484503), test loss: 31.523619175\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.75928878784,6.39999257642), test loss: 2.5511530757\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (16.1649436951,43.5443596129), test loss: 38.947023654\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.4962220192,6.27559251468), test loss: 3.02942807674\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (16.3558826447,43.271208263), test loss: 33.6141599655\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.88286024332,6.15841761428), test loss: 2.75700508356\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (33.0242729187,42.9826862211), test loss: 37.5010809422\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (0.939345777035,6.04797043144), test loss: 2.96057991982\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (15.4662008286,42.6908262371), test loss: 35.9076980829\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.65963447094,5.94264768284), test loss: 2.99157333672\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (52.0933609009,42.3844683494), test loss: 32.9275337934\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.2288980484,5.84202736291), test loss: 2.6298202455\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (26.781047821,42.0548015572), test loss: 37.7144866943\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.66908693314,5.74517297532), test loss: 3.08625917435\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (29.9971103668,41.7337719284), test loss: 32.6835017204\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.2858850956,5.65268369591), test loss: 2.4194132477\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (28.7988681793,41.3955697962), test loss: 36.2875645161\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.92055058479,5.56359931708), test loss: 3.03097460568\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (54.8302993774,41.0566289421), test loss: 29.2088905811\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.08549928665,5.47893204724), test loss: 2.47188650072\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (26.9684581757,40.7079001562), test loss: 36.3044223785\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.94609773159,5.39828272062), test loss: 2.96227690279\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (28.1940364838,40.3529484507), test loss: 28.0489889622\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.86014246941,5.32080247299), test loss: 2.66079184115\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (16.5894508362,39.9891597525), test loss: 33.9929553509\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.99960684776,5.2466989266), test loss: 2.76993497014\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (23.7080478668,39.6194554988), test loss: 25.1617649078\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.44591569901,5.1757846774), test loss: 2.33033982068\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (36.3563690186,39.2583529521), test loss: 31.3789738178\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.77178907394,5.10675979433), test loss: 2.81351178288\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (27.143693924,38.8969605864), test loss: 28.5964118004\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.66982209682,5.04066569583), test loss: 2.71229227483\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (10.3562602997,38.5320021467), test loss: 29.7623657942\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.2034047842,4.97734445214), test loss: 2.87382020056\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (14.9370365143,38.1769264813), test loss: 29.5253031969\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.14923715591,4.91676472689), test loss: 2.89499427527\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (60.1045036316,37.8249517793), test loss: 26.1928462029\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.51438522339,4.85854962666), test loss: 2.57577426434\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (32.5515556335,37.4710621667), test loss: 33.081261301\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.815614283085,4.80239682084), test loss: 2.9862796694\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (26.9076881409,37.1330392621), test loss: 27.3523778439\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.14766347408,4.74826963771), test loss: 2.3946031332\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.1510362625,36.7991780846), test loss: 32.9303938866\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.6189109087,4.69576710694), test loss: 2.97235475183\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (28.3147792816,36.4739014583), test loss: 25.9064507484\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.59334945679,4.64543847656), test loss: 2.44189949632\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (45.2859725952,36.1564574273), test loss: 33.4161121368\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.78199219704,4.59737111628), test loss: 2.98045287132\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (34.3653488159,35.8491160353), test loss: 26.8690472126\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.07250070572,4.55083749539), test loss: 2.64445130825\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (6.19621372223,35.5462073756), test loss: 32.1309402466\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.746831119061,4.50592561856), test loss: 2.81047651172\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (6.85506916046,35.2505461037), test loss: 24.9701759815\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.873958468437,4.46274712937), test loss: 2.45136736631\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.1506233215,34.9686768529), test loss: 30.259844327\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.54043245316,4.42033777752), test loss: 2.84643572867\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (24.8408317566,34.6924686677), test loss: 29.0603210449\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.45874547958,4.37951580893), test loss: 2.73774466068\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.1534996033,34.4206380944), test loss: 30.0495282888\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.63026618958,4.34012075685), test loss: 2.92513503432\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (9.33727931976,34.1604714991), test loss: 29.1862896442\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.02642118931,4.30227828732), test loss: 2.800853163\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (9.62807941437,33.9072695719), test loss: 26.464471817\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.32184267044,4.26566640962), test loss: 2.76007603109\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.1383666992,33.6580046127), test loss: 32.478774929\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.15908265114,4.23004287573), test loss: 2.98772628307\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (43.3520431519,33.4209994082), test loss: 28.2978048086\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.97231209278,4.1954598248), test loss: 2.60113916397\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (15.1888971329,33.188930019), test loss: 32.9310056686\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.09265375137,4.16156302425), test loss: 2.98715999722\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (11.4934682846,32.9621065554), test loss: 26.9607975006\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.68878364563,4.12881568777), test loss: 2.43306491971\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (17.7506046295,32.7413549149), test loss: 33.4588866711\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.1048874855,4.09733450986), test loss: 2.86678954363\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (15.1853380203,32.5294498575), test loss: 27.9704041958\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.60867238045,4.06678285818), test loss: 2.64180506766\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (16.2818450928,32.3205962575), test loss: 32.3520773649\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.875595867634,4.03698240927), test loss: 2.79582703412\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (11.4426259995,32.1169705165), test loss: 26.1807191849\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.62014102936,4.00818196314), test loss: 2.558857885\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (36.5235786438,31.9219413443), test loss: 30.6341918468\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.67178821564,3.979567372), test loss: 2.80859518051\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (21.1489944458,31.729514651), test loss: 29.8692176819\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.57965159416,3.95193878131), test loss: 2.76367206126\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (16.911655426,31.5402368808), test loss: 30.4227679253\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.736139178276,3.92502687224), test loss: 2.92646821141\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (17.049495697,31.35887513), test loss: 29.6173571587\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.96047782898,3.89915547988), test loss: 2.82820992023\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (33.0367736816,31.1820651451), test loss: 27.0924325228\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.59135270119,3.87388197087), test loss: 2.73628076017\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (20.5994567871,31.0064934324), test loss: 31.2496862888\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.776639819145,3.84919490618), test loss: 2.91125118136\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (14.7043399811,30.8384061904), test loss: 28.7206618786\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.14630794525,3.82502160666), test loss: 2.5940543443\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.0572242737,30.6738210455), test loss: 31.6719150066\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (6.26301860809,3.80115490987), test loss: 2.99344560802\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (56.1401901245,30.5111426323), test loss: 27.6191712856\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.0617389679,3.77792937159), test loss: 2.45261513889\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.7647266388,30.3518896648), test loss: 32.7499238968\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.60774457455,3.75562053468), test loss: 2.84923790097\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (14.0665454865,30.1987111369), test loss: 28.8855604649\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.33797168732,3.73380475469), test loss: 2.60302720368\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (16.4407157898,30.0465151198), test loss: 31.7924875259\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.50891375542,3.71239741513), test loss: 2.80095971823\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (9.57789421082,29.8974807289), test loss: 25.3490839958\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.02444529533,3.69150900976), test loss: 2.59306084663\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (5.21535015106,29.7539682241), test loss: 30.2454069614\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.04447889328,3.67075387774), test loss: 2.66518368721\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (12.9728908539,29.6125339466), test loss: 29.3931083679\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.32220077515,3.650590658), test loss: 2.76415221393\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (9.44105434418,29.4720271349), test loss: 31.401953721\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.39065551758,3.63089441992), test loss: 2.89299642146\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (17.1017570496,29.3376473359), test loss: 29.1794434071\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (3.88957977295,3.61185780943), test loss: 2.87959721088\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (21.2528419495,29.2049720063), test loss: 26.8349232674\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.09860420227,3.59314845159), test loss: 2.6827908501\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (27.1990413666,29.0737811723), test loss: 30.5889136314\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.30373668671,3.57489650211), test loss: 2.82074234486\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (11.8211784363,28.9475591889), test loss: 29.6777250767\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (4.56561183929,3.55684159702), test loss: 2.57292626202\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (9.91909313202,28.823528801), test loss: 31.9061845303\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.60775279999,3.53894591399), test loss: 3.00549249649\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (27.7571525574,28.699628356), test loss: 29.0641581059\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.51889824867,3.52159203998), test loss: 2.53101497293\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.05487632751,28.5797182859), test loss: 31.7840567589\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.20238518715,3.50482250746), test loss: 2.85167882442\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (5.0667924881,28.4631431003), test loss: 29.4921665192\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.845137417316,3.48837341777), test loss: 2.57325511426\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (22.8955574036,28.346841435), test loss: 31.4984558105\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.12618923187,3.47215654494), test loss: 2.79718925953\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (20.6792526245,28.2331649697), test loss: 26.9811566353\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (5.61799621582,3.45630282071), test loss: 2.65481047034\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (51.311416626,28.1235300004), test loss: 30.0890682936\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (4.36389303207,3.44046594337), test loss: 2.70945809186\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (47.0200996399,28.0143350291), test loss: 25.0599180222\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.65682935715,3.42504858288), test loss: 2.48678514957\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (18.6723575592,27.9057551978), test loss: 30.1486947536\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.01666355133,3.40991840165), test loss: 2.80294697583\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (5.31272268295,27.8014001556), test loss: 28.9818948746\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.57027173042,3.39519207065), test loss: 2.86564829946\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (18.4627075195,27.6979982632), test loss: 30.8803286552\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.499102711678,3.38078817612), test loss: 2.87063677013\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (14.3192014694,27.5953641472), test loss: 30.0459295988\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.19895243645,3.36665664778), test loss: 2.83992360234\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (10.7836551666,27.4969801436), test loss: 30.3855632782\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.11378860474,3.3525308548), test loss: 2.52684987485\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (6.79827404022,27.3998231801), test loss: 31.4283313751\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.524898767471,3.33865757957), test loss: 2.99011523128\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (32.3026161194,27.3014437082), test loss: 30.3303162098\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.86731243134,3.32506136639), test loss: 2.53374523818\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (22.9729576111,27.207000787), test loss: 30.6704732418\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.84335541725,3.31192137749), test loss: 2.80610353351\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (15.281370163,27.1149968378), test loss: 29.9653595924\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.47475552559,3.29903047542), test loss: 2.56805163473\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (48.1935348511,27.0225054849), test loss: 31.1606412649\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.55991852283,3.28624197336), test loss: 2.75072401464\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (8.96140098572,26.9322073673), test loss: 27.1464858055\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.26935243607,3.27365736434), test loss: 2.57796571255\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (7.66495609283,26.844173286), test loss: 29.7495902061\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.20730185509,3.26111146171), test loss: 2.68902366757\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (9.60087776184,26.7566922392), test loss: 25.5801596642\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.24832320213,3.24887639911), test loss: 2.49167463481\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (11.4588699341,26.6698897022), test loss: 30.2620345592\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.33516597748,3.23682446602), test loss: 2.79948407561\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (17.2169113159,26.5860493651), test loss: 29.1558260441\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.26457095146,3.22508320735), test loss: 2.89795858264\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (12.7754993439,26.5023866279), test loss: 30.3693345547\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.94325256348,3.21358289701), test loss: 2.84578787684\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (20.2065601349,26.4194238355), test loss: 30.0454838276\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.1140961647,3.20223715426), test loss: 2.89407257736\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (7.32996749878,26.3396182813), test loss: 29.5763679028\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.46280705929,3.19086792398), test loss: 2.51383746266\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (24.3547935486,26.2605471054), test loss: 31.4524142265\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.94986104965,3.17969157026), test loss: 2.98656418324\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (16.0321159363,26.1799946644), test loss: 29.9727933407\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.916637957096,3.16866493903), test loss: 2.42910598814\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (25.6767539978,26.103230936), test loss: 31.2582351208\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.727394223213,3.15804809279), test loss: 2.79706265032\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (30.3197116852,26.0282914565), test loss: 29.6188049316\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (4.27591943741,3.14763156644), test loss: 2.55037303269\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (4.55135536194,25.9517981918), test loss: 31.6378652573\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.866506099701,3.13722592816), test loss: 2.83063479066\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (4.26774501801,25.8780945776), test loss: 27.5579341412\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.10220694542,3.12694653414), test loss: 2.54317395389\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (16.4560585022,25.805551477), test loss: 30.1823828697\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.12970638275,3.11669239762), test loss: 2.6615998745\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (19.9953041077,25.7331798162), test loss: 25.5057085991\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.54454421997,3.10672192142), test loss: 2.46974001229\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (23.8335819244,25.6613666554), test loss: 29.9592461109\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.65248370171,3.09684108647), test loss: 2.76130779088\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (13.8553199768,25.5916832974), test loss: 29.0925562859\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.790220975876,3.08717931208), test loss: 2.8853092283\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (31.2444820404,25.5220915099), test loss: 30.3123088837\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.49940764904,3.07772454896), test loss: 2.83135757148\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (6.59031867981,25.4527760039), test loss: 29.8953022957\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.948917925358,3.06840789166), test loss: 2.84019133449\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (12.1436271667,25.3864967394), test loss: 28.5817848206\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (3.61678123474,3.05901037747), test loss: 2.51083272398\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (17.7932415009,25.3203691136), test loss: 31.7245433092\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.15614581108,3.04975505809), test loss: 2.94465489686\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (10.1232757568,25.2530740283), test loss: 29.7973876953\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.53308343887,3.04065523056), test loss: 2.4355289638\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (14.1976928711,25.1884257148), test loss: 31.45826087\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.08737897873,3.03184380584), test loss: 2.82587872744\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (10.8061265945,25.1254059863), test loss: 29.0799972534\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.478779554367,3.02313703678), test loss: 2.48012361526\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (34.6050300598,25.0612670636), test loss: 32.3838770151\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.89219141006,3.0145296348), test loss: 2.86191224456\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (9.14653968811,24.9991074796), test loss: 27.9335650921\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.49942183495,3.00593233605), test loss: 2.5226118356\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (26.5165901184,24.9377603545), test loss: 31.0138449192\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (3.00094938278,2.99738138071), test loss: 2.72870194614\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (14.8931360245,24.8762742237), test loss: 25.397690773\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (4.79168367386,2.98901567813), test loss: 2.44241007864\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (15.7804336548,24.8152124826), test loss: 29.4646433592\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.47255051136,2.98074844515), test loss: 2.72593256235\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (4.94929552078,24.7560697935), test loss: 28.4814046144\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.325685381889,2.97267236129), test loss: 2.64016447663\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (9.68462371826,24.6965985413), test loss: 30.0607357025\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.20335769653,2.96472467544), test loss: 2.81674560606\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (21.356426239,24.6381556565), test loss: 29.5290868759\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.15631771088,2.95690952085), test loss: 2.81915644705\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (5.5965924263,24.5816902083), test loss: 28.9473117352\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.48482191563,2.94895840839), test loss: 2.63570503891\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (16.0611152649,24.5250522105), test loss: 31.522020483\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.42210793495,2.94117459306), test loss: 2.87823964059\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (8.95546245575,24.4674142921), test loss: 29.5315508842\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.05951249599,2.93346936259), test loss: 2.38028182685\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (10.474155426,24.4118118147), test loss: 31.5418745041\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.58532756567,2.9259886386), test loss: 2.82886956036\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (10.2262744904,24.3576920663), test loss: 28.7903327465\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.92538189888,2.91866169635), test loss: 2.50071642697\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (15.6525630951,24.3022532718), test loss: 32.7908831596\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.65095186234,2.9113189875), test loss: 2.89566293359\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (11.0481491089,24.24914892), test loss: 27.9352755547\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.64106082916,2.90402822958), test loss: 2.55381303728\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (12.8173465729,24.1958713855), test loss: 31.2005188942\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.731464743614,2.89674347737), test loss: 2.67107459009\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (20.624912262,24.1427931511), test loss: 25.5946142673\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.49401760101,2.8895928853), test loss: 2.40107697248\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (9.92754554749,24.0899524603), test loss: 29.2995364666\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.485680222511,2.88258954234), test loss: 2.69573336542\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (15.7079620361,24.0390767365), test loss: 28.6084123611\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.02079951763,2.87572578713), test loss: 2.67135069072\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (20.1032371521,23.9875771952), test loss: 29.6798071861\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.36330854893,2.86892817771), test loss: 2.83036335707\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (19.0136871338,23.9366603884), test loss: 29.3867734909\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.79682600498,2.8622461558), test loss: 2.80799188018\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (13.1607580185,23.8877379705), test loss: 28.0189567566\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.54505705833,2.85541751922), test loss: 2.55870022178\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (22.056344986,23.8381676041), test loss: 31.8149035931\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.26475596428,2.84873783162), test loss: 2.86955411434\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (17.2797851562,23.7878890373), test loss: 29.8536869526\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.73983705044,2.84212154226), test loss: 2.41569812\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (12.5774917603,23.739439576), test loss: 32.1141828775\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.63748693466,2.83570051665), test loss: 2.92481852174\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (47.5773468018,23.6921267687), test loss: 28.1433959961\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.1604142189,2.82936741691), test loss: 2.40240462124\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.96191883087,23.6435387557), test loss: 32.7585554838\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.772209107876,2.82305462694), test loss: 2.88320489228\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (21.6216812134,23.5969197904), test loss: 27.9000263691\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.947191476822,2.81675987592), test loss: 2.52122942507\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (9.5141210556,23.550175042), test loss: 31.8604711294\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.01288509369,2.8104803408), test loss: 2.71785620749\n",
      "\n",
      "MC # 3, Hype # hyp4, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold4/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (424.076416016,inf), test loss: 239.307913971\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (331.441467285,inf), test loss: 400.115715027\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (69.9696960449,121.082321024), test loss: 45.4790266037\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.68200874329,113.640939578), test loss: 3.24111510813\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (106.251663208,83.9062611213), test loss: 39.4652812481\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.65989375114,58.5061815116), test loss: 3.22282505333\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (32.0069198608,71.4738180838), test loss: 44.1510080814\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (5.17256450653,40.121380026), test loss: 3.36866415739\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (73.301864624,65.1196262109), test loss: 41.125510788\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.02501249313,30.9234353313), test loss: 3.51449883282\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (30.5233078003,61.3857153137), test loss: 41.7096511841\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.77116417885,25.4080905167), test loss: 2.82529127002\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (43.3958930969,58.8252034877), test loss: 46.4928632259\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.986415565014,21.73439297), test loss: 3.561215657\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (65.0262527466,56.984770677), test loss: 43.4592884064\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.61447620392,19.1126872364), test loss: 2.85192779303\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (34.6237258911,55.567407594), test loss: 42.1599876404\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.28847277164,17.1410331618), test loss: 3.1893554613\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (44.7099075317,54.4435520112), test loss: 37.0514592409\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.758555531502,15.6111054805), test loss: 2.81889418662\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (42.5422286987,53.5003766627), test loss: 45.2706376076\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.98088264465,14.3862617491), test loss: 3.16110164523\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (19.8185424805,52.7715452123), test loss: 34.4851561069\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.36727190018,13.3834803535), test loss: 2.70520537049\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (20.1005096436,52.0948888442), test loss: 43.4646420002\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.3334941864,12.5470663126), test loss: 3.39423413277\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (18.2544517517,51.5772123897), test loss: 37.9770479202\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.05884552002,11.8403641564), test loss: 3.36643863022\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (59.1345100403,51.0864758404), test loss: 44.1096116066\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.56634235382,11.2357674154), test loss: 3.30871117115\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (17.6169586182,50.6513315725), test loss: 41.4354085445\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.2481443882,10.7123988907), test loss: 3.31623567641\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (36.4249801636,50.258955995), test loss: 41.8867014408\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.95259928703,10.2523095365), test loss: 2.84697203338\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (44.6591873169,49.8923383119), test loss: 43.4934661865\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.7272901535,9.84764654737), test loss: 3.41226867437\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (20.8312397003,49.5421454333), test loss: 36.7380644321\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.24635529518,9.48688156451), test loss: 2.78933643997\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (75.5651855469,49.2479455901), test loss: 43.5645216942\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.26901817322,9.16392878729), test loss: 3.34507835507\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (40.7389945984,48.9425218956), test loss: 35.0538584232\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (5.88503074646,8.87244668184), test loss: 2.71471871138\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (42.3747329712,48.6923641032), test loss: 44.1085749626\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.08457756042,8.60990996938), test loss: 3.44068231583\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (18.242149353,48.4318592475), test loss: 31.9246789455\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.86420333385,8.37104642148), test loss: 2.7858596921\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (39.771446228,48.1870347877), test loss: 42.8864820242\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (6.07836914062,8.15299252231), test loss: 3.33815100491\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (62.840511322,47.9504047114), test loss: 38.4268102169\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.35526132584,7.95162343635), test loss: 3.46710895896\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (49.9331893921,47.7158940443), test loss: 38.6803157806\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.70869898796,7.76685143137), test loss: 2.80799352527\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (38.0453605652,47.4815503077), test loss: 43.4971655846\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.80225467682,7.59568780836), test loss: 3.39329866767\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (49.6961784363,47.2679535105), test loss: 40.3868803501\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.16102862358,7.43625665465), test loss: 2.73207281232\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (50.399772644,47.0447170807), test loss: 40.7231761456\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.86334991455,7.28747097575), test loss: 3.53274275661\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (27.7757453918,46.8445844107), test loss: 34.5797129631\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.94664740562,7.149714343), test loss: 2.60761765242\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (38.1100845337,46.6389119273), test loss: 42.1694966316\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.290038406849,7.02044228488), test loss: 3.266241166\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (61.754196167,46.4321577318), test loss: 32.8363387108\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (5.72672796249,6.8985708458), test loss: 2.74575832486\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (30.7014865875,46.22623536), test loss: 39.9234457493\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.90387415886,6.78293705553), test loss: 3.06355168521\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (30.2762336731,46.0160813439), test loss: 34.6491291761\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.36868810654,6.67298099608), test loss: 3.16619013846\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (40.5971565247,45.8023227016), test loss: 38.924495554\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.24206137657,6.56802212059), test loss: 3.0881249547\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (140.393264771,45.5965143405), test loss: 37.3868641853\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.65236496925,6.46713364781), test loss: 3.20377649665\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (26.7112045288,45.3808073572), test loss: 37.8583162308\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.17101716995,6.3708171016), test loss: 2.54055941403\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (28.4806060791,45.1743854781), test loss: 39.4742320538\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.63463830948,6.2799879737), test loss: 3.35343123674\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (22.0740222931,44.9536416866), test loss: 35.0196506023\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.9173810482,6.19311191194), test loss: 2.53218413889\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (31.7076396942,44.7304413017), test loss: 39.044316721\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.72297358513,6.10981418572), test loss: 3.02746490836\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (15.0477962494,44.5017066637), test loss: 31.2044060946\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.09246349335,6.02993002821), test loss: 2.62508372366\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (19.5339736938,44.2608794961), test loss: 38.9261875629\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.04672455788,5.95308441283), test loss: 2.96326530576\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (17.9953384399,44.0103162799), test loss: 27.7218251705\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.23601818085,5.87921947938), test loss: 2.45528538227\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (9.0654592514,43.7551504087), test loss: 36.9132979393\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.88852882385,5.80721709309), test loss: 3.03121709824\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (12.5745887756,43.4980556422), test loss: 32.3171818495\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.21585702896,5.73759891443), test loss: 3.19439176917\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (16.6362800598,43.2369863938), test loss: 31.4035836458\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.15533232689,5.67109306405), test loss: 2.6018569231\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (26.30418396,42.9618302126), test loss: 37.1756424904\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (4.86820840836,5.60637845372), test loss: 3.14907929301\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (23.4408321381,42.6826576036), test loss: 33.7803678036\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (6.09969425201,5.54360365256), test loss: 2.40260451138\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (36.1718521118,42.3974493354), test loss: 35.5862431526\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.42029476166,5.48281572954), test loss: 3.01032892764\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (40.7414894104,42.1010882819), test loss: 28.9615414143\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (5.6884560585,5.42360389245), test loss: 2.39243936688\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (36.5073661804,41.7987874664), test loss: 34.6004094601\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.75375127792,5.36596009253), test loss: 2.90701408088\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (26.9976539612,41.4942585843), test loss: 26.5983742714\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (5.96379613876,5.30947273943), test loss: 2.56249006093\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (6.50881195068,41.188980436), test loss: 33.4233110428\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.10215616226,5.254613961), test loss: 2.89883444905\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (28.845785141,40.8818712639), test loss: 29.3955845356\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.02184152603,5.20178821376), test loss: 3.16671576202\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (22.6053524017,40.5677490799), test loss: 32.0168334723\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.08468043804,5.15000894916), test loss: 3.05215866268\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (16.5498275757,40.2556730508), test loss: 29.7650020599\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.985828638077,5.09994667572), test loss: 3.08423914015\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (20.1675624847,39.9443612199), test loss: 30.0006236076\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.36994695663,5.05139778877), test loss: 2.29138731658\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (20.5125293732,39.6293058307), test loss: 32.3704693794\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.36461186409,5.00380290956), test loss: 3.17523071766\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (14.1864099503,39.3179167436), test loss: 30.6920280457\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.56238150597,4.95770208514), test loss: 2.4362273097\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (8.57565498352,39.0117580942), test loss: 33.4140566111\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.66896569729,4.91229619058), test loss: 3.04332605004\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.455915451,38.7106536681), test loss: 27.3732066154\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.49872422218,4.86832694212), test loss: 2.50747734904\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (17.4969787598,38.4132425744), test loss: 34.0900799751\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (6.68199825287,4.8258454322), test loss: 2.92261309624\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (19.7179012299,38.1191537958), test loss: 24.5785262585\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.693952202797,4.78444086405), test loss: 2.4529111594\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.9609394073,37.8325493097), test loss: 33.4083413601\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.969968557358,4.74432071148), test loss: 3.07194081023\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (22.2920207977,37.552221659), test loss: 28.9900822163\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.775109171867,4.70540947507), test loss: 3.22226981223\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (22.7484416962,37.274407106), test loss: 33.5161480904\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.64645206928,4.66733379916), test loss: 2.93724518418\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (10.507557869,37.0030089974), test loss: 31.616757226\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.51103138924,4.63038972209), test loss: 3.15408694148\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (13.7710208893,36.7403623848), test loss: 30.6403610706\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.7746515274,4.5938866719), test loss: 2.33422824144\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (20.061290741,36.4831253541), test loss: 33.4774509668\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (5.72787189484,4.55849379006), test loss: 3.22845445573\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (35.1448516846,36.2310750839), test loss: 29.3854275703\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.9833650589,4.52409528449), test loss: 2.4505998522\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (2.45042705536,35.983601632), test loss: 33.1590442657\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.5251083374,4.49085147072), test loss: 3.06210554689\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (18.4423828125,35.7443168294), test loss: 27.5026740074\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.794820189476,4.45845569099), test loss: 2.62783911526\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (57.1068344116,35.5105963188), test loss: 33.6568528175\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (4.53957462311,4.42696833653), test loss: 2.95986583233\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (36.4038543701,35.2792636138), test loss: 30.9670356274\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.61126351357,4.39596972264), test loss: 3.15304358602\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (9.45098876953,35.0547104383), test loss: 33.8936176538\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.47118377686,4.36588599814), test loss: 3.18505967557\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (15.4066734314,34.8371425496), test loss: 29.655063343\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.18167662621,4.33604784095), test loss: 3.21535900235\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.274230957,34.6235721695), test loss: 30.4245050907\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.92416369915,4.30700571977), test loss: 2.53305873275\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.1810474396,34.4139439897), test loss: 32.7458146095\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.60088706017,4.27886722095), test loss: 3.22557292581\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (24.0524139404,34.2098042549), test loss: 33.1867307663\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.833603739738,4.25159449296), test loss: 2.50696278065\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (11.3026304245,34.0110121516), test loss: 32.3268862247\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.94668936729,4.22493379323), test loss: 3.03572072685\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.974319458,33.815752647), test loss: 30.275371933\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.17635214329,4.19884679186), test loss: 2.60365775675\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (38.7216949463,33.6248606403), test loss: 34.2183808088\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.79386663437,4.17328197075), test loss: 2.92815125436\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (8.36552429199,33.4385712811), test loss: 27.0279527187\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.03352594376,4.14825816505), test loss: 2.60745722055\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (9.84905815125,33.2576469181), test loss: 34.708615303\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.75225639343,4.12351411988), test loss: 3.10762666762\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (18.8282661438,33.0796868122), test loss: 30.1010676384\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.43402218819,4.09936953507), test loss: 3.35499897003\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (53.3799629211,32.9046556671), test loss: 35.0828688622\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.50016272068,4.07583229551), test loss: 3.18653873801\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (10.2329902649,32.7336082722), test loss: 30.3330768585\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.919628679752,4.05309152283), test loss: 3.17562953234\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (18.0281829834,32.5674309654), test loss: 32.370323658\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.76886701584,4.03073401241), test loss: 2.42323350161\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (22.0641479492,32.4037723249), test loss: 33.4515700817\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (3.12736058235,4.00889840332), test loss: 3.24911906719\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (7.57934951782,32.2426266986), test loss: 29.9575476408\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.488075077534,3.98738609533), test loss: 2.48819256127\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (7.9527888298,32.0864465058), test loss: 35.0037427425\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.61580276489,3.9662309585), test loss: 3.17558331788\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (29.7609958649,31.9341358882), test loss: 29.0622288227\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.50777518749,3.9453686369), test loss: 2.6471635282\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (14.241147995,31.7833040315), test loss: 33.8744836807\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.76260209084,3.92493469386), test loss: 3.01911842674\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (25.3797645569,31.634442405), test loss: 25.4678336143\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.55674660206,3.90499875482), test loss: 2.71493685544\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (25.7264785767,31.4904195979), test loss: 35.101379776\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.06752204895,3.88572069995), test loss: 3.16825761199\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (10.7830181122,31.3491874712), test loss: 31.1237341881\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.852735459805,3.86665536218), test loss: 3.37045102715\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (15.4855384827,31.2091856279), test loss: 30.4299883127\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.29610133171,3.848008317), test loss: 2.51690960675\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (22.7797718048,31.0721849369), test loss: 33.0528874397\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.51256465912,3.82966476602), test loss: 3.2131280303\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (5.35995817184,30.9392412747), test loss: 32.9589020729\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.95867729187,3.8115679216), test loss: 2.44518129826\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (33.1826629639,30.8084752246), test loss: 32.7210383654\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.59473466873,3.79370315643), test loss: 3.07400181293\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (12.2835111618,30.679436231), test loss: 31.4992181301\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.31428813934,3.77614261889), test loss: 2.58715649545\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (8.72204494476,30.5509433954), test loss: 34.1256473064\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.803606987,3.75900388595), test loss: 2.97337704301\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (15.3613262177,30.4272795959), test loss: 27.7794860363\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.78038108349,3.74239919861), test loss: 2.65377729535\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (15.0854206085,30.305966646), test loss: 34.6815925479\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.330410808325,3.72596334102), test loss: 3.07166896462\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (23.6908607483,30.184971632), test loss: 30.1469477654\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.428563147783,3.70985753066), test loss: 3.25578012466\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (6.86308097839,30.0661524039), test loss: 35.0631705761\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.83520531654,3.69394216423), test loss: 3.17435677946\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (13.8240318298,29.951584105), test loss: 30.0693362951\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.36454200745,3.67824373662), test loss: 3.14634236395\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (7.30592870712,29.8375816243), test loss: 32.7166371346\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.91787815094,3.6627416441), test loss: 2.37322217524\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (13.4275112152,29.7256793189), test loss: 33.8984523773\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.44565629959,3.64750624973), test loss: 3.19673894048\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (5.39871644974,29.6137748141), test loss: 31.2236693859\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.865489542484,3.63253674483), test loss: 2.39266753942\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (9.12508583069,29.5060278481), test loss: 34.8634841919\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.61197137833,3.61807636256), test loss: 3.09545614421\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (23.3562240601,29.400199864), test loss: 29.3969122171\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.12658369541,3.60376973525), test loss: 2.61805375814\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (32.6088600159,29.2941604359), test loss: 34.7959877014\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.14920687675,3.58965755899), test loss: 2.9888551563\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (23.3238353729,29.1903930694), test loss: 26.6944816828\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.52627885342,3.57572344749), test loss: 2.529168576\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (17.4190940857,29.0898737826), test loss: 35.1043830156\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.75119638443,3.56194738316), test loss: 3.0962257117\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (36.6369590759,28.9898691151), test loss: 30.1458271265\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (4.64565229416,3.54830085194), test loss: 3.29941759109\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (14.8606948853,28.8913749803), test loss: 30.4210138321\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.3355512619,3.53493354091), test loss: 2.57819643021\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (4.81977462769,28.7929915992), test loss: 33.3421521187\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.613941013813,3.52176837748), test loss: 3.18637662828\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (18.206495285,28.6979593727), test loss: 32.4596745014\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.13869071007,3.50900238425), test loss: 2.4013883546\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (11.6841640472,28.6042224958), test loss: 33.5515721321\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.4239615202,3.49640805663), test loss: 3.08948821425\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.97206687927,28.5104214322), test loss: 32.0966696739\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.74665141106,3.48393217626), test loss: 2.53713181019\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (22.6863899231,28.4187585604), test loss: 33.869113636\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.02457475662,3.47160649042), test loss: 2.97134872079\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (45.2605514526,28.3295843533), test loss: 28.4148995876\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.81931996346,3.45940759179), test loss: 2.71148003191\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (16.8900222778,28.2408844412), test loss: 33.9045933247\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.38273692131,3.44724580673), test loss: 2.89704892337\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (22.154291153,28.1537777669), test loss: 31.1884520531\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.25187492371,3.43544893556), test loss: 3.26174766421\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (33.754032135,28.0663780712), test loss: 35.0838680983\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.84155213833,3.42375167274), test loss: 3.1745390892\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (26.5635032654,27.9814329373), test loss: 30.1587566376\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.671568751335,3.41234952997), test loss: 3.14392799139\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (11.7085342407,27.8982136539), test loss: 32.2426433086\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.21339702606,3.4011708606), test loss: 2.42001618892\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (17.8093261719,27.8145665495), test loss: 32.3767443895\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.964908361435,3.39005165659), test loss: 3.21025194526\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (23.0438632965,27.7323716987), test loss: 33.5908542633\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.53558313847,3.37906023246), test loss: 2.49390262216\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.17715549469,27.6524376004), test loss: 34.2150696754\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.76861011982,3.36813443393), test loss: 3.0861744985\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (13.3529205322,27.5732219953), test loss: 29.1817415237\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.24696660042,3.35727014161), test loss: 2.56112641096\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (18.209148407,27.4948261508), test loss: 35.4048278451\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.0976755619,3.34668384788), test loss: 2.91627472937\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (16.9750480652,27.4167152175), test loss: 26.1753961086\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.29865562916,3.33621649821), test loss: 2.48180553615\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (21.231048584,27.3405218048), test loss: 34.9645417929\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.41051006317,3.32602981866), test loss: 3.05767179132\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (11.026925087,27.26605758), test loss: 30.3161820889\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.505956292152,3.31597455136), test loss: 3.24335430264\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (22.5591430664,27.1906013354), test loss: 35.5499572277\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.76041686535,3.305991271), test loss: 3.0569001168\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (8.78384685516,27.1166137907), test loss: 32.5760377407\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.08300220966,3.29609543363), test loss: 3.15716471672\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (16.1509361267,27.0447589429), test loss: 32.0688879013\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (4.79212999344,3.28627563918), test loss: 2.33779138923\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (17.3994102478,26.9732191514), test loss: 34.3419060826\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.85389113426,3.27645955615), test loss: 3.13410918117\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (25.1379947662,26.9023944479), test loss: 31.0432318449\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.43576145172,3.26694687181), test loss: 2.50111379921\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (7.60853099823,26.8317244112), test loss: 34.0336672544\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.26488506794,3.2574881335), test loss: 3.05748368204\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (16.447599411,26.7628158), test loss: 28.8176621437\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.798214316368,3.2482559153), test loss: 2.62104122043\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (35.1482620239,26.6954726109), test loss: 34.1517387867\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (3.91287565231,3.23920983337), test loss: 2.8784329921\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (10.3191490173,26.626999581), test loss: 31.7081040382\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.41724205017,3.23016917181), test loss: 3.09039103985\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (21.3809814453,26.560048083), test loss: 34.5981310368\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.23505139351,3.22123021142), test loss: 3.09673748612\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (25.1065864563,26.4947617184), test loss: 30.1859905124\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (3.77170348167,3.2122545352), test loss: 3.14392346144\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (5.37225151062,26.4297567739), test loss: 31.4293181181\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.31081497669,3.203414073), test loss: 2.52243839502\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (20.4570617676,26.3651566961), test loss: 32.6813257694\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (4.77969837189,3.19476769897), test loss: 3.17289609909\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (23.7170333862,26.3010789299), test loss: 34.2065254688\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.0225212574,3.18619879795), test loss: 2.52021830678\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (8.41686058044,26.2382245622), test loss: 32.4561262608\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.4566514492,3.17781817475), test loss: 2.99956442714\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (6.04076242447,26.1766733975), test loss: 32.6770782471\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.33341896534,3.16956129947), test loss: 2.69853384942\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (10.8267126083,26.1144525947), test loss: 34.5791424274\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.46446585655,3.16136309483), test loss: 2.93311648965\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (18.454908371,26.0532998364), test loss: 27.1950785637\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.17502880096,3.15322455443), test loss: 2.54221644998\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (6.84599590302,25.993625316), test loss: 34.9306070328\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.474316000938,3.14500905755), test loss: 2.99184848666\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (14.4175796509,25.9343617391), test loss: 29.92512393\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.47523498535,3.13698438368), test loss: 3.17220060825\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (7.72604990005,25.8750949313), test loss: 35.545499754\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.25965309143,3.12902571241), test loss: 3.07994869053\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (17.8316993713,25.8164239587), test loss: 30.2068500519\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.51544570923,3.12125948587), test loss: 3.07873867154\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (10.9614543915,25.7590179515), test loss: 33.2336737156\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.91135853529,3.11360379458), test loss: 2.38781478256\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (47.1633529663,25.7025144658), test loss: 33.6386699677\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.99297320843,3.10607628999), test loss: 3.2209243536\n",
      "\n",
      "MC # 3, Hype # hyp4, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold5/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (166.366851807,inf), test loss: 144.830742836\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (355.396362305,inf), test loss: 379.589585876\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (27.8077793121,71.690703218), test loss: 42.4563213348\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.48169469833,77.286984778), test loss: 3.30361035466\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (34.5720062256,58.470034889), test loss: 36.661105442\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.57487916946,40.4258209513), test loss: 3.00209399164\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (44.0626564026,54.0582082887), test loss: 41.7971586704\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.64566135406,28.1196849786), test loss: 3.57445597649\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (27.2302722931,51.9369607531), test loss: 39.0255992413\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.26969456673,21.973446204), test loss: 3.61906912327\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (27.6484298706,50.5576146261), test loss: 39.6635499477\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.910440325737,18.2856333656), test loss: 3.45461367667\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (60.7186851501,49.6162751691), test loss: 41.0294138908\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (6.07724952698,15.8245655716), test loss: 3.79842063785\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (51.8675079346,48.8601959808), test loss: 36.6781269073\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.66216993332,14.0649927773), test loss: 2.75522666574\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (44.8727111816,48.2278047528), test loss: 41.8822142124\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.13535583019,12.7425096303), test loss: 3.69088878334\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (50.3782196045,47.7358528749), test loss: 38.3931143284\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.89208674431,11.7104283603), test loss: 2.79685175717\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (23.183801651,47.363078985), test loss: 40.5124285221\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.10633039474,10.8889186821), test loss: 4.00283312798\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (45.8650665283,47.0124147867), test loss: 37.9484014034\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.49689722061,10.2157332695), test loss: 2.80988208055\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (56.7862968445,46.6970938581), test loss: 38.776437664\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (5.05124044418,9.65424876334), test loss: 3.68091887832\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (47.3583564758,46.3816274775), test loss: 38.8077899456\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.66318941116,9.17806779354), test loss: 2.96358748674\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (33.3792495728,46.0738753029), test loss: 40.8438011169\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.81422841549,8.76776473521), test loss: 3.46802562475\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (28.2664718628,45.8048163663), test loss: 37.4340086937\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.30687093735,8.40919313254), test loss: 2.90606421679\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (20.3157100677,45.5804445686), test loss: 40.5973585129\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (6.36154651642,8.09702888613), test loss: 3.60242974162\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (59.5236549377,45.3482631996), test loss: 34.2282895088\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.55972027779,7.8202339342), test loss: 2.84931593239\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.525177002,45.1216735154), test loss: 38.9419275284\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.53025436401,7.57357182289), test loss: 3.19807180166\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (29.4793815613,44.8820117812), test loss: 36.7025925159\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.18834340572,7.35179147642), test loss: 3.46455851197\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (44.3778343201,44.6367349047), test loss: 36.9540926933\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.26624584198,7.15039841787), test loss: 3.24639245868\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (52.8153533936,44.4081447981), test loss: 37.4322860241\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (5.66899204254,6.96591238665), test loss: 3.58864477277\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (40.1076507568,44.1979083845), test loss: 33.4439248562\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.28138208389,6.79853259817), test loss: 2.81845999658\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (23.9924964905,43.9752899707), test loss: 38.5491137505\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.855154395103,6.64443523005), test loss: 3.52895722687\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (17.6270217896,43.7524749336), test loss: 34.9861205101\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.44698762894,6.5025940523), test loss: 2.55931761861\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (35.6071929932,43.5133891555), test loss: 36.8835309505\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.55465626717,6.37070844956), test loss: 3.61270847321\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (46.5972900391,43.2637993383), test loss: 33.7791304111\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.1171836853,6.24725326314), test loss: 2.58583308458\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (43.3424530029,43.0202718761), test loss: 35.1168033719\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.29723644257,6.13084743351), test loss: 3.61345872879\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (61.2455635071,42.786496485), test loss: 34.7995791912\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.28246593475,6.02248829786), test loss: 2.70285256952\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (31.8756008148,42.5358752504), test loss: 36.295154953\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.64601325989,5.92036972648), test loss: 3.35406684875\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (40.5127983093,42.2818541188), test loss: 32.697820282\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (5.26390647888,5.82419666942), test loss: 2.70764858723\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (43.152381897,42.0076668953), test loss: 35.9725175858\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.19517326355,5.73245209818), test loss: 2.94312420487\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (27.909986496,41.723102178), test loss: 29.4573261738\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.09119272232,5.64485568451), test loss: 2.60901204646\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (39.8173904419,41.4393647595), test loss: 33.5932424545\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.68308353424,5.5604435165), test loss: 3.08367615938\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (46.3433380127,41.1578858344), test loss: 31.1047688961\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.17942237854,5.48052377658), test loss: 3.26567617655\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (25.0080795288,40.8600525103), test loss: 31.7547885418\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.33921146393,5.40382895927), test loss: 3.06985087991\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (43.5137214661,40.5580627669), test loss: 30.9845753193\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (5.07807397842,5.33045232087), test loss: 3.32238966525\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (14.6369829178,40.2372130151), test loss: 27.2911415577\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.6990852356,5.25957145219), test loss: 2.43407852948\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (20.6324920654,39.9090904943), test loss: 31.7691229105\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.31836414337,5.19082243057), test loss: 3.27347660959\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (17.3753166199,39.5823660796), test loss: 28.6315608978\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.883974611759,5.12382195209), test loss: 2.30155293941\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (23.3969841003,39.2562528665), test loss: 30.4009821892\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.45197844505,5.05978812256), test loss: 3.39616795182\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (27.9340839386,38.9206972949), test loss: 27.5394715309\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (5.72097110748,4.99793593041), test loss: 2.27314110696\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (28.3030204773,38.585658467), test loss: 28.8305455089\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.95396280289,4.9382817309), test loss: 3.20792201161\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (22.4887123108,38.2415549014), test loss: 29.5149400234\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.16092777252,4.88047009053), test loss: 2.43907916844\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (2.73934054375,37.8965748364), test loss: 29.6403432608\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.06183815002,4.82430697812), test loss: 3.04032517076\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (15.0406112671,37.5605175318), test loss: 27.1268106461\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.424036026,4.76937340421), test loss: 2.48986507207\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (18.4995727539,37.2298678514), test loss: 30.8712972641\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.29030275345,4.71677037268), test loss: 2.91562408507\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (20.361240387,36.8983003226), test loss: 25.3852468491\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.3387928009,4.66600731361), test loss: 2.47181346416\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.9324855804,36.5748969846), test loss: 29.9690895319\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.02901697159,4.61722516358), test loss: 2.89499996603\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (23.6836090088,36.251026505), test loss: 27.57821064\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.53785085678,4.56995983799), test loss: 3.26127332151\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (10.1383056641,35.932969939), test loss: 29.2244721413\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.90306520462,4.52408005258), test loss: 3.02715525627\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (7.54658222198,35.6277003103), test loss: 27.5101385117\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.86612510681,4.47923790877), test loss: 3.32467977405\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (17.3203430176,35.3309824067), test loss: 24.9408423424\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.15165114403,4.43624166704), test loss: 2.48784705549\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (20.5784568787,35.0377801249), test loss: 29.0388195992\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.02118086815,4.39471847082), test loss: 3.31511172652\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.3639163971,34.7549535787), test loss: 27.5578904629\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.13472402096,4.35491616947), test loss: 2.24854328036\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (22.7540283203,34.4752481596), test loss: 28.8670266628\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.889381051064,4.31623345417), test loss: 3.32002934217\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (11.3400306702,34.2022565766), test loss: 27.5393178463\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.25836086273,4.27861995287), test loss: 2.25390349925\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (14.5551023483,33.9421566896), test loss: 28.1583953619\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (4.60308170319,4.24177586503), test loss: 3.21430104375\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.3990278244,33.6894395318), test loss: 29.4683616161\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.650918900967,4.20622991867), test loss: 2.4929903537\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (17.5097866058,33.4402277112), test loss: 29.5378138542\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.36985492706,4.17188998316), test loss: 3.07838678956\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.40117836,33.2010644237), test loss: 27.7380919695\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.68245112896,4.13893384479), test loss: 2.53116851151\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.7886352539,32.964914033), test loss: 32.7510898829\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.361128509045,4.10684206213), test loss: 2.86238766313\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (9.76264953613,32.7345061577), test loss: 26.3346297026\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.00808405876,4.07556946612), test loss: 2.51560403109\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.1936836243,32.5153987128), test loss: 31.377992177\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.53306484222,4.0448830681), test loss: 2.98743462563\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (9.2891254425,32.3022370458), test loss: 28.7099989653\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.43040883541,4.01515446417), test loss: 3.33555516899\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (24.9605007172,32.0919386382), test loss: 30.3733399868\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.94121170044,3.98637621236), test loss: 3.12708567977\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (16.6209259033,31.8896377525), test loss: 27.829749918\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.26415932178,3.9587314598), test loss: 3.40979389548\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (16.7398853302,31.6897749054), test loss: 27.338100338\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.591108620167,3.93174639499), test loss: 2.50354502797\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (8.20072364807,31.4945041726), test loss: 29.1104422092\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.57644891739,3.90539636129), test loss: 3.32460958362\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (12.7442131042,31.3089706574), test loss: 28.3345175743\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.57112908363,3.87945425655), test loss: 2.25239603221\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (4.74078845978,31.1276043621), test loss: 29.5523639917\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.84431803226,3.85428390981), test loss: 3.35960606337\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (33.7240180969,30.9487403274), test loss: 28.1397763252\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.4241399765,3.82985131461), test loss: 2.30038235188\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (14.1460800171,30.7758300732), test loss: 28.9624605894\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.06305778027,3.80635761851), test loss: 3.2106408596\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (17.3270454407,30.6048337987), test loss: 31.8553308249\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.03189074993,3.78338183258), test loss: 2.51626024842\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (7.7442650795,30.4375563634), test loss: 30.3209294558\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.19580614567,3.76087914504), test loss: 3.11354716122\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (8.5207195282,30.2784704425), test loss: 28.0275800228\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.15875840187,3.73867098509), test loss: 2.55169312507\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (9.43175125122,30.1226607242), test loss: 32.5072886467\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.33281707764,3.71709907124), test loss: 2.95267376304\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (35.0769119263,29.9684240379), test loss: 26.6298961639\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.96126914024,3.69607411121), test loss: 2.53112146258\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (13.1537714005,29.8191347086), test loss: 32.0792236328\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.03218305111,3.67584122096), test loss: 3.00918227881\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (15.6288747787,29.6713159618), test loss: 29.3633820534\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.57868385315,3.65603977555), test loss: 3.29844071865\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (10.6225986481,29.5262771997), test loss: 31.1316424608\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.00033283234,3.63657660842), test loss: 3.12434339374\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (10.2465896606,29.3882419193), test loss: 27.92489779\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.39935171604,3.61734626554), test loss: 3.38158541918\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.847363472,29.2527158814), test loss: 26.4975098133\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.12458896637,3.59860025246), test loss: 2.48914637417\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (20.9635848999,29.1181804116), test loss: 29.5632018089\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.952798366547,3.5803015586), test loss: 3.31127367616\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (9.69957923889,28.9878166955), test loss: 28.9619137287\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.72800564766,3.5627177558), test loss: 2.28695404828\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (40.2201690674,28.8590242913), test loss: 28.9690894127\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.51855230331,3.54543427546), test loss: 3.38685159087\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (20.8679847717,28.731562399), test loss: 28.2442819118\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.12182712555,3.52843195195), test loss: 2.22538232803\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (8.54947090149,28.6102341024), test loss: 29.3465076923\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.19786846638,3.51154504771), test loss: 3.18644095659\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (18.3031730652,28.4913459172), test loss: 30.0787323952\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (3.59867715836,3.49514249364), test loss: 2.51942323446\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (7.06702184677,28.3724978887), test loss: 31.1762763977\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.03765892982,3.47900947396), test loss: 3.1464787364\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.08923149109,28.2575570306), test loss: 28.5671209335\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.98682200909,3.46354058253), test loss: 2.56744178534\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (44.599155426,28.1439312058), test loss: 33.8231427193\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.75642132759,3.44833012133), test loss: 2.98313880265\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (22.770904541,28.0308813911), test loss: 26.8457150459\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.39872598648,3.43329983296), test loss: 2.50573632419\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (7.63045406342,27.9232750762), test loss: 31.8503437996\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.29180848598,3.41834841156), test loss: 2.99505938888\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (33.051940918,27.8181410211), test loss: 28.9695903301\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.34584760666,3.40383658408), test loss: 3.18594845235\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (4.01848554611,27.7119622542), test loss: 30.8918920755\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.765675365925,3.38950272042), test loss: 3.10548876226\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (14.569565773,27.6097901856), test loss: 27.5446388245\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.17886662483,3.37577062071), test loss: 3.34480520189\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (42.7354621887,27.5084374712), test loss: 27.7164247513\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.24543380737,3.36226131019), test loss: 2.48975265771\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (23.724275589,27.4072887118), test loss: 28.8830909491\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.43828344345,3.34887091411), test loss: 3.30924599171\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (4.68306732178,27.3110594908), test loss: 28.810740757\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.539665102959,3.33552158287), test loss: 2.26206699312\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (29.8873405457,27.2168443963), test loss: 29.1152383566\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.71462130547,3.32256599132), test loss: 3.35425069928\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (8.45754051208,27.1216212753), test loss: 27.6904939175\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.61505150795,3.30975653726), test loss: 2.19397048354\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (13.6968069077,27.0297302404), test loss: 29.572439456\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.592603385448,3.29745409293), test loss: 3.15325280726\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (19.023147583,26.9384927706), test loss: 31.3866060257\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.64746189117,3.28537093598), test loss: 2.53956097364\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (18.8767700195,26.8476211499), test loss: 31.014608717\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.3584728241,3.27333809279), test loss: 3.12078595757\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (20.8896579742,26.7609359149), test loss: 28.0234406948\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (3.39335823059,3.26138777186), test loss: 2.5299233228\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (18.1390724182,26.6756037446), test loss: 32.0614660263\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.13803684711,3.2496720004), test loss: 2.92778908014\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (11.1732616425,26.5895076064), test loss: 26.6425494909\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.79608726501,3.23813543566), test loss: 2.48900007606\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (12.7963485718,26.5063427807), test loss: 31.6710734367\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.35558664799,3.2270533844), test loss: 2.9648663044\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (11.7320861816,26.4236419305), test loss: 29.623690033\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.65493202209,3.21614605341), test loss: 3.18174485266\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (18.1204910278,26.3411990712), test loss: 31.0724966049\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.13434123993,3.20526771879), test loss: 3.06418457627\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (24.1175308228,26.2624675888), test loss: 27.3488474846\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (4.51156282425,3.19446946132), test loss: 3.27645998597\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (5.80733108521,26.1849227781), test loss: 26.4331408024\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.672562122345,3.18383797173), test loss: 2.51078943908\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (9.31480693817,26.1064160546), test loss: 29.1418045044\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.75234127045,3.17338805278), test loss: 3.29714081734\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (15.1385583878,26.0307924127), test loss: 29.0511613607\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.34140145779,3.16332672464), test loss: 2.28077878803\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (11.7090539932,25.9553077462), test loss: 28.8254528761\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.93347239494,3.15343818915), test loss: 3.33903079629\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (13.5868082047,25.8799504007), test loss: 27.7168531418\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.557254076,3.14356583677), test loss: 2.19097793102\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (17.7036552429,25.8081396187), test loss: 29.5172370434\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (6.41224384308,3.13376168673), test loss: 3.10468136072\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (4.92535543442,25.7370846231), test loss: 29.6328891277\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.317662626505,3.12402195139), test loss: 2.50138380527\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (6.63410520554,25.6652023526), test loss: 31.0368684292\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.19968223572,3.11448920355), test loss: 3.1037581265\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (20.1892318726,25.5959523696), test loss: 28.1075517178\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.7105089426,3.10532181722), test loss: 2.54072094262\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (17.8328399658,25.5267198157), test loss: 32.4613635778\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.93776464462,3.09628628073), test loss: 2.88269471824\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (11.3610744476,25.4574791965), test loss: 26.9015883446\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.57986021042,3.08726126138), test loss: 2.47911904454\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (18.7877311707,25.391568452), test loss: 31.116680932\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (5.1032128334,3.07829018771), test loss: 2.8629719615\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (13.2191524506,25.3262839595), test loss: 28.8126805305\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.335811883211,3.06935675905), test loss: 3.11680822968\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (15.8761749268,25.2601154908), test loss: 30.687987709\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.62200152874,3.06061615831), test loss: 3.04554493427\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (15.9220314026,25.1963041471), test loss: 27.0838930488\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.30167472363,3.05220920794), test loss: 3.27102162242\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (55.2022743225,25.1330007836), test loss: 30.133904171\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.77529358864,3.04394248455), test loss: 2.62387060076\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (17.0323867798,25.0686288345), test loss: 28.3433955669\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.86091947556,3.0356297922), test loss: 3.30725406259\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (11.9400444031,25.0076963806), test loss: 28.6221611142\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.55465173721,3.02738871797), test loss: 2.26813407242\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (14.1013679504,24.9473723689), test loss: 29.0848056555\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.31187331676,3.01916183524), test loss: 3.32246793211\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (16.1030654907,24.8861609702), test loss: 27.0069569588\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.43353033066,3.01110995475), test loss: 2.18482497782\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (19.3354759216,24.8271850716), test loss: 29.5057273149\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.4336155653,3.00335920914), test loss: 3.0855104208\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (56.966255188,24.768502861), test loss: 31.3786710262\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.86973810196,2.99573348165), test loss: 2.51595480144\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (16.4904251099,24.7088197162), test loss: 30.7789428711\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.33909988403,2.98807833355), test loss: 3.09622995406\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (14.0079946518,24.6523321157), test loss: 28.0295317173\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.83002746105,2.98044800253), test loss: 2.52943532467\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (10.8860740662,24.5963651599), test loss: 31.3136928558\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.40865373611,2.97283981419), test loss: 2.86171301305\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (14.5362348557,24.5394720565), test loss: 26.5670851707\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.12981319427,2.96540031048), test loss: 2.46923257113\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (19.5619907379,24.4845938257), test loss: 30.9733650684\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.46583557129,2.95822224606), test loss: 2.82867836952\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (33.3500289917,24.4301704364), test loss: 30.3127187014\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.33623552322,2.95118221306), test loss: 3.17464163005\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (18.0371360779,24.3746099541), test loss: 30.7464077473\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (3.16523838043,2.94408510576), test loss: 2.99910956919\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (9.30447673798,24.3218741724), test loss: 26.8440993071\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.61441302299,2.93699259379), test loss: 3.2309453696\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (15.619717598,24.2698214242), test loss: 30.1999243736\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.3122484684,2.92994803034), test loss: 2.6825335443\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (12.7975254059,24.2166815872), test loss: 28.7580827475\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.36540794373,2.92303900404), test loss: 3.28246612251\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (16.6401481628,24.1655129346), test loss: 28.1488953829\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.43671929836,2.91636965711), test loss: 2.26903664023\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (50.0902786255,24.1151077533), test loss: 29.6328098774\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (3.9919192791,2.90986246541), test loss: 3.35466021001\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (23.3170852661,24.0627954707), test loss: 26.6991591215\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (3.14798498154,2.90325134929), test loss: 2.12979316264\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.59277439117,24.0134692258), test loss: 29.7927906513\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.937655568123,2.89664380888), test loss: 3.06239290237\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (16.8662853241,23.9648238703), test loss: 29.0247307777\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (4.73934745789,2.89012712834), test loss: 2.41368818879\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (28.5289974213,23.9152641378), test loss: 30.7710364342\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.961642324924,2.88363673845), test loss: 3.10874195695\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (24.017911911,23.867294032), test loss: 27.5000762463\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.542411804199,2.87742336434), test loss: 2.49273685813\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (51.9617767334,23.8199258487), test loss: 33.2823729038\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.55348086357,2.87135199753), test loss: 2.94832308292\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (33.8417510986,23.7710016515), test loss: 26.7337461948\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (4.6720161438,2.86520494262), test loss: 2.44819234014\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (13.1130933762,23.7246475827), test loss: 30.9974878311\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.45096874237,2.85901317123), test loss: 2.80822631717\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (18.255607605,23.678947138), test loss: 28.4662081718\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (4.75844097137,2.85292232699), test loss: 3.07652505636\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (33.5158042908,23.6324408947), test loss: 31.0544896126\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.788524508476,2.84684295584), test loss: 3.00865830183\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (25.9528694153,23.5872669742), test loss: 27.0311608791\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.577759504318,2.84103262547), test loss: 3.18900172114\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (13.6107311249,23.5427528838), test loss: 32.7276465416\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.09727787971,2.83535171371), test loss: 2.93856637478\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (24.449552536,23.4966856844), test loss: 28.4077507257\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.48973894119,2.82959829346), test loss: 3.28747921437\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (28.1749801636,23.4532580519), test loss: 28.1576283932\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.88867115974,2.82380280184), test loss: 2.2641302675\n",
      "run time for single CV loop: 7083.30782819\n",
      "\n",
      "MC # 4, Hype # hyp5, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold1/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (308.010253906,inf), test loss: 163.136920166\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (315.214080811,inf), test loss: 385.460577393\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (23.6880702972,76.01784373), test loss: 45.5509419441\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.01078510284,76.484606971), test loss: 3.34984630942\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (49.1505508423,60.5627029729), test loss: 37.2310348034\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.10792207718,39.7528222602), test loss: 2.8509732604\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (54.1467819214,55.4110112988), test loss: 45.0000165462\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.596912622452,27.492397742), test loss: 3.41551982462\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (42.4099006653,52.8440859654), test loss: 42.6546666622\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.25942230225,21.3624128567), test loss: 3.53255350888\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (52.2396774292,51.2125352592), test loss: 45.1317059994\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.5704100132,17.6858749146), test loss: 3.31362994909\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (79.3422088623,50.0785838499), test loss: 45.4342774868\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (6.05853462219,15.2373710118), test loss: 3.4915491581\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (20.320350647,49.2279379192), test loss: 42.9928383827\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.57810759544,13.4829590341), test loss: 2.63732415438\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (68.9313201904,48.5106828407), test loss: 46.6243822098\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.41637134552,12.1658332523), test loss: 3.85788037181\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (34.5854911804,47.9255017373), test loss: 42.5892377853\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.59641361237,11.1416134066), test loss: 2.6559689045\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (20.3348770142,47.4437778378), test loss: 45.4009232521\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.29207062721,10.3191979517), test loss: 3.68289471865\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (18.0292549133,47.0466392892), test loss: 40.4954378128\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (5.11602210999,9.64698230824), test loss: 2.64216992855\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (66.4738464355,46.6776551585), test loss: 46.5336756706\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (7.32776689529,9.08621566693), test loss: 3.44626772404\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (41.259475708,46.3372169079), test loss: 36.6990056515\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.75278687477,8.60959668171), test loss: 2.91739333272\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (29.0862979889,45.9979448425), test loss: 42.6699718475\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.15742540359,8.20160485607), test loss: 3.20447002947\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (10.9796085358,45.6686835622), test loss: 33.9790598869\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.6768579483,7.84634975603), test loss: 2.85600695908\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (64.4086761475,45.3878139485), test loss: 42.7578248024\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.24711036682,7.53318545726), test loss: 3.22836125195\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (19.2507324219,45.1088337944), test loss: 40.5759848118\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.51179134846,7.25559907357), test loss: 3.39477525055\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (61.7193374634,44.843752209), test loss: 43.1663883209\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (5.67073535919,7.00916242039), test loss: 3.01909537017\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (23.7893638611,44.56349218), test loss: 41.2492228985\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.26448357105,6.78912534922), test loss: 3.40501123667\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (37.449256897,44.2749930135), test loss: 38.0002612114\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.39076679945,6.59009300925), test loss: 2.54408936054\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (15.0705862045,43.97506661), test loss: 41.5150139809\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.17520141602,6.40952433783), test loss: 3.65919209123\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (42.5249252319,43.6731074392), test loss: 37.5697348356\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.82095098495,6.24420379276), test loss: 2.57694398463\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (24.8550186157,43.3835979335), test loss: 42.2441036701\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.766509532928,6.09106383228), test loss: 3.44080868363\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (31.1714363098,43.0947872104), test loss: 35.0574596405\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.20035040379,5.95039382529), test loss: 2.58135634661\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (16.3737411499,42.7894353937), test loss: 41.5205099583\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.11853814125,5.8201045459), test loss: 3.34552599192\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (36.9727554321,42.4783267308), test loss: 29.7998690128\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.95374751091,5.6995624463), test loss: 2.62222106755\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (14.5470504761,42.1660342458), test loss: 37.3325780392\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.17033100128,5.58693969511), test loss: 3.11865967512\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (15.8772764206,41.8217991317), test loss: 34.09375875\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.42076718807,5.4809570948), test loss: 3.26732991934\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (26.3221759796,41.4965284998), test loss: 35.632273078\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (4.0245256424,5.38135177155), test loss: 3.1665948987\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (59.7617225647,41.161482123), test loss: 33.8812368393\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.97625637054,5.28721252352), test loss: 3.33022478372\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (27.1508769989,40.8237192928), test loss: 30.1754876137\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.02986419201,5.19930988293), test loss: 2.5929544732\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (35.5691452026,40.4751150542), test loss: 35.0434726715\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.15578222275,5.11600967832), test loss: 3.43894948065\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (20.6176452637,40.1197306939), test loss: 30.3034962654\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.66823649406,5.03719054464), test loss: 2.37919567823\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (15.5185222626,39.7460728006), test loss: 35.0768776894\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.69791138172,4.96201115975), test loss: 3.49046061933\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (18.5113067627,39.3710189156), test loss: 30.0591255665\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.3766759634,4.89041891466), test loss: 2.4070935145\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (30.0028076172,39.0078678776), test loss: 37.4862233877\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (6.86912155151,4.82183316501), test loss: 3.48209647536\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (18.2858886719,38.648026246), test loss: 28.0471400976\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.54741978645,4.75635954595), test loss: 2.63499552608\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (8.8269367218,38.2890571867), test loss: 35.0945469379\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.17794942856,4.69404748719), test loss: 3.33233364522\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (32.3904533386,37.9321102353), test loss: 24.2709448814\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.76137208939,4.63520283391), test loss: 2.61279542446\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (23.1458244324,37.580640889), test loss: 32.8479913712\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.65132951736,4.57911984801), test loss: 3.31823811233\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (45.6108551025,37.2330113705), test loss: 29.987502861\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.5597717762,4.52509710032), test loss: 3.17807087302\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (8.10319900513,36.8913155427), test loss: 32.191004324\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.54739379883,4.47342363459), test loss: 3.32143844962\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (33.522819519,36.5691136242), test loss: 29.723554945\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.97603046894,4.42314475822), test loss: 3.3492307514\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (21.2156944275,36.2508023264), test loss: 26.1228252411\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.02983021736,4.37533491143), test loss: 2.68186029494\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (16.8193531036,35.9376737579), test loss: 33.1348511696\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.6991147995,4.32932656695), test loss: 3.59747020155\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (14.0331392288,35.6340395969), test loss: 27.5371446133\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.22064113617,4.28552873659), test loss: 2.43260880858\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (61.8633880615,35.3416701033), test loss: 34.3950210571\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.23950076103,4.24388209306), test loss: 3.545089975\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (21.1778392792,35.0473401524), test loss: 27.131444478\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.48417878151,4.20341099295), test loss: 2.49272405207\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (24.8942375183,34.7696000801), test loss: 36.2299416065\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.64272785187,4.1640939587), test loss: 3.59450186193\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (20.2947978973,34.4969488277), test loss: 26.9887892246\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.71976709366,4.12611834478), test loss: 2.65981515646\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (32.549156189,34.2320957886), test loss: 34.1798727036\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.35304689407,4.08983187245), test loss: 3.46738678515\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (22.8772830963,33.9723172625), test loss: 24.5834329605\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.58346438408,4.05445506489), test loss: 2.54676221013\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (17.0619888306,33.7212541305), test loss: 32.8598136187\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.89446640015,4.02063251776), test loss: 3.36470537186\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.165184021,33.4732120307), test loss: 29.6574338436\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.80220985413,3.98781673883), test loss: 3.20697828829\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (24.1049690247,33.2296883749), test loss: 32.2776805401\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.58399498463,3.95583141685), test loss: 3.33154533803\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (7.65500974655,32.9986098082), test loss: 30.5948447227\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.741902709007,3.92435520359), test loss: 3.50007365346\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (5.04204273224,32.7706049575), test loss: 26.7665175438\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.507221400738,3.89383498889), test loss: 2.60816572607\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (38.1959037781,32.5448795163), test loss: 32.0034518719\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.3373837471,3.86427472193), test loss: 3.54772509634\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.9957561493,32.3262569929), test loss: 27.4919073582\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.83526229858,3.83593142465), test loss: 2.2500207752\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (4.41396474838,32.1138100521), test loss: 33.5159576416\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.738254487514,3.80834126743), test loss: 3.41649844348\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (18.3319168091,31.902418822), test loss: 28.7643100262\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.45678877831,3.78126061264), test loss: 2.48563134968\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (9.19412994385,31.6966337135), test loss: 35.7942892551\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.69306278229,3.75473640468), test loss: 3.46328894198\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (9.8657169342,31.4972876789), test loss: 27.3925849438\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.5057169199,3.72864796711), test loss: 2.61289787889\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (15.706401825,31.3008000336), test loss: 33.6535006523\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.03086256981,3.70334355936), test loss: 3.26175189912\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (19.7572746277,31.1059572727), test loss: 24.2485254765\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.82985496521,3.67855574872), test loss: 2.53535430878\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (6.98507213593,30.9162010445), test loss: 33.8972022057\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.890983104706,3.65461767525), test loss: 3.28408952653\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (12.7690906525,30.7301658304), test loss: 29.3421072483\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.91354227066,3.63139938559), test loss: 3.15238110423\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (21.6542720795,30.544834273), test loss: 32.8420912266\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.53708219528,3.60843008072), test loss: 3.18126253635\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (19.5021800995,30.3669907609), test loss: 30.043875742\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.5920445919,3.58581817243), test loss: 3.40281812251\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (6.15899181366,30.1911337229), test loss: 28.6283759594\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.5354295969,3.56356839274), test loss: 2.45988331139\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (6.86379098892,30.0164256807), test loss: 31.5533056259\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.56406521797,3.54191882052), test loss: 3.51457192302\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (12.5038385391,29.8452675987), test loss: 27.8663475037\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.01920557022,3.52086178022), test loss: 2.2504175663\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.3616113663,29.6780396175), test loss: 33.2919188976\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.321280986071,3.50034448226), test loss: 3.3148802191\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (17.5696258545,29.5111569037), test loss: 30.429447031\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.50414276123,3.48025458649), test loss: 2.44050774872\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (20.1505756378,29.3471709631), test loss: 35.0630488873\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.20530545712,3.46042484927), test loss: 3.34698153585\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (5.99303722382,29.1884010207), test loss: 27.3030005455\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.02857446671,3.44067526442), test loss: 2.57567302436\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (21.4118785858,29.0304027844), test loss: 32.8847030163\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.36774230003,3.42147572999), test loss: 3.15101325959\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (5.15748929977,28.8724335178), test loss: 24.4627580166\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.927172899246,3.40255830926), test loss: 2.49635066092\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (28.430480957,28.7185334325), test loss: 34.5184396267\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.762504458427,3.38433947977), test loss: 3.16873491704\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (21.9450035095,28.5686085023), test loss: 29.3267368793\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (3.92304825783,3.36649192063), test loss: 3.18884733915\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (6.01613330841,28.4163249256), test loss: 33.1039011002\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.989352822304,3.34876990278), test loss: 3.12539423257\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.1091651917,28.2696387408), test loss: 30.7788049698\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.01992249489,3.33119317727), test loss: 3.31445593089\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (40.0685539246,28.1254163862), test loss: 29.399352169\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.77576112747,3.31390648662), test loss: 2.39055230767\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (15.9610319138,27.9802856451), test loss: 31.2488283157\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.18231761456,3.29698137156), test loss: 3.43384513855\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (13.4216556549,27.8375723921), test loss: 29.948119545\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.79645621777,3.28036498574), test loss: 2.28451224416\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (12.6530790329,27.6977906365), test loss: 33.8539945126\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.6007361412,3.2641450383), test loss: 3.26061643064\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (9.64015293121,27.5584489444), test loss: 31.1957691669\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.37261104584,3.24824004116), test loss: 2.4540278554\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (19.444726944,27.421349683), test loss: 34.925336647\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.24698925018,3.23254656476), test loss: 3.16350354552\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (17.0470695496,27.2869532885), test loss: 27.2898090363\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (5.0030040741,3.21689526218), test loss: 2.6146494031\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (16.7142810822,27.1539948621), test loss: 33.5496268272\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.28244066238,3.20152806912), test loss: 3.08446704149\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (9.23704338074,27.0208474758), test loss: 25.0771849632\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.68734931946,3.18631911014), test loss: 2.53900330961\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (16.830488205,26.8891904763), test loss: 33.6563990593\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.40184688568,3.1716023601), test loss: 3.08097917438\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (17.5347614288,26.7610316576), test loss: 28.6852777958\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.91404366493,3.15713589225), test loss: 3.07166591585\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (21.8260707855,26.6323224866), test loss: 34.5217430115\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.19044351578,3.14285159271), test loss: 3.10557250828\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (6.42290496826,26.5053899951), test loss: 31.2156393051\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.19236540794,3.12871774186), test loss: 3.24338561296\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (21.0311698914,26.3823466899), test loss: 31.126309967\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.30037117004,3.11455200263), test loss: 2.40976777077\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (14.8286743164,26.2578229198), test loss: 31.9334905624\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.662181675434,3.10073207016), test loss: 3.34693560004\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (13.9019441605,26.1342016231), test loss: 30.8245225906\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.715517163277,3.08708737999), test loss: 2.29152424783\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (10.7937164307,26.0125833414), test loss: 35.0933115005\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.21934580803,3.0737422942), test loss: 3.26764958501\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (39.1872291565,25.8936509124), test loss: 34.9028967857\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.87734532356,3.06075955213), test loss: 2.81249266267\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (8.46853637695,25.7736704201), test loss: 35.795402813\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.59720492363,3.0477841145), test loss: 3.16840770096\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (11.8738250732,25.6568617473), test loss: 26.27585783\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.732469677925,3.03477558352), test loss: 2.5501994133\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (22.8781166077,25.5406275559), test loss: 34.3642362595\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.43871688843,3.02208680119), test loss: 3.03906766772\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (17.9140148163,25.4244187637), test loss: 26.5747490406\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.18696761131,3.00955548708), test loss: 2.86782559156\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (18.1527748108,25.3094665827), test loss: 36.3196128368\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.07805466652,2.99717479098), test loss: 3.11232121587\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (11.064125061,25.1965908177), test loss: 28.6503666878\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.92310142517,2.98504741707), test loss: 2.96553049982\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (18.2528991699,25.0840764001), test loss: 32.95866189\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.8544588089,2.97316313887), test loss: 2.69747529626\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (19.0285453796,24.9727847772), test loss: 30.8133849621\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.76967048645,2.96137044417), test loss: 3.15321755409\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (3.34125900269,24.8635747818), test loss: 32.4451225758\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.777507662773,2.94947641454), test loss: 2.42424801588\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (3.87279987335,24.754489609), test loss: 32.5977310658\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.530616343021,2.93782291413), test loss: 3.29884374738\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (25.0953712463,24.6451614449), test loss: 31.1487878323\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.18497729301,2.92634659058), test loss: 2.31557817012\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (5.12627887726,24.537480901), test loss: 36.1038323879\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.39216041565,2.91512663125), test loss: 3.26648203433\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (4.88208293915,24.4319795375), test loss: 30.0278253078\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.877881407738,2.9040576957), test loss: 2.5521896109\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (8.24211883545,24.325950111), test loss: 38.2613246441\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.85635685921,2.89314263093), test loss: 3.16186723933\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (4.6185631752,24.2215861299), test loss: 27.2447827339\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.85770559311,2.88218460017), test loss: 2.54902418256\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.69624948502,24.1185153806), test loss: 36.2452180862\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.846165180206,2.87135220509), test loss: 3.0290137127\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (8.62812614441,24.0153084582), test loss: 29.7465094566\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.42344093323,2.8606613354), test loss: 2.88028577566\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (12.8671531677,23.9125800411), test loss: 34.8659340858\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.40192353725,2.85009895981), test loss: 3.11748771966\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (4.26417541504,23.8112684107), test loss: 31.1671071053\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.667672991753,2.83972478806), test loss: 3.07681141496\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (6.41957712173,23.7109992925), test loss: 31.9052560806\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.94234764576,2.82958270465), test loss: 2.71869778633\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (10.4088478088,23.6113944416), test loss: 30.9327101231\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.3829100132,2.81944327182), test loss: 3.23129871488\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (11.3832168579,23.5131128553), test loss: 32.6740408421\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.64874601364,2.80931726688), test loss: 2.43576090112\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (5.60680961609,23.415177594), test loss: 36.233221817\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.32931673527,2.79931423015), test loss: 3.30236862302\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (6.9318151474,23.3172615144), test loss: 33.8482873917\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.21304631233,2.78943013256), test loss: 2.48320808709\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (9.08633136749,23.2201027348), test loss: 37.2578903675\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.10200428963,2.77972360683), test loss: 3.32745417953\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (4.28245973587,23.1246215898), test loss: 29.2645787239\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.262998402119,2.77014401035), test loss: 2.55138350427\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.01789093018,23.0292518809), test loss: 37.5150164127\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.67098069191,2.76074628613), test loss: 3.0931599766\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (19.2987861633,22.9349541236), test loss: 26.7813296795\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.05691671371,2.75132933508), test loss: 2.43677577078\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (4.23034381866,22.8417415148), test loss: 35.9584379673\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.801076591015,2.74189210285), test loss: 3.06327113584\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (13.280834198,22.748508188), test loss: 29.6470952511\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.876591742039,2.73264067741), test loss: 2.79250288904\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (6.66957235336,22.6550435401), test loss: 35.7720880985\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.24061453342,2.72345152667), test loss: 3.1074451685\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (13.7604694366,22.5631063469), test loss: 32.1358808279\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.48631170392,2.71448933178), test loss: 2.98597966284\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (9.99088287354,22.472517607), test loss: 33.0918747425\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.74291503429,2.70561714227), test loss: 2.7866767779\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.54896593094,22.3815372303), test loss: 32.8863851547\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.09277582169,2.69683920764), test loss: 3.26320958138\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (5.47032022476,22.2918596115), test loss: 33.967437315\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.85433292389,2.68798324119), test loss: 2.43018033803\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (22.1702861786,22.2029147413), test loss: 36.8615111828\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.84485864639,2.6792800892), test loss: 3.2363250792\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (8.07186603546,22.1134726998), test loss: 33.579831171\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.12794399261,2.67063034291), test loss: 2.57596368417\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (7.95808124542,22.0249785221), test loss: 39.4541759491\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.18847560883,2.66214520225), test loss: 3.34818126559\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (8.11940288544,21.9372845047), test loss: 30.2541622639\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.72217774391,2.65370722325), test loss: 2.57528005838\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (4.40364599228,21.8501738272), test loss: 37.5930757046\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.883006632328,2.64547959352), test loss: 3.11359810233\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (15.9415960312,21.7639511718), test loss: 27.1216453075\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.09739208221,2.63723809616), test loss: 2.4540787302\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (8.78638648987,21.6782344361), test loss: 36.4495703697\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (3.40397262573,2.62896566295), test loss: 3.06060536653\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (10.037112236,21.5929972171), test loss: 30.1165281773\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.96355581284,2.62084545553), test loss: 2.79284372926\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (6.37282752991,21.5074773345), test loss: 37.5647621155\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.84474146366,2.61271920489), test loss: 3.22594147846\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (9.42974185944,21.4227203854), test loss: 32.2905243874\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.34759402275,2.60481134333), test loss: 3.1989280045\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (8.60563468933,21.3393648396), test loss: 33.5940206528\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.3433816433,2.5969696794), test loss: 2.75130253136\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (10.8304805756,21.2558202464), test loss: 32.7897321224\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.06937563419,2.58923441656), test loss: 3.26077207923\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (9.97482967377,21.1731374903), test loss: 34.0242736816\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.29793918133,2.58149722947), test loss: 2.30443582758\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (10.6392154694,21.0914100594), test loss: 37.0990179062\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.18149781227,2.5737156636), test loss: 3.18209785819\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (8.72587776184,21.0093374409), test loss: 34.8400456905\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.629631638527,2.56609631006), test loss: 2.57714348286\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (10.0317077637,20.927663018), test loss: 39.4059954643\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.736440896988,2.55853164883), test loss: 3.22782161385\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (7.64530801773,20.8465698837), test loss: 30.7157876015\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.781531095505,2.55104225209), test loss: 2.63444232941\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (25.3047733307,20.766788277), test loss: 42.2171427727\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.23517870903,2.54377016806), test loss: 3.27517232895\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (2.89137482643,20.686770074), test loss: 27.6068536758\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.51796901226,2.53646623535), test loss: 2.51404305995\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (9.36566448212,20.6076634853), test loss: 36.6921751022\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.473013937473,2.52905642412), test loss: 3.07920114696\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (11.8069190979,20.5289117548), test loss: 30.8109798431\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (3.08085465431,2.52187537966), test loss: 2.97952933908\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (10.8780975342,20.4500863187), test loss: 38.0827653408\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.4403014183,2.51467922363), test loss: 3.13760341853\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (11.1037693024,20.3719586595), test loss: 35.2866220951\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.800166785717,2.5075812691), test loss: 3.22991847694\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (5.74553012848,20.294530963), test loss: 34.7755290031\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.13455986977,2.50056835748), test loss: 2.58273649067\n",
      "\n",
      "MC # 4, Hype # hyp5, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold2/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (337.421569824,inf), test loss: 188.322025299\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (410.04989624,inf), test loss: 474.920243835\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (41.6848754883,99.662234087), test loss: 49.3469465256\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.65894842148,135.267345662), test loss: 3.80443761349\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (51.2217178345,74.267168951), test loss: 39.9123156309\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.13033998013,69.2729254827), test loss: 3.00346179008\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (14.7829036713,65.6478822947), test loss: 47.5236746073\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.98568546772,47.2501558437), test loss: 3.89016975164\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (22.64270401,61.3994893892), test loss: 43.2317611694\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.24032640457,36.2348377021), test loss: 3.89699437916\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (47.1485786438,58.7566375479), test loss: 49.083969593\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.27313232422,29.6281525046), test loss: 3.66984316707\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (57.7218704224,56.9582259269), test loss: 45.971703434\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.96677660942,25.2261929208), test loss: 3.85799198747\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (19.1735229492,55.6058156543), test loss: 45.1379426956\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.42262494564,22.0736494027), test loss: 2.698608464\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (38.1670227051,54.5246827778), test loss: 47.4265327454\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.2761015892,19.7060201454), test loss: 4.13251498342\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (32.7316398621,53.661098641), test loss: 44.3211388111\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.6459915638,17.8647889128), test loss: 2.62393943667\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (131.359085083,52.9340737763), test loss: 46.1807253838\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.06793260574,16.3854139825), test loss: 4.06605819464\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (58.1780815125,52.362557931), test loss: 40.5431322098\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.6416144371,15.1750197079), test loss: 2.6964089781\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (79.0399475098,51.8390085234), test loss: 48.7087594032\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (5.68309545517,14.1658594911), test loss: 3.84619646072\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (44.5106887817,51.3698204022), test loss: 40.563541317\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (4.44226169586,13.3092200632), test loss: 2.76343080401\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (31.5991630554,50.895477761), test loss: 45.5861441612\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.28213977814,12.571854462), test loss: 3.46700718403\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (98.1361694336,50.4279450985), test loss: 35.1693390846\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.88890433311,11.9299229969), test loss: 2.83063830733\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (36.2659492493,50.0028497269), test loss: 44.6221256733\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.14220094681,11.3664654306), test loss: 3.52061969042\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (13.0859565735,49.5788561174), test loss: 40.7214903831\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.88957500458,10.86592608), test loss: 3.658692801\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (38.7945289612,49.2038271707), test loss: 45.2074722409\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (6.39778709412,10.4216816957), test loss: 3.30938954055\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (13.5353212357,48.8198289884), test loss: 43.0693612576\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.47576904297,10.0236075375), test loss: 3.63884490728\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (30.1858081818,48.4348865882), test loss: 40.0744993687\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.903868317604,9.66320020315), test loss: 2.6182302177\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (45.3287811279,48.0451553419), test loss: 42.956939888\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.02068686485,9.33666227772), test loss: 3.81960482597\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (35.479473114,47.6562962793), test loss: 36.9559603691\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.53139877319,9.03872634712), test loss: 2.39480326772\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (47.8729782104,47.2696764168), test loss: 41.160386467\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.87831604481,8.76413226594), test loss: 3.61047805548\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (24.0886802673,46.8946165342), test loss: 34.934845233\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.793395519257,8.51147214591), test loss: 2.57512479573\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (22.9060440063,46.5085737106), test loss: 43.4113675594\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.43323945999,8.2789976922), test loss: 3.59454982281\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (28.1097755432,46.1208801135), test loss: 32.0132555962\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.48999810219,8.06433936181), test loss: 2.56748365313\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (52.3938980103,45.7184365511), test loss: 39.1092785358\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.25342702866,7.86367923263), test loss: 3.32252342701\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (20.1211566925,45.2902957519), test loss: 33.6480968475\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.7135822773,7.67514044496), test loss: 3.41491422951\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (38.6544647217,44.8628008419), test loss: 37.6605951786\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (6.36619520187,7.49865804203), test loss: 3.38010830283\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.1812648773,44.4235151087), test loss: 34.0338575125\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.87144422531,7.33194450218), test loss: 3.51567946821\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (60.9247360229,43.998390369), test loss: 30.8536004066\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (4.80198669434,7.17577617534), test loss: 2.71427112296\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (25.4389305115,43.5585044347), test loss: 36.2654524803\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.90963459015,7.02910175361), test loss: 3.66249120533\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (18.9503879547,43.1185669991), test loss: 31.9003176689\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.15827047825,6.89091313308), test loss: 2.33740884662\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (36.1201438904,42.6708714331), test loss: 36.9533521891\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.07135391235,6.76045515393), test loss: 3.69221122265\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (22.0231666565,42.2216436252), test loss: 27.3422007322\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.94690227509,6.63675648852), test loss: 2.25705401599\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (107.338546753,41.7893714486), test loss: 36.9302954912\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (4.1349439621,6.51906878617), test loss: 3.5706020236\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (23.9561405182,41.3552358783), test loss: 27.8090594769\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.83644866943,6.40634343439), test loss: 2.44303383529\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (47.3440055847,40.9365472098), test loss: 36.8358463287\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.76815032959,6.29907142018), test loss: 3.31626000106\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (14.0148191452,40.521432377), test loss: 25.3480133057\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.88042867184,6.19709854373), test loss: 2.48642857969\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (22.4648933411,40.1170357323), test loss: 34.5094223022\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.656603336334,6.09974576885), test loss: 3.37953628898\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (13.0270586014,39.7139020481), test loss: 30.7327780485\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.62224769592,6.00679726088), test loss: 3.38486686051\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (26.3611812592,39.3223812847), test loss: 33.0615869522\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.49057626724,5.91781532126), test loss: 3.29989757538\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (7.96905469894,38.9441642335), test loss: 31.1236540318\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.03469514847,5.83194915717), test loss: 3.46272351742\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (28.9116573334,38.5767311497), test loss: 27.8506328344\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.05085277557,5.74972672867), test loss: 2.48719318062\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (7.32842969894,38.2150141847), test loss: 34.9514128685\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.19656825066,5.67105944703), test loss: 3.62764227092\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (47.5148506165,37.8674618311), test loss: 28.4518177986\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.9347205162,5.59603129998), test loss: 2.21571771204\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (38.0899429321,37.5295478297), test loss: 34.513653326\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.68666315079,5.52395158237), test loss: 3.53099722564\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (85.7923736572,37.1920078429), test loss: 28.1145405769\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.79338788986,5.45413136332), test loss: 2.29154331386\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.1584205627,36.8679442047), test loss: 36.5912513733\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.63205218315,5.38695745272), test loss: 3.44633046687\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (9.63731956482,36.552255774), test loss: 26.7693695068\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.71742296219,5.32186492747), test loss: 2.40691324919\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (13.9449291229,36.2467319816), test loss: 34.8161854982\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.77487528324,5.25913755411), test loss: 3.23695288897\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (16.1999168396,35.9478011725), test loss: 24.9319468021\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.882212340832,5.19872338528), test loss: 2.37825842202\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.5335664749,35.6573760005), test loss: 34.3511724472\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.84607732296,5.14056702497), test loss: 3.19397084713\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (37.8858413696,35.3724694537), test loss: 31.3633097649\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.72105789185,5.0843059204), test loss: 3.27533212453\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (11.3137645721,35.0917826975), test loss: 33.8890232205\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.61357665062,5.02985568456), test loss: 3.241924496\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (7.9044418335,34.822317849), test loss: 31.3544344664\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.55435061455,4.9768275376), test loss: 3.48655333817\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (14.7060441971,34.5583495134), test loss: 29.7684880733\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.82587683201,4.92524420375), test loss: 2.43198349923\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (32.6875839233,34.301499017), test loss: 33.7176418781\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.44225502014,4.87544219907), test loss: 3.51841398478\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.163564682,34.0490181063), test loss: 28.7680674791\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.576482951641,4.82738746226), test loss: 2.22342507094\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.2206611633,33.8046718931), test loss: 33.9640151024\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.754615485668,4.78084322022), test loss: 3.4063275069\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (26.2334327698,33.5607013768), test loss: 28.9910462856\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.76222550869,4.73556582477), test loss: 2.33500658572\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (8.64920711517,33.3230171383), test loss: 35.4965193748\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.08069884777,4.69161574874), test loss: 3.28602159321\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.8477573395,33.0928678481), test loss: 26.445300746\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.32717037201,4.64838997639), test loss: 2.37481589913\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (17.4616737366,32.8659078665), test loss: 33.6562428236\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.55175900459,4.60647595033), test loss: 2.9967481643\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (12.671546936,32.6422287592), test loss: 24.5999961853\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.25348651409,4.56580449271), test loss: 2.32864150852\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (24.2135028839,32.424894115), test loss: 34.9972246647\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.736712992191,4.52649709314), test loss: 3.1296048224\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (5.6328959465,32.2136761548), test loss: 33.0078858852\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.15769934654,4.48824532443), test loss: 3.48297718018\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (3.60315942764,32.0000321788), test loss: 34.1644059658\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.32922255993,4.45084939591), test loss: 3.12683996856\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (6.64912700653,31.7956733358), test loss: 32.0366888046\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.39459943771,4.41428896451), test loss: 3.3418297261\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (50.9527893066,31.5931340947), test loss: 29.9514073253\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (4.12327480316,4.37849891216), test loss: 2.23982586563\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.0781784058,31.3942902746), test loss: 33.0051213741\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.966091215611,4.34361899959), test loss: 3.49501736462\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (25.7822227478,31.1981878358), test loss: 30.6703622341\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.20305871964,4.30966148074), test loss: 2.17454419583\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (10.8422574997,31.0068695543), test loss: 33.9367297173\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.8989648819,4.27670313848), test loss: 3.33842925429\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (12.4174394608,30.8171923023), test loss: 31.3828709602\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.595851421356,4.24446881209), test loss: 2.32807741165\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (15.5115890503,30.6307431674), test loss: 35.3118212342\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.78862977028,4.21307641702), test loss: 3.16057560444\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (12.1932544708,30.4485467206), test loss: 26.1869282246\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.6051363945,4.18210136772), test loss: 2.36713365018\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (6.6062502861,30.26819293), test loss: 33.6181238651\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.820575416088,4.15172441089), test loss: 2.94338033795\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (9.45499038696,30.0908511593), test loss: 25.1449271917\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.33934116364,4.12208475915), test loss: 2.41811269522\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (36.6634254456,29.916176636), test loss: 34.2316550732\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.9228836298,4.09331381524), test loss: 3.21743259132\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (16.9559860229,29.7460218282), test loss: 30.5183341026\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.0568125248,4.06520715615), test loss: 3.27524745166\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (14.1518182755,29.5747996288), test loss: 35.3938849449\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.965159773827,4.03765730165), test loss: 3.14140084386\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (20.1142940521,29.4081262866), test loss: 31.3483724117\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.22081422806,4.01073042652), test loss: 3.29152132273\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (9.51184368134,29.2443144939), test loss: 30.8220839977\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.27373099327,3.98397132564), test loss: 2.24571607709\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (10.3129844666,29.0816429667), test loss: 33.95303545\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.3440772295,3.95790482017), test loss: 3.38219847381\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (4.96045970917,28.9205741459), test loss: 30.1614244699\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.639343202114,3.93237685287), test loss: 2.08428442478\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (15.4252729416,28.7630455418), test loss: 35.1629417658\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.96448135376,3.90757068328), test loss: 3.26848720014\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (16.3438873291,28.6095883006), test loss: 33.2469833374\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.210747689009,3.8832641725), test loss: 2.63672258556\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (7.47096776962,28.454092088), test loss: 35.9395296097\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.91272902489,3.85938579737), test loss: 3.13812675178\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (4.87318515778,28.3038417796), test loss: 25.8204128981\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.95161736012,3.83578690441), test loss: 2.35313330591\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.67035484314,28.153450412), test loss: 34.3405074596\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.96677970886,3.81264659583), test loss: 3.08559995592\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.5383243561,28.0063793298), test loss: 26.7836968899\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.85892605782,3.78996088747), test loss: 2.84289520681\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (20.4754562378,27.8597661427), test loss: 35.5347854614\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.840694189072,3.76766947862), test loss: 3.1383938536\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (9.53059005737,27.7167357423), test loss: 29.5694597721\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.64233231544,3.74596258943), test loss: 3.13473404944\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (16.3733730316,27.5744541321), test loss: 31.7468269825\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.381808340549,3.72465383011), test loss: 2.60193707943\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (33.1313667297,27.4344559488), test loss: 32.5963050842\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.06134080887,3.70376288707), test loss: 3.35974810719\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (10.8662223816,27.2963308776), test loss: 31.0008519888\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (3.87999343872,3.68301281885), test loss: 2.28845499158\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (13.765414238,27.1591666948), test loss: 35.4923410892\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.06803107262,3.66261044784), test loss: 3.38408494592\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (13.9000520706,27.0234809534), test loss: 29.6942029953\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.34080529213,3.64249092485), test loss: 2.10711897314\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (10.0229110718,26.8891617458), test loss: 37.095070982\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.83225250244,3.62295613876), test loss: 3.2903031826\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (8.90121936798,26.7588178167), test loss: 27.2275126934\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.44446778297,3.60377760572), test loss: 2.29444929361\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (15.5596971512,26.6270875183), test loss: 37.809526968\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.62766575813,3.58490840108), test loss: 3.14282799661\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (9.74132728577,26.4985036945), test loss: 25.3734931469\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.38207817078,3.56633000615), test loss: 2.25926451981\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.56366062164,26.3715737431), test loss: 35.4091788292\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.54408931732,3.54782547156), test loss: 3.05380901992\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (18.8835887909,26.24488196), test loss: 30.5326704741\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.752772569656,3.52970870898), test loss: 2.98222199082\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (15.845454216,26.1193131604), test loss: 34.5828740597\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.884877204895,3.51186901419), test loss: 3.08457648903\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (11.6063299179,25.9958494849), test loss: 30.8680584192\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.23657214642,3.49440576261), test loss: 3.09883621633\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (33.3091697693,25.8751971625), test loss: 34.0700180054\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.64197301865,3.47734993581), test loss: 2.86604091823\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (12.2812986374,25.7534724078), test loss: 34.4352107048\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.81937134266,3.46045492906), test loss: 3.38819722235\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (15.5195121765,25.634874474), test loss: 31.1706156254\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.41612040997,3.44366748779), test loss: 2.27294036746\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (12.3550491333,25.5159116882), test loss: 37.2551207542\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.74165141582,3.42718217242), test loss: 3.36818705499\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (19.3563098907,25.3989999883), test loss: 30.294096756\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.02323198318,3.41093703481), test loss: 2.31168861389\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (8.10939979553,25.282267934), test loss: 39.2298689842\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.873935580254,3.39492623178), test loss: 3.36696671844\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (6.24350070953,25.1682973569), test loss: 26.6513072014\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.924229562283,3.37923374163), test loss: 2.31434624493\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (3.82339787483,25.0546537373), test loss: 37.4620353699\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.78183817863,3.36384915495), test loss: 3.08592133522\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (6.11542272568,24.9425055544), test loss: 25.2861968994\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.58912563324,3.34865880763), test loss: 2.32751568854\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (20.2953529358,24.8317483326), test loss: 35.6748693705\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (4.78044986725,3.3334813921), test loss: 3.05805682242\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (10.3562202454,24.7211962248), test loss: 30.7556153774\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (4.21087932587,3.31859672978), test loss: 3.04447883368\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (23.317905426,24.6115845693), test loss: 36.556470561\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.64175224304,3.30380929794), test loss: 3.23298515379\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (12.7543334961,24.5029085445), test loss: 31.7766732216\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.09503388405,3.28939559421), test loss: 3.31729312241\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.83869361877,24.3971343323), test loss: 31.6808246613\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.10418367386,3.27521308198), test loss: 2.58407280147\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (5.81544351578,24.2900824734), test loss: 34.3735523701\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.2188770771,3.26123008542), test loss: 3.36086186171\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (7.7205657959,24.1854950178), test loss: 30.8849600554\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.41279959679,3.24738794947), test loss: 2.13465759456\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (16.392829895,24.0817562728), test loss: 37.4688352585\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.65026545525,3.23356804531), test loss: 3.31735877693\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (9.07512760162,23.9779005462), test loss: 30.6218416214\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.77566349506,3.22001316295), test loss: 2.31530245543\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (11.051735878,23.8750259118), test loss: 39.6471544743\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.70467865467,3.20659638888), test loss: 3.28905337453\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (8.61606407166,23.7735257812), test loss: 27.6905431509\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.0498945713,3.19341606074), test loss: 2.42620371878\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (22.5632400513,23.6736452259), test loss: 38.9121064186\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.730463981628,3.18049993496), test loss: 3.08512711823\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (4.54830741882,23.5737364956), test loss: 25.4905324936\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.701612353325,3.16770708889), test loss: 2.35387421548\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (9.38829708099,23.4753891106), test loss: 36.2460315228\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.973043084145,3.15492879372), test loss: 3.10094837844\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (19.5116443634,23.3768826217), test loss: 31.0733578444\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.823201298714,3.1423728365), test loss: 2.98938320279\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (12.3778162003,23.2795612707), test loss: 36.8790055752\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.83280193806,3.12992523), test loss: 3.16282515824\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (11.7811422348,23.1825212235), test loss: 35.3423467159\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.922017753124,3.11765870156), test loss: 3.3353433758\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (10.6432800293,23.0873616267), test loss: 33.3097064018\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.80702209473,3.10557634923), test loss: 2.52315461338\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (23.810956955,22.9924771213), test loss: 34.5255934715\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.37240993977,3.09370732763), test loss: 3.37488964051\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (13.1260623932,22.8986347245), test loss: 31.7699587822\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.15161550045,3.08197716392), test loss: 2.23330758214\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (5.31529712677,22.8053583453), test loss: 37.9354062557\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.751913905144,3.07013752705), test loss: 3.35342730582\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (14.2607736588,22.7126142768), test loss: 33.3489256859\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.95392203331,3.05858634612), test loss: 2.52009757161\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (17.3093910217,22.6200863359), test loss: 38.6168327808\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.05108976364,3.04708502269), test loss: 3.19124316573\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (2.9789454937,22.5284928908), test loss: 27.810652566\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.816600263119,3.035807429), test loss: 2.43042740077\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (5.1910238266,22.4390231453), test loss: 36.3362318993\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.41226458549,3.02470226963), test loss: 2.96040637493\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (9.14729213715,22.3484874969), test loss: 26.9809900522\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.46139574051,3.01372649333), test loss: 2.44032666385\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.21880102158,22.2597898725), test loss: 36.4087527275\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.66363072395,3.00279805552), test loss: 3.18031691909\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (20.0781669617,22.1714439544), test loss: 31.3320163727\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.915175616741,2.99190698905), test loss: 3.17789815962\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (9.40355300903,22.083203455), test loss: 37.8630393028\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.26295268536,2.98118260684), test loss: 3.16164783239\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (10.9178714752,21.9955232015), test loss: 34.3316268682\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.73238563538,2.97053862546), test loss: 3.30536462963\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (12.2268238068,21.9088961845), test loss: 34.6172610521\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.08684706688,2.96006014265), test loss: 2.43601994216\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (6.31304550171,21.823245422), test loss: 35.0382390976\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.30190730095,2.94977568209), test loss: 3.37916911244\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (9.79246616364,21.7379746949), test loss: 33.1213779211\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.40119695663,2.93956226155), test loss: 2.27319345325\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (8.06815910339,21.6534878678), test loss: 40.0178429604\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.601693153381,2.92932104116), test loss: 3.43396905363\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (12.3141098022,21.5688720798), test loss: 31.670531702\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (3.25014948845,2.91926963886), test loss: 2.49706439674\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (6.20532417297,21.4852752351), test loss: 39.024096489\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.65220808983,2.90924404028), test loss: 3.16733720005\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (11.1987504959,21.4017289848), test loss: 27.7463716507\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.23149895668,2.8993809106), test loss: 2.44694137573\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.27289247513,21.3196434166), test loss: 37.1972891808\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.393840461969,2.88961151429), test loss: 2.924036403\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.45904588699,21.2376050466), test loss: 26.7939662933\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.367744863033,2.88002463525), test loss: 2.51086580902\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.98350381851,21.1567156909), test loss: 38.0909730911\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.80203962326,2.87052096821), test loss: 3.1920488894\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.48581981659,21.0760081174), test loss: 32.4981442928\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.664409637451,2.86091088969), test loss: 3.16918718815\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (4.72930765152,20.9956593818), test loss: 38.2534810305\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.43403711915,2.85151365289), test loss: 3.0089732796\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (10.8695116043,20.9154401432), test loss: 34.7771241188\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.821360588074,2.84214118596), test loss: 3.30539548993\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (3.67595481873,20.8361043535), test loss: 35.2918343067\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.67182052135,2.8329484854), test loss: 2.48950972855\n",
      "\n",
      "MC # 4, Hype # hyp5, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold3/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (327.077148438,inf), test loss: 172.111526489\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (355.017028809,inf), test loss: 428.947509766\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (27.8663272858,88.6448303947), test loss: 44.4877717018\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.76585102081,114.012144926), test loss: 3.98599659801\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (50.4519615173,66.9046624775), test loss: 31.2049734592\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.026065588,58.6371384551), test loss: 2.76693662405\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (15.8407154083,59.4312610121), test loss: 42.3169947147\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.935164153576,40.1697763079), test loss: 4.00037870407\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (28.0932769775,55.7715754584), test loss: 34.7764253616\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.67124402523,30.9295859492), test loss: 3.64321461916\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (86.7363739014,53.5018197923), test loss: 44.4171466827\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.53670036793,25.3854699609), test loss: 3.94841184616\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (54.5582809448,51.8503300229), test loss: 38.061320734\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.19690084457,21.6889391451), test loss: 3.63809925914\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (26.9509887695,50.6419521813), test loss: 41.87059021\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.71576762199,19.0404342417), test loss: 3.21606982648\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (39.8086242676,49.6805366633), test loss: 40.8042433262\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.71154689789,17.049869532), test loss: 3.73004677892\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (18.3454093933,48.8639356223), test loss: 39.8805450678\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.98624944687,15.5003770863), test loss: 2.94663257003\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (41.1319847107,48.1573558848), test loss: 38.2427583218\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.87949442863,14.2546872853), test loss: 3.75434070826\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (68.8600997925,47.575887321), test loss: 33.3774991989\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.41861152649,13.2365360247), test loss: 2.74521281123\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (47.7956848145,46.9793799082), test loss: 40.8146141291\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.53399729729,12.3870329695), test loss: 3.83323767781\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (22.6453037262,46.3785638007), test loss: 32.3244953632\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.07496166229,11.6654590607), test loss: 2.79840381742\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (91.6469192505,45.8139473917), test loss: 39.6783037663\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.83290886879,11.0455453324), test loss: 3.69480450153\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (26.3244018555,45.2540915056), test loss: 30.3274598122\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.736646413803,10.5051673628), test loss: 3.25710280836\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (85.3387298584,44.734731422), test loss: 38.3055257082\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.35185432434,10.0310455454), test loss: 3.87003855109\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (17.2664718628,44.2025590963), test loss: 30.9562901258\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.02395665646,9.61009873248), test loss: 3.52586964369\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (14.823846817,43.7153729118), test loss: 33.5419723034\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.72334051132,9.23693461473), test loss: 3.12176423073\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (25.3462028503,43.2092997859), test loss: 33.96584692\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.30313158035,8.90129569144), test loss: 3.39822711796\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (42.8186454773,42.7099834073), test loss: 34.9163310528\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.31552267075,8.59778856588), test loss: 2.9184537828\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (20.6174087524,42.2034498478), test loss: 34.3057785034\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.69888925552,8.32207144226), test loss: 3.55295582414\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (38.8017959595,41.6998254794), test loss: 30.4447757244\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.918609619141,8.0698314945), test loss: 2.73942525089\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (39.6337013245,41.1996276955), test loss: 33.1567025661\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (5.84007740021,7.83832999428), test loss: 3.47346054316\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (20.2385730743,40.7167957007), test loss: 27.7010974884\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.9319767952,7.62471669443), test loss: 2.79706567526\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (65.5881347656,40.2356732878), test loss: 36.514125824\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.01558351517,7.42855584426), test loss: 3.6553219974\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (13.6365280151,39.7482605161), test loss: 23.9742666245\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.82644033432,7.24763899582), test loss: 2.68251130283\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (14.6494626999,39.2746354526), test loss: 34.0559252262\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.39933383465,7.07874811691), test loss: 3.7047824949\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (27.7892723083,38.8048191428), test loss: 27.5940352201\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.54231023788,6.92095303781), test loss: 3.3223747015\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (14.4024810791,38.3420518704), test loss: 34.4744029522\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.8694807291,6.77353541719), test loss: 3.8800001502\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (24.2226486206,37.8968433298), test loss: 27.6217673779\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.81110203266,6.63440453556), test loss: 3.41601033658\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (18.8045368195,37.4667605525), test loss: 28.4815466166\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.06226563454,6.50456090143), test loss: 3.10375395566\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (52.4256744385,37.0429506533), test loss: 31.9049696922\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.68649077415,6.38267784414), test loss: 3.5442789495\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (13.2201900482,36.6297470456), test loss: 30.8203984261\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.59763813019,6.2678901053), test loss: 2.80489457548\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (28.3989944458,36.2370716386), test loss: 32.1489385128\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.29801654816,6.15963833717), test loss: 3.49461533427\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (56.557056427,35.8478942088), test loss: 25.958893609\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.55755138397,6.05629490619), test loss: 2.64431185722\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (24.4642715454,35.476197968), test loss: 33.5481177807\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.30750179291,5.9583965815), test loss: 3.39287695885\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (15.7653512955,35.1146210395), test loss: 26.3412893772\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.05397200584,5.86500015516), test loss: 2.79557655156\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (16.250837326,34.7694533365), test loss: 35.6331239462\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.40178585052,5.77704207952), test loss: 3.71912118793\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (3.47911882401,34.4308401388), test loss: 23.3930192947\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.91337382793,5.69320366067), test loss: 2.66703689694\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (10.3524093628,34.1073124725), test loss: 34.7330617905\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.9126329422,5.61331405874), test loss: 3.75623085499\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (25.0308456421,33.79198331), test loss: 27.3583150625\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.54538345337,5.5370333083), test loss: 3.42621461004\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (35.9602203369,33.4856312253), test loss: 35.1407924652\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.83173334599,5.46362196694), test loss: 3.84586892724\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (16.6307983398,33.1929476127), test loss: 27.8094228268\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.39181232452,5.39347794064), test loss: 3.46993895769\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (23.6445541382,32.911168106), test loss: 29.2385418415\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (6.38970565796,5.32592260746), test loss: 2.97182711065\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (9.67155265808,32.6364008684), test loss: 31.4636188984\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.49024868011,5.26154496932), test loss: 3.53585991859\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.2892627716,32.3658932336), test loss: 30.8462125778\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.97759783268,5.20004671061), test loss: 2.71395598352\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (22.0716781616,32.1109898239), test loss: 32.6329704762\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.11872625351,5.14092943078), test loss: 3.37858934402\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (28.8727493286,31.8583007008), test loss: 28.697361207\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (5.2574262619,5.08387747016), test loss: 2.67169438004\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.4994821548,31.6132361863), test loss: 33.6690448761\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.78760766983,5.0286877819), test loss: 3.4606562078\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.7026157379,31.3782937489), test loss: 26.9488869667\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.55401873589,4.97513483746), test loss: 2.7579313308\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (16.1241207123,31.1499310684), test loss: 35.0429438591\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.880240440369,4.92371471126), test loss: 3.6021237731\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (9.36408042908,30.9237359618), test loss: 22.9255717516\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.546359300613,4.87434084282), test loss: 2.50025498122\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (16.7242393494,30.7048129318), test loss: 35.4801933289\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.86625623703,4.826859747), test loss: 3.75216997862\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (21.1948986053,30.4963239502), test loss: 29.3699177265\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.06457948685,4.78095459303), test loss: 3.38590964377\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (39.094581604,30.287651987), test loss: 35.8016983032\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.9565410614,4.73617882992), test loss: 3.81516249478\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (3.39517903328,30.0862968767), test loss: 28.1683471203\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.80887818336,4.69268523803), test loss: 3.40682335198\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (11.1585063934,29.8890095375), test loss: 30.7500368118\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.92836332321,4.65035224813), test loss: 2.85663739443\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (23.6398258209,29.697671717), test loss: 31.0473499298\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.844861745834,4.60975048869), test loss: 3.52416266799\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (18.9906272888,29.5073073304), test loss: 31.4850120068\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.27248382568,4.57031056517), test loss: 2.68970823288\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (18.6784820557,29.3248794801), test loss: 31.7146967173\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.88911771774,4.53221743516), test loss: 3.29951385856\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (21.8023719788,29.1440276385), test loss: 30.0447505236\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.697895765305,4.49509641295), test loss: 2.67046171129\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (29.9842834473,28.967346188), test loss: 32.682504797\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.73591911793,4.4587951747), test loss: 3.32175135911\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.1175460815,28.7948627864), test loss: 26.5832498074\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.77340221405,4.42353880622), test loss: 2.70678451359\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (18.4089984894,28.6272735296), test loss: 34.5184592247\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.63808226585,4.38900815698), test loss: 3.52484197021\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (10.3837680817,28.460979615), test loss: 22.5132601261\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.73857069016,4.3555398258), test loss: 2.49858587086\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (36.3550376892,28.2966028154), test loss: 34.9220665455\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.41291713715,4.32295125708), test loss: 3.60644048154\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (27.4307022095,28.1393223116), test loss: 27.2722995281\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.622217237949,4.29121948491), test loss: 3.30811195374\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (4.52671337128,27.9806845677), test loss: 35.5352136135\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.821638822556,4.26012828594), test loss: 3.46739591956\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (17.4910736084,27.8267358654), test loss: 28.2395599365\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.70970726013,4.22968572212), test loss: 3.30791372955\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (9.69991970062,27.6762313371), test loss: 31.7317456245\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.801461696625,4.19966300655), test loss: 2.80717083663\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (17.1711425781,27.5286975047), test loss: 31.2826763868\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.28629684448,4.17054268051), test loss: 3.39243144393\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (7.24088335037,27.3798835882), test loss: 31.4027827263\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.51933944225,4.14221542939), test loss: 2.61984503865\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (15.3828678131,27.2358119107), test loss: 31.5027235031\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.48286771774,4.11468653801), test loss: 3.17984108031\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (23.387928009,27.0962730044), test loss: 32.1670285225\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.40743803978,4.08788273959), test loss: 2.75449483842\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (3.93927145004,26.9554826349), test loss: 34.1925211191\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.64155340195,4.06127592842), test loss: 3.31230769753\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (8.20348739624,26.8179442282), test loss: 25.76010499\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.65010118484,4.03526740067), test loss: 2.64827958941\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.5146799088,26.6825549204), test loss: 34.0365613461\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (5.03967428207,4.00971776341), test loss: 3.42549756467\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (40.99817276,26.5501397225), test loss: 28.1515417099\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.36863994598,3.98487725505), test loss: 3.13927967846\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.4197425842,26.4160290047), test loss: 35.320593977\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.911543011665,3.96047455228), test loss: 3.5577568531\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.7564239502,26.2874312972), test loss: 26.8905216694\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.39734768867,3.93683131962), test loss: 3.15857620388\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (10.8839626312,26.1586758409), test loss: 29.6561523438\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.49143576622,3.9135197056), test loss: 2.85610417575\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (14.8054389954,26.0330685475), test loss: 29.6552676916\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.683953464031,3.89054373881), test loss: 3.24988519251\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.10169410706,25.9081098242), test loss: 32.3525171757\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.71120560169,3.86803585645), test loss: 2.72226315141\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (7.51475572586,25.7863466695), test loss: 31.8953592777\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.02329039574,3.84581595964), test loss: 3.23108547628\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (26.8490352631,25.6643599249), test loss: 29.7085151434\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.22153759003,3.82414380765), test loss: 2.45359675586\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (29.5857315063,25.543434314), test loss: 32.3776180983\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.02687573433,3.80296565918), test loss: 3.17120512426\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (18.1867675781,25.4261837204), test loss: 28.0989892006\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.994743108749,3.78224415335), test loss: 2.57133262157\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.8805389404,25.307907771), test loss: 34.7963427067\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.24111104012,3.76180798038), test loss: 3.21341306567\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (15.8571014404,25.1923279752), test loss: 24.5465962887\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.27553069592,3.74166123411), test loss: 2.49201173037\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (7.87599802017,25.0783272), test loss: 34.5313921928\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.29844665527,3.72169501659), test loss: 3.3671895802\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (8.9772605896,24.9659965923), test loss: 27.9343537807\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.39832997322,3.70216939794), test loss: 3.03580248654\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (10.9746809006,24.8522532238), test loss: 35.9799822807\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.753198802471,3.68298663529), test loss: 3.53800738156\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (20.0604991913,24.7419699579), test loss: 28.5363945484\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.923430800438,3.66430983462), test loss: 2.98130382001\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (30.6512641907,24.6338794085), test loss: 31.24781003\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.37578845024,3.64599965128), test loss: 2.8118740499\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (8.41660118103,24.5249634009), test loss: 30.403150177\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.22256827354,3.62774953127), test loss: 3.19183119982\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (4.83350467682,24.4174869186), test loss: 33.0006828785\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.25436329842,3.609759026), test loss: 2.68144675791\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (4.49282598495,24.3117835303), test loss: 32.4109201431\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.73823726177,3.59203688248), test loss: 3.2015075475\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (15.0250892639,24.207123476), test loss: 29.0653851509\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.22260737419,3.57466945212), test loss: 2.40282595456\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (9.43563842773,24.1016013181), test loss: 33.8933528423\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.83953046799,3.5575216022), test loss: 3.0456356883\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (11.8094787598,23.9990504185), test loss: 27.1439219475\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.70584487915,3.5408111235), test loss: 2.48769622445\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (29.7024307251,23.8969965793), test loss: 36.2170749187\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.90515685081,3.52430002051), test loss: 3.21710070968\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (10.3643283844,23.7962176611), test loss: 24.5859338284\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.42743778229,3.50793174758), test loss: 2.41925281584\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (32.7953453064,23.6961590429), test loss: 34.8774261475\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.47896742821,3.49178697458), test loss: 3.37287361026\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (5.60136079788,23.59730825), test loss: 28.2666113377\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.41154837608,3.4758183669), test loss: 2.87054381669\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (15.7451848984,23.4984293248), test loss: 36.186198616\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.648250103,3.4601303395), test loss: 3.45526778996\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (8.35340499878,23.4002628278), test loss: 27.2928693295\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.985313475132,3.44472937779), test loss: 3.08240964413\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (4.59311008453,23.3041286351), test loss: 32.4639541149\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.52363717556,3.42959152074), test loss: 2.82841374278\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (17.976650238,23.2080878085), test loss: 30.5431571484\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.17784690857,3.41465537156), test loss: 3.14093360603\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (15.682598114,23.1128220956), test loss: 32.4615801334\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.07649457455,3.39982762569), test loss: 2.49812462628\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.1514492035,23.019011494), test loss: 32.4510694027\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.43167209625,3.38510721115), test loss: 2.99495761991\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (30.3432159424,22.9259480705), test loss: 30.454307127\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (3.58243703842,3.37066194491), test loss: 2.50659048557\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (14.6752195358,22.8317389752), test loss: 34.2175112247\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.58605051041,3.35634784325), test loss: 3.06294456422\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (19.7707118988,22.7398167531), test loss: 27.0243753433\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.6962954998,3.34239728475), test loss: 2.53294178247\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (11.4988536835,22.6493224193), test loss: 35.4512874126\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.0796097517,3.32863685866), test loss: 3.19640015662\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (7.8535323143,22.5582602678), test loss: 24.7012413502\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.72963064909,3.31494457192), test loss: 2.34074625671\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (11.936498642,22.4680752104), test loss: 35.5868668079\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (5.77167415619,3.3014339722), test loss: 3.37305378914\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (7.92030954361,22.3792186008), test loss: 26.8427937031\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.72624349594,3.28800067753), test loss: 2.90475863814\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (9.23239707947,22.290529008), test loss: 38.1519491673\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.25358438492,3.27477578072), test loss: 3.37868608236\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (13.3908262253,22.2017789284), test loss: 28.8843645811\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.68204665184,3.26171386376), test loss: 3.00705692172\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (6.21459054947,22.1143383297), test loss: 34.9640309334\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.822017908096,3.24890508141), test loss: 2.70006129146\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (15.4683952332,22.0279499419), test loss: 30.1082975864\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.84302020073,3.2362715363), test loss: 3.11656586528\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (13.4650211334,21.9418075113), test loss: 33.4531049728\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.31703233719,3.22369499942), test loss: 2.55581089556\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (16.2293815613,21.8563665513), test loss: 32.9207281351\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.21484720707,3.21119831451), test loss: 2.99329673052\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (9.11610412598,21.7714140919), test loss: 32.688173914\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.30890715122,3.19887934298), test loss: 2.41358040273\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (7.98512744904,21.6866625402), test loss: 34.8882581234\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.3151845932,3.18669609864), test loss: 2.99216627777\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (15.4703550339,21.6023254995), test loss: 27.2251612663\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.51029634476,3.17471616269), test loss: 2.54801198542\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (1.29177200794,21.5191400412), test loss: 35.9526148319\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.373143404722,3.16288082459), test loss: 3.15344721973\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (5.05944299698,21.436445326), test loss: 24.5917768478\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.23545789719,3.15125739224), test loss: 2.55997874141\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (6.11433696747,21.3539756953), test loss: 36.1979598045\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.3912987709,3.13962992714), test loss: 3.27775204182\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.75372219086,21.2727631682), test loss: 27.4172884464\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.28122091293,3.1280597647), test loss: 2.83301508129\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (4.40324735641,21.1915626358), test loss: 35.6182703495\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.318228900433,3.11667563102), test loss: 2.93779464364\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (24.1941280365,21.1102757763), test loss: 28.8011460781\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.95644271374,3.10538743626), test loss: 2.92336802185\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (3.9323952198,21.0294555737), test loss: 36.0484144211\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.31589901447,3.09432680768), test loss: 2.70368085802\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (3.36665701866,20.9505243381), test loss: 30.1317290306\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.551478683949,3.08338193404), test loss: 3.04924875498\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (6.02088260651,20.8707390956), test loss: 33.6977919102\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (3.66886115074,3.07255320837), test loss: 2.56549950242\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (4.50188970566,20.7918383373), test loss: 33.0020850658\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.38489925861,3.06171783482), test loss: 2.98675594628\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (24.9833126068,20.713923698), test loss: 31.1247252941\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.35476660728,3.05100127208), test loss: 2.616397807\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (6.55757904053,20.6356208671), test loss: 36.3641826153\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (3.67527914047,3.04041592992), test loss: 2.97561595738\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (11.0333967209,20.5576729665), test loss: 27.4294522285\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.99350309372,3.02992429093), test loss: 2.52024404407\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (5.06384515762,20.4803771874), test loss: 37.6626411438\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.60368049145,3.01959571665), test loss: 3.08022582233\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (6.19680356979,20.4041521099), test loss: 28.7762634277\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.98556804657,3.00941826796), test loss: 2.87604252994\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (14.4967336655,20.3279411402), test loss: 37.7401504993\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (3.74027585983,2.99927400088), test loss: 3.31155031919\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (5.96793174744,20.2522767662), test loss: 26.6890829086\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.498194783926,2.98911712689), test loss: 2.84945162684\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (10.3593177795,20.1768183856), test loss: 34.8253090382\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.37577033043,2.97914872983), test loss: 2.7829289943\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (12.7279462814,20.101771256), test loss: 29.4485630274\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (4.64569568634,2.96927960315), test loss: 2.99481445551\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (14.2556400299,20.0266240704), test loss: 36.9908277988\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.13568401337,2.95949461844), test loss: 2.66875852942\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (5.26164245605,19.9526117629), test loss: 32.2154975414\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.31666314602,2.94980015296), test loss: 2.96336815357\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (5.10294485092,19.8787383655), test loss: 32.7733485699\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.01708304882,2.94031014069), test loss: 2.45719048381\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (7.77271366119,19.8054550039), test loss: 33.968037796\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (4.02002811432,2.93078871415), test loss: 2.94168917537\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (2.9095416069,19.7326114185), test loss: 29.2273934841\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.802022635937,2.9212799332), test loss: 2.45498239696\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.86581611633,19.6600895469), test loss: 37.4744709015\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.449034273624,2.91192932108), test loss: 2.98180167973\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (13.0547294617,19.5875762589), test loss: 26.5052722931\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.08986067772,2.9026434322), test loss: 2.45426166058\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.27130413055,19.5151936625), test loss: 37.5103329659\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.87248826027,2.8935159132), test loss: 3.16682378352\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (3.90469431877,19.4443956856), test loss: 28.3854857922\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.8485724926,2.88447513801), test loss: 2.85090140849\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (7.62580966949,19.3730353372), test loss: 38.6956506968\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.41189599037,2.87553036298), test loss: 3.33367786407\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.03345823288,19.3024502177), test loss: 27.2596468687\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.64533042908,2.86655141983), test loss: 2.84715379775\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.28252696991,19.2323195834), test loss: 34.8392364979\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.31125235558,2.85764974013), test loss: 2.81715660989\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (6.65750217438,19.1622262666), test loss: 33.5286768436\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.66482293606,2.84885560206), test loss: 2.93027730286\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (11.5174360275,19.0922569553), test loss: 39.5839689732\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.74011063576,2.84012901508), test loss: 2.63921302259\n",
      "\n",
      "MC # 4, Hype # hyp5, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold4/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (332.621154785,inf), test loss: 183.230451584\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (207.981521606,inf), test loss: 250.114366913\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (34.6066970825,73.2130864706), test loss: 44.9040770292\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.720783948898,19.751952157), test loss: 3.12207624316\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (106.290512085,60.1262529693), test loss: 40.1983589649\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.11555957794,11.5567706654), test loss: 3.40805902481\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (22.5617523193,55.8522455985), test loss: 42.5154732227\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.23236989975,8.82018156042), test loss: 3.24711319804\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (8.03492641449,53.4856867993), test loss: 43.987770462\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.55549383163,7.44471696107), test loss: 3.37218008935\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (29.3711853027,52.2027871136), test loss: 41.2909255028\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.29692697525,6.6205182963), test loss: 2.72022822797\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (17.3715057373,51.2175491382), test loss: 46.5452653885\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.39135885239,6.07057503583), test loss: 3.39847768545\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (58.0684204102,50.5072823514), test loss: 42.1773614407\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.80491924286,5.68057124218), test loss: 2.81030053496\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (29.1188163757,49.9377707965), test loss: 44.7556888103\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.57889187336,5.38196128865), test loss: 3.31541019678\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (28.2920913696,49.4599566237), test loss: 37.9749099731\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.3122754097,5.15070407667), test loss: 3.06793465614\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (51.2618865967,49.0171808002), test loss: 46.819594574\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.37014436722,4.9643280923), test loss: 3.21504300833\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (207.272232056,48.6781104923), test loss: 36.0367197037\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (8.00666046143,4.8105922686), test loss: 2.70385460854\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (17.4657249451,48.3359092809), test loss: 43.1313597679\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.39929628372,4.67971789155), test loss: 3.25142698884\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (19.7684173584,48.0817487821), test loss: 39.1539227486\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.26874017715,4.57175458861), test loss: 3.21342188418\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (69.6454772949,47.8185690264), test loss: 42.694156599\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (7.86213302612,4.47883809128), test loss: 3.06424108744\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (39.4568138123,47.5731828246), test loss: 42.8493644238\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (9.09485340118,4.39751059037), test loss: 3.23595786989\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (20.5558853149,47.3427638997), test loss: 40.2561543465\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.64082145691,4.32528515553), test loss: 2.59826549888\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (36.7167015076,47.1049681244), test loss: 43.7798112869\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.61647319794,4.26067416658), test loss: 3.28228352368\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (26.227016449,46.8585718692), test loss: 35.3971401691\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.81424880028,4.20310178582), test loss: 2.61936972141\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (19.1829967499,46.6267852476), test loss: 45.1109623909\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.78313589096,4.14917830472), test loss: 3.29594111443\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (36.2516479492,46.4151197756), test loss: 35.0609869957\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.2334151268,4.10140218647), test loss: 2.82836614251\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (35.5217132568,46.2015852757), test loss: 42.0143363476\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.40388917923,4.05717681506), test loss: 3.04825930595\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (8.79924964905,45.9696407967), test loss: 31.7181026459\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.3958799839,4.01678546909), test loss: 2.58101339042\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (38.203918457,45.7444189725), test loss: 39.8161403656\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.77426099777,3.97840372881), test loss: 3.06834376454\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (18.1591148376,45.4956632561), test loss: 37.476117754\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.56840252876,3.9422134199), test loss: 3.19583392143\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (25.247756958,45.236448434), test loss: 35.0739076138\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.86854958534,3.90806064896), test loss: 2.7593411684\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (31.6462535858,44.9790742734), test loss: 41.9984850883\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.62827944756,3.87580131081), test loss: 3.11635598242\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (45.2541313171,44.7102420681), test loss: 34.8053318977\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.23767852783,3.84400864357), test loss: 2.40907044411\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (13.2004547119,44.4414662495), test loss: 40.3207365036\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.994658529758,3.81379526496), test loss: 3.1638286829\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (68.9482650757,44.1624381861), test loss: 32.3050758362\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.61978340149,3.78526944696), test loss: 2.62236857116\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (47.1479110718,43.8745180809), test loss: 39.2760663033\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.7014837265,3.7587226568), test loss: 3.03580507636\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (31.1419124603,43.5707326137), test loss: 30.3866627693\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.95318865776,3.73151720948), test loss: 2.82451557517\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (30.736164093,43.2559744691), test loss: 35.032566452\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.25458502769,3.70578256461), test loss: 2.76847897172\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (16.6127090454,42.919302773), test loss: 32.3502642393\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.45599842072,3.68001897945), test loss: 2.87434089482\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (29.8134460449,42.593463996), test loss: 33.6649616718\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.7870759964,3.65485656136), test loss: 2.89729940295\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (17.629907608,42.2439241772), test loss: 32.7390354156\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (5.47626876831,3.62967039446), test loss: 3.01086635292\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (21.566198349,41.9002136384), test loss: 29.8445288181\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (0.774207651615,3.6052377416), test loss: 2.54326915443\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (12.5096588135,41.5400167305), test loss: 34.2803927422\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.56408143044,3.58111714321), test loss: 3.02844896317\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (12.4344882965,41.178777403), test loss: 30.0904361725\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.27230405807,3.55789189564), test loss: 2.33703712821\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (11.9976844788,40.8160005933), test loss: 32.1361512184\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.600877285,3.53446743204), test loss: 2.87636861205\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (13.6330337524,40.4467853152), test loss: 27.5982031822\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.76140642166,3.51130498627), test loss: 2.57063640654\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (15.2328014374,40.0743542263), test loss: 33.1001232624\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.65430736542,3.48830508594), test loss: 2.79372314811\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (10.1433353424,39.7105771261), test loss: 24.0096417904\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.28616952896,3.46542885518), test loss: 2.40941718817\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (19.9950866699,39.3527080121), test loss: 31.3448516846\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.19432687759,3.44277038626), test loss: 2.77280284166\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (18.0304908752,39.001323282), test loss: 28.1907045364\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.829846262932,3.42101589788), test loss: 2.81980606019\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (19.2359008789,38.6528062223), test loss: 31.4735586405\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.24139261246,3.39947182942), test loss: 2.79370060563\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (8.934923172,38.3135651084), test loss: 28.870206666\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.42904782295,3.37865703304), test loss: 2.91473912001\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (22.0037231445,37.9841096283), test loss: 29.3731329918\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.30508375168,3.35841028742), test loss: 2.37159656286\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (28.2537117004,37.6546668492), test loss: 31.2134966373\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.85562896729,3.33829744511), test loss: 2.92295430601\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (22.0325660706,37.3340766768), test loss: 25.9354195356\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.903566420078,3.31865607536), test loss: 2.30035547316\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.1937503815,37.0240892663), test loss: 32.7173571825\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.21556901932,3.29897102163), test loss: 2.81926339865\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (19.0297298431,36.7215196264), test loss: 26.1270943403\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.43426203728,3.28008761837), test loss: 2.59485228658\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (48.8036308289,36.4256762948), test loss: 30.5696047544\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.53180360794,3.26144208922), test loss: 2.66073639393\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.0103778839,36.1372150147), test loss: 23.1050815821\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.802889108658,3.24374264281), test loss: 2.34035627544\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.7868080139,35.8575186706), test loss: 31.4997230291\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.43158340454,3.22646694366), test loss: 2.8188627243\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (21.6461601257,35.5832995191), test loss: 27.8573789358\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.91679978371,3.20944273004), test loss: 2.84756257087\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (55.570941925,35.312079539), test loss: 27.9510822296\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (4.83166408539,3.19270808198), test loss: 2.64293885231\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (22.7899150848,35.0498048877), test loss: 30.5064768314\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (6.38268947601,3.17638435653), test loss: 2.84978229702\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (22.7799015045,34.7943761745), test loss: 28.2882592201\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.66286015511,3.15995412793), test loss: 2.30788176954\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (29.3966693878,34.5438363883), test loss: 30.5268074751\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.61938095093,3.14411260892), test loss: 2.74719028771\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (15.4022674561,34.2959818522), test loss: 27.5654839277\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.40285301208,3.12847071025), test loss: 2.48582336307\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (40.9050140381,34.058831637), test loss: 32.5275085449\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.9162273407,3.11376346805), test loss: 2.82509979606\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.3531541824,33.8272922494), test loss: 25.5698923826\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.854733407497,3.09907700608), test loss: 2.57831580788\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (29.0450572968,33.59730456), test loss: 30.1371888161\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.32042717934,3.08478217871), test loss: 2.63912911415\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (8.22350692749,33.3708728721), test loss: 28.6100843191\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.91984450817,3.07053141591), test loss: 2.63844564557\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (20.3756408691,33.1549433943), test loss: 31.5654058456\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.81280970573,3.05657293894), test loss: 2.8618059814\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (57.0612335205,32.9401857828), test loss: 28.142985487\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.55934858322,3.04271384824), test loss: 2.89500715435\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (25.9849967957,32.7301971413), test loss: 28.5493652344\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.21478581429,3.02926872395), test loss: 2.60205335915\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (9.63574123383,32.5223444127), test loss: 31.1810915709\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.770644128323,3.01594322575), test loss: 2.86562834233\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (33.4294776917,32.3216405208), test loss: 29.1403361559\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.7721953392,3.00324826509), test loss: 2.30621676594\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (9.39567947388,32.1254079109), test loss: 30.4131734133\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.954423248768,2.99074498148), test loss: 2.68427102417\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.0140800476,31.9295675954), test loss: 28.0202805519\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.326396167278,2.97838373333), test loss: 2.49938519299\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (22.2723503113,31.7380737569), test loss: 31.0826015472\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.69525647163,2.96607641782), test loss: 2.64465727508\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (7.62994718552,31.551517892), test loss: 25.2161898375\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.78064918518,2.9539538461), test loss: 2.48754702806\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.2118988037,31.3678811046), test loss: 32.3143043041\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.66568040848,2.94182694525), test loss: 2.71741495728\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (33.3263816833,31.1878436636), test loss: 26.8744663715\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (4.32431745529,2.93019570188), test loss: 2.63381573558\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (23.3462524414,31.0095866354), test loss: 32.6331231594\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.35535526276,2.91857730332), test loss: 2.87181985974\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.8749313354,30.8358675102), test loss: 27.786085844\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.86789882183,2.90741219143), test loss: 2.8661642015\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (19.0334243774,30.6663543827), test loss: 31.0377527952\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.591283559799,2.8964699636), test loss: 2.49691165686\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (17.593164444,30.4961664323), test loss: 30.9201281548\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.55164265633,2.88554487463), test loss: 2.86913515031\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (7.15943908691,30.329756875), test loss: 28.0155958176\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.977202415466,2.87481691221), test loss: 2.30372647196\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (10.4138813019,30.1677501263), test loss: 31.986799264\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.41915392876,2.86394790913), test loss: 2.74774467349\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (18.8785076141,30.006639047), test loss: 26.6954632759\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (4.84386920929,2.85336386469), test loss: 2.59150491953\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (10.045835495,29.8471663667), test loss: 31.5545613766\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.22512483597,2.84285374468), test loss: 2.66087884009\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (6.80477380753,29.6917075104), test loss: 23.5399780989\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.37496030331,2.83279489851), test loss: 2.29015440047\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (2.84646344185,29.5390293787), test loss: 31.094866991\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.35776019096,2.82281001667), test loss: 2.72795326114\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (34.8502693176,29.3878067969), test loss: 26.5812749863\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.31096744537,2.81303244828), test loss: 2.69715864062\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (8.74605560303,29.2378180987), test loss: 31.8991853237\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.6664737463,2.80330056422), test loss: 2.81414221823\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (5.43199205399,29.0915008353), test loss: 30.3520034313\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.98847794533,2.79364481583), test loss: 2.78561589122\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (34.8944244385,28.9474242482), test loss: 29.6896980286\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.957842051983,2.78400951816), test loss: 2.37738674581\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (7.38039398193,28.803842041), test loss: 30.9818043709\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.58757567406,2.77452373624), test loss: 2.6928944096\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (14.1648426056,28.6609380054), test loss: 28.0671588421\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.28432226181,2.76510981706), test loss: 2.39522858858\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (10.2011013031,28.5221789148), test loss: 33.1752707005\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.978179037571,2.75609481365), test loss: 2.83476217911\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (16.8334999084,28.3864916752), test loss: 26.4780288458\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.22618353367,2.74715228058), test loss: 2.51175005436\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.885020256,28.2497697912), test loss: 30.8347434521\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.17014956474,2.73833391166), test loss: 2.57692081034\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (13.953918457,28.1157501635), test loss: 26.9696124554\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.45725190639,2.72950026729), test loss: 2.46854097843\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (16.8343105316,27.9848782237), test loss: 31.8090033531\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.845211148262,2.72075100327), test loss: 2.76094385982\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (7.32438373566,27.8537704493), test loss: 26.4850445747\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.01686263084,2.71204912347), test loss: 2.61925460398\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (13.9454536438,27.7251515954), test loss: 30.0434818268\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.00870895386,2.70352277376), test loss: 2.57600018978\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (22.1266555786,27.5965650667), test loss: 31.3184783936\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.21923017502,2.69499851575), test loss: 2.79933616519\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (21.9760475159,27.4710120564), test loss: 30.6648495197\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.01861035824,2.6867657584), test loss: 2.23788554966\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (12.2753620148,27.3473761579), test loss: 31.0307566166\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.12373065948,2.6786776285), test loss: 2.61514518857\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (13.0276594162,27.2232705584), test loss: 30.2420465946\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (3.10678815842,2.6706619879), test loss: 2.47802521884\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (4.03891086578,27.1018595675), test loss: 31.3103941679\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.05057239532,2.66257079641), test loss: 2.57521219552\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (8.24838447571,26.9821646863), test loss: 25.9842542171\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (3.45714879036,2.65456310185), test loss: 2.45775464773\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (6.03788423538,26.8628236856), test loss: 32.7712773085\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.62917399406,2.64659727562), test loss: 2.65782418847\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (23.7521629333,26.7451065506), test loss: 26.1468575001\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (3.75559473038,2.63879268861), test loss: 2.50213058889\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (14.4373273849,26.6279391401), test loss: 31.5786857605\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.995857834816,2.63103524103), test loss: 2.80378805399\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.07383346558,26.5129418227), test loss: 28.170443964\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.899247229099,2.62349425219), test loss: 2.74757480323\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (46.559425354,26.4000816183), test loss: 32.4250572681\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (3.57267785072,2.61611442911), test loss: 2.534810251\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (15.0641498566,26.2862687478), test loss: 30.7215613365\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.17899906635,2.60867453952), test loss: 2.78040905893\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (7.74412202835,26.1747227565), test loss: 30.3908205986\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.59201383591,2.60134576385), test loss: 2.34803245366\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (5.64012336731,26.0650060999), test loss: 32.3037781239\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.19491672516,2.59387441215), test loss: 2.69373369962\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (14.8939495087,25.9551693655), test loss: 26.7871234894\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.50392949581,2.58657442642), test loss: 2.52038077116\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (11.7014656067,25.8458051678), test loss: 33.1952233553\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.91522300243,2.57931029281), test loss: 2.68108745366\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (12.1175785065,25.7384616776), test loss: 24.8147535324\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.17558145523,2.57230733479), test loss: 2.26357721388\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (15.656211853,25.6324581753), test loss: 31.5197859287\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.88738644123,2.56529817886), test loss: 2.63876119405\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (15.7724256516,25.5265618262), test loss: 26.4984042645\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.48021247983,2.5584310852), test loss: 2.63048422635\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (21.4808769226,25.4224673714), test loss: 32.3310289383\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (3.46406579018,2.55160939579), test loss: 2.78102791607\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.83431911469,25.3195525109), test loss: 30.0426232338\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.64906001091,2.54474548003), test loss: 2.71211484969\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (12.1537904739,25.2169420603), test loss: 32.4215991497\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.4309489727,2.53792606594), test loss: 2.38564073741\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (10.5847635269,25.1151454233), test loss: 32.2540161133\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.17457890511,2.53115401526), test loss: 2.70091640353\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (7.71097373962,25.0131724533), test loss: 28.1139210224\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.26836061478,2.5244048824), test loss: 2.32726648003\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (4.37926959991,24.9133044878), test loss: 34.2787088394\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.79734015465,2.5178989185), test loss: 2.84417288899\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (13.5964002609,24.8149730247), test loss: 27.4056987762\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.794515430927,2.51142484454), test loss: 2.52731144428\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (14.1226768494,24.7161372719), test loss: 31.7779941559\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.38545048237,2.50504018572), test loss: 2.52819920778\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (24.3834991455,24.6189866278), test loss: 24.7769134998\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.871705293655,2.49862006108), test loss: 2.31620103121\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (28.7736377716,24.5228523082), test loss: 31.8348051786\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.53752446175,2.49222460036), test loss: 2.68805562854\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (11.8200206757,24.4270430375), test loss: 27.0197284698\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.66552639008,2.48586119112), test loss: 2.61297956705\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (26.0895500183,24.3317748049), test loss: 31.5886689663\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.84328794479,2.47956787848), test loss: 2.64443761259\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (14.0425901413,24.2363725119), test loss: 31.6659911633\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.79479634762,2.47330245589), test loss: 2.7444283247\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (15.4816436768,24.1429818456), test loss: 31.1960317612\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.00406885147,2.46718452664), test loss: 2.21855860054\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (18.5817871094,24.0504762238), test loss: 32.4217119694\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.5603852272,2.46115801247), test loss: 2.60677729249\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (11.8692893982,23.9574463018), test loss: 30.0603462696\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.50570130348,2.45516172263), test loss: 2.46143572032\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (18.2251491547,23.8665537514), test loss: 33.0831890345\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.24975252151,2.44913680222), test loss: 2.60462727249\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (13.0070819855,23.775845042), test loss: 26.8468176842\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.19763755798,2.44308906857), test loss: 2.49531090856\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (5.14786338806,23.6853399696), test loss: 32.9133100986\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.517769277096,2.43713370561), test loss: 2.56816035882\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (9.32355213165,23.5954543357), test loss: 26.820864439\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.55718803406,2.43119855706), test loss: 2.49361602664\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (4.55036354065,23.5059523182), test loss: 32.649899435\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.10540723801,2.42536615355), test loss: 2.79193476215\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (9.00661468506,23.4175343894), test loss: 28.6316464424\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.820118486881,2.41960078454), test loss: 2.6939863205\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (8.76553916931,23.3299921362), test loss: 32.3195581913\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.14553976059,2.41393158294), test loss: 2.53864498436\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (4.10927057266,23.242849697), test loss: 31.3769146442\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.60754179955,2.40828065062), test loss: 2.79269526601\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (4.29406785965,23.1563123006), test loss: 33.0878218651\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.26364183426,2.40261409781), test loss: 2.46790338159\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (2.68230462074,23.0708714619), test loss: 32.7215338707\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.955649197102,2.39690341678), test loss: 2.64469967782\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (4.50275659561,22.9850681259), test loss: 28.3563371658\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.560711205006,2.3912801381), test loss: 2.5889787972\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (34.3010826111,22.8994449737), test loss: 34.8133589268\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.86184406281,2.38564710061), test loss: 2.59819960296\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.06004047394,22.8149094043), test loss: 26.7675991535\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.805909633636,2.38018209766), test loss: 2.37311875522\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.21049165726,22.731316707), test loss: 32.7717908382\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.623358488083,2.37472429379), test loss: 2.6559127301\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (10.3148021698,22.6475620466), test loss: 27.1887047768\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.22026658058,2.36937665153), test loss: 2.56874211729\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (7.77745723724,22.5650536812), test loss: 32.8281368017\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.47023046017,2.36398624346), test loss: 2.70838543028\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (7.43593883514,22.483269585), test loss: 28.8444451332\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.3185005188,2.35858787249), test loss: 2.69930553734\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.91920781136,22.4014704109), test loss: 34.6007120609\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.19140744209,2.35325584911), test loss: 2.46655187011\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (10.7238836288,22.3198606955), test loss: 33.6924705505\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.50504732132,2.34791885456), test loss: 2.78341801763\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (2.69826364517,22.2383723071), test loss: 29.5386382103\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.921905517578,2.34258018415), test loss: 2.33728309125\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (2.19355297089,22.1580020331), test loss: 34.9031027794\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.63645505905,2.3373811602), test loss: 2.80635767579\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.46625804901,22.0782782877), test loss: 28.911204052\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.25904607773,2.3322238962), test loss: 2.58873799443\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (10.0558185577,21.9985636811), test loss: 32.8173411846\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.845463871956,2.32712736235), test loss: 2.50102152824\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.61843109131,21.9199580406), test loss: 25.9905576468\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.05124163628,2.32199466143), test loss: 2.35054737628\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.73631858826,21.8415847351), test loss: 32.196048665\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.994473099709,2.31684179403), test loss: 2.62653380036\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (22.4008426666,21.7637623305), test loss: 27.53250947\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (3.24928164482,2.31173690659), test loss: 2.57472255677\n",
      "\n",
      "MC # 4, Hype # hyp5, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold5/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (444.639160156,inf), test loss: 212.632344437\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (284.774871826,inf), test loss: 367.186116028\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (95.2616195679,165.218854168), test loss: 84.600970602\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.936531126499,80.1927230994), test loss: 3.75360427573\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (43.6028594971,112.668075649), test loss: 42.9540923119\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.96672987938,41.744416584), test loss: 3.68259901404\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (192.332962036,90.9296842596), test loss: 43.1303962231\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (7.15193796158,28.8820850856), test loss: 3.38484638333\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.5730495453,79.9253796123), test loss: 43.0646811962\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.7018289566,22.4418860179), test loss: 3.51122334898\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (32.5931587219,73.3969665256), test loss: 38.5509740829\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (7.18991613388,18.5858739931), test loss: 2.81358702183\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (38.3801879883,68.967652286), test loss: 47.2975535393\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.34521436691,16.0156019741), test loss: 3.62276910543\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (27.6840801239,65.7966728516), test loss: 41.5488505363\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.034532547,14.176559049), test loss: 2.84350070953\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (51.3038101196,63.3614487522), test loss: 46.1097062111\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.39910125732,12.7982002077), test loss: 3.63566640913\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (30.2736167908,61.4457736601), test loss: 39.6765269279\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.08603191376,11.7257444555), test loss: 2.95598170161\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (43.4398727417,59.9543945023), test loss: 46.7505735874\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.30174303055,10.867940319), test loss: 3.5391260922\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (29.9686012268,58.6975881138), test loss: 38.1778771877\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (0.82669699192,10.1635533495), test loss: 2.759350425\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (39.5092163086,57.6883539946), test loss: 44.8286448479\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (5.1628651619,9.58021285304), test loss: 3.46275309324\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (17.2471179962,56.7954189277), test loss: 34.9153910637\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.37527036667,9.08562521206), test loss: 2.59755977392\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (73.0744018555,56.0380404524), test loss: 43.4322789669\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (5.44033193588,8.66071881319), test loss: 3.2851398319\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (33.5772209167,55.340767157), test loss: 41.4896545887\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.27914524078,8.29269489827), test loss: 3.40838642716\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (14.1487188339,54.7179475969), test loss: 43.8410719872\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.38964223862,7.97000578604), test loss: 3.09676017761\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (70.0760192871,54.2007878188), test loss: 45.0679523468\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.93297290802,7.68394929858), test loss: 3.3530597955\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (50.945350647,53.708966225), test loss: 39.5061674118\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.63246798515,7.42665246241), test loss: 2.68209121823\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (19.4127483368,53.2917267418), test loss: 45.2427735806\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.61133038998,7.19787020972), test loss: 3.55788931251\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (23.4960536957,52.8944661164), test loss: 37.2251489162\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.581709980965,6.99118790432), test loss: 2.49040319026\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (44.2133560181,52.5390838814), test loss: 44.6119149685\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.581867694855,6.80335168112), test loss: 3.35754659772\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (41.6137313843,52.1858627058), test loss: 38.0773338795\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.95336997509,6.63252941904), test loss: 2.86596453786\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (72.421875,51.8561632952), test loss: 46.2350760937\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.23493003845,6.47593567535), test loss: 3.41287656426\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (57.7528305054,51.5705453798), test loss: 35.3122496605\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.90166330338,6.33243188148), test loss: 2.66001890898\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (35.6013641357,51.2743439795), test loss: 42.0822882652\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.74962830544,6.19872077595), test loss: 3.26305008531\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (33.7757110596,51.0184455076), test loss: 39.3311573982\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.917624056339,6.0762374073), test loss: 2.93158539832\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (23.320482254,50.7626936121), test loss: 42.4048890114\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.485792040825,5.96230699716), test loss: 3.05779973567\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (34.8731994629,50.5270321957), test loss: 43.4001359463\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.08471107483,5.85596748519), test loss: 3.42774193883\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (102.265609741,50.2899509446), test loss: 38.3005984306\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (7.42227268219,5.75728720274), test loss: 2.54405740499\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (117.21913147,50.0501804445), test loss: 43.8365050316\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (7.65230560303,5.66401932754), test loss: 3.51747772694\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (27.6968917847,49.8323984732), test loss: 37.9165508747\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.65721559525,5.5764297233), test loss: 2.53226781785\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (40.9032707214,49.6107747715), test loss: 42.5788993835\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.78710651398,5.49356917913), test loss: 3.40380862951\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (24.1672115326,49.4107216292), test loss: 36.0411158562\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.68536388874,5.4159303815), test loss: 2.54032894373\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (51.2019920349,49.2061149594), test loss: 44.3217502117\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.37535429001,5.34304292909), test loss: 3.25206163526\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (25.0234298706,49.0049856588), test loss: 36.9141650915\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.940710783005,5.27432803933), test loss: 2.82014274597\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (38.695526123,48.7991165438), test loss: 40.4693133354\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.4270966053,5.20978727757), test loss: 3.04162368178\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (16.664150238,48.5829254883), test loss: 37.9629419327\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.86358225346,5.14759949214), test loss: 3.12137892246\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (24.4240570068,48.3820723483), test loss: 38.0826376915\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.12698030472,5.08825087192), test loss: 3.03016917408\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (20.9872398376,48.1725304029), test loss: 38.884213829\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.09392738342,5.03105002483), test loss: 3.10468351245\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (26.9172554016,47.9760662964), test loss: 34.1390345097\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.4357817173,4.97633155197), test loss: 2.51057730317\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (39.3885307312,47.7727069795), test loss: 42.0027338982\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.07845544815,4.92417366076), test loss: 3.31963108182\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (34.4355773926,47.5695654696), test loss: 35.573482132\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.49479436874,4.87341897141), test loss: 2.37140630782\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (77.0965423584,47.3611778592), test loss: 41.4735906601\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.49635362625,4.82493709784), test loss: 3.23117766976\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (141.9269104,47.1401221298), test loss: 33.8556203842\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.45617675781,4.77760350079), test loss: 2.62643378675\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (22.2009849548,46.9189859101), test loss: 41.0798882008\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.9763109684,4.73173431296), test loss: 3.19913039207\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (95.8460083008,46.6953503389), test loss: 31.6754299641\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.82761192322,4.68700335832), test loss: 2.44898838401\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (97.0287322998,46.4743139082), test loss: 38.2122489929\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.03974676132,4.64371237477), test loss: 3.04438196421\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (65.5558853149,46.2416051015), test loss: 27.6528682947\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.66474950314,4.60173574673), test loss: 2.34549479485\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (12.8418292999,46.0011988305), test loss: 36.1988230228\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (7.2750711441,4.56098195973), test loss: 2.98685520291\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (13.7094955444,45.7499327477), test loss: 34.2643418789\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.71912956238,4.52114091902), test loss: 3.0078994453\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (72.1654510498,45.4856825478), test loss: 29.2547824144\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.7528617382,4.48196092581), test loss: 2.48943240643\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (30.5952148438,45.2150149653), test loss: 37.6038993835\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.4553732872,4.44368104353), test loss: 3.29107328057\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (32.5294036865,44.9413222178), test loss: 29.313818121\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.18199014664,4.40603369211), test loss: 2.2779596895\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (21.6906661987,44.6650321241), test loss: 35.8373357296\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.25604200363,4.36933392407), test loss: 3.26731946468\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (13.2558546066,44.3782816098), test loss: 26.4890574217\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.21767878532,4.33359675836), test loss: 2.19642172158\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (25.9858207703,44.0899494366), test loss: 36.6883985519\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.3390712738,4.29879726594), test loss: 3.22502500415\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (34.5124511719,43.799443188), test loss: 27.2160997629\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.57847166061,4.26474413516), test loss: 2.54887564182\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (17.7773857117,43.4998915336), test loss: 34.0300357819\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.51843142509,4.23120072089), test loss: 3.18383343071\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (12.8090343475,43.2016890412), test loss: 24.907190156\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.45135903358,4.19836742724), test loss: 2.41952485144\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (24.8095321655,42.9078482957), test loss: 32.0714840651\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.58614134789,4.16600582537), test loss: 2.99642412961\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (16.2501144409,42.6162250318), test loss: 30.5557055473\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.92552900314,4.1344034752), test loss: 3.06220054626\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (5.47915267944,42.3233895594), test loss: 32.0839522362\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.95841050148,4.10367432832), test loss: 3.04473395348\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (28.1606750488,42.0374147724), test loss: 31.6273858309\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.27492129803,4.07399517506), test loss: 3.29974503219\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (6.40190792084,41.7525059174), test loss: 27.8275449038\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.93012046814,4.04495176834), test loss: 2.44204822481\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (41.8080635071,41.4659575228), test loss: 32.1756854773\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.49766778946,4.0164407871), test loss: 3.51318823099\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.8927412033,41.1821905961), test loss: 28.1127117157\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.04227781296,3.9885530224), test loss: 2.40652347207\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (11.0253572464,40.9063581383), test loss: 32.2086074829\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.49803638458,3.96098485018), test loss: 3.20573164225\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (19.5125675201,40.6347862278), test loss: 28.2457705975\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.88714194298,3.93414893968), test loss: 2.56778161824\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (8.03482532501,40.3641090139), test loss: 33.6995950222\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.27992987633,3.90803375601), test loss: 3.24559613466\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (10.4959115982,40.1022969311), test loss: 25.6682694435\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.07489442825,3.8828215975), test loss: 2.52358115464\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.8066091537,39.8451174717), test loss: 31.7864275932\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.01177704334,3.85807604804), test loss: 3.08844162971\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (19.8138694763,39.5885526528), test loss: 30.3384649754\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.01098513603,3.83379523077), test loss: 3.06443317235\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (14.1013450623,39.3362103093), test loss: 31.8902877569\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.942178189754,3.80995976565), test loss: 3.09447147697\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (10.3941707611,39.0918879199), test loss: 31.8034729481\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.44766616821,3.78646584581), test loss: 3.36049204171\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (29.6705093384,38.8519779864), test loss: 26.6780139446\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.36667823792,3.76342380246), test loss: 2.54289319515\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (41.9480171204,38.6130336718), test loss: 31.8912196159\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.28160345554,3.74089448316), test loss: 3.40135563314\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.6061840057,38.3814822417), test loss: 27.3008627415\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.8092443943,3.7191810005), test loss: 2.29355536699\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.3309745789,38.1548125004), test loss: 31.9797806978\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.55808746815,3.69778959655), test loss: 3.20006061792\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (39.947517395,37.9289607194), test loss: 28.6079204082\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.01262569427,3.67676477264), test loss: 2.53855397403\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (18.8383178711,37.706018598), test loss: 33.3147653103\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.35586893559,3.65604568051), test loss: 3.28862099648\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (9.66092300415,37.4894000315), test loss: 27.7343203068\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.12312078476,3.63558835692), test loss: 2.60786549449\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (6.43516540527,37.2765267709), test loss: 31.1448529243\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.348684817553,3.61544734945), test loss: 3.05498471856\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (47.5982131958,37.0652260307), test loss: 27.7597104549\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.18645596504,3.5957710924), test loss: 2.92612289935\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (15.2969169617,36.8581550631), test loss: 31.4902215242\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.42855787277,3.57671818432), test loss: 3.03510934114\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (25.8164482117,36.656408611), test loss: 30.1482236862\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.897192001343,3.55790394889), test loss: 3.11739439368\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.53351688385,36.454181572), test loss: 26.3036283016\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.10365128517,3.53943275716), test loss: 2.50844501555\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (5.84418392181,36.2548962918), test loss: 32.4610284328\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.738488078117,3.52114765411), test loss: 3.35200748146\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.0294036865,36.0612919306), test loss: 29.0376841545\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.71119070053,3.50307455247), test loss: 2.40955528021\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (16.1247673035,35.8703362976), test loss: 33.270904541\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.67381548882,3.48524674797), test loss: 3.21635496318\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (12.6541728973,35.680176491), test loss: 27.2319811344\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.78582811356,3.46782460655), test loss: 2.32573345304\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (44.8547058105,35.4943031087), test loss: 34.337358737\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.41042470932,3.45085936597), test loss: 3.25623473227\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (21.5747375488,35.3118896102), test loss: 27.2824822903\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.551877200603,3.43408135391), test loss: 2.58371485472\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (15.8545742035,35.1289488916), test loss: 32.6212407112\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.979564666748,3.4176238376), test loss: 3.11029672027\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (13.4511795044,34.9488023488), test loss: 24.2117584229\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.20509243011,3.40133067969), test loss: 2.32940624356\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (10.5005187988,34.7735476189), test loss: 31.6607875824\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (4.20033836365,3.3852326594), test loss: 2.92364570796\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (20.5536746979,34.6000656011), test loss: 29.3688840389\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (4.65153408051,3.36925284504), test loss: 2.92710224688\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (8.85916805267,34.42678407), test loss: 32.7670779705\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.9389526844,3.35354506649), test loss: 3.00788002908\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (11.2255001068,34.2574623115), test loss: 30.7745555878\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.855322360992,3.3382723595), test loss: 3.15090432167\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (5.45813655853,34.0907260688), test loss: 29.6533231735\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.173543468118,3.32316979139), test loss: 2.45374935716\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (8.61177253723,33.924210703), test loss: 31.6979006767\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.65127325058,3.30839400206), test loss: 3.19786724746\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (11.0715408325,33.7593807196), test loss: 27.4480540276\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.539648234844,3.29366817609), test loss: 2.25745781958\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (12.3839950562,33.5986705099), test loss: 33.1361713886\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.38163661957,3.27919683739), test loss: 3.11750926673\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (10.6084299088,33.4397132179), test loss: 28.3785727501\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.733915328979,3.2647068043), test loss: 2.48430266678\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (35.8817062378,33.280858566), test loss: 34.2923941851\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.80021858215,3.25049425155), test loss: 3.10737015903\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.8195400238,33.1242790827), test loss: 25.7270261765\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.962217211723,3.23662989869), test loss: 2.40537341833\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (12.2369556427,32.9709635686), test loss: 31.6895669937\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.80769991875,3.22293205085), test loss: 2.93102741539\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (20.9764537811,32.8176636213), test loss: 28.9110848904\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.238057747483,3.20948676629), test loss: 2.71505852938\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (22.2497158051,32.6657719937), test loss: 32.4518024921\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.39540934563,3.19613902433), test loss: 2.91712359488\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.4322118759,32.5169434411), test loss: 30.2237980843\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.48878324032,3.1829494354), test loss: 3.11852688193\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (13.3988113403,32.3698551605), test loss: 29.4877053738\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.01325309277,3.16975017383), test loss: 2.47026761174\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (10.1415214539,32.2225172397), test loss: 31.1923382282\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.70480751991,3.15680304995), test loss: 3.20784547329\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (22.9412727356,32.0776423953), test loss: 29.9303607464\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.9343868494,3.14413716679), test loss: 2.26750227064\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (14.426525116,31.9350136491), test loss: 32.3057493687\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.79388618469,3.13161073419), test loss: 3.05365269631\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (12.9012813568,31.7924559259), test loss: 30.6813664436\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.454909682274,3.11929743522), test loss: 2.47898915112\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (15.2065401077,31.6516655277), test loss: 33.1490745783\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.767277657986,3.10708487914), test loss: 3.00873169601\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (35.7100448608,31.5134576046), test loss: 27.5409857273\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.9662784338,3.0950169838), test loss: 2.54165598452\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (10.5385036469,31.3756027144), test loss: 32.2208966017\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (3.15684556961,3.08294498576), test loss: 2.87157847881\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (24.8546123505,31.2385230006), test loss: 29.862644434\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (4.33601140976,3.07107521929), test loss: 2.87067436576\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (6.87328910828,31.102823034), test loss: 32.8828125954\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.84457051754,3.05937958664), test loss: 2.95256575644\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (9.29216098785,30.9694376003), test loss: 29.2715501308\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.30141282082,3.04789672501), test loss: 2.87334491014\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (36.7996292114,30.8367697782), test loss: 28.7171611786\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.99634122849,3.03655106965), test loss: 2.54693703651\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (13.560084343,30.704748427), test loss: 31.7363373756\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.91378331184,3.02528999984), test loss: 3.15693079531\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (11.5944452286,30.5752161433), test loss: 29.6702214718\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.659365177155,3.01414505058), test loss: 2.32517760992\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (5.90188217163,30.4459583358), test loss: 34.4148697853\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.791584610939,3.00301628805), test loss: 3.05795432627\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (21.4718379974,30.3175349744), test loss: 30.408927393\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.29943132401,2.99207868086), test loss: 2.4816608429\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (20.009054184,30.1900561341), test loss: 35.1247608662\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.03595948219,2.98127330469), test loss: 3.17467794716\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (11.7517261505,30.0647701885), test loss: 28.2018146038\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.44553256035,2.97065273957), test loss: 2.50122708678\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.90773773193,29.9399817466), test loss: 33.183721137\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.0706152916,2.96015072651), test loss: 2.94670620412\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (26.9471111298,29.8160989258), test loss: 24.8001464844\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.34275913239,2.94975441422), test loss: 2.3175929904\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (14.7278194427,29.6940430926), test loss: 32.179453373\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.927743792534,2.93936572155), test loss: 2.85381083488\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (11.1497879028,29.5721117247), test loss: 28.6918255329\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.8199057579,2.92909024008), test loss: 2.73606886864\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (14.3395404816,29.4509678396), test loss: 33.0431251526\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.24782252312,2.91891725608), test loss: 2.76273069382\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (3.53203725815,29.3306909046), test loss: 32.8242678642\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.40277326107,2.90887295225), test loss: 3.10413793921\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.17667770386,29.2123399377), test loss: 31.9159092903\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.698266148567,2.89900500355), test loss: 2.45772388279\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (34.2122077942,29.0948246964), test loss: 33.7204736233\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.63793492317,2.8892526056), test loss: 3.11573275328\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (8.71380996704,28.9774998656), test loss: 28.7770641327\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.93689441681,2.87958070722), test loss: 2.22056281865\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (2.71983289719,28.8618938141), test loss: 34.5750699759\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.18540048599,2.86991088484), test loss: 3.04067967534\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (20.0867977142,28.7470796778), test loss: 29.2934528828\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.3737026453,2.86036338739), test loss: 2.48570642471\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (34.1040039062,28.632400441), test loss: 35.8197308302\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.30805635452,2.85086441526), test loss: 3.15905971527\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (11.7961139679,28.51802777), test loss: 26.6111919403\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.57211399078,2.84150298375), test loss: 2.4387658\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (8.59866523743,28.4057073794), test loss: 32.8962253094\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.477072119713,2.83227889743), test loss: 2.84677593112\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (30.4626922607,28.294747593), test loss: 30.4617516518\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.82087182999,2.82320395581), test loss: 2.68233377337\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (15.7560558319,28.1832023066), test loss: 34.1280132294\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.82072210312,2.81417283669), test loss: 2.92389563918\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.29684877396,28.0732395964), test loss: 31.4487423658\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (4.01878786087,2.80515810778), test loss: 3.05938195884\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (4.88346672058,27.9643098792), test loss: 31.6780448914\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.49468326569,2.79620114682), test loss: 2.48877670169\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (11.2723464966,27.85530833), test loss: 31.9295419693\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.39257478714,2.78731599008), test loss: 3.17787511945\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (15.595785141,27.74665925), test loss: 31.6540935516\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.96401619911,2.77856046963), test loss: 2.35679039359\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (12.0628089905,27.6398520877), test loss: 34.4960517406\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.963785052299,2.76989466939), test loss: 3.04185932279\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (13.1019878387,27.5341197238), test loss: 35.2862454414\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.232765465975,2.76140179579), test loss: 2.81589124501\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (6.69778299332,27.4279032935), test loss: 35.1450029373\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.566241383553,2.75293899337), test loss: 3.00936373621\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (5.60103940964,27.3231996881), test loss: 28.4962460041\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.60914206505,2.74449418184), test loss: 2.52587509453\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (12.3199424744,27.2194181464), test loss: 34.0170377254\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.49559044838,2.73610227612), test loss: 2.82903868556\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (25.2108001709,27.1157161187), test loss: 29.5617115736\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (3.83139610291,2.72779159034), test loss: 2.71927861869\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (11.4188699722,27.0120997346), test loss: 34.2776942253\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.50663280487,2.71955098577), test loss: 2.96795943975\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (9.70284080505,26.9100279609), test loss: 30.1379164934\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.46739578247,2.71142306954), test loss: 2.84779784977\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (6.7130613327,26.8090281658), test loss: 33.2123654842\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.433225423098,2.70340185751), test loss: 2.82459838986\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (6.84496212006,26.7079015707), test loss: 32.5872189522\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.13592457771,2.69545302578), test loss: 3.10787255168\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (12.8620452881,26.6078610911), test loss: 31.6497006416\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.754945278168,2.68751968262), test loss: 2.34771124125\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (10.4274606705,26.5087760844), test loss: 35.60164814\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.51794481277,2.67962169477), test loss: 3.05176787972\n",
      "run time for single CV loop: 7082.78103304\n",
      "\n",
      "MC # 4, Hype # hyp4, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold1/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (308.010253906,inf), test loss: 163.100341034\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (315.214080811,inf), test loss: 385.944714355\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (22.8557033539,89.9526918039), test loss: 45.4068871021\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.11339998245,108.115351361), test loss: 3.24910709858\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (48.874584198,67.4813833609), test loss: 37.1672992468\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.09121346474,55.6221335534), test loss: 2.75437138677\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (54.1710853577,59.9481276086), test loss: 44.9786309242\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.975509524345,38.1122774194), test loss: 3.37866278291\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (41.8028717041,56.2242632604), test loss: 42.9895614147\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.62376236916,29.3645913021), test loss: 3.42626628876\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (52.0817718506,53.9284157753), test loss: 45.3293861866\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.70318102837,24.1189570013), test loss: 3.26550698578\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (79.5983657837,52.3815924444), test loss: 45.7336709499\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (6.73009681702,20.6263432363), test loss: 3.46183489561\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (20.9934844971,51.2611817068), test loss: 43.5330698013\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.71713089943,18.1274800001), test loss: 2.6601443857\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (69.9635696411,50.363109426), test loss: 47.1945361137\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.312021493912,16.2537494503), test loss: 3.79733771682\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (35.4870605469,49.6599283013), test loss: 43.4575706005\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.04617404938,14.7979309781), test loss: 2.75082008839\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (20.9287109375,49.1025236188), test loss: 46.0413467884\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.17191529274,13.6315956951), test loss: 3.72564983368\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (17.9862270355,48.6625481293), test loss: 41.8201831341\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (5.85426998138,12.6790782967), test loss: 2.70017498136\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (69.2963867188,48.2758251699), test loss: 47.6593868256\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (8.64435195923,11.8854957475), test loss: 3.47505022287\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (45.0803031921,47.9401439742), test loss: 38.017674017\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.22246384621,11.2119213257), test loss: 2.94324959517\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (32.1740150452,47.6200284568), test loss: 44.2808643341\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.40053772926,10.6362652051), test loss: 3.21349821091\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (10.5683298111,47.3254281017), test loss: 35.6766707897\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.40819311142,10.1363005988), test loss: 2.83311395943\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (69.4480895996,47.0942371625), test loss: 44.622454071\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.78173184395,9.69719837967), test loss: 3.29189609289\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (22.4021396637,46.8770531673), test loss: 43.0263347626\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.27932226658,9.30982597687), test loss: 3.42368850112\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (70.8147583008,46.6872384959), test loss: 45.8808841467\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (6.62427711487,8.96634671373), test loss: 3.1079908669\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (25.953962326,46.4754267486), test loss: 43.7568555355\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.73386311531,8.65692001605), test loss: 3.37829122543\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (40.4713134766,46.2642526343), test loss: 40.9941971302\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.564980387688,8.37533655199), test loss: 2.57297008038\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (16.7531833649,46.0493553899), test loss: 44.0832044601\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.95913147926,8.1197624186), test loss: 3.63134976029\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (46.4289360046,45.8402992516), test loss: 41.2639712334\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.85817325115,7.8859553495), test loss: 2.63794617653\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (29.3970375061,45.6508684101), test loss: 45.0563626289\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.801398873329,7.67024286098), test loss: 3.49812761545\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (38.7777709961,45.4739459355), test loss: 39.2836935043\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.41955816746,7.47250375995), test loss: 2.62054757178\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (21.2574977875,45.2862219816), test loss: 45.2382033825\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.19226193428,7.28977211179), test loss: 3.33714745641\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (40.604637146,45.1021753258), test loss: 34.2987437248\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.83946698904,7.12062434557), test loss: 2.62310763896\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (16.4652309418,44.9247036205), test loss: 41.5343815804\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.66269135475,6.9630920335), test loss: 3.06333758235\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (22.5655097961,44.7193692095), test loss: 38.4158446789\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.66101133823,6.81553622278), test loss: 3.13480859399\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (29.8296604156,44.5413249561), test loss: 40.2743217468\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (4.60830116272,6.67717155285), test loss: 3.1064340651\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (57.4173698425,44.3563600706), test loss: 39.6031675339\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (5.25453615189,6.54718302016), test loss: 3.28439536393\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (32.6855926514,44.1762982556), test loss: 36.4438233376\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.0385415554,6.42549169061), test loss: 2.60953864455\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (48.8322143555,43.9854429312), test loss: 40.5825723648\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.13431596756,6.31050792455), test loss: 3.37971179187\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (37.1146125793,43.7937541708), test loss: 37.2587506056\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.3814766407,6.20167395827), test loss: 2.43479805291\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (26.9463233948,43.5883569662), test loss: 40.5329467773\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.780057311058,6.09868294979), test loss: 3.4579548955\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (24.1118240356,43.3783109875), test loss: 37.3777587414\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.34123873711,6.00097417858), test loss: 2.42455578744\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (32.8969039917,43.171660233), test loss: 42.9706609726\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (6.8495926857,5.90733844223), test loss: 3.37551779151\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (26.1966972351,42.9657738185), test loss: 34.9453922272\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.03494596481,5.81834615112), test loss: 2.52566287518\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.2413406372,42.7510210537), test loss: 40.1214923859\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.7079577446,5.73320639618), test loss: 3.11447839141\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (41.3066139221,42.5280315083), test loss: 29.9546812534\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.82009994984,5.65248669455), test loss: 2.44997321367\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (23.6674633026,42.2993551729), test loss: 36.3295392752\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.31027793884,5.57477361556), test loss: 3.02190241218\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (60.4678764343,42.0605187392), test loss: 34.1569446087\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.93091678619,5.50018452065), test loss: 2.95433117747\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (24.6134262085,41.8129609107), test loss: 34.8772954464\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.11513948441,5.42853602893), test loss: 3.00782500505\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (33.6949806213,41.5692852587), test loss: 34.2160732269\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.98245942593,5.35906822511), test loss: 3.15703125894\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (24.2058143616,41.3190153368), test loss: 29.8176032066\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.22888076305,5.29280600065), test loss: 2.52113983035\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (26.9037628174,41.0562630557), test loss: 37.0387973785\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.277926802635,5.2292185471), test loss: 3.42147938311\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (22.1440048218,40.7862638798), test loss: 30.6899430513\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.90837430954,5.16830265524), test loss: 2.33906184733\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (81.5295333862,40.5096344479), test loss: 35.5907363176\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.99299073219,5.1099903508), test loss: 3.37454826832\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (24.3294906616,40.2162308854), test loss: 29.2725410938\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.00337719917,5.05310078989), test loss: 2.41682478786\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (38.7757034302,39.931778936), test loss: 37.067153573\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.24868392944,4.99776208956), test loss: 3.372589311\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (12.306350708,39.6416557415), test loss: 28.013223362\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.54589879513,4.94432674569), test loss: 2.49170196652\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (30.970741272,39.3529891383), test loss: 33.7159735203\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.09792327881,4.89311590313), test loss: 3.20313032269\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (24.0611190796,39.0598490444), test loss: 24.1877752781\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.69622612,4.84327254815), test loss: 2.43551641107\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (24.550951004,38.7679435018), test loss: 31.5980960608\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.65925931931,4.79531569877), test loss: 3.13731054515\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (19.5006942749,38.4726746841), test loss: 28.9636617899\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.27531433105,4.74896753213), test loss: 3.07049856186\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (25.7986621857,38.1780902314), test loss: 30.5793370247\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.51284480095,4.70399692509), test loss: 3.13829061389\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (9.87655067444,37.8926882701), test loss: 30.225615263\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.570148944855,4.65997309628), test loss: 3.37318997383\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (6.73507118225,37.610075523), test loss: 25.8692811489\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.448061048985,4.61751523012), test loss: 2.49838342071\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (38.6234741211,37.3289158081), test loss: 31.9966725349\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.1289024353,4.57650539769), test loss: 3.48627682626\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (16.9778404236,37.0538002832), test loss: 26.7490530491\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.74849963188,4.53716517901), test loss: 2.22092860639\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (4.84414577484,36.7853766021), test loss: 32.8972328901\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.814186096191,4.49895934037), test loss: 3.38965480924\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (26.3622093201,36.51903374), test loss: 27.4710165024\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.949852824211,4.46177842434), test loss: 2.49625128061\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (10.5765399933,36.2594198533), test loss: 34.9184957027\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.86115682125,4.42558052168), test loss: 3.43522564769\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (9.0966835022,36.0086678328), test loss: 27.1045410872\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.49424111843,4.39019958329), test loss: 2.64262131453\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (16.7559509277,35.7631197793), test loss: 32.5488557339\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.69312500954,4.35610465095), test loss: 3.22786130309\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (18.2207317352,35.5216293778), test loss: 24.108885479\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.33298540115,4.32290485021), test loss: 2.57185417116\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (10.4400930405,35.2869838886), test loss: 32.0806117535\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.911470413208,4.29087698111), test loss: 3.23952984512\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (12.9090595245,35.0579228242), test loss: 29.3973108292\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.39540743828,4.25994988852), test loss: 3.21698421687\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (23.2316303253,34.832057352), test loss: 31.9712623596\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.8684155941,4.2296187418), test loss: 3.20013942719\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (18.9236564636,34.6162918979), test loss: 30.3263865471\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.67053174973,4.19990297331), test loss: 3.44939775765\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (8.14494228363,34.4049061165), test loss: 27.3033491611\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.68125081062,4.17090677446), test loss: 2.49698829055\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (7.94412612915,34.1971236598), test loss: 31.6186640739\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (4.0970697403,4.14284850761), test loss: 3.5818954587\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (14.1245479584,33.9947835741), test loss: 27.1413677216\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.0376213789,4.11565936319), test loss: 2.30237895548\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (17.5166568756,33.798543637), test loss: 32.8132719517\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.418125957251,4.08928941729), test loss: 3.36793785691\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (18.5613918304,33.6045941323), test loss: 29.5628004074\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.61026668549,4.06357273917), test loss: 2.52652009428\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (24.2651481628,33.4147546648), test loss: 34.1996402264\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.50508415699,4.03839911773), test loss: 3.41009609401\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (5.39855480194,33.2333085533), test loss: 27.4184981823\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.05436086655,4.01344869082), test loss: 2.68447060287\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (21.7147941589,33.0545189324), test loss: 32.0343307495\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.32341194153,3.9893587961), test loss: 3.20857372284\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (5.61646175385,32.8774854463), test loss: 24.6789416313\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.862303495407,3.96575530248), test loss: 2.58887718022\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (34.904624939,32.7061771941), test loss: 32.7661416292\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.485543370247,3.94304397358), test loss: 3.24118129909\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (23.1754207611,32.5406413967), test loss: 30.2438583851\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (4.38605213165,3.92096476914), test loss: 3.36527365446\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (4.74063301086,32.3735338076), test loss: 32.5050765514\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.37786281109,3.89910863501), test loss: 3.22711187303\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (12.4351043701,32.2145075898), test loss: 30.780643034\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.45325565338,3.87756124577), test loss: 3.42771803439\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (50.4512557983,32.0596421149), test loss: 28.2356812477\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (4.58974170685,3.85645894538), test loss: 2.45122066736\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (16.7768497467,31.9054174598), test loss: 31.5278353691\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.20219826698,3.8359475086), test loss: 3.57059695721\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (19.0859336853,31.7549052515), test loss: 28.6824650764\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.49916195869,3.81583564141), test loss: 2.38163957298\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (12.6649913788,31.6086267547), test loss: 32.8780740738\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.40092182159,3.796373483), test loss: 3.33605491072\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (11.7872562408,31.4639779885), test loss: 30.5571510792\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.77602291107,3.77727195213), test loss: 2.55950558484\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (22.321811676,31.321907692), test loss: 33.3643646717\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.54294419289,3.75853633382), test loss: 3.24169342816\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (23.7384243011,31.1854788312), test loss: 27.1849841118\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (6.30220937729,3.73997492669), test loss: 2.72133712173\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (17.9289550781,31.0511260218), test loss: 31.968073082\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.97331094742,3.72178003708), test loss: 3.15792699456\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (9.19536304474,30.9180595477), test loss: 24.6646049976\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.55717587471,3.7039354851), test loss: 2.59978437424\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (19.3398971558,30.7870906569), test loss: 32.4895406246\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.44219279289,3.68667005452), test loss: 3.18241328299\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (19.5942268372,30.6607353402), test loss: 29.7109455109\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.76170563698,3.66984047151), test loss: 3.3171995163\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (33.1854934692,30.5350354617), test loss: 33.1206013203\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.12002205849,3.65321661623), test loss: 3.20537645817\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (4.69394397736,30.4107717816), test loss: 30.8680420876\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.55780899525,3.63686975799), test loss: 3.36849256456\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (27.7585983276,30.292985178), test loss: 29.0363548756\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.43831920624,3.62054040934), test loss: 2.42140591443\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (19.5592460632,30.1746259596), test loss: 31.7407222271\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.02746653557,3.60471450343), test loss: 3.48134441972\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (18.8341808319,30.0573905955), test loss: 29.0589569092\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.00732374191,3.58911343753), test loss: 2.35538495183\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (12.1924438477,29.9430336195), test loss: 34.1799417973\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.5573451519,3.57398713877), test loss: 3.37313072085\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (54.9058151245,29.8325967311), test loss: 31.1606526852\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (3.70755505562,3.55928301833), test loss: 2.64411668181\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (20.0557632446,29.7204046875), test loss: 33.6842140675\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.36221015453,3.54462621529), test loss: 3.23741007745\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (16.2728691101,29.613392993), test loss: 25.3369659424\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.948536872864,3.53003167282), test loss: 2.64498249292\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (20.4736919403,29.5074919902), test loss: 32.4886760712\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.15745210648,3.5157395223), test loss: 3.13496312201\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (18.4206752777,29.4026232904), test loss: 25.8322679996\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.06328010559,3.50182487559), test loss: 2.95885084867\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (29.354385376,29.2997782242), test loss: 33.6553048134\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.37404370308,3.48807607522), test loss: 3.17635401785\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (12.4525232315,29.1992451434), test loss: 29.0360846519\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.1822810173,3.47470617152), test loss: 3.17456463873\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (23.0295906067,29.0992869781), test loss: 30.0308190346\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.26623368263,3.46158446224), test loss: 2.71210806668\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (25.1533908844,29.0003547877), test loss: 30.3316733837\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.39964532852,3.44861499043), test loss: 3.28656046093\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (6.06706857681,28.9057978367), test loss: 28.9871406555\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.718622565269,3.43560861137), test loss: 2.39624920785\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (4.99638748169,28.811794835), test loss: 32.1023635149\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.556340515614,3.42291206381), test loss: 3.39944315553\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (35.7545585632,28.7175005269), test loss: 28.5251918077\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.26385402679,3.4104142145), test loss: 2.28459967673\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.03888702393,28.6256981522), test loss: 34.7769306183\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.09558796883,3.39833252026), test loss: 3.3227293849\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (5.47900009155,28.5363745207), test loss: 29.0125231743\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.861040592194,3.38644935887), test loss: 2.57724060714\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (17.7486419678,28.4465560014), test loss: 34.86003685\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.31106817722,3.37467383716), test loss: 3.2469736129\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (7.75734376907,28.3586164994), test loss: 25.8077344418\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.08108353615,3.36299592459), test loss: 2.59242174923\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (7.01902389526,28.2733728983), test loss: 33.2331803322\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.2604572773,3.35138441442), test loss: 3.1139840275\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (12.4231700897,28.1884310071), test loss: 29.592890358\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.99158298969,3.34004355049), test loss: 3.04579956234\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (15.3562860489,28.1039452146), test loss: 32.4021146536\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.20997929573,3.3288294298), test loss: 3.15530825257\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (8.70716667175,28.021351263), test loss: 29.4036568642\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.99606269598,3.31792640881), test loss: 3.19757368565\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (10.770067215,27.9399382665), test loss: 27.5200273991\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.4091861248,3.30727506389), test loss: 2.62375689894\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (22.1014156342,27.8586112381), test loss: 30.2110292196\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.05665588379,3.29663992176), test loss: 3.38021228015\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (12.3057022095,27.7804501575), test loss: 28.3694256783\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.00519967079,3.28605775502), test loss: 2.34766585529\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (9.56175231934,27.7030742942), test loss: 33.4667746067\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.65531146526,3.27559979286), test loss: 3.33623705506\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (7.38091754913,27.6255790966), test loss: 29.3593676567\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (3.56329298019,3.26534343798), test loss: 2.37186202109\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (14.6630058289,27.5494038889), test loss: 35.1407514811\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.12478029728,3.25530666795), test loss: 3.38434427381\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (15.8586978912,27.4752552937), test loss: 27.6290190697\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.508836209774,3.24547597961), test loss: 2.55710804164\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (20.8241004944,27.400584386), test loss: 33.9065444469\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.32281517982,3.23578203794), test loss: 3.15266692638\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (23.1500167847,27.3269054115), test loss: 25.104357481\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.34427821636,3.22616399381), test loss: 2.45751760453\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (4.99052906036,27.2562140948), test loss: 32.6318428516\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.98018014431,3.21646825336), test loss: 3.0932982415\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (21.3071136475,27.1853829051), test loss: 29.5728629827\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.44208776951,3.20704199588), test loss: 2.92468112707\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (6.40505456924,27.1140721578), test loss: 32.4053571939\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.880890250206,3.19767827723), test loss: 3.11088236719\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (32.6845283508,27.0448174113), test loss: 29.4613549471\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.538705527782,3.18862980753), test loss: 3.11703989506\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (25.5567378998,26.9774490215), test loss: 27.7966701508\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (4.10548782349,3.17975406418), test loss: 2.61208148599\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (4.87628269196,26.9081875283), test loss: 31.4585289001\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.38403367996,3.17087292689), test loss: 3.38761448264\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (10.5137491226,26.8419539105), test loss: 28.4732479095\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.66516304016,3.16198093493), test loss: 2.30396216214\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (44.7796287537,26.7770955316), test loss: 33.6651902199\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (4.29074907303,3.15322442179), test loss: 3.2848970145\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (16.2766704559,26.7112855106), test loss: 28.1945239544\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.36388874054,3.14461379092), test loss: 2.37942834198\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (17.0637798309,26.646484954), test loss: 35.6702512741\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.68337607384,3.13609893456), test loss: 3.35181292295\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (12.9608573914,26.583241125), test loss: 27.5199172974\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.99643850327,3.12780363185), test loss: 2.57084764242\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (10.6659212112,26.5197748003), test loss: 33.656855011\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.44977259636,3.11960565275), test loss: 3.14894489646\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (22.350194931,26.4570626523), test loss: 24.6646723747\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.43320870399,3.11148992362), test loss: 2.45906237513\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (17.9723014832,26.3965679228), test loss: 32.2967953205\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (5.8849697113,3.10334625566), test loss: 3.03631547391\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (18.619184494,26.3363434293), test loss: 29.3144918919\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (3.70948934555,3.09530227974), test loss: 2.99197813272\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (10.6508598328,26.2758058395), test loss: 32.427016449\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (3.13243293762,3.08732691901), test loss: 3.08312498331\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (14.2882270813,26.2157002134), test loss: 29.9769128561\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.38838493824,3.07957797025), test loss: 3.29102580845\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (18.5246315002,26.1576814308), test loss: 27.6617247581\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.60831654072,3.07197527118), test loss: 2.53355274498\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (27.1288452148,26.099080461), test loss: 31.03861866\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.56194019318,3.06441386701), test loss: 3.36797394156\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (4.35956096649,26.0408107512), test loss: 27.9756106377\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.63482391834,3.05689983385), test loss: 2.15783535838\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (25.6372871399,25.9855298849), test loss: 33.1530649185\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.38879179955,3.04930764394), test loss: 3.20683200359\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (17.6505317688,25.9291310153), test loss: 28.8930958748\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.963835835457,3.04192196877), test loss: 2.40214056075\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (21.1159648895,25.8726107814), test loss: 35.2313668728\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.23384904861,3.03456899566), test loss: 3.26046113223\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (13.3963603973,25.8173110019), test loss: 27.5994475842\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.15204977989,3.02740250897), test loss: 2.55702968687\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (52.0480575562,25.7636643022), test loss: 33.2842636108\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (3.62113928795,3.02042683979), test loss: 3.07209929079\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (17.9733829498,25.7084658603), test loss: 24.6981491089\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.21066856384,3.01339918947), test loss: 2.47506169677\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (15.9380950928,25.6556851985), test loss: 32.1068614483\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.707651257515,3.00632092162), test loss: 3.02899232805\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (19.5746459961,25.6030370762), test loss: 29.8931890488\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.75661039352,2.99937634671), test loss: 3.14589676857\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (20.8236980438,25.5502748961), test loss: 32.6111723423\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.82985758781,2.9925566797), test loss: 3.04435345232\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (26.8180007935,25.4982621223), test loss: 31.2482242823\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.36706328392,2.98577305771), test loss: 3.26622200608\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (10.2435016632,25.4471647092), test loss: 28.216610384\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.41813087463,2.97913948098), test loss: 2.36179946363\n",
      "\n",
      "MC # 4, Hype # hyp4, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold2/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (337.421569824,inf), test loss: 189.557435608\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (410.04989624,inf), test loss: 478.835630798\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (44.3184013367,109.916797775), test loss: 49.3417692184\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.69073367119,187.397709809), test loss: 3.37364140153\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (48.9100036621,78.8698500113), test loss: 38.8949173689\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.29086780548,95.2929243673), test loss: 2.76759830713\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (13.5988082886,68.2343909845), test loss: 46.4385244131\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.895746946335,64.5609630745), test loss: 3.51759803593\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.550994873,62.8847893488), test loss: 42.2594480515\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.25397253036,49.2055876905), test loss: 3.59067491293\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (45.5453643799,59.5672881468), test loss: 47.7807515144\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.61530780792,40.0007993401), test loss: 3.32829980254\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (54.659614563,57.3317530023), test loss: 44.7058924675\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.60359334946,33.8704777302), test loss: 3.64654470682\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (18.3288726807,55.6741178816), test loss: 43.9144470453\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.49925136566,29.4863285458), test loss: 2.54158286452\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (37.5865249634,54.3698035265), test loss: 46.2051531792\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.83296227455,26.1986601756), test loss: 3.92469747663\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (31.3539619446,53.3365279777), test loss: 43.2267111778\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.73398208618,23.6435525467), test loss: 2.58275414109\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (126.977256775,52.4794639132), test loss: 44.9286370754\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.59722614288,21.5957416012), test loss: 3.92311031818\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (55.4666748047,51.8025974722), test loss: 39.8503189564\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.46516752243,19.9224434931), test loss: 2.63773662448\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (76.8231964111,51.1965224466), test loss: 47.5639169693\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.9219121933,18.5289176979), test loss: 3.65522145033\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (39.3615951538,50.6643323848), test loss: 39.4173368216\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (4.34859848022,17.3480048962), test loss: 2.78568413854\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (31.2782173157,50.1573686502), test loss: 45.3010601759\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.2678194046,16.3361915051), test loss: 3.27027227879\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (95.3879318237,49.7099627892), test loss: 35.1294239998\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.05730104446,15.4589093359), test loss: 2.78196736574\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (37.2429847717,49.3155422352), test loss: 44.3725244999\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.40092349052,14.6903665124), test loss: 3.39245441556\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (13.1880054474,48.9318523618), test loss: 41.0097891808\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.33982849121,14.0107239831), test loss: 3.55852553844\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (39.9188461304,48.6049993163), test loss: 45.5480264783\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (7.3821978569,13.4077599968), test loss: 3.20644443035\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (12.8866443634,48.2780910338), test loss: 43.4792510986\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.62347316742,12.868296879), test loss: 3.59067190886\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (33.4041862488,47.9599065755), test loss: 40.7919535637\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.16906428337,12.3805191443), test loss: 2.61811347604\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (48.7329101562,47.6457841397), test loss: 43.3515557289\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.56611132622,11.9397899193), test loss: 3.7979359448\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (38.9601173401,47.3400652623), test loss: 38.310365653\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (6.03957462311,11.5385429), test loss: 2.50374754667\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (54.5903549194,47.043876844), test loss: 41.8398565292\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.96234309673,11.1695793009), test loss: 3.62590206861\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (27.1393642426,46.7681264448), test loss: 36.8594464779\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.51288688183,10.8314197825), test loss: 2.57248471677\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (22.7005500793,46.4870838784), test loss: 44.7957028866\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.780106544495,10.5202413462), test loss: 3.52993425727\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (34.5085372925,46.2120934967), test loss: 34.6361690998\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (5.04858160019,10.2332412817), test loss: 2.63673328161\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (59.7157669067,45.9335144126), test loss: 41.2967913151\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.42018914223,9.96554377334), test loss: 3.1923891902\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (21.8614501953,45.6400049836), test loss: 36.008437562\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.68678402901,9.71603937215), test loss: 3.28108736873\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (42.0735092163,45.3495922638), test loss: 40.1073165417\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (8.06899642944,9.4833444047), test loss: 3.26193092465\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (18.5008621216,45.0497175085), test loss: 36.8003743649\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.0986866951,9.26416276651), test loss: 3.45804920495\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (73.6748352051,44.7671232612), test loss: 34.613325119\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (7.24770402908,9.05897804709), test loss: 2.65129576921\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (35.0326957703,44.4654283215), test loss: 39.3948224068\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.12390041351,8.86485997679), test loss: 3.57632675171\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (29.9807777405,44.161701346), test loss: 35.4217619658\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.65123677254,8.67964168245), test loss: 2.34638075531\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (42.3724937439,43.839370279), test loss: 39.0327438593\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.60428667068,8.50413026619), test loss: 3.6256023407\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (14.9040813446,43.5086367816), test loss: 29.9258167982\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.22682523727,8.33766915836), test loss: 2.23880722523\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (138.899642944,43.1836211134), test loss: 38.1197474003\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (5.39591646194,8.17916012441), test loss: 3.45227493048\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (29.1148204803,42.8428405856), test loss: 30.1114359856\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.33720755577,8.02786834021), test loss: 2.40438281894\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (64.4451065063,42.5047317835), test loss: 38.3088477135\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.66456317902,7.88425595325), test loss: 3.24017826617\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.1633691788,42.1549482292), test loss: 27.3818742514\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.7059391737,7.74780121887), test loss: 2.37674526274\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (22.1559906006,41.798003011), test loss: 34.2414253712\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.653805375099,7.6168499854), test loss: 3.1628634721\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (10.5841035843,41.4302479665), test loss: 30.1623002768\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.23751211166,7.49174481253), test loss: 3.22560278475\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (33.8896598816,41.0601492067), test loss: 31.9594181061\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.66198146343,7.37189247363), test loss: 3.12660753727\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (7.39924526215,40.6908529644), test loss: 30.3781811237\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.1190571785,7.25633514127), test loss: 3.33375837207\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (25.4115695953,40.32492222), test loss: 27.6700437784\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.93670272827,7.1455468391), test loss: 2.42262477279\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (10.595867157,39.9552063183), test loss: 34.1562698126\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.92622506618,7.03938630867), test loss: 3.55198029131\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (39.628364563,39.5919761299), test loss: 28.4545064688\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.71575331688,6.9378865211), test loss: 2.17373597026\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (29.4559822083,39.2314699698), test loss: 33.4669854641\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.15873193741,6.84010573619), test loss: 3.48935345411\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (93.6560897827,38.8689105153), test loss: 26.5671762466\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.91181755066,6.74570445621), test loss: 2.30748306066\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.0551986694,38.5153940948), test loss: 34.7211009741\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.0344080925,6.65489486005), test loss: 3.41548479199\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (7.77565908432,38.1691065332), test loss: 26.3220911503\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.69002223015,6.56712272686), test loss: 2.42658251524\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (13.449505806,37.8330345002), test loss: 33.2791153431\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.63318681717,6.48265630144), test loss: 3.22246899009\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (17.4428062439,37.5035169015), test loss: 24.4945706844\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.870323777199,6.40146356048), test loss: 2.39750610143\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (15.5951652527,37.1831744403), test loss: 32.157458353\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.8142786026,6.32329700309), test loss: 3.19077247381\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (35.2781066895,36.8683087867), test loss: 29.6063117504\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.01848363876,6.24775597543), test loss: 3.30368235707\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (11.3850717545,36.5602483128), test loss: 31.7503559828\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.16340470314,6.17487061323), test loss: 3.26128611863\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (10.02186203,36.2642763763), test loss: 30.369824791\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.36976456642,6.10411846297), test loss: 3.56472040117\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (14.6655063629,35.9769059269), test loss: 29.0140454292\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.0885925293,6.0355074497), test loss: 2.50167460516\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (34.7206687927,35.6978146345), test loss: 32.8010347366\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.20981550217,5.96932525859), test loss: 3.62262178957\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.7811040878,35.4261506592), test loss: 28.5085203648\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.944307267666,5.90566300628), test loss: 2.21504597366\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.6851596832,35.1636666592), test loss: 32.5915967464\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.13977265358,5.84395719089), test loss: 3.45087876618\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (27.1137313843,34.9045997946), test loss: 28.5870810986\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.08774852753,5.78405317345), test loss: 2.43627250195\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (9.53827476501,34.6531747909), test loss: 34.0373764038\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.26651036739,5.72604739445), test loss: 3.39071709812\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (14.7460155487,34.4120346112), test loss: 26.9028664351\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.79485034943,5.66927771822), test loss: 2.48571920991\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (17.6680183411,34.1764586385), test loss: 32.5970869541\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.683203578,5.61426805518), test loss: 3.14445444345\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (11.9472351074,33.9461714395), test loss: 25.2133302689\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.3488689661,5.56102914867), test loss: 2.47249599993\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (25.2687492371,33.7243187325), test loss: 33.0516635418\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.604410052299,5.50963408608), test loss: 3.28111121058\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (5.47182559967,33.5091915727), test loss: 31.6752665997\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.53073191643,5.45966739175), test loss: 3.58092064708\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (4.62704467773,33.2941864569), test loss: 32.4886484146\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.48245584965,5.41092595229), test loss: 3.25627180636\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (6.15923213959,33.0900038089), test loss: 31.2817882538\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.92091727257,5.36344299249), test loss: 3.55133106112\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (52.1921539307,32.890079318), test loss: 29.9234024763\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (4.6071138382,5.31708829704), test loss: 2.35693243593\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.6508893967,32.6949939054), test loss: 32.5716216803\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.23102986813,5.27206312821), test loss: 3.67927155495\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (34.7274246216,32.5046503454), test loss: 29.8657830238\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.43193554878,5.22834446235), test loss: 2.28210294992\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.9621582031,32.3201005872), test loss: 32.278924799\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.68734717369,5.18593350784), test loss: 3.49187501073\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.1777153015,32.1380652921), test loss: 31.0667436123\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.70081949234,5.14447890902), test loss: 2.44947228283\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (20.0607681274,31.9606486134), test loss: 33.8173319101\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.58149719238,5.10421757968), test loss: 3.3268830508\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (13.0631313324,31.7891099947), test loss: 26.9272179604\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.782310009,5.0647047634), test loss: 2.45813141167\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.9102935791,31.6213948062), test loss: 32.3019042969\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.20005977154,5.02598960203), test loss: 3.16812495589\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (8.86498260498,31.4572295179), test loss: 25.019181788\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.82922792435,4.9883582607), test loss: 2.49255214036\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (43.5750465393,31.2974191004), test loss: 33.1477323651\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.58726930618,4.95191581392), test loss: 3.39979245961\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (24.4502830505,31.1419471174), test loss: 30.5059588909\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.4059098959,4.91633942832), test loss: 3.58193617761\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (17.8077220917,30.9868705594), test loss: 33.7606729031\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.876336157322,4.88146688353), test loss: 3.34927735329\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (18.8627033234,30.8363513204), test loss: 30.5300771236\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.22630572319,4.84751896584), test loss: 3.5193299368\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (12.4372386932,30.6908066272), test loss: 30.2826050282\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.89113581181,4.8139129648), test loss: 2.37110416591\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.3734369278,30.5473529988), test loss: 33.1618721485\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.8805886507,4.7811928273), test loss: 3.56133895516\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (6.6461057663,30.4060726795), test loss: 29.9990087509\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.49833342433,4.74928296199), test loss: 2.25258463919\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (19.0237770081,30.2692408298), test loss: 33.2107009172\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.5293097496,4.71834615179), test loss: 3.42947236598\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (22.6110534668,30.1363772665), test loss: 31.0914490223\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.54360717535,4.68803614643), test loss: 2.55739816278\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (7.48329353333,30.0017579264), test loss: 34.0249358654\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.74953961372,4.65829045057), test loss: 3.32356680036\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (3.2481610775,29.8735728982), test loss: 26.6903922558\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.7132076025,4.62905437267), test loss: 2.55615082979\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.15007209778,29.7461986541), test loss: 32.7920816183\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.41518139839,4.60034066784), test loss: 3.28648761213\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (14.7984371185,29.6226172107), test loss: 26.4425553322\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.53770136833,4.5723613142), test loss: 3.10018033385\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (19.2659130096,29.499952721), test loss: 34.0131919146\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.920864582062,4.54495370827), test loss: 3.34658692032\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (12.1751928329,29.3813644246), test loss: 29.4314853668\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.28121185303,4.51828659988), test loss: 3.45462885499\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (21.7389431,29.2629984633), test loss: 30.1196034431\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.338032364845,4.49206680896), test loss: 2.72647214681\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (35.0371208191,29.1471028731), test loss: 31.7521330357\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.31462955475,4.46645113226), test loss: 3.58774172068\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (16.7869873047,29.0346297657), test loss: 29.4952828407\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (4.85869979858,4.44112692234), test loss: 2.34058549404\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (16.4233703613,28.9239066388), test loss: 34.096360445\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.20224046707,4.41618510457), test loss: 3.55791501999\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (19.4439888,28.8147288478), test loss: 28.0805803776\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.00696730614,4.39175379198), test loss: 2.14574087411\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (12.5247459412,28.7073119086), test loss: 35.0813306808\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.81264603138,4.36806916671), test loss: 3.45623948276\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (10.6053237915,28.6034549826), test loss: 28.3965827465\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.42619979382,4.3448505539), test loss: 2.46070873141\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (23.5499382019,28.4986035142), test loss: 34.7115226269\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.9806842804,4.32194712005), test loss: 3.30210478306\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (6.50343990326,28.3965001451), test loss: 25.8088407516\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.66192555428,4.29952125271), test loss: 2.46122245789\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.22970676422,28.2975372732), test loss: 33.460338068\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.04029512405,4.27719538378), test loss: 3.28607981503\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (20.5567207336,28.1992065367), test loss: 30.6831807852\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.10407650471,4.25535709338), test loss: 3.3224635601\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (20.2695236206,28.1018128082), test loss: 32.8562986374\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.29898929596,4.23398383674), test loss: 3.29570807815\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (14.7752285004,28.0070035965), test loss: 30.1826180458\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.50620639324,4.21313190485), test loss: 3.41041918397\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (49.5308914185,27.9147519411), test loss: 28.5464205265\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.78501582146,4.19271215789), test loss: 2.65397989154\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (38.9616088867,27.8208702908), test loss: 32.7482210636\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.58812522888,4.17251322572), test loss: 3.62702733129\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (21.0095787048,27.7311893265), test loss: 29.1665869713\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.85029470921,4.15256808159), test loss: 2.33539038152\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (17.3220329285,27.6414132501), test loss: 35.0038969517\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.28558421135,4.13287576618), test loss: 3.5128620863\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (29.510974884,27.5539820891), test loss: 27.7363057613\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (3.40961313248,4.11365694713), test loss: 2.31062256098\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (8.78001213074,27.4667332946), test loss: 35.9191676378\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.970600485802,4.09473027117), test loss: 3.45085132122\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (8.82584381104,27.3824559133), test loss: 26.9004056931\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.06004345417,4.07621620074), test loss: 2.4750892356\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (6.81507825851,27.2975133884), test loss: 34.1512346745\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.33937740326,4.05802911344), test loss: 3.19630389512\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (10.2968845367,27.2140832601), test loss: 25.5468243599\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.05264365673,4.04012825823), test loss: 2.45184079409\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (23.9945335388,27.1332098314), test loss: 33.1112131953\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (5.52405738831,4.02231753434), test loss: 3.22556795478\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (16.5048789978,27.0528283991), test loss: 30.4380075932\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (5.49373340607,4.0048041623), test loss: 3.36019928455\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (46.140499115,26.9730179907), test loss: 32.9372903347\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.3736140728,3.98753592868), test loss: 3.30848153234\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (16.8108520508,26.8941666207), test loss: 30.8468017101\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.61037564278,3.9707610204), test loss: 3.58758426607\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (13.5392894745,26.817715149), test loss: 28.6537985563\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.9268014431,3.95427776745), test loss: 2.56527194977\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (11.8404197693,26.7397297538), test loss: 32.3617964268\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.22733902931,3.93792980154), test loss: 3.59708358645\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (10.0036211014,26.6641097069), test loss: 28.5321916103\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (3.81361436844,3.92190973051), test loss: 2.15926688015\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (25.1219444275,26.5904268431), test loss: 33.8947850704\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.70555651188,3.90585576936), test loss: 3.4008808434\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (15.2080020905,26.5166788514), test loss: 27.8938493252\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.79720830917,3.89012985133), test loss: 2.32366842628\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (11.9162502289,26.443418467), test loss: 35.4729876995\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.97759330273,3.87466925646), test loss: 3.36224800646\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (12.5104732513,26.3721408953), test loss: 27.0520486832\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.46332800388,3.85955796428), test loss: 2.48990371525\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (47.3153800964,26.3019881233), test loss: 33.6453508854\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.71355009079,3.84468977885), test loss: 3.13629327416\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (6.18889427185,26.23104018), test loss: 25.3655063152\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.600142300129,3.83000262396), test loss: 2.47665854692\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (14.5155658722,26.1631710825), test loss: 32.5595907688\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.13729596138,3.81539823), test loss: 3.18768182099\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (19.9292526245,26.094851272), test loss: 30.8590166569\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.796038687229,3.80094561333), test loss: 3.30264975727\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (17.8890762329,26.0278827915), test loss: 33.0023186803\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.48462629318,3.78678394012), test loss: 3.24617057145\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (13.732252121,25.9612260917), test loss: 31.9110406876\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.19021308422,3.77284935551), test loss: 3.55505832732\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (16.1052322388,25.8965623942), test loss: 29.6019331217\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.1586432457,3.75915917911), test loss: 2.504670614\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (29.5615310669,25.8311226497), test loss: 32.1700917244\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.19061207771,3.74566230709), test loss: 3.57974555492\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (20.45246315,25.7667417671), test loss: 29.5922624111\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.3026638031,3.73238430373), test loss: 2.20726845264\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.75972652435,25.7041157298), test loss: 33.5367133141\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.850389480591,3.71902500496), test loss: 3.39862811565\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (15.1999874115,25.6420709294), test loss: 29.5639982224\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.70712864399,3.70594044356), test loss: 2.4116192162\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (33.8051452637,25.5798347264), test loss: 33.9615963936\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.92202544212,3.69303153884), test loss: 3.24570787251\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (5.68386125565,25.5191104274), test loss: 26.8920424461\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.918105602264,3.68044333346), test loss: 2.42036406398\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (6.09217262268,25.459965565), test loss: 31.6745390177\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.86367821693,3.66804262455), test loss: 3.06057808697\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (15.2136440277,25.3992731334), test loss: 26.1673192978\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.02283215523,3.65571586969), test loss: 2.47319650352\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (11.0270042419,25.3404964693), test loss: 32.5664173603\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.83271121979,3.64357178707), test loss: 3.25171481669\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (35.5666236877,25.2829010485), test loss: 30.8400916576\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.23006343842,3.6314154468), test loss: 3.45008041859\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (15.2472934723,25.2252851145), test loss: 33.4287951469\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.61388683319,3.61946526066), test loss: 3.22911627293\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (11.6309661865,25.1677863946), test loss: 32.2541582584\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.78444170952,3.60767996227), test loss: 3.487610358\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (10.5812711716,25.1118040226), test loss: 30.4145503521\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.50178289413,3.59616065851), test loss: 2.3278819859\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (10.2204780579,25.0561194855), test loss: 32.2690555573\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.37558984756,3.584795923), test loss: 3.5438031137\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (23.0888748169,25.0007035648), test loss: 30.0300133944\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.94621622562,3.57356713266), test loss: 2.23007991016\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (26.0460586548,24.9469612483), test loss: 33.4384595394\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.63596200943,3.56233892821), test loss: 3.38942514658\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (17.3273773193,24.8927433602), test loss: 29.1040059805\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (3.02268385887,3.55122465577), test loss: 2.41963305324\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (14.1876296997,24.8395446256), test loss: 33.9530745029\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (4.74131298065,3.54028564947), test loss: 3.22776348889\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (16.0668926239,24.7864978003), test loss: 26.3713478565\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.6904540062,3.52953155063), test loss: 2.44606858492\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (8.07123470306,24.7349612612), test loss: 31.7348218918\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.323745042086,3.51892145158), test loss: 3.00232814252\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (11.3850059509,24.6824190921), test loss: 25.9257692337\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.938915848732,3.5084690581), test loss: 2.55420428962\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (10.9543704987,24.6310780026), test loss: 33.7661569595\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.17153596878,3.49815642534), test loss: 3.23840531409\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (11.0910778046,24.5810019203), test loss: 30.4219936848\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.37013602257,3.48773818644), test loss: 3.35055997968\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (4.43799161911,24.5310757944), test loss: 33.6564622641\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.308210164309,3.47751282722), test loss: 2.9650301069\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (23.1733360291,24.4806594902), test loss: 32.482986784\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.56782102585,3.46738691389), test loss: 3.40203804374\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (5.29116249084,24.4319814906), test loss: 30.6591520309\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.30347418785,3.45754404613), test loss: 2.36162731946\n",
      "\n",
      "MC # 4, Hype # hyp4, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold3/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (327.077148438,inf), test loss: 172.365135956\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (355.017028809,inf), test loss: 430.359063721\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (32.8491592407,126.509708374), test loss: 49.4371682167\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.07422971725,204.173309363), test loss: 3.97304186225\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (51.8043289185,86.6913719423), test loss: 31.8508023262\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.0150308609,103.782084305), test loss: 2.68840587139\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (16.3843345642,73.0385306999), test loss: 42.9950572491\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.710482120514,70.3027766594), test loss: 4.03314210773\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (28.235912323,66.3303910785), test loss: 35.6665486813\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.62102925777,53.560274477), test loss: 3.50848668814\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (90.7381134033,62.2570560469), test loss: 45.5290538311\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.6249063015,43.5168326789), test loss: 3.95106172264\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (58.3395614624,59.4207299525), test loss: 39.0473916531\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.218791008,36.8214227741), test loss: 3.60072193742\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (28.1264343262,57.388126608), test loss: 43.0543803692\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.52258443832,32.0315136501), test loss: 3.20687302947\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (43.6540527344,55.8291212485), test loss: 41.8225862503\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.46695566177,28.4349768676), test loss: 3.70111325979\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (20.0635738373,54.5688433388), test loss: 41.4983051062\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.29831218719,25.6373607214), test loss: 2.97348210812\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (42.5712509155,53.544963541), test loss: 39.1227649689\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.64221334457,23.3936162808), test loss: 3.73431978822\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (73.8390731812,52.7321327361), test loss: 35.163132906\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.34175443649,21.5593566212), test loss: 2.75747391582\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (53.25157547,52.0069762412), test loss: 42.7811241627\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.71310997009,20.0299152053), test loss: 3.75476413965\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (25.5317726135,51.3672877889), test loss: 34.9674544334\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.09894490242,18.7333467046), test loss: 2.82251278162\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (99.1756286621,50.8050309719), test loss: 42.5997533798\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.02747344971,17.6211707557), test loss: 3.74571368098\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (35.9116249084,50.2820494354), test loss: 33.1003703594\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.943107247353,16.6542490321), test loss: 3.11373747587\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (101.396850586,49.8312851048), test loss: 41.6310365677\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.30728149414,15.8078381759), test loss: 3.94007312655\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (19.5868148804,49.3861438589), test loss: 34.6260373592\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.34334945679,15.0583502474), test loss: 3.41235484779\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (17.6193122864,49.0198737962), test loss: 38.9336560726\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.64620566368,14.393529489), test loss: 3.15386824608\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (34.6948165894,48.6451990568), test loss: 38.1612490177\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.52056932449,13.7971686661), test loss: 3.32639379203\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (56.662071228,48.2973753092), test loss: 40.4248271704\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.1994304657,13.259178517), test loss: 2.97624427974\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (26.1689338684,47.955367475), test loss: 38.3488319397\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.85330748558,12.7711702487), test loss: 3.52799024582\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (50.5375900269,47.6270293582), test loss: 36.2973614693\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.01686441898,12.3262056386), test loss: 2.78713938892\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (52.368309021,47.2691861328), test loss: 36.0265178919\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (6.05798053741,11.918813426), test loss: 3.47121147513\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (26.3839626312,46.9018462152), test loss: 31.619285202\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.45551156998,11.5438415724), test loss: 2.70934848785\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (71.6645507812,46.5270331241), test loss: 39.4634292603\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.48383069038,11.1971338073), test loss: 3.55003206134\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (18.3670578003,46.1374713911), test loss: 28.3740558624\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.72709274292,10.8761516259), test loss: 2.60213819593\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (19.2195262909,45.7550143372), test loss: 36.9243276596\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.57023656368,10.5769894834), test loss: 3.636339733\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (32.1237869263,45.368681937), test loss: 29.350102663\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.60249042511,10.29780915), test loss: 3.10342519283\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (32.0105209351,44.9766444533), test loss: 36.4014853954\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.25607419014,10.0373647194), test loss: 3.83350425959\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (29.5304641724,44.5857325085), test loss: 29.7893084049\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.18401622772,9.79245089666), test loss: 3.28669610769\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (17.8976593018,44.2031302141), test loss: 31.7622361422\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.14038264751,9.56338427264), test loss: 3.11667144746\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (73.7415313721,43.810407378), test loss: 33.9678346157\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.3110575676,9.3482047591), test loss: 3.41482251287\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (23.1395664215,43.4109584279), test loss: 33.0825080395\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.08487796783,9.14552892175), test loss: 2.80487045348\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (50.6711463928,43.0144950989), test loss: 32.348592186\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.19306945801,8.95431890765), test loss: 3.3991912961\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (79.5578231812,42.6076659485), test loss: 26.3474416494\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.23495030403,8.77274311016), test loss: 2.59813463688\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (26.4217948914,42.2044453426), test loss: 32.9875708103\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.20300865173,8.60097110382), test loss: 3.38225157857\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (19.5106811523,41.7971763556), test loss: 26.0671472549\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.0767660141,8.43754749549), test loss: 2.7044091478\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (15.5085496902,41.4009739243), test loss: 34.2545425892\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.46783852577,8.28312330275), test loss: 3.62020114064\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (1.66804790497,40.9973576125), test loss: 22.2993566036\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.21441674232,8.13628983136), test loss: 2.60643680096\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (11.9932785034,40.6006494274), test loss: 32.4233377934\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.62396287918,7.99646180644), test loss: 3.77398760617\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (28.7287330627,40.2041961585), test loss: 25.2803775787\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.74159240723,7.86312346395), test loss: 3.30927462131\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (41.0993347168,39.8125716512), test loss: 32.2081408262\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.88956308365,7.73550581518), test loss: 3.8569434166\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (20.3629188538,39.4291670151), test loss: 26.4022753716\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.52743005753,7.61377903277), test loss: 3.4603869319\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (25.9624290466,39.0582479463), test loss: 27.8892895222\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (6.97377824783,7.49701423861), test loss: 3.06252662241\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (8.17265701294,38.6929933789), test loss: 30.7379989624\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.33994436264,7.38557687988), test loss: 3.51503649354\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.3147821426,38.3319323145), test loss: 29.5257579088\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.35543966293,7.27928996647), test loss: 2.75893406868\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (25.9699172974,37.985929425), test loss: 30.9323676348\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.53582787514,7.17730955129), test loss: 3.39123506546\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (31.6477851868,37.6447009246), test loss: 26.2683250189\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (5.70650100708,7.07925765447), test loss: 2.69190884084\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (9.8775062561,37.3119459918), test loss: 31.9711435795\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.61816859245,6.98491812339), test loss: 3.52644779682\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (14.9634990692,36.9921773697), test loss: 25.5783674002\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.35611486435,6.89392049572), test loss: 2.76274472475\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (16.1880149841,36.6827198829), test loss: 33.243688488\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.724992573261,6.8065805578), test loss: 3.71100152135\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (14.9400901794,36.3780131999), test loss: 22.3916407108\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.524726867676,6.72273736998), test loss: 2.54417462349\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.5722427368,36.0833557534), test loss: 33.2990570664\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.20045804977,6.64228929773), test loss: 3.90170307755\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (22.9630870819,35.8017121074), test loss: 26.9021073818\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.11463797092,6.56461156928), test loss: 3.31408204287\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (44.986618042,35.5236802441), test loss: 33.845564127\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (5.08530902863,6.48935226681), test loss: 3.94990159869\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (2.81369113922,35.2558362086), test loss: 27.3121931553\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.47167944908,6.41646011691), test loss: 3.48149735332\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (12.729598999,34.9963884388), test loss: 30.388005662\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.17511844635,6.3454804705), test loss: 2.99865224063\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (22.7383785248,34.7457622245), test loss: 30.5603722095\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.837499082088,6.27702080533), test loss: 3.55070425868\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (17.5846672058,34.4992258314), test loss: 30.572500658\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.13674771786,6.21084705458), test loss: 2.80992891192\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.8882331848,34.2636291478), test loss: 30.5667541504\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.73862457275,6.14701337668), test loss: 3.34681277275\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (27.9639110565,34.0325102407), test loss: 29.130312109\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.657845497131,6.08498738792), test loss: 2.75505194664\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (29.920042038,33.8081081833), test loss: 31.6265218735\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.70525956154,6.02471349081), test loss: 3.39009221196\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (16.6473922729,33.5909793142), test loss: 26.4446033478\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.86428487301,5.96641638931), test loss: 2.76505725533\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (18.9717025757,33.3818102795), test loss: 33.4817106247\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.68039536476,5.90960507283), test loss: 3.66467685699\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (9.06450748444,33.175490501), test loss: 22.2657770634\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.88692760468,5.85462147783), test loss: 2.53911993206\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (36.9715270996,32.9733400516), test loss: 33.9207239628\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.52322816849,5.8015178053), test loss: 3.81675726771\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (36.8161048889,32.7799689702), test loss: 27.0054667473\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.841469228268,5.74998920814), test loss: 3.39694340825\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (5.55233430862,32.5880927564), test loss: 34.4523265839\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.28174698353,5.69970728266), test loss: 3.66432236731\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (19.8969364166,32.4022048963), test loss: 27.3546542168\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.06029653549,5.65077373721), test loss: 3.3949695617\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.3881378174,32.222750921), test loss: 31.2894697189\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.903561353683,5.60289006488), test loss: 2.97150990367\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (19.1999702454,32.0482761637), test loss: 31.0197824955\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.67184197903,5.55646527562), test loss: 3.50579925179\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (7.58227062225,31.8741620711), test loss: 30.7916879416\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.8566532135,5.51136281914), test loss: 2.76407650709\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (18.440612793,31.7065138987), test loss: 30.378274107\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.49267435074,5.46769447575), test loss: 3.28999105096\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (26.7689170837,31.5452513744), test loss: 30.7238358974\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.98066592216,5.42518513549), test loss: 2.80868688822\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (5.56480979919,31.3843874641), test loss: 33.4525337696\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.95380222797,5.38338413969), test loss: 3.48129271269\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (11.7633857727,31.2284323204), test loss: 26.0425936699\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.67736005783,5.34265280793), test loss: 2.75823460221\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (14.7281913757,31.0767627957), test loss: 33.4169444561\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (5.45798921585,5.30285854119), test loss: 3.61959879398\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (49.8869094849,30.9293150637), test loss: 27.9968603373\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.47959542274,5.26413206541), test loss: 3.29282393456\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (16.0688648224,30.781318298), test loss: 34.422582221\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.989813089371,5.22631601185), test loss: 3.87358259261\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (13.5178785324,30.6402581682), test loss: 26.7675926685\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.45067095757,5.18969773853), test loss: 3.36216728985\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (15.8950843811,30.500527049), test loss: 28.8468274832\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.04992842674,5.15373968638), test loss: 3.11948641837\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (19.2426795959,30.3642317282), test loss: 29.3522070408\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.788188934326,5.11848489493), test loss: 3.3947907269\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.52606105804,30.2310563201), test loss: 31.4195463181\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.84684801102,5.08414356933), test loss: 2.93841878176\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (9.34769248962,30.1024106765), test loss: 31.3352050781\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.533223629,5.05037195843), test loss: 3.40296863616\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (27.9275436401,29.9744855456), test loss: 28.4011727095\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.20932865143,5.01744636738), test loss: 2.63109435439\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (37.6807174683,29.8484231142), test loss: 31.5843046904\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.27231001854,4.98542680334), test loss: 3.32434942722\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (29.0858650208,29.727135359), test loss: 28.1562549114\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.03716886044,4.9541492531), test loss: 2.73093471527\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.013086319,29.6056568835), test loss: 33.7502902031\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.36839365959,4.92341281109), test loss: 3.43924198449\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (16.8430976868,29.4875057571), test loss: 24.4491357803\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.94382119179,4.89328840457), test loss: 2.63545608968\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (11.3809909821,29.372893854), test loss: 33.484283638\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (4.42825508118,4.86363094469), test loss: 3.67008727193\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (13.052154541,29.260733025), test loss: 27.9232132435\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.0995092392,4.83461838425), test loss: 3.26721132398\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (17.3624076843,29.1476533886), test loss: 35.2025518417\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.879940271378,4.80622324543), test loss: 3.88925663829\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (31.2370529175,29.0388073835), test loss: 26.9961409807\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.860697329044,4.77867175451), test loss: 3.2635343492\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (33.3436355591,28.9330721781), test loss: 29.7582461596\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (4.13149261475,4.75169330284), test loss: 3.0604482919\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (8.04298305511,28.826977592), test loss: 29.9426172733\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.31336843967,4.72496829521), test loss: 3.41555093229\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (7.5470366478,28.7234304501), test loss: 31.13468256\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.41858816147,4.69875110416), test loss: 2.87683134079\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (4.77029275894,28.6225679245), test loss: 31.5560688257\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.04725432396,4.67301723538), test loss: 3.39405929446\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (14.1034288406,28.5237726022), test loss: 27.1197497368\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.43689918518,4.64781463699), test loss: 2.57954293191\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (24.7191696167,28.4241276936), test loss: 32.9854861259\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.45543456078,4.62308399912), test loss: 3.28909661472\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (17.9503211975,28.3283705743), test loss: 26.4166563511\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.91233676672,4.59901222844), test loss: 2.67592381835\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (39.1575622559,28.2337001133), test loss: 34.4594017744\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (3.99365282059,4.57530702629), test loss: 3.4786930263\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (15.6147232056,28.1399195163), test loss: 24.0593421936\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.38945627213,4.55188991531), test loss: 2.55216394663\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (50.0334815979,28.048911761), test loss: 33.4190111637\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.55246448517,4.52896470363), test loss: 3.73728550673\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (14.6261863708,27.9593097046), test loss: 27.0854620457\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.75171613693,4.50629548415), test loss: 3.21469600499\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (25.057352066,27.8702130485), test loss: 34.8213954449\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (3.40552926064,4.48410981059), test loss: 3.81826076508\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (14.1218986511,27.7818414628), test loss: 26.9454877377\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.35582685471,4.46242909299), test loss: 3.37671704292\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (12.5988349915,27.6964860496), test loss: 29.9695685148\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.53171402216,4.4411634051), test loss: 3.05948032141\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (29.8738632202,27.6112032675), test loss: 29.7515259743\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.14768648148,4.42022222232), test loss: 3.41802254021\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (18.7675018311,27.5270042755), test loss: 30.276387167\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.44585776329,4.39955701573), test loss: 2.67338800728\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (14.9112224579,27.4456833001), test loss: 30.6207884312\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (4.71297359467,4.37915152701), test loss: 3.23694618642\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (38.2907447815,27.3657723791), test loss: 28.0182792187\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (4.30579519272,4.3590950175), test loss: 2.60729898065\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (25.5116348267,27.2842359242), test loss: 32.9744723797\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.33889579773,4.33935283694), test loss: 3.34581199884\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (33.4783325195,27.2060925481), test loss: 26.1552763939\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.60642194748,4.32018725913), test loss: 2.65450513065\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (16.0542831421,27.1295125309), test loss: 33.1010278702\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.51031839848,4.30131301609), test loss: 3.49975794554\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (14.7524862289,27.0524032947), test loss: 24.0620910168\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.772923469543,4.28255828111), test loss: 2.4793102771\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (22.9224472046,26.9771935976), test loss: 33.6471746922\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (6.08129501343,4.26415756896), test loss: 3.72393470407\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (8.7462644577,26.9034048806), test loss: 26.646566534\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (3.4297747612,4.24593908346), test loss: 3.28881455064\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (15.3052883148,26.8309327257), test loss: 35.7786296368\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.19407939911,4.22801925594), test loss: 3.7689032048\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (19.7373046875,26.7576344408), test loss: 28.2648359537\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.61399841309,4.21040983762), test loss: 3.3410459131\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (10.966917038,26.6867233922), test loss: 31.4497320652\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.16874921322,4.19321405471), test loss: 2.89497757256\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (23.7118778229,26.6168915845), test loss: 29.3378360271\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (4.20524024963,4.17627142493), test loss: 3.42101664543\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (31.5076141357,26.5472963523), test loss: 30.4888667107\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (3.9156730175,4.159452933), test loss: 2.7172036916\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (31.9902515411,26.4796775633), test loss: 30.158481884\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.29823803902,4.14286564769), test loss: 3.2159429729\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (17.7142410278,26.4127241129), test loss: 28.83539505\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.5922832489,4.1264810526), test loss: 2.56367163062\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (15.9414863586,26.3461075255), test loss: 32.8900935173\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (3.22355365753,4.11035443455), test loss: 3.31829072237\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (16.4857196808,26.279722564), test loss: 26.040453124\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.50956320763,4.0945682461), test loss: 2.67108426839\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (5.09443664551,26.2154440879), test loss: 32.4532487392\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.971226096153,4.07903848518), test loss: 3.45893161297\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (6.60760498047,26.1511645356), test loss: 22.8137694836\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.46020507812,4.06374688774), test loss: 2.7208840996\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (7.7255654335,26.087426989), test loss: 33.6259377003\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.01858282089,4.04856151979), test loss: 3.66486623883\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (9.14186096191,26.0259567459), test loss: 26.4816818714\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.34831166267,4.03352899955), test loss: 3.25399161279\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (4.0858631134,25.9650710066), test loss: 33.6069214582\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.479712843895,4.0186977328), test loss: 3.16740990281\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (43.1845397949,25.9033370375), test loss: 27.850313282\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.6244969368,4.0040922039), test loss: 3.23832506239\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (10.6366653442,25.8429362834), test loss: 32.6446397066\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.70431232452,3.98986713568), test loss: 2.8691832155\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (4.41210603714,25.7844336593), test loss: 29.1387640953\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.485289812088,3.9758127541), test loss: 3.40265191495\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (19.1521682739,25.7251183849), test loss: 29.9771245003\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (4.06201076508,3.96186813571), test loss: 2.68606970608\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (11.2515573502,25.6671439604), test loss: 29.9065957069\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.63023984432,3.94804950847), test loss: 3.18634279966\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (40.0800666809,25.6105042502), test loss: 27.7742716789\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.5101583004,3.93438121206), test loss: 2.67087576985\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (21.2360610962,25.5540237442), test loss: 33.5707033634\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.9403834343,3.92090766445), test loss: 3.33549348116\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (13.0080385208,25.4969101829), test loss: 25.8201678753\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.38960123062,3.9076320315), test loss: 2.63652359843\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (8.3094291687,25.4415615931), test loss: 33.3518660784\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.06666731834,3.8946297519), test loss: 3.40222337842\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (10.2740402222,25.3872332595), test loss: 27.921249342\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (3.10147333145,3.88181664146), test loss: 3.17609674633\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (42.1360092163,25.3329998471), test loss: 34.4278019905\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (5.92760467529,3.86907689406), test loss: 3.72672796249\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (15.4302148819,25.2798578739), test loss: 26.1860939503\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.03500461578,3.85640897008), test loss: 3.24762488604\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (15.3592634201,25.2271278818), test loss: 30.6754979849\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.72095263004,3.8439175625), test loss: 2.94273200929\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (17.9608249664,25.1747384029), test loss: 28.3679181814\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (6.25054454803,3.83164997276), test loss: 3.31845860481\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (17.9618911743,25.1220429487), test loss: 32.4575305939\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.70916366577,3.81951195028), test loss: 2.80484991074\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (10.7006225586,25.0712706198), test loss: 30.0771547556\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.64884757996,3.80756892086), test loss: 3.27600252032\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (15.1811933517,25.0201682864), test loss: 28.673744154\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.73865818977,3.79582593639), test loss: 2.52379679084\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (20.2324771881,24.9696387202), test loss: 31.3685115099\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.94790744781,3.78411921217), test loss: 3.23463470936\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (7.74735736847,24.9204789234), test loss: 26.6768836021\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.63116145134,3.77249139826), test loss: 2.56748534441\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (10.693444252,24.8719573597), test loss: 33.704605484\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.335929870605,3.76100228052), test loss: 3.29598207772\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (28.7167472839,24.8227260228), test loss: 23.692000103\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.33112430573,3.7497013996), test loss: 2.51231169105\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (8.25613498688,24.7739429096), test loss: 33.8228843927\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.14087915421,3.73863378588), test loss: 3.5158516854\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (10.8712482452,24.7271758468), test loss: 27.2513340473\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.98018085957,3.72771512051), test loss: 3.24175979793\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (19.0619106293,24.6793667588), test loss: 34.8311415434\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.1820166111,3.71683596031), test loss: 3.72046546936\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (11.7296152115,24.6326716023), test loss: 25.5574671984\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (3.99617862701,3.70606880103), test loss: 3.12661388218\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (20.2569942474,24.586931042), test loss: 29.902474165\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.76101565361,3.69533700071), test loss: 2.88536742628\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (17.7244110107,24.5412404338), test loss: 30.3173871994\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.04829692841,3.68477553422), test loss: 3.29145703316\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (21.638584137,24.4947343808), test loss: 32.8943849087\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.77817487717,3.67433225024), test loss: 2.70166870058\n",
      "\n",
      "MC # 4, Hype # hyp4, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold4/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (332.621154785,inf), test loss: 183.654831505\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (207.981521606,inf), test loss: 253.595676422\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (35.9176673889,87.763584341), test loss: 44.524667716\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.699271559715,35.2195165693), test loss: 3.02384578586\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (107.269515991,67.4847638369), test loss: 40.4574435711\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.9813990593,19.2442135429), test loss: 3.16953863502\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (22.6085720062,60.8165878016), test loss: 42.7111636162\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.0888915062,13.9174419329), test loss: 3.17261161208\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (8.30645561218,57.2863195155), test loss: 44.4198308945\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.77438569069,11.2504683185), test loss: 3.28343532681\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (30.2702484131,55.3302980465), test loss: 41.7541266441\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.3060874939,9.65331284556), test loss: 2.59454129636\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (17.33020401,53.9099391894), test loss: 46.9829025269\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.29041159153,8.58855460525), test loss: 3.3422991842\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (58.4887199402,52.9008448669), test loss: 42.7565547466\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.81973314285,7.83165400779), test loss: 2.74840429127\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (29.7461071014,52.1175713339), test loss: 45.2806262016\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.700488090515,7.25962673212), test loss: 3.24912039638\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (29.2004852295,51.4831762419), test loss: 38.7137266159\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.3568572998,6.81655405583), test loss: 2.89067997932\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (52.0470237732,50.9197608996), test loss: 47.523779583\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.27774620056,6.46131465933), test loss: 3.20357148051\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (212.391296387,50.4887494058), test loss: 36.8389043331\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (8.09349250793,6.17051830802), test loss: 2.65411629081\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (17.9478683472,50.0747068865), test loss: 43.7672340155\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.44806420803,5.92649048753), test loss: 3.16457009315\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (19.4203681946,49.7677549717), test loss: 39.9775515556\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.41790628433,5.72286198708), test loss: 3.17259714901\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (72.3002471924,49.4665107775), test loss: 43.6049303532\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (8.16045570374,5.54865675983), test loss: 3.04983359277\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (40.3058052063,49.1956906516), test loss: 43.8229203224\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (9.0837802887,5.39725078094), test loss: 3.24128190577\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (21.2429275513,48.9504883729), test loss: 41.4725530624\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.49771928787,5.26420163565), test loss: 2.56669575274\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (37.4813041687,48.7081128339), test loss: 44.8306458473\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.37939310074,5.14643396351), test loss: 3.28150937855\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (27.4937820435,48.4655479042), test loss: 36.7425985336\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.13804793358,5.04201264072), test loss: 2.61062757671\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (20.9096870422,48.2454622504), test loss: 46.3189839363\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.71443343163,4.94666582487), test loss: 3.3367423594\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (37.2771759033,48.0572613663), test loss: 36.78610816\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.22582840919,4.86239784769), test loss: 2.80732182264\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (37.0964279175,47.8783409173), test loss: 43.6654262543\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.588098526,4.7859053163), test loss: 3.06065175533\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (10.8664693832,47.6892879626), test loss: 33.7327699184\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.73397612572,4.71712113276), test loss: 2.63489352465\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (42.2807617188,47.5187999408), test loss: 42.0180940628\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.92135453224,4.65348410304), test loss: 3.11015166044\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (21.5994682312,47.3373447092), test loss: 39.7286041737\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.68563055992,4.59482415806), test loss: 3.2475376606\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (29.344039917,47.154857933), test loss: 38.0320184469\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.10977864265,4.54072302677), test loss: 2.79808033109\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (34.1335792542,46.9822348317), test loss: 45.0073573112\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.76917409897,4.49062516478), test loss: 3.21051301062\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (52.8926353455,46.8062137103), test loss: 38.4618993759\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.52560710907,4.44306332765), test loss: 2.50922878981\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (19.3376960754,46.6416287053), test loss: 43.3225984573\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.23331987858,4.39890086329), test loss: 3.27433946133\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (78.8102416992,46.4753271389), test loss: 36.1980081081\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.64934122562,4.35792370551), test loss: 2.72661345601\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (55.6767578125,46.3084402433), test loss: 43.4422454834\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.19596552849,4.32038015767), test loss: 3.17167520523\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (35.9844207764,46.1334675628), test loss: 34.5691709995\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.20517611504,4.28318221414), test loss: 2.8830914259\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (33.8592948914,45.9571458872), test loss: 40.1077876568\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.61432647705,4.24860265777), test loss: 2.88901745677\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (15.0384626389,45.7639336512), test loss: 36.4488477707\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.83277273178,4.21503159929), test loss: 2.96481776834\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (33.7264289856,45.5905559868), test loss: 38.5258536816\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.86948943138,4.18305940347), test loss: 3.03753166199\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (23.8914489746,45.393959527), test loss: 38.6644659042\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (5.41210222244,4.15201693394), test loss: 3.17845821381\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (32.5310096741,45.2115530001), test loss: 35.763699317\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (0.835636496544,4.12266110036), test loss: 2.65095112622\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (14.4704713821,45.0107442582), test loss: 40.630232811\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.69410729408,4.09433984194), test loss: 3.20826280713\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (31.0251941681,44.8066080288), test loss: 35.9410757065\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.30547237396,4.06739657176), test loss: 2.50265809894\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.3401355743,44.5970266224), test loss: 37.6572971344\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.17665433884,4.04068131814), test loss: 3.06330230832\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (19.473197937,44.3751802167), test loss: 32.3106537342\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.15583562851,4.01474957265), test loss: 2.6639213711\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (31.6421222687,44.1389895273), test loss: 38.9961910725\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.604616642,3.98931651664), test loss: 2.98180779815\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (25.0434608459,43.8984732209), test loss: 28.4652920723\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.78988623619,3.96408265679), test loss: 2.5080303669\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (14.4352893829,43.6530336202), test loss: 35.2170621157\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.66173458099,3.93940112466), test loss: 2.92298953831\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (24.3173542023,43.4041147523), test loss: 31.2579086781\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.850309848785,3.91554314937), test loss: 2.88674220741\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (28.3375263214,43.139601938), test loss: 33.9704614162\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.09951210022,3.89183544814), test loss: 2.8208031565\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (10.5871734619,42.870516914), test loss: 32.8410200119\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.89379692078,3.86849690568), test loss: 3.01837198734\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (49.3488311768,42.5952457284), test loss: 31.8697892904\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.81909489632,3.84555577082), test loss: 2.36885071099\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (51.6439552307,42.3053003832), test loss: 34.2406884193\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.12937879562,3.8223954479), test loss: 3.02168302536\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (23.7531528473,42.0077519876), test loss: 26.3607760429\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.17863869667,3.79958226879), test loss: 2.29283942282\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (23.3767700195,41.7078557873), test loss: 34.5661635399\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.27653849125,3.77632900974), test loss: 2.91154993176\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (20.8134593964,41.4049798868), test loss: 26.4502741337\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.80644249916,3.75373144521), test loss: 2.51676071286\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (68.3671798706,41.0984445327), test loss: 31.4911243439\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.15672206879,3.73110969575), test loss: 2.6849165082\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.7243881226,40.7867067382), test loss: 23.2220655918\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.855236768723,3.70918838204), test loss: 2.32559749037\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (23.7752304077,40.4759486651), test loss: 30.3492288113\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.36304855347,3.68735925357), test loss: 2.73095569909\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (22.9523544312,40.1623441138), test loss: 27.3583034992\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.37819337845,3.66560951862), test loss: 2.84662515521\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (66.2667770386,39.8463667626), test loss: 26.5704954863\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (5.6743516922,3.64408907953), test loss: 2.54110993743\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (25.3194103241,39.5357690404), test loss: 31.7184468985\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (6.55351161957,3.62285023919), test loss: 2.84919672906\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (27.6795692444,39.2278929676), test loss: 27.5552332878\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.88032674789,3.60147649133), test loss: 2.24907835424\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (29.2943572998,38.9238829725), test loss: 30.5911703348\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.8184170723,3.58070126407), test loss: 2.781380409\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (16.9844932556,38.6214386114), test loss: 26.5759124756\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.50488388538,3.56018571972), test loss: 2.45159236193\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (40.0281906128,38.3285152465), test loss: 31.8612287521\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.20948338509,3.54063139862), test loss: 2.80334723294\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.9341125488,38.0410707443), test loss: 25.3542687178\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.13151144981,3.52107533472), test loss: 2.56096276492\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (30.0094108582,37.7566855223), test loss: 29.2288063288\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.29372680187,3.5020517187), test loss: 2.61042663157\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (9.31683349609,37.4762525946), test loss: 28.2225838661\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.36469864845,3.48325255607), test loss: 2.63990143836\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (23.5073509216,37.2081795706), test loss: 30.0059926271\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.17353773117,3.46480333194), test loss: 2.83301755786\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (55.9204101562,36.9428093342), test loss: 27.8813816547\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.33286142349,3.44658610885), test loss: 2.92000434697\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (28.5014648438,36.6845837381), test loss: 27.7097923756\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.46594619751,3.42900959924), test loss: 2.54836999774\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (11.5465736389,36.4300595207), test loss: 30.9577304125\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.638916373253,3.41165487486), test loss: 2.87565674782\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (27.5875968933,36.1847133991), test loss: 28.9270911694\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.23852062225,3.39510475136), test loss: 2.31535463333\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (8.84089660645,35.9458731222), test loss: 29.8937772751\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.895369887352,3.37882075384), test loss: 2.6953015089\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.0898895264,35.7092947672), test loss: 27.6128352165\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.47442227602,3.36287256036), test loss: 2.54422254264\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (23.589679718,35.4780660463), test loss: 30.2823682547\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.46082472801,3.34713318845), test loss: 2.64104791582\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (6.95151472092,35.2539176371), test loss: 25.0553404331\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.95604801178,3.33171158805), test loss: 2.501589939\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (17.0663375854,35.034545449), test loss: 31.1224378109\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.72957801819,3.31641815715), test loss: 2.71196890175\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (33.7434921265,34.8202551065), test loss: 26.9500808239\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (4.37372303009,3.30178000166), test loss: 2.71182261109\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (30.3427352905,34.6099050138), test loss: 31.4535106182\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.34347224236,3.28728548588), test loss: 2.86662479639\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (9.86499404907,34.4056917154), test loss: 27.7797395706\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.09724879265,3.27340371162), test loss: 2.958881028\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (21.833946228,34.2074395536), test loss: 30.0192056656\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.790483236313,3.25981331693), test loss: 2.48442986012\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (23.5212917328,34.0102315695), test loss: 29.9973372698\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.4149107933,3.24638137455), test loss: 2.91044052243\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (6.46064662933,33.8179351747), test loss: 28.0195869446\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.03948354721,3.23327090488), test loss: 2.382581155\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (12.7074737549,33.6321823339), test loss: 31.4664527655\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (4.040620327,3.22009755632), test loss: 2.74782436937\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (21.1272983551,33.4491557072), test loss: 26.892668581\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (5.60547876358,3.20733541279), test loss: 2.6464040339\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (8.22597312927,33.2691618543), test loss: 31.0573605061\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.269759655,3.19473931636), test loss: 2.69116762131\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (7.07949304581,33.0944908921), test loss: 24.2339221478\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.21867370605,3.18275181006), test loss: 2.41497915387\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (4.40970087051,32.9245110012), test loss: 30.8567851543\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.64139652252,3.17091939707), test loss: 2.77030137479\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (36.9440002441,32.7570948074), test loss: 27.3648993492\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.69710445404,3.1593260352), test loss: 2.8434630096\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (12.0999765396,32.5913739075), test loss: 31.943331337\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.685245454311,3.14790686381), test loss: 2.87241472602\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (6.12617492676,32.4316326418), test loss: 30.7499799252\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.12787532806,3.13661912668), test loss: 2.87109698355\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (37.2111549377,32.2752991981), test loss: 29.7082593918\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.994149267673,3.1254087895), test loss: 2.42983335555\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.24690818787,32.1203817339), test loss: 30.628843236\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.77512276173,3.11443374215), test loss: 2.73046373427\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (17.5288944244,31.9674131825), test loss: 28.1839289665\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.67533063889,3.1036181084), test loss: 2.49865109324\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (13.0845375061,31.8200362838), test loss: 32.3355091095\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.842433571815,3.09333507401), test loss: 2.83328021616\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (21.9553222656,31.6765387168), test loss: 27.4345816851\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.15316915512,3.08314500994), test loss: 2.61475465894\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (15.1346435547,31.5330929291), test loss: 30.0304981232\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.28047847748,3.07313472647), test loss: 2.6296111919\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (20.1878395081,31.3924161996), test loss: 28.2039311886\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.51464509964,3.06317869346), test loss: 2.62563331127\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (19.3923206329,31.2570129861), test loss: 31.8961973429\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.30233669281,3.05337794984), test loss: 2.85855746269\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (6.89772033691,31.1217756289), test loss: 27.4067249298\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.21108913422,3.04362170295), test loss: 2.77469507158\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (21.9076118469,30.9904541796), test loss: 29.1385048866\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.08917212486,3.03418372651), test loss: 2.58722347617\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (23.446472168,30.85948862), test loss: 31.1400381565\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.29455375671,3.02477139749), test loss: 2.88330745697\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (26.5999355316,30.7327124807), test loss: 29.3826695442\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.746745347977,3.01576180818), test loss: 2.31222423613\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (14.3826665878,30.6090926287), test loss: 30.0872657299\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.46057426929,3.00693361024), test loss: 2.66865688562\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (19.571138382,30.4850067226), test loss: 29.7612609386\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (3.99578380585,2.99818129583), test loss: 2.54490375817\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (5.80118942261,30.3635646487), test loss: 30.0569006681\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.395195961,2.9894197011), test loss: 2.59479822069\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (10.3125629425,30.2458037449), test loss: 26.1153856277\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (4.22755050659,2.98076895172), test loss: 2.52637963742\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (13.3582801819,30.1290939249), test loss: 31.6923590422\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.04042434692,2.97220922808), test loss: 2.6678689599\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (22.2055339813,30.0142307358), test loss: 27.153571105\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (4.22179508209,2.96387191203), test loss: 2.70265132189\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (16.7472991943,29.9010167977), test loss: 31.695417881\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.943847060204,2.95562977235), test loss: 2.87786886096\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (5.88716459274,29.7909378443), test loss: 28.0701116085\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.04992008209,2.94769933748), test loss: 2.9065487206\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (54.8445625305,29.6830048452), test loss: 30.5808385849\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (4.11798906326,2.93990701582), test loss: 2.49990143776\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (26.0582809448,29.5743301225), test loss: 30.0071649551\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.73898911476,2.93210818739), test loss: 2.86763509512\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (8.47115039825,29.4687713825), test loss: 30.0174339294\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (3.46509003639,2.92446661722), test loss: 2.39736487865\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (6.83149814606,29.3657089708), test loss: 31.4263377666\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.28756010532,2.91665155366), test loss: 2.74143249094\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (14.9729099274,29.2631506933), test loss: 26.8999586105\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.6861846447,2.90907747577), test loss: 2.56992200911\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (15.9716253281,29.1617842316), test loss: 31.6453442812\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.52903962135,2.90159053335), test loss: 2.74363824502\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (13.6007633209,29.0627664487), test loss: 24.8278051138\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.0179142952,2.89444938032), test loss: 2.35942706168\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (21.2356395721,28.9660080173), test loss: 31.0562648058\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.57824802399,2.88731987004), test loss: 2.71277641505\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (20.1976203918,28.8693735612), test loss: 27.4583833218\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.482973068953,2.8803100735), test loss: 2.80948196352\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (33.8884391785,28.7740717514), test loss: 32.0550012589\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (4.07227325439,2.87340896932), test loss: 2.84770592004\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.81048583984,28.6815138322), test loss: 30.1930544376\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.49920964241,2.86647009623), test loss: 2.82552013397\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (13.1826286316,28.5897600185), test loss: 30.9666666508\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.23130893707,2.85959984473), test loss: 2.41054523587\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (11.9853115082,28.4990955143), test loss: 30.9819394112\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.18626260757,2.85280066726), test loss: 2.74172098637\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.74798727036,28.4082259166), test loss: 27.3589099884\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.2206223011,2.84605741292), test loss: 2.3610635832\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (7.90321731567,28.3207636498), test loss: 32.6666564465\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.90835916996,2.83967285419), test loss: 2.85728155077\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (14.1392993927,28.234963148), test loss: 27.4013885021\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.860370635986,2.83329547445), test loss: 2.5996542573\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (20.3046588898,28.1485942416), test loss: 30.2498816013\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.88232350349,2.8269992224), test loss: 2.57915134802\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (22.437412262,28.0635783744), test loss: 24.4823712349\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.46939086914,2.82069928526), test loss: 2.37879575193\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (52.4819602966,27.9814178972), test loss: 31.1148350239\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.67099905014,2.81446964309), test loss: 2.79267294407\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (17.5683250427,27.8990593153), test loss: 27.7882801056\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.88740873337,2.80821186639), test loss: 2.81974384785\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (42.653881073,27.8181600509), test loss: 29.3313098431\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.7200255394,2.80214949108), test loss: 2.60352530777\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (15.4483604431,27.7370655411), test loss: 31.1374181509\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.43453478813,2.79609311591), test loss: 2.85526449382\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (15.2151603699,27.6585099506), test loss: 29.1988233566\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.40825986862,2.79026711533), test loss: 2.2394220382\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (32.070640564,27.5817223819), test loss: 30.3901273727\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (3.93026995659,2.78453854419), test loss: 2.66400565505\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (17.294342041,27.5037155653), test loss: 28.9212099075\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.91831052303,2.77880569413), test loss: 2.4839710772\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (20.7316188812,27.4276153325), test loss: 30.3190378547\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.61262512207,2.77309773737), test loss: 2.6199717164\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (18.1170005798,27.3531364661), test loss: 26.1820546389\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.55596709251,2.76734817382), test loss: 2.52845980227\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.47319793701,27.2790227383), test loss: 30.9168242693\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.846588134766,2.76172985944), test loss: 2.60195763111\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (14.3918581009,27.2055683797), test loss: 26.9040341377\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (3.73269915581,2.75615858086), test loss: 2.67436877787\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (5.18547964096,27.1329482684), test loss: 31.6218883276\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.50640511513,2.75071634879), test loss: 2.837782076\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (17.2225666046,27.0623349031), test loss: 27.8996935368\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.12254548073,2.74541864271), test loss: 2.8546762526\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (12.1511526108,26.9919692571), test loss: 29.8871483803\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.48272609711,2.74016258029), test loss: 2.49115327001\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (3.26196432114,26.9217964942), test loss: 29.7745797634\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.70653557777,2.73495269579), test loss: 2.8742580086\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (9.73022079468,26.8532477407), test loss: 31.2826521873\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.90045785904,2.72976475436), test loss: 2.40090459436\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (7.55888652802,26.7859589342), test loss: 30.3126725435\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.9314031601,2.72449066217), test loss: 2.6890530616\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.8253068924,26.7184240552), test loss: 27.175262928\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.521548390388,2.71933255786), test loss: 2.53932617009\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (55.8439559937,26.6513688757), test loss: 31.7576313496\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.34182572365,2.71419415323), test loss: 2.66947056502\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (12.3918857574,26.5858907678), test loss: 25.2407496214\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.945343613625,2.70931545776), test loss: 2.38246638477\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (13.4398288727,26.5215792056), test loss: 31.0076291919\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.933421730995,2.70441025839), test loss: 2.69364798367\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (18.2519817352,26.4569454331), test loss: 27.0435153008\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.677475214,2.699612513), test loss: 2.74270655811\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (17.4280128479,26.3929552422), test loss: 31.7494507313\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.01450634003,2.69479495649), test loss: 2.76320423782\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (9.41659927368,26.331254381), test loss: 28.2363966942\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.66625714302,2.68999222166), test loss: 2.81584440172\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.66763877869,26.269330609), test loss: 31.4615853786\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.4273891449,2.6852243417), test loss: 2.40546213984\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (16.2353439331,26.2079055296), test loss: 31.1553232193\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.54948043823,2.68049993917), test loss: 2.79600775242\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.73398685455,26.1460588426), test loss: 27.4016590118\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.04822611809,2.67577162779), test loss: 2.25844504759\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (2.43305563927,26.0864671517), test loss: 32.4450817108\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.05981969833,2.6712816478), test loss: 2.83003868759\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (8.7478427887,26.0277494442), test loss: 27.3711965084\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.89904987812,2.6668133841), test loss: 2.5864703536\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (16.6497516632,25.9682100527), test loss: 30.378328681\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.11792230606,2.66236599748), test loss: 2.56087362468\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (6.58239459991,25.909804225), test loss: 24.408682704\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.60839748383,2.65791621017), test loss: 2.3400414288\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (6.90066242218,25.8527006606), test loss: 30.860894084\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.14468359947,2.6534803695), test loss: 2.70584591627\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (27.6503753662,25.7960826399), test loss: 27.1092759371\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.8139693737,2.64903794694), test loss: 2.74650801122\n",
      "\n",
      "MC # 4, Hype # hyp4, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold5/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (444.639160156,inf), test loss: 212.662215805\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (284.774871826,inf), test loss: 368.215486145\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (152.761825562,186.834276741), test loss: 130.756403255\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.714414596558,133.025523425), test loss: 3.24442169368\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (53.0098648071,140.739998772), test loss: 59.9932005405\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.71554327011,68.1242946455), test loss: 3.61135002971\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (194.235824585,110.610239511), test loss: 42.6905007362\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (7.07308769226,46.4676656791), test loss: 3.21710775197\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (29.6805686951,94.5907405291), test loss: 42.9089463711\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.78918075562,35.6210030657), test loss: 3.29356675744\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (32.3856124878,85.0308173416), test loss: 38.1640201092\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (7.30545520782,29.120151022), test loss: 2.69735640883\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (38.3726844788,78.5788219022), test loss: 46.8800686836\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.12300992012,24.7876984496), test loss: 3.44989007711\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (27.2673225403,73.9621720368), test loss: 41.1410783291\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.88543128967,21.6913495892), test loss: 2.73210968971\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (51.0209465027,70.4436337891), test loss: 45.6508497715\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.4408864975,19.3707725041), test loss: 3.47718243599\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (29.9015789032,67.6859469852), test loss: 39.3957793713\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.10570764542,17.5661661604), test loss: 2.84161966443\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (43.2852630615,65.5212918063), test loss: 46.2739715099\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.25831913948,16.1227844543), test loss: 3.4412635088\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (29.0618972778,63.7135700779), test loss: 37.9770886898\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (0.879465520382,14.9402238479), test loss: 2.6999573648\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (38.9991226196,62.2448524688), test loss: 44.3603559017\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (5.29603481293,13.9579036721), test loss: 3.26309595704\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (16.4540576935,60.9630472521), test loss: 34.6295469999\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.80588793755,13.1261297004), test loss: 2.5773240447\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (72.1974563599,59.8722422059), test loss: 43.0241369247\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (5.45702457428,12.4125484115), test loss: 3.19837760627\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (33.638053894,58.8864839777), test loss: 41.0828610897\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.38563489914,11.7945784294), test loss: 3.26577035189\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (13.8525218964,58.0110522604), test loss: 43.4387519836\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.40725374222,11.2535145192), test loss: 3.01458740234\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (68.337928772,57.2709676116), test loss: 44.6543739319\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.41361188889,10.7765186228), test loss: 3.27637600005\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (49.8906860352,56.5809543614), test loss: 39.2045692921\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.19764900208,10.3515132134), test loss: 2.65245612264\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (19.3073005676,55.9861141211), test loss: 44.7285638809\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.63481426239,9.97273539714), test loss: 3.4967816174\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (23.0592460632,55.4288266195), test loss: 36.9248790264\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.588838577271,9.63148417099), test loss: 2.5534403801\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (44.6393508911,54.9288699645), test loss: 44.0820943356\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.475994825363,9.32223944288), test loss: 3.35992273092\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (41.4798812866,54.444563427), test loss: 37.8242695332\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.01096868515,9.04143532168), test loss: 2.85441526175\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (71.5806274414,53.9951661912), test loss: 45.8131268978\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.34488344193,8.78477176655), test loss: 3.35801195502\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (56.3564682007,53.602246447), test loss: 35.2408311844\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.02290773392,8.54976497586), test loss: 2.65515628457\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (35.6751785278,53.2120076476), test loss: 41.7078546047\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.03964757919,8.33236954762), test loss: 3.22667816877\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (33.5333404541,52.8701245775), test loss: 39.1218181133\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.938920915127,8.13266011044), test loss: 2.92356367707\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (23.4307861328,52.5357521865), test loss: 42.1143904209\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.46440950036,7.94744050524), test loss: 3.06998120546\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (35.8359298706,52.2284801496), test loss: 42.9779006958\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.2570772171,7.77509887693), test loss: 3.4525370419\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (103.056747437,51.9259863702), test loss: 38.2511814117\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (7.11143732071,7.6151334952), test loss: 2.60458189249\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (116.280082703,51.626336931), test loss: 43.5249406815\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (8.00727748871,7.46492771663), test loss: 3.52116714716\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (26.4273834229,51.3539583292), test loss: 37.9561916828\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.73394489288,7.32416396574), test loss: 2.60763869882\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (40.8960418701,51.0824740045), test loss: 42.1489219666\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.20967769623,7.19164873132), test loss: 3.45849052072\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (23.5979804993,50.8371674842), test loss: 36.2310110092\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.94224381447,7.06739877323), test loss: 2.59923124611\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (52.0167350769,50.5929814042), test loss: 44.2599776745\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.97195196152,6.95047650543), test loss: 3.27704027891\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (24.2312278748,50.3605200153), test loss: 37.1729179144\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.932893812656,6.83937733199), test loss: 2.87054349184\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (36.2998352051,50.1277211017), test loss: 40.605337429\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.37429380417,6.73493585468), test loss: 3.04576215148\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (17.6823692322,49.8885938522), test loss: 38.0777892828\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.18788814545,6.63534266936), test loss: 3.08772134781\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (24.8009567261,49.6682456935), test loss: 38.2414367199\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.85206604004,6.54069277567), test loss: 3.06204227209\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (21.5841827393,49.4426421526), test loss: 39.2886688232\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.05449390411,6.45032509474), test loss: 3.13472168446\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (27.6777801514,49.2338750651), test loss: 34.8776611328\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.70826196671,6.36432777371), test loss: 2.6050912559\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (42.4242248535,49.0222306426), test loss: 42.4129788876\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.53567504883,6.28263195825), test loss: 3.3281774044\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (35.1042022705,48.8145822554), test loss: 36.2786841393\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (5.12615442276,6.20386102639), test loss: 2.47834290862\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (81.0402526855,48.605595054), test loss: 41.9860753059\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.65985393524,6.12891112753), test loss: 3.28480831385\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (144.170318604,48.3880777162), test loss: 34.9344563484\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.47442531586,6.05661254224), test loss: 2.69706105888\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (22.6385116577,48.1738259654), test loss: 41.6955843449\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.88314294815,5.98699288076), test loss: 3.23072686791\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (102.037231445,47.9607615484), test loss: 33.2225215435\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.9651017189,5.91978218357), test loss: 2.50686321855\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (104.659538269,47.7540627214), test loss: 38.9178635359\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.54278421402,5.85518034268), test loss: 3.0458476007\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (73.0307846069,47.5396651373), test loss: 29.3839537621\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.02114105225,5.79284344401), test loss: 2.41713641286\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (14.2039842606,47.3242947894), test loss: 37.5410224915\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (9.12037086487,5.73256070563), test loss: 2.98839523792\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.5648231506,47.106690487), test loss: 35.6371464252\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.38077163696,5.67398388295), test loss: 3.02371547818\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (76.9054718018,46.8804512151), test loss: 31.5834156752\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.92704200745,5.61714332275), test loss: 2.48599960208\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (33.4117889404,46.6495206773), test loss: 38.9948082924\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.38296937943,5.56185197887), test loss: 3.16261706054\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (32.0514297485,46.4178202794), test loss: 31.8009380341\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.37432169914,5.50791001938), test loss: 2.32388808131\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (27.2695503235,46.186668572), test loss: 37.4920704842\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.44342458248,5.45548505754), test loss: 3.22802811861\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (12.4879083633,45.9457857312), test loss: 28.5328588486\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.44025707245,5.40446816273), test loss: 2.24061972797\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (29.8821334839,45.7014864324), test loss: 37.9707282066\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.3682076931,5.35474655684), test loss: 3.12242251039\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (44.8187713623,45.4493095321), test loss: 28.8408047676\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.32566189766,5.30615309068), test loss: 2.52810845673\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (15.7580089569,45.1732216433), test loss: 34.4330157042\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.06422472,5.25866240017), test loss: 2.94724978507\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (14.0025892258,44.8870282162), test loss: 25.6000831604\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.7139775753,5.21210855099), test loss: 2.31684035957\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (33.8143959045,44.5980546474), test loss: 31.3558729887\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.30431056023,5.16630086185), test loss: 2.80595921278\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (16.8415851593,44.3054795892), test loss: 29.4173707962\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.69012570381,5.1214285726), test loss: 2.80985898376\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (8.99804782867,44.0063018359), test loss: 30.9423153162\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.988809645176,5.07761073821), test loss: 2.79106357992\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (26.4752006531,43.7081143739), test loss: 31.0166388988\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.5654501915,5.03489809967), test loss: 3.05401491821\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (5.73005199432,43.4061167861), test loss: 27.3485804081\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.64803361893,4.99268426773), test loss: 2.32875804901\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (39.8265914917,43.0986575802), test loss: 31.2171794653\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.77084350586,4.95129429393), test loss: 3.23864959478\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (12.5921201706,42.7911919798), test loss: 26.8904105663\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.096783638,4.91072669447), test loss: 2.28744160235\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (9.80367946625,42.4887557237), test loss: 30.5306154251\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.06212973595,4.87074174067), test loss: 3.02041348219\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (21.0870094299,42.1886687051), test loss: 26.8282221556\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.66960287094,4.83174747335), test loss: 2.42745220065\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (7.54093694687,41.8885146555), test loss: 31.9014677286\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.09400653839,4.79376702379), test loss: 3.01966295242\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.6999931335,41.5955504872), test loss: 24.8283407688\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.39805078506,4.75689611743), test loss: 2.4080772534\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.7763414383,41.3067104867), test loss: 29.8806342602\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.45467615128,4.72070369249), test loss: 2.86314315498\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (18.4807453156,41.0189290565), test loss: 28.2227718115\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.05854201317,4.68538529311), test loss: 2.83856863976\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.9085407257,40.7358682707), test loss: 29.7631017208\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.07730138302,4.65076776908), test loss: 2.88906066567\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (9.91454315186,40.4607004446), test loss: 29.644740057\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.745262146,4.61677735686), test loss: 3.12454923987\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (31.2660675049,40.1911145718), test loss: 26.1399636865\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.47110652924,4.58350222672), test loss: 2.47882409692\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (38.8090324402,39.9237024708), test loss: 30.7416231155\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.05875051022,4.55106481053), test loss: 3.21905891895\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.947602272,39.6645222209), test loss: 26.8850973845\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.66672039032,4.51970808334), test loss: 2.23905499279\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (14.4504756927,39.4114380419), test loss: 30.9595052242\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.60015535355,4.4889203791), test loss: 3.06988027692\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (39.2714080811,39.1610662768), test loss: 28.2852884293\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.02360844612,4.45885731663), test loss: 2.51056681871\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (19.3114414215,38.9148988423), test loss: 32.1057462454\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.4665915966,4.42935198745), test loss: 3.14861908853\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (11.9249420166,38.6765591376), test loss: 27.1366838932\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.41885674,4.40037270364), test loss: 2.55659583509\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (6.41580104828,38.4434294104), test loss: 30.4061179161\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.300179392099,4.3719254733), test loss: 2.9901504159\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (47.7110939026,38.2133898886), test loss: 27.2102560997\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.06021976471,4.34423506137), test loss: 2.82450449616\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (15.6760053635,37.9890378304), test loss: 30.7019313812\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.52701246738,4.31742002377), test loss: 2.97288682014\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (28.8302326202,37.7713769783), test loss: 29.381157732\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.899939060211,4.29106897691), test loss: 3.04367515296\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.98806095123,37.5549853315), test loss: 26.2813926458\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.43931245804,4.26533511307), test loss: 2.49464605153\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (6.43686008453,37.3427369606), test loss: 31.9612043381\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.948779463768,4.23998045801), test loss: 3.27531662881\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (15.3593111038,37.137613213), test loss: 28.7456344366\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.65948820114,4.21505943636), test loss: 2.39371507168\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (17.2497673035,36.9365852985), test loss: 32.9134936333\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.95711970329,4.19056196685), test loss: 3.20123943388\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (12.8742961884,36.737708127), test loss: 27.1154219866\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.12297439575,4.1667107906), test loss: 2.3308713302\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (44.3752059937,36.5442689305), test loss: 33.5098013401\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.75276994705,4.14353379138), test loss: 3.23299982548\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (21.6059970856,36.3555430338), test loss: 27.673206377\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.765895009041,4.12072674009), test loss: 2.60111520886\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (15.565946579,36.1677883231), test loss: 32.2265362501\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.903841972351,4.09844505284), test loss: 3.09149909317\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (13.9357118607,35.9836186916), test loss: 24.6026385784\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.42955350876,4.07649757122), test loss: 2.38035849333\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (10.1814479828,35.805653515), test loss: 31.5762413979\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (4.65031719208,4.05491836285), test loss: 2.95354825854\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (22.4446392059,35.630872024), test loss: 29.5655333996\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (5.76171970367,4.03360162359), test loss: 2.97073700577\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (8.26513385773,35.4570687328), test loss: 32.2770700932\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.87403273582,4.01272874356), test loss: 2.99437226653\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (11.2196960449,35.2882105155), test loss: 30.4300321579\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.13586986065,3.99249717611), test loss: 3.13841506839\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (7.54906749725,35.1230643757), test loss: 29.4119046926\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.15992218256,3.97256396125), test loss: 2.4734964028\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (8.34848213196,34.9589068345), test loss: 31.3955946922\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.59758281708,3.95312851975), test loss: 3.25389971733\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (10.2589683533,34.7970497581), test loss: 27.4520007372\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.751912891865,3.93387353081), test loss: 2.32726746798\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (14.4549007416,34.6408326068), test loss: 32.6564785242\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.69356799126,3.9150040006), test loss: 3.1420645088\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (11.3780822754,34.4872484005), test loss: 28.6893771172\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.807277679443,3.89623445494), test loss: 2.53548534513\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (41.2388763428,34.3346798192), test loss: 33.7299220324\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.02860069275,3.87786999138), test loss: 3.15902803242\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.9404668808,34.1851375056), test loss: 26.0608053684\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.882052361965,3.86003278316), test loss: 2.48210558295\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (14.0517482758,34.0397363563), test loss: 31.4067306519\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.08503079414,3.84248027424), test loss: 2.96977841854\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (22.0830535889,33.8948130473), test loss: 29.3152371168\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.282052457333,3.82529371343), test loss: 2.85040398538\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (22.4681835175,33.7518479546), test loss: 32.1851321697\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.55091428757,3.80830746058), test loss: 2.96723833084\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (12.0023212433,33.6132433208), test loss: 30.5282473564\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.64882695675,3.79159998998), test loss: 3.20606905371\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (18.4897232056,33.4772695476), test loss: 28.6202534676\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.58414196968,3.77496389924), test loss: 2.49589172006\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (12.1860294342,33.3416516651), test loss: 31.116624856\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.25998926163,3.75867333873), test loss: 3.28679534197\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (28.2358398438,33.2091120407), test loss: 29.207773304\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.27092003822,3.74282729411), test loss: 2.32558805943\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (15.9942703247,33.0794991821), test loss: 31.5339254379\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.33561897278,3.72721775515), test loss: 3.11144828796\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (16.3811645508,32.9503199102), test loss: 30.4162652016\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.692658901215,3.71189075558), test loss: 2.50909511447\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (21.4273910522,32.8230019513), test loss: 32.1484346628\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.09752869606,3.69676046473), test loss: 3.0673544094\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (41.203578949,32.6998429706), test loss: 28.133438158\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.72070932388,3.68187798173), test loss: 2.60980618894\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (10.7063865662,32.5774532684), test loss: 31.503871274\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.95433998108,3.66701391687), test loss: 2.9650339067\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (22.6240749359,32.4563007796), test loss: 30.6016748428\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (4.91426229477,3.65248771916), test loss: 3.01475848556\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (7.29629421234,32.3372289603), test loss: 31.8920802593\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.08935832977,3.63822764366), test loss: 2.98022434115\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (9.57284641266,32.2209824078), test loss: 29.4327338696\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.34505558014,3.62427593931), test loss: 3.0120318979\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (46.2799568176,32.105574693), test loss: 27.0974738836\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.49141216278,3.6105307478), test loss: 2.50727419853\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (19.1576843262,31.990620985), test loss: 31.7295710564\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.48461294174,3.5969289554), test loss: 3.2724204272\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (14.6037092209,31.8798434448), test loss: 28.6878960133\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.823803186417,3.58352849068), test loss: 2.31513336301\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (7.82492637634,31.7693903446), test loss: 33.5439599991\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.04986214638,3.5701579927), test loss: 3.14284333289\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (26.1522464752,31.6603527993), test loss: 28.8276191473\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (3.44627761841,3.55711854553), test loss: 2.42083639801\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (18.8565120697,31.5526843163), test loss: 33.9147537231\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.47443890572,3.54423091974), test loss: 3.23147469759\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (14.5188083649,31.4476765472), test loss: 27.9445087433\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.35885620117,3.53166470557), test loss: 2.56544114649\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (10.5232601166,31.3431964492), test loss: 31.9916761637\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.69945788383,3.51926080668), test loss: 3.0092787981\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (40.5899810791,31.2394363949), test loss: 24.7772428036\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (4.85386657715,3.50700473401), test loss: 2.37809447944\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (18.4933509827,31.1388070909), test loss: 31.5125725031\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.14861106873,3.49481754719), test loss: 2.8832555294\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (9.44092464447,31.0384241971), test loss: 29.2954114437\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.07926273346,3.48274223547), test loss: 2.87257107496\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (20.2586135864,30.939557696), test loss: 31.801881814\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.12248337269,3.47090192275), test loss: 2.75674176514\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (6.92244529724,30.8415265123), test loss: 32.045510006\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.36443829536,3.45920999812), test loss: 3.14614000171\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (7.90507125854,30.7460989593), test loss: 29.7957870483\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.810630857944,3.44780385705), test loss: 2.42423885763\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (46.9333000183,30.6515226673), test loss: 31.9954436779\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.13955354691,3.43655794616), test loss: 3.18658145666\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (17.0538673401,30.5565477412), test loss: 27.6103929996\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.48860645294,3.42540798574), test loss: 2.20669923723\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (2.91469573975,30.464637624), test loss: 33.1853778601\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.32624697685,3.41432794635), test loss: 3.11047986448\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (26.2582855225,30.3738303142), test loss: 28.6162176132\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.76459884644,3.40335787776), test loss: 2.5044827044\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (49.7927856445,30.2835104088), test loss: 33.7508271217\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.48154997826,3.39255659538), test loss: 3.2001131326\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (16.5378723145,30.1933733074), test loss: 25.967545414\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.79757773876,3.38188186792), test loss: 2.47778838873\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (13.8500537872,30.1060849578), test loss: 31.4871789217\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.85264647007,3.37146572915), test loss: 2.87400131524\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (50.9260063171,30.020117114), test loss: 29.1368699551\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (3.80314254761,3.3612409695), test loss: 2.73043095171\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (34.1053085327,29.9329433303), test loss: 32.5156609535\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (3.99911594391,3.35105894154), test loss: 2.93026454449\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (9.59155654907,29.8482497318), test loss: 30.9331739902\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (4.3299741745,3.34092991381), test loss: 3.16587971747\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (6.76529502869,29.765009301), test loss: 29.3175211906\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.47228741646,3.33087836497), test loss: 2.4076738745\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (13.7577056885,29.6821558298), test loss: 31.0954219818\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.37710142136,3.32096330593), test loss: 3.27175360322\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (17.3051662445,29.599346505), test loss: 29.9209858894\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.40533733368,3.31119319635), test loss: 2.31394097507\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (15.7898330688,29.5191585722), test loss: 31.6628539085\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.15258097649,3.3016253706), test loss: 3.06454981565\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (19.2241287231,29.4400889769), test loss: 31.3147819996\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.387961655855,3.29224152971), test loss: 2.54813304394\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (8.41994380951,29.3596591121), test loss: 32.3129633427\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.726959347725,3.28289969725), test loss: 3.04990181327\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (8.12496376038,29.2816463426), test loss: 28.0406963348\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.72022294998,3.27360892019), test loss: 2.5692779243\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (18.6411380768,29.2050705555), test loss: 31.935888505\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.43912267685,3.26438247349), test loss: 2.87929111123\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (37.398979187,29.1288791237), test loss: 29.4618117332\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (3.0961587429,3.25526615775), test loss: 2.84155157804\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (19.272228241,29.0522713718), test loss: 32.1742552042\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.84218454361,3.24627131669), test loss: 2.92692902088\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (9.5040473938,28.9779800227), test loss: 29.4674717188\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.92428398132,3.23747731314), test loss: 2.95145447254\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (8.14745044708,28.9048790663), test loss: 28.1193251371\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.643754005432,3.22879246668), test loss: 2.54537796378\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (21.0862617493,28.8310214954), test loss: 31.5313676834\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.33599579334,3.22020013933), test loss: 3.23146274686\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (19.1945152283,28.7584654858), test loss: 28.9674199104\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.02748370171,3.21164012359), test loss: 2.20591862202\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (13.574432373,28.687611678), test loss: 32.9031986713\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.67829155922,3.20311809385), test loss: 3.06972266436\n",
      "run time for single CV loop: 7159.16797113\n",
      "\n",
      "MC # 5, Hype # hyp5, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold1/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (325.185119629,inf), test loss: 224.620440674\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (270.925567627,inf), test loss: 304.230241394\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (69.7206878662,79.9683240538), test loss: 45.4334626675\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.85022830963,28.2225339878), test loss: 3.6204238385\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (38.0961837769,62.7130521321), test loss: 39.4195718288\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.80870890617,15.7878209092), test loss: 3.75431527495\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (30.927406311,56.9303063879), test loss: 44.3515403748\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (7.61199378967,11.6404864572), test loss: 3.45723187923\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (49.2065887451,53.8889541111), test loss: 42.3069678307\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.84837532043,9.54855481923), test loss: 3.68771294355\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (26.9653396606,52.0676330132), test loss: 42.1938512325\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.65329647064,8.29156814231), test loss: 2.90020218194\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (37.6606826782,50.7001248663), test loss: 44.6852181435\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.03620505333,7.45057987445), test loss: 3.92971234918\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (47.3380966187,49.6601396052), test loss: 39.3056464195\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.3039855957,6.84930901617), test loss: 2.68624817431\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (35.8544006348,48.8294053173), test loss: 43.6436040401\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.96170210838,6.39116414805), test loss: 3.57655596733\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (31.4391555786,48.0937365568), test loss: 36.5854392052\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.92916154861,6.03389699712), test loss: 2.91549066901\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (63.6406593323,47.4372394874), test loss: 44.0433555603\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.70894169807,5.74474604374), test loss: 3.43179720044\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (15.5617008209,46.8923515681), test loss: 32.8158535242\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.62452793121,5.50902787426), test loss: 2.62292611301\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (19.9186096191,46.3647604902), test loss: 41.8254341602\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (7.46718454361,5.30971421464), test loss: 3.68021035194\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (70.9442596436,45.9004281266), test loss: 36.0850192547\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.85240626335,5.14067830125), test loss: 3.47808677256\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (13.1224575043,45.4074771613), test loss: 36.5010639191\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.39850330353,4.99617465009), test loss: 2.88805927336\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (42.736076355,44.9273419696), test loss: 40.9503754139\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.24558472633,4.87420199961), test loss: 3.6153251946\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (39.0298156738,44.4576557166), test loss: 37.3110202789\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.19594192505,4.76536316607), test loss: 2.86063759327\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (22.7252578735,43.9739888308), test loss: 40.9315629959\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.98855483532,4.66861918855), test loss: 3.64421101213\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (26.5575714111,43.4844509375), test loss: 33.9141934633\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.09140253067,4.58079054506), test loss: 2.91016723812\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (54.9746894836,43.0276580023), test loss: 40.4445900917\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.95345497131,4.50126339866), test loss: 3.50256472826\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (33.0554122925,42.557688094), test loss: 29.8300144911\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.234126091,4.42653814957), test loss: 2.63877627552\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (21.3317184448,42.0984944287), test loss: 37.7557510376\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.85232043266,4.35840716791), test loss: 3.56763908863\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (59.0395088196,41.6274570406), test loss: 32.201388526\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.58821928501,4.29502080951), test loss: 3.00820146203\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (39.5696182251,41.1507216938), test loss: 34.6052420139\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.26646590233,4.23556523131), test loss: 3.33286555111\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (20.0330944061,40.6808923524), test loss: 33.0446734905\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.87890768051,4.17875456872), test loss: 3.47283360362\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (38.0312194824,40.2040579446), test loss: 30.8967932224\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.0177462101,4.12474680953), test loss: 2.70007720888\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (33.5367279053,39.7218556289), test loss: 36.4783494473\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.67360901833,4.07340977436), test loss: 3.65883181393\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (79.1924209595,39.2621621854), test loss: 31.3303355694\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.97434711456,4.02444287841), test loss: 2.46457152367\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (18.8225879669,38.8063466401), test loss: 35.0246626377\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.21456623077,3.9768490189), test loss: 3.69368925095\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (22.4394950867,38.3629781211), test loss: 29.8033260345\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.2936745882,3.93220165004), test loss: 2.59120317101\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (28.3301525116,37.9226834706), test loss: 36.0266791821\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (5.2375087738,3.88958469953), test loss: 3.42019066215\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (11.1091718674,37.493044236), test loss: 26.4983003616\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (5.45304298401,3.8487537443), test loss: 2.49170449376\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (19.0597190857,37.0832799897), test loss: 33.9482183456\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.3209271431,3.80966753032), test loss: 3.34639417231\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (34.5631446838,36.6735499111), test loss: 29.9319871902\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (5.98780250549,3.77194672679), test loss: 3.37293028533\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (5.62340688705,36.2749667818), test loss: 34.0496227264\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.31368517876,3.73576563173), test loss: 3.26835769117\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (22.7146606445,35.8981940618), test loss: 31.7545004368\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (5.97765254974,3.70081290261), test loss: 3.33568491936\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (13.9815788269,35.5346720101), test loss: 30.4291770935\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.09047269821,3.66664746019), test loss: 2.51651751101\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (31.3356113434,35.1824835112), test loss: 34.1745866299\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.11618089676,3.63464411237), test loss: 3.70884442329\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (35.7878417969,34.8370881988), test loss: 27.55632658\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.30012559891,3.6035157022), test loss: 2.34764082134\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (11.314827919,34.5052909127), test loss: 35.667763567\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.68908166885,3.57384692009), test loss: 3.34800887704\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (29.0473861694,34.1883582191), test loss: 28.2779434681\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.06936073303,3.54544236186), test loss: 2.53977888823\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (25.4020614624,33.8720640502), test loss: 34.7148091316\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.2473025322,3.5174383963), test loss: 3.22526681125\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (10.0555648804,33.5680719373), test loss: 24.1590269566\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.3565955162,3.49079566685), test loss: 2.49618044645\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (8.67891693115,33.276296971), test loss: 33.2178573847\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.7970559597,3.46432705909), test loss: 3.26719470918\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (24.5498542786,32.9960463157), test loss: 30.415109539\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.26977348328,3.43895453691), test loss: 3.44326528609\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (31.2254142761,32.7218854736), test loss: 28.2754232883\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.37164092064,3.41443204748), test loss: 2.71802276373\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (5.65954303741,32.4533457066), test loss: 34.1143240452\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.18806815147,3.39101859619), test loss: 3.46884970069\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (17.2052001953,32.1958176234), test loss: 29.8033380985\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.14897632599,3.36837922482), test loss: 2.3872084409\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (47.0017814636,31.9454827179), test loss: 35.0030665874\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (4.23049640656,3.34643818464), test loss: 3.31226942837\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (63.9859848022,31.6960710015), test loss: 31.2097800732\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.32414031029,3.32486188419), test loss: 2.52295920402\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (4.65027761459,31.4561601216), test loss: 34.7887037039\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.29041683674,3.30416069438), test loss: 3.29544489384\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (12.9214639664,31.2239505446), test loss: 28.4288710117\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.26058936119,3.28337130815), test loss: 2.57660194486\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (8.14063072205,30.9983834244), test loss: 33.2075000286\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.673417568207,3.26320756066), test loss: 3.27168109417\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (7.47006559372,30.7756790902), test loss: 30.7116158009\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.00905776024,3.24388347392), test loss: 3.33511302173\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (13.4264698029,30.5588822475), test loss: 33.7362632036\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.33531713486,3.22532899072), test loss: 3.24572944045\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (8.72920036316,30.3485483651), test loss: 30.9790376902\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.10958361626,3.20707023905), test loss: 3.29226546884\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (6.97849988937,30.1406383965), test loss: 30.3346951962\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.87636089325,3.18935886933), test loss: 2.4563670367\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (20.207036972,29.9366611368), test loss: 32.0686273575\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (4.01508045197,3.17200756612), test loss: 3.50646383762\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.9313297272,29.7389125441), test loss: 30.5077222347\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.29467535019,3.15503166878), test loss: 2.41888955235\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (19.1211585999,29.5455499427), test loss: 33.5184328794\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.97973704338,3.13801439173), test loss: 3.17265562117\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.4262838364,29.3564553032), test loss: 29.1820305824\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.9605474472,3.12156570585), test loss: 2.52889243513\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (47.5833473206,29.1681464082), test loss: 36.3776175737\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.19352912903,3.10539073571), test loss: 3.27633625865\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (24.0171051025,28.9853425127), test loss: 25.3252497196\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.23512232304,3.09000544894), test loss: 2.44095285982\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.5672712326,28.806263597), test loss: 34.2994753122\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.82446205616,3.07477834367), test loss: 3.22763326913\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (9.99069023132,28.628123148), test loss: 29.1038535118\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.503007292747,3.05987629801), test loss: 3.18197469115\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (7.45662021637,28.4543583145), test loss: 34.2619053364\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.704614043236,3.04532893098), test loss: 2.88265507221\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (10.4232645035,28.2846143032), test loss: 32.0878747463\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.64705085754,3.03092084488), test loss: 3.32719398737\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (17.7218322754,28.1178689613), test loss: 31.6054356575\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.71205925941,3.01664694025), test loss: 2.3983663246\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (11.9494075775,27.9543831522), test loss: 34.7988868713\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.868418097496,3.00262074799), test loss: 3.30673598051\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (10.6529960632,27.7888777993), test loss: 29.7808028698\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.56456708908,2.98880643123), test loss: 2.34440125078\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (17.4308567047,27.6305136191), test loss: 36.0723206997\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.85757184029,2.97567116359), test loss: 3.30815701932\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (5.04606342316,27.4746419824), test loss: 27.9016886711\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.567581057549,2.96259349026), test loss: 2.54122581482\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (7.29000282288,27.3181801771), test loss: 34.3700458527\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.40259504318,2.94982243371), test loss: 3.08161816001\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (15.72590065,27.1656786036), test loss: 30.5491490364\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.69829797745,2.93716627235), test loss: 2.93973555565\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (10.9411487579,27.016600569), test loss: 34.4260500431\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.17609739304,2.92464490531), test loss: 3.15189167112\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (16.3985862732,26.8693149053), test loss: 29.8676339865\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.01898670197,2.91228073316), test loss: 3.2117079556\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (23.9100284576,26.7237388219), test loss: 30.7127673149\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.17249727249,2.90009832142), test loss: 2.56724346578\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (6.12706708908,26.5771332418), test loss: 33.3601438046\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.763105869293,2.88804936211), test loss: 3.32990209162\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (9.9717464447,26.4358246114), test loss: 31.570242691\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.92077493668,2.87656304905), test loss: 2.34180245101\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (23.780248642,26.2965751229), test loss: 34.0176838875\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.919519305229,2.86510077477), test loss: 3.14308839142\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (5.27025938034,26.1567497037), test loss: 31.8829835415\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.14503836632,2.85385865521), test loss: 2.4027276352\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (19.7006397247,26.0203531218), test loss: 35.6482447624\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.32120347023,2.84269442625), test loss: 3.10283010602\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.7437229156,25.886116271), test loss: 27.1061881542\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.61580747366,2.83164339808), test loss: 2.41582499146\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (11.0611038208,25.7534307205), test loss: 34.6840759754\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.00770711899,2.8206769765), test loss: 3.11753706485\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (14.9177398682,25.6222375215), test loss: 28.5381914139\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.49986457825,2.80992404533), test loss: 3.02826269269\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.9102840424,25.4903440397), test loss: 36.5451941013\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.53416442871,2.79929537697), test loss: 3.06231378168\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (20.9438285828,25.3620288798), test loss: 32.5586497068\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.10274159908,2.78899378401), test loss: 3.12776450366\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (5.01492977142,25.2358738956), test loss: 33.833907938\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.87435674667,2.77881208206), test loss: 2.37990745455\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (12.1092128754,25.109483612), test loss: 32.7100093365\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.910732984543,2.76878554053), test loss: 3.23766757846\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (13.3991298676,24.9853513337), test loss: 31.8750965118\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.818866550922,2.75878572415), test loss: 2.22378047407\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (8.63627052307,24.8630749541), test loss: 36.0870473862\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.36995232105,2.7489217829), test loss: 3.15982595682\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.2797718048,24.7424821338), test loss: 28.4070535183\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.94139862061,2.73908862701), test loss: 2.39391320646\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (11.1592826843,24.6222108543), test loss: 37.9812967777\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.20442986488,2.72939877638), test loss: 3.17881640196\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (11.0070438385,24.5025164183), test loss: 25.3782129765\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.74548077583,2.71985683376), test loss: 2.21393181682\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (13.5721826553,24.3850722311), test loss: 35.6503702164\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.52441477776,2.71049530143), test loss: 3.06431905925\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (13.3168544769,24.2699279844), test loss: 29.9229485512\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.26067554951,2.70137436838), test loss: 2.99165501744\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (10.0438556671,24.1539260792), test loss: 32.2586345196\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.5834363699,2.69231126812), test loss: 2.59950591028\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (14.4663333893,24.0402130845), test loss: 34.1552181721\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.25264513493,2.68328125587), test loss: 3.21127232015\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (22.2616043091,23.9284552214), test loss: 33.196765995\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (4.61152696609,2.6742829284), test loss: 2.21586023867\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (2.65092468262,23.8172657303), test loss: 36.2968835354\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.06777644157,2.66535842983), test loss: 3.09652664065\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (16.623714447,23.7064050977), test loss: 32.9573322773\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (5.09574317932,2.65655868361), test loss: 2.4720630914\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.60065078735,23.5963322189), test loss: 37.9996952534\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.760666549206,2.64790740351), test loss: 3.15877422392\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (3.49299764633,23.4878992278), test loss: 29.9168492794\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.657313466072,2.63936023967), test loss: 2.46196492016\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (8.24670505524,23.3814848866), test loss: 37.218269825\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.465641379356,2.63104819059), test loss: 3.0064255774\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (9.94320106506,23.2747286822), test loss: 31.5304877758\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.87063407898,2.6227775493), test loss: 2.93880695403\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (14.9742927551,23.1694639625), test loss: 36.8347136497\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.23313379288,2.61454934849), test loss: 3.01461081952\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (5.4456038475,23.0660682447), test loss: 33.2057142258\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.638126850128,2.60625894399), test loss: 3.04932434559\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (12.5732498169,22.9630341761), test loss: 34.0803001881\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (4.38887405396,2.59811048698), test loss: 2.51047437191\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (31.2651901245,22.8604229485), test loss: 33.0994022369\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.58595967293,2.58998746992), test loss: 3.21789010763\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.1288776398,22.758210404), test loss: 36.9502884865\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.823945760727,2.58209860672), test loss: 2.39640304595\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.0957736969,22.6573735007), test loss: 36.0546610832\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.05174160004,2.57425816976), test loss: 3.04854230881\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (30.8101081848,22.5580829461), test loss: 33.3094489098\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.47557020187,2.56660401384), test loss: 2.59804521799\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (6.75956249237,22.4589374391), test loss: 38.3895316124\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.678363919258,2.5589204954), test loss: 3.02039176822\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (7.28444576263,22.3610313364), test loss: 26.9811137199\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.55485582352,2.55138211514), test loss: 2.32359643281\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (3.44099354744,22.2647839928), test loss: 37.5437541246\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.645025074482,2.54371297899), test loss: 3.05493586659\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (3.88704228401,22.16829766), test loss: 29.75796175\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.416165828705,2.53613924051), test loss: 3.01301817596\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.94214963913,22.072430016), test loss: 40.0481463432\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.06533026695,2.52869378983), test loss: 3.09857812226\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (15.4007015228,21.9770507887), test loss: 33.080040884\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.05612301826,2.5213385757), test loss: 3.11520307064\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (9.92113876343,21.8829769681), test loss: 35.2364987373\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.891355037689,2.51408070564), test loss: 2.38978656083\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (17.7901477814,21.789841333), test loss: 36.0022331715\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.0724465847,2.50699408449), test loss: 3.18555360138\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (7.43498754501,21.6972081867), test loss: 32.1274821758\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.03564620018,2.4998731001), test loss: 2.27355744541\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.4744243622,21.6057555354), test loss: 38.5619950771\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (5.28643751144,2.49285752728), test loss: 3.21034965217\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (11.3661832809,21.5153001658), test loss: 30.0022809982\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.24968290329,2.48573765265), test loss: 2.46684753299\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (6.24724149704,21.4245444679), test loss: 37.3073607922\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.07495236397,2.47870270029), test loss: 2.94456289262\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (22.4366798401,21.3343278469), test loss: 30.735750246\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.99291455746,2.47175691409), test loss: 2.76497350633\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (17.7956523895,21.2448443047), test loss: 38.5072573185\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.20792162418,2.46489889806), test loss: 3.08193501979\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (6.95824193954,21.1564017218), test loss: 31.8551981926\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.93970066309,2.45814699713), test loss: 2.93092365861\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (10.8376674652,21.0686117257), test loss: 34.6104993343\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.442900657654,2.45148920483), test loss: 2.59326301515\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (10.8968982697,20.98161269), test loss: 35.7876039505\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.56091976166,2.44484425528), test loss: 3.22456946969\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (4.77498149872,20.8955167446), test loss: 35.4156434536\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.70134353638,2.43823206975), test loss: 2.29488055706\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (3.88666152954,20.8099649869), test loss: 37.7288692474\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.08268499374,2.43161951391), test loss: 3.05057543069\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (7.16982746124,20.7247437983), test loss: 35.7209309101\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.30571126938,2.42506408016), test loss: 2.41794795692\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (8.59688949585,20.6392792181), test loss: 38.8199988842\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.217552244663,2.41849244361), test loss: 3.0356035322\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (4.83738708496,20.5547827323), test loss: 29.4499353409\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.15144753456,2.41209725684), test loss: 2.47519048452\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (4.71222305298,20.4716237309), test loss: 38.1914291859\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.447369217873,2.40578620613), test loss: 2.98518029153\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (18.407207489,20.3885070968), test loss: 31.0554032803\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.1125934124,2.39950257542), test loss: 2.9239391923\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (4.57639789581,20.3063878048), test loss: 39.1954223156\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.06053805351,2.39324281269), test loss: 3.01829419136\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.19859981537,20.2250180015), test loss: 33.1058522224\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.62263250351,2.3869960346), test loss: 3.0601890862\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (21.3688373566,20.1438048945), test loss: 36.6156341553\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.99588155746,2.38080486809), test loss: 2.41649873853\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (13.1704311371,20.0633003299), test loss: 34.4435267448\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.09787654877,2.3746422772), test loss: 3.16688703597\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.34144306183,19.9822587384), test loss: 34.7661235809\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.444714635611,2.36843941015), test loss: 2.28741966784\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (16.3363075256,19.9023394639), test loss: 39.4922287941\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.13832116127,2.36242273435), test loss: 3.17681477666\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (4.55717658997,19.8234530127), test loss: 30.8793474674\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.60735976696,2.35645907573), test loss: 2.44195092022\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (8.12190055847,19.7443600266), test loss: 39.4035889626\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.75968444347,2.35051675098), test loss: 2.98535492718\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (15.0291776657,19.6667993744), test loss: 28.1108285904\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.64088439941,2.34461472199), test loss: 2.25170996189\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (11.4422645569,19.5894046416), test loss: 39.1660489559\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.488487005234,2.33868563525), test loss: 2.9920240283\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (12.6319532394,19.5122504225), test loss: 31.140784049\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.856906414032,2.3328071468), test loss: 2.96587125659\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (19.3557434082,19.4359143293), test loss: 36.2812298298\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.27346050739,2.32697636899), test loss: 2.6335735023\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (14.4086294174,19.3589736569), test loss: 38.4789615154\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.38408493996,2.321121163), test loss: 3.1664178282\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (5.94357299805,19.283151534), test loss: 37.2583220959\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (3.05665493011,2.31543120214), test loss: 2.40882408619\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (4.37817907333,19.2083428333), test loss: 39.7599340439\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.05474817753,2.30973760705), test loss: 3.13609319329\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.56862711906,19.1331206516), test loss: 35.4999741554\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.290059328079,2.30412557069), test loss: 2.42222357243\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (10.1151208878,19.0595053032), test loss: 41.7255555153\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.281043767929,2.29852485616), test loss: 3.20945190787\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (2.53523302078,18.9858610481), test loss: 30.4524573326\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.82564997673,2.29288753197), test loss: 2.49646643251\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (6.36940670013,18.9126517529), test loss: 39.5724018574\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.12030112743,2.28729032475), test loss: 2.98241728842\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (8.39202404022,18.8398839439), test loss: 34.740344286\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.10974097252,2.28172851794), test loss: 3.00212881863\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.2714138031,18.7668937214), test loss: 41.1542861938\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.892081618309,2.27618503806), test loss: 3.10557161272\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (7.49827384949,18.6948533925), test loss: 33.0459839582\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.707038164139,2.27073877854), test loss: 3.10645450354\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (12.7430858612,18.6236902472), test loss: 36.8810938597\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.75901854038,2.26534350764), test loss: 2.66199535131\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (9.33897972107,18.5523201461), test loss: 35.9350016594\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.23148596287,2.26000449231), test loss: 3.17094003558\n",
      "\n",
      "MC # 5, Hype # hyp5, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold2/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (254.606445312,inf), test loss: 222.582277679\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (375.161407471,inf), test loss: 395.043247986\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (64.2439117432,96.4775659313), test loss: 44.0506927967\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.10102033615,90.6798564035), test loss: 3.24863110185\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (131.238998413,71.7727234685), test loss: 41.888102293\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (8.3400812149,46.9357148815), test loss: 3.34277612567\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (41.001411438,63.4564185192), test loss: 43.4459039927\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.73092961311,32.3526462969), test loss: 3.19856700599\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (65.3542785645,59.1893629448), test loss: 45.307510519\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (6.47440814972,25.0597179161), test loss: 3.31624466181\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (41.3263473511,56.6455121222), test loss: 41.6809202671\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.66536569595,20.6894228068), test loss: 2.8826957494\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (86.6824035645,54.917210616), test loss: 46.0881812096\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (6.78292608261,17.7810561126), test loss: 3.5685680151\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (25.8331298828,53.6160024021), test loss: 40.2510107994\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.77044826746,15.7048196866), test loss: 3.00846143365\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (49.3029022217,52.6203048728), test loss: 45.4759957314\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.04474687576,14.1468952299), test loss: 3.33572438359\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (69.3806762695,51.8187553284), test loss: 38.6733453274\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.1698923111,12.9371288924), test loss: 3.17473633885\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (35.8990249634,51.123767341), test loss: 43.3501345634\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.85503816605,11.9671459437), test loss: 3.06496843398\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (38.0091438293,50.4969281117), test loss: 35.3957575798\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.49292135239,11.173761284), test loss: 3.03462780416\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (46.8291358948,49.9625152963), test loss: 41.7237823486\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.82843494415,10.5103984065), test loss: 3.26152893305\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (13.431886673,49.517385995), test loss: 42.1293412209\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.37413811684,9.95105103928), test loss: 3.34062785804\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (24.1553211212,49.1186325087), test loss: 38.4642913342\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.38187146187,9.47305438045), test loss: 3.00804187953\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (79.9177093506,48.7508903257), test loss: 46.4231658936\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.12955665588,9.05882200053), test loss: 3.58218141794\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (50.9346466064,48.3846746586), test loss: 39.3559227943\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (5.14147996902,8.69613263537), test loss: 2.91110651493\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (22.5298900604,48.0475725774), test loss: 43.0630009174\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.00935959816,8.37710068465), test loss: 3.50801126659\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (38.847568512,47.7207649578), test loss: 35.8471571922\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.86815547943,8.09288504579), test loss: 3.12169331908\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (35.7811965942,47.3765224077), test loss: 43.7693132401\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.9238960743,7.83767199671), test loss: 3.34304206371\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (64.8050079346,47.068447736), test loss: 34.0384412766\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.37176632881,7.60770084803), test loss: 2.95537490845\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (40.246837616,46.7421170282), test loss: 39.3265972614\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.33950901031,7.39910730064), test loss: 3.47254007459\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (21.2210426331,46.4478675826), test loss: 38.3438298702\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.83526611328,7.21078784777), test loss: 3.28633964062\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (39.125919342,46.1380401533), test loss: 40.5523647785\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.928093373775,7.03841661539), test loss: 3.13913147151\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (96.3730316162,45.8270077996), test loss: 41.163947916\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.60142421722,6.8811496874), test loss: 3.30087376237\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (16.4303340912,45.5024578012), test loss: 37.3054587841\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.20871710777,6.7351249296), test loss: 3.03438433707\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (25.4873771667,45.1751792486), test loss: 41.9329571247\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.67502784729,6.600334685), test loss: 3.49602452219\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (30.3382911682,44.8279781073), test loss: 31.7149769068\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.59329295158,6.47452907017), test loss: 3.07785004973\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (37.1101112366,44.489611042), test loss: 40.2350313187\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (8.45537281036,6.35727492493), test loss: 3.2732006669\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (28.7458114624,44.1334192522), test loss: 30.953122282\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.87170958519,6.24636299916), test loss: 2.9223739326\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.7566452026,43.7787779954), test loss: 37.4069520235\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.99306249619,6.14260008413), test loss: 3.22336504459\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (70.4294586182,43.4162325597), test loss: 32.561045599\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (6.7440571785,6.04522161913), test loss: 2.94572392106\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (18.8177757263,43.0434129054), test loss: 34.6617460489\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.79209303856,5.95312168139), test loss: 3.0366176784\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (26.9120388031,42.6614184365), test loss: 37.3082650661\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (0.832729160786,5.86517372675), test loss: 3.29460546374\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (34.9734802246,42.2777259232), test loss: 31.0811822414\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.59587442875,5.78168756248), test loss: 2.71545403898\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (41.0326881409,41.8783320379), test loss: 36.3300643444\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.5723221302,5.70155743165), test loss: 3.49859354496\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (11.9006757736,41.4702885641), test loss: 29.7207941532\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.46508646011,5.62518250435), test loss: 2.82462035418\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (25.8140926361,41.0692980827), test loss: 37.1533231735\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.87171268463,5.55089850543), test loss: 3.18481972218\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.0502376556,40.6731961407), test loss: 27.3385507107\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.43957471848,5.47976395819), test loss: 2.79821673334\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (22.9545307159,40.2829961528), test loss: 34.4260621548\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.713988542557,5.41151921265), test loss: 3.06856664419\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (31.0832176208,39.895711502), test loss: 25.1243254185\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.54642772675,5.34517564682), test loss: 2.70700327158\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (39.4400405884,39.5116734036), test loss: 34.9570550442\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.07569670677,5.2805919125), test loss: 3.06173975468\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (12.0598258972,39.1400753679), test loss: 30.9421600342\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.5350883007,5.21777785882), test loss: 3.26750211418\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (30.408405304,38.7698597174), test loss: 29.3155966282\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.84279608727,5.15534693574), test loss: 2.89461371005\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (20.0876350403,38.406006143), test loss: 35.7147155285\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.37432980537,5.09425883956), test loss: 3.46375314593\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (15.9703302383,38.0586185107), test loss: 29.5996047497\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.39170205593,5.03522785473), test loss: 2.62438906431\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (18.9311466217,37.7170811691), test loss: 34.4751006603\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.483568191528,4.97813055005), test loss: 3.26099746823\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (17.4841842651,37.386546013), test loss: 28.1351985455\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.999901711941,4.9236133233), test loss: 2.75935429484\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (14.5875797272,37.0625872048), test loss: 35.4719956398\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.93895030022,4.87091946655), test loss: 3.24229871333\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (56.5229492188,36.7498648064), test loss: 26.0300163746\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.4089050293,4.82073716093), test loss: 2.73813066781\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (4.23321437836,36.4452875858), test loss: 35.3853873968\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.568732023239,4.77186780852), test loss: 3.30620415211\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (21.3612480164,36.1452979467), test loss: 30.6224545002\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.87752223015,4.72482231013), test loss: 3.10917845666\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (12.2458839417,35.8525900962), test loss: 33.5483791828\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.629439234734,4.67906968974), test loss: 3.17791102231\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (13.1930837631,35.5712404677), test loss: 31.2978931904\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (5.45637559891,4.63490588363), test loss: 3.10362987816\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (26.9760398865,35.2981133299), test loss: 29.9639341831\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.23369503021,4.59176752374), test loss: 2.67847862244\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (12.1329345703,35.0292410826), test loss: 34.7092357874\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.39464592934,4.55005249376), test loss: 3.14869055152\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (25.149312973,34.7686405589), test loss: 27.8872159481\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.63184356689,4.50986761626), test loss: 2.70884817541\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (3.10286211967,34.5136278432), test loss: 35.5264145613\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.37520313263,4.47116300982), test loss: 3.02197203338\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.2911434174,34.2664398902), test loss: 28.0443684101\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.739157795906,4.4336077303), test loss: 2.78474586457\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (22.418170929,34.0250614322), test loss: 33.6615865707\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.55993044376,4.39716964242), test loss: 3.07049514055\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.2778129578,33.7857465232), test loss: 30.4074119568\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.51256656647,4.36154982067), test loss: 2.81513498724\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (14.1683979034,33.5521692721), test loss: 32.3992599964\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.15499866009,4.32684213925), test loss: 3.00008282214\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (7.82563257217,33.3267915737), test loss: 31.6361419201\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.870445072651,4.29279943972), test loss: 3.11234604865\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (18.9907016754,33.1048977643), test loss: 32.9735097885\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.45612239838,4.25988369197), test loss: 2.66893635988\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (21.2833709717,32.8897748104), test loss: 33.1081681013\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.869881808758,4.22805648057), test loss: 3.13861430883\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (16.7612762451,32.6760618646), test loss: 30.8472532749\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.31788444519,4.19682669571), test loss: 2.57698306143\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (28.3910369873,32.468812223), test loss: 37.7077780962\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.905806541443,4.16669084673), test loss: 3.08582528234\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (6.94555091858,32.2663087123), test loss: 26.778955555\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.77308666706,4.13747404494), test loss: 2.73998071849\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (35.0001754761,32.0652690887), test loss: 33.2765177727\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.81116175652,4.1087165813), test loss: 2.93614703417\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (20.7510910034,31.8669462355), test loss: 25.7913969517\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.75116837025,4.08051230377), test loss: 2.70715581477\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (14.6889019012,31.6768760895), test loss: 33.1716689348\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.21980297565,4.05295654183), test loss: 3.08093658686\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (46.1174163818,31.4873064286), test loss: 29.1678669453\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.25223064423,4.02593271279), test loss: 2.87948229909\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (19.0717067719,31.3026235393), test loss: 31.0308143616\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.32119417191,3.99970091385), test loss: 2.90312463343\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (7.88516330719,31.1187668114), test loss: 33.8065019846\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.16056203842,3.97396364322), test loss: 3.2069180727\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (14.6593799591,30.9404764239), test loss: 30.7464252472\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.61379480362,3.94918098828), test loss: 2.53646956235\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (9.72712230682,30.7659308598), test loss: 34.9946023464\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.609880030155,3.92484513705), test loss: 3.12989852428\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (17.7767295837,30.592141029), test loss: 29.9079293251\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.83035326004,3.9010178934), test loss: 2.69791561216\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (6.47009658813,30.4206801301), test loss: 35.0808284044\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.59417784214,3.87759400967), test loss: 3.15172051042\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (8.58289718628,30.2543523755), test loss: 26.4840679646\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.0084733963,3.85453034603), test loss: 2.68988684714\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (28.5236549377,30.0901588683), test loss: 32.9126612663\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.64092814922,3.8319074432), test loss: 3.04428814054\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (5.05593347549,29.9277678524), test loss: 29.4067070961\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.340167611837,3.80974105377), test loss: 2.90472183824\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (11.6036205292,29.7671791252), test loss: 33.6439326763\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.14021778107,3.78810000423), test loss: 3.03273391873\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (5.2685046196,29.610037007), test loss: 31.9854445457\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.06400036812,3.76704824747), test loss: 2.97628240585\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.92652225494,29.455302572), test loss: 32.2811543465\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.05524897575,3.74648176664), test loss: 2.63148551285\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.843164444,29.3037740799), test loss: 34.4861598969\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.64863801003,3.72630716619), test loss: 2.99868144989\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.341673851,29.1512664929), test loss: 31.1392477512\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.02390360832,3.70634519854), test loss: 2.63494508713\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (17.4812374115,29.0029346173), test loss: 34.3156151772\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.43756103516,3.68676380342), test loss: 2.91095647216\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (8.01554584503,28.8566231201), test loss: 27.301031208\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.26783287525,3.6673040453), test loss: 2.67907829434\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.0082950592,28.7124622757), test loss: 36.7087986469\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.13397216797,3.64837494564), test loss: 2.9284304142\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (9.48569297791,28.5696812803), test loss: 27.5720876217\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.53488755226,3.62987258868), test loss: 2.64831530452\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (23.6069602966,28.4286324078), test loss: 34.8119147301\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.8690097332,3.6115222178), test loss: 2.87990584821\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (10.1729974747,28.2893164962), test loss: 30.5538402557\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.327614486217,3.59372306461), test loss: 2.92691977918\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (4.6714887619,28.1538093871), test loss: 31.9601203918\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.814891219139,3.57630985713), test loss: 2.70673120022\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (13.8085861206,28.0166429835), test loss: 31.5824491024\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.24169898033,3.55905421416), test loss: 3.05436713994\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (15.7554864883,27.8833436194), test loss: 30.366008091\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.26691174507,3.54198572515), test loss: 2.4928815484\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (39.6306762695,27.7520547531), test loss: 35.4748462677\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.54472446442,3.52515639219), test loss: 2.96260170341\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (33.9560432434,27.6215275373), test loss: 26.9395008564\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (3.86648535728,3.50857192185), test loss: 2.68165409267\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (13.1237983704,27.4919874829), test loss: 34.8223450661\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.11771678925,3.49229244922), test loss: 2.76640190482\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (15.1154747009,27.3638001034), test loss: 26.2908618927\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.46243047714,3.47620052974), test loss: 2.63941147327\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (6.33602905273,27.2374998611), test loss: 34.0144582748\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.49819004536,3.46062980782), test loss: 2.99168012142\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (10.2979316711,27.114228935), test loss: 29.5508430004\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.792258143425,3.44524976116), test loss: 2.81658267379\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (12.3657150269,26.989946888), test loss: 32.0641177654\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.392576307058,3.43009494615), test loss: 2.89299228489\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (21.6445217133,26.8682270759), test loss: 32.2324645042\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (3.52171134949,3.41511074343), test loss: 3.11039589047\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (8.37017059326,26.7477638132), test loss: 31.5371450901\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.90294599533,3.40020865989), test loss: 2.47694775015\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (21.7789134979,26.6290967939), test loss: 34.2455928564\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.15838551521,3.38552983895), test loss: 2.93558486104\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (4.805331707,26.5102310043), test loss: 30.7723790169\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.452935039997,3.37107285084), test loss: 2.68318031877\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (7.51804161072,26.3928967648), test loss: 35.8786785126\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (3.27935791016,3.3568252885), test loss: 2.99997765422\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (7.50896406174,26.2768085084), test loss: 28.7136151791\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.738649964333,3.34292323255), test loss: 2.63426357806\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (11.4835510254,26.1626823718), test loss: 35.6327726364\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.49524450302,3.32924682747), test loss: 2.94113124013\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (40.5129814148,26.0496255541), test loss: 30.1334192753\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.0714840889,3.31576587838), test loss: 2.7915420264\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (20.5416755676,25.9365154205), test loss: 34.9736362934\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.98142874241,3.30235817152), test loss: 2.95704598725\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (12.4475650787,25.8250124847), test loss: 29.200221014\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.5646739006,3.28913007106), test loss: 2.84454749525\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (6.01077318192,25.7154168546), test loss: 32.5076083183\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.17389822006,3.2758900113), test loss: 2.59084316641\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (3.32034254074,25.605541268), test loss: 37.2386611938\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.00942146778,3.26296060996), test loss: 2.96987264156\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (14.9757490158,25.4975315538), test loss: 30.6118139744\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.31973314285,3.25026580573), test loss: 2.66065280437\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (12.94024086,25.3892000828), test loss: 35.7145312786\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.10698509216,3.23760315032), test loss: 2.82182461023\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (13.952255249,25.2830460321), test loss: 27.8532139778\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.51367521286,3.22525629872), test loss: 2.63608500361\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (4.08478116989,25.1781451488), test loss: 35.2988625526\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.652607083321,3.21313613635), test loss: 2.84840205908\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (9.98784065247,25.0726824271), test loss: 27.3712044716\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.02153253555,3.20108386528), test loss: 2.51901458502\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (7.43366909027,24.969042188), test loss: 33.6756461143\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.772103846073,3.18909832048), test loss: 2.81900452375\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (27.9874305725,24.8670613546), test loss: 29.2725989819\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.27756476402,3.17720735317), test loss: 2.82549968958\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (7.01440382004,24.7643173503), test loss: 32.0921569824\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.78717184067,3.16546930505), test loss: 2.69533590972\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (10.6863765717,24.6634539374), test loss: 32.5380126953\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.38962078094,3.15389260897), test loss: 2.9793710351\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (11.1986732483,24.5621076874), test loss: 32.1357096672\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.962697863579,3.14239397445), test loss: 2.43294952363\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (5.2062792778,24.4624079659), test loss: 35.8844803333\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.91783618927,3.13120706452), test loss: 2.89456439614\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (10.5207843781,24.364102505), test loss: 27.4559237957\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.581775665283,3.12014763304), test loss: 2.57968316674\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (14.7535896301,24.2656990643), test loss: 34.208171463\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.409575879574,3.10920403663), test loss: 2.6695456773\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (19.3331069946,24.1683362914), test loss: 26.9944969654\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (4.35563182831,3.09834031718), test loss: 2.60275102854\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (6.165766716,24.0720946076), test loss: 34.567908287\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (3.57414937019,3.08745789154), test loss: 2.90029707849\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (34.9982948303,23.9760963928), test loss: 28.5656718254\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.24094605446,3.07674777051), test loss: 2.63887474984\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (18.5917873383,23.8807929732), test loss: 34.3038380623\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.74014389515,3.06614982277), test loss: 2.88456656337\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (9.38589286804,23.7854904153), test loss: 32.0862861633\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.68759536743,3.05565712376), test loss: 2.95972470641\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (21.8937473297,23.6914948391), test loss: 34.6005702972\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.65540516376,3.04536269139), test loss: 2.38602536619\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (11.5965003967,23.5982173114), test loss: 36.7857297659\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (4.09669971466,3.03523074897), test loss: 2.91180776656\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (27.8638210297,23.5061562731), test loss: 33.3358291626\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.32480812073,3.02519191582), test loss: 2.74403977394\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (12.8234462738,23.4134543741), test loss: 37.5780593395\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.46546912193,3.01518911613), test loss: 2.92139530331\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (11.7133083344,23.322339322), test loss: 27.6902999401\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.52907061577,3.00525925908), test loss: 2.62709356546\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (5.78352880478,23.2316856144), test loss: 35.2633303642\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.927963078022,2.99532167195), test loss: 2.88213733584\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (4.04275560379,23.1414988783), test loss: 30.3966270447\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.740814268589,2.98557641284), test loss: 2.61617719531\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (11.8988294601,23.051552889), test loss: 35.0684486866\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.46649670601,2.97596925737), test loss: 2.8852024436\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (10.9811840057,22.9621251371), test loss: 29.4961423397\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.73317825794,2.9663543769), test loss: 2.79177594483\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (10.8667984009,22.8734689644), test loss: 33.2402606487\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.72773444653,2.95696991035), test loss: 2.54237098992\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (6.85882329941,22.7861940054), test loss: 36.5266890049\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.963637948036,2.94768956937), test loss: 2.89935619533\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (5.1942615509,22.698043041), test loss: 31.7048486233\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.279200732708,2.9384846158), test loss: 2.59951703846\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (11.652217865,22.6117687193), test loss: 36.5280486107\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.938706040382,2.92927538922), test loss: 2.741105479\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (8.74629211426,22.5254395768), test loss: 29.8754886627\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.301422506571,2.92009901047), test loss: 2.6834744513\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (5.95298433304,22.4397050418), test loss: 35.8037106991\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.34242522717,2.91102627838), test loss: 2.74534874558\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (12.2539253235,22.3541602367), test loss: 27.7237598419\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.1000995636,2.90207261461), test loss: 2.47522008121\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (5.62471914291,22.269024044), test loss: 37.1817349911\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.17845475674,2.89315596824), test loss: 2.80730378032\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (6.70094823837,22.1845403474), test loss: 29.4984093189\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.1793628931,2.88443635647), test loss: 2.77992234826\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (12.4260463715,22.1012504536), test loss: 34.4317876339\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.422342956066,2.87578677657), test loss: 2.7017431289\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (15.5823478699,22.0177063981), test loss: 31.5940657616\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.56034731865,2.86725348632), test loss: 2.88825760484\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (5.99914932251,21.9351244812), test loss: 32.6140769005\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.38927650452,2.8586806399), test loss: 2.47268449068\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.0424656868,21.852853271), test loss: 37.087636137\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.9131064415,2.85015367809), test loss: 2.83734914362\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (23.1720581055,21.7712604127), test loss: 27.7507698536\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.94869852066,2.84170912335), test loss: 2.60306208134\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (15.8490743637,21.6895159727), test loss: 36.1911587238\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.15277075768,2.83335827763), test loss: 2.61261334717\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (11.4686985016,21.6083658438), test loss: 28.340880537\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.14721524715,2.82502279533), test loss: 2.62413256466\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (14.1086177826,21.5276506603), test loss: 36.8726844788\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.00375080109,2.81688994484), test loss: 2.90910735726\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (10.4618272781,21.4477549002), test loss: 31.3243533611\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.15385580063,2.80878068891), test loss: 2.61611796618\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (8.55305480957,21.368221364), test loss: 35.0136137962\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.18004858494,2.80082043254), test loss: 2.83105926216\n",
      "\n",
      "MC # 5, Hype # hyp5, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold3/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (326.189117432,inf), test loss: 169.600862503\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (260.067443848,inf), test loss: 319.99054718\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (63.530921936,76.3587529974), test loss: 46.3910602093\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.71593999863,31.3591586179), test loss: 3.65579982698\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (42.3182449341,60.5051218181), test loss: 37.1553574085\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (0.97894513607,17.4096943038), test loss: 3.61341566443\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (35.5364341736,55.2315322326), test loss: 44.4898553371\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (6.16038179398,12.7563166382), test loss: 3.66613591909\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (116.758789062,52.3986582978), test loss: 38.5165739059\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.80200719833,10.4152270128), test loss: 3.72188993692\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (27.3689498901,50.671029788), test loss: 39.305376029\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.37829732895,9.00546570152), test loss: 3.01469691396\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (25.5304374695,49.3496714966), test loss: 41.8618237972\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.02688932419,8.06057944196), test loss: 3.88318507671\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (62.0298156738,48.3371161265), test loss: 39.7468185902\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.16151499748,7.38567871), test loss: 2.75899104774\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (28.7641086578,47.4847275081), test loss: 42.2617687225\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.910440146923,6.86973229475), test loss: 3.68691263199\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (32.3646736145,46.7424547905), test loss: 33.8749001026\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.57244348526,6.46809303094), test loss: 2.98548066616\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (38.5369262695,46.0528924822), test loss: 43.6274489403\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.82123756409,6.14164838545), test loss: 3.48129929304\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (79.5010223389,45.4836562081), test loss: 30.318532896\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (5.31463193893,5.87168339743), test loss: 2.59802704751\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (13.659942627,44.8938146006), test loss: 40.9409626722\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.15541553497,5.64247445261), test loss: 3.62431923151\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (70.5638275146,44.3811476422), test loss: 32.1346742868\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.30626106262,5.44848754909), test loss: 3.30652089715\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (15.1821966171,43.8321857126), test loss: 39.7014587402\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.00069284439,5.27856468851), test loss: 3.43130393624\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (24.4566879272,43.3027841091), test loss: 34.7748575449\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.47683215141,5.13048256266), test loss: 3.54981426597\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (37.7957191467,42.7792579093), test loss: 34.0249083519\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.39420831203,4.99636083443), test loss: 2.85077491105\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (33.4715156555,42.2475755941), test loss: 37.4361759186\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.23708033562,4.87652515632), test loss: 3.60122965872\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (23.9297065735,41.7092352212), test loss: 30.4720890045\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.34561014175,4.76713500105), test loss: 2.63262931406\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (19.5682888031,41.2000572124), test loss: 38.89874506\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (0.854698181152,4.66736249056), test loss: 3.54089813232\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (28.930562973,40.6855272311), test loss: 26.8642616272\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.29343032837,4.57479864542), test loss: 2.66548267007\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (21.6846160889,40.1892206692), test loss: 36.4109779835\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.17036437988,4.49117180521), test loss: 3.49033166766\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (18.5851707458,39.6910082813), test loss: 22.6328059196\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.863890826702,4.41296871913), test loss: 2.71829776764\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (24.9711513519,39.1994855047), test loss: 35.5534793854\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.76413047314,4.34119683884), test loss: 3.43588247299\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (21.0258998871,38.7181774703), test loss: 29.4287270546\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.43478107452,4.27403656945), test loss: 3.56081137657\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (7.70099496841,38.2390259935), test loss: 27.4814368725\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.55612325668,4.21111515839), test loss: 2.84461221695\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (30.5720367432,37.7670961297), test loss: 33.9073107243\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.46200025082,4.151695629), test loss: 3.64280261397\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (77.7030563354,37.3232990295), test loss: 28.1282444\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.69206094742,4.09592918445), test loss: 2.45125575364\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (24.7521896362,36.8892400797), test loss: 33.6326501846\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.63127207756,4.04283612432), test loss: 3.56092814207\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (47.1599578857,36.4756827894), test loss: 26.6342642307\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.35073637962,3.99401353833), test loss: 2.60522022247\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (16.0001373291,36.0688823276), test loss: 35.0234098911\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.43500471115,3.94742860492), test loss: 3.39142721146\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (30.6617965698,35.6781405276), test loss: 24.4761814117\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.08768475056,3.90381330299), test loss: 2.72306216657\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (14.0375547409,35.3052013813), test loss: 33.8533042908\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.10100626945,3.86249421717), test loss: 3.43415565491\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (7.30288362503,34.9367444637), test loss: 28.0112108946\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.00711560249,3.82297909789), test loss: 3.40236359984\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (17.258600235,34.5777513992), test loss: 33.4804686069\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.13351750374,3.78500677427), test loss: 3.42536011189\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (17.1152381897,34.2413668887), test loss: 29.5801912308\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (5.42117500305,3.74874010461), test loss: 3.58179433644\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (19.6684093475,33.9172382879), test loss: 29.3040108681\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.43912935257,3.7137549411), test loss: 2.5424782604\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (12.1034975052,33.6036782212), test loss: 31.7328661919\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.66580557823,3.68102870885), test loss: 3.68732677996\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (18.7981338501,33.2996594324), test loss: 28.9066178799\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.78304958344,3.64935019138), test loss: 2.5341465652\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (13.5801610947,33.0054111348), test loss: 35.2117529154\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.43122768402,3.61955461402), test loss: 3.50634809583\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (22.8985538483,32.7253531449), test loss: 25.7899864912\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.20860624313,3.590910111), test loss: 2.71261574179\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (40.7530059814,32.446316938), test loss: 34.947697401\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (4.35267066956,3.56309414126), test loss: 3.43486590683\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (20.4838428497,32.1745213731), test loss: 22.8249139786\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.44322514534,3.53604887699), test loss: 2.55943669081\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (26.749130249,31.917472854), test loss: 34.5668261051\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.54917812347,3.50972356272), test loss: 3.41820201874\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (13.9354305267,31.6683217656), test loss: 28.0535877943\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.6093378067,3.48442272464), test loss: 3.44385315776\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (23.0173950195,31.4246516564), test loss: 28.8702536106\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (5.25131797791,3.46027854808), test loss: 2.81406488717\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (17.0843734741,31.1876751573), test loss: 31.0703009605\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.34032857418,3.43682750115), test loss: 3.44769450426\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (11.3036193848,30.9585104331), test loss: 28.1336074591\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.35727310181,3.41441146578), test loss: 2.5203469336\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (11.3663005829,30.7370860503), test loss: 33.8188209534\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.07925748825,3.3927347092), test loss: 3.43647778034\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (17.4586696625,30.5154558148), test loss: 27.9335305452\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.2536239624,3.37148381531), test loss: 2.52413290739\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (20.952173233,30.3003565314), test loss: 33.7858328819\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.6182346344,3.35077811083), test loss: 3.379775846\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (6.98436403275,30.0934456959), test loss: 25.5267644405\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.02373576164,3.33021953721), test loss: 2.77036786675\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (16.8630027771,29.8918793427), test loss: 32.8311256886\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (8.05222320557,3.31054346992), test loss: 3.18512997329\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (23.6290340424,29.6934069985), test loss: 28.9481939793\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.28234100342,3.29133801838), test loss: 3.2747753799\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (5.80062580109,29.4992586017), test loss: 34.0369216204\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.71976971626,3.27289468845), test loss: 3.39039782286\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (12.9589595795,29.3111070841), test loss: 28.5490427017\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.843628883362,3.25491685652), test loss: 3.41193719804\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (48.1740455627,29.1276903778), test loss: 29.1982078075\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.8795671463,3.23754528782), test loss: 2.71981482208\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (63.402256012,28.9429377261), test loss: 31.150788641\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.14451694489,3.22019952666), test loss: 3.50201485753\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.8505306244,28.7632615401), test loss: 29.1931301117\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.12064695358,3.20339249476), test loss: 2.46967650056\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (8.13900756836,28.5904138606), test loss: 33.7163176537\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.44809246063,3.18647452294), test loss: 3.33285581768\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.8621702194,28.4193948819), test loss: 27.3640069485\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.429101139307,3.17012955584), test loss: 2.66282971352\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (11.1337947845,28.2502443529), test loss: 34.6134553432\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.57151269913,3.15431542932), test loss: 3.26952191889\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.3011322021,28.0857697063), test loss: 23.3457839012\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.54168081284,3.13905687868), test loss: 2.43855074644\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (18.651676178,27.9244603962), test loss: 33.2946797609\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.02836227417,3.12396449313), test loss: 3.26946784854\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.5388145447,27.764392172), test loss: 27.4374869347\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.4621155262,3.10922080477), test loss: 3.20710019469\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (21.2074737549,27.6069709592), test loss: 33.980446434\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (4.09534215927,3.09470279302), test loss: 3.33180342838\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.1128768921,27.4526572245), test loss: 28.7323540926\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (5.33008909225,3.0803540212), test loss: 3.36415731162\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (15.8846988678,27.3025291849), test loss: 29.9785017967\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.77430868149,3.06599497348), test loss: 2.52791426629\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (27.4638652802,27.1536204093), test loss: 31.820368433\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.02758622169,3.05209732152), test loss: 3.36768525839\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (31.1719589233,27.0051302527), test loss: 28.7736595631\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.50094127655,3.03833028951), test loss: 2.35559662431\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (13.0501184464,26.8608822741), test loss: 34.9475625277\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.31034207344,3.02523162426), test loss: 3.38937624395\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (18.7614059448,26.7187220661), test loss: 26.0864361286\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.49024200439,3.01216826897), test loss: 2.61050722599\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (40.0968894958,26.5768772755), test loss: 33.0239076138\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.30624008179,2.99942582068), test loss: 3.14326087758\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (11.562335968,26.4369802016), test loss: 23.8244924545\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.618236064911,2.98672962048), test loss: 2.79381655902\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (9.1437587738,26.3002491069), test loss: 33.6917500496\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.06870985031,2.97418576844), test loss: 3.26519647986\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (21.9467506409,26.1657229183), test loss: 27.8591044426\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.29039382935,2.96164490436), test loss: 3.21465141028\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (4.61574697495,26.0322355462), test loss: 29.174339819\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.58163833618,2.94950403688), test loss: 2.75463746935\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.4919624329,25.898689271), test loss: 30.9901248932\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.46718287468,2.93744580632), test loss: 3.36247512698\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (29.9384479523,25.7693178707), test loss: 29.4737847328\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.68547058105,2.92584678104), test loss: 2.33533673882\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (7.16438484192,25.6419579831), test loss: 32.7589155674\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.340501993895,2.9142845676), test loss: 3.18435626626\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (17.8992919922,25.5135990224), test loss: 29.3159126282\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.53427284956,2.90297940657), test loss: 2.55415843874\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (24.4224205017,25.3880987511), test loss: 33.9443325996\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.29616928101,2.89171346925), test loss: 3.1501428768\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (11.3622970581,25.2653215338), test loss: 24.7732819557\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.77264642715,2.88053593665), test loss: 2.52690515071\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (5.48543357849,25.1424869972), test loss: 34.5519206047\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.34818768501,2.86942805577), test loss: 3.12086069584\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (19.6476325989,25.0217815145), test loss: 27.1426400661\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.66969645023,2.85858001177), test loss: 3.00886999369\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (8.79239463806,24.9008654479), test loss: 34.3381712437\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.30183601379,2.84779196681), test loss: 3.21756242514\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (7.41024923325,24.7823494006), test loss: 28.7348936558\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.11419296265,2.83737318873), test loss: 3.27784738541\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (12.799448967,24.6665547629), test loss: 31.7404968739\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.943408727646,2.82703938766), test loss: 2.58343085349\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (24.9945716858,24.5497003534), test loss: 31.1198155403\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.08223557472,2.81686845728), test loss: 3.27923951149\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (12.4517822266,24.4349111257), test loss: 31.0659593582\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.866015255451,2.80667039947), test loss: 2.43276369572\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (13.8649320602,24.3224039009), test loss: 34.5596049786\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.916833996773,2.79658842751), test loss: 3.28325331658\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (33.6263465881,24.2102754701), test loss: 26.3789103985\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.28540182114,2.78660233558), test loss: 2.52213635445\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (25.3188285828,24.0988629009), test loss: 35.3348119259\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.56043815613,2.77680035086), test loss: 3.18074360192\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (3.14906859398,23.9880263512), test loss: 23.6105353355\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.909070611,2.76705062701), test loss: 2.36805417538\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (6.44290876389,23.8790746577), test loss: 34.1587404728\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.71707653999,2.75760587425), test loss: 3.19053311497\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.2767887115,23.7719020826), test loss: 26.8754315853\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.13157439232,2.74823290241), test loss: 3.08850610256\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (6.01182460785,23.6640118105), test loss: 31.0200953722\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.17406177521,2.73899552918), test loss: 2.73010735512\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (20.8943462372,23.55817247), test loss: 30.1205195189\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.64623296261,2.72975853179), test loss: 3.20287202001\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (28.6915683746,23.4538990497), test loss: 31.4345047712\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.37229442596,2.72054863592), test loss: 2.4930414319\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (23.7789669037,23.3500095049), test loss: 33.3168805122\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.3184223175,2.7114418671), test loss: 3.13942127824\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (33.8559150696,23.2467846328), test loss: 29.4326507568\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.29881358147,2.70251785657), test loss: 2.4908246994\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (8.81181430817,23.1439562334), test loss: 34.7733342648\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.64076709747,2.69363870178), test loss: 3.15699147582\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (14.8080129623,23.0428536719), test loss: 26.377266407\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.7280382514,2.68498695528), test loss: 2.56695609093\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (4.95524120331,22.9431188485), test loss: 35.2538881302\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.91092431545,2.67644126726), test loss: 3.04895321131\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (5.44685554504,22.8426618283), test loss: 28.1444794655\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.642551481724,2.66795940117), test loss: 2.94328505695\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (15.1503276825,22.7441815627), test loss: 34.7448531866\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (3.43817543983,2.65955259508), test loss: 3.21503905952\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (16.3352661133,22.646448084), test loss: 28.4665659904\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.94622635841,2.65108950908), test loss: 3.16690925062\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (6.92483854294,22.5493144445), test loss: 33.3012604237\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.537644326687,2.64270825377), test loss: 2.73617408872\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (8.94876766205,22.4529179085), test loss: 30.5278564453\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.17978262901,2.6345561212), test loss: 3.2185436517\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (9.20251846313,22.3568208405), test loss: 32.8088371754\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.57222151756,2.62638665965), test loss: 2.49744558036\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (12.1496515274,22.2619183554), test loss: 34.5299016953\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.15658688545,2.61841895846), test loss: 3.16351564527\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (8.99638366699,22.168664934), test loss: 28.3963016987\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.829465210438,2.61057129984), test loss: 2.55848684609\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (13.5736236572,22.0743118347), test loss: 36.5912209034\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.61677360535,2.60278735204), test loss: 3.10969694555\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (3.28592586517,21.9814279846), test loss: 24.4401416779\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.780762195587,2.59495164915), test loss: 2.32547475249\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (20.0599021912,21.8897062465), test loss: 35.0728285313\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (4.4445605278,2.58716903204), test loss: 3.12235243171\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (3.62019252777,21.7981028185), test loss: 28.4290919304\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.08952736855,2.57946297822), test loss: 2.99860134721\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (12.8308172226,21.7069334673), test loss: 36.4372983932\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.66281998158,2.5719038196), test loss: 3.22663495392\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (8.23292541504,21.6166927712), test loss: 30.5933530331\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.87802445889,2.56439646518), test loss: 3.15440325439\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (17.1807727814,21.5269407492), test loss: 33.8622908592\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.30383181572,2.55695940587), test loss: 2.55502255857\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (13.9519577026,21.4387100394), test loss: 34.1475607395\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.08669805527,2.54971704983), test loss: 3.18151262999\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (7.4498295784,21.3497474303), test loss: 31.2427778721\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.9032459259,2.54249018849), test loss: 2.39633651525\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.7035770416,21.2619985276), test loss: 35.9840423107\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.732123553753,2.53523589382), test loss: 3.21538215578\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (3.94654130936,21.1750989487), test loss: 27.9261557102\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (0.986168980598,2.52798053109), test loss: 2.52444629967\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (7.62345695496,21.0884766274), test loss: 35.9628012896\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (4.89900684357,2.52088464907), test loss: 2.98050867468\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (4.83105182648,21.0020120319), test loss: 27.1418903828\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.74137091637,2.51376862963), test loss: 2.78520083427\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (6.914041996,20.9164410029), test loss: 37.4282577038\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.46675693989,2.50684701061), test loss: 3.23955080509\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (8.07297706604,20.8312645247), test loss: 28.3428888321\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.703717529774,2.49992686511), test loss: 2.95811355114\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (23.6023273468,20.7473710211), test loss: 34.8741303444\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.52594053745,2.49316835385), test loss: 2.84577998519\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (6.5219335556,20.6632335115), test loss: 31.4507785797\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.32917940617,2.48642504889), test loss: 3.14322608113\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (15.1095352173,20.5796641883), test loss: 33.0656576633\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.80982112885,2.47968185216), test loss: 2.38761038482\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.60530471802,20.4971041144), test loss: 34.1858169556\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.24502301216,2.47289310221), test loss: 3.02653114423\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (13.7934617996,20.4148217232), test loss: 32.3988325119\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.540043950081,2.46623868667), test loss: 2.61410255134\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (8.52509689331,20.3323494568), test loss: 36.6924236536\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.00267100334,2.45963974036), test loss: 3.00184368789\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (15.462266922,20.250776237), test loss: 26.564637661\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.59127283096,2.45317692419), test loss: 2.4380362615\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (8.66014289856,20.1698801688), test loss: 36.3497614861\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.56064558029,2.44673106068), test loss: 3.05158511251\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (22.9388751984,20.0896352245), test loss: 28.3927018166\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.01228511333,2.44037739278), test loss: 2.81936153173\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (5.60462474823,20.0094772719), test loss: 37.4684042454\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.20732402802,2.43407180161), test loss: 3.18732206225\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.42447662354,19.9300977682), test loss: 29.9424182892\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.998874843121,2.42774628828), test loss: 3.12741509676\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (3.19067907333,19.8514385021), test loss: 34.7252872467\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.953609287739,2.42140556174), test loss: 2.58119506836\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (15.1168785095,19.772646899), test loss: 32.8562213182\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.31081676483,2.41519913109), test loss: 3.1098865509\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (15.6093606949,19.6942511276), test loss: 34.4841712475\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.3221218586,2.40899105262), test loss: 2.51533510536\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (4.82427215576,19.6162492018), test loss: 36.4352898121\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.952232897282,2.40290955764), test loss: 3.225388816\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (9.17547035217,19.538994035), test loss: 27.9861701012\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.16094064713,2.3968446437), test loss: 2.4905461356\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (9.56357383728,19.4623565773), test loss: 38.0621747017\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.77015924454,2.39092633761), test loss: 3.06926719695\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (3.35196471214,19.3860764007), test loss: 25.6204833031\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.88242733479,2.38498745675), test loss: 2.3244042486\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (6.46099853516,19.3101934373), test loss: 37.0330807209\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.04947280884,2.37901230648), test loss: 3.14619044513\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (8.37769126892,19.2349403503), test loss: 28.0481857777\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.38428914547,2.37309574968), test loss: 2.92021265328\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.13873577118,19.1597185887), test loss: 34.120462513\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.09166121483,2.3672385398), test loss: 2.77691757232\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (12.671040535,19.0847520322), test loss: 30.4762465954\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.21795809269,2.3614169242), test loss: 3.06917622089\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (16.3432807922,19.0104660714), test loss: 34.6429956913\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.91675555706,2.35570563403), test loss: 2.52434095442\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.30252504349,18.9365479165), test loss: 35.6306899548\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.318044096231,2.34997145921), test loss: 3.06193940043\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (6.88832044601,18.8633116275), test loss: 32.9170716286\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.289871573448,2.34440379044), test loss: 2.56351274103\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (10.6232624054,18.7904944129), test loss: 36.9387127399\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.9153110981,2.33879938442), test loss: 3.1003052026\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.9735584259,18.7179575682), test loss: 27.3581002712\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.14965486526,2.33311730446), test loss: 2.50021009445\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (3.43770337105,18.6460289512), test loss: 38.5896498442\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.65688765049,2.32759078594), test loss: 2.93172204792\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (11.5525550842,18.5742571375), test loss: 29.3432947636\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.814682781696,2.32205733081), test loss: 2.80324554443\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.69721460342,18.5024925301), test loss: 38.0067420959\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.00853133202,2.31653160254), test loss: 3.19745070934\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (9.65666770935,18.4316468801), test loss: 29.9246147633\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.97096860409,2.31114418102), test loss: 3.08452988863\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.72831630707,18.36123513), test loss: 35.6036983013\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.06521236897,2.30572842407), test loss: 2.79590554833\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (12.0329904556,18.2909833737), test loss: 31.9413645744\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.48365688324,2.30043438064), test loss: 3.13005287051\n",
      "\n",
      "MC # 5, Hype # hyp5, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold4/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (344.979309082,inf), test loss: 190.296416092\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (264.570800781,inf), test loss: 320.166978455\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (27.5208320618,85.9845789995), test loss: 43.5820291996\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.17688655853,28.184454971), test loss: 3.63047710061\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (55.8857421875,65.9415967908), test loss: 36.8363109112\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (0.874173045158,15.5551679315), test loss: 3.87178815454\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (13.6720504761,59.2708524046), test loss: 43.601640749\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.56310653687,11.3333830153), test loss: 3.67757333517\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (57.5863800049,55.7410127325), test loss: 39.5753092766\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (5.21748828888,9.21890853331), test loss: 3.73079172671\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (23.4764194489,53.6186019569), test loss: 41.6769851685\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.60671758652,7.94443647651), test loss: 2.91646556109\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (93.1790466309,52.1329257849), test loss: 42.4811087608\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.79216003418,7.09248833539), test loss: 3.79097052813\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (11.7631778717,51.0101618821), test loss: 37.8392727375\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.12990486622,6.483170143), test loss: 2.69885988235\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (19.4308452606,50.1208138106), test loss: 41.6761441708\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.57438063622,6.01797141878), test loss: 3.47159953415\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (30.5620765686,49.3661347652), test loss: 35.6414795399\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.77623128891,5.65289165148), test loss: 3.05774076581\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (117.054252625,48.7237379021), test loss: 42.5964039803\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.74759101868,5.35832427774), test loss: 3.2786503315\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (31.7188587189,48.1698199839), test loss: 30.3701142788\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.90796685219,5.11377110066), test loss: 2.71533856094\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (22.3984966278,47.6540632755), test loss: 40.7169242859\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.5931789875,4.90535367971), test loss: 3.40412879884\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (36.5897827148,47.2092736379), test loss: 34.9505292416\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.89246070385,4.72710231788), test loss: 3.5753426604\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (10.5671291351,46.7794884127), test loss: 36.4983895779\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.32657361031,4.57325407911), test loss: 2.98054115772\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (34.3210906982,46.3691936799), test loss: 40.997802496\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.57073736191,4.43977506701), test loss: 3.60566977262\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (41.401184082,45.9703081108), test loss: 37.70159688\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.08285963535,4.31961459792), test loss: 2.72761982679\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (102.33946991,45.5785461778), test loss: 38.7135717869\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.85677337646,4.21300530738), test loss: 3.39809464812\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (153.185714722,45.1792160939), test loss: 33.5867433548\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.35094976425,4.11575579878), test loss: 2.86824218184\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (18.463809967,44.7765739846), test loss: 40.6292119741\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.55332136154,4.02746096605), test loss: 3.47337201536\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (34.2062454224,44.3777356323), test loss: 31.5193938255\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.04657566547,3.94545195512), test loss: 2.91881864965\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (31.5925350189,43.9937420175), test loss: 37.7792185307\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.98975563049,3.87066836528), test loss: 3.37826012969\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (77.3138427734,43.6057605129), test loss: 30.908106184\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.47225809097,3.80201869497), test loss: 3.42047293186\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (16.0201931,43.2030232801), test loss: 37.806880331\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.99883198738,3.73945882349), test loss: 3.45760410577\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (34.1926269531,42.80263044), test loss: 33.4747775078\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.3832192421,3.68053840639), test loss: 3.55731777251\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (40.8389396667,42.396738465), test loss: 34.1085248947\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.975681900978,3.62603299029), test loss: 2.70308326483\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (66.3174972534,41.9768951056), test loss: 35.3261700869\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.38882493973,3.5745687115), test loss: 3.49323576093\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (26.9423694611,41.5506834457), test loss: 30.3733191013\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.76543164253,3.52605412076), test loss: 2.71659271121\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (13.6371297836,41.1272505297), test loss: 36.8331605911\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.59163177013,3.47964209053), test loss: 3.40125786066\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (27.0502700806,40.7147487161), test loss: 29.0023463249\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.953531503677,3.436305287), test loss: 2.88459137976\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (29.8303127289,40.3005873912), test loss: 36.0346292973\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (6.41018009186,3.39563463811), test loss: 3.37688594311\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (24.8429374695,39.8820021765), test loss: 24.0845279694\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.17672359943,3.3571417281), test loss: 2.85120942742\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (13.077796936,39.4713796247), test loss: 35.6462482929\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.05952453613,3.32086060117), test loss: 3.43347883224\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (27.2553100586,39.0675814541), test loss: 29.1051576138\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.00582766533,3.28688986205), test loss: 3.49137939811\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (20.0679969788,38.6611735037), test loss: 30.1350404739\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.53805589676,3.25388004134), test loss: 2.99293334484\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (10.3346214294,38.2609849006), test loss: 33.2684022903\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.11076676846,3.22223295513), test loss: 3.59647094011\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (26.1845054626,37.8740495285), test loss: 31.43635602\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (5.89174461365,3.19185437997), test loss: 2.63259806782\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (23.5719928741,37.499653588), test loss: 32.3460553408\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.6796245575,3.16228076745), test loss: 3.40818334222\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (17.541179657,37.1327981231), test loss: 31.2749570608\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.62763774395,3.13477316757), test loss: 2.91052987576\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (20.567150116,36.771453254), test loss: 35.6996441364\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.7082324028,3.10811121542), test loss: 3.4506821692\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (11.9232997894,36.4225584566), test loss: 26.6088493824\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.83933281898,3.08284686183), test loss: 2.86131904721\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (14.7470989227,36.0865685641), test loss: 34.8171014309\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.85000193119,3.05866309638), test loss: 3.43332418501\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (14.8765459061,35.7518775434), test loss: 27.9534233093\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.02291750908,3.03483723585), test loss: 3.45300057232\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (10.7217445374,35.4255406343), test loss: 35.8675359011\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.67252635956,3.01180387254), test loss: 3.50326001942\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (14.4278240204,35.1152732246), test loss: 29.5022774696\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.10251092911,2.9894798489), test loss: 3.50334599316\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (10.0085039139,34.8154366525), test loss: 32.5228934288\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.849852204323,2.9675410427), test loss: 2.67671100199\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (48.7749404907,34.5244679071), test loss: 32.8433425903\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.4939520359,2.94684646664), test loss: 3.45418668687\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (21.1866016388,34.2356308898), test loss: 30.0135685921\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.61378824711,2.92645067145), test loss: 2.70162460208\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (31.4393692017,33.9587350696), test loss: 34.965274179\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.15083241463,2.90700483081), test loss: 3.44268401563\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (13.456237793,33.692218429), test loss: 29.8226798534\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.28625869751,2.88830340106), test loss: 2.91610713452\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (5.80985784531,33.4278627355), test loss: 34.8289062023\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.14743614197,2.86984300984), test loss: 3.30254514813\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (25.8276157379,33.168303562), test loss: 30.432222414\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.0976831913,2.85165756217), test loss: 3.24560009837\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (41.7371902466,32.9234217997), test loss: 36.4472949982\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.39293527603,2.83397984494), test loss: 3.50458372235\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.6114187241,32.6804247059), test loss: 29.1695086002\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.19166493416,2.816472122), test loss: 3.48155593872\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (9.7194442749,32.4460366909), test loss: 31.3973580837\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.24165034294,2.79969895813), test loss: 2.91364671588\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (8.23226833344,32.2138294308), test loss: 32.8243012428\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.759708166122,2.78324228648), test loss: 3.53346651495\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (12.2085933685,31.9909530582), test loss: 32.7654228687\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.22877883911,2.76759227789), test loss: 2.63756963611\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (26.8015937805,31.7752836953), test loss: 32.1240001202\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.13472521305,2.75228279051), test loss: 3.34128096104\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (27.6735420227,31.5597479259), test loss: 33.0628364086\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.44382858276,2.73714226843), test loss: 2.99271814823\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.3540048599,31.347625132), test loss: 35.6969174862\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.18507957458,2.72222696281), test loss: 3.37572290599\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (26.3932647705,31.1466484276), test loss: 27.5149926186\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.33129131794,2.70762431966), test loss: 2.82771692276\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (9.43123912811,30.9453929139), test loss: 35.2578064322\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.23379421234,2.69329337051), test loss: 3.29446637332\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (10.6995716095,30.7511687932), test loss: 28.1520396709\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.17636871338,2.67932455031), test loss: 3.31563243866\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (7.4318780899,30.5558406358), test loss: 36.5727233887\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.34263658524,2.66546599251), test loss: 3.36589750648\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.8836526871,30.3686719436), test loss: 30.3942761183\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.2021856308,2.65227678785), test loss: 3.41231198907\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (6.65250682831,30.186850103), test loss: 31.9221293926\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.357192575932,2.63928922232), test loss: 2.68565594256\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (22.4919948578,30.0053305418), test loss: 32.9937461138\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.412820667028,2.62662253732), test loss: 3.41659694612\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (18.9664859772,29.8261712167), test loss: 30.8997691631\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.98644244671,2.61407108785), test loss: 2.7072496295\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (2.94198346138,29.6554230282), test loss: 33.7291258574\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.6242595911,2.60168371158), test loss: 3.35748613179\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (40.9115829468,29.4849439923), test loss: 29.4443820715\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.16446256638,2.58948902467), test loss: 2.86808139682\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (24.510093689,29.3177940649), test loss: 34.218239069\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.577349483967,2.57750829813), test loss: 3.20104563236\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (11.9608316422,29.1494233248), test loss: 29.3367754459\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.66780471802,2.56572126141), test loss: 3.07881740332\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (32.1851234436,28.9883436373), test loss: 35.3775414467\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.37354791164,2.55441371463), test loss: 3.34823877811\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (7.11772918701,28.8299367367), test loss: 28.9692378759\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.14302563667,2.5432031172), test loss: 3.46816656739\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.8015365601,28.6719547361), test loss: 32.5300864458\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.46126008034,2.53222810871), test loss: 2.9871149838\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (26.982793808,28.516097501), test loss: 31.831434679\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.84723114967,2.52136772514), test loss: 3.52156065106\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (6.45117950439,28.36546538), test loss: 33.2836444855\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.65133941174,2.51057341323), test loss: 2.61412463486\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (9.66863918304,28.2168657759), test loss: 31.9866839886\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.88228535652,2.50002798312), test loss: 3.2652839452\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (16.4020557404,28.0697941929), test loss: 30.7770287514\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.85809350014,2.48961961255), test loss: 2.80729526877\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (20.0030059814,27.9229439351), test loss: 34.937493372\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.50425386429,2.47936945306), test loss: 3.17689048946\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (10.1059932709,27.7804129372), test loss: 26.2946745872\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.650901675224,2.46940554214), test loss: 2.76264861822\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (4.21377229691,27.6398803324), test loss: 35.0233036041\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.35819160938,2.45956882926), test loss: 3.27451964617\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (8.347240448,27.5005301094), test loss: 26.9591756821\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.08243942261,2.44998993775), test loss: 3.20623777509\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (42.4561004639,27.3624472328), test loss: 36.3684194326\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.44364309311,2.44048871314), test loss: 3.20050567389\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (14.3401145935,27.2271743323), test loss: 29.8904781342\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.67948174477,2.43103169609), test loss: 3.41008785069\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (6.11695194244,27.0937716018), test loss: 32.3568599701\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.48067712784,2.42157884794), test loss: 2.68514731824\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (4.76671171188,26.9621405795), test loss: 33.0989583731\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.312638759613,2.41234826921), test loss: 3.33394881189\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (8.20434474945,26.8306966056), test loss: 32.3764888763\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.920617699623,2.4032285021), test loss: 2.82301005125\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (19.1848506927,26.7019407874), test loss: 34.1022656441\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.25688385963,2.39439874353), test loss: 3.35061776638\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (11.2487487793,26.5756431635), test loss: 29.3189678669\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.24926018715,2.38570628776), test loss: 2.83982979059\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (34.473777771,26.4512335688), test loss: 34.9447900295\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.0208799839,2.37718414137), test loss: 3.17928765267\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (11.4498167038,26.326695166), test loss: 28.5033810377\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.738360524178,2.36867216714), test loss: 3.04036416709\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (8.85935401917,26.2045574698), test loss: 35.4005009174\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.49900960922,2.36025632144), test loss: 3.29817974567\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (8.22476196289,26.0839822564), test loss: 28.3958395481\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.99472916126,2.35179686455), test loss: 3.30108031631\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.2704524994,25.9645311625), test loss: 33.3241136551\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.8637765646,2.34352527956), test loss: 2.84817695022\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (31.4901618958,25.8449896401), test loss: 31.301508069\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.56600689888,2.33534226648), test loss: 3.40292755067\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (5.88602542877,25.7266279156), test loss: 34.1808469296\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.86216950417,2.32735699647), test loss: 2.62196699083\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (5.48101425171,25.611118401), test loss: 32.4444684267\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.501170098782,2.31944623408), test loss: 3.29176962972\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (16.4997215271,25.4979801431), test loss: 34.0051411152\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.155372947454,2.31177267621), test loss: 3.11905740499\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (15.5499000549,25.3840850807), test loss: 37.1582638979\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.74927639961,2.30415415334), test loss: 3.14736048579\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (14.0756158829,25.2723685458), test loss: 25.8198693037\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.84466278553,2.29656758661), test loss: 2.67634343505\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (4.05638599396,25.1623615854), test loss: 35.4414415359\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.736234843731,2.28890353958), test loss: 3.27855243981\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (12.8077983856,25.0536625685), test loss: 27.2142181635\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.26387536526,2.28147893385), test loss: 3.13545252681\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (13.891752243,24.9440767413), test loss: 33.9753054142\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (4.44236755371,2.27409259446), test loss: 3.03292186558\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (8.8475818634,24.8355938646), test loss: 31.7541451454\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.08915781975,2.26680801719), test loss: 3.44418094754\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (9.62998867035,24.7288112048), test loss: 33.3769606113\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.359779000282,2.25962261783), test loss: 2.65200601816\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (26.5837974548,24.6244569785), test loss: 33.1885242224\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.38054537773,2.25265292712), test loss: 3.32894262671\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (12.3930511475,24.5184532767), test loss: 34.1905265808\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.31122982502,2.24569402424), test loss: 2.92213096619\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.23304462433,24.4148703672), test loss: 32.91308074\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.773181915283,2.23871067237), test loss: 3.25244636238\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (8.41847133636,24.313047452), test loss: 29.133238101\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.63520288467,2.23181454952), test loss: 2.91586210728\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (13.2733507156,24.212307344), test loss: 36.389529705\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.416851609945,2.22498205066), test loss: 3.26561045349\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (20.8096618652,24.1114007165), test loss: 27.9220806122\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.01060557365,2.21826193267), test loss: 3.0695761472\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (16.6174659729,24.0111745376), test loss: 35.9255438328\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.76462709904,2.21157870726), test loss: 3.28231455088\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (12.2973003387,23.9124712795), test loss: 28.2733258486\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.03084039688,2.20499941362), test loss: 3.30385976434\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.6474905014,23.8156612007), test loss: 34.4158063173\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.49888885021,2.19856545917), test loss: 2.81403608024\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (9.75350379944,23.7174513042), test loss: 30.4525096893\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.639490067959,2.19220176876), test loss: 3.39809649885\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.63985347748,23.6207710596), test loss: 32.731718874\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.6154859066,2.18581832735), test loss: 2.64756473601\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (7.28666496277,23.5254552096), test loss: 34.6044444561\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.884961247444,2.17944562175), test loss: 3.39130700231\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (5.71848058701,23.4307118681), test loss: 30.6837234735\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.84469604492,2.17315217833), test loss: 2.80926475227\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (17.1221046448,23.3362311454), test loss: 35.8264427185\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.441862732172,2.16690357795), test loss: 3.12258644998\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (16.8126983643,23.2421788267), test loss: 26.4865381241\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.58045363426,2.16073767646), test loss: 2.68418317437\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (13.1083374023,23.1498841589), test loss: 36.7540373802\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.544917345047,2.15469111387), test loss: 3.26471403241\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (9.66555213928,23.0591295466), test loss: 26.7495727539\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (0.559922099113,2.14872356155), test loss: 3.21351249516\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (8.55487155914,22.9678429422), test loss: 34.8560006142\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.61976599693,2.14283055868), test loss: 3.14382199049\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (10.0369853973,22.8776511591), test loss: 31.5868428707\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.667590260506,2.13690432419), test loss: 3.37078659534\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (25.4910430908,22.7887874337), test loss: 33.7409361362\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.952326536179,2.13099024232), test loss: 2.72516106367\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (7.01711130142,22.6997605656), test loss: 33.5191408634\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.23036718369,2.12519317819), test loss: 3.32431894839\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.26727104187,22.6108457989), test loss: 34.0894209385\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.689900994301,2.11938274939), test loss: 2.90038460195\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (5.52977371216,22.5218866642), test loss: 33.482274437\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.63275539875,2.11363914623), test loss: 3.20753791332\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (6.01199960709,22.434217474), test loss: 29.3221838474\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.06095385551,2.10798129123), test loss: 2.95090287924\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (7.08394145966,22.3481205619), test loss: 35.7909095287\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.622722029686,2.10237353119), test loss: 3.18684439957\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (21.1020698547,22.2619179764), test loss: 28.9419770956\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.982058167458,2.09687446011), test loss: 3.24266645014\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (12.9369087219,22.1770348509), test loss: 36.7177506447\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.05438435078,2.09138626123), test loss: 3.33277879059\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (9.14436531067,22.093018011), test loss: 28.2777106524\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.799527108669,2.08583985664), test loss: 3.39509475231\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.72357368469,22.0092750181), test loss: 35.9320849657\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.54604923725,2.08042787505), test loss: 2.8472820282\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (9.94415187836,21.925737024), test loss: 32.3904006004\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.3370051384,2.0749994475), test loss: 3.44275767505\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.66094255447,21.8418851997), test loss: 33.8502385616\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.89013457298,2.06959989646), test loss: 2.64916647673\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.82355213165,21.759182917), test loss: 35.2292209387\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.1647336483,2.06433453455), test loss: 3.40181506276\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (3.23381567001,21.6774441556), test loss: 30.5934765816\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.237115398049,2.05906067548), test loss: 2.90099817514\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (9.24869728088,21.5954915919), test loss: 36.525664711\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.845419824123,2.05389503237), test loss: 3.12976871133\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (6.40589475632,21.5144140919), test loss: 26.8370182753\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.44275760651,2.04870572243), test loss: 2.79632745385\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (8.72486591339,21.4343336431), test loss: 37.5406290293\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.25509643555,2.04349437137), test loss: 3.38019206524\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (27.6040325165,21.354742274), test loss: 27.848356843\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.23895478249,2.03839614424), test loss: 3.33183995485\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (18.9740943909,21.275378386), test loss: 34.5745138645\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.584661722183,2.03331772453), test loss: 3.14391370416\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.09934854507,21.1957289691), test loss: 31.943424964\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.37102723122,2.02822955222), test loss: 3.39054121971\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (14.2649841309,21.1176348098), test loss: 35.0397165775\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.38104248047,2.02326427128), test loss: 2.7171879977\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (6.83536291122,21.0400862281), test loss: 34.4848205328\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.614952802658,2.01825287348), test loss: 3.42203712463\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (6.67052173615,20.9626860365), test loss: 36.1604588032\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.527564644814,2.01338904672), test loss: 3.08251951337\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (11.3491363525,20.8858253014), test loss: 36.7141808987\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.37902116776,2.00855001878), test loss: 3.42144076526\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.8196554184,20.8093021095), test loss: 29.7278427601\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.44304776192,2.00362097217), test loss: 2.98140518367\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (14.0833568573,20.7332495701), test loss: 36.7162388563\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.47997450829,1.99879802442), test loss: 3.19797199667\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (11.1918544769,20.6570735732), test loss: 27.901722455\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.28312194347,1.99398664129), test loss: 3.13786830902\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (17.8940963745,20.5811036019), test loss: 38.083464694\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.9516838789,1.9891675642), test loss: 3.40751641989\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.86092758179,20.5062543812), test loss: 28.1728528738\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.530233204365,1.98446537987), test loss: 3.34345895648\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.86453962326,20.4320961861), test loss: 36.8511461735\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.621848106384,1.97975001605), test loss: 2.88871700764\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (8.58308792114,20.3585181707), test loss: 33.5061072111\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.42634034157,1.97515303197), test loss: 3.44514400959\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (6.1645154953,20.2853534595), test loss: 34.7167304039\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.91850686073,1.97054387338), test loss: 2.9197514087\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.85201597214,20.2126631329), test loss: 36.4147460461\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (4.25151729584,1.96588865291), test loss: 3.47526227236\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (2.50046491623,20.1404083774), test loss: 31.3577658176\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.765458464622,1.96127613498), test loss: 2.99981037974\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (22.2185745239,20.0682482467), test loss: 37.06963377\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.776891231537,1.95675625953), test loss: 3.19103237391\n",
      "\n",
      "MC # 5, Hype # hyp5, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold5/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (329.583618164,inf), test loss: 187.080747986\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (310.225769043,inf), test loss: 370.986979675\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (29.1671943665,78.2847372723), test loss: 47.7859746456\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.629386603832,38.1102688026), test loss: 3.61868668497\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (31.9751091003,63.5430015993), test loss: 41.4650551319\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.4266910553,20.5446732928), test loss: 3.84112423658\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (70.1187362671,58.7450267359), test loss: 45.0683607817\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.90478694439,14.6702225082), test loss: 3.56409136653\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (35.9571228027,56.1638259566), test loss: 43.1462879658\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.46017360687,11.7251206125), test loss: 3.95669170022\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (37.4360122681,54.6983382368), test loss: 43.0642096996\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.79332280159,9.95923801942), test loss: 2.79621317089\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (63.8199996948,53.6072137291), test loss: 48.5358754158\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (5.00863838196,8.77698179923), test loss: 3.89355251789\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (36.2663345337,52.8085729551), test loss: 44.6370822191\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (8.3425321579,7.92884611434), test loss: 2.66071606278\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (46.4770126343,52.1700942523), test loss: 46.1270679474\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.11147499084,7.28866717867), test loss: 3.75519560575\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (97.1143264771,51.6015829281), test loss: 40.1651278973\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.42197728157,6.78682800027), test loss: 3.31811404228\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (30.4862728119,51.1046506766), test loss: 47.485943079\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.21940803528,6.38469093403), test loss: 3.59434107542\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (62.0665855408,50.6922616184), test loss: 38.6370258331\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.68801653385,6.05042695359), test loss: 2.84114051759\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (16.8046550751,50.3547509582), test loss: 45.5778011322\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.97123575211,5.77131124735), test loss: 3.46234217286\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (51.4957733154,50.024532856), test loss: 38.9894743443\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.52209413052,5.53428146319), test loss: 3.48788882494\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (78.5089416504,49.7210704859), test loss: 42.5834350586\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.71915721893,5.33079910953), test loss: 3.43112661839\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (43.6834373474,49.4248415088), test loss: 42.2746592045\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.986149549484,5.15040204984), test loss: 3.87563120723\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (51.4852523804,49.1240792724), test loss: 40.3282933712\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.66110336781,4.99290136216), test loss: 2.72052917182\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (52.1449394226,48.8190428188), test loss: 45.5188358307\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.8905236721,4.85191500856), test loss: 3.77879000902\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (156.605331421,48.5486499841), test loss: 40.9719241619\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.72826147079,4.72458358222), test loss: 2.50574952364\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (23.0145969391,48.2711069622), test loss: 43.0032526493\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.63343262672,4.60903205824), test loss: 3.61980429292\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (37.246307373,48.0172481573), test loss: 36.8545186043\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.47801113129,4.50520010794), test loss: 2.7802763015\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (58.6548156738,47.7387555874), test loss: 44.0146671772\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.61361932755,4.40962747998), test loss: 3.45184075534\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (28.2362518311,47.4676876233), test loss: 34.8428926945\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.46004390717,4.32163174107), test loss: 2.75907477438\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (41.3448028564,47.1834253021), test loss: 41.2193868399\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.56934702396,4.24087611351), test loss: 3.14363791943\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (22.3903236389,46.8708834812), test loss: 35.6567727327\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.32061028481,4.16483693212), test loss: 3.56173929572\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (39.4136886597,46.577144096), test loss: 38.5080272675\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.26211643219,4.09447315742), test loss: 3.29687669873\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (36.5975532532,46.2683736008), test loss: 37.4243427277\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.43524909019,4.02768692816), test loss: 3.56025611758\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (17.8298835754,45.9666511982), test loss: 34.8056522846\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.35472488403,3.96546142945), test loss: 2.55599471927\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (35.6356048584,45.641853748), test loss: 39.4961684704\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.898070514202,3.90699540119), test loss: 3.63969414234\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (15.1923303604,45.311236901), test loss: 35.2371575356\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.91864180565,3.8525438451), test loss: 2.52014960647\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (41.1471328735,44.9724215207), test loss: 37.3722651958\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.69038164616,3.80038703702), test loss: 3.48310722262\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (40.8066482544,44.6124508658), test loss: 31.3959474564\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.35212087631,3.75139268002), test loss: 2.79757565856\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (32.7279701233,44.2391189525), test loss: 38.4986450195\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.10998165607,3.70462499196), test loss: 3.44835003316\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (17.5857524872,43.8682452395), test loss: 28.1157834768\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.88303470612,3.65993725833), test loss: 2.68993030488\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (12.1406850815,43.4971009311), test loss: 34.9593201399\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.45917880535,3.61705601064), test loss: 3.21235086918\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (27.3267707825,43.1192993711), test loss: 30.8413380623\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.13026094437,3.57669068943), test loss: 3.57124383152\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (2.90043520927,42.723599365), test loss: 33.1316378832\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.76915574074,3.53857636133), test loss: 3.37292216122\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (15.6263618469,42.3314807505), test loss: 33.0434294701\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.9019998312,3.50204059859), test loss: 3.58361307383\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (72.7789916992,41.9353419615), test loss: 28.6612374306\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.49277496338,3.46733654108), test loss: 2.4979970023\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (76.418296814,41.5344471724), test loss: 33.3606091499\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.51927793026,3.43360672349), test loss: 3.74446818233\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (11.5221748352,41.1456757483), test loss: 30.3329751015\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.75729584694,3.40105512869), test loss: 2.53271729648\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (50.9682769775,40.7620350314), test loss: 32.6816592455\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.25719976425,3.36962551347), test loss: 3.50159415305\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (31.1072406769,40.3891035119), test loss: 28.1242383957\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.2325861454,3.33957525618), test loss: 2.74308072031\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (12.4319267273,40.0147256351), test loss: 34.4476754665\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.972015023232,3.31057033601), test loss: 3.41012234688\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (14.6143255234,39.6529781653), test loss: 25.8865085125\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.51095175743,3.283050605), test loss: 2.71109920889\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (15.245839119,39.3003125167), test loss: 32.7651830912\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.73549246788,3.2563835317), test loss: 3.37423587441\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (16.1805477142,38.9512806547), test loss: 30.1803723574\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.35537558794,3.230655911), test loss: 3.50812205225\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (20.332567215,38.6087599707), test loss: 33.0164152145\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.17604279518,3.20558676348), test loss: 3.45427851081\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (11.1428470612,38.2808686448), test loss: 32.114942193\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.37077927589,3.18105744877), test loss: 3.60110934228\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (26.2462539673,37.9630374726), test loss: 29.142377758\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (6.64983940125,3.15765164804), test loss: 2.52764707506\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (21.7159461975,37.6493021107), test loss: 33.2886129856\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.05390739441,3.13477790421), test loss: 3.65422766507\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (31.5300559998,37.3440918532), test loss: 30.206327796\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.97386884689,3.11289727948), test loss: 2.48121228218\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (21.4538497925,37.0482022681), test loss: 33.1048396587\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.92760658264,3.09161585398), test loss: 3.52697939873\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (42.7243843079,36.7582013591), test loss: 29.4168485403\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.2025270462,3.07113009823), test loss: 2.82607510388\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (52.5267295837,36.4712938285), test loss: 33.5153365612\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (5.36649370193,3.05096547291), test loss: 3.48966265023\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (5.84882211685,36.1977311297), test loss: 26.4842591286\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.23769581318,3.03104312584), test loss: 2.76690342724\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (28.4975547791,35.9294665327), test loss: 33.6884990215\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.64293384552,3.01185636765), test loss: 3.31667020321\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (28.1224842072,35.6686096981), test loss: 30.3469505548\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.74435687065,2.99325095635), test loss: 3.4261351794\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (26.4742393494,35.4097367691), test loss: 34.6499293327\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.22075438499,2.97503899436), test loss: 3.39068008959\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (34.8945274353,35.1596376639), test loss: 33.1756240487\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.13772916794,2.95747695451), test loss: 3.56593725681\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (9.40166664124,34.9168431349), test loss: 30.4197203636\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.690749108791,2.9403899951), test loss: 2.61672385931\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.2210273743,34.6750343367), test loss: 33.0400847673\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.09844481945,2.923647376), test loss: 3.6546082437\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (15.1119766235,34.4385520286), test loss: 29.4148000002\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.2085506916,2.90722331424), test loss: 2.4234790951\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (6.56978321075,34.2110678192), test loss: 34.3917996526\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.83949422836,2.89086717448), test loss: 3.51972012222\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (12.3327608109,33.9873066752), test loss: 28.2538386822\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.349438011646,2.87499839382), test loss: 2.79123741388\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (48.662979126,33.766672912), test loss: 33.3132966518\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.73297357559,2.85961983012), test loss: 3.35038887411\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.5280647278,33.5503594307), test loss: 26.3292733431\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.964704155922,2.84468865484), test loss: 2.67125624418\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (15.0618124008,33.3401569682), test loss: 33.3098495483\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.43778181076,2.83004450119), test loss: 3.26918544024\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (19.2109661102,33.1318049494), test loss: 30.4259632111\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.231417089701,2.81580804378), test loss: 3.41181983352\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (7.35141849518,32.9252747691), test loss: 34.186742115\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.83179855347,2.80167708595), test loss: 3.34829648435\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (25.614402771,32.7275677676), test loss: 30.9632927179\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.982113778591,2.78773236806), test loss: 3.5417940557\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (30.3749008179,32.5315917113), test loss: 30.7359454155\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.8096177578,2.77407774954), test loss: 2.49419172555\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (47.3199386597,32.3390178657), test loss: 34.0191101551\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.34671258926,2.76075671274), test loss: 3.54373319149\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (14.1789627075,32.1475532443), test loss: 30.274139452\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.72710454464,2.74766377919), test loss: 2.32120147794\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (9.52102279663,31.961452975), test loss: 34.7811140418\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.02241706848,2.73498432044), test loss: 3.44208474159\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (5.81694793701,31.7802176126), test loss: 31.9749363422\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.836964964867,2.72256578867), test loss: 2.93476750255\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (22.4084815979,31.5978404943), test loss: 33.7618476152\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.61899459362,2.71027153818), test loss: 3.3544490546\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (6.47331142426,31.4192349474), test loss: 26.069818306\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.06863260269,2.69821616445), test loss: 2.55243898034\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (16.4890899658,31.2460761498), test loss: 33.2627689004\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.671447873116,2.68602694248), test loss: 3.21803968251\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (18.2216014862,31.0741558781), test loss: 29.9762526512\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.97430610657,2.67430970327), test loss: 3.39878279269\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (35.2041702271,30.9025586944), test loss: 34.5795608759\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.64141941071,2.66271743744), test loss: 3.2617149204\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (32.5208282471,30.7358026498), test loss: 31.2134734869\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.32427740097,2.65152711728), test loss: 3.4424043268\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (11.444978714,30.572648274), test loss: 31.6631079197\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.382993876934,2.64044932702), test loss: 2.53579634279\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (27.073392868,30.4094110394), test loss: 33.0061189175\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.900473535061,2.62969434878), test loss: 3.51295264959\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (38.6334342957,30.2482474555), test loss: 29.1656444073\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.83107709885,2.61895503688), test loss: 2.3662938565\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (20.4503993988,30.0913011992), test loss: 34.2836792231\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.40496635437,2.60829181536), test loss: 3.40955612659\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (11.626994133,29.9358799478), test loss: 28.1798471451\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.714821457863,2.59774265308), test loss: 2.64283144027\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (24.6608448029,29.7819068561), test loss: 34.2535463333\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.65257906914,2.58747245372), test loss: 3.25560923368\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (17.0602340698,29.6289271871), test loss: 26.1821208477\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.80914962292,2.57733615916), test loss: 2.49601003826\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (12.6542844772,29.4796628648), test loss: 33.6696949005\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.36361670494,2.5674262945), test loss: 3.1811503306\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (13.849067688,29.3335297811), test loss: 31.4378197193\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.05967903137,2.55780479515), test loss: 3.48084557056\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (5.30865335464,29.18563478), test loss: 34.5283948779\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.05898141861,2.5481896132), test loss: 3.22808087468\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (7.762591362,29.0418216101), test loss: 31.0478654385\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.26562130451,2.53864904517), test loss: 3.41916693449\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (15.4518127441,28.9004715774), test loss: 31.8657622337\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.96755075455,2.52909910162), test loss: 2.50538341403\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (5.7908449173,28.759215673), test loss: 33.906289959\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.63172578812,2.51976992902), test loss: 3.46196231842\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.5235385895,28.6181014345), test loss: 29.0613429546\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.424959778786,2.51054214552), test loss: 2.34914937168\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (8.20270347595,28.4806450455), test loss: 34.7081176758\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.12389063835,2.50160119157), test loss: 3.35270280242\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (15.2494697571,28.3459883926), test loss: 28.3927514076\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.30681192875,2.49275264183), test loss: 2.7119984448\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (25.1820087433,28.2105899453), test loss: 34.0702501774\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.39393305779,2.48412502953), test loss: 3.14476130605\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (18.282875061,28.077134481), test loss: 25.2268111706\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.15026402473,2.47549091287), test loss: 2.46535395086\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (12.5882225037,27.9466488027), test loss: 33.6901315212\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.98099160194,2.46688577618), test loss: 3.19441116452\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (5.33847141266,27.8166097529), test loss: 30.2241470098\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.25215482712,2.45834951755), test loss: 3.20554890931\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (9.49109077454,27.6870821279), test loss: 34.0543570995\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.91707491875,2.44993952961), test loss: 2.8813174516\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (2.64257764816,27.5585634897), test loss: 34.9738118649\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.61175966263,2.44168793415), test loss: 3.46802831292\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (8.65377998352,27.4329076856), test loss: 33.2287361622\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.477141439915,2.43353691312), test loss: 2.4765093267\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (40.9363937378,27.3086113003), test loss: 33.7950812817\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.01923441887,2.4256125182), test loss: 3.43642461896\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (20.8542346954,27.1844403124), test loss: 29.8257173538\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.529069423676,2.41773946746), test loss: 2.37086573541\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.33500957489,27.062467177), test loss: 34.7514991283\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.65688633919,2.40985574372), test loss: 3.37052468061\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (27.7835254669,26.9420875434), test loss: 28.4893323421\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.74704658985,2.40203532853), test loss: 2.7267012462\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (15.1551761627,26.8217117876), test loss: 35.7733997345\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.02708864212,2.3943330941), test loss: 3.1094174087\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (5.16404056549,26.7014824972), test loss: 25.6211587906\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.942836046219,2.38667874758), test loss: 2.47555282563\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (4.29809093475,26.5837537609), test loss: 34.2370675564\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.305778265,2.37917959168), test loss: 3.18291079402\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (12.7239580154,26.4677374169), test loss: 28.8033940792\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.771081328392,2.37177995444), test loss: 3.23178739548\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (4.80819988251,26.3511613918), test loss: 32.4325433731\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.206073656678,2.36457068733), test loss: 2.74945300967\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (13.2909984589,26.2365194088), test loss: 32.2719331741\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.711403131485,2.3573231708), test loss: 3.43269435763\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (4.34565114975,26.1236633158), test loss: 33.0673123837\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.859136939049,2.3500818542), test loss: 2.48245229125\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (17.971950531,26.0114943105), test loss: 34.4552734852\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (5.16626214981,2.34305079626), test loss: 3.42010791302\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (9.98291492462,25.8988313365), test loss: 30.3351894855\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.00574076176,2.33595092041), test loss: 2.47948145568\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (20.2664642334,25.7875370283), test loss: 38.148329401\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.76969027519,2.32901114055), test loss: 3.48639676273\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (14.0231513977,25.6777382651), test loss: 28.9312730312\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.01098823547,2.32212439341), test loss: 2.66754718423\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (22.4944400787,25.5686915846), test loss: 34.4648109913\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.56250572205,2.31542641879), test loss: 3.1056874305\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (23.1307601929,25.4605292997), test loss: 25.5308092117\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.03778505325,2.30876645684), test loss: 2.48477912247\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (4.49670982361,25.3535953238), test loss: 34.4419116974\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.18078696728,2.30201654337), test loss: 3.13271908164\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (16.9292831421,25.2477003573), test loss: 29.1691013813\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.1118915081,2.29544381465), test loss: 3.17981593311\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (13.6337156296,25.1419036962), test loss: 32.0831633091\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.42618513107,2.28891288722), test loss: 2.63861581385\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (14.8674516678,25.036385873), test loss: 35.4144697428\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.8416261673,2.28243390558), test loss: 3.40224079043\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (15.7372016907,24.9322763173), test loss: 35.6145442486\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.451078802347,2.2760031056), test loss: 2.48314898908\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (5.25168800354,24.8295544063), test loss: 34.6601819277\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.751352190971,2.26967352045), test loss: 3.40237905979\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.78815078735,24.7263996954), test loss: 31.2140421867\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.14120578766,2.26349494545), test loss: 2.50199880004\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (10.3718700409,24.624682352), test loss: 36.9909095287\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.84130454063,2.25728897189), test loss: 3.47398820817\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (5.28332614899,24.5243959503), test loss: 28.9182446957\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.02295684814,2.25104383135), test loss: 2.65740759969\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (9.46109390259,24.4242993945), test loss: 34.7727897406\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.305722296238,2.24492646474), test loss: 3.09170921296\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (23.5259971619,24.3242828977), test loss: 26.7952516794\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.02615666389,2.23886461806), test loss: 2.61120156497\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (6.28578186035,24.2249963399), test loss: 35.1918660164\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.779209733009,2.23287118313), test loss: 3.13203097582\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (10.2020606995,24.1271472974), test loss: 28.951431489\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.85091304779,2.22688881754), test loss: 3.23280008435\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (11.0038452148,24.0294242133), test loss: 32.6029700279\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.223525494337,2.22105571344), test loss: 2.74028496444\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (4.35584545135,23.9326510337), test loss: 33.6467735648\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.17919707298,2.21521484015), test loss: 3.3269952625\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (9.37377643585,23.8367910658), test loss: 33.5764812469\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.671898007393,2.20933435446), test loss: 2.56548246294\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (20.423412323,23.7415124265), test loss: 35.117224431\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.49970555305,2.20357019656), test loss: 3.37698202133\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (21.9900379181,23.6463737109), test loss: 32.3828489304\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.35509550571,2.19782432047), test loss: 2.58615361154\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (9.85322380066,23.5518745585), test loss: 38.2215035915\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.50602388382,2.19216950955), test loss: 3.51080723852\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (8.42946624756,23.4582156359), test loss: 29.2972004414\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.89170408249,2.18652035361), test loss: 2.675730142\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (8.87247276306,23.3656681459), test loss: 36.5351599932\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.405134916306,2.1809404751), test loss: 3.15151580274\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (9.2319316864,23.2727562053), test loss: 28.7035709858\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.43638074398,2.1754322543), test loss: 2.83933186233\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (8.45559120178,23.1809289067), test loss: 35.4617767334\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.44790315628,2.16992300567), test loss: 3.11748075038\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (4.66250801086,23.0902769727), test loss: 28.9198628426\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.524738609791,2.16434058802), test loss: 3.20971254706\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (11.9115781784,22.9995082584), test loss: 32.587172842\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.20487046242,2.158917018), test loss: 2.73539533317\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (21.6454219818,22.9087817229), test loss: 34.060871315\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.39385032654,2.15349529219), test loss: 3.28775051236\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (14.0552272797,22.8191731719), test loss: 34.8681556702\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.604232847691,2.14815802164), test loss: 2.52801062018\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.78144836426,22.730649863), test loss: 35.4504640102\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.252728819847,2.14282377173), test loss: 3.3989187181\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (18.6154937744,22.642057557), test loss: 33.8883956909\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.658535718918,2.1376126316), test loss: 2.64027827978\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (23.4178504944,22.5544117976), test loss: 36.6384504557\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.2277238369,2.13236237091), test loss: 3.3753362596\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (4.10908508301,22.4670758366), test loss: 29.3256128311\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.772976338863,2.12708309337), test loss: 2.71568255424\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (8.27590370178,22.380372454), test loss: 36.773217082\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.434532523155,2.12186084235), test loss: 3.03483668268\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (10.9366931915,22.293801933), test loss: 30.6029139996\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.5163654089,2.11670112244), test loss: 3.17142468989\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (7.31255245209,22.20778792), test loss: 38.2602423668\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.00268745422,2.11161323143), test loss: 3.21285862476\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (13.7942085266,22.1225851339), test loss: 28.9785325527\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.21197557449,2.10650852151), test loss: 3.19938541651\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.89978694916,22.0383196445), test loss: 36.5482754707\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.883595526218,2.10153021808), test loss: 2.97173820436\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.87412548065,21.9538452467), test loss: 33.3553650856\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.16727256775,2.09655680898), test loss: 3.3100246489\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (4.02435493469,21.8701733278), test loss: 34.128953886\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.02797496319,2.09153975082), test loss: 2.53200079501\n",
      "run time for single CV loop: 7146.15686417\n",
      "\n",
      "MC # 5, Hype # hyp4, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold1/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (325.185119629,inf), test loss: 225.088996124\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (270.925567627,inf), test loss: 306.591719055\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (73.5703125,102.53388946), test loss: 46.4978662014\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.94776439667,45.9013255116), test loss: 3.42596271634\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (37.3221702576,74.2567749701), test loss: 39.7567341328\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.77966547012,24.5959488353), test loss: 3.35434994102\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (31.39752388,64.6722544575), test loss: 44.4456621647\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (7.50275802612,17.5115592486), test loss: 3.33392902017\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (47.7878723145,59.6856425598), test loss: 42.3701750278\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.64999628067,13.9611868934), test loss: 3.54212721288\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (27.9253864288,56.6579464796), test loss: 42.200394392\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.9172680378,11.8277353297), test loss: 2.70706375241\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (38.5245323181,54.5083288744), test loss: 44.6856345177\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.920870006084,10.4044377032), test loss: 3.7825697124\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (46.9398231506,52.9372426371), test loss: 39.5098906994\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.36394715309,9.38958716044), test loss: 2.64693329632\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (35.8905067444,51.7309943005), test loss: 43.7676806927\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.91121041775,8.62292905243), test loss: 3.47291797996\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (32.4595298767,50.7233117712), test loss: 37.0377431393\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.80993747711,8.02563178723), test loss: 2.76630908847\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (64.7387008667,49.8712904737), test loss: 44.5742683887\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.81324005127,7.54529631204), test loss: 3.29593271911\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (16.0321273804,49.1921089901), test loss: 33.8045413733\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.60813832283,7.15185778428), test loss: 2.53180149794\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (20.6883468628,48.5772714458), test loss: 42.4553305149\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.36034822464,6.82179791785), test loss: 3.48690979481\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (75.4672698975,48.0635390023), test loss: 37.2571761608\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.79216742516,6.54265578132), test loss: 3.32178682685\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (13.7817287445,47.5569380273), test loss: 38.152726984\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.32791686058,6.30236190715), test loss: 2.78059213459\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (48.5984230042,47.099966416), test loss: 42.6189493179\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.31082630157,6.09466445747), test loss: 3.51424427032\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (40.3596801758,46.6779159011), test loss: 39.6844864368\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.81628203392,5.91047926534), test loss: 2.71931436956\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (26.1485385895,46.2606149565), test loss: 42.4489552498\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.93221521378,5.74773262375), test loss: 3.55479601622\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (29.34283638,45.8541339223), test loss: 36.2764827967\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.46232628822,5.60164097031), test loss: 2.70426468849\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (60.7615013123,45.4937743964), test loss: 42.4832487583\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.40014004707,5.47055784117), test loss: 3.45869631171\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (40.0584182739,45.1325674096), test loss: 33.12062428\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.39403152466,5.3506116951), test loss: 2.56368875802\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (25.2587757111,44.791228637), test loss: 40.244086504\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.79703330994,5.24223110421), test loss: 3.36874520779\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (73.769203186,44.4464300366), test loss: 34.8512161493\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.94079828262,5.14308472654), test loss: 2.90850028098\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (43.4031906128,44.098330392), test loss: 36.9684152603\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.399923563,5.05163147909), test loss: 3.17351641953\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (24.2774391174,43.7585715057), test loss: 35.9775557995\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.88790178299,4.96636711541), test loss: 3.3905287087\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (44.3030090332,43.410348505), test loss: 34.8244238138\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.05742955208,4.88701078711), test loss: 2.5965355128\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (39.4433021545,43.05243185), test loss: 39.2364182472\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.77470231056,4.81277634399), test loss: 3.53694193959\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (106.069412231,42.7097858481), test loss: 35.0279902697\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.67235541344,4.74283105638), test loss: 2.46019772291\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (16.2425613403,42.3623627566), test loss: 36.6695109367\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.82085037231,4.67657894835), test loss: 3.59131515026\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (24.2896289825,42.0145145949), test loss: 31.2319683313\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.16780209541,4.61502998498), test loss: 2.44366859198\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (34.8555297852,41.6479280886), test loss: 38.149832201\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (6.88738679886,4.55784653411), test loss: 3.3381498754\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (15.7390851974,41.277119743), test loss: 28.5853972912\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (6.7327709198,4.50351356482), test loss: 2.49546912909\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (36.7363739014,40.9070463059), test loss: 34.7894876003\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.94665312767,4.45175339878), test loss: 3.15492987335\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (43.0459251404,40.5226247932), test loss: 29.281741643\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (6.48325395584,4.40190648722), test loss: 3.16614336371\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (14.3396854401,40.1328106196), test loss: 33.9229685545\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.29720830917,4.35401740706), test loss: 3.00556562245\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (23.7543869019,39.7505226274), test loss: 31.7444507599\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (5.76439476013,4.30737597183), test loss: 3.16615469158\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (10.8317117691,39.3722412596), test loss: 31.1099813938\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.99045288563,4.2621908067), test loss: 2.38861252367\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (29.0974369049,38.9920601195), test loss: 33.7516891479\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.04825353622,4.2191930674), test loss: 3.47665103078\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (36.3531799316,38.6067674652), test loss: 26.4938026428\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.05310297012,4.17706195769), test loss: 2.22742115557\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (11.6734008789,38.2270883896), test loss: 34.5958049297\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.87560129166,4.13651653655), test loss: 3.20829780102\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (17.0333766937,37.8528843336), test loss: 26.6350561619\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.79460072517,4.09743126368), test loss: 2.38886604309\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (21.2904624939,37.4762077416), test loss: 33.1465063572\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.25631308556,4.05898516974), test loss: 3.01847013533\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (12.0410385132,37.1064199092), test loss: 23.2342322826\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.63348913193,4.02216377958), test loss: 2.32390903533\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (8.31887626648,36.7480145165), test loss: 31.2886309028\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.79983794689,3.98565005701), test loss: 3.0675539434\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (23.581413269,36.4008763986), test loss: 28.7593182087\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.27036952972,3.95059717796), test loss: 3.23564302474\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (34.4625511169,36.0593354024), test loss: 27.6088224888\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.81046319008,3.91656636746), test loss: 2.52580810785\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (4.41896057129,35.7243509039), test loss: 33.1905047178\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.17132568359,3.88394963175), test loss: 3.34791447818\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.5162010193,35.4015440612), test loss: 29.2754097939\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.25146627426,3.85231282643), test loss: 2.28886262476\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (52.0867424011,35.0881828145), test loss: 33.3331720352\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (4.19051980972,3.8217134653), test loss: 3.18102625906\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (71.0623092651,34.7790965882), test loss: 29.7132379532\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.39089536667,3.79179041471), test loss: 2.40530888885\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (5.67928934097,34.4814017522), test loss: 33.4681007743\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.70588850975,3.76301365597), test loss: 3.18751987517\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (12.2679805756,34.1957327465), test loss: 27.7390939713\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.38317084312,3.73449402393), test loss: 2.44059932232\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (8.13774490356,33.9195438197), test loss: 32.3388150692\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.65088057518,3.70683967585), test loss: 3.15336549282\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (8.95404815674,33.6495840248), test loss: 29.7768323898\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.28080844879,3.68034005619), test loss: 3.15136544257\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (13.7661008835,33.3886737684), test loss: 32.7380235195\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.00621128082,3.6549730477), test loss: 3.10219346583\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (8.96309375763,33.1368715246), test loss: 30.2006912231\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.18342232704,3.63011276122), test loss: 3.2175288856\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (8.47390556335,32.8912726625), test loss: 30.2723377466\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.32125329971,3.60616335193), test loss: 2.38567152321\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (20.1709480286,32.6525062308), test loss: 32.0985497475\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (4.08221769333,3.58280756285), test loss: 3.43351209164\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.9119968414,32.4228151497), test loss: 30.5686621189\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.39914464951,3.56007495969), test loss: 2.39879242778\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (17.8994941711,32.201098977), test loss: 32.7995433092\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.1596364975,3.53763293017), test loss: 3.18180206418\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (16.0768852234,31.9857292291), test loss: 29.1600764275\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.99427342415,3.51594845895), test loss: 2.47092613727\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (50.5311508179,31.7744064659), test loss: 35.2037068844\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.13240885735,3.49483683158), test loss: 3.21055372059\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (25.7777309418,31.5706987098), test loss: 25.761931181\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.59425354004,3.47477502461), test loss: 2.38772720993\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.8970146179,31.3731578029), test loss: 33.693698597\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.91254472733,3.45500979031), test loss: 3.17703099251\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (12.3245820999,31.1792657386), test loss: 29.3373426914\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.514307320118,3.43588106785), test loss: 3.1721965462\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (8.29806518555,30.9909548521), test loss: 33.8751478195\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.773923158646,3.4172554404), test loss: 2.8400332585\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.5950107574,30.8097945241), test loss: 32.2632780075\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.70651340485,3.39898468739), test loss: 3.34002655745\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (19.5940265656,30.6340330337), test loss: 31.3325077534\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (4.40631198883,3.3810399308), test loss: 2.38978413939\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (12.2662496567,30.4626080439), test loss: 34.7285773039\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.779653429985,3.36348051526), test loss: 3.34722430706\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (10.7391681671,30.2915110523), test loss: 29.4694009304\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.53791499138,3.34640107825), test loss: 2.34166538417\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (22.6273002625,30.1289815813), test loss: 35.3158448696\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.96341133118,3.33013927165), test loss: 3.30436078161\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (6.99778366089,29.970876342), test loss: 28.2617980003\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.513403892517,3.31408126944), test loss: 2.55657827556\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (7.26133441925,29.8139258836), test loss: 33.3112443447\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.81310677528,3.2985426165), test loss: 3.09901166558\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (18.1864566803,29.660925563), test loss: 31.1687783241\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.08133339882,3.28318361123), test loss: 2.98516896963\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (13.185546875,29.5146612394), test loss: 33.5431805372\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.53500270844,3.26817019601), test loss: 3.17309751511\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (20.7746925354,29.3712215705), test loss: 30.101117301\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.30017566681,3.25333437023), test loss: 3.26721103191\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (25.6866531372,29.2306155931), test loss: 29.9102030754\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.31768810749,3.23891002404), test loss: 2.54643225223\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (5.52961397171,29.090317577), test loss: 33.5435174465\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.849010586739,3.22475720536), test loss: 3.38888273835\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (13.56665802,28.9561848616), test loss: 31.215913415\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.02078533173,3.21126093339), test loss: 2.36447611153\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (35.7377510071,28.8258418389), test loss: 33.2802349567\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.07570135593,3.19792946852), test loss: 3.1876116991\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (4.9979300499,28.6957117385), test loss: 31.7424209118\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.33745908737,3.18490789581), test loss: 2.43746474385\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (20.554813385,28.5685389361), test loss: 34.0267399788\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.37687563896,3.17203049104), test loss: 3.12329354286\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (17.9527549744,28.4468003716), test loss: 27.6564628601\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.918459773064,3.15944471334), test loss: 2.44302264154\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (13.4422836304,28.3264958531), test loss: 33.6906067133\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.13877916336,3.14689405568), test loss: 3.18980967104\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (14.7007884979,28.2092429264), test loss: 29.036508894\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.72631263733,3.1347975531), test loss: 3.12734918594\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.4958591461,28.0916276102), test loss: 34.6026380062\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.65262162685,3.12287282416), test loss: 3.11821050793\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (32.1301651001,27.9785109318), test loss: 30.8198409796\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.866089344025,3.11137213788), test loss: 3.21848737299\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (5.5114812851,27.8685409241), test loss: 31.8791191101\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.15065956116,3.10010462393), test loss: 2.38877511322\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (15.4523983002,27.7586007273), test loss: 32.662293148\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.971804738045,3.08900407966), test loss: 3.29866266847\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (16.7385616302,27.6506292117), test loss: 29.9172868252\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.00126338005,3.07803875427), test loss: 2.25818795264\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (15.9502010345,27.5467917246), test loss: 34.5668404102\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.82390606403,3.06727243266), test loss: 3.19695156813\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (13.1205816269,27.4449638616), test loss: 28.0415511608\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.13277101517,3.05652666197), test loss: 2.43427166641\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (13.4972629547,27.3442165298), test loss: 34.6779670715\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.88498294353,3.04612741214), test loss: 3.19611479044\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (12.4211902618,27.2444458178), test loss: 25.1705618382\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.06566429138,3.03586864094), test loss: 2.26916328669\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (13.9039812088,27.1470841126), test loss: 33.84854424\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.84112215042,3.02593869485), test loss: 3.10451221764\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (18.0951786041,27.0533721219), test loss: 29.7219884872\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.53274333477,3.01626625568), test loss: 3.14358583391\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (18.1070976257,26.9580893259), test loss: 29.5129546881\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.96097445488,3.00664029791), test loss: 2.6039645493\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (16.8190345764,26.8651824202), test loss: 32.8813281059\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.64555978775,2.99719472821), test loss: 3.32018983364\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (22.9910392761,26.7755806499), test loss: 30.7412820339\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (4.88283252716,2.98775473981), test loss: 2.23913561702\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (5.15228509903,26.6873463893), test loss: 33.7250341654\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.24305486679,2.97842450989), test loss: 3.17231230438\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (18.6232032776,26.5998186736), test loss: 30.1866793633\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (6.2730922699,2.96936301661), test loss: 2.42620488852\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (11.7991504669,26.5130473357), test loss: 34.7251391172\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.714600682259,2.96042472905), test loss: 3.17129175663\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (4.44874668121,26.4286666434), test loss: 28.9154428959\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.806584000587,2.95175550728), test loss: 2.48616277874\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (8.23198890686,26.3464994214), test loss: 33.4376253366\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.760140895844,2.94328118353), test loss: 3.04657088071\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (16.4231929779,26.2634978087), test loss: 31.1125613689\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.76592302322,2.93486646724), test loss: 3.08149895668\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (20.342590332,26.1824037073), test loss: 33.6931393862\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.33353996277,2.92659865568), test loss: 3.07535348535\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (7.52031183243,26.1037341609), test loss: 30.8814587355\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.652799487114,2.91822711112), test loss: 3.15060363412\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (19.0806770325,26.0264212659), test loss: 30.5940587044\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (5.16524124146,2.91005694496), test loss: 2.47512390763\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (40.8527069092,25.949563234), test loss: 32.2378603458\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.83766245842,2.9019705262), test loss: 3.32394258976\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.5376853943,25.8732173879), test loss: 32.3034187317\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.880201280117,2.89420086342), test loss: 2.39898688495\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (18.215669632,25.798791116), test loss: 33.1115519047\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.25050401688,2.8865429466), test loss: 3.10060929954\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (49.2478790283,25.7256994437), test loss: 30.8591059685\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (3.03277254105,2.87902389845), test loss: 2.48408589065\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (7.04726886749,25.6519617076), test loss: 34.4904217958\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.12338149548,2.87153563605), test loss: 3.04927587509\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (10.5656299591,25.5808306728), test loss: 25.9952837467\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.34787321091,2.86423061397), test loss: 2.35229283571\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.17406082153,25.5112183188), test loss: 33.9581213474\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.973281621933,2.85675840675), test loss: 3.09254127741\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (4.63492107391,25.4423777945), test loss: 29.0126028061\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.418196886778,2.84945525187), test loss: 3.12054232955\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (11.9200649261,25.3736865889), test loss: 34.8644562244\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.94305896759,2.84229963955), test loss: 3.08073176742\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (15.92902565,25.3058486127), test loss: 31.9402876139\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.83565664291,2.83535733423), test loss: 3.18569767773\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (14.6195926666,25.2398024698), test loss: 31.1507627487\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.61114668846,2.82849880305), test loss: 2.3857260257\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (30.5282363892,25.1739395872), test loss: 33.50607481\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.81376361847,2.82178534122), test loss: 3.236729303\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (35.0987434387,25.1087303444), test loss: 28.786228323\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.33872127533,2.81512120438), test loss: 2.233111763\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (21.9226875305,25.0450438742), test loss: 34.8446546316\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (5.98152446747,2.8085415185), test loss: 3.18093664646\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (14.1868543625,24.9827568371), test loss: 28.1337912083\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.69459545612,2.80187283547), test loss: 2.46902686507\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (7.26615381241,24.9207464083), test loss: 33.2480406761\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.60638904572,2.7953221426), test loss: 2.99069823772\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (35.4643173218,24.8584811888), test loss: 28.6317412615\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.11056089401,2.78885982999), test loss: 2.76595566273\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (37.9064064026,24.7980595996), test loss: 33.6842102051\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.3724398613,2.78265483887), test loss: 3.08316147029\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (13.9737024307,24.7386089193), test loss: 29.5611868143\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.3706125021,2.77646202747), test loss: 3.06270669699\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (18.6952991486,24.6788092967), test loss: 29.7018641949\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.421732515097,2.77040400175), test loss: 2.47157972306\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (32.3792114258,24.6201756993), test loss: 32.6799134851\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (3.5304236412,2.76440426316), test loss: 3.30514120162\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (5.4040517807,24.5628621939), test loss: 30.2529788017\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.47759747505,2.75840150093), test loss: 2.26449753642\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (8.28681945801,24.506583482), test loss: 33.1752178669\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.03129911423,2.75243388812), test loss: 3.06547751129\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (13.5122890472,24.4507399787), test loss: 30.9776156902\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.52132296562,2.74652449683), test loss: 2.37499023974\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (12.9773197174,24.3935440225), test loss: 33.7826690912\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.378595113754,2.74063412319), test loss: 3.02553534508\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (13.8236551285,24.3388423578), test loss: 27.9691061497\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.43596303463,2.73502864228), test loss: 2.43835540414\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (16.7088928223,24.2854724829), test loss: 33.4913677216\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.555971264839,2.729454484), test loss: 3.02318090647\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (27.6231689453,24.2309209701), test loss: 29.0840323448\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.19578421116,2.72394069272), test loss: 2.9867734611\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.5363240242,24.1773162218), test loss: 34.0490800381\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.77167904377,2.71844962206), test loss: 3.03137412965\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (11.8013505936,24.1256734585), test loss: 30.2993528366\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.964098155499,2.71300060303), test loss: 3.13900561333\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (44.8240699768,24.074257363), test loss: 31.692475605\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (3.58037137985,2.70756903345), test loss: 2.36316637397\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (27.692281723,24.023273143), test loss: 32.2933872938\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.81063985825,2.70222613924), test loss: 3.24392652214\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (14.5638389587,23.971134737), test loss: 30.04785676\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.659945070744,2.69687466437), test loss: 2.20390501916\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (26.9744224548,23.9211359257), test loss: 34.4384589195\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.96595573425,2.69175599362), test loss: 3.12803955674\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (9.10488605499,23.8718320592), test loss: 28.6081732273\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.97575330734,2.68668234028), test loss: 2.38072054088\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (19.8029994965,23.8220361001), test loss: 34.6288056374\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.61031222343,2.68164413924), test loss: 3.01846597493\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (30.4364643097,23.7730822056), test loss: 24.9083338261\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.09904098511,2.67662677263), test loss: 2.20735573024\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (42.9356079102,23.7255087266), test loss: 33.6973150253\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.44790148735,2.67165813966), test loss: 3.00099981427\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (25.8348312378,23.6781337631), test loss: 28.8754130602\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.974132955074,2.66665112488), test loss: 3.072980389\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (43.5698776245,23.6314081682), test loss: 29.9681691647\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.62654852867,2.66179055024), test loss: 2.54906775355\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (20.6002883911,23.5834295439), test loss: 32.3337219238\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.47600376606,2.65691078256), test loss: 3.22214757055\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (12.0745792389,23.5371449853), test loss: 31.512805748\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (3.10834026337,2.65221654827), test loss: 2.33822275102\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (5.43365573883,23.4921798099), test loss: 34.2481516361\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.21990633011,2.64756935818), test loss: 3.14891222119\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.90387153625,23.4459072164), test loss: 29.8049170494\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.419952481985,2.64295645133), test loss: 2.27873494923\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (22.6979141235,23.4009035396), test loss: 34.8327271461\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.278041481972,2.63835789614), test loss: 3.12884063125\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (11.4036741257,23.3567804513), test loss: 27.6789097309\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (4.51975107193,2.63379075822), test loss: 2.4073222965\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (13.8496894836,23.3132444516), test loss: 33.1586567402\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.03391194344,2.62917002263), test loss: 2.93527463377\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (28.7615833282,23.269704188), test loss: 30.5231461525\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.60787320137,2.62468921942), test loss: 2.92436803281\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (22.1767711639,23.2257505146), test loss: 34.0996026039\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.99030065536,2.62020109339), test loss: 3.03214295208\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (9.06967544556,23.1829384544), test loss: 29.6189468384\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.958668231964,2.61585523233), test loss: 3.11520672739\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (38.1712265015,23.1413571941), test loss: 30.8879629612\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.76360559464,2.61160059842), test loss: 2.53337064385\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (17.4248390198,23.0984821099), test loss: 31.9728753567\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.55966925621,2.60732389489), test loss: 3.26198200583\n",
      "\n",
      "MC # 5, Hype # hyp4, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold2/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (254.606445312,inf), test loss: 223.883646393\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (375.161407471,inf), test loss: 396.929171753\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (76.8071060181,130.962807271), test loss: 46.9853678703\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.20043706894,162.508713109), test loss: 3.11955465674\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (133.119873047,89.5211552606), test loss: 42.3475986481\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (8.25169563293,82.7892883629), test loss: 3.1103579998\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (41.2089157104,75.3792582324), test loss: 43.7830591679\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.58631372452,56.2142939439), test loss: 3.09023398459\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (66.1073303223,68.2229166867), test loss: 45.6919085503\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (5.7188539505,42.9267328419), test loss: 3.21744714081\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (42.3193740845,63.9622422267), test loss: 42.1737094879\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.6281080246,34.9586386522), test loss: 2.7351428777\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (87.8498306274,61.0922255066), test loss: 46.3439287186\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (6.52075386047,29.650103125), test loss: 3.46421101093\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (25.8605442047,58.9962393506), test loss: 40.889500761\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.769557833672,25.8575517526), test loss: 2.88385777473\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (51.9389801025,57.4220130854), test loss: 45.79286623\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.80904269218,23.0115475238), test loss: 3.28250613511\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (70.8725738525,56.1827706303), test loss: 39.5131536722\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.58895635605,20.7998447089), test loss: 2.9971568644\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (36.676235199,55.1486440808), test loss: 43.8190683365\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.35279273987,19.0289967375), test loss: 2.97160661817\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (38.1975440979,54.2533689972), test loss: 36.1728007793\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.54146146774,17.5807090796), test loss: 2.89530531168\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (48.6756477356,53.5028781241), test loss: 42.4450870275\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.28942012787,16.3722601611), test loss: 3.1649207741\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (13.650478363,52.8839521987), test loss: 43.082165432\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.55230712891,15.3523374197), test loss: 3.27170909345\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (24.5358581543,52.3442153752), test loss: 39.6367512226\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.47638785839,14.47958285), test loss: 2.86454277933\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (81.9586791992,51.8647794475), test loss: 47.2755395889\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.41120624542,13.7235283302), test loss: 3.52909476757\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (51.7031402588,51.409163481), test loss: 40.4953777313\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.79614877701,13.0619817223), test loss: 2.81414298415\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (24.0135040283,51.002863663), test loss: 43.8654414177\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.01454639435,12.4796197982), test loss: 3.4583732605\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (41.4269447327,50.6249116391), test loss: 37.3249808788\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.72266602516,11.9616179296), test loss: 2.99765362144\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (37.0568962097,50.2447285276), test loss: 44.9961608887\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.77508509159,11.4974713533), test loss: 3.29086503386\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (67.435295105,49.916105705), test loss: 35.9775848389\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.47463607788,11.0796922111), test loss: 2.86919313073\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (45.0332183838,49.5806306526), test loss: 40.6707178593\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.12196278572,10.7016642736), test loss: 3.34774901271\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (23.7988739014,49.2925601866), test loss: 40.263069725\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.66712617874,10.3594178071), test loss: 3.26795791984\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (39.4040565491,49.0018808694), test loss: 42.7071684361\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.911421239376,10.0468205029), test loss: 3.07869950533\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (98.7209777832,48.7212617326), test loss: 43.0140416145\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.71138286591,9.76136818379), test loss: 3.30726832151\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (19.4618358612,48.4390132031), test loss: 39.6083064556\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.18763422966,9.49785983374), test loss: 2.94310494363\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (29.0351486206,48.1646193114), test loss: 43.8783085823\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.69295883179,9.25507489259), test loss: 3.49594221711\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (33.8447418213,47.8801419864), test loss: 34.6579187393\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.43754577637,9.02981349081), test loss: 3.01628800631\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (40.2385940552,47.6133894696), test loss: 42.7138269424\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (8.72423171997,8.82069192106), test loss: 3.29754499197\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (29.8853549957,47.3347357914), test loss: 34.6293818474\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.75034332275,8.62488805154), test loss: 2.91230281889\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (23.7040176392,47.0687075987), test loss: 39.4996075153\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.11417579651,8.44262309599), test loss: 3.19072104692\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (83.4208602905,46.8009813132), test loss: 36.0506118774\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (6.76724672318,8.27246076102), test loss: 3.0104629457\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (24.7647037506,46.5264733571), test loss: 37.6737909794\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.03342890739,8.11286756419), test loss: 3.05315481424\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (43.5703659058,46.247348639), test loss: 39.4856581688\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (0.821309864521,7.96232768789), test loss: 3.33648284078\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (27.5281105042,45.9645919699), test loss: 35.1326169968\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.63004112244,7.82072728904), test loss: 2.70741798282\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (48.088470459,45.6709734529), test loss: 39.6212778091\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.22605538368,7.68623978986), test loss: 3.52035132647\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (14.6971273422,45.3629168505), test loss: 33.1108908653\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.71849012375,7.55877268563), test loss: 2.85700971484\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (34.152469635,45.054483118), test loss: 39.2233031273\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.8894238472,7.43678867148), test loss: 3.2808138907\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (23.586856842,44.7451071754), test loss: 30.682867074\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.84675335884,7.32136766282), test loss: 2.85603095293\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (26.2376670837,44.4299863634), test loss: 35.8441166401\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.731567978859,7.21166981759), test loss: 3.04602802992\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (46.8470344543,44.1059015297), test loss: 27.5705610752\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.50977110863,7.10659521668), test loss: 2.7652231276\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (39.3879776001,43.7692776886), test loss: 33.8793717146\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.949767649174,7.00557163161), test loss: 3.07988579869\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (11.6368503571,43.4301839082), test loss: 32.4012876272\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.96350216866,6.90925752072), test loss: 3.30829865336\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (30.4652957916,43.0801770488), test loss: 29.6994287968\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.75077414513,6.81622406685), test loss: 2.7830748558\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (26.9212970734,42.7186310609), test loss: 36.5126410246\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.99235010147,6.72655182198), test loss: 3.44562657475\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (17.3906021118,42.3603535549), test loss: 29.2777677298\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.46060419083,6.63983740668), test loss: 2.62152369916\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (21.7010211945,41.9945635297), test loss: 33.7177967072\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.624044418335,6.55587523696), test loss: 3.36945177317\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (11.5689640045,41.6324569918), test loss: 26.9046058655\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.858932614326,6.4750722433), test loss: 2.6956505388\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (10.3210859299,41.2660619371), test loss: 34.2158118963\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.18940258026,6.39649325023), test loss: 3.17847115099\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (59.8449707031,40.902864987), test loss: 25.2392208576\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.57077407837,6.32065639414), test loss: 2.65263715386\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.2160663605,40.5410286711), test loss: 31.5232648849\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.740850925446,6.2465144251), test loss: 3.09199026227\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (18.7877235413,40.1803904999), test loss: 29.3695440769\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.02921020985,6.17462085654), test loss: 3.0322368741\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (15.4456558228,39.8226994059), test loss: 31.5309013128\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.991459131241,6.10440377951), test loss: 2.93209484816\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (15.2508621216,39.4757116446), test loss: 30.6499659061\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (6.41465616226,6.03624210659), test loss: 3.01610686481\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (27.0169487,39.135286778), test loss: 28.6057414532\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.1151843071,5.96946552136), test loss: 2.54589518905\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (11.9443187714,38.801028887), test loss: 33.6235718727\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.71635210514,5.90469178336), test loss: 3.1219627738\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (25.4409618378,38.4753848255), test loss: 26.3877496958\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.77007842064,5.84193771454), test loss: 2.5981785506\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (2.89321660995,38.1569063144), test loss: 33.1925588608\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.8936753273,5.7811919326), test loss: 2.96876916289\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (14.7224311829,37.8481509903), test loss: 26.478300333\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.577011346817,5.72206036463), test loss: 2.68379387707\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (20.7715415955,37.5476336846), test loss: 32.2865432978\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.27498531342,5.66466507571), test loss: 2.92825021446\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.9723014832,37.2516726251), test loss: 28.5698448181\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.92960047722,5.60863965303), test loss: 2.74371931255\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (14.9761209488,36.9636654819), test loss: 31.2808417082\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.25797402859,5.55415968443), test loss: 2.87520944923\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (7.93320131302,36.6865508809), test loss: 30.6038971424\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.947598516941,5.5008329988), test loss: 2.96950210333\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (21.4974861145,36.416191847), test loss: 30.1475203991\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.16456365585,5.4492742266), test loss: 2.57967445552\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.1263504028,36.1544819066), test loss: 32.4773777962\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.729556322098,5.39936017681), test loss: 3.0984639585\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (17.2198753357,35.8972640013), test loss: 29.4752553463\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.46170794964,5.35060516015), test loss: 2.52508666515\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (27.5326080322,35.6486593329), test loss: 35.229534483\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.632023990154,5.30341201955), test loss: 3.05805476904\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (8.31936740875,35.407670869), test loss: 26.7704997063\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.86153161526,5.2576730242), test loss: 2.68170300275\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (34.2905158997,35.170399633), test loss: 32.2924658775\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.79444074631,5.2128958948), test loss: 2.87768293619\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (22.9307823181,34.9378744447), test loss: 25.8897477627\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.63032519817,5.16917343743), test loss: 2.66594953835\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (16.2616672516,34.7156478558), test loss: 32.7264275074\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.39981484413,5.12652855265), test loss: 3.04346242547\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (47.4586143494,34.4966818667), test loss: 29.0620311975\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.12906551361,5.08488087274), test loss: 2.87130636275\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (21.4965515137,34.2837439003), test loss: 30.3459517002\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.46829152107,5.04449554202), test loss: 2.86366973519\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (8.80999183655,34.0741821234), test loss: 33.9633246183\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.14855957031,5.0050405406), test loss: 3.19739530683\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.6507453918,33.8716594525), test loss: 30.4737339973\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.62564730644,4.96694916667), test loss: 2.5204110235\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.3723297119,33.6750995089), test loss: 33.0983415127\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.552316665649,4.92970235267), test loss: 3.12083349228\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (18.7579250336,33.4806343848), test loss: 29.5878620148\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.84074985981,4.89333906546), test loss: 2.70425149202\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (5.90710020065,33.2902271652), test loss: 34.1079056501\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.598318576813,4.85776942707), test loss: 3.16253678799\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (8.8670539856,33.1066348001), test loss: 26.6119893551\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.99590873718,4.82291219242), test loss: 2.70043821335\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (32.4954299927,32.9276303824), test loss: 32.7952564478\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.67674636841,4.78884749946), test loss: 3.08372578621\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (5.1317243576,32.7513175873), test loss: 30.1265475273\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.269240021706,4.75555820262), test loss: 2.9524205029\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (10.2424373627,32.5787376042), test loss: 33.6663431644\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.07788825035,4.7231651473), test loss: 3.07082987875\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (5.49100780487,32.4105089654), test loss: 31.054572773\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.02020859718,4.69166876523), test loss: 2.99877829254\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (11.6571836472,32.2464727634), test loss: 30.8111534119\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.11766767502,4.66096010732), test loss: 2.660966748\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (13.9102687836,32.0866458149), test loss: 33.9339675188\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.76168227196,4.63091774961), test loss: 3.08031392992\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (16.5604095459,31.9275020171), test loss: 29.1579123974\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.96949088573,4.60135656243), test loss: 2.69738739133\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (17.6583061218,31.7725200864), test loss: 32.944127965\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.37236881256,4.57246361878), test loss: 2.97604920864\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (7.85600376129,31.6221632976), test loss: 27.3068188667\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.48068344593,4.54395757704), test loss: 2.71624058783\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (25.1022949219,31.4746652745), test loss: 33.9699690104\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.39965128899,4.51621046625), test loss: 2.99899525046\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (13.3833761215,31.3299782209), test loss: 28.3178170204\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.36393260956,4.48918719674), test loss: 2.71000475883\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (24.4380321503,31.1873949957), test loss: 33.652919364\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.04643917084,4.4625588742), test loss: 2.95058934093\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (11.6971025467,31.0477074565), test loss: 30.9580348492\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.380946606398,4.43674580163), test loss: 3.02672481686\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (5.59016609192,30.9127990604), test loss: 30.1604185581\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.979886293411,4.41155447179), test loss: 2.73340733051\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (15.7666625977,30.777038472), test loss: 31.9164867878\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.25814020634,4.38671384654), test loss: 3.14255744815\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (20.1416988373,30.6448743411), test loss: 29.5876655102\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.51716184616,4.36230519632), test loss: 2.5533293575\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (41.277797699,30.5170498536), test loss: 34.4119086981\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.86452198029,4.33833635026), test loss: 3.05522101521\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (41.8750572205,30.390622452), test loss: 26.9003759861\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (4.23594999313,4.31476761291), test loss: 2.72629522979\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (17.7685089111,30.2661117786), test loss: 32.8511803865\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.20935630798,4.29175046627), test loss: 2.87642091811\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (17.1329250336,30.1431533288), test loss: 26.3633847237\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.70708549023,4.26910657239), test loss: 2.69949527085\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (7.98850727081,30.0232029231), test loss: 33.8454359293\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.14557850361,4.24718009672), test loss: 3.08148424029\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (16.0555095673,29.9069445277), test loss: 28.6278728962\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.853838682175,4.22562628693), test loss: 2.93647740781\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (16.8914871216,29.789950274), test loss: 30.8282478333\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.450494945049,4.20441514895), test loss: 2.94771791697\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (30.0180358887,29.6756305207), test loss: 32.6942633152\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (3.53413367271,4.18355658597), test loss: 3.24267962575\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (11.1278038025,29.5641009053), test loss: 30.4365493298\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.31283855438,4.16293584616), test loss: 2.52402544916\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (28.8247013092,29.4551992519), test loss: 33.1426476479\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.4711946249,4.14267763317), test loss: 3.04467970133\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.36960411072,29.3467645502), test loss: 28.935811615\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.472618103027,4.12277923474), test loss: 2.72483042777\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (8.22675609589,29.2402015182), test loss: 34.1873913288\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (3.86037802696,4.10327577288), test loss: 3.13035205901\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.24760627747,29.1353667792), test loss: 26.8824407578\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.779329955578,4.08425358475), test loss: 2.7136377722\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (15.1800498962,29.0330376551), test loss: 33.0475185871\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.35384225845,4.06561007924), test loss: 3.07772920132\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (54.0660743713,28.9324133747), test loss: 30.1676200628\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.45565581322,4.04726346842), test loss: 2.91218271255\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (51.7034339905,28.8318296), test loss: 33.3156921387\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.08754253387,4.02910361504), test loss: 3.08391838819\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (16.1600952148,28.7331256529), test loss: 29.4364634037\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.36646485329,4.01126563604), test loss: 2.98676857054\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (5.96928787231,28.6375107174), test loss: 30.76590662\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.35176420212,3.99352618645), test loss: 2.6329887867\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (8.4191942215,28.5423178599), test loss: 34.7269629002\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.21432805061,3.9761800621), test loss: 3.03969016075\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (24.3829154968,28.4494091614), test loss: 28.2569392204\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.60033321381,3.95924236278), test loss: 2.67821252942\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (16.4187488556,28.3563516291), test loss: 33.8328558922\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.40928602219,3.94241455166), test loss: 2.9481711179\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (16.0212364197,28.2658285475), test loss: 27.2200836658\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.61743271351,3.92606628827), test loss: 2.70348295867\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (6.62121915817,28.1774611006), test loss: 32.7268406391\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.923592090607,3.91002562975), test loss: 2.98825437427\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (13.1884145737,28.088184445), test loss: 27.791959548\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.04716396332,3.89411715373), test loss: 2.67571337521\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (7.82363891602,28.0006583331), test loss: 32.3999750137\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.03482162952,3.87839220161), test loss: 2.93437907249\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (45.7206115723,27.9163092648), test loss: 29.728498888\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.86242961884,3.86286998767), test loss: 2.99863152355\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (8.80662727356,27.8315374291), test loss: 30.4853331566\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.30141401291,3.84753538209), test loss: 2.72221981585\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (16.6519699097,27.7488925366), test loss: 32.1832011461\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.33090949059,3.83251715952), test loss: 3.06234765649\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (16.4739494324,27.6660115599), test loss: 30.0741179466\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.21994721889,3.81764575902), test loss: 2.5138496846\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (8.21037769318,27.5852854561), test loss: 34.5455211639\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.41307449341,3.80323490689), test loss: 3.01295872331\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (15.0190105438,27.5064447518), test loss: 26.7932845592\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.741689682007,3.78900751662), test loss: 2.66509672403\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (22.1748352051,27.4270223188), test loss: 32.154209137\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.426722228527,3.77494580614), test loss: 2.84088629484\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (38.8643264771,27.3489638921), test loss: 26.3066121578\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (5.01628780365,3.76105805397), test loss: 2.70074195862\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.88949203491,27.2727783382), test loss: 33.1450716496\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (3.83193778992,3.74723492364), test loss: 3.03152480721\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (38.0103340149,27.1975917572), test loss: 28.6852835417\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.51235795021,3.73364359446), test loss: 2.81088708639\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (26.9882545471,27.1231981841), test loss: 31.2902369022\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.80587053299,3.72023761196), test loss: 2.93496384621\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (8.59163475037,27.0490330876), test loss: 32.8249659061\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.34723520279,3.7070222771), test loss: 3.13762038946\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (27.8664398193,26.9762717258), test loss: 31.0279690742\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.51828277111,3.69412022206), test loss: 2.46147256196\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (17.5627784729,26.904710617), test loss: 33.2905910969\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (3.96872138977,3.68143867131), test loss: 3.03843407035\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (50.2483901978,26.8344778457), test loss: 30.2241720915\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.96880626678,3.66891515481), test loss: 2.68369860351\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (42.3492660522,26.7635198073), test loss: 34.2753754139\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.17327427864,3.65645915973), test loss: 3.09283282459\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (12.069196701,26.6941625693), test loss: 26.4225598335\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.24509680271,3.64417546961), test loss: 2.71113996208\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (7.17618560791,26.626450926), test loss: 32.6799144506\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.11131536961,3.63192155672), test loss: 3.00423222482\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (5.19176673889,26.559063536), test loss: 29.6100804806\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.992039680481,3.61991475799), test loss: 2.83071854711\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (27.8235530853,26.492661185), test loss: 33.2103368759\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.72714591026,3.6081476753), test loss: 3.02133516371\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (14.0210533142,26.4261724888), test loss: 30.0349258423\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.74195885658,3.59639918681), test loss: 2.94015128911\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (16.2003383636,26.3611482575), test loss: 31.2057784557\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.02068090439,3.58499333663), test loss: 2.60473537445\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (15.6941432953,26.2979243646), test loss: 33.7981822968\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.30551147461,3.57375804568), test loss: 3.05043379068\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (9.32919502258,26.2331583268), test loss: 28.9635602474\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.30683773756,3.56256995502), test loss: 2.63493484408\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (16.498336792,26.1702151033), test loss: 32.7491380453\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.15928840637,3.55148167478), test loss: 2.92364564836\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (18.8998184204,26.1085630447), test loss: 27.3894028664\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.485657066107,3.54047580621), test loss: 2.68572956324\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (10.7367687225,26.0473670314), test loss: 32.9135030746\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.7726341486,3.5295859162), test loss: 2.92556761205\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (21.0626525879,25.986785304), test loss: 27.0423404694\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.32916724682,3.51890565624), test loss: 2.6101223588\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (9.72480010986,25.9261163276), test loss: 33.5467877388\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.30014014244,3.50829303051), test loss: 2.88704469353\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (7.25577831268,25.866826576), test loss: 30.2413606405\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.29517161846,3.49798486068), test loss: 2.97961563915\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (30.4372596741,25.8090118821), test loss: 30.8339431763\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.674814760685,3.48779301018), test loss: 2.74211110175\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (18.4366168976,25.7501459226), test loss: 31.2431759357\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.36032891273,3.47768815355), test loss: 3.06719512343\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (17.1892299652,25.6924100457), test loss: 29.9183448315\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.05055212975,3.46763869168), test loss: 2.48887438178\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.76086711884,25.6359869205), test loss: 34.0653030396\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.41965532303,3.45765184066), test loss: 2.9691441834\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (35.077255249,25.5802148267), test loss: 26.6037378311\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.84043383598,3.44781017701), test loss: 2.61501177251\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (36.5587463379,25.5245860429), test loss: 32.0414821625\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.92762637138,3.4380703656), test loss: 2.78378928602\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (23.8869419098,25.4692020721), test loss: 26.2305664539\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.80329942703,3.42840396982), test loss: 2.66228668988\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (23.4073123932,25.4144420267), test loss: 34.0171693325\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.94715297222,3.419012567), test loss: 2.98330444992\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (14.1700153351,25.3607600722), test loss: 28.4772723913\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.73638224602,3.40971475612), test loss: 2.79343403578\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (14.0115690231,25.3072776678), test loss: 31.673231554\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.63976943493,3.40053790142), test loss: 2.93138350546\n",
      "\n",
      "MC # 5, Hype # hyp4, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold3/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (326.189117432,inf), test loss: 169.731838226\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (260.067443848,inf), test loss: 321.934211731\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (72.5440750122,95.6220876541), test loss: 47.8259185791\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (6.26899719238,52.3902573493), test loss: 3.65916224122\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (43.3370437622,70.6348856125), test loss: 37.5275728226\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.46705091,28.0889940037), test loss: 3.49995900393\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (36.6897125244,62.218065162), test loss: 45.1515265465\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (6.20760345459,19.9823598042), test loss: 3.70296228528\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (119.18661499,57.8599648538), test loss: 39.2476164818\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.47926950455,15.9169592089), test loss: 3.74176999927\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (29.3218460083,55.2901329788), test loss: 40.5998399973\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.65556144714,13.4779709267), test loss: 3.13740484118\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (28.2370033264,53.4956075283), test loss: 43.0245548248\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.77387094498,11.8496055887), test loss: 3.86770300269\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (63.3622741699,52.2039420897), test loss: 41.8246376991\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.6158323288,10.6881444982), test loss: 3.00105364323\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (34.6600036621,51.1918485608), test loss: 43.822015667\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.10815751553,9.80878799312), test loss: 3.73164027333\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (33.8343963623,50.3726673107), test loss: 36.1760846615\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.76140242815,9.1269210583), test loss: 3.08764743209\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (40.2080078125,49.667344401), test loss: 46.240114212\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.44329547882,8.57793921874), test loss: 3.53802403808\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (91.1326293945,49.1348641148), test loss: 33.6299778461\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (6.63363409042,8.12723268828), test loss: 2.72592023015\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (19.6213665009,48.6164403108), test loss: 44.287834549\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.43416595459,7.74845106111), test loss: 3.6973657012\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (80.9628982544,48.2192464454), test loss: 35.5052066326\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.42627477646,7.42870135526), test loss: 3.2944354713\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (17.9123592377,47.8224421775), test loss: 44.5316253901\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.9999102354,7.1524655942), test loss: 3.53280155957\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (29.9034099579,47.4687790187), test loss: 39.2983119965\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.84164333344,6.91313596586), test loss: 3.62573949099\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (39.8308677673,47.1403413537), test loss: 39.9762904167\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.68514513969,6.69957627103), test loss: 3.01097100377\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (38.5501251221,46.8164214703), test loss: 42.1097471237\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.65770876408,6.51095615652), test loss: 3.68303971291\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (27.5881652832,46.4847168611), test loss: 36.9525355339\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.67615890503,6.34089956802), test loss: 2.86997413635\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (27.8776874542,46.1924726882), test loss: 43.5582228661\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.4401422739,6.18692626497), test loss: 3.55155871809\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (42.2052230835,45.8776914036), test loss: 32.7462271214\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (5.57159137726,6.04426016529), test loss: 2.70484073162\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (16.3867092133,45.579053543), test loss: 42.9288474083\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (6.08159637451,5.91386336384), test loss: 3.51602532864\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (39.075012207,45.2807686688), test loss: 27.9247070551\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.776836395264,5.79300939941), test loss: 2.63615984321\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (41.4383735657,44.9761335613), test loss: 41.4791535378\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.92915034294,5.68135444214), test loss: 3.40034937859\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (23.0643749237,44.6736494303), test loss: 34.181029129\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.29789853096,5.5764884391), test loss: 3.43111483455\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (15.8592176437,44.3643065988), test loss: 34.3892637253\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.50525593758,5.47842392598), test loss: 2.84182860553\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (40.2046279907,44.0399159532), test loss: 39.4163624763\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.80908739567,5.38586822835), test loss: 3.48064351976\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (134.321273804,43.7235068949), test loss: 34.840194416\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (4.0282907486,5.2980355518), test loss: 2.50245185494\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (23.7159156799,43.3984691036), test loss: 37.8696127415\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.80662512779,5.21458697013), test loss: 3.52759613395\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (32.1045761108,43.0793080892), test loss: 29.354109621\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.21828317642,5.13622118484), test loss: 2.46714678109\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (39.1882209778,42.746943405), test loss: 39.2223714352\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.58662366867,5.06137729153), test loss: 3.26008722782\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (31.250869751,42.4096062874), test loss: 27.111665225\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (0.874849617481,4.98996840383), test loss: 2.54738733768\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (19.1311035156,42.0698756137), test loss: 36.2842425346\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.38443422318,4.92147396014), test loss: 3.16822095811\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (12.6384840012,41.7181155405), test loss: 28.2320174456\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.27298343182,4.85584135686), test loss: 3.11706157327\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (17.706741333,41.3561295481), test loss: 34.8792547226\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.75078248978,4.7925876343), test loss: 3.20054115653\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (12.629491806,40.9995845558), test loss: 30.5328203678\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (5.2620010376,4.73157204028), test loss: 3.36458672881\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (27.1249160767,40.6454989557), test loss: 30.2543745995\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.48842287064,4.67281395698), test loss: 2.43761410117\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (11.8431034088,40.2897960308), test loss: 33.147685051\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.46105027199,4.61697731636), test loss: 3.51175855398\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (22.6783161163,39.9294099092), test loss: 28.9270763397\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.21544027328,4.56296529359), test loss: 2.43779919595\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (16.3598098755,39.5702650378), test loss: 35.1837652683\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (5.79775381088,4.51150884903), test loss: 3.35995384306\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (29.7536048889,39.2142267772), test loss: 24.6911834478\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.51325821877,4.46185929529), test loss: 2.57614549696\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (46.83568573,38.8533257187), test loss: 34.4490744352\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (4.39928674698,4.41401396818), test loss: 3.25996247828\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (27.3179321289,38.4922523957), test loss: 21.7734755993\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.49085354805,4.36774390095), test loss: 2.45907902718\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (28.8141880035,38.1433181291), test loss: 33.0736001015\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.72305345535,4.32294496378), test loss: 3.28837755919\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (16.9739151001,37.8017695392), test loss: 26.1951626301\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.50442051888,4.28000097006), test loss: 3.3397080183\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (21.3872661591,37.4660772829), test loss: 27.8468708038\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (5.16815805435,4.2388912154), test loss: 2.75136902183\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (18.5674781799,37.1366171691), test loss: 30.0361761093\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.33692407608,4.1992310876), test loss: 3.36212157309\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (12.419962883,36.8164638787), test loss: 27.1701333046\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.33165597916,4.16130299735), test loss: 2.500255768\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.4688434601,36.5054036973), test loss: 32.4508025169\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.960790395737,4.12477682842), test loss: 3.40025901198\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (25.1006832123,36.1974842684), test loss: 25.3858188629\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.63160562515,4.08941246761), test loss: 2.50522382259\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (22.1208229065,35.8977223719), test loss: 32.9526939869\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.57789492607,4.05523642736), test loss: 3.34388562739\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (8.82796669006,35.6108210555), test loss: 24.2524573803\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.14113724232,4.02176530611), test loss: 2.74520022869\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (18.895904541,35.3331927259), test loss: 32.1627405167\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (8.67623615265,3.98982802256), test loss: 3.19137686193\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (28.2277107239,35.0620470231), test loss: 27.9232876301\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.30473041534,3.95881423603), test loss: 3.31596435308\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (5.88345050812,34.7988190077), test loss: 32.5542440176\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.74113130569,3.92912186773), test loss: 3.41321189702\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (14.6596927643,34.5450372946), test loss: 27.7698806286\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.760548591614,3.90046165814), test loss: 3.47654595971\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (55.7498474121,34.2991117948), test loss: 27.9276409149\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.89856004715,3.87283280384), test loss: 2.74117644429\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (83.5579833984,34.0551854625), test loss: 30.3535612106\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.99446308613,3.84574731124), test loss: 3.55088884234\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.9800758362,33.8190531866), test loss: 28.6210146427\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.37545585632,3.81959842272), test loss: 2.4997362107\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (9.36596870422,33.5936317356), test loss: 33.5156584263\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.7552523613,3.79369582189), test loss: 3.41629489064\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.4028091431,33.3738806102), test loss: 26.1959260464\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.389141976833,3.76883704796), test loss: 2.68872904629\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (9.55709266663,33.1587167858), test loss: 34.0512447357\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.86446094513,3.7448757042), test loss: 3.33673628867\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (17.8286209106,32.9510153246), test loss: 23.386700201\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.65041875839,3.72180964209), test loss: 2.53404439688\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (18.9705867767,32.7497207257), test loss: 32.9420140028\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.29127979279,3.69936754485), test loss: 3.3492159456\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (22.2515029907,32.5520824176), test loss: 27.9411423683\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.77639520168,3.6775614534), test loss: 3.3759506762\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (20.6424636841,32.3590113402), test loss: 33.7730541468\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (4.2463054657,3.65632328873), test loss: 3.42617364228\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (19.8943862915,32.1726908753), test loss: 28.6136700153\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (5.87943220139,3.63554956154), test loss: 3.47239070982\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (19.7126293182,31.9931994422), test loss: 29.1447144508\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.57611632347,3.61500344076), test loss: 2.57648277432\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (29.6709938049,31.8174197083), test loss: 31.5613688469\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.63121128082,3.59525703617), test loss: 3.46979027689\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (37.4007263184,31.6441645535), test loss: 27.3928742409\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.00551652908,3.57587285487), test loss: 2.39887729138\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (15.4452400208,31.4772406889), test loss: 34.6458524227\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.29013729095,3.55743685332), test loss: 3.50369024277\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (19.5920619965,31.3150859727), test loss: 26.1049886942\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.76441478729,3.53932989569), test loss: 2.71631169915\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (48.9409408569,31.1550729091), test loss: 31.9744299889\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.56468343735,3.52173939012), test loss: 3.24245998859\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (17.6076698303,30.997910655), test loss: 23.888569355\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.637461543083,3.50445713717), test loss: 2.95458962619\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (12.2736597061,30.8471181259), test loss: 33.7312683821\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.28685593605,3.48749088789), test loss: 3.38212096542\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (25.4530925751,30.7006339036), test loss: 28.5728228569\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.36006689072,3.47072359489), test loss: 3.40926490128\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (5.54888296127,30.5565665522), test loss: 28.3293888092\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.61212468147,3.45454256191), test loss: 2.80036760792\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (17.9873485565,30.4137168115), test loss: 31.4986165762\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.62731170654,3.4386347225), test loss: 3.52598395944\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (40.6897773743,30.2772051317), test loss: 28.1423161745\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.41849136353,3.42344366544), test loss: 2.3719599396\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.49847126007,30.1437513089), test loss: 32.5436393023\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.318278670311,3.40843208754), test loss: 3.33002786785\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (20.4234027863,30.01035852), test loss: 29.0252354145\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.520187079906,3.39381044682), test loss: 2.64433907121\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (33.6354942322,29.8802705683), test loss: 33.0559563637\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (4.23832988739,3.37945553322), test loss: 3.26501365677\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (15.6998062134,29.7558009487), test loss: 24.5318834782\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.77270960808,3.36527594554), test loss: 2.645913288\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (4.7136425972,29.6327119694), test loss: 33.7280160189\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.50087857246,3.35129134326), test loss: 3.26871819198\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (26.9205722809,29.5129936512), test loss: 27.9485497236\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.91055679321,3.3377337261), test loss: 3.2870131731\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (7.46301555634,29.3929850093), test loss: 34.0472326279\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.951649606228,3.32432011776), test loss: 3.36869067252\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (11.0218906403,29.277635934), test loss: 29.0699047089\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.71993660927,3.31152025112), test loss: 3.49498505294\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (19.7367782593,29.1659237461), test loss: 29.6384554863\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.845274031162,3.2989151494), test loss: 2.59504705667\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (29.3838329315,29.0531808013), test loss: 31.006509161\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.11265206337,3.2865102706), test loss: 3.47962033153\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (19.1710186005,28.9426270432), test loss: 29.4371508837\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.0062558651,3.27424720474), test loss: 2.49069022536\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (20.4005966187,28.8374406631), test loss: 34.4159049034\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.34614944458,3.2622346432), test loss: 3.43923462331\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (46.5991668701,28.733096589), test loss: 25.8079172373\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (4.19620227814,3.250304105), test loss: 2.62948931605\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (34.3888092041,28.630597688), test loss: 33.9400259495\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.35823369026,3.23875714553), test loss: 3.33198607564\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (6.23744249344,28.5285778992), test loss: 23.299718523\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.14005255699,3.22727596493), test loss: 2.51547280848\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (13.7808151245,28.4298035767), test loss: 33.6976690769\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.06151461601,3.21630157039), test loss: 3.33840103745\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.5569953918,28.3335246376), test loss: 28.1265411854\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.11075043678,3.20549368161), test loss: 3.40012924671\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (6.9997882843,28.2366332106), test loss: 28.9371915579\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.95147919655,3.19480421646), test loss: 2.76423041523\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (28.2475967407,28.1414369338), test loss: 29.8677764893\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.80355036259,3.18422470438), test loss: 3.42684821486\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (47.6141014099,28.050557094), test loss: 28.9104676962\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.23416948318,3.17382840894), test loss: 2.49992928356\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (23.1482963562,27.9604189685), test loss: 32.6204147816\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.22605752945,3.16344688946), test loss: 3.36250262856\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (54.9507293701,27.8718219751), test loss: 27.4723908901\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (3.64824533463,3.15345742229), test loss: 2.53443178385\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.87167167664,27.7833147906), test loss: 33.5827070236\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.64222562313,3.14349693805), test loss: 3.3064237237\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (26.5898284912,27.6978119497), test loss: 25.2839289188\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.578630387783,3.1339109797), test loss: 2.72523825467\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (10.6633834839,27.6141934455), test loss: 32.2641304493\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.12916612625,3.12452758107), test loss: 3.14443049133\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (4.96571397781,27.5294452565), test loss: 29.0817349911\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.634700238705,3.11516801365), test loss: 3.25306949019\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (23.644859314,27.446262448), test loss: 33.4517170191\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.61790037155,3.10594520577), test loss: 3.36210296303\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (10.773434639,27.3663836333), test loss: 28.6611796379\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.9162311554,3.09680506494), test loss: 3.43775714636\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.31540298462,27.2876984255), test loss: 29.5474952698\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.709141016006,3.08768263171), test loss: 2.69008612931\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (12.3253326416,27.2095515245), test loss: 30.1202896118\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.59263193607,3.07890250254), test loss: 3.47146776319\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (9.3708782196,27.1323291638), test loss: 29.9465669632\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.62965917587,3.0701389566), test loss: 2.49137324691\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (15.3718700409,27.0569246687), test loss: 33.4954371214\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.83162999153,3.06171826873), test loss: 3.35310213566\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (14.1420879364,26.9836587674), test loss: 27.1618312836\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.17162013054,3.05342605084), test loss: 2.6553104341\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (24.921257019,26.9087546041), test loss: 34.0338290453\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (3.86161565781,3.04518432606), test loss: 3.29254397899\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (5.41125965118,26.8349768504), test loss: 23.5911144018\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.15344703197,3.03696878682), test loss: 2.47998444736\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (25.8297767639,26.7644251632), test loss: 33.4165385008\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (5.22342634201,3.02880053384), test loss: 3.23012864441\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (8.78631782532,26.6944796142), test loss: 28.0636612177\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.18420314789,3.02076499368), test loss: 3.35285396278\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (18.9700622559,26.6249733223), test loss: 34.1336822987\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.6856572628,3.01292827009), test loss: 3.33566140831\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (27.3394737244,26.5565604667), test loss: 28.8782596111\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.12955212593,3.00517118129), test loss: 3.34426627755\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (16.2716178894,26.4892685099), test loss: 29.5312089443\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.22031235695,2.99761773085), test loss: 2.53004549742\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (29.2178020477,26.4241236997), test loss: 31.8946312904\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.73063063622,2.99025610653), test loss: 3.38327418566\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (15.2320671082,26.356984447), test loss: 28.0699777603\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.69531965256,2.98284711447), test loss: 2.38764196336\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (19.0913906097,26.2913064255), test loss: 34.197476244\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.81828057766,2.97553167279), test loss: 3.3737906456\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (7.32300376892,26.2281482785), test loss: 25.6981548309\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.43811154366,2.9681668246), test loss: 2.59093109667\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (14.1877412796,26.1655954894), test loss: 32.1475142479\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (6.23523044586,2.96102637896), test loss: 3.11045876145\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (9.51918506622,26.1030525398), test loss: 25.5775301218\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.76367044449,2.95390558109), test loss: 3.01476905942\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (6.93414449692,26.0414715441), test loss: 34.0700005293\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.46265053749,2.94699745343), test loss: 3.29534323215\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (11.50963974,25.9812333383), test loss: 27.882875824\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.03945636749,2.94020935747), test loss: 3.24491210282\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (50.1412086487,25.9222723049), test loss: 28.9183663845\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.64187550545,2.93356005795), test loss: 2.72343578637\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (33.8205490112,25.8620362147), test loss: 30.6030519724\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.64201116562,2.92689431071), test loss: 3.42514930069\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (8.81254386902,25.8027471807), test loss: 28.5822040081\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.97159099579,2.92031331341), test loss: 2.33505696058\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (9.74152755737,25.7459379797), test loss: 32.1950771332\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.48724079132,2.91363988055), test loss: 3.2167153798\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (13.6513633728,25.6893699287), test loss: 28.3685253382\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.33963906765,2.90713011373), test loss: 2.5782120198\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (15.0939674377,25.632570566), test loss: 32.6271670938\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.40009450912,2.90075229359), test loss: 3.15447500497\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (26.3060894012,25.5770534352), test loss: 24.043693924\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (3.37015509605,2.89453015725), test loss: 2.55743835866\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (17.5968475342,25.5224511243), test loss: 32.9892775059\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (4.15416622162,2.88839383399), test loss: 3.19860175848\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (55.6831512451,25.4682671714), test loss: 27.8373080254\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.8342730999,2.88232091834), test loss: 3.15014625788\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (6.25605583191,25.4135897119), test loss: 33.7655531406\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.67831873894,2.87630017516), test loss: 3.28703352213\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (15.2658433914,25.3604031457), test loss: 28.8323535204\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.52068579197,2.87031071045), test loss: 3.36780596972\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (10.2601394653,25.308754831), test loss: 29.9923033953\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.82997643948,2.86424534744), test loss: 2.50037918687\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (32.2874450684,25.257255021), test loss: 30.6595907688\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.56686210632,2.85837641701), test loss: 3.35359195471\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (47.3261947632,25.2053693621), test loss: 29.27873528\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.87567186356,2.8525027002), test loss: 2.42718963623\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (10.6241149902,25.1545074293), test loss: 34.0725021839\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.05006599426,2.8468468784), test loss: 3.39702664018\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (17.3565979004,25.1048902264), test loss: 25.6148245335\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.10307502747,2.84123527784), test loss: 2.57334677726\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (36.1348762512,25.0547508021), test loss: 33.4581209183\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.94149637222,2.83571620523), test loss: 3.22418142855\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (20.2403030396,25.0050638539), test loss: 23.1291258335\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.40009665489,2.83022392879), test loss: 2.44484878778\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (11.1448154449,24.9567118344), test loss: 33.3730168343\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.56719565392,2.82472206144), test loss: 3.2363302052\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (21.4605560303,24.9092961513), test loss: 27.5179979324\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.87815523148,2.81921283268), test loss: 3.2615087986\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (11.6874065399,24.8618287042), test loss: 28.7678021193\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.43503212929,2.81382132439), test loss: 2.69033782482\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (31.6219043732,24.8139909676), test loss: 29.3959774017\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.99101996422,2.80845756851), test loss: 3.34036716819\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (40.1725387573,24.767816397), test loss: 29.029207468\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.34441995621,2.80330640559), test loss: 2.47557455525\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (9.15102958679,24.7221164881), test loss: 32.3084214687\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.616213142872,2.79813342747), test loss: 3.28715442419\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (11.9884576797,24.6755690651), test loss: 28.4489497185\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.191404983401,2.79305569756), test loss: 2.49233482778\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (44.5058135986,24.6302348413), test loss: 32.8857709408\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (6.25194454193,2.78805029277), test loss: 3.24270639718\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (9.55538368225,24.5858519553), test loss: 24.3412408352\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.38169527054,2.78293894183), test loss: 2.60150714517\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (6.80943584442,24.5417521548), test loss: 32.8110015392\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.39770364761,2.77791812312), test loss: 3.03820281923\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (20.2396507263,24.4983004763), test loss: 27.7473930359\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.32384622097,2.77296501173), test loss: 3.08744632006\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (6.3500161171,24.4538537324), test loss: 33.3113819122\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.849858760834,2.767999485), test loss: 3.27550673783\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (15.4151706696,24.4111670475), test loss: 28.1502263069\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.6228518486,2.76326556203), test loss: 3.33755380809\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (22.8386554718,24.3693124068), test loss: 29.4566421509\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.00549554825,2.75852371644), test loss: 2.71454819292\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (18.5140838623,24.3261010045), test loss: 29.8687553883\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.54076659679,2.75383337073), test loss: 3.40026793182\n",
      "\n",
      "MC # 5, Hype # hyp4, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold4/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (344.979309082,inf), test loss: 190.453663254\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (264.570800781,inf), test loss: 322.298353577\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (28.2630290985,104.705468228), test loss: 45.2182923317\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.63129520416,51.7731354665), test loss: 3.41515132189\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (57.1990203857,75.789158155), test loss: 37.3792463779\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (0.985173046589,27.359853656), test loss: 3.39148535728\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (14.677614212,66.0854488586), test loss: 44.1121547699\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.39773082733,19.2151605757), test loss: 3.46068656445\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (58.8507461548,61.10962901), test loss: 40.5124312878\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (5.30289363861,15.1379823748), test loss: 3.56036553383\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (24.0819587708,58.1584430437), test loss: 42.6094290257\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.89769887924,12.6898284688), test loss: 2.82751480937\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (95.8916931152,56.1435542103), test loss: 43.2910273075\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.94001340866,11.0580950443), test loss: 3.62610993385\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (11.4475507736,54.6641086807), test loss: 39.0234239101\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.959002137184,9.89522249866), test loss: 2.70266278386\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (20.6903495789,53.527355428), test loss: 42.3610473633\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.44650626183,9.01860397876), test loss: 3.4066908747\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (33.2736282349,52.6007217926), test loss: 36.9038246632\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.35386991501,8.3367469703), test loss: 2.94757317305\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (120.280548096,51.8406801374), test loss: 43.6691875458\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (7.94587612152,7.79165932972), test loss: 3.19472893775\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (33.0823898315,51.209135092), test loss: 31.944834137\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.10542917252,7.34382202459), test loss: 2.646325773\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (24.5050392151,50.6470433761), test loss: 41.9211428165\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.1079044342,6.96798815675), test loss: 3.33853138685\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (40.7032852173,50.1859954909), test loss: 36.8051445961\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.40787708759,6.64984505423), test loss: 3.41608951837\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (11.3340854645,49.7627662441), test loss: 38.5865036488\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.22375917435,6.37675012636), test loss: 2.93820139766\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (36.365032196,49.3792476022), test loss: 43.1766424656\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.45967650414,6.14079342703), test loss: 3.4976983577\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (45.6596069336,49.0268465043), test loss: 40.0169228554\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.48304867744,5.93128260051), test loss: 2.67909879088\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (111.609680176,48.6998933911), test loss: 40.7659418106\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (6.86098766327,5.74659584014), test loss: 3.4069271028\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (159.8931427,48.3817787315), test loss: 36.1677295685\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.44417476654,5.58032339631), test loss: 2.76858322024\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (23.2807388306,48.0741157522), test loss: 43.229723978\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.23012876511,5.43040390859), test loss: 3.42184434533\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (43.1241493225,47.7831814154), test loss: 35.083435297\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.89229798317,5.29297447597), test loss: 2.76723228097\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (37.5800971985,47.5228938259), test loss: 40.7069769382\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.82463359833,5.16848083499), test loss: 3.25326091647\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (94.4782409668,47.2694840053), test loss: 34.2586599827\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.21674919128,5.05415462733), test loss: 3.11863619685\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (23.4922409058,47.007177779), test loss: 41.1317655087\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.2160487175,4.9497872008), test loss: 3.2704621613\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (48.2783241272,46.7571530617), test loss: 37.055310154\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.79422903061,4.85194864998), test loss: 3.41426734924\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (51.0875549316,46.5063103366), test loss: 38.6148522377\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.59315013885,4.76186457838), test loss: 2.60669310987\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (83.2913818359,46.2478771104), test loss: 39.510559082\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.56949710846,4.67737404054), test loss: 3.4299844265\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (32.0823822021,45.9821383102), test loss: 34.6134423256\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.34555792809,4.59814972339), test loss: 2.62930104136\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (19.2981357574,45.7187181703), test loss: 40.5501123905\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.48872971535,4.52253995082), test loss: 3.33814160526\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (39.0200653076,45.4691735694), test loss: 32.7499203205\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.51167750359,4.45201906684), test loss: 2.67228652984\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (41.7689781189,45.2165652992), test loss: 39.1473986626\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (7.52865695953,4.38536200354), test loss: 3.18045038581\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (40.6807556152,44.9486818685), test loss: 27.3869831085\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (0.926528036594,4.32200369273), test loss: 2.6094012484\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (17.2541503906,44.6833328316), test loss: 37.8693095207\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.0369066,4.26156434569), test loss: 3.20964620709\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (23.4479293823,44.4137381396), test loss: 32.2900046349\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.65974736214,4.20451539328), test loss: 3.28411349654\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (20.1126346588,44.1325609772), test loss: 33.614653635\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.03552222252,4.14953849788), test loss: 2.82762436122\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (20.2305107117,43.8415971912), test loss: 36.9833671093\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.22624921799,4.0968209663), test loss: 3.4151204437\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (28.8536834717,43.5488254523), test loss: 33.7169726849\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (5.83901405334,4.04602895635), test loss: 2.48721399307\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (29.1469974518,43.2583396285), test loss: 34.2096695423\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.36671578884,3.99702195966), test loss: 3.30479208231\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (16.5912456512,42.961992777), test loss: 30.8245655537\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.45821881294,3.95067477542), test loss: 2.66307263076\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (37.7271003723,42.6522044959), test loss: 36.5236329794\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.9992916584,3.90598817991), test loss: 3.30253722072\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (10.7688770294,42.340849404), test loss: 27.0285178185\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.20472383499,3.8631053691), test loss: 2.64217562377\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (18.0898780823,42.0268255756), test loss: 33.885567522\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.00773239136,3.82202361067), test loss: 3.21015859842\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (12.0009174347,41.7015837699), test loss: 27.386738801\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.14753496647,3.7822502495), test loss: 3.26592039466\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (11.8484525681,41.3702543351), test loss: 34.1738264561\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.05632400513,3.74392821867), test loss: 3.3424580127\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (26.3784255981,41.0419933788), test loss: 29.4277586937\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.805658102036,3.70684307529), test loss: 3.38242956847\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (7.9524435997,40.7147636118), test loss: 31.2961579323\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.784820079803,3.67088860925), test loss: 2.61496834159\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (35.0423622131,40.3883142477), test loss: 32.1554751873\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.48427677155,3.63677110425), test loss: 3.40686155558\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (28.9499931335,40.0552666827), test loss: 27.7774036407\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.4884762764,3.60373367295), test loss: 2.65916117132\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (33.5898132324,39.7257220406), test loss: 33.5073513508\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.08614373207,3.57211091008), test loss: 3.39890768528\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (14.3194561005,39.4003552278), test loss: 27.3469739914\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.30858182907,3.54180362748), test loss: 2.81863467991\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (8.91429519653,39.0731509107), test loss: 32.7429001331\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.37954854965,3.51240632677), test loss: 3.21373186707\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (29.5260047913,38.7466273137), test loss: 27.3965005875\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.18651890755,3.48381709873), test loss: 3.19018499851\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (42.2305908203,38.432833465), test loss: 33.5561506271\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.27214932442,3.45624320283), test loss: 3.46567050219\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (14.2324314117,38.1188998556), test loss: 27.3638373137\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.44225764275,3.42929841726), test loss: 3.49052299261\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (12.1131515503,37.8139706256), test loss: 29.66369977\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.21828091145,3.40356023708), test loss: 2.91158863157\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (13.1773166656,37.5112180079), test loss: 31.3464222908\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.813976407051,3.37860463115), test loss: 3.55703976154\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (13.0038099289,37.217615297), test loss: 31.1188815832\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.45635890961,3.35487823695), test loss: 2.65494229794\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (28.4504795074,36.9322632979), test loss: 30.6183459997\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.16206204891,3.33182573128), test loss: 3.36888490617\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (29.0747642517,36.6491057226), test loss: 30.2744015217\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.21213781834,3.30934848356), test loss: 2.91042591333\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (12.7961654663,36.3706432178), test loss: 34.4753243685\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.32006192207,3.28742532806), test loss: 3.42148164511\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (29.6673793793,36.106160661), test loss: 26.0497508049\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.50879669189,3.26614358154), test loss: 2.80779449046\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (7.80247116089,35.843458288), test loss: 34.086422658\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.46793150902,3.24535667083), test loss: 3.38088276982\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (11.6574325562,35.5908457066), test loss: 26.7807617903\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.01493692398,3.22527071343), test loss: 3.43267299235\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (7.12388849258,35.3394522635), test loss: 35.5385582447\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.32173466682,3.20561473495), test loss: 3.48507784605\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (12.790813446,35.0987388406), test loss: 29.8816719532\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.03233528137,3.18697489028), test loss: 3.49781956077\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (8.81788444519,34.8653683504), test loss: 31.4282567024\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.343026578426,3.16871998541), test loss: 2.71351229548\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (22.719625473,34.6348533969), test loss: 32.2992322206\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.388527542353,3.15099419269), test loss: 3.48868977129\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (20.0095825195,34.4082650085), test loss: 30.0710436821\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.12715673447,3.1336204879), test loss: 2.74689356089\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (3.02102613449,34.1932702334), test loss: 32.9647449255\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.6682446003,3.11662877521), test loss: 3.44417838156\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (43.4007835388,33.9809898934), test loss: 29.3465437412\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.69249868393,3.10001687014), test loss: 2.90681558549\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (24.0094909668,33.7748038248), test loss: 33.7324384689\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.605749309063,3.08381614295), test loss: 3.26957654357\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.1045703888,33.5695868092), test loss: 29.2692805052\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.22519350052,3.06804848012), test loss: 3.22128588557\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (34.2579193115,33.3741512136), test loss: 35.3190583706\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.74161589146,3.05301369164), test loss: 3.50110568702\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (7.5653386116,33.1832573388), test loss: 28.8747333288\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.15409398079,3.03820373977), test loss: 3.61494921744\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (15.603477478,32.9950182293), test loss: 32.2668153405\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.92963576317,3.02379717056), test loss: 2.99185006022\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (32.7955360413,32.8101157208), test loss: 31.7707929611\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.27406525612,3.00967044173), test loss: 3.59064986706\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (7.29779911041,32.6330294207), test loss: 33.4515259743\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.91471838951,2.99571779656), test loss: 2.66777639687\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.327999115,32.459772657), test loss: 31.4512933493\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.22463941574,2.98212135958), test loss: 3.33596419692\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (17.1731681824,32.2898755349), test loss: 30.9899329662\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.06214857101,2.96881504588), test loss: 2.83939274549\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (23.114616394,32.1216063465), test loss: 34.9227314472\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.86994075775,2.95582860537), test loss: 3.26748919785\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.24376297,31.9597763165), test loss: 26.7716096878\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.874342441559,2.94335293456), test loss: 2.78214197755\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (5.54810857773,31.8015404205), test loss: 35.3664808512\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.4421415329,2.93110095297), test loss: 3.43549643457\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.5235691071,31.6459040214), test loss: 27.6256748676\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.75831747055,2.91917749794), test loss: 3.37556772977\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (60.7195892334,31.4928527123), test loss: 36.6434507847\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.76893544197,2.90744011597), test loss: 3.32253129482\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (15.9512052536,31.34427882), test loss: 30.273980093\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.84648346901,2.89586863736), test loss: 3.53499845862\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (7.94698762894,31.1993512949), test loss: 32.5700278521\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.85315179825,2.88434249883), test loss: 2.6945009619\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.61805009842,31.0575025256), test loss: 32.9358009338\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.247908264399,2.87317404822), test loss: 3.39718024135\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (7.06278467178,30.9167352036), test loss: 31.6039587855\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.50430178642,2.8622303915), test loss: 2.77652791739\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (24.3279094696,30.780010926), test loss: 32.8922911167\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.68102526665,2.85171698097), test loss: 3.38702028692\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (13.3923883438,30.6467540474), test loss: 30.2022334576\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.35880208015,2.84141899335), test loss: 2.88294535875\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (43.9434776306,30.5162377571), test loss: 34.2608270645\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.8080291748,2.83136981183), test loss: 3.26600351036\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.7366218567,30.3861281473), test loss: 29.3716268778\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.01858973503,2.82136534145), test loss: 3.18238821626\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (9.2725982666,30.2601562966), test loss: 35.8526533246\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.027135849,2.81158086443), test loss: 3.45056760013\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (9.66420269012,30.1372537011), test loss: 29.0250291348\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.48631930351,2.80173823288), test loss: 3.42842515707\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (13.6129674911,30.016639187), test loss: 32.9796411991\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.85721230507,2.79216237849), test loss: 2.80824266672\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (41.8684082031,29.8968501904), test loss: 31.7481884718\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.83629775047,2.78281121851), test loss: 3.51433289945\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (6.63184547424,29.7791282581), test loss: 33.5672219276\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.16188430786,2.77376822111), test loss: 2.63657114506\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (10.3635902405,29.6650974029), test loss: 31.5315932035\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.938593447208,2.7648969273), test loss: 3.30075607002\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (18.907497406,29.55405595), test loss: 33.1512403965\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.284641742706,2.75627752559), test loss: 2.95464842916\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (23.4696311951,29.4423915062), test loss: 35.2900006533\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.74005746841,2.74769532202), test loss: 3.22248697579\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (17.8963680267,29.3329947337), test loss: 26.4537125111\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.00941514969,2.73925271632), test loss: 2.66495667398\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (5.98838567734,29.22701811), test loss: 35.5983983517\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.996595859528,2.73073288577), test loss: 3.36995723248\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (14.6540622711,29.1233324983), test loss: 27.6546760559\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.75055408478,2.722530594), test loss: 3.25964268446\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (18.1424541473,29.0192865508), test loss: 33.0948399305\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (5.58494615555,2.71446705937), test loss: 2.99940589368\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (13.7870111465,28.9170863753), test loss: 31.1192015886\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.76617217064,2.7065428283), test loss: 3.46448660493\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (15.040263176,28.8176371827), test loss: 32.5961953044\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.47915917635,2.69885895262), test loss: 2.61902210414\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (38.5523223877,28.7212908681), test loss: 32.5285469532\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (4.02109670639,2.69142684611), test loss: 3.37759031057\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (14.325720787,28.6231125198), test loss: 32.6086266041\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.68413758278,2.68393908784), test loss: 2.77416924238\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (8.02633857727,28.5272879945), test loss: 31.7270432949\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.970525026321,2.67653201404), test loss: 3.27579924464\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (14.8490362167,28.4347985947), test loss: 29.5364422798\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (4.34012794495,2.66921344962), test loss: 2.84487972558\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (29.627702713,28.3439728844), test loss: 34.4884857893\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.48928666115,2.66194263699), test loss: 3.16687915623\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (31.7880973816,28.2529440377), test loss: 28.608693409\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (3.21946954727,2.65492679762), test loss: 3.13977714181\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (17.7336158752,28.1629979673), test loss: 35.7954878092\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.85043072701,2.64792886932), test loss: 3.37827755809\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (12.8062000275,28.0754626159), test loss: 28.3074595928\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.5037279129,2.64119345331), test loss: 3.39722596407\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (5.78668546677,27.9905978254), test loss: 33.321530962\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.38532602787,2.6345984122), test loss: 2.74171735346\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (16.2459430695,27.904246186), test loss: 30.741661787\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.587416827679,2.62800452789), test loss: 3.50213977993\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (9.47860622406,27.8193219894), test loss: 31.824823451\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.11961960793,2.6214785828), test loss: 2.58251794875\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (11.7335157394,27.7375556139), test loss: 33.1419989824\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.65771198273,2.61501145888), test loss: 3.37384486496\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (5.75123882294,27.6569709691), test loss: 30.1100022078\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.56292653084,2.60854459367), test loss: 2.74552325904\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (26.5111598969,27.5765395923), test loss: 35.3643145084\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.896720707417,2.60229445008), test loss: 3.21197654903\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (17.7001800537,27.4963094887), test loss: 25.6517167568\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.6769964695,2.59607552665), test loss: 2.57593672872\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (23.0461616516,27.4188557771), test loss: 35.2518632412\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.798709452152,2.5900922484), test loss: 3.2841555357\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (10.8143253326,27.3431285928), test loss: 27.510266614\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (0.5025806427,2.58423136076), test loss: 3.321386078\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (18.5939769745,27.2665549534), test loss: 31.7398845196\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.62173390388,2.57839677442), test loss: 2.95531476736\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (14.6537265778,27.1906439047), test loss: 31.6146847248\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.04474651814,2.57255292374), test loss: 3.44874387681\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (47.1680603027,27.1183355758), test loss: 31.8222625732\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.09023976326,2.56679151253), test loss: 2.50874914527\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (10.3355636597,27.0457962054), test loss: 32.0485719681\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.12344479561,2.5610287015), test loss: 3.30146498382\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.47707366943,26.9740194966), test loss: 32.2756699562\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.911775231361,2.55543159653), test loss: 2.7586192131\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (11.1656637192,26.9018636647), test loss: 31.7037376761\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.873988091946,2.54984984976), test loss: 3.24268914759\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (13.6872386932,26.8321698223), test loss: 29.1968900681\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.16768670082,2.54448937608), test loss: 2.85058401823\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (19.4630470276,26.7640503769), test loss: 33.8933226824\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.17497050762,2.5392062564), test loss: 3.16588176191\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (34.827205658,26.6953270558), test loss: 28.8411298275\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.75676989555,2.5339560841), test loss: 3.22968282104\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (17.2214889526,26.6271549868), test loss: 35.4502636909\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.48658704758,2.52873732516), test loss: 3.35708935559\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (17.4669418335,26.5620429026), test loss: 28.061830759\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.09018659592,2.52354051893), test loss: 3.38116066754\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (8.41927337646,26.4965416469), test loss: 33.8764439106\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.36708164215,2.51841440704), test loss: 2.7137614131\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (13.8452663422,26.4320887556), test loss: 31.671381855\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.67011332512,2.51334827604), test loss: 3.43863270581\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.84435653687,26.366555877), test loss: 31.6044761539\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.16364431381,2.50828227258), test loss: 2.48250190318\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (14.1050462723,26.3036723869), test loss: 33.3250568867\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.0709733963,2.503465166), test loss: 3.38684056699\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (11.3142967224,26.2420682558), test loss: 30.2846921444\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.416798919439,2.49866138515), test loss: 2.78605858684\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (17.0640850067,26.1796461986), test loss: 35.1079823732\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.16330838203,2.49393802395), test loss: 3.18537194431\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (14.4790611267,26.1176991286), test loss: 25.993201685\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.9446182251,2.48919962516), test loss: 2.61159855723\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (11.1621217728,26.0585834097), test loss: 35.1791998863\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.41597747803,2.4844914833), test loss: 3.28372163177\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (39.4701194763,25.9993292943), test loss: 28.0771012306\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.1583302021,2.47984071145), test loss: 3.29193530381\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (22.9900436401,25.9406849301), test loss: 31.2066135883\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.591228544712,2.47524835158), test loss: 2.92573724985\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (9.65873146057,25.880777235), test loss: 31.6071575642\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.94717860222,2.47066843038), test loss: 3.43321288526\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (17.4111366272,25.8237633381), test loss: 32.3385388851\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.41521549225,2.4663068494), test loss: 2.54372254908\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (10.7468299866,25.7675503299), test loss: 31.6999049902\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.978492736816,2.46192540791), test loss: 3.32612116933\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (13.4873142242,25.7107511794), test loss: 33.7652459621\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.29974746704,2.45763593614), test loss: 2.83633534312\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (36.1903457642,25.6545642745), test loss: 33.9971598148\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (3.96380352974,2.45338287466), test loss: 3.36240210831\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (6.50321578979,25.5999996364), test loss: 29.2007759094\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.75157475471,2.44905762401), test loss: 2.81859184951\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (18.6184005737,25.5460491688), test loss: 33.9558320045\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.7565587759,2.44482418673), test loss: 3.14541567564\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (18.1208992004,25.4921027313), test loss: 27.6577646732\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.06386590004,2.44063572427), test loss: 3.07542204857\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (29.5125904083,25.4376307389), test loss: 35.4266843796\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.05988883972,2.43646858116), test loss: 3.33558085263\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (11.2338142395,25.3850352639), test loss: 27.7607193708\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.600670397282,2.43245356569), test loss: 3.35651065707\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (7.02633810043,25.3332100912), test loss: 33.6818788052\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.00150299072,2.42846686781), test loss: 2.7126524061\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (19.0728263855,25.2814274075), test loss: 31.8659226418\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.35591316223,2.42456750321), test loss: 3.4581563592\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (35.2230606079,25.229904179), test loss: 31.6309060097\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.99269664288,2.42067534949), test loss: 2.61725752056\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (13.8088188171,25.1795261713), test loss: 33.569781208\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (6.93080759048,2.41678181889), test loss: 3.39942235053\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (3.98267531395,25.1299096829), test loss: 29.7225595474\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.978403627872,2.41282406391), test loss: 2.76494312286\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (21.6050796509,25.0805997925), test loss: 34.1780920982\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.890764892101,2.40900603413), test loss: 3.17870343626\n",
      "\n",
      "MC # 5, Hype # hyp4, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold5/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (329.583618164,inf), test loss: 187.278231812\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (310.225769043,inf), test loss: 373.050793457\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (30.0391235352,97.695594882), test loss: 47.9828958035\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.681119501591,74.5407116879), test loss: 3.40132185817\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (32.4229812622,73.4669606206), test loss: 41.4691462994\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.31657600403,38.7416965403), test loss: 3.47991395593\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (70.0872955322,65.4930344516), test loss: 45.2524531364\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.92448079586,26.7997538003), test loss: 3.46400694847\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (35.4198684692,61.3386487452), test loss: 43.4905796051\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.46212267876,20.8273603816), test loss: 3.76216728985\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (36.6475601196,58.9451108832), test loss: 43.5532929897\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.28889322281,17.2471536127), test loss: 2.70684432983\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (65.9668273926,57.2489873618), test loss: 48.808937645\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (5.5064239502,14.8585917739), test loss: 3.81658709049\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (37.1086959839,56.0301828074), test loss: 45.1695983887\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (8.92494010925,13.1507494298), test loss: 2.68661873341\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (46.6978302002,55.0875199023), test loss: 46.406706953\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.16206693649,11.8680905279), test loss: 3.72646507621\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (98.4417800903,54.2944241442), test loss: 40.9548567295\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.54204082489,10.8686136394), test loss: 3.02149298191\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (31.594461441,53.630093011), test loss: 48.0741636276\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.27107286453,10.068577768), test loss: 3.5632021606\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (64.936126709,53.0915033385), test loss: 39.7045277119\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.01728653908,9.40982951725), test loss: 2.79382006526\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (17.1607894897,52.6616187924), test loss: 46.273677063\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.15200543404,8.86074121832), test loss: 3.34585588872\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (51.7194213867,52.2615095329), test loss: 39.7756199837\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.54108440876,8.39525794893), test loss: 3.38602118492\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (79.2617874146,51.903384912), test loss: 43.6189450741\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.21793580055,7.99588382336), test loss: 3.31324676275\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (45.5789871216,51.571095973), test loss: 42.9862285137\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.12365341187,7.64599895037), test loss: 3.78406067491\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (52.7699127197,51.2507085683), test loss: 41.9130665541\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.83159649372,7.34089764322), test loss: 2.65978328586\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (53.8271484375,50.9402872417), test loss: 46.6564608574\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.79897403717,7.07011310575), test loss: 3.71499904394\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (167.50050354,50.6780158315), test loss: 42.990280056\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.35206127167,6.82760864601), test loss: 2.55688961446\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (26.510011673,50.4196908763), test loss: 44.2842954636\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.02252626419,6.60985043778), test loss: 3.65338546038\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (38.4395294189,50.1982087564), test loss: 38.9889656305\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.52481985092,6.41446254503), test loss: 2.72669929862\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (62.313999176,49.962081809), test loss: 46.0581697941\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.50754857063,6.2363712414), test loss: 3.43476601839\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (26.3785762787,49.7447526975), test loss: 37.2721681833\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.50488972664,6.07363929019), test loss: 2.80436917245\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (42.4848709106,49.5234354131), test loss: 43.8468344212\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.4528824091,5.92509027102), test loss: 3.09264406264\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (26.3830909729,49.2845567925), test loss: 37.7219974756\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.32788288593,5.787375501), test loss: 3.41857597232\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (42.7591667175,49.0742542463), test loss: 41.5975441456\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.78215408325,5.66033551247), test loss: 3.21428927779\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (38.396232605,48.8576973612), test loss: 40.6404377937\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.40833711624,5.5415672178), test loss: 3.5178307116\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (24.3606491089,48.6609980914), test loss: 39.221460247\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.53086733818,5.43152464215), test loss: 2.52677531838\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (46.6135101318,48.4503110053), test loss: 43.5327721596\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.38647162914,5.32886540452), test loss: 3.60685451627\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (17.9287281036,48.241835967), test loss: 40.3000553608\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.45240259171,5.23299271338), test loss: 2.54192784727\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (44.108581543,48.0332724866), test loss: 41.1222242355\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.99421298504,5.14201377873), test loss: 3.46965770423\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (49.3134384155,47.8109603228), test loss: 36.0802768707\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.18896102905,5.05684318727), test loss: 2.74207112193\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (39.8775787354,47.5806067744), test loss: 43.2769338608\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.17896282673,4.97618234341), test loss: 3.32583600283\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (18.9737224579,47.3554208779), test loss: 33.6215389729\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (5.10503721237,4.89915312852), test loss: 2.59733118713\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (15.9540643692,47.1365788538), test loss: 40.1864536285\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.45431578159,4.82609412516), test loss: 3.10620871782\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (41.8814315796,46.9148817459), test loss: 34.649868536\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.14228796959,4.7569562015), test loss: 3.26878522038\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (6.41682481766,46.6766387342), test loss: 37.9929490805\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.50517272949,4.69116447758), test loss: 3.12828861475\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (33.4662742615,46.4405076314), test loss: 37.3380418777\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.749083280563,4.62797487559), test loss: 3.45063551664\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (88.7784118652,46.1888708194), test loss: 34.5797878504\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.4477725029,4.56788034312), test loss: 2.38745027184\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (101.79347229,45.9255165619), test loss: 39.1309177399\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.00226092339,4.51001104271), test loss: 3.56243156791\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (14.2194805145,45.6640145736), test loss: 35.4621646404\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.89469814301,4.4542414417), test loss: 2.45467353165\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (67.7599029541,45.3928015506), test loss: 36.3754294872\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.27391171455,4.40059094515), test loss: 3.40254152715\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (41.576751709,45.1245253984), test loss: 31.4465957642\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.96601057053,4.34920751303), test loss: 2.56821212023\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (20.5192184448,44.8387027738), test loss: 38.2061759472\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.18263339996,4.29971453808), test loss: 3.2208720386\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (22.3149414062,44.5480877804), test loss: 28.451851368\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.20410776138,4.25219115443), test loss: 2.55206489265\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (15.9581394196,44.2507102295), test loss: 34.8534561872\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.736841380596,4.20623238248), test loss: 3.09675224125\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (20.5200881958,43.9324287516), test loss: 29.776233077\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.62746655941,4.16207541039), test loss: 3.28914309144\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (30.0574321747,43.6001711891), test loss: 32.7889413595\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.4339993,4.1192559035), test loss: 3.15760025978\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (10.981095314,43.2679529595), test loss: 31.8365370274\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.51492667198,4.07749256435), test loss: 3.41397771984\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (24.8705120087,42.9365254513), test loss: 29.3285981655\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (7.42355203629,4.03754649902), test loss: 2.46001215279\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (24.1382331848,42.5988136043), test loss: 33.7417838573\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.32071638107,3.99869346675), test loss: 3.51672366858\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (26.1657905579,42.2593420785), test loss: 29.1936693192\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.72438430786,3.96153020138), test loss: 2.46723234951\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (32.3631896973,41.9206178433), test loss: 32.6980628967\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.32347774506,3.92543747084), test loss: 3.41509497166\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (45.3659629822,41.5790315479), test loss: 27.7245364666\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.17021226883,3.89073196518), test loss: 2.7076883018\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (46.7885627747,41.2364347045), test loss: 33.2898054123\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.67232275009,3.85701008913), test loss: 3.26846514642\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (6.22837972641,40.9049844408), test loss: 25.0566091537\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.61671614647,3.8239856058), test loss: 2.67007621825\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (26.407989502,40.5752888946), test loss: 32.3347748756\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.81521892548,3.79223250022), test loss: 3.17775120437\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (25.1708831787,40.2532341729), test loss: 28.5381337643\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.87212264538,3.76157935728), test loss: 3.38692951202\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (28.8585014343,39.9328576625), test loss: 32.5180452824\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.45585656166,3.73182561128), test loss: 3.27461933345\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (36.4336929321,39.620704693), test loss: 30.9170438409\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.868511497974,3.70311992195), test loss: 3.52696811259\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (10.9062290192,39.3167340224), test loss: 29.3896109104\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.741999745369,3.67538497572), test loss: 2.56423389018\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.396522522,39.0156504425), test loss: 32.7239080071\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.38683235645,3.64845878262), test loss: 3.59949083626\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (17.6348266602,38.7212484394), test loss: 28.7500552416\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.23174786568,3.62223725864), test loss: 2.45604372621\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (6.64579343796,38.4379281705), test loss: 33.9554964423\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.24097490311,3.59644521183), test loss: 3.46400262117\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.5599813461,38.1614343446), test loss: 27.8600859404\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.370578289032,3.57145735697), test loss: 2.7734873265\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (52.318862915,37.8903016447), test loss: 33.5732513189\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.88346135616,3.54736374714), test loss: 3.28419861197\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (14.0830993652,37.6256286779), test loss: 26.1842495441\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.00496792793,3.52408939003), test loss: 2.69292107522\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (15.789182663,37.3691749388), test loss: 32.695695591\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.9479598999,3.50137126021), test loss: 3.23559572101\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (20.309425354,37.1172449767), test loss: 29.9661873102\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.237501338124,3.47940667966), test loss: 3.43806678653\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (8.81786727905,36.8691916531), test loss: 33.6355617762\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.20480084419,3.45786591431), test loss: 3.33408316076\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (25.6610603333,36.6329715327), test loss: 31.0187812209\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.20310676098,3.43673421025), test loss: 3.54232309163\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (31.2848358154,36.4005875085), test loss: 30.8794410706\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.13330578804,3.41617677163), test loss: 2.54280982614\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (46.4044456482,36.1742300313), test loss: 34.2401413918\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.31771230698,3.39624747533), test loss: 3.5731302321\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.8974161148,35.9510996801), test loss: 29.8520288944\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.89005720615,3.37674234032), test loss: 2.41046818793\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (10.1131534576,35.7349546116), test loss: 34.4849564314\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.56131744385,3.35793879741), test loss: 3.46291418076\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (4.35646343231,35.5261811777), test loss: 30.8350149632\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.789702534676,3.33964956827), test loss: 2.83311970383\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (27.4419975281,35.3182681302), test loss: 33.3821663141\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.68770289421,3.32167030905), test loss: 3.33653187752\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (5.8388376236,35.1154625631), test loss: 26.8473447323\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.26782035828,3.30413766221), test loss: 2.62495808303\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (17.2441654205,34.9206402064), test loss: 33.5883564472\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.601308941841,3.28664287384), test loss: 3.2584433645\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (18.2766933441,34.7293560328), test loss: 30.7228028774\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.28707003593,3.26983641858), test loss: 3.50275089145\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (36.2279396057,34.5395382622), test loss: 34.8988864183\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.57092869282,3.25331433689), test loss: 3.30895811021\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (36.8891105652,34.3565284852), test loss: 31.5852619171\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.90481770039,3.23744182005), test loss: 3.53352770805\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (12.9853391647,34.1782379871), test loss: 31.6184372902\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.38068073988,3.22179540637), test loss: 2.60810721815\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (28.2865428925,34.0017959155), test loss: 33.3355837107\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.966908693314,3.20665996455), test loss: 3.606021595\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (39.281124115,33.8278841392), test loss: 29.3439454079\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.0267765522,3.19169657766), test loss: 2.44502708763\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (31.1212291718,33.6610078911), test loss: 34.3921622276\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.54570329189,3.17695251084), test loss: 3.45315869451\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (10.455956459,33.4969811318), test loss: 28.6741007805\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.609139084816,3.16245186277), test loss: 2.72694328725\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (28.7579193115,33.3356516998), test loss: 34.6991532087\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (3.86732578278,3.14841517353), test loss: 3.27874878943\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (24.4355049133,33.1764581498), test loss: 26.5452185154\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.80547094345,3.13458145251), test loss: 2.58445206434\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (15.5675191879,33.0219861968), test loss: 33.4821570873\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.19798612595,3.12118126514), test loss: 3.27651791871\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (18.1530952454,32.8721037388), test loss: 31.5768832684\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.36278831959,3.10819940527), test loss: 3.53249819279\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (4.47835254669,32.7203404031), test loss: 34.5091606379\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.42763650417,3.09526036715), test loss: 3.29709658176\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (9.90071868896,32.5749191404), test loss: 32.064622736\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.51070833206,3.08255818125), test loss: 3.53165397048\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (17.2163047791,32.4331028697), test loss: 31.6635342598\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.02493834496,3.06994220494), test loss: 2.52190334797\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (6.53045415878,32.29290238), test loss: 34.4170668602\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.82508409023,3.05768128481), test loss: 3.54259749651\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (13.7698783875,32.1530310876), test loss: 28.902237463\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.622479915619,3.04560443572), test loss: 2.4134570837\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (9.91737747192,32.0181251976), test loss: 35.0280191422\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.79168987274,3.03399512632), test loss: 3.43270173073\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (23.8076591492,31.8867501635), test loss: 30.0349647045\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.10088133812,3.02254305933), test loss: 2.81791212559\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (35.4910430908,31.7554400665), test loss: 34.0169782162\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.9252601862,3.01137036923), test loss: 3.22197825015\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (21.7961502075,31.6256839655), test loss: 26.4982194424\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.13278651237,3.00027856115), test loss: 2.58132584989\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (16.4608821869,31.501063243), test loss: 33.607717371\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (4.63301229477,2.98932083026), test loss: 3.25240922272\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (11.2281093597,31.3781598769), test loss: 30.8520153999\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.41092467308,2.97848123525), test loss: 3.39908062518\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (12.8481731415,31.2562317893), test loss: 34.2805985928\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.8186147213,2.96790011126), test loss: 2.97991197705\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (3.42897796631,31.1358557333), test loss: 34.2257407188\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.82736110687,2.95755197307), test loss: 3.59135572016\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.959854126,31.0189676037), test loss: 31.5911219597\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.928688406944,2.94743864886), test loss: 2.53530247062\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (54.8366737366,30.9042953821), test loss: 33.6967812061\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.78492724895,2.93760063639), test loss: 3.5256791234\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (42.5674514771,30.7891205595), test loss: 29.4735618591\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.778032422066,2.92780085799), test loss: 2.43471774757\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (10.1060190201,30.6775077864), test loss: 34.7160692215\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.18110656738,2.91808039281), test loss: 3.44087468386\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (32.9654579163,30.5685121149), test loss: 28.7943927765\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.34616279602,2.90851197009), test loss: 2.76506207287\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (22.5345802307,30.4606867349), test loss: 34.8397375107\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.21142363548,2.8991312038), test loss: 3.18882332444\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (6.86750030518,30.3524069334), test loss: 26.3423513412\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.927594780922,2.88985317314), test loss: 2.55395701826\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (9.87104415894,30.2479047593), test loss: 33.8630450964\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.20386779308,2.88087779152), test loss: 3.24751020968\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (13.5840072632,30.1453366314), test loss: 30.8485476494\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.723281264305,2.87205402248), test loss: 3.44612625837\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (9.44249153137,30.0427583535), test loss: 31.3773666143\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.295779883862,2.86339432085), test loss: 2.77484739423\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (15.4374237061,29.9414966814), test loss: 32.8887912273\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.14029932022,2.85476635321), test loss: 3.59461933076\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (8.34039878845,29.8437651958), test loss: 31.5843673944\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.13762497902,2.84613805267), test loss: 2.50959227383\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (25.6980571747,29.7475362987), test loss: 34.4924712658\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (6.38905239105,2.83780553961), test loss: 3.49065360427\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (13.5661039352,29.6508947281), test loss: 29.0461807251\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.27293491364,2.829467991), test loss: 2.44753473699\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (31.6814041138,29.5562549779), test loss: 36.1486031055\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.49509906769,2.82138199831), test loss: 3.52639612034\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (18.1096076965,29.4634227736), test loss: 29.3570266008\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.75818061829,2.81341512127), test loss: 2.78731166422\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (36.8305778503,29.3717192879), test loss: 33.7477030277\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.76154756546,2.80565881925), test loss: 3.20617196262\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (42.8794364929,29.2801033576), test loss: 26.2874148846\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (4.84706306458,2.79794456069), test loss: 2.56712620258\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (3.35212349892,29.1914937886), test loss: 33.4478141546\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.23746228218,2.79015519555), test loss: 3.16882752776\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (24.3165493011,29.1040699361), test loss: 30.498535645\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (3.16313123703,2.78260067195), test loss: 3.36778686345\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (23.5396022797,29.0175635577), test loss: 29.0551496029\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.00171279907,2.77514901839), test loss: 2.58975803852\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (22.1291217804,28.9306125162), test loss: 35.2479921103\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.33678007126,2.76776494288), test loss: 3.57585963011\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (32.1045608521,28.8463274307), test loss: 31.6302658081\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.749855816364,2.76056269387), test loss: 2.50295811594\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.91961669922,28.7636548927), test loss: 34.1793803692\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.635357797146,2.75351158067), test loss: 3.48508680761\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (15.4752388,28.6804444959), test loss: 30.005517292\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.20317578316,2.7465516091), test loss: 2.52193222642\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (14.6041889191,28.5984221478), test loss: 34.9876232624\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.98107075691,2.73963398834), test loss: 3.50615722239\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.19886922836,28.5191631518), test loss: 28.7221558571\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.69467639923,2.73265548587), test loss: 2.72498629391\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (12.1883964539,28.4404315949), test loss: 33.880999136\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.233519926667,2.72583747341), test loss: 3.17795243561\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (51.1624221802,28.3618293643), test loss: 26.2894641638\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.75119662285,2.71915345542), test loss: 2.59787349403\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (12.6539897919,28.2841341616), test loss: 33.7408921719\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.985231399536,2.71260912389), test loss: 3.14337440431\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (13.1413221359,28.2084083095), test loss: 30.4542493105\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.5205540657,2.70614102961), test loss: 3.42637863159\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (17.6394691467,28.1326581718), test loss: 28.9403184414\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.211097747087,2.69982216193), test loss: 2.62644877583\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.691157341,28.057200033), test loss: 34.0300660253\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.8296084404,2.69351852834), test loss: 3.51568067372\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (19.925868988,27.984706445), test loss: 30.7508841991\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.979544460773,2.68720534972), test loss: 2.4995021075\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (31.6388759613,27.9124511071), test loss: 34.7924322844\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.04065060616,2.68100591219), test loss: 3.44534432888\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (38.8290328979,27.8406304316), test loss: 29.8274727821\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.98980569839,2.67489458716), test loss: 2.51680810899\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (13.7390232086,27.7687858125), test loss: 36.1130822062\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.69539022446,2.66883757793), test loss: 3.50211150348\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (9.41645050049,27.698694562), test loss: 28.5966483116\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (3.24805212021,2.66294488921), test loss: 2.73965794444\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (6.57409763336,27.630407391), test loss: 33.1498364925\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.687377929688,2.65715328353), test loss: 3.15857179314\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (21.5313949585,27.5608790238), test loss: 27.9087770462\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.44885873795,2.65138010339), test loss: 2.87726754248\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.0719294548,27.4925584594), test loss: 33.4397404432\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (3.09119844437,2.64568534778), test loss: 3.13858833462\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (12.0128927231,27.4265318371), test loss: 30.2964662552\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.619757592678,2.63986890158), test loss: 3.37080962658\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (19.7607688904,27.3605886536), test loss: 28.8742221713\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (3.27824091911,2.63427410753), test loss: 2.61801201999\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (35.6994972229,27.2938575304), test loss: 34.9073524952\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.65253925323,2.6286751264), test loss: 3.46916317344\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (32.1497077942,27.2292380016), test loss: 31.119930315\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.88141465187,2.62326757077), test loss: 2.4911746487\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (10.8174953461,27.1657743924), test loss: 34.1008620739\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.372926056385,2.61787541905), test loss: 3.46470527649\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (28.451997757,27.1018070536), test loss: 30.7066986561\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.796503305435,2.61262154036), test loss: 2.54818643332\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (40.3212966919,27.0383524777), test loss: 34.4536943436\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.10050106049,2.60736354805), test loss: 3.42839484215\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (20.5218086243,26.9768652058), test loss: 28.6293655396\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.29705905914,2.60209538223), test loss: 2.71829855144\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (9.64146995544,26.9158103213), test loss: 33.8331519604\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.511823952198,2.59686760079), test loss: 3.07759921253\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (27.1132164001,26.8548740724), test loss: 29.6375945091\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (3.31412792206,2.59176576746), test loss: 3.10591542125\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (16.7236251831,26.7939364038), test loss: 34.1935249567\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.85967695713,2.58668110473), test loss: 3.1419759661\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (13.5490922928,26.734640407), test loss: 30.1936834812\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.13089227676,2.58171555159), test loss: 3.35342723727\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (22.8882274628,26.676631367), test loss: 29.9837677002\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.47341430187,2.57689213999), test loss: 2.70752578676\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.32614278793,26.6168611373), test loss: 33.6673820972\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.23932051659,2.57201478745), test loss: 3.47141193748\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (7.98732948303,26.5592957616), test loss: 30.1287266254\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.46248316765,2.56717278109), test loss: 2.35951186121\n",
      "run time for single CV loop: 7069.49835086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:124: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:126: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Deign Net Architecutre and Run Caffe\n",
    "exp_name = 'Exp13_MC'\n",
    "cohort = 'ADNI1and2'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'HC_CT'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "multi_label = False\n",
    "start_MC=1\n",
    "n_MC=5\n",
    "MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "start_fold = 1\n",
    "n_folds = 5\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "niter = 80000\n",
    "batch_size = 256\n",
    "\n",
    "pretrain = False\n",
    "multimodal_autoencode = False\n",
    "load_pretrained_weights = True\n",
    "\n",
    "if Clinical_Scale in ['BOTH','ADAS13_DX'] :\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "# dr: Dropout rate, lr: learning rate of each modality, tr: loss-weight for each task\n",
    "hype_configs = {\n",
    "            'hyp4':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':25,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':.1,'CT':1,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':2,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}},  \n",
    "    \n",
    "            'hyp5':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':25,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':6,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':2,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}},\n",
    "    \n",
    "#             'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':50,'COMB_ff':50,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':8,'COMB':1},\n",
    "#                       'tr':{'ADAS':1,'MMSE':2,'DX':1},'solver_conf':{'base_lr':3e-6, 'wt_decay':1e-3}},  \n",
    "        \n",
    "#             'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':200,'HC_CT_ff':25,'COMB_ff':50,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':0.5,'CT':0.5,'COMB':2},\n",
    "#                       'tr':{'ADAS':1,'MMSE':4,'DX':1},'solver_conf':{'base_lr':3e-6, 'wt_decay':1e-3}}, \n",
    "        \n",
    "#             'hyp6':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':50,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':1,'CT':1,'HC_CT':1},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}},   \n",
    "                    \n",
    "                }\n",
    "\n",
    "CV_perf_MC = {}\n",
    "for mc in MC_list:\n",
    "    CV_perf_hype = {}\n",
    "    for hype in hype_configs.keys():    \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "        tr = hype_configs[hype]['tr']\n",
    "        solver_configs = hype_configs[hype]['solver_conf']\n",
    "        \n",
    "        if hype in ['hyp1']:\n",
    "            HC_snap = 20000 #for ADNI2 #5000 for ADNI1\n",
    "            CT_snap = 80000 #for ADNI2 #5000 for ADNI1\n",
    "            pre_hype = 'hyp1'\n",
    "        elif hype in ['hyp2']:\n",
    "            HC_snap = 20000 #8000 for ADNI2 #5000 for ADNI1\n",
    "            CT_snap = 80000 #28000 for ADNI2 #5000 for ADNI1\n",
    "            pre_hype = 'hyp1'\n",
    "        elif hype in ['hyp3']:\n",
    "            HC_snap = 20000 #8000 for ADNI2 #5000 for ADNI1\n",
    "            CT_snap = 80000 #28000 for ADNI2 #5000 for ADNI1\n",
    "            pre_hype = 'hyp3'\n",
    "        elif hype in ['hyp4']:\n",
    "            HC_snap = 20000 #8000 for ADNI2 #5000 for ADNI1\n",
    "            CT_snap = 80000 #28000 for ADNI2 #5000 for ADNI1\n",
    "            pre_hype = 'hyp4'\n",
    "        elif hype in ['hyp5']:\n",
    "            HC_snap = 20000 #8000 for ADNI2 #5000 for ADNI1\n",
    "            CT_snap = 80000 #28000 for ADNI2 #5000 for ADNI1\n",
    "            pre_hype = 'hyp4'\n",
    "        else:\n",
    "            print 'unknown hyp config'\n",
    "            \n",
    "        CV_perf = {}\n",
    "        start_time = time.time()\n",
    "        for fid in fid_list:            \n",
    "            print ''\n",
    "            print 'MC # {}, Hype # {}, Fold # {}'.format(mc, hype, fid)\n",
    "            snap_prefix = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}'.format(mc,fid,exp_name,hype,modality)\n",
    "\n",
    "            train_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/train_C688.txt'.format(mc,fid)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "\n",
    "            train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "            with open(train_filename_txt, 'w') as f:\n",
    "                    f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "            if pretrain:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_train.prototxt'.format(mc,fid)\n",
    "                with open(train_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(train_filename_txt, batch_size, node_sizes, modality)))            \n",
    "            else:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(train_net_path, 'w') as f:            \n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            if pretrain:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_test.prototxt'.format(mc,fid)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(test_filename_txt, batch_size, node_sizes,modality)))\n",
    "            else:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            # Define Solver\n",
    "            solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "            with open(solver_path, 'w') as f:\n",
    "                f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, snap_prefix)))\n",
    "\n",
    "            ### load the solver and create train and test nets\n",
    "            #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "            #solver = caffe.get_solver(solver_path)            \n",
    "            solver = caffe.NesterovSolver(solver_path)\n",
    "\n",
    "            if load_pretrained_weights:                    \n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_HC_CT_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                snap_path = baseline_dir + net_name.format(mc,fid,cohort,pre_hype,HC_snap,CT_snap)               \n",
    "                print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "                solver.net.copy_from(snap_path)\n",
    "\n",
    "            #run caffe\n",
    "            results = run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode)\n",
    "            CV_perf[fid] = results\n",
    "            \n",
    "        print('run time for single CV loop: {}'.format(time.time() - start_time))\n",
    "\n",
    "        CV_perf_hype[hype] = CV_perf\n",
    "        \n",
    "CV_perf_MC[mc] = CV_perf_hype\n",
    "\n",
    "# pickleIt(CV_perf_MC, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.4444444444\n",
      "3.54166666667\n"
     ]
    }
   ],
   "source": [
    "#Time for training \n",
    "#HC_CT fold 9,10, number of iterations: 40k, two hyper-params\n",
    "#start time: 14:47\n",
    "#end time: 15:31\n",
    "#runtime_mean = (13+31)/4\n",
    "#print runtime_mean\n",
    "\n",
    "tx=140 #time for 10k iters\n",
    "itx=5 # num of 10k iters\n",
    "hx=2 #hyp choices\n",
    "fx=10 #k-folds\n",
    "mx=5 #mc-folds\n",
    "\n",
    "num_hrs = (tx*itx*hx*fx*mx)/(3600.0)\n",
    "print num_hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbcAAAJoCAYAAABVztwOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f442ffe1e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "#niter = 30000\n",
    "test_interval = 500\n",
    "n_mc = len(CV_perf_MC.keys())\n",
    "\n",
    "for m, mc in enumerate(CV_perf_MC.keys()): \n",
    "    CV_perf_hype = CV_perf_MC[mc]\n",
    "    \n",
    "    for hype in CV_perf_hype.keys(): \n",
    "        CV_perf = CV_perf_hype[hype]\n",
    "        n_CV_configs = len(CV_perf)\n",
    "        pid = m*n_CV_configs + 1\n",
    "        \n",
    "        for f, fid in enumerate(fid_list):  \n",
    "            train_loss_list = CV_perf[fid]['train_loss']\n",
    "            test_loss_list = CV_perf[fid]['test_loss']\n",
    "            \n",
    "            for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "                #plt.figure()\n",
    "                ax1 = plt.subplot(n_mc,n_CV_configs,pid)            \n",
    "                ax1.plot(arange(niter), train_loss, label='train',linewidth='.5',alpha=0.25)\n",
    "                ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test_{}'.format(hype), linewidth='3')                            \n",
    "                ax1.set_xlabel('iteration')\n",
    "                ax1.set_ylabel('loss')\n",
    "                ax1.set_title('fid: {}, hyp: {}, Test loss: {:.2f}'.format(fid, hype, test_loss[-1]))\n",
    "                ax1.legend(loc=1)\n",
    "                ax1.set_ylim(0,100)\n",
    "            pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp11'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'R_HC'\n",
    "batch_size = 256\n",
    "encoding_layer = 'code'\n",
    "weight_layers = 'code'\n",
    "multi_task = False\n",
    "\n",
    "if modality in ['L_HC', 'R_HC']:\n",
    "    snap_iter = 10000\n",
    "    \n",
    "    if modality == 'R_HC':\n",
    "        output_node_size = 16471\n",
    "    else: \n",
    "        output_node_size = 16086\n",
    "        \n",
    "    hype_configs = {\n",
    "                   'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':8000,'out':output_node_size},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "\n",
    "elif modality in ['CT']:  \n",
    "    snap_iter = 100000\n",
    "    \n",
    "    hype_configs = {\n",
    "                    'hyp1':{'node_sizes':{'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-4, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "elif modality in ['HC_CT']:  \n",
    "    snap_iter = 20000\n",
    "    hype_configs = {\n",
    "                 'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':500,'out_HC':16086,'out_CT':686},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "hype = 'hyp1'\n",
    "pretrain_preproc = 'ae_preproc_sparse_HC_10k_CT_100k_{}'.format(hype)\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "\n",
    "# Save either L or R HC encodings. Turn on CT + CS save only for one of these. \n",
    "# (Also decide if you want to copy CT raw scores or encodings)\n",
    "save_encodings = True\n",
    "save_scores = False\n",
    "CT_encodings = True\n",
    "\n",
    "for cohort in ['inner_train', 'inner_test','outer_test']:\n",
    "    data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "    test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)       \n",
    "    input_nodes = ['X_{}'.format(modality)]\n",
    "    #input_nodes = ['X_L_HC','X_CT']\n",
    "    net_file = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "    with open(net_file, 'w') as f:\n",
    "        f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "\n",
    "    test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "    with open(test_filename_txt, 'w') as f:\n",
    "        f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "    sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "    if modality == 'CT':\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "    else:\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "        \n",
    "    print 'Check Sparsity for model: {}'.format(model_file)\n",
    "    \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    \n",
    "    encodings_hdf_file = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,pretrain_preproc)\n",
    "    \n",
    "    if save_encodings:    \n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        input_data.create_dataset(input_nodes[0],data=encodings)           \n",
    "        input_data.close()\n",
    "        \n",
    "    if save_scores:\n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        if not CT_encodings: #copy raw scores for CT instead of encodings\n",
    "            CT_data = load_data(data_path, 'X_CT','no_preproc')                    \n",
    "            input_data.create_dataset('X_CT', data=CT_data)    \n",
    "            \n",
    "        adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "        mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "        input_data.create_dataset('y', data=adas_scores)           \n",
    "        input_data.create_dataset('y3', data=mmse_scores)    \n",
    "        input_data.create_dataset('dx_cat3', data=dx_labels)\n",
    "        input_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting TSNE embeddings \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "exp_name = 'Exp11'\n",
    "#preproc = 'no_preproc'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "Clinical_Scale = 'ADAS13_DX' \n",
    "batch_size = 256\n",
    "snap_interval = 2000\n",
    "snap_start = 2000\n",
    "encoding_layer = 'ff3'\n",
    "weight_layers = 'ff3'\n",
    "cohort = 'outer_test'\n",
    "multi_task = False\n",
    "\n",
    "modality = 'HC_CT'\n",
    "snap_end = 21000\n",
    "\n",
    "\n",
    "hype_configs = {\n",
    "            'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-2}},\n",
    "}\n",
    "\n",
    "hype = 'hyp1'\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "tr = hype_configs[hype]['tr']\n",
    "\n",
    "data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "test_net_path = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)       \n",
    "#input_node = 'X_{}'.format(modality)\n",
    "\n",
    "net_file = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)\n",
    "with open(net_file, 'w') as f:\n",
    "    #f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "\n",
    "test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "with open(test_filename_txt, 'w') as f:\n",
    "    f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "n_snaps = len(np.arange(snap_start,snap_end,snap_interval))\n",
    "tsne_encodings = {}\n",
    "net_weights = {}\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    print sn, snap_iter\n",
    "    model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    net_weights[snap_iter] = results['wt_dict']\n",
    "    print encodings.shape\n",
    "\n",
    "    adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "    mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "    dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "    tsne_model = TSNE(n_components=2, random_state=0, init='pca')\n",
    "    tsne_encodings[snap_iter] = tsne_model.fit_transform(encodings) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "plot_encodings = True\n",
    "plot_wts = False\n",
    "\n",
    "color_scores = dx_labels\n",
    "cm = plt.get_cmap('RdYlBu') \n",
    "marker_size = 100\n",
    "marker_size = (np.mod(color_scores+1,2)+1)*50\n",
    "\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    plt.subplot(np.ceil(n_snaps/2.0),2,sn+1)\n",
    "    if plot_encodings:\n",
    "        plt.scatter(tsne_encodings[snap_iter][:,0],tsne_encodings[snap_iter][:,1],c=color_scores,s=marker_size,cmap=cm)\n",
    "    if plot_wts:\n",
    "        plt.imshow(net_weights[snap_iter][weight_layers])\n",
    "        \n",
    "    plt.title(snap_iter)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp13_MC 1 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.61858022659895595, 0.62875461594061355, 0.59079822157997586, 0.69850887382402604, 0.63744750649765869, 0.75420399068066013, 0.52199318165328856, 0.54449722676873669, 0.6391757646022046, 0.62946407166913665]\n",
      "ADAS mse: [55.986201791199868, 53.227942796720761, 57.723859005256735, 50.054351323884823, 46.749388013667094, 33.865470382034424, 81.467562185314634, 62.143084940096429, 45.465608971893772, 59.680599640933586]\n",
      "ADAS means: 0.626342367982, 54.6364069051\n",
      "\n",
      "MMSE corr: [0.4730551267658909, 0.44368079009207845, 0.55725489253570804, 0.53946546689551067, 0.54905686878633175, 0.56864237107697824, 0.48423551197742254, 0.47005268813806889, 0.57594813388092514, 0.50161461906751015]\n",
      "MMSE mse: [6.0860001658994607, 5.757063727203759, 4.6518702272698933, 5.7154023536786287, 5.2266785871553996, 4.9209327462834631, 6.4080045390753337, 5.2685624268729381, 4.3120091621109387, 5.4481379092938003]\n",
      "MMSE means: 0.516300646922, 5.37946618448\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {1: 6, 2: 3, 3: 18, 4: 16, 5: 9, 6: 19, 7: 19, 8: 7, 9: 18, 10: 18}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.61858022659895595, 0.62875461594061355, 0.59079822157997586, 0.69850887382402604, 0.63744750649765869, 0.75420399068066013, 0.52361266365683135, 0.54449722676873669, 0.6391757646022046, 0.62946407166913665]\n",
      "ADAS mse: [55.986201791199868, 53.227942796720761, 57.723859005256735, 50.054351323884823, 46.749388013667094, 33.865470382034424, 81.459686081887824, 62.143084940096429, 45.465608971893772, 59.680599640933586]\n",
      "ADAS means: 0.626504316182, 54.6356192948\n",
      "\n",
      "MMSE corr: [0.4730551267658909, 0.44368079009207845, 0.55725489253570804, 0.53946546689551067, 0.54905686878633175, 0.56864237107697824, 0.47465351218425372, 0.47005268813806889, 0.57594813388092514, 0.50161461906751015]\n",
      "MMSE mse: [6.0860001658994607, 5.757063727203759, 4.6518702272698933, 5.7154023536786287, 5.2266785871553996, 4.9209327462834631, 6.5480357435320258, 5.2685624268729381, 4.3120091621109387, 5.4481379092938003]\n",
      "MMSE means: 0.515342446942, 5.39346930493\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {1: 6, 2: 3, 3: 18, 4: 16, 5: 9, 6: 19, 7: 15, 8: 7, 9: 18, 10: 18}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.59379932880976105, 0.62875461594061355, 0.58253695164772368, 0.64273334030806617, 0.64534958592212532, 0.75420399068066013, 0.47648764867820492, 0.53443200874454211, 0.61320770515936351, 0.56646181075598878]\n",
      "ADAS mse: [67.073377742657016, 53.227942796720761, 58.875183508372942, 58.583924834958815, 47.825408780320757, 33.865470382034424, 85.866194525509613, 64.957420272723354, 48.803028617925065, 69.348135980746974]\n",
      "ADAS means: 0.603796698665, 58.8426087442\n",
      "\n",
      "MMSE corr: [0.46879377102061476, 0.44368079009207845, 0.56074291241244234, 0.54734377062024708, 0.58438780661162437, 0.56864237107697824, 0.52200353005555067, 0.49956136401038925, 0.6277555929857237, 0.51234285504996735]\n",
      "MMSE mse: [5.7475669805699656, 5.757063727203759, 4.6441329040121078, 5.6176117577728766, 4.8835871511267603, 4.9209327462834631, 6.0572353860665755, 4.9908640002687923, 3.8813729257211738, 5.357309665191111]\n",
      "MMSE means: 0.533525476394, 5.18576772442\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 5, 2: 3, 3: 11, 4: 18, 5: 19, 6: 19, 7: 4, 8: 12, 9: 15, 10: 19}\n",
      "Exp13_MC 2 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.60441611384705707, 0.61343507085111137, 0.68683172137132709, 0.60476409650892515, 0.69343404878447801, 0.65456067558266373, 0.59535729418442773, 0.62185391132688528, 0.48715856672353258, 0.59453582514641845]\n",
      "ADAS mse: [56.927545053141998, 61.842489110058814, 50.010473877216, 44.49872880897572, 41.41432130555738, 57.724600780821397, 64.192613963956902, 54.932889072633152, 69.571359372325986, 56.291409080118548]\n",
      "ADAS means: 0.615634732433, 55.7406430425\n",
      "\n",
      "MMSE corr: [0.53560102994131198, 0.53783607970572322, 0.56125393699101023, 0.3734731614414144, 0.51062398885679006, 0.53916665295624422, 0.37364945410106415, 0.53593555571387896, 0.59132455131233097, 0.31443171608687864]\n",
      "MMSE mse: [5.4802581718318546, 5.4393839433541604, 4.9693249017171555, 6.3011575815964171, 4.6844098228680764, 4.9892709651263587, 6.3974993398537192, 5.3437775101983656, 5.2973846930804136, 6.9949929287010617]\n",
      "MMSE means: 0.487329612711, 5.58974598583\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 7, 2: 7, 3: 7, 4: 4, 5: 8, 6: 7, 7: 8, 8: 10, 9: 4, 10: 5}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.60441611384705707, 0.61343507085111137, 0.68683172137132709, 0.60476409650892515, 0.69343404878447801, 0.65456067558266373, 0.59535729418442773, 0.62185391132688528, 0.48715856672353258, 0.59301007915744519]\n",
      "ADAS mse: [56.927545053141998, 61.842489110058814, 50.010473877216, 44.49872880897572, 41.41432130555738, 57.724600780821397, 64.192613963956902, 54.932889072633152, 69.571359372325986, 55.943650221164319]\n",
      "ADAS means: 0.615482157834, 55.7058671566\n",
      "\n",
      "MMSE corr: [0.53560102994131198, 0.53783607970572322, 0.56125393699101023, 0.3734731614414144, 0.51062398885679006, 0.53916665295624422, 0.37364945410106415, 0.53593555571387896, 0.59132455131233097, 0.26723842605610371]\n",
      "MMSE mse: [5.4802581718318546, 5.4393839433541604, 4.9693249017171555, 6.3011575815964171, 4.6844098228680764, 4.9892709651263587, 6.3974993398537192, 5.3437775101983656, 5.2973846930804136, 7.3933675546251933]\n",
      "MMSE means: 0.482610283708, 5.62958344843\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 7, 2: 7, 3: 7, 4: 4, 5: 8, 6: 7, 7: 8, 8: 10, 9: 4, 10: 4}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.60441611384705707, 0.61343507085111137, 0.65181432624132818, 0.5803861159291841, 0.68465497262563535, 0.63180993076136127, 0.55792133962086554, 0.60313065128462195, 0.50154823310250141, 0.53233967412769723]\n",
      "ADAS mse: [56.927545053141998, 61.842489110058814, 55.642497865441669, 53.154446701524606, 44.144036747170176, 61.423811121695081, 67.752933194529945, 57.608352211170015, 73.866034507002382, 68.255192911107386]\n",
      "ADAS means: 0.596145642839, 60.0617339423\n",
      "\n",
      "MMSE corr: [0.53560102994131198, 0.53783607970572322, 0.57366663560743081, 0.49610945020045494, 0.54231115216923853, 0.54032881160914703, 0.42024908658188442, 0.53527354646540848, 0.61919734244051328, 0.44923922281558215]\n",
      "MMSE mse: [5.4802581718318546, 5.4393839433541604, 4.777773903837363, 5.2887312312954116, 4.5010535638620821, 4.8124231190054161, 6.2227885921561681, 5.3067616203050472, 5.0133223187986822, 6.034410272315804]\n",
      "MMSE means: 0.524981235754, 5.28769067368\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 7, 2: 7, 3: 12, 4: 18, 5: 8, 6: 11, 7: 8, 8: 18, 9: 8, 10: 19}\n",
      "Exp13_MC 3 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.67748939471890279, 0.70563263877885252, 0.5952121913397449, 0.66980937761830472, 0.58937453449069943, 0.62701459247054692, 0.57050159972342518, 0.58129843493242683, 0.68986997251182114, 0.65913567065064926]\n",
      "ADAS mse: [40.0427684969521, 39.122567151513486, 59.912573759823289, 54.238904444328369, 60.926751626941268, 57.181330756254525, 63.477709633107118, 58.87638483910203, 58.997735448784056, 44.700456346467625]\n",
      "ADAS means: 0.636533840724, 53.7477182503\n",
      "\n",
      "MMSE corr: [0.62908961101195648, 0.56297464322857327, 0.46587489128215998, 0.5460477288910589, 0.44492302779450288, 0.66732971513719896, 0.45095884011105331, 0.53280621517494642, 0.51228614533496197, 0.48765521913123422]\n",
      "MMSE mse: [4.7755468677653834, 4.9663229621024403, 5.8122273659162706, 4.8412114075239465, 5.9581982912768181, 4.1449091414463455, 5.75325162197759, 5.2624273885422319, 5.5841580616848328, 5.4128106777579275]\n",
      "MMSE means: 0.52999460371, 5.2511063786\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 17, 2: 6, 3: 3, 4: 19, 5: 5, 6: 16, 7: 16, 8: 10, 9: 15, 10: 6}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.67748939471890279, 0.70563263877885252, 0.5952121913397449, 0.66980937761830472, 0.58937453449069943, 0.62686871703043201, 0.57050159972342518, 0.58179530824707193, 0.68986997251182114, 0.65913567065064926]\n",
      "ADAS mse: [40.0427684969521, 39.122567151513486, 59.912573759823289, 54.238904444328369, 60.926751626941268, 57.132937309572078, 63.477709633107118, 58.853073228388503, 58.997735448784056, 44.700456346467625]\n",
      "ADAS means: 0.636568940511, 53.7405477446\n",
      "\n",
      "MMSE corr: [0.62908961101195648, 0.56297464322857327, 0.46587489128215998, 0.5460477288910589, 0.44492302779450288, 0.66346554491556331, 0.45095884011105331, 0.51994561777766768, 0.51228614533496197, 0.48765521913123422]\n",
      "MMSE mse: [4.7755468677653834, 4.9663229621024403, 5.8122273659162706, 4.8412114075239465, 5.9581982912768181, 4.2939743614497008, 5.75325162197759, 5.4583212103028815, 5.5841580616848328, 5.4128106777579275]\n",
      "MMSE means: 0.528322126948, 5.28560228278\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 17, 2: 6, 3: 3, 4: 19, 5: 5, 6: 14, 7: 16, 8: 16, 9: 15, 10: 6}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.66015752946243678, 0.67979524821437909, 0.58677081766685812, 0.64601863828907968, 0.57205852719003358, 0.62701459247054692, 0.57170681455688466, 0.58129843493242683, 0.69331130395113083, 0.65009860325788915]\n",
      "ADAS mse: [43.417176739895332, 42.565602898927295, 63.369381082162683, 61.166266607863761, 64.269686434892222, 57.181330756254525, 63.695175831724661, 58.87638483910203, 59.408561626440985, 49.350744799043703]\n",
      "ADAS means: 0.626823050999, 56.3300311616\n",
      "\n",
      "MMSE corr: [0.63013678922987049, 0.5742369777061056, 0.50688995641851287, 0.55641459135581806, 0.47010178651699946, 0.66732971513719896, 0.45123948352176613, 0.53280621517494642, 0.51971481063734681, 0.52252006326840006]\n",
      "MMSE mse: [4.6867087605929267, 4.8894422672001117, 5.5015737322710399, 4.7158225338784856, 5.792934963818448, 4.1449091414463455, 5.7449492114527816, 5.2624273885422319, 5.5380955748943581, 5.252688137172532]\n",
      "MMSE means: 0.543139038897, 5.15295517113\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {1: 8, 2: 12, 3: 15, 4: 14, 5: 9, 6: 16, 7: 17, 8: 10, 9: 19, 10: 17}\n",
      "Exp13_MC 4 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.62157171659335464, 0.59754105500158394, 0.62786665063909575, 0.57166488801360249, 0.65507183317866968, 0.6060348401155996, 0.72806707026266193, 0.67333231602101162, 0.63492381459372105, 0.5919854450494535]\n",
      "ADAS mse: [59.858618066754332, 51.235160668549803, 63.842654364469311, 65.14862214951448, 50.723210676300582, 56.201573349479574, 41.764574127130793, 50.356792771384669, 49.588214674802053, 54.289428046080083]\n",
      "ADAS means: 0.630805962947, 54.3008848894\n",
      "\n",
      "MMSE corr: [0.61799723918209049, 0.39249103689300086, 0.51354047352637833, 0.41468478345829851, 0.57214862209928641, 0.58583844379998173, 0.55705398391379191, 0.59012418556041624, 0.49379501997442615, 0.50848613262317266]\n",
      "MMSE mse: [4.62593941925648, 5.6273016894924632, 5.1490721944394773, 5.8741007598820056, 5.3462909221802182, 5.307750431847313, 5.6541286939615532, 5.3768547982293313, 5.0870326740390244, 4.9309954445697146]\n",
      "MMSE means: 0.524615992103, 5.29794670279\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 19, 2: 5, 3: 10, 4: 7, 5: 18, 6: 19, 7: 19, 8: 13, 9: 15, 10: 13}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.62157171659335464, 0.60076218352997579, 0.62786665063909575, 0.57166488801360249, 0.65507183317866968, 0.6060348401155996, 0.72806707026266193, 0.67120334180713681, 0.63492381459372105, 0.5919854450494535]\n",
      "ADAS mse: [59.858618066754332, 51.172504988012172, 63.842654364469311, 65.14862214951448, 50.723210676300582, 56.201573349479574, 41.764574127130793, 50.181884354232245, 49.588214674802053, 54.289428046080083]\n",
      "ADAS means: 0.630915178378, 54.2771284797\n",
      "\n",
      "MMSE corr: [0.61799723918209049, 0.41192223059587141, 0.51354047352637833, 0.41468478345829851, 0.57214862209928641, 0.58583844379998173, 0.55705398391379191, 0.55904554163288067, 0.49379501997442615, 0.50848613262317266]\n",
      "MMSE mse: [4.62593941925648, 5.7437360316073773, 5.1490721944394773, 5.8741007598820056, 5.3462909221802182, 5.307750431847313, 5.6541286939615532, 5.8483988924246875, 5.0870326740390244, 4.9309954445697146]\n",
      "MMSE means: 0.523451247081, 5.35674454642\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 19, 2: 6, 3: 10, 4: 7, 5: 18, 6: 19, 7: 19, 8: 7, 9: 15, 10: 13}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.62036287045495475, 0.61007594604270576, 0.60244650546755474, 0.55456930095421686, 0.65198199176681548, 0.60296860447682332, 0.70854781105603004, 0.67333231602101162, 0.62441535577256946, 0.59074091797123529]\n",
      "ADAS mse: [60.186995503171516, 51.496711638151233, 67.307927276308234, 68.146005111015569, 52.657702003950483, 56.436708283677156, 44.17504627666424, 50.356792771384669, 50.750794041865468, 54.829426636972443]\n",
      "ADAS means: 0.623944161998, 55.6344109543\n",
      "\n",
      "MMSE corr: [0.61582182882297543, 0.44577426434220502, 0.53246394320880441, 0.46666350148587021, 0.57390619044769176, 0.58967848835690206, 0.56787743618687792, 0.59012418556041624, 0.4965441908437509, 0.51914160788064079]\n",
      "MMSE mse: [4.6058410129174803, 5.5120172339240154, 4.8879963086805276, 5.5163726451061654, 5.3460762977193967, 5.1225487240854175, 5.5301051408163699, 5.3768547982293313, 5.0351486565846137, 4.8865236051154044]\n",
      "MMSE means: 0.539799563714, 5.18194844232\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 18, 2: 16, 3: 16, 4: 16, 5: 15, 6: 9, 7: 16, 8: 13, 9: 10, 10: 17}\n",
      "Exp13_MC 5 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.61360249309437631, 0.64040047484152363, 0.67984786337953451, 0.68405198285073365, 0.59024016364602083, 0.59950766721259785, 0.56270066318652479, 0.56210352856020418, 0.63313533638218145, 0.73471390000223702]\n",
      "ADAS mse: [53.772107509490063, 47.646646564225868, 47.639466201948316, 53.878944176868039, 49.228077137957079, 66.237013754197349, 61.605093037212008, 64.555187246959832, 59.290946527003591, 40.842164451339663]\n",
      "ADAS means: 0.630030407316, 54.4695646607\n",
      "\n",
      "MMSE corr: [0.6252425376488514, 0.40879130981463535, 0.58638641255364221, 0.60197908631269115, 0.49985893401025627, 0.44236426826447994, 0.4783047148977026, 0.47107508216516825, 0.61218837775948809, 0.47225500454261155]\n",
      "MMSE mse: [4.5105363024995313, 6.1846056452623746, 4.361543898566052, 4.7004239200162701, 5.272171375081558, 6.1833991219284465, 5.464045638237228, 5.8049277812970921, 5.3430780792163874, 5.2276674477138192]\n",
      "MMSE means: 0.519844572797, 5.30523992098\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {1: 19, 2: 6, 3: 19, 4: 19, 5: 12, 6: 19, 7: 8, 8: 7, 9: 12, 10: 19}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.61360249309437631, 0.64040047484152363, 0.67984786337953451, 0.68405198285073365, 0.59024016364602083, 0.59950766721259785, 0.56270066318652479, 0.56210352856020418, 0.63313533638218145, 0.73471390000223702]\n",
      "ADAS mse: [53.772107509490063, 47.646646564225868, 47.639466201948316, 53.878944176868039, 49.228077137957079, 66.237013754197349, 61.605093037212008, 64.555187246959832, 59.290946527003591, 40.842164451339663]\n",
      "ADAS means: 0.630030407316, 54.4695646607\n",
      "\n",
      "MMSE corr: [0.6252425376488514, 0.40879130981463535, 0.58638641255364221, 0.60197908631269115, 0.49985893401025627, 0.44236426826447994, 0.4783047148977026, 0.47107508216516825, 0.61218837775948809, 0.47225500454261155]\n",
      "MMSE mse: [4.5105363024995313, 6.1846056452623746, 4.361543898566052, 4.7004239200162701, 5.272171375081558, 6.1833991219284465, 5.464045638237228, 5.8049277812970921, 5.3430780792163874, 5.2276674477138192]\n",
      "MMSE means: 0.519844572797, 5.30523992098\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {1: 19, 2: 6, 3: 19, 4: 19, 5: 12, 6: 19, 7: 8, 8: 7, 9: 12, 10: 19}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.58446120653472045, 0.65162690417798408, 0.67984786337953451, 0.65977737443567708, 0.59024016364602083, 0.59950766721259785, 0.55156764961853399, 0.55293693527695875, 0.63313533638218145, 0.72525972980512754]\n",
      "ADAS mse: [57.714298999111683, 50.905020405467759, 47.639466201948316, 58.331534206838484, 49.228077137957079, 66.237013754197349, 63.659576770121276, 66.046399087901108, 59.290946527003591, 43.687697380464648]\n",
      "ADAS means: 0.622836083047, 56.2740030471\n",
      "\n",
      "MMSE corr: [0.63502201130075409, 0.50598269758657399, 0.58638641255364221, 0.60160249364785667, 0.49985893401025627, 0.44236426826447994, 0.4944420461950485, 0.47015903234932188, 0.61218837775948809, 0.47301923392614309]\n",
      "MMSE mse: [4.4205154359111187, 5.7392482024712024, 4.361543898566052, 4.6529939591115701, 5.272171375081558, 6.1833991219284465, 5.3701256414263012, 5.7374225811921349, 5.3430780792163874, 5.1779883468685659]\n",
      "MMSE means: 0.532102550759, 5.22584866418\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 13, 2: 19, 3: 19, 4: 6, 5: 12, 6: 19, 7: 13, 8: 6, 9: 12, 10: 9}\n",
      "Exp13_MC 6 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.5430729574825548, 0.67472011428756062, 0.6286977433577664, 0.66575386505547174, 0.66577063302355466, 0.66129383222036642, 0.62345159890788571, 0.62180399052463808, 0.69000238979904671, 0.57054986587253753]\n",
      "ADAS mse: [65.140716841294477, 48.143897224565443, 46.40258886433854, 50.637697750593759, 58.266189476165948, 51.358570596226492, 60.931033880352103, 61.844299267953978, 39.190635825664089, 58.813500343866764]\n",
      "ADAS means: 0.634511699053, 54.0729130071\n",
      "\n",
      "MMSE corr: [0.49565135362047952, 0.56840264614474723, 0.4671921950980174, 0.51868441683142719, 0.62054215068464202, 0.53255584377873533, 0.50188963591188218, 0.49467631803939111, 0.54348842605329051, 0.52155520120679288]\n",
      "MMSE mse: [5.391382913538397, 4.3493794506950136, 5.4568643617373409, 5.6601355304058014, 5.0779228185611647, 4.9530951974466273, 5.6946576995238747, 6.3621385448517032, 5.2374705448740118, 5.0965834601598834]\n",
      "MMSE means: 0.526463818737, 5.32796305218\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {1: 4, 2: 12, 3: 19, 4: 7, 5: 10, 6: 10, 7: 15, 8: 18, 9: 13, 10: 19}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.5430729574825548, 0.67472011428756062, 0.6286977433577664, 0.66575386505547174, 0.66577063302355466, 0.65956215841967059, 0.62345159890788571, 0.62180399052463808, 0.69000238979904671, 0.57054986587253753]\n",
      "ADAS mse: [65.140716841294477, 48.143897224565443, 46.40258886433854, 50.637697750593759, 58.266189476165948, 51.301888711690346, 60.931033880352103, 61.844299267953978, 39.190635825664089, 58.813500343866764]\n",
      "ADAS means: 0.634338531673, 54.0672448186\n",
      "\n",
      "MMSE corr: [0.49565135362047952, 0.56840264614474723, 0.4671921950980174, 0.51868441683142719, 0.62054215068464202, 0.52204712856266278, 0.50188963591188218, 0.49467631803939111, 0.54348842605329051, 0.52155520120679288]\n",
      "MMSE mse: [5.391382913538397, 4.3493794506950136, 5.4568643617373409, 5.6601355304058014, 5.0779228185611647, 5.1842681861926661, 5.6946576995238747, 6.3621385448517032, 5.2374705448740118, 5.0965834601598834]\n",
      "MMSE means: 0.525412947215, 5.35108035105\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {1: 4, 2: 12, 3: 19, 4: 7, 5: 10, 6: 7, 7: 15, 8: 18, 9: 13, 10: 19}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.54568108711480057, 0.67472011428756062, 0.6286977433577664, 0.65742484433269488, 0.65841659953592679, 0.6598674469070086, 0.61997449251277048, 0.6128629372189871, 0.68918312074385746, 0.55320834733039936]\n",
      "ADAS mse: [68.224908557790698, 48.143897224565443, 46.40258886433854, 51.990440571237997, 59.06075357710359, 52.218525043201261, 61.353577295814205, 62.94904810898781, 42.256335220620677, 61.659317316434723]\n",
      "ADAS means: 0.630003673334, 55.425939178\n",
      "\n",
      "MMSE corr: [0.49932376366939407, 0.56840264614474723, 0.4671921950980174, 0.54458450115692247, 0.62104054386959717, 0.55081297866573098, 0.49736580738085567, 0.49179570767901659, 0.56020765074139744, 0.53974026016962717]\n",
      "MMSE mse: [5.3585956110147377, 4.3493794506950136, 5.4568643617373409, 5.3645068087040855, 4.9608433582259108, 4.7443052955304132, 5.6758479313728509, 6.2491052439418127, 5.0802992849356468, 4.9161055500718573]\n",
      "MMSE means: 0.534046605458, 5.21558528962\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 5, 2: 12, 3: 19, 4: 19, 5: 14, 6: 19, 7: 17, 8: 8, 9: 19, 10: 19}\n",
      "Exp13_MC 7 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.61928563701985961, 0.6233165550280817, 0.67774341373136671, 0.66874173981446483, 0.63210772864116749, 0.64316937686741937, 0.58380553326176032, 0.60842771390749772, 0.58935561850431739, 0.62818730135890577]\n",
      "ADAS mse: [49.870731637902388, 54.197790414192163, 49.253012603405807, 54.667943710566412, 51.308217043015524, 53.199279636970687, 60.426781473500483, 58.902234887284187, 58.394378097337416, 56.7377003690059]\n",
      "ADAS means: 0.627414061813, 54.6958069873\n",
      "\n",
      "MMSE corr: [0.48717907848163911, 0.45567682689660077, 0.609078523441415, 0.5495562172659727, 0.57039905845007621, 0.57800805419604062, 0.53150426576437504, 0.50732729158596312, 0.52522401531626273, 0.45309180088387369]\n",
      "MMSE mse: [5.2062491890782496, 6.0456548914508605, 4.7355624833345393, 5.4834662359004192, 4.5161720930168654, 5.0767083280031651, 4.8024716349089216, 5.9498342774646487, 5.1779826520548538, 6.3695986878617683]\n",
      "MMSE means: 0.526704513228, 5.33637004731\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {1: 6, 2: 8, 3: 19, 4: 15, 5: 18, 6: 18, 7: 5, 8: 17, 9: 14, 10: 17}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.61928563701985961, 0.6259379465185646, 0.67677705249949838, 0.66874173981446483, 0.63210772864116749, 0.64316937686741937, 0.58380553326176032, 0.60842771390749772, 0.58935561850431739, 0.62818730135890577]\n",
      "ADAS mse: [49.870731637902388, 54.097520897373045, 49.136114749783381, 54.667943710566412, 51.308217043015524, 53.199279636970687, 60.426781473500483, 58.902234887284187, 58.394378097337416, 56.7377003690059]\n",
      "ADAS means: 0.627579564839, 54.6740902503\n",
      "\n",
      "MMSE corr: [0.48717907848163911, 0.46734101919327375, 0.59995211921332037, 0.5495562172659727, 0.57039905845007621, 0.57800805419604062, 0.53150426576437504, 0.50732729158596312, 0.52522401531626273, 0.45309180088387369]\n",
      "MMSE mse: [5.2062491890782496, 6.1488131166611506, 4.866698351805046, 5.4834662359004192, 4.5161720930168654, 5.0767083280031651, 4.8024716349089216, 5.9498342774646487, 5.1779826520548538, 6.3695986878617683]\n",
      "MMSE means: 0.526958292035, 5.35979945668\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {1: 6, 2: 10, 3: 15, 4: 15, 5: 18, 6: 18, 7: 5, 8: 17, 9: 14, 10: 17}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.55851106114039117, 0.62635398140594933, 0.66706604997704322, 0.65789070582800802, 0.588753549433217, 0.63865027066162627, 0.58163641572077629, 0.61064384600908561, 0.5846968013678191, 0.60355192205855479]\n",
      "ADAS mse: [62.257442989751425, 54.82011502968939, 52.177689451901387, 56.700341192293422, 56.820166331043957, 53.870262979090015, 61.88969396055915, 59.593292374539914, 58.857290320971885, 60.722439896628458]\n",
      "ADAS means: 0.61177546036, 57.7708734526\n",
      "\n",
      "MMSE corr: [0.4814931786986289, 0.48096668681785126, 0.60526420881016096, 0.55531531466913597, 0.57439306377548771, 0.58190705966081813, 0.54870067860564864, 0.51073253879141334, 0.5128722043841355, 0.46805662544061188]\n",
      "MMSE mse: [4.8911057223039078, 5.9633802278928236, 4.6752765959365892, 5.4306432404607694, 4.4523945937133007, 5.0447649886690238, 4.6569437301151799, 5.8794825908760737, 5.1165151295825027, 6.1223287565834541]\n",
      "MMSE means: 0.531970155965, 5.22328355761\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 3, 2: 19, 3: 12, 4: 10, 5: 18, 6: 12, 7: 8, 8: 19, 9: 13, 10: 19}\n",
      "Exp13_MC 8 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.58225223914378688, 0.69937226407048814, 0.70988823183109118, 0.6500035240613834, 0.44885610654653846, 0.64500413467858353, 0.61970037968015435, 0.71862669240716415, 0.58406784244363352, 0.50490627628793061]\n",
      "ADAS mse: [63.751545189139193, 42.906135937329225, 50.802568929016331, 58.836369309464459, 56.147863961339766, 54.218182908234787, 53.837203179928487, 47.786514345448801, 50.277649077783188, 67.373377397909522]\n",
      "ADAS means: 0.616267769115, 54.5937410236\n",
      "\n",
      "MMSE corr: [0.46673057975880539, 0.59978862832698265, 0.61789636823178096, 0.55951516303276094, 0.33949101485918742, 0.5712614591141274, 0.36121454629416089, 0.64421737934876266, 0.46338142654979847, 0.4790610905048916]\n",
      "MMSE mse: [6.2201017629661779, 4.8106180768940305, 4.626192721196829, 5.7282201902236167, 6.2424821830876427, 5.0968502046173798, 6.7759883929445648, 4.1200119714918779, 5.7975961457227809, 4.5664374606731828]\n",
      "MMSE means: 0.510255765602, 5.39844991098\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 19, 2: 19, 3: 19, 4: 10, 5: 4, 6: 5, 7: 9, 8: 19, 9: 5, 10: 7}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.58225223914378688, 0.69937226407048814, 0.70988823183109118, 0.6500035240613834, 0.44885610654653846, 0.64500413467858353, 0.61970037968015435, 0.71862669240716415, 0.58406784244363352, 0.50490627628793061]\n",
      "ADAS mse: [63.751545189139193, 42.906135937329225, 50.802568929016331, 58.836369309464459, 56.147863961339766, 54.218182908234787, 53.837203179928487, 47.786514345448801, 50.277649077783188, 67.373377397909522]\n",
      "ADAS means: 0.616267769115, 54.5937410236\n",
      "\n",
      "MMSE corr: [0.46673057975880539, 0.59978862832698265, 0.61789636823178096, 0.55951516303276094, 0.33949101485918742, 0.5712614591141274, 0.36121454629416089, 0.64421737934876266, 0.46338142654979847, 0.4790610905048916]\n",
      "MMSE mse: [6.2201017629661779, 4.8106180768940305, 4.626192721196829, 5.7282201902236167, 6.2424821830876427, 5.0968502046173798, 6.7759883929445648, 4.1200119714918779, 5.7975961457227809, 4.5664374606731828]\n",
      "MMSE means: 0.510255765602, 5.39844991098\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 19, 2: 19, 3: 19, 4: 10, 5: 4, 6: 5, 7: 9, 8: 19, 9: 5, 10: 7}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.56661705473816415, 0.69411910891570361, 0.70667915090348243, 0.65340830212607803, 0.48768573666204196, 0.64219405631217819, 0.57203813247423063, 0.71862669240716415, 0.57733702305468781, 0.48152648611334908]\n",
      "ADAS mse: [69.064219353043129, 45.938386639478026, 51.339761561324202, 59.785001776610855, 59.047461406051895, 55.502231314327233, 60.817504170781547, 47.786514345448801, 54.847459097743638, 71.415536173687499]\n",
      "ADAS means: 0.610023174371, 57.5544075838\n",
      "\n",
      "MMSE corr: [0.48793529690841925, 0.60097800537151769, 0.61885942193182997, 0.55580533300448554, 0.39507590162597128, 0.60225489280218136, 0.41080804349982114, 0.64421737934876266, 0.52247229795222705, 0.50980449883559509]\n",
      "MMSE mse: [5.9974726134298066, 4.7968702417211206, 4.5912051342182671, 5.7278687158584045, 6.0334185332981942, 4.7878159145941881, 6.1938068880516592, 4.1200119714918779, 5.4351937655575107, 4.2666010050709158]\n",
      "MMSE means: 0.534821107128, 5.19502647833\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 16, 2: 17, 3: 17, 4: 9, 5: 17, 6: 11, 7: 18, 8: 19, 9: 19, 10: 16}\n",
      "Exp13_MC 9 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.60179307717377262, 0.60937226449327586, 0.59435606703056376, 0.64085140894509796, 0.68442890124683331, 0.66060971452500261, 0.63802554046438564, 0.59498411679012553, 0.67013752372846203, 0.6334416539341774]\n",
      "ADAS mse: [55.428818369237369, 66.334251961626322, 66.26094048177886, 44.593289578674387, 39.416935319466624, 60.571629514669134, 44.900467359893874, 57.552376645020097, 52.312576235067255, 54.399630507536024]\n",
      "ADAS means: 0.632800026833, 54.1770915973\n",
      "\n",
      "MMSE corr: [0.50876856747335031, 0.55680571448282212, 0.53229432640468533, 0.56406061017077269, 0.58385905890557521, 0.61997672908198864, 0.40328464027702449, 0.4678087626121103, 0.56020414668457563, 0.51100110507338292]\n",
      "MMSE mse: [4.9049158300428948, 6.4362570406286768, 5.4261534359794901, 3.9685906188336637, 4.361136845996926, 4.6316304644539263, 6.1130123086602284, 6.0164741950244043, 5.3941470364420123, 5.2448633883232789]\n",
      "MMSE means: 0.530806366117, 5.24971811644\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 19, 2: 17, 3: 19, 4: 13, 5: 11, 6: 19, 7: 3, 8: 18, 9: 8, 10: 6}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.60179307717377262, 0.60937226449327586, 0.59435606703056376, 0.64085140894509796, 0.67184331363149619, 0.66060971452500261, 0.63802554046438564, 0.59323265591707119, 0.67013752372846203, 0.63273227813622013]\n",
      "ADAS mse: [55.428818369237369, 66.334251961626322, 66.26094048177886, 44.593289578674387, 39.093356334958415, 60.571629514669134, 44.900467359893874, 57.528606701933057, 52.312576235067255, 54.307437138481333]\n",
      "ADAS means: 0.631295384405, 54.1331373676\n",
      "\n",
      "MMSE corr: [0.50876856747335031, 0.55680571448282212, 0.53229432640468533, 0.56406061017077269, 0.56319188512469753, 0.61997672908198864, 0.40328464027702449, 0.46713169105798624, 0.56020414668457563, 0.48624327856858574]\n",
      "MMSE mse: [4.9049158300428948, 6.4362570406286768, 5.4261534359794901, 3.9685906188336637, 4.8947151739357517, 4.6316304644539263, 6.1130123086602284, 6.0488852785825085, 5.3941470364420123, 5.3643695582710977]\n",
      "MMSE means: 0.526196158933, 5.31826767458\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 19, 2: 17, 3: 19, 4: 13, 5: 4, 6: 19, 7: 3, 8: 17, 9: 8, 10: 4}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.60179307717377262, 0.61028656666612913, 0.58511116905444016, 0.60137059496787382, 0.67774379203411961, 0.64064594471752923, 0.63056305582261829, 0.59050594902901621, 0.66786304075825398, 0.60184649632055154]\n",
      "ADAS mse: [55.428818369237369, 67.339057473251344, 67.672718809559782, 48.952827240711912, 40.447426052422685, 64.003305535163747, 46.907582874070599, 59.719666614149297, 53.691370414097307, 60.386436308675187]\n",
      "ADAS means: 0.620772968654, 56.4549209691\n",
      "\n",
      "MMSE corr: [0.50876856747335031, 0.56026926031273838, 0.5351180096175111, 0.57070835457966607, 0.59378710528512069, 0.62318936589521912, 0.43341469962179074, 0.4651578269145667, 0.54988563461582818, 0.55163676695432851]\n",
      "MMSE mse: [4.9049158300428948, 6.3971934211332302, 5.3763920577222724, 3.9193234835674557, 4.2181665977067144, 4.6104761566366506, 5.9219530696651121, 5.8513567792883734, 5.3734927281202136, 4.9204784605885523]\n",
      "MMSE means: 0.539193559127, 5.14937485845\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 19, 2: 18, 3: 10, 4: 14, 5: 18, 6: 12, 7: 6, 8: 11, 9: 7, 10: 18}\n",
      "Exp13_MC 10 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.63699169915385112, 0.6975779930253122, 0.60868948710547954, 0.65830111813013681, 0.59080894332267297, 0.66559139069897, 0.56833142500958733, 0.64433793330124922, 0.60607731297191336, 0.61029430073566149]\n",
      "ADAS mse: [57.819338650312062, 42.816983418129531, 61.460846398298756, 48.997677229472217, 50.424716286914176, 59.822284201191685, 54.316696034459369, 52.12366723283801, 57.934524262631207, 54.703947348659334]\n",
      "ADAS means: 0.628700160345, 54.0420681063\n",
      "\n",
      "MMSE corr: [0.54317479808062141, 0.59531800111499145, 0.52667527984710572, 0.54461368057340076, 0.42837430526194603, 0.6797195815568724, 0.45898898624885642, 0.470859140265939, 0.47138934769394319, 0.5727546018966243]\n",
      "MMSE mse: [5.1353501570466511, 4.4971769403735751, 5.6624711917510977, 5.6315306535462915, 5.8785116051510764, 4.6954497587271096, 5.2260624452084894, 5.7894785496665913, 5.885652499728085, 4.5074127673927125]\n",
      "MMSE means: 0.529186772254, 5.29090965686\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 14, 2: 18, 3: 11, 4: 16, 5: 5, 6: 11, 7: 7, 8: 5, 9: 8, 10: 9}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.63699169915385112, 0.6975779930253122, 0.60868948710547954, 0.65632964216723366, 0.59080894332267297, 0.66559139069897, 0.56833142500958733, 0.64433793330124922, 0.60403848745671807, 0.61029430073566149]\n",
      "ADAS mse: [57.819338650312062, 42.816983418129531, 61.460846398298756, 48.775024300098416, 50.424716286914176, 59.822284201191685, 54.316696034459369, 52.12366723283801, 57.891627747268544, 54.703947348659334]\n",
      "ADAS means: 0.628299130198, 54.0155131618\n",
      "\n",
      "MMSE corr: [0.54317479808062141, 0.59531800111499145, 0.52667527984710572, 0.53550037400654837, 0.42837430526194603, 0.6797195815568724, 0.45898898624885642, 0.470859140265939, 0.46555322286295914, 0.5727546018966243]\n",
      "MMSE mse: [5.1353501570466511, 4.4971769403735751, 5.6624711917510977, 5.9899930058476141, 5.8785116051510764, 4.6954497587271096, 5.2260624452084894, 5.7894785496665913, 5.9830927490672767, 4.5074127673927125]\n",
      "MMSE means: 0.527691829114, 5.33649991702\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}}\n",
      "\n",
      "opt_snap: {1: 14, 2: 18, 3: 11, 4: 8, 5: 5, 6: 11, 7: 7, 8: 5, 9: 7, 10: 9}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.58284944776830427, 0.6915421531603908, 0.59557423550866662, 0.63458136335260007, 0.59109487360835877, 0.66388108667683388, 0.57860108994702353, 0.63820983273212395, 0.60726812471987102, 0.60585885445778009]\n",
      "ADAS mse: [64.671503468771405, 43.50061958660222, 63.338318602991869, 50.744865505508827, 53.625757438816095, 60.062817586943822, 55.686025255118786, 55.497192157450222, 58.259036119688652, 55.421534644040086]\n",
      "ADAS means: 0.618946106193, 56.0807670366\n",
      "\n",
      "MMSE corr: [0.56850405708453489, 0.59729713135961315, 0.53386782531845534, 0.53064953980934515, 0.45052368674734472, 0.68517554500747957, 0.51454382959558131, 0.50270441255033094, 0.4899867707697097, 0.59109352603457777]\n",
      "MMSE mse: [4.9172360399906134, 4.4479725295675143, 5.458746309193832, 5.5589838910824376, 5.4812489114746361, 4.6097770497926831, 4.9337963968122382, 5.5749367004322155, 5.6974420374183525, 4.2913755150039474]\n",
      "MMSE means: 0.546434632428, 5.09715153808\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 6}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {1: 15, 2: 13, 3: 7, 4: 5, 5: 17, 6: 17, 7: 9, 8: 15, 9: 16, 10: 19}\n"
     ]
    }
   ],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "#exp_name = 'Exp11'\n",
    "#niter = 20000\n",
    "# modality = 'HC_CT'\n",
    "start_MC=1\n",
    "n_MC=10\n",
    "MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "start_fold = 1\n",
    "n_folds = 10\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "#preproc = 'no_preproc'\n",
    "#batch_size = 256\n",
    "snap_interval = 4000\n",
    "snap_start = 4000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "#Clinical_Scale = 'ADAS13'\n",
    "\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "idx = 0  \n",
    "df_perf_dict_adas = {}\n",
    "df_perf_dict_mmse = {}\n",
    "df_perf_dict_adas_tuned = {}\n",
    "df_perf_dict_mmse_tuned = {}\n",
    "df_perf_dict_opt = {}\n",
    "\n",
    "for mc in MC_list:\n",
    "    print exp_name, mc, modality\n",
    "\n",
    "    for hype in hype_configs.keys():      \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "\n",
    "        for fid in fid_list:\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "            #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "            #with open(test_filename_txt, 'w') as f:\n",
    "            #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                    f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC']\n",
    "                elif modality == 'CT':\n",
    "                    f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_CT_SpecCluster_dyn']\n",
    "                elif modality == 'HC_CT':\n",
    "                    f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC','X_CT_SpecCluster_dyn']\n",
    "                elif modality == 'HC_CT_unified_hyp1':\n",
    "                    f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_HC_CT']\n",
    "                else:\n",
    "                    print 'Wrong modality'\n",
    "\n",
    "            #print 'Hype # {}, MC # {}, Fold # {}, Clinical_Scale {}'.format(hype, mc, fid, Clinical_Scale)\n",
    "            data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            if Clinical_Scale == 'ADAS13':\n",
    "                act_scores = load_data(data_path, 'adas','no_preproc')\n",
    "            elif Clinical_Scale == 'MMSE': \n",
    "                act_scores = load_data(data_path, 'mmse','no_preproc')\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                act_scores_adas13 = load_data(data_path, 'adas','no_preproc')\n",
    "                act_scores_mmse = load_data(data_path, 'mmse','no_preproc')\n",
    "            else:\n",
    "                print 'unknown clinical scale'\n",
    "\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort)\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            sub.call([\"cp\", baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort), baseline_dir + \n",
    "                      'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)])\n",
    "            #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "            #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "\n",
    "            if Clinical_Scale in ['ADAS13', 'MMSE']:                \n",
    "                multi_task = False\n",
    "                cs_list = ['opt']\n",
    "                iter_euLoss = []\n",
    "                iter_r = []        \n",
    "                iter_pred_scores = []\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = np.squeeze(results['X_out'])            \n",
    "                    iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                    iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "                fold_r[config_idx] = np.array(iter_r)\n",
    "                fold_act_scores[fid] = act_scores\n",
    "                fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                multi_task = True\n",
    "                cs_list = ['adas','mmse']\n",
    "                iter_euLoss_adas13 = []\n",
    "                iter_r_adas13 = []        \n",
    "                iter_pred_scores_adas13 = []\n",
    "                iter_euLoss_mmse = []\n",
    "                iter_r_mmse = []        \n",
    "                iter_pred_scores_mmse = []\n",
    "\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = results['X_out']   \n",
    "                    encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                    encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                    iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                    iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                    iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                    iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "                fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "                fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "                fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "                fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "                fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "                fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "                fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "\n",
    "    \n",
    "    if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "        fold_perf_dict = {'fold_r':fold_r,'fold_euLoss':fold_euLoss,'fold_act_scores':fold_act_scores,'fold_pred_scores':fold_pred_scores}\n",
    "    else: \n",
    "        fold_perf_dict = {'fold_r_adas13':fold_r_adas13,'fold_r_mmse':fold_r_mmse,'fold_euLoss_adas13':fold_euLoss_adas13,'fold_euLoss_mmse':fold_euLoss_mmse,\n",
    "                         'fold_act_scores_adas13':fold_act_scores_adas13,'fold_act_scores_mmse':fold_act_scores_mmse,\n",
    "                          'fold_pred_scores_adas13':fold_pred_scores_adas13,'fold_pred_scores_mmse':fold_pred_scores_mmse}\n",
    "        \n",
    "    opt_metric = 'euLoss' #euLoss or corr or both\n",
    "    #task_weights = {'ADAS':1,'MMSE':0} \n",
    "    save_multitask_results = False\n",
    "    CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/'  \n",
    "    save_path = '{}{}_{}_NN_{}'.format(CV_model_dir,exp_name,mc,modality)\n",
    "    \n",
    "    # Create dictionaries of perfromance based on differnt task weights \n",
    "    for key, task_weights in {'adas':{'ADAS':1,'MMSE':0},'mmse':{'ADAS':0,'MMSE':1},'both':{'ADAS':1,'MMSE':1}}.items():\n",
    "        print 'Weights Tuned for: {}'.format(key)                              \n",
    "        results = compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path)  \n",
    "        adas_pred_scores = results['adas_pred_CV_scores']\n",
    "        adas_act_scores = results['adas_act_CV_scores']\n",
    "        mmse_pred_scores = results['mmse_pred_CV_scores']\n",
    "        mmse_act_scores = results['mmse_act_CV_scores']\n",
    "        \n",
    "        # populate the perf dictionary for 10 MC x 10 folds\n",
    "        model_choice = 'APANN'    \n",
    "        for f, fid in enumerate(fid_list): \n",
    "            if key in ['adas','mmse']:\n",
    "                r_valid = results['{}_r'.format(key)][f]\n",
    "                MSE_valid = results['{}_mse'.format(key)][f]\n",
    "                RMSE_valid = results['{}_rmse'.format(key)][f]\n",
    "                \n",
    "\n",
    "                if key == 'adas':\n",
    "                    df_perf_dict_adas_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                                    'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0,\n",
    "                                                    'pred_scores':adas_pred_scores, 'act_scores':adas_act_scores}\n",
    "                elif key == 'mmse':\n",
    "                    df_perf_dict_mmse_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                                    'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0,\n",
    "                                                    'pred_scores':mmse_pred_scores, 'act_scores':mmse_act_scores}\n",
    "                else: \n",
    "                    print 'unknown key'\n",
    "\n",
    "            elif key == 'both':                    \n",
    "                for cs in cs_list:            \n",
    "                    r_valid = results['{}_r'.format(cs)][f]\n",
    "                    MSE_valid = results['{}_mse'.format(cs)][f]\n",
    "                    RMSE_valid = results['{}_rmse'.format(cs)][f]\n",
    "\n",
    "                    if cs == 'adas':\n",
    "                        df_perf_dict_adas[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                                  'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0,\n",
    "                                                  'pred_scores':adas_pred_scores, 'act_scores':adas_act_scores}\n",
    "                    elif cs == 'mmse':\n",
    "                        df_perf_dict_mmse[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                                  'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0,\n",
    "                                                  'pred_scores':mmse_pred_scores, 'act_scores':mmse_act_scores}  \n",
    "                    else: \n",
    "                        print 'unknown key'\n",
    "\n",
    "\n",
    "            else: #single task dictionary\n",
    "                df_perf_dict_opt[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                \n",
    "\n",
    "            idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.19165413   7.69251163   4.17063713  -3.10327045  -2.14980698\n",
      "  10.57621574  -1.15747173  -4.83323853  -6.83380318   8.38311577\n",
      "   9.55613224  -6.59080124   7.21377563   1.60269936  14.48505211\n",
      "   0.56687355  -3.14930813  -0.40178482  -8.33746147   8.44085312\n",
      "   1.70791348  -4.32915298   1.81736938  -6.0538121   -0.60653496\n",
      "   5.22998428   2.5357093    0.42538158  11.50906364  -6.96784218\n",
      " -19.69970894  -2.13622673   5.93899734  10.40242489   1.36518185\n",
      "  -0.51688202   8.01778595 -19.82795723  -8.83472355  -1.93143265\n",
      "   1.81590454 -15.35012436  -1.74704552   6.89324284  14.00945091\n",
      " -11.23031624 -15.04920292  -8.98065083  -6.33333199  10.14040955\n",
      "   6.93676758   1.70050232  -7.15806492  -1.638218    -6.45249836\n",
      "  -2.77902977   3.77913094  -0.15781013   4.1582814   -9.81236267\n",
      "   1.04980095  -4.55159958  -3.78840446  16.3254718    2.83596428\n",
      "   3.84368324  15.03104584   1.22026627 -15.21646309 -15.39570713\n",
      "  -3.44275761   8.76411438   6.71894646   0.52087593   6.51832199\n",
      "  -0.12314606  -4.85672092  -5.64736557  -8.82988787  -2.49696732\n",
      "   2.45319748 -13.0667305   -3.41227531   4.17386055 -11.59353542\n",
      "  -1.87021065 -11.44768524   7.08463955   3.88779163   7.33945084\n",
      "  -6.01238632 -15.11613178  10.88659477   3.71912193  -6.38169861\n",
      "   1.06531715   8.70314503   3.54639816  21.35590744  -5.62894821\n",
      "   0.53701019  -4.90799713  15.43747711   6.62040138   0.65046787\n",
      "   7.36891556  20.15582657 -15.77274895   7.70636082 -15.57774067\n",
      "   5.15675354  -2.80807304  -0.11309242  -4.39125347   7.40314293\n",
      "   3.32567787  -4.89025593   9.46137428   1.23850822  -0.81269455\n",
      "   4.59230614  -6.77495384   6.79185581  -9.25320673  -0.33506012\n",
      "   3.26252937   5.59076405   1.46875763   5.96831512  -5.20798922\n",
      "  -0.14884853  -2.89777374 -12.4953661    1.35099316   0.6576519\n",
      "  -3.5523777   -9.55607128]\n"
     ]
    }
   ],
   "source": [
    "print adas_pred_scores[5][0] - adas_act_scores[5][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results at: /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/\n"
     ]
    }
   ],
   "source": [
    "#Save df style dictionaly for seaborn plots\n",
    "cohort = 'ADNI1and2'\n",
    "update = 5\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_up_{}.pkl'.format(exp_name, cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse,df_perf_dict_path)\n",
    "\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_tuned_up_{}.pkl'.format(exp_name,cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas_tuned,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE_tuned_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse_tuned,df_perf_dict_path)\n",
    "\n",
    "\n",
    "print 'saving results at: {}'.format(CV_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 25)\n",
    "n_rows = n_folds\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "plot_perf = False\n",
    "\n",
    "if multi_task:\n",
    "    euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "    corrs = [fold_r_adas13,fold_r_mmse]\n",
    "else:\n",
    "    euLosses = [fold_euLoss]\n",
    "    corrs = [fold_r]\n",
    "    \n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        \n",
    "        if plot_perf:\n",
    "            plt.figure(fid)\n",
    "            plt.subplot(n_rows,n_cols,1)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "            plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "            plt.legend()\n",
    "            plt.subplot(n_rows,n_cols,2)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "            plt.title('correlation, fid: {}'.format(fid))\n",
    "            plt.legend(loc=2)\n",
    "\n",
    "print preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path):\n",
    "    fold_r_dict = {}\n",
    "    fold_euLoss_dict = {}\n",
    "    fold_act_scores_dict = {}\n",
    "    fold_pred_scores_dict = {}\n",
    "\n",
    "    if multi_task:\n",
    "        fold_r_dict['ADAS'] = fold_perf_dict['fold_r_adas13']\n",
    "        fold_r_dict['MMSE'] = fold_perf_dict['fold_r_mmse']\n",
    "        fold_euLoss_dict['ADAS'] = fold_perf_dict['fold_euLoss_adas13']\n",
    "        fold_euLoss_dict['MMSE'] = fold_perf_dict['fold_euLoss_mmse']\n",
    "        fold_act_scores_dict['ADAS'] = fold_perf_dict['fold_act_scores_adas13']\n",
    "        fold_act_scores_dict['MMSE'] = fold_perf_dict['fold_act_scores_mmse']\n",
    "        fold_pred_scores_dict['ADAS'] = fold_perf_dict['fold_pred_scores_adas13']\n",
    "        fold_pred_scores_dict['MMSE'] = fold_perf_dict['fold_pred_scores_mmse']\n",
    "        \n",
    "        NN_results = computePerfMetrics(fold_r_dict, fold_euLoss_dict, fold_act_scores_dict, fold_pred_scores_dict, opt_metric, hype_configs, \n",
    "                                        Clinical_Scale,task_weights)\n",
    "        adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "        mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "        adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "        mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "        adas_rmse = NN_results['opt_ADAS']['CV_RMSE'].values()\n",
    "        mmse_rmse = NN_results['opt_MMSE']['CV_RMSE'].values()\n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        adas_pred_CV_scores = NN_results['opt_ADAS']['predicted_CV_scores']\n",
    "        adas_act_CV_scores = NN_results['opt_ADAS']['actual_CV_scores']\n",
    "        mmse_pred_CV_scores = NN_results['opt_MMSE']['predicted_CV_scores']\n",
    "        mmse_act_CV_scores = NN_results['opt_MMSE']['actual_CV_scores']\n",
    "        \n",
    "        \n",
    "        print 'ADAS corr: {}'.format(adas_r)\n",
    "        print 'ADAS mse: {}'.format(adas_mse)\n",
    "        print 'ADAS means: {}, {}'.format(np.mean(adas_r),np.mean(adas_mse))\n",
    "        print ''\n",
    "        print 'MMSE corr: {}'.format(mmse_r)\n",
    "        print 'MMSE mse: {}'.format(mmse_mse)\n",
    "        print 'MMSE means: {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse))\n",
    "        print ''\n",
    "        print 'opt_hyp: {}'.format(opt_hyp)\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        \n",
    "        return {'adas_r':adas_r, 'adas_mse':adas_mse, 'adas_rmse':adas_rmse, 'mmse_r':mmse_r, 'mmse_mse':mmse_mse, 'mmse_rmse':mmse_rmse,\n",
    "               'adas_pred_CV_scores':adas_pred_CV_scores,'adas_act_CV_scores':adas_act_CV_scores,\n",
    "               'mmse_pred_CV_scores':mmse_pred_CV_scores,'mmse_act_CV_scores':mmse_act_CV_scores}\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        NN_results = computeSingleTaskPerfMetrics(fold_perf_dict, opt_metric, hype_configs)\n",
    "        opt_r = NN_results['opt_r'].values()        \n",
    "        opt_mse = NN_results['opt_mse'].values()        \n",
    "        opt_rmse = NN_results['opt_rmse'].values()        \n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['actual_CV_scores']\n",
    "        print 'opt corr: {}'.format(opt_r)\n",
    "        print 'opt mse: {}'.format(opt_mse)\n",
    "        print 'means: {}, {}'.format(np.mean(opt_r),np.mean(opt_mse))\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        print ''\n",
    "    \n",
    "        return {'opt_r':opt_r, 'opt_mse':opt_mse, 'opt_rmse':opt_rmse,'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    \n",
    "\n",
    "\n",
    "    if save_multitask_results:\n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')        \n",
    "        save_path = save_path + '_' + st + '.pkl' \n",
    "        print 'saving results at: {}'.format(save_path)\n",
    "        output = open(save_path, 'wb')\n",
    "        pickle.dump(NN_results, output)\n",
    "        output.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predicted_CV_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6f668dc9c41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted_CV_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual_CV_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predicted_CV_scores'"
     ]
    }
   ],
   "source": [
    "a = np.squeeze(results['predicted_CV_scores'][7])\n",
    "b = np.squeeze(results['actual_CV_scores'][7])\n",
    "\n",
    "print a , b                               \n",
    "print stats.pearsonr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    \n",
    "     # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "\n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    opt_rmse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "\n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    opt_rmse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "\n",
    "    print 'Clinical_Scale: {}'.format(Clinical_Scale)\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        print 'This function does not work for single clinical scale models. Use the code from the next cell'\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "\n",
    "        fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "        fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "        fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "        fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "\n",
    "        for fid in fid_hype_map.keys():\n",
    "            r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "            r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "            r_perf_array = np.array(fid_r_perf[fid])\n",
    "            euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "            euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "            euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "            if opt_metric == 'euLoss':\n",
    "                h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "            else:\n",
    "                h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "\n",
    "            opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "            opt_snap[fid] = snp\n",
    "\n",
    "            opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "            opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "            opt_rmse_adas[fid] = np.sqrt(2*euLoss_perf_array_adas[h,snp]) #RMSE\n",
    "            opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "            opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "            opt_rmse_mmse[fid] = np.sqrt(2*euLoss_perf_array_mmse[h,snp]) #RMSE\n",
    "\n",
    "            actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "            opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "            actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "            opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "        opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'CV_RMSE':opt_rmse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "        opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'CV_RMSE':opt_rmse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_snap':opt_snap, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "def computeSingleTaskPerfMetrics(fold_perf_dict,opt_metric,hype_configs):\n",
    "    corrs = fold_perf_dict['fold_r']\n",
    "    euLosses = fold_perf_dict['fold_euLoss']\n",
    "    act_scores = fold_perf_dict['fold_act_scores']\n",
    "    pred_scores = fold_perf_dict['fold_pred_scores']\n",
    "\n",
    "    \n",
    "    NN_multitask_results = {}\n",
    "   \n",
    "    snap_array = np.arange(snap_start,niter+1,snap_start)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = {}\n",
    "    opt_mse = {}\n",
    "    opt_rmse = {}\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "    actual_scores = {}\n",
    "    opt_pred_scores = {}\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "        # if want to find best hyp from mse values\n",
    "        if opt_metric == 'euLoss':\n",
    "            h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        else:\n",
    "            h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "            \n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        r = r_perf_array[h,snp]        \n",
    "        opt_r[fid] = r\n",
    "        opt_mse[fid] = 2*eu_loss\n",
    "        opt_rmse[fid] = np.sqrt(2*eu_loss)\n",
    "        #print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)        \n",
    "        opt_snap[fid] = snp\n",
    "        opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "        actual_scores[fid] = fold_act_scores[fid]\n",
    "        opt_pred_scores[fid] = fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp]\n",
    "\n",
    "    #print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r.values()), np.mean(opt_mse.values()))\n",
    "    #print opt_r\n",
    "    #print opt_mse\n",
    "    return {'opt_r':opt_r,'opt_mse':opt_mse,'opt_rmse':opt_rmse,'opt_hype':opt_hype,'opt_snap':opt_snap, 'actual_scores':actual_scores,'opt_pred_scores':opt_pred_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "\n",
    "model_file = 'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-05-06-17-14-49.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {4:0,5:1}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_ADAS13_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(5.64658243068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update fold performance for multitask\n",
    "print NN_results['opt_ADAS'].keys()\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "#model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl'\n",
    "model_file_soa = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-23-52.pkl'\n",
    "print 'current state of art: ' + model_file_soa\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file_soa,'rb'))\n",
    "\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "#load old file\n",
    "# model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-02-48.pkl'\n",
    "# print 'updated fold result file: ' + model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "idx_pairs = {1:1,3:3}\n",
    "CV_data = update_multifold_perf(boxplots_dir + model_file_soa, NN_results, idx_pairs)\n",
    "\n",
    "print \"\"\n",
    "print 'updated CV_data'\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "#print 'saving results at: {}'.format(save_name)\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(CV_data, output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():        \n",
    "        for idx in idx_pairs.keys():            \n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "\n",
    "    return CV_data\n",
    "\n",
    "def update_multifold_perf(saved_perf_file, new_perf_dict, idx_pairs):    \n",
    "    perf_metrics = ['CV_r', 'actual_CV_scores', 'CV_MSE', 'predicted_CV_scores']    \n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        if key == 'opt_hype':\n",
    "            for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                CV_data[key][idx_key] = new_perf_dict[key][idx_val]\n",
    "                \n",
    "        else:\n",
    "            for pm in perf_metrics:\n",
    "                for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                    CV_data[key][pm][idx_key] = new_perf_dict[key][pm][idx_val]\n",
    "    \n",
    "    return CV_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "model_files = ['Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl',             \n",
    "             'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-05-23-07-31-29.pkl']\n",
    "\n",
    "#'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-03-19-29-16_Fold9_10.pkl'\n",
    "\n",
    "Clinical_Scale = 'ADAS'\n",
    "perf_array = []\n",
    "for mf in model_files:\n",
    "    CV_data = pickle.load(open(boxplots_dir + mf,'rb'))['opt_{}'.format(Clinical_Scale)]        \n",
    "    perf_array.append(CV_data['CV_r'].values())\n",
    "\n",
    "print (np.max(np.array(perf_array),axis=0))\n",
    "print np.mean(np.max(np.array(perf_array),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "\n",
    "\n",
    "save_detailed_results = True\n",
    "multi_task = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results    \n",
    "    if not multi_task:\n",
    "        NN_results = {}\n",
    "        NN_results['CV_MSE'] = opt_mse\n",
    "        NN_results['CV_r'] = opt_r\n",
    "        NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "        NN_results['actual_CV_scores'] = actual_scores\n",
    "        NN_results['tid_snap_config_dict'] = opt_hype\n",
    "        print opt_r\n",
    "        print opt_mse\n",
    "        \n",
    "    else:\n",
    "        NN_results = NN_multitask_results\n",
    "        print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "        print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "        print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "        print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])\n",
    "        \n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_BOTH_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    for modality in modalities:\n",
    "        in_data = load_data(in_data_path, 'Fold_{}_{}'.format(fid,modality), preproc)         \n",
    "\n",
    "        # HDF5 is pretty efficient, but can be further compressed.\n",
    "        comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "        with h5py.File(out_data_path, 'a') as f:      \n",
    "            if modality == 'X_R_CT': #Fix the typo            \n",
    "                modality = 'X_CT'            \n",
    "\n",
    "            f.create_dataset('{}'.format(modality), data=in_data, **comp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp6'\n",
    "exp_name_out = 'Exp6_MC'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "modalities = ['X_L_HC','X_R_HC','X_R_CT','adas','mmse','dx']\n",
    "dataset = 'ADNI1'\n",
    "preproc = 'no_preproc' #binary labels\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "\n",
    "for mc in np.arange(1,3,1):\n",
    "    for fid in np.arange(1,11,1):\n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_train_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_valid_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_ADAS13_NN_valid_MC_{}.h5'.format(exp_name,dataset,mc)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name_out)\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC: fid[1:4] 8000; fid[5,6] 6000, fid[7,8,9]: 8000 fid 10: 6000\n",
    "CT: fid[1:5] 12000 fid[6:10] 6000\n",
    "\n",
    "#spawn net\n",
    "ADNI_FF_train_hyp1_CT.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n"
     ]
    }
   ],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "cohort = 'ADNI2'\n",
    "modality = 'HC_CT'\n",
    "pretrain_snap_HC = 20000 #ADNI1: 5000\n",
    "pretrain_snap_CT = 20000 #ADNI1: 5000\n",
    "n_folds = 10\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/' \n",
    "#Custom snaps per fold\n",
    "#HC_iter = [8000,8000,8000,8000,6000,6000,8000,8000,8000,6000]\n",
    "#CT_iter = [12000,12000,12000,12000,12000,6000,6000,6000,6000,6000]\n",
    "#mc=1\n",
    "hc_hyp = 'hyp1'\n",
    "ct_hyp = 'hyp1'\n",
    "pretrain_hyp = 'hyp2' #This is hyp generated using optimal combination of hc and ct hyps. Prototxt file is saved with this hyp extension\n",
    "\n",
    "exp_name = 'Exp11_MC'\n",
    "\n",
    "for mc in np.arange(6,11,1):\n",
    "    for fid in np.arange(1,n_folds+1,1):\n",
    "        print 'fid: {}'.format(fid)\n",
    "        #for AE_branch in ['CT','R_HC','L_HC']:\n",
    "        for AE_branch in ['CT','HC']:\n",
    "            print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "            if AE_branch == 'L_HC':\n",
    "                params_FF = ['L_ff1', 'L_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'R_HC':\n",
    "                params_FF = ['R_ff1', 'R_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'HC':\n",
    "                params_FF = ['L_ff1','R_ff1']\n",
    "                AE_iter = pretrain_snap_HC\n",
    "                hyp = hc_hyp\n",
    "            elif AE_branch == 'CT':\n",
    "                #params_FF = ['ff1', 'ff2']\n",
    "                params_FF = ['ff1']\n",
    "                AE_iter = pretrain_snap_CT\n",
    "                hyp = ct_hyp\n",
    "                #fid for pretain is 1 because it's same definition for all the folds.\n",
    "                #Only use this during 1 of the modalities to avoid overwritting\n",
    "                print 'Spawning new net'\n",
    "                pretrain_net = caffe.Net(baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,pretrain_hyp,modality), caffe.TRAIN)\n",
    "            else:\n",
    "                print 'Wrong AE branch'\n",
    "\n",
    "            # conv_params = {name: (weights, biases)}\n",
    "            conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "            for conv in params_FF:\n",
    "                print 'target {} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "            # Review AE net params \n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hyp,AE_branch)\n",
    "            #model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "            model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hyp,AE_branch,AE_iter) \n",
    "\n",
    "            AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "            #params_AE = ['encoder1', 'code']\n",
    "            params_AE = params_FF #if you are using pretrained NN\n",
    "\n",
    "            # fc_params = {name: (weights, biases)}\n",
    "            fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "            for fc in params_AE:\n",
    "                print 'pretrained {} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "            #transplant net parameters\n",
    "            for pr, pr_conv in zip(params_AE, params_FF):\n",
    "                conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "                conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "            save_net = True\n",
    "            if save_net:\n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_{}_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                save_path = baseline_dir + net_name.format(mc,fid,cohort,pretrain_hyp,modality,pretrain_snap_HC,pretrain_snap_CT)\n",
    "                print \"Saving net to \" + save_path\n",
    "                pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_data['opt_ADAS']['CV_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print n_snaps/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
