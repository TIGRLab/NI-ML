{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "import h5py\n",
    "import numpy as np\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "# Some defs to load data and extract encodings from trained net\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    #print net.blobs.items()[0]\n",
    "    #print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        #print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        #print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        #print net.blobs[input_node].shape\n",
    "        \n",
    "        data_layers.append(net.blobs[input_node])    \n",
    "        #print 'X_list shape: {}'.format(X_list[i].shape)\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "    \n",
    "     \n",
    "    net.reshape()            \n",
    "    #print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        #print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas_bl  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas_bl,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1, concat_param=dict(axis=1))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas_bl)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas_m12)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas_bl,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.adas_m12,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT,n.adas_bl  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_CT,n.adas_bl,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas_bl)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas_m12)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas_bl,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.adas_m12,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT_unified(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_HC_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_HC_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_HC_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_HC_CT,n.adas_bl,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_HC_CT, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=4, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.adas_bl,n.adas_m12  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas, n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "#     n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,n.ff1, concat_param=dict(axis=1))\n",
    "    #n.concat = L.Concat(n.X_L_HC,n.X_R_HC,n.X_CT, concat_param=dict(axis=1))\n",
    "    \n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=lr['COMB']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.dropC1 = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2) #This is done automatically by caffe! \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        #n.ff2_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2ADAS13 = L.ReLU(n.ff2_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        #n.ff2_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2MMSE = L.ReLU(n.ff2_MMSE, in_place=True)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        n.ff1_DX = L.InnerProduct(n.ff3, num_output=node_sizes['DX_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1DX = L.ReLU(n.ff1_DX, in_place=True)\n",
    "        \n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas_bl,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.adas_m12,loss_weight=tr['MMSE'])  \n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_DX = L.InnerProduct(n.ff1_DX, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.DX_loss = L.SoftmaxWithLoss(n.output_DX, n.dx_cat3,loss_weight=tr['DX'])  \n",
    "    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_CT, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.dropC1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "        \n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_R_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_L_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))       \n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='HC_CT': #multimodal AE - experimental stage\n",
    "        HC_node_split = 0.8\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_L_HC = L.InnerProduct(n.X_L_HC, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.9*node_sizes['En1'])))        \n",
    "        n.NLinEn_L_HC = L.ReLU(n.encoder_L_HC, in_place=True)\n",
    "        \n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_CT = L.InnerProduct(n.X_CT, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.1*node_sizes['En1'])))        \n",
    "        n.NLinEn_CT = L.ReLU(n.encoder_CT, in_place=True)\n",
    "        \n",
    "        #Concat         \n",
    "        n.encoder1 = L.Concat(n.encoder_L_HC,n.encoder_CT, concat_param=dict(axis=1))\n",
    "        \n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers (or common encoder layers for multimodal) \n",
    "    \n",
    "#     n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.Sigmoid(n.encoder2, in_place=True)\n",
    "    #code layer\n",
    "#    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='xavier'))  \n",
    "    \n",
    "#     n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "    \n",
    "    #Decoder layers\n",
    "    if modality == 'CT':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)                    \n",
    "    \n",
    "    elif modality =='R_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    elif modality =='HC_CT':        \n",
    "        n.decoder_L_HC = L.InnerProduct(n.code, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_L_CT = L.ReLU(n.decoder_L_HC, in_place=True)\n",
    "        n.decoder_CT = L.InnerProduct(n.code, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_CT = L.ReLU(n.decoder_CT, in_place=True)\n",
    "        \n",
    "        n.output_L_HC = L.InnerProduct(n.decoder_L_HC, num_output=node_sizes['out_HC'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_HC = L.SigmoidCrossEntropyLoss(n.output_L_HC, n.X_L_HC)\n",
    "        \n",
    "        n.output_CT = L.InnerProduct(n.decoder_CT, num_output=node_sizes['out_CT'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_CT = L.EuclideanLoss(n.output_CT, n.X_CT)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    return n.to_proto()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode):\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 20\n",
    "    \n",
    "    if multimodal_autoencode:\n",
    "        train_loss_HC = zeros(niter)\n",
    "        test_loss_HC = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_loss_CT = zeros(niter)\n",
    "        test_loss_CT = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "        for it in range(niter):            \n",
    "            solver.step(1)  # SGD by Caffe \n",
    "            train_loss_HC[it] = solver.net.blobs['loss_HC'].data\n",
    "            train_loss_CT[it] = solver.net.blobs['loss_CT'].data\n",
    "            \n",
    "            if it % test_interval == 0:                        \n",
    "                t_loss_HC = 0\n",
    "                t_loss_CT = 0                                \n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()   \n",
    "                    t_loss_HC += solver.test_nets[0].blobs['loss_HC'].data\n",
    "                    t_loss_CT += solver.test_nets[0].blobs['loss_CT'].data\n",
    "                \n",
    "                test_loss_HC[it // test_interval] = t_loss_HC/(test_iter)\n",
    "                test_loss_CT[it // test_interval] = t_loss_CT/(test_iter)\n",
    "                print 'HC Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_HC[it], np.sum(train_loss_HC)/it, test_loss_HC[it // test_interval])\n",
    "                print 'CT Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_CT[it], np.sum(train_loss_CT)/it, test_loss_CT[it // test_interval])\n",
    "                \n",
    "        perf = {'train_loss':[train_loss_HC, train_loss_CT],'test_loss':[test_loss_HC,test_loss_CT]}\n",
    "    else: \n",
    "        \n",
    "        #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "        # losses will also be stored in the log\n",
    "        test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "        if not multi_task:\n",
    "            train_loss = zeros(niter)\n",
    "            test_loss = zeros(int(np.ceil(niter / test_interval)))  \n",
    "\n",
    "        else: \n",
    "            if multi_label:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_DX_loss = zeros(niter)\n",
    "                test_DX_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "            else:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_MMSE_loss = zeros(niter)\n",
    "                test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "\n",
    "        #output = zeros((niter, batch_size))\n",
    "        #solver.restore()\n",
    "        #the main solver loop\n",
    "        for it in range(niter):\n",
    "            #solver.net.forward()\n",
    "            solver.step(1)  # SGD by Caffe    \n",
    "            # store the train loss\n",
    "            if not multi_task:\n",
    "                train_loss[it] = solver.net.blobs['loss'].data        \n",
    "            else: \n",
    "                if multi_label:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_DX_loss[it] = solver.net.blobs['DX_loss'].data\n",
    "                else:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "            # store the output on the first test batch\n",
    "            # (start the forward pass at conv1 to avoid loading new data)\n",
    "            #solver.test_nets[0].forward()\n",
    "            #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "            # run a full test every so often\n",
    "            # (Caffe can also do this for us and write to a log, but we show here\n",
    "            #  how to do it directly in Python, where more complicated things are easier.)\n",
    "            if it % test_interval == 0:        \n",
    "                t_loss = 0\n",
    "                t_ADAS13_loss = 0\n",
    "                t_MMSE_loss = 0\n",
    "                t_DX_loss = 0\n",
    "                correct = 0\n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()                \n",
    "                    if not multi_task:\n",
    "                        t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                        if multi_label:\n",
    "                            correct += sum(solver.test_nets[0].blobs['output'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "                    else: \n",
    "                        if multi_label:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_DX_loss += solver.test_nets[0].blobs['DX_loss'].data\n",
    "                            correct += sum(solver.test_nets[0].blobs['output_DX'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "\n",
    "                        else:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "                if not multi_task:\n",
    "                    test_loss[it // test_interval] = t_loss/(test_iter)\n",
    "                    if multi_label:\n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                    print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                        it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval], test_acc[it // test_interval])\n",
    "                else:\n",
    "                    if multi_label:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_DX_loss[it // test_interval] = t_DX_loss/(test_iter) \n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'DX Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                            it, train_DX_loss[it], np.sum(train_DX_loss)/it, test_DX_loss[it // test_interval],test_acc[it // test_interval])\n",
    "                    else:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_MMSE_loss[it // test_interval] = t_MMSE_loss/(test_iter)             \n",
    "\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "        if not multi_task:\n",
    "            perf = {'train_loss':[train_loss],'test_loss':[test_loss],'test_acc':[test_acc]}\n",
    "        else:\n",
    "            if multi_label:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_DX_loss],'test_loss':[test_ADAS13_loss,test_DX_loss],'test_acc':[test_acc]}\n",
    "            else:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "                \n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,snap_prefix):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    \n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    #s.solver_type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 100000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    #s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 4000\n",
    "    s.snapshot_prefix = snap_prefix\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MC # 1, Hype # hyp1, Fold # 6\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (298.669494629,inf), test loss: 204.244886398\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (396.710266113,inf), test loss: 240.818777275\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (54.879196167,63.3738438568), test loss: 39.8269898415\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (107.672874451,85.6153345585), test loss: 60.4031380653\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (28.6195106506,53.9878165007), test loss: 41.0715205669\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (55.9521446228,78.8884673672), test loss: 70.8775791168\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (104.051048279,50.9177382132), test loss: 36.8361605167\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (274.949462891,76.7685777836), test loss: 60.1887943268\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (10.9362468719,49.3008451769), test loss: 40.2226394176\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (13.5881719589,75.5362451849), test loss: 66.4049544334\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (63.5255355835,48.3348683496), test loss: 38.7225042343\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (65.4257354736,74.7680590927), test loss: 68.9988227844\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (37.6690750122,47.6625621947), test loss: 38.2164270401\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (151.832504272,74.2829191662), test loss: 58.6878929138\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (21.8863525391,47.1850317319), test loss: 40.1144904137\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (26.4031944275,73.8818424917), test loss: 65.8492750168\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (21.2821941376,46.8260196682), test loss: 38.7600993156\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (62.3050537109,73.592922951), test loss: 62.3818477631\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (86.7616882324,46.536814224), test loss: 39.9673260689\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (77.0180511475,73.3297129228), test loss: 59.9469118118\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (11.8683261871,46.2661800644), test loss: 40.7066608429\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (35.7289123535,73.0990920877), test loss: 68.3849824905\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (46.7604064941,46.0460962769), test loss: 37.3443684578\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (97.5181121826,72.9080635485), test loss: 61.9414852142\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (37.7280426025,45.8439965366), test loss: 39.2991721153\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (42.6019363403,72.7081974427), test loss: 65.954264164\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (32.9213218689,45.6838172031), test loss: 39.4092350006\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (38.338508606,72.5681516816), test loss: 70.5093836784\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (32.3659858704,45.5296201527), test loss: 37.8674627304\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (59.9622039795,72.4244308713), test loss: 59.5690239906\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (49.0688552856,45.3825068439), test loss: 39.1036510468\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (73.7148132324,72.2758800219), test loss: 65.1227068901\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (60.8282165527,45.2614546915), test loss: 38.6198245049\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (105.20716095,72.1699445827), test loss: 62.3522615433\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (21.7746200562,45.1270237061), test loss: 37.8123155594\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (22.7225418091,72.0150158277), test loss: 57.4672048569\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (56.2223510742,45.0138115294), test loss: 40.3467923164\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (69.574256897,71.8835547669), test loss: 66.3667748451\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (41.7622299194,44.898025381), test loss: 37.9156429768\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (48.6492462158,71.7614577268), test loss: 60.70305233\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (118.679634094,44.7965206205), test loss: 38.2083041668\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (153.426681519,71.6355956949), test loss: 58.1467957497\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (30.289855957,44.6859494379), test loss: 38.7956907749\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (49.7297706604,71.5031526427), test loss: 68.0893268585\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (19.6493492126,44.5869035212), test loss: 35.2706496239\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (38.1017227173,71.3705289285), test loss: 56.9825919151\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (40.8893737793,44.4772094242), test loss: 38.6049224854\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (60.3199615479,71.2336507612), test loss: 64.1887397766\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (8.96200561523,44.3684786141), test loss: 37.9320230484\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (9.88355255127,71.0952434722), test loss: 63.2617064476\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (44.2177276611,44.2573799374), test loss: 36.246517992\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (48.3404464722,70.9506598412), test loss: 56.4604652405\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (40.0684967041,44.1519618002), test loss: 40.946453476\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (59.0283737183,70.8172963634), test loss: 66.4103725433\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (36.2530975342,44.0378238741), test loss: 37.7714091301\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (52.1789703369,70.6704859233), test loss: 61.3655801773\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (35.4395523071,43.9223004293), test loss: 36.8954808235\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (77.683807373,70.515751983), test loss: 55.5356948853\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (12.4588050842,43.8104734941), test loss: 37.5661547661\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (24.8508071899,70.3676165561), test loss: 64.5679140091\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (45.6102523804,43.68956721), test loss: 33.7756490707\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (45.828338623,70.1900511042), test loss: 55.8565093994\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (11.7235374451,43.5686801601), test loss: 37.1602556705\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (21.9196090698,70.0144345643), test loss: 61.8270773888\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (74.7019424438,43.4475946284), test loss: 35.5636441708\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (102.586013794,69.8420225273), test loss: 63.7628714561\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (64.8906555176,43.3229452649), test loss: 34.9400403976\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (117.152458191,69.6533730072), test loss: 53.5802725792\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (24.4170150757,43.1894697263), test loss: 35.9529784203\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (32.7865486145,69.4512528412), test loss: 60.6440285683\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (21.6457080841,43.0600145081), test loss: 35.4586727142\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (68.7933349609,69.2474187316), test loss: 57.6362613678\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (30.969127655,42.919609099), test loss: 34.8825418949\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (58.7777252197,69.0335347301), test loss: 53.0139382362\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (32.5624198914,42.7774630563), test loss: 38.7018356323\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (45.6271362305,68.8116493279), test loss: 66.0782038212\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (73.8686828613,42.6348935443), test loss: 34.6286645174\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (158.866592407,68.5871088573), test loss: 58.5860150337\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (32.9337043762,42.4877873732), test loss: 36.9167898655\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (84.1126708984,68.3561022717), test loss: 61.5489500999\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (14.2227621078,42.3367858698), test loss: 34.8562594414\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (38.5359344482,68.1165394912), test loss: 62.2983242035\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (64.3952178955,42.1872919183), test loss: 31.6760526657\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (107.46321106,67.8760414407), test loss: 50.5151456833\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (24.2028026581,42.0329979087), test loss: 34.5500049591\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (38.6961898804,67.6275693415), test loss: 57.8454414368\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (39.8560256958,41.878081754), test loss: 34.3541267395\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (66.1526641846,67.3710915206), test loss: 55.9129253387\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (41.1482925415,41.7230077493), test loss: 32.8274513245\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (185.755905151,67.1186164605), test loss: 50.5544935226\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (12.7838115692,41.5657240604), test loss: 35.3590167522\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (15.8363237381,66.8541964307), test loss: 58.0322406769\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (19.610162735,41.4099117844), test loss: 32.4501874924\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (27.127325058,66.5925416663), test loss: 51.8464046001\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (79.8550567627,41.2540666374), test loss: 33.6941993713\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (61.7561645508,66.3280525173), test loss: 51.3005647659\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (20.6391029358,41.0941306192), test loss: 33.7797840118\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (66.7636413574,66.0599456556), test loss: 60.1452028275\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (33.089214325,40.9343906727), test loss: 31.2673383236\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (63.8837242126,65.7911410861), test loss: 52.4240930557\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (15.1541366577,40.7748366314), test loss: 34.7213429451\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (11.8019304276,65.5221095318), test loss: 59.2373476982\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (29.4726104736,40.6187052053), test loss: 34.7199809074\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (41.888381958,65.2585867929), test loss: 63.3423233032\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (40.2053375244,40.4596575325), test loss: 32.0533375263\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (76.8875579834,64.9910066269), test loss: 50.2437171936\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (42.0077362061,40.3022449055), test loss: 36.1449723721\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (48.8857002258,64.7246142002), test loss: 59.3527099609\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (34.3766326904,40.1476973522), test loss: 32.986441946\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (80.0432891846,64.4642093639), test loss: 54.629032135\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (29.3147182465,39.9905037847), test loss: 33.1720636368\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (35.338104248,64.1962700631), test loss: 50.3556595325\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (55.9567184448,39.8384162828), test loss: 33.4344066143\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (45.0495834351,63.9347592737), test loss: 58.0235228062\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (24.6781730652,39.6847930725), test loss: 30.1698345661\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (34.487991333,63.6757764531), test loss: 50.5251158714\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (54.1581420898,39.533420238), test loss: 32.3919159889\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (60.6739349365,63.414030818), test loss: 55.0153201103\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (22.6863479614,39.3840318594), test loss: 32.6824351788\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (31.4898109436,63.1580826936), test loss: 59.465237999\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (35.1205596924,39.2364769868), test loss: 31.6475389481\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (33.3019447327,62.9007750357), test loss: 50.8389019966\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.9232749939,39.085744314), test loss: 32.485461545\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (20.5830554962,62.643144998), test loss: 56.3421189308\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (20.657459259,38.9393452994), test loss: 36.7203860283\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (29.3349246979,62.3923489865), test loss: 62.7801567554\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.5138187408,38.7922009515), test loss: 31.9979093552\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (25.3322105408,62.1394552827), test loss: 51.2711503029\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (23.0332832336,38.6482551452), test loss: 37.9035457134\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (27.7808876038,61.8918391968), test loss: 66.4446602821\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (10.6528663635,38.5050471064), test loss: 30.3015298843\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (17.9312133789,61.6476053344), test loss: 50.3045039654\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (16.7752456665,38.3625873419), test loss: 36.0350762844\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (42.7954978943,61.4028113644), test loss: 57.2033671379\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (38.780128479,38.2223329149), test loss: 30.5699291945\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (85.5701065063,61.1629340773), test loss: 55.3280916214\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (43.1232452393,38.0840709864), test loss: 30.2787790775\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (32.4557991028,60.9219028012), test loss: 50.6122325897\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (21.8747596741,37.9473985447), test loss: 31.8142651558\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (26.1469764709,60.6845787765), test loss: 55.2369527817\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (29.8103294373,37.8093464897), test loss: 31.5186767578\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (30.5035495758,60.4491861102), test loss: 52.5903297424\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (33.0484275818,37.677691993), test loss: 32.5643271446\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (65.5726318359,60.2188993689), test loss: 52.6672245979\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (36.6629600525,37.5441386309), test loss: 32.6159930706\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (39.0604858398,59.9872413694), test loss: 55.0614925385\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (23.5166721344,37.4117731603), test loss: 31.2811451197\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (17.8631381989,59.7552621833), test loss: 51.6319993973\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (14.4830856323,37.2817055238), test loss: 31.8376504421\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (32.6653938293,59.5310075714), test loss: 49.6267021179\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.7055797577,37.1526928763), test loss: 41.1340773582\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (33.4971389771,59.3078253947), test loss: 75.8376342773\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (56.0430755615,37.0233399361), test loss: 29.4840423822\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (66.0605316162,59.0838478286), test loss: 50.2151125908\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (7.77805137634,36.8986288298), test loss: 33.8824198246\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (36.7029876709,58.8691113541), test loss: 59.165780735\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (10.6893482208,36.7730457794), test loss: 32.1959068537\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (12.9275360107,58.6530212775), test loss: 58.3451413155\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (28.1006736755,36.6476152781), test loss: 30.7737090111\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (33.5733261108,58.4372821383), test loss: 49.2358448029\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (17.0825595856,36.5273889776), test loss: 31.6875163078\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (13.5214147568,58.229411329), test loss: 54.0104646683\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (12.6119699478,36.4063225834), test loss: 32.2025090933\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (43.767375946,58.0197903751), test loss: 52.9878228188\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (2.46182918549,36.2858315507), test loss: 32.3712847233\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (13.6523990631,57.8108639143), test loss: 50.307968235\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (29.0331611633,36.1690715582), test loss: 33.0623280525\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (30.3010101318,57.6090331804), test loss: 56.5449285507\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.6220779419,36.0521486777), test loss: 29.6610943079\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (14.9135160446,57.4072956694), test loss: 49.5296338081\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (45.6398887634,35.9355181342), test loss: 30.9507731915\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (32.6789093018,57.2051020845), test loss: 53.3246118546\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (35.9265899658,35.8214185466), test loss: 30.9485311508\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (75.6239929199,57.0061010998), test loss: 56.0365221977\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (39.7529525757,35.707529851), test loss: 30.9475722313\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (42.2355499268,56.8105077214), test loss: 50.6571097374\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (12.3423509598,35.5931917388), test loss: 32.1083101749\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (26.9272136688,56.6153010246), test loss: 56.8469901562\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (28.1498947144,35.4830435746), test loss: 31.2392981052\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (62.3126487732,56.4244567859), test loss: 52.0902739525\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (21.0686779022,35.372784059), test loss: 31.2931898594\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (37.3487167358,56.2360586969), test loss: 49.8838636398\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (20.7717933655,35.2620425575), test loss: 33.8783771515\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (45.6330337524,56.0481094153), test loss: 57.5232662201\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (46.4596252441,35.1544967868), test loss: 36.3197348595\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (121.030181885,55.8627421618), test loss: 60.606291008\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (10.0763721466,35.0480225042), test loss: 31.3458563089\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (16.2054367065,55.6807187004), test loss: 49.2203773499\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (40.7734260559,34.9418054047), test loss: 35.7157241106\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (33.8507156372,55.4979219622), test loss: 63.1856045723\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (19.9637355804,34.8367698212), test loss: 30.5191590786\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (78.5435409546,55.3179891865), test loss: 50.8538062096\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (12.9586000443,34.7337188919), test loss: 32.6459736824\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (19.728061676,55.1405571498), test loss: 55.0721246719\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (11.5238513947,34.6314083337), test loss: 31.9972441673\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (19.1483345032,54.9648871191), test loss: 53.5279356956\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (38.7028274536,34.5298326371), test loss: 31.2871557951\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (21.3208503723,54.7870237161), test loss: 50.294592762\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (18.3931255341,34.4299331225), test loss: 32.5968661308\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (16.8959712982,54.6159118511), test loss: 53.759522438\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (14.9652404785,34.3298367656), test loss: 43.966309166\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (32.3301124573,54.4454867652), test loss: 72.8188136578\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (22.1186218262,34.2297509497), test loss: 32.2743463039\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (21.200138092,54.2727813891), test loss: 50.0787631512\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (14.827123642,34.1339899533), test loss: 33.5767870426\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (18.9732265472,54.1091651797), test loss: 58.8375284195\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (10.0375823975,34.0364086084), test loss: 30.579594183\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (20.4327793121,53.9443103111), test loss: 50.6835517883\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (10.686170578,33.9386962193), test loss: 40.2932211876\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (16.6586780548,53.7762316171), test loss: 67.2655855179\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (38.4537963867,33.8458030365), test loss: 32.1271334648\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (75.0408325195,53.6183092708), test loss: 56.5087493896\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (12.5757341385,33.751325424), test loss: 32.4806809425\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (17.8121948242,53.4569677433), test loss: 51.5321495056\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (51.1202049255,33.6575317956), test loss: 31.2463649035\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (64.3317642212,53.2950556178), test loss: 54.5405357361\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (24.9765396118,33.5656790369), test loss: 34.9961622238\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (34.6039123535,53.1386718299), test loss: 56.2660510063\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (50.333984375,33.4750348276), test loss: 41.7958914757\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (52.8243560791,52.9829797605), test loss: 67.898886013\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (32.2209854126,33.382846381), test loss: 35.557213068\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (30.202381134,52.8248445979), test loss: 60.7631586075\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (17.2662830353,33.2946225422), test loss: 31.184826088\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (18.2815589905,52.6710383925), test loss: 51.6470896721\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (8.44881248474,33.2054878448), test loss: 32.1267164946\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (23.2226333618,52.519959743), test loss: 53.9424056053\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (11.5003051758,33.1157676513), test loss: 35.2902183533\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (24.7535171509,52.3663771256), test loss: 64.7192934036\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (9.51410484314,33.0290153255), test loss: 32.4153286934\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (15.9464893341,52.2169552092), test loss: 53.380603981\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (15.3862295151,32.9432323314), test loss: 32.3132110596\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (42.0332565308,52.0717468146), test loss: 55.3432728767\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (22.3351097107,32.8551917847), test loss: 37.4515453339\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (36.1163330078,51.9219914572), test loss: 60.2317312241\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (26.1092948914,32.7709217984), test loss: 32.6689308643\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (60.2587127686,51.7772038413), test loss: 51.5550675392\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (8.41612815857,32.6873299181), test loss: 33.6808722496\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (9.17488479614,51.6360451282), test loss: 54.9803198814\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (22.5680274963,32.6016288196), test loss: 38.7062338352\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (12.836938858,51.4893521595), test loss: 60.4701889038\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 7\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (141.032684326,inf), test loss: 190.174523163\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (160.913238525,inf), test loss: 216.277368164\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (40.1812515259,59.6432469816), test loss: 38.8965679169\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (88.4781341553,80.2947057695), test loss: 56.4135105133\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (38.4770545959,50.84277352), test loss: 41.2479527473\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (45.9804229736,74.1772465668), test loss: 62.7427416801\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (12.7287397385,47.8685947831), test loss: 36.6499824524\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (31.4092826843,72.0578932158), test loss: 55.7444124222\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (124.356178284,46.4063916004), test loss: 40.2244661808\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (302.208251953,71.0890897598), test loss: 58.3423128128\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (25.7349948883,45.4766203655), test loss: 40.120553112\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (28.1533889771,70.3930887661), test loss: 61.5907130241\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (58.1705741882,44.8452573853), test loss: 37.1644436836\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (63.1864814758,69.884542168), test loss: 54.0674391747\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (52.0834693909,44.3932340621), test loss: 40.871586895\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (53.5947418213,69.5285408765), test loss: 58.3693353653\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (56.0173072815,44.0440396875), test loss: 37.2381475449\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (216.720809937,69.2761839805), test loss: 56.403223896\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (20.121099472,43.7582875013), test loss: 37.9896353722\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (10.4014778137,69.0241804557), test loss: 54.1690081596\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (55.8119277954,43.535544197), test loss: 38.7144292355\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (96.8113250732,68.8357558424), test loss: 59.8658288956\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (43.6736907959,43.3324642697), test loss: 36.1332064629\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (63.4618835449,68.6584732515), test loss: 54.1570968628\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (135.077728271,43.1671788569), test loss: 39.3007754803\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (122.178642273,68.5004559842), test loss: 57.8635004044\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (50.4259490967,43.0129976416), test loss: 39.0419863701\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (125.497772217,68.3597575515), test loss: 60.4921500206\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (39.7310905457,42.8628773447), test loss: 36.7958319664\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (82.4862823486,68.2158020245), test loss: 53.1452854156\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (21.9563217163,42.7330617598), test loss: 40.5108208656\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (30.4534931183,68.0877445938), test loss: 58.8179271698\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (31.3480110168,42.6113567328), test loss: 35.2007681847\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (30.6717300415,67.9591522581), test loss: 55.8092020988\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (26.3710975647,42.5038234995), test loss: 37.342634201\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (41.9216575623,67.8508404251), test loss: 53.0075365067\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (36.4082984924,42.39844298), test loss: 39.1303068161\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (80.0567932129,67.7375322383), test loss: 61.1597021103\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (36.7726249695,42.2980133746), test loss: 35.7662143707\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (43.0377502441,67.6319240008), test loss: 53.6085944176\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (16.6910705566,42.1985272545), test loss: 38.6612960339\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (33.4041099548,67.5195477656), test loss: 56.8647245407\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (123.782714844,42.1104911149), test loss: 38.2952691555\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (273.01739502,67.4279570675), test loss: 58.8557281494\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (21.1321334839,42.0156587504), test loss: 35.9221280575\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (24.6899051666,67.3180225288), test loss: 52.2131596565\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (50.5623168945,41.9235687156), test loss: 39.3292785406\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (61.8850746155,67.1997814581), test loss: 57.4909457207\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (45.0353851318,41.8358002139), test loss: 33.6884690762\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (48.3929138184,67.0875101709), test loss: 53.3888347626\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (52.9916000366,41.7486915415), test loss: 36.464288044\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (211.63885498,66.9832352605), test loss: 51.8313994408\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (20.6646270752,41.6560069166), test loss: 37.279727459\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (22.4298286438,66.8610114175), test loss: 58.2242619514\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (58.6905975342,41.5682675991), test loss: 34.8974816799\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (101.203590393,66.7450274943), test loss: 51.3518671989\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (35.15965271,41.4738892624), test loss: 37.6278634548\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (53.80078125,66.6208083449), test loss: 55.2016334534\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (129.640319824,41.3828956319), test loss: 36.9593724251\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (114.67792511,66.4924135007), test loss: 56.8904860497\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (39.8777236938,41.2871589252), test loss: 35.1304739952\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (98.061164856,66.3601278945), test loss: 50.6381183624\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (35.7396278381,41.1853073215), test loss: 37.9546958923\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (77.8207473755,66.2194325494), test loss: 55.6057897568\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (22.755947113,41.0845414948), test loss: 33.5164185047\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (35.6814613342,66.0756046511), test loss: 53.1897639275\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (22.6268100739,40.9804121291), test loss: 35.2766343594\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (20.1627292633,65.922735868), test loss: 50.0642553329\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (24.841594696,40.8778709027), test loss: 35.9981138706\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (37.4143295288,65.7730111058), test loss: 56.1324730873\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (25.9364509583,40.7697693527), test loss: 34.7213567257\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (60.0991973877,65.6120520071), test loss: 50.6368630409\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (33.783454895,40.6602013942), test loss: 36.4221823692\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (36.7246932983,65.449084644), test loss: 52.4112134933\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (17.866859436,40.5461480691), test loss: 34.2762990475\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (33.0184822083,65.2757044979), test loss: 52.1973986626\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (109.960662842,40.4335853092), test loss: 34.5580365896\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (220.219009399,65.1061532047), test loss: 49.1005012989\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (16.7391757965,40.3137631973), test loss: 36.0513032913\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (16.6072635651,64.9238061008), test loss: 53.5787789345\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (42.2379455566,40.1920918873), test loss: 31.3973905087\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (55.2238426208,64.7313052979), test loss: 48.5843325615\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (41.8476257324,40.069787402), test loss: 33.9873721123\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (42.2522163391,64.5364031999), test loss: 48.1183974743\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (44.5403213501,39.945045638), test loss: 34.2832105637\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (190.683883667,64.3416786844), test loss: 53.0558714867\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (18.3301124573,39.8164697772), test loss: 31.5153851509\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (23.0032672882,64.1338004263), test loss: 45.9851833344\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (78.6680603027,39.6897302092), test loss: 35.4391042709\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (112.168785095,63.9269071671), test loss: 49.7221409321\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (22.7278404236,39.5575301503), test loss: 32.5581145763\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (34.6468734741,63.7130521494), test loss: 49.3667559624\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (111.546386719,39.4272512542), test loss: 32.2841307163\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (90.9729309082,63.4960655363), test loss: 45.9551229\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (20.6927814484,39.2929041607), test loss: 33.6404639721\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (60.5854530334,63.2758669984), test loss: 50.5168936729\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (27.1708984375,39.1560897812), test loss: 29.267501235\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (60.7376937866,63.0534154083), test loss: 45.2412693977\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (22.7302322388,39.0200583163), test loss: 32.5848124504\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (38.2055854797,62.8299426038), test loss: 46.1709052563\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (16.4682712555,38.8818572983), test loss: 33.457007885\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (11.7559814453,62.6017426537), test loss: 51.7986025333\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (35.6216125488,38.7460244531), test loss: 30.1361756802\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (73.9379196167,62.3784601979), test loss: 44.1209460258\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (14.5573310852,38.6074520478), test loss: 37.173126936\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (34.5864753723,62.1506919329), test loss: 51.76825037\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (26.8945636749,38.4702699904), test loss: 31.5275114536\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (29.6979064941,61.9256720699), test loss: 47.811003685\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (15.3641500473,38.3312912722), test loss: 32.7740949154\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (36.2351150513,61.6966008918), test loss: 47.6713101864\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (82.8876800537,38.1946355658), test loss: 32.7577255726\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (172.488098145,61.4729953442), test loss: 49.8687369347\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (10.5941371918,38.0559465127), test loss: 28.7700076342\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (7.78957462311,61.2462995202), test loss: 44.4858470917\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (34.9881362915,37.9189234407), test loss: 32.3542708635\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (46.9190139771,61.0189382468), test loss: 46.7220032215\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (39.3369407654,37.7829782447), test loss: 32.5020261288\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (34.4954032898,60.7930318167), test loss: 49.939764595\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (34.0099372864,37.6473993246), test loss: 29.3453351974\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (164.079284668,60.570861306), test loss: 43.4350497723\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.027671814,37.5109327416), test loss: 32.5725791931\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (25.4656734467,60.3435780865), test loss: 44.9008956909\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (61.7458953857,37.3782425656), test loss: 30.4145194054\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (86.5463638306,60.1214232088), test loss: 45.8440989017\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (16.068107605,37.2442132013), test loss: 30.2845412731\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (23.3773651123,59.8996659255), test loss: 43.7650201321\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (76.7037353516,37.112581783), test loss: 32.1533168316\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (57.990486145,59.6782422546), test loss: 48.403075695\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (10.945602417,36.9797464661), test loss: 29.0811608315\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (41.6883621216,59.4567633707), test loss: 44.8315759659\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (18.0994415283,36.8471232573), test loss: 31.9206176281\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (41.0874443054,59.2373068819), test loss: 45.7660814762\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (16.6186771393,36.7174343007), test loss: 33.9294965267\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (32.7196807861,59.0216407948), test loss: 52.5236950874\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (15.7949571609,36.5872989995), test loss: 29.1726497173\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (11.1541423798,58.8048551126), test loss: 42.668810463\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (50.3464508057,36.4595642412), test loss: 31.9378967047\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (126.809516907,58.5928676943), test loss: 45.3563549042\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (12.4763813019,36.3306327352), test loss: 35.8275706291\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (31.7532978058,58.3786670376), test loss: 54.5613471985\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (23.5154838562,36.2045128176), test loss: 29.5212324142\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (27.6950588226,58.1689346384), test loss: 42.1669499874\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.7345056534,36.0783193327), test loss: 31.6909017086\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (36.3844108582,57.9588734374), test loss: 47.7320925713\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (69.7433013916,35.9543654973), test loss: 31.3145115376\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (146.242874146,57.7540842073), test loss: 48.9990296364\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.9631242752,35.8294578443), test loss: 30.5304309845\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (13.4649925232,57.5477209945), test loss: 44.9171867371\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (28.7851352692,35.7063062496), test loss: 30.7272658825\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (41.5990104675,57.3411623093), test loss: 46.8856952667\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (33.8771514893,35.5843961908), test loss: 29.2049159527\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (25.8154067993,57.1379839218), test loss: 42.5247290134\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (27.6148967743,35.4633695392), test loss: 31.1992841721\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (150.978775024,56.939333186), test loss: 44.8606342316\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (17.2710208893,35.3421045662), test loss: 28.353348875\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (26.7996330261,56.7378555604), test loss: 44.1704366684\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (50.7998886108,35.2234939816), test loss: 31.089923048\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (70.6714096069,56.5396655526), test loss: 45.3701887608\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.5147342682,35.1050902464), test loss: 29.7836911678\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (22.0960903168,56.3433613257), test loss: 44.7185219765\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (57.2841949463,34.9885230684), test loss: 29.254402113\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (38.7986221313,56.1484281663), test loss: 44.6801380157\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (9.68340778351,34.8717280767), test loss: 30.5987188816\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (30.6895389557,55.9543010641), test loss: 44.4722693443\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.6745204926,34.755660866), test loss: 30.4401589394\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (22.3519306183,55.7633285941), test loss: 46.0751988411\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (9.96458911896,34.6420027975), test loss: 30.8150124073\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (21.9858169556,55.5746381373), test loss: 45.3412717342\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (15.5797338486,34.5284854665), test loss: 30.6164361\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (10.6879062653,55.3860896036), test loss: 44.1017157078\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (55.4235687256,34.4167284642), test loss: 28.677677536\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (144.090423584,55.2019524158), test loss: 44.8112279892\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (11.6971626282,34.3046923509), test loss: 32.1690398693\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (29.5466041565,55.0176109274), test loss: 45.2428492069\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (24.5437965393,34.1952624536), test loss: 30.3641157627\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (30.7664775848,54.8366705779), test loss: 45.1745034218\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.0952720642,34.0859544952), test loss: 33.5215069056\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (32.6817817688,54.6550936817), test loss: 51.4230388165\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (59.4493255615,33.9780846625), test loss: 35.4081676483\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (127.762184143,54.478058654), test loss: 53.4459186554\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (15.0543851852,33.8703018606), test loss: 31.1316926003\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (20.7267475128,54.3013165465), test loss: 46.7394985199\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (18.852148056,33.7642728149), test loss: 30.4354035378\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (36.5710411072,54.1251159899), test loss: 43.9999752998\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (31.9489593506,33.659914164), test loss: 32.3823805332\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (23.6221008301,53.9507370449), test loss: 45.4064046383\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (23.8639373779,33.5561775701), test loss: 28.9064207077\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (137.290603638,53.7794735442), test loss: 45.3796447754\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (18.0430145264,33.4525138068), test loss: 30.1122591019\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (25.9276542664,53.6069151167), test loss: 42.180781889\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (45.9819831848,33.3510370351), test loss: 32.7408486843\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (68.3175888062,53.4375527529), test loss: 49.4678577423\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (12.449464798,33.2503586511), test loss: 30.9586974621\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (24.1056671143,53.2697692313), test loss: 46.1348579407\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (51.6739044189,33.1508432928), test loss: 31.0518688679\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (34.5867996216,53.102103466), test loss: 43.8943712234\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (14.1478242874,33.0511513546), test loss: 31.9331403255\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (25.5184783936,52.9350571322), test loss: 47.8669324875\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (7.40826272964,32.9520050092), test loss: 32.4189053535\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (15.747628212,52.7714035364), test loss: 46.1678970337\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (6.50729656219,32.855361662), test loss: 34.4014467001\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (18.5560321808,52.610069933), test loss: 49.8092725754\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (14.055814743,32.7590130399), test loss: 29.3867290258\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (11.0587396622,52.4487191132), test loss: 45.8175205708\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (52.8982582092,32.6635114153), test loss: 33.2669960976\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (129.003952026,52.2895080888), test loss: 47.1637627602\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (11.4142684937,32.5675897196), test loss: 36.4457042933\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (27.5544776917,52.1303631189), test loss: 54.0126174927\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (24.3361091614,32.4740350592), test loss: 31.1202983856\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (31.9152297974,51.9745880043), test loss: 45.5183250427\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (15.4802808762,32.3813165), test loss: 35.5694570541\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (28.5826950073,51.8192681097), test loss: 50.2058179855\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (45.0738945007,32.2891133532), test loss: 33.6927838802\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (89.4177703857,51.665859787), test loss: 51.2125607491\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (12.9779376984,32.1971989981), test loss: 31.7026381969\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (25.1459770203,51.5132370281), test loss: 45.3188167572\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (14.080493927,32.1058730261), test loss: 32.7959384918\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (37.6872634888,51.3604939358), test loss: 46.7110075474\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (29.49751091,32.0168861489), test loss: 32.0455393076\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (21.7743396759,51.210477695), test loss: 47.8634171963\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (20.5318450928,31.9285465676), test loss: 32.687040329\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (119.736000061,51.063002518), test loss: 47.2615633965\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (21.9113464355,31.8399223972), test loss: 31.2517030716\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (27.4675960541,50.9135339617), test loss: 45.759043169\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (42.3178367615,31.7522807171), test loss: 39.0650301933\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (62.4503898621,50.7656511771), test loss: 61.0647539139\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (11.6667194366,31.6657579767), test loss: 33.758390379\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (21.887588501,50.620200581), test loss: 46.4675473213\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (41.7924003601,31.5809017722), test loss: 33.0283082485\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (25.611782074,50.4761392819), test loss: 48.6818366051\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (18.6257400513,31.4958475353), test loss: 32.9920965195\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (26.0805053711,50.3319295884), test loss: 47.5067742825\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.41169166565,31.4102804079), test loss: 32.0310556412\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (16.2028045654,50.1889216654), test loss: 46.3783842087\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.1759314537,31.3267138106), test loss: 35.2753986835\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (17.2922859192,50.0479381584), test loss: 53.5449189186\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (11.9340744019,31.2441342507), test loss: 30.7690551281\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (14.6413640976,49.9083803898), test loss: 43.2993823051\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (49.9134597778,31.1625676118), test loss: 32.8731226921\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (111.851829529,49.7709212736), test loss: 48.8616603851\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 8\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (290.231811523,inf), test loss: 208.764561462\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (376.4921875,inf), test loss: 253.775780869\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (56.781463623,61.5780750084), test loss: 46.8363547325\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (66.7081451416,81.6136869812), test loss: 79.1510659218\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (48.5875930786,52.1527813029), test loss: 45.994263649\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (120.20790863,75.1462088909), test loss: 87.8415639877\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (19.6112174988,48.9637873027), test loss: 45.0891466141\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (69.0357589722,72.9065047563), test loss: 78.0068065643\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.2504062653,47.3504763441), test loss: 47.8908013344\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (62.1010932922,71.7550093582), test loss: 88.3778966904\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (13.0711650848,46.3614572487), test loss: 44.3640563965\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (28.4043998718,71.0260456022), test loss: 77.0818389893\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (42.769821167,45.6863756617), test loss: 46.7672590256\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (47.1583862305,70.5184029276), test loss: 78.8451196671\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (54.7103157043,45.2234888248), test loss: 46.6474094391\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (120.96937561,70.2239204836), test loss: 84.5405515671\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (31.7594795227,44.8312590424), test loss: 41.6675047874\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (31.1552410126,69.9014974011), test loss: 73.523116684\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (25.5115356445,44.5123297472), test loss: 46.792504406\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (18.4850406647,69.6288876251), test loss: 85.4564172745\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (19.1328907013,44.2576189103), test loss: 44.8547599792\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (75.6474151611,69.415412152), test loss: 78.7772098541\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (45.0811157227,44.0416392057), test loss: 44.579128027\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (40.51121521,69.2194800392), test loss: 76.7110143661\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (17.5457057953,43.8495056574), test loss: 46.1456622124\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (29.6968917847,69.0417272852), test loss: 81.2503797531\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (36.0261650085,43.6779095802), test loss: 43.0250162125\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (41.2920913696,68.8984459446), test loss: 75.2901800156\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (96.6258926392,43.531298042), test loss: 45.7127468109\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (126.720176697,68.7555564721), test loss: 81.4280321121\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (25.0338973999,43.3827533431), test loss: 44.2477517128\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (25.3839492798,68.6013324513), test loss: 83.6658407211\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (58.0347213745,43.2636370748), test loss: 44.0118951797\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (107.882598877,68.4789747356), test loss: 76.4702899933\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (15.7056436539,43.134458648), test loss: 46.339944458\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (43.0433731079,68.3400194782), test loss: 84.9763483047\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (21.252779007,43.012711583), test loss: 42.6669764519\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (25.8394603729,68.2046124077), test loss: 74.3499380112\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (78.4628295898,42.9073571558), test loss: 45.0655282021\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (68.7107849121,68.0797672997), test loss: 76.8292713165\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (33.8949661255,42.7929253722), test loss: 45.0759894371\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (69.9452819824,67.9498242673), test loss: 83.8019083023\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (11.483168602,42.6805945286), test loss: 39.8641303062\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (30.9281921387,67.8188979908), test loss: 71.0220966339\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (39.2263946533,42.5717153423), test loss: 46.4419500351\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (82.8368453979,67.6882986867), test loss: 85.7697004318\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (49.9439353943,42.4642908042), test loss: 44.409621191\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (98.7184906006,67.559047702), test loss: 79.1543611526\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (29.2495269775,42.3515548173), test loss: 43.9185927391\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (46.0729255676,67.4185281089), test loss: 76.1991752625\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (19.4707298279,42.2378039407), test loss: 45.9044919968\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (22.3906097412,67.2730083726), test loss: 82.8030952454\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (51.2439880371,42.1270466237), test loss: 41.2677567482\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (52.4142532349,67.1282146555), test loss: 74.1617639542\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (26.3885135651,42.0145123384), test loss: 44.9772615433\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (29.3782997131,66.9822703921), test loss: 82.1992850304\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (36.2012062073,41.8986609871), test loss: 44.3372487068\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (59.7929992676,66.8252755524), test loss: 85.1012187958\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (45.7938957214,41.781297908), test loss: 43.685444355\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (84.8830871582,66.6659503454), test loss: 76.0933620453\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (34.9488334656,41.6598771613), test loss: 45.4651070595\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (54.0522918701,66.4962679985), test loss: 83.6885767937\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (57.1046218872,41.5376280667), test loss: 40.2637263775\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (78.119430542,66.3214315485), test loss: 71.1770124435\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (22.0117797852,41.4091076765), test loss: 42.6296566486\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (64.4708404541,66.1356536856), test loss: 73.3363014221\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (108.155929565,41.2850013672), test loss: 42.049075079\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (216.502197266,65.9531065075), test loss: 78.9896774292\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (19.5535583496,41.1529883349), test loss: 38.1961186886\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (50.1088790894,65.7582552651), test loss: 66.6663061142\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (17.3262519836,41.0165952232), test loss: 43.3843910217\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (40.1875190735,65.5498361192), test loss: 79.9066488266\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (46.9224853516,40.87832004), test loss: 40.0604012489\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (60.0649452209,65.3351577659), test loss: 71.0230268478\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (38.3052330017,40.7373535564), test loss: 41.2612518787\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (55.1792411804,65.1155128331), test loss: 71.3221691132\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (57.1985855103,40.5964038515), test loss: 41.7896287441\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (71.1507339478,64.8894877051), test loss: 75.6893963814\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (30.7640647888,40.44974554), test loss: 36.0530365944\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (169.230682373,64.6621281634), test loss: 65.0023004532\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (31.0125637054,40.3006885486), test loss: 42.5692074776\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (36.5480041504,64.4223462906), test loss: 77.6545482635\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (16.5038223267,40.1519999786), test loss: 39.2441380978\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (9.86722373962,64.1794455231), test loss: 75.0988897324\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (93.6298522949,40.0023604205), test loss: 40.1988626957\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (76.4782409668,63.9315995459), test loss: 70.2465678215\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (23.1092300415,39.8505521698), test loss: 40.6455405235\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (21.1351356506,63.6828345385), test loss: 72.5689476013\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (29.24908638,39.6951037112), test loss: 38.1647522449\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (36.422328949,63.428095713), test loss: 68.0229308128\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (79.5749282837,39.5418998506), test loss: 40.0614609718\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (73.6754302979,63.1720207828), test loss: 69.2723381042\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (34.9704322815,39.3878491317), test loss: 38.9034543037\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (40.4214324951,62.9137851281), test loss: 76.5804894924\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (33.8960571289,39.2325186595), test loss: 36.8525821209\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (72.0999069214,62.6574494927), test loss: 64.1556284904\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.1956510544,39.0747431909), test loss: 40.8292726517\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (16.1992301941,62.3967815773), test loss: 77.2623907089\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (36.1648979187,38.9189660684), test loss: 37.0719834805\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (48.5058670044,62.1376866984), test loss: 67.0428125381\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (22.9981536865,38.7643831392), test loss: 41.3604365826\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (31.6599693298,61.8793087485), test loss: 73.3466262817\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (24.4325942993,38.6096454831), test loss: 41.4527109146\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (29.0643100739,61.620835833), test loss: 79.9008759499\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (26.1735343933,38.4548305538), test loss: 35.0668253899\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (27.6056575775,61.362879304), test loss: 65.9054625034\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (39.5711746216,38.3021230751), test loss: 40.6333147526\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (65.4966506958,61.1077183777), test loss: 78.1407234669\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (23.4163799286,38.1511040018), test loss: 41.2210821152\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (31.9355640411,60.8536774444), test loss: 76.4936493874\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.1964378357,38.0001104831), test loss: 41.2612343788\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (41.7227096558,60.6009215575), test loss: 75.3884364128\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (19.9701824188,37.8501632656), test loss: 38.1175638199\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (37.9275512695,60.3508538088), test loss: 70.6788743973\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.7132234573,37.700815623), test loss: 35.0053536892\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (40.300617218,60.0998137081), test loss: 63.4541752815\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.2229213715,37.5543070336), test loss: 39.702154541\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (35.2035064697,59.8517739516), test loss: 70.3923048973\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (36.5539932251,37.4092372787), test loss: 35.8376821041\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (46.8144111633,59.6072714146), test loss: 70.4788951874\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (33.789768219,37.2661645807), test loss: 42.1665870667\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (71.3267440796,59.3686705113), test loss: 77.4390223503\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (8.7056312561,37.1221610721), test loss: 38.7552315712\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (10.552397728,59.1261326574), test loss: 74.2788908005\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (19.7009277344,36.9806169604), test loss: 35.0892557621\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (27.9814529419,58.8858393073), test loss: 64.1977808475\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (24.6417388916,36.8412918374), test loss: 39.5209923029\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (24.7839050293,58.64954829), test loss: 70.4782354832\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (36.7535591125,36.7031454815), test loss: 36.9475035191\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (26.3179111481,58.4166880115), test loss: 72.5257143021\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (12.4708061218,36.5658440913), test loss: 32.9598737717\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (11.3224420547,58.1836874254), test loss: 61.6661101341\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (21.9961242676,36.4304515018), test loss: 38.7970583439\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (35.9560089111,57.9568840794), test loss: 74.9692834854\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (47.1819190979,36.2978043238), test loss: 34.7078933477\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (57.323600769,57.7310325226), test loss: 64.3723355293\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (8.20533466339,36.1649869763), test loss: 38.7391228676\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (14.4119281769,57.5066490315), test loss: 69.0988794327\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (40.9283905029,36.0349001928), test loss: 36.0438031673\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (86.5483932495,57.285460182), test loss: 68.3675741196\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (12.046672821,35.9049827524), test loss: 33.2160184383\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (19.3931884766,57.063835367), test loss: 61.4194584846\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (34.8273468018,35.7778029406), test loss: 38.0659640789\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (36.7080879211,56.8469622265), test loss: 74.0666676521\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (51.5919570923,35.652357893), test loss: 35.7461866379\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (33.6664848328,56.6332739354), test loss: 72.7055599213\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (24.4510154724,35.5269990681), test loss: 36.7402576208\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (18.5207710266,56.4200953014), test loss: 67.4017087936\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (10.2513237,35.403401028), test loss: 37.1200965405\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (32.4496536255,56.2111735244), test loss: 72.8058191299\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (16.4523010254,35.2817062548), test loss: 34.2861104727\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (31.2912273407,56.0044764063), test loss: 63.6106804848\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (38.2983894348,35.1615733194), test loss: 39.3699269295\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (58.0759162903,55.8020783795), test loss: 71.587283802\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (3.86964082718,35.041001656), test loss: 39.4676378727\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (9.95549297333,55.5984154726), test loss: 80.6794364452\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (11.50822258,34.9232851153), test loss: 33.4980678558\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (17.5110702515,55.3976850026), test loss: 63.3088435173\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (22.2417488098,34.8077497652), test loss: 38.0091777325\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (55.1028366089,55.2015086865), test loss: 73.3574846268\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (38.0334587097,34.6934452892), test loss: 33.7436019897\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (83.9155426025,55.0088679591), test loss: 63.7182923317\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (19.0612716675,34.5788696724), test loss: 37.370595789\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (37.5734863281,54.8156446174), test loss: 68.9508711815\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (21.4269561768,34.4662805397), test loss: 36.0866477966\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (44.0801086426,54.6246241965), test loss: 70.7664643288\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (24.5036487579,34.3559315277), test loss: 33.0300734043\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (32.1299209595,54.4371044209), test loss: 62.5697385788\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (37.2797546387,34.2464853401), test loss: 46.7673196793\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (47.7997932434,54.2528224262), test loss: 84.0112293243\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (16.7282485962,34.1368961065), test loss: 37.0737659454\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (42.1856842041,54.0691810356), test loss: 70.7534732819\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (36.3617477417,34.029668209), test loss: 37.1228832245\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (66.2936630249,53.8873916346), test loss: 67.9392253876\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (37.0297660828,33.9245488436), test loss: 35.1693660259\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (77.3276748657,53.710572518), test loss: 68.3857832909\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (17.0548992157,33.8191129929), test loss: 34.8273207664\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (19.8243103027,53.5332109562), test loss: 62.8468819618\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (19.4894561768,33.7143403153), test loss: 36.4162287235\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (29.8490047455,53.3570443758), test loss: 65.5593088627\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (28.9427680969,33.6114441111), test loss: 32.7472982407\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (68.3269119263,53.1830482598), test loss: 66.2695111275\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (46.1028900146,33.5109734049), test loss: 33.7539374352\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (50.986946106,53.0120218741), test loss: 61.1098173141\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (2.31259536743,33.410408603), test loss: 36.1145399332\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (10.9355010986,52.8433151096), test loss: 71.6580313683\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (26.1092948914,33.3104818182), test loss: 34.3937096119\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (39.6835098267,52.6780184282), test loss: 63.6766838074\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (52.2551498413,33.212598995), test loss: 39.3345313549\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (50.0249900818,52.5112458615), test loss: 69.6423480988\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (62.4406890869,33.1165048372), test loss: 34.2656967163\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (46.9689102173,52.3475132411), test loss: 66.6515423775\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (18.4016685486,33.0210113959), test loss: 32.0992251873\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (12.1018009186,52.1871753768), test loss: 59.6317419052\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (22.2000217438,32.9250026288), test loss: 37.2818190098\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (25.831577301,52.0263446584), test loss: 73.6751676083\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (19.850435257,32.8305329699), test loss: 36.1538668633\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (25.2431526184,51.8659644992), test loss: 70.5399990082\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (37.8283233643,32.7388762774), test loss: 35.8665724754\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (25.1534500122,51.7086726542), test loss: 66.3210902214\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (21.8306465149,32.6473400458), test loss: 34.3970567703\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (46.766368866,51.5547303823), test loss: 65.3113220215\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (21.2691726685,32.5555881706), test loss: 33.2228058338\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (19.945192337,51.4016847921), test loss: 60.4221762657\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (30.0434093475,32.4650201933), test loss: 36.8834358215\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (58.3127632141,51.2489898419), test loss: 65.6178160667\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (13.4042158127,32.3761156033), test loss: 37.545075655\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (16.3316383362,51.0984206934), test loss: 77.567845583\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (22.7166652679,32.2881205603), test loss: 40.0637461662\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (32.0767936707,50.9507687492), test loss: 75.7900255203\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (18.2818069458,32.2000328203), test loss: 36.0563159943\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (13.5384044647,50.8033590452), test loss: 73.4973788261\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (26.1716499329,32.1131920945), test loss: 34.6831143379\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (22.6617622375,50.6565987542), test loss: 65.0030938148\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (12.6853599548,32.0277484607), test loss: 37.5564468861\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (16.2508583069,50.5123884318), test loss: 67.8124559402\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (14.529955864,31.9435519778), test loss: 37.3850581169\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (29.0901145935,50.3711209675), test loss: 74.554227829\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (11.2917137146,31.8589734823), test loss: 33.2241783142\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (37.5494728088,50.2301839989), test loss: 60.683379364\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (6.96226358414,31.7749868238), test loss: 36.7632830143\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (16.9414634705,50.0885508129), test loss: 73.0746543884\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (12.4462070465,31.6929730298), test loss: 35.3139616489\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (33.4142036438,49.9497900691), test loss: 63.7105962753\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (38.9055519104,31.6121992806), test loss: 36.6160764217\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (74.0397872925,49.8143079485), test loss: 67.0528375149\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (34.857093811,31.531372363), test loss: 44.3607475281\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (103.307929993,49.6801922254), test loss: 82.3030725479\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (9.68759918213,31.4509160306), test loss: 33.2481239319\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (25.5542888641,49.5447806542), test loss: 60.3066389084\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (15.5408039093,31.3714221596), test loss: 36.7146443844\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (12.0495815277,49.4096259138), test loss: 71.5647423744\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (27.8729171753,31.2934154105), test loss: 36.4978633881\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (26.3498954773,49.2779004805), test loss: 73.6824066639\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (32.6698303223,31.2155974958), test loss: 40.8831259966\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (28.8358306885,49.1480370424), test loss: 73.6804813862\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (22.090883255,31.1384886007), test loss: 35.7554674149\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (28.8638000488,49.0174698095), test loss: 71.253494072\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (21.1924247742,31.061723635), test loss: 34.0274153233\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (108.756217957,48.8889498697), test loss: 61.3744941711\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 9\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (290.217132568,inf), test loss: 200.713718414\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (376.270172119,inf), test loss: 234.018132782\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (37.7284698486,59.9641802139), test loss: 42.820548439\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (59.1267318726,76.0818649712), test loss: 65.2022831917\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (55.7332115173,50.5308369832), test loss: 45.7990411758\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (73.0035858154,69.617417841), test loss: 79.7202838898\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (120.64541626,47.3947166894), test loss: 41.0746857643\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (242.786254883,67.5121623491), test loss: 67.003537178\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (12.589758873,45.7747895641), test loss: 42.5070754051\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (11.7165803909,66.3491541386), test loss: 70.4321531296\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (44.6318283081,44.8076696949), test loss: 43.9094909668\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (40.0988998413,65.6375028168), test loss: 77.4657284737\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (41.9574127197,44.1498625937), test loss: 42.0306809425\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (44.1078910828,65.188099802), test loss: 64.2889568329\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (58.9601325989,43.6739545719), test loss: 43.256294775\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (90.2956695557,64.8233212354), test loss: 72.6503581047\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (28.1996898651,43.3151225246), test loss: 43.150330019\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (57.1957168579,64.5534140265), test loss: 69.7010822296\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (57.8102226257,43.0153923784), test loss: 42.9893577576\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (48.7846221924,64.304106315), test loss: 64.8794922829\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (21.70378685,42.7539874221), test loss: 44.8773306847\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (38.1559715271,64.1029637262), test loss: 75.8152284622\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (8.79963588715,42.5383956144), test loss: 42.5208010674\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (19.2112426758,63.9351046952), test loss: 69.2413715363\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (55.3341598511,42.3547572865), test loss: 42.0731622696\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (44.4287185669,63.7755802333), test loss: 69.9372533798\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (31.1342220306,42.1956074085), test loss: 44.6930931091\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (61.970199585,63.6501775626), test loss: 79.7354776382\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (17.641708374,42.0437447309), test loss: 41.5048657417\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (45.946559906,63.5217490008), test loss: 66.856335783\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (42.8325157166,41.9064211653), test loss: 43.4238417625\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (46.4986152649,63.3976312636), test loss: 73.5171998978\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (32.3250045776,41.7932986917), test loss: 44.2028776169\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (47.2306671143,63.3098543844), test loss: 71.8739879608\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (46.0032920837,41.6768081807), test loss: 40.9178774834\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (84.1292572021,63.1955605191), test loss: 62.6066806793\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (49.9998588562,41.5711811119), test loss: 43.3295233727\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (190.329299927,63.1029410518), test loss: 71.653501606\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (18.7670307159,41.4702088054), test loss: 41.4406907082\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (18.7868995667,62.9965189038), test loss: 66.9325509071\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (40.0782546997,41.3815853003), test loss: 41.5638511896\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (56.9223060608,62.9079032645), test loss: 63.3752403259\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (71.385307312,41.2853151409), test loss: 44.3200906754\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (67.5333480835,62.8054461248), test loss: 76.9431629181\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (8.52222442627,41.190956065), test loss: 39.5881500244\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (32.3609237671,62.7082746116), test loss: 65.198129797\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (49.5390090942,41.0998782908), test loss: 41.9543104172\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (78.6697692871,62.6153917794), test loss: 69.3327968597\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (32.4403495789,41.0074472141), test loss: 42.9202366829\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (71.7299194336,62.514054534), test loss: 77.2793386459\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (44.5357055664,40.9226536569), test loss: 41.2233115196\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (53.5873413086,62.4216585684), test loss: 63.6972570419\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (29.0422782898,40.8335494134), test loss: 42.6676037312\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (37.8495292664,62.3247677199), test loss: 73.1539909363\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (17.7666358948,40.7418291682), test loss: 42.2291132927\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (47.4723930359,62.2220617077), test loss: 68.6150686264\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (12.3891973495,40.6606863438), test loss: 41.642868042\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (25.7806777954,62.1347343641), test loss: 62.5464983463\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (34.307434082,40.5704085547), test loss: 42.9694437027\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (58.2006034851,62.0253843227), test loss: 72.6141469955\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (11.8897781372,40.4831796365), test loss: 41.4085634232\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (25.7031345367,61.9189442841), test loss: 65.9986215591\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (21.6791439056,40.3958847946), test loss: 40.2484827995\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (14.6296415329,61.8173592806), test loss: 66.1019908905\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (26.8042125702,40.3094669965), test loss: 40.8341254234\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (29.2486858368,61.7095414197), test loss: 72.7207429886\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (16.9744186401,40.2123984419), test loss: 38.3002763271\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (26.3277397156,61.5896458821), test loss: 62.9335441113\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (31.4100379944,40.118077479), test loss: 41.1504978895\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (70.1660232544,61.4720946315), test loss: 69.4855168819\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (43.3100814819,40.0156909146), test loss: 41.8702206612\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (64.6603317261,61.3467825289), test loss: 69.0959048271\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (31.7737560272,39.9129167219), test loss: 38.7577983856\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (28.8618125916,61.2155130167), test loss: 60.5058284283\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (33.1871643066,39.8095226189), test loss: 41.6903741837\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (46.66456604,61.0849724193), test loss: 70.0398034096\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (13.9725399017,39.7026847399), test loss: 41.2316049576\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (31.3025951385,60.9488159826), test loss: 67.0548755646\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (12.303352356,39.5906273127), test loss: 38.8814668655\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (25.891576767,60.8021831654), test loss: 58.9108909607\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (38.2516860962,39.4836363039), test loss: 40.8049543858\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (73.696472168,60.6651471177), test loss: 70.3738667488\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (44.4499664307,39.3659376983), test loss: 36.8306262016\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (58.6234169006,60.5057904617), test loss: 61.0711686134\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (44.0760803223,39.2519368985), test loss: 39.0792209148\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (57.6957588196,60.3510677174), test loss: 64.328523922\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (82.5625610352,39.1331878472), test loss: 38.9140725136\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (103.898094177,60.1934825172), test loss: 69.7298892021\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (38.1129989624,39.015115985), test loss: 38.0918771744\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (71.8287811279,60.030766715), test loss: 58.6206711769\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (26.6189079285,38.8890724207), test loss: 38.9705321312\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (35.391746521,59.856188409), test loss: 66.6335447311\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (25.5059547424,38.7652414179), test loss: 39.2032011032\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (67.4870605469,59.6830577328), test loss: 64.9350246429\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (33.3713111877,38.6343118731), test loss: 37.2092036486\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (83.521697998,59.5030669991), test loss: 56.9282942295\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (13.4861803055,38.5052967389), test loss: 40.6604040623\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (18.6488780975,59.3230262493), test loss: 69.6316571236\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (53.7160644531,38.3741309142), test loss: 38.6887109041\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (101.722564697,59.1384268439), test loss: 62.4946001053\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (46.5325393677,38.2432315347), test loss: 37.2534488201\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (87.8409729004,58.9551347187), test loss: 58.6714926243\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (36.6602172852,38.1072316586), test loss: 37.0429011345\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (47.0795860291,58.761013446), test loss: 65.986013031\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (25.392168045,37.9765909511), test loss: 35.3563576698\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (38.1510772705,58.5769785565), test loss: 57.8423550606\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (28.8447685242,37.8391722664), test loss: 37.4932379246\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (25.2968120575,58.3780438994), test loss: 63.0061149597\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (41.7798423767,37.7074907088), test loss: 37.2626533031\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (34.433265686,58.1869884399), test loss: 61.3314778328\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (28.423915863,37.5709077192), test loss: 35.8276052952\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (37.7014160156,57.9908808091), test loss: 55.2669705391\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (89.8986053467,37.4406095867), test loss: 38.1363967896\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (116.119384766,57.7991248439), test loss: 62.079665184\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (24.1132698059,37.3033349921), test loss: 37.3071940422\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (26.270866394,57.5973269449), test loss: 61.0083863258\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (41.56665802,37.1711538784), test loss: 36.2565293312\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (33.6857566833,57.4011493283), test loss: 54.8910945892\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (20.5640354156,37.0329470051), test loss: 38.4255931854\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (30.5375919342,57.1985851573), test loss: 66.9101042747\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (23.5290031433,36.9004734447), test loss: 38.6803941727\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (37.5457839966,57.0051282486), test loss: 65.7703636646\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (43.4877357483,36.7645270302), test loss: 36.3887049198\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (66.4045028687,56.8022614484), test loss: 61.0696385384\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (23.9532928467,36.6325927318), test loss: 42.5892237186\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (36.1569900513,56.6083977735), test loss: 76.9693604946\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (30.3662815094,36.4969232514), test loss: 36.1402231216\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (61.2603759766,56.4060469854), test loss: 56.5191541672\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (71.3408813477,36.3680661967), test loss: 39.1425860405\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (152.393920898,56.2160170463), test loss: 63.6157254219\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (5.87953281403,36.2334848256), test loss: 35.7070518017\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (19.893289566,56.0127857985), test loss: 58.7318449974\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (45.4359970093,36.1054133734), test loss: 34.8665431976\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (43.5494155884,55.8202960428), test loss: 53.6852294922\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (23.3610095978,35.9741747672), test loss: 35.3127677917\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (33.9593849182,55.6224305979), test loss: 59.8816560745\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (44.0362739563,35.848060785), test loss: 34.1644090176\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (21.7647457123,55.4307336525), test loss: 55.0592488766\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (12.6871891022,35.7189711803), test loss: 34.6351897717\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (26.0072574615,55.2330376137), test loss: 52.2407530308\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (47.0926475525,35.5943098694), test loss: 37.2571052551\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (33.6307182312,55.0426894783), test loss: 67.8116471291\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (7.42370414734,35.4655160946), test loss: 32.7736117363\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (13.4438056946,54.8454089687), test loss: 55.4306254387\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (13.6903953552,35.3426360752), test loss: 36.3173523903\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (20.2749633789,54.6597091683), test loss: 63.1220729828\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (17.5425739288,35.2160462537), test loss: 34.9239459038\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (31.5367012024,54.4639061126), test loss: 58.8183385849\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (11.614356041,35.0953053052), test loss: 36.1240212917\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (34.9785919189,54.28095387), test loss: 56.6577992916\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (3.3375389576,34.9705585414), test loss: 34.3516453743\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (13.0016288757,54.0878838721), test loss: 58.6917903423\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (33.9063644409,34.8517874759), test loss: 33.9117766857\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (50.7129821777,53.9068196751), test loss: 55.6464994907\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (16.2996940613,34.730724974), test loss: 35.6927710533\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (13.0995359421,53.718706558), test loss: 54.1228233337\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (14.6481771469,34.6135807691), test loss: 34.8840758562\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (55.275226593,53.5392184525), test loss: 60.7393683434\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (27.1869716644,34.4952062116), test loss: 32.7351564884\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (147.733566284,53.3546019737), test loss: 54.1172570229\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (10.9906978607,34.3808379197), test loss: 39.3765487671\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (16.1322250366,53.178010492), test loss: 62.3191205025\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (16.6216583252,34.2652626878), test loss: 33.4878394604\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (17.6996498108,52.9947587084), test loss: 60.1873590469\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (21.8608512878,34.1527799762), test loss: 36.4672494888\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (19.5629997253,52.820604102), test loss: 59.6840821266\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.15515422821,34.0383385624), test loss: 34.3157837868\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (28.982131958,52.6399537416), test loss: 58.1225745201\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (37.4366226196,33.9286648321), test loss: 39.6137802601\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (79.478515625,52.4711385718), test loss: 65.7330147266\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (27.7683448792,33.8160603753), test loss: 34.2761218548\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (55.4445495605,52.2929961972), test loss: 52.2218070984\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (23.6347770691,33.7088063331), test loss: 36.7291782856\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (42.6627349854,52.1275377221), test loss: 63.5540481567\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (18.8272781372,33.5989219534), test loss: 35.1965658665\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (24.8430347443,51.9541412623), test loss: 57.2370680809\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (16.7208480835,33.4932146785), test loss: 39.280416441\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (30.7315483093,51.7907885891), test loss: 58.4425461769\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (17.7547950745,33.3872854916), test loss: 33.739225769\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (32.2612190247,51.6232068832), test loss: 59.5689368248\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (32.8905792236,33.2842199831), test loss: 35.0577651501\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (27.6898727417,51.4620948839), test loss: 59.2798249245\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (10.5904626846,33.1806051068), test loss: 35.1400561333\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (6.56219387054,51.2959983873), test loss: 57.5561045647\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (51.5440750122,33.080689166), test loss: 35.6776350498\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (54.5160255432,51.1418642396), test loss: 61.6051082611\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (19.4099063873,32.9799723121), test loss: 34.5576281548\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (23.8124656677,50.9795332681), test loss: 52.6320454597\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (20.9132843018,32.8817686572), test loss: 34.075232029\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (26.5571632385,50.8254916727), test loss: 59.7007351875\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (27.5898780823,32.7828467762), test loss: 34.6884605885\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (55.6882362366,50.6658831478), test loss: 56.2404789925\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (41.2985610962,32.6864589555), test loss: 37.8555250168\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (47.3552398682,50.5153065957), test loss: 56.1900705338\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (17.065990448,32.5893247788), test loss: 35.5915596485\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (21.2678623199,50.3592601621), test loss: 63.0447434902\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (11.9831733704,32.4956678982), test loss: 34.5252264261\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (16.9141044617,50.2129949469), test loss: 56.1751003265\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (14.2441358566,32.4009561315), test loss: 35.5006363869\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (28.2861061096,50.0616087886), test loss: 57.5264953613\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (16.3888607025,32.3085388484), test loss: 34.4976151943\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (30.3640098572,49.9169805023), test loss: 59.344653511\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (34.125,32.2171699502), test loss: 33.1559620857\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (65.4669342041,49.7708317672), test loss: 53.2887561798\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (17.210100174,32.126291902), test loss: 34.7293871403\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (31.5725898743,49.6282978298), test loss: 58.7366137505\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (49.3285064697,32.0370884773), test loss: 33.8798888683\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (56.9515075684,49.4837687377), test loss: 55.0991498947\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (24.9668922424,31.9482519012), test loss: 35.0838562965\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (18.3333148956,49.3464771257), test loss: 53.0720871925\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (19.6849708557,31.8612345998), test loss: 34.6956716537\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (31.9622116089,49.205218938), test loss: 56.5468556404\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (23.5210762024,31.7744429626), test loss: 37.6296416283\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (16.5022087097,49.0690340949), test loss: 59.9317025661\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (28.0073013306,31.6882333895), test loss: 35.213043499\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (33.2812423706,48.9293445593), test loss: 52.6950133324\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (18.824836731,31.6024416269), test loss: 33.6278316021\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (31.6620864868,48.7959875708), test loss: 58.6557200432\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (21.1841373444,31.5182057121), test loss: 36.6713579178\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (20.7076835632,48.660275804), test loss: 61.2105872154\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (27.6599197388,31.4346369708), test loss: 35.217041254\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (69.3974227905,48.5298844008), test loss: 57.4455080986\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (25.7175712585,31.3518400555), test loss: 33.6740783215\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (51.1679267883,48.3972916643), test loss: 59.5561233521\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (29.0083179474,31.2692169291), test loss: 51.3479764938\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (31.2122993469,48.2679643257), test loss: 75.8432353973\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (31.8409938812,31.1891049303), test loss: 41.5339566231\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (68.3080749512,48.1394141135), test loss: 66.6066131592\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (22.7254867554,31.1081554296), test loss: 34.6482131004\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (13.5270385742,48.0121530563), test loss: 55.3643274307\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (33.7706031799,31.0298080818), test loss: 36.6266886234\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (22.4076156616,47.8851161643), test loss: 55.3306258202\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (28.2975387573,30.9507220331), test loss: 35.6360606194\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (52.0792999268,47.7623040554), test loss: 60.3871482372\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (59.7637557983,30.8745326675), test loss: 35.3245889664\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (60.76612854,47.637431012), test loss: 53.896409893\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (23.5517539978,30.7970021218), test loss: 35.854539156\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (35.5134124756,47.5151682047), test loss: 60.0563329697\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (28.8699645996,30.7213561338), test loss: 36.195247364\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (23.5584316254,47.3912633301), test loss: 65.0528441906\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 10\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (248.176742554,inf), test loss: 201.602082825\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (302.379608154,inf), test loss: 239.926378059\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (42.8565368652,61.3693131409), test loss: 40.2512813568\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (67.6208114624,84.8504518366), test loss: 65.1579595566\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (20.1483764648,52.1965081668), test loss: 43.1621729851\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (30.925113678,78.3805838857), test loss: 74.7596208572\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (11.7990512848,49.0833130509), test loss: 39.6757032394\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (25.033908844,75.9860084263), test loss: 68.2340379715\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (54.5741882324,47.5656072979), test loss: 39.5952157497\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (81.9900512695,74.9105888865), test loss: 64.1746687889\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (19.7787342072,46.6095763885), test loss: 43.6310336113\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (74.9721221924,74.1584158003), test loss: 76.4639530182\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (28.9610462189,45.9126321905), test loss: 39.2828092575\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (45.3498039246,73.600589148), test loss: 67.973531723\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (47.7615623474,45.4433349382), test loss: 39.2875335217\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (59.0806007385,73.2382199298), test loss: 63.9890902519\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (9.61193466187,45.0458069814), test loss: 43.5001763344\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (46.2163848877,72.9243007224), test loss: 76.5146230698\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (35.2202148438,44.7509160644), test loss: 38.736797595\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (28.2378120422,72.7006028052), test loss: 66.4125191689\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (36.9095916748,44.5047523495), test loss: 38.8035038471\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (41.7589492798,72.4936208838), test loss: 63.3190285683\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (34.6140670776,44.3087355925), test loss: 42.3229357719\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (62.9695854187,72.3113109286), test loss: 75.7075788498\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (10.171792984,44.1168020683), test loss: 37.1304808855\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (25.6127853394,72.124315441), test loss: 66.4319018364\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (26.4360733032,43.9374422134), test loss: 38.5883482456\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (30.2165699005,71.9402128579), test loss: 69.3589606762\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (30.2204360962,43.7958191241), test loss: 42.5889164925\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (65.2578277588,71.8176947335), test loss: 77.7205701828\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (93.8414154053,43.6531287664), test loss: 37.3163759708\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (196.521392822,71.6820146243), test loss: 66.4910339355\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (21.6445732117,43.5211718194), test loss: 37.9335196972\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (67.6884307861,71.5467564543), test loss: 67.7736834049\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (81.1910552979,43.4047672772), test loss: 39.9972258568\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (107.474784851,71.4249449431), test loss: 74.8464320183\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (22.8224620819,43.2922489234), test loss: 35.7813904762\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (24.7909088135,71.2939367912), test loss: 64.2064320087\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (30.1085777283,43.1787579252), test loss: 38.0737441063\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (92.104598999,71.1629286333), test loss: 67.5990099907\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (53.5227050781,43.0674383243), test loss: 39.8594650269\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (53.3212852478,71.0211794791), test loss: 75.7670068741\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (30.2043495178,42.9630801596), test loss: 35.7382257938\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (59.3091278076,70.9096300156), test loss: 64.3190027237\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (33.151802063,42.8621707596), test loss: 38.1445065498\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (68.843963623,70.7975045677), test loss: 67.6161323547\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (63.4093284607,42.7598413875), test loss: 39.7017206192\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (77.8531646729,70.6577846868), test loss: 74.6539083481\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (51.6390838623,42.6571770195), test loss: 34.5452183723\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (83.5112533569,70.529326031), test loss: 61.624090004\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (80.261177063,42.5613102692), test loss: 39.0771698475\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (66.7270431519,70.3962754023), test loss: 70.2010070801\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (35.0121841431,42.4460575725), test loss: 38.8229164124\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (45.2257919312,70.2493537843), test loss: 74.4499588966\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (22.8414878845,42.3380866876), test loss: 34.1729214668\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (23.673614502,70.1058438179), test loss: 60.851373291\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (45.9289703369,42.2249141001), test loss: 38.8988960266\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (67.5409317017,69.9551532184), test loss: 69.897621727\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (28.2331848145,42.1125250291), test loss: 38.0218025208\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (45.4129638672,69.8080755712), test loss: 72.0927709579\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (43.0126190186,41.9980903836), test loss: 33.7247861862\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (184.74559021,69.6457231407), test loss: 58.8112236977\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (43.610584259,41.886603844), test loss: 37.5991036892\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (82.8689727783,69.4791547615), test loss: 67.4787061691\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (31.0485191345,41.7660286753), test loss: 37.3283340931\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (91.6651382446,69.2991338697), test loss: 65.9839097977\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (33.632976532,41.6363184183), test loss: 33.4115583181\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (44.0580711365,69.1042112556), test loss: 57.6292915344\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (24.0982589722,41.5118550963), test loss: 37.5380629539\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (48.1844711304,68.9180364294), test loss: 68.949371767\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (36.1609115601,41.377035124), test loss: 36.8982635498\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (64.7519683838,68.7175621353), test loss: 65.0696842194\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (50.5129547119,41.2464118611), test loss: 32.965390873\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (55.8556442261,68.5172182209), test loss: 56.1019937515\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (28.4636974335,41.108992528), test loss: 35.7507563591\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (37.1343460083,68.3055068354), test loss: 65.2531421661\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (31.2742366791,40.9735132662), test loss: 35.7824864388\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (52.2395362854,68.0871590441), test loss: 63.2217007637\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (16.005361557,40.8289564037), test loss: 32.1166618109\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (24.2704696655,67.8550132234), test loss: 54.2780416727\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (40.6991691589,40.6793528543), test loss: 36.2075470448\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (74.3274993896,67.6150120406), test loss: 64.0262181282\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (15.2084197998,40.5341062761), test loss: 35.2872782707\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (49.3386459351,67.3837742286), test loss: 61.1834447861\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (64.8663787842,40.383601596), test loss: 33.2640721798\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (179.946166992,67.1438356454), test loss: 56.7482002258\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (42.8803901672,40.230793027), test loss: 36.3861643791\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (43.1429100037,66.8901754848), test loss: 63.2991128922\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (12.4114599228,40.0787237098), test loss: 33.1524878502\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (16.1569595337,66.6399612295), test loss: 57.8926938057\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (73.8977355957,39.9266840491), test loss: 32.7244645596\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (70.5334243774,66.384929588), test loss: 53.6932800293\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (46.2993812561,39.7694867257), test loss: 35.6384543896\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (54.8166885376,66.1223250004), test loss: 62.5312761307\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (74.9809799194,39.6143420454), test loss: 32.5911943913\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (172.070343018,65.8638197659), test loss: 56.9609278679\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (14.9128341675,39.4567332683), test loss: 32.237225008\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (35.4807815552,65.6041081198), test loss: 53.3088533401\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (21.1695461273,39.3030836214), test loss: 35.3053141594\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (45.2756958008,65.3496032838), test loss: 62.4400215149\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (23.9421768188,39.1472177924), test loss: 32.4106667042\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (28.5975494385,65.083382487), test loss: 56.2155656338\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (59.6073226929,38.9946309981), test loss: 34.8581216335\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (70.2277984619,64.8254494659), test loss: 58.0883255959\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (7.6067404747,38.8413133738), test loss: 35.2704140663\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (28.1851425171,64.5643963607), test loss: 63.3619738102\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (6.03694868088,38.6851529921), test loss: 33.0332581997\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (12.535528183,64.3026043316), test loss: 57.8725090981\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (32.0975608826,38.5357063142), test loss: 31.9906154156\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (44.3414459229,64.0488372392), test loss: 53.7538422585\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (13.8279504776,38.3810555462), test loss: 33.8460422516\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (29.4367675781,63.7910582176), test loss: 61.4428030968\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (26.1571598053,38.2323119454), test loss: 30.0894110441\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (14.5125627518,63.5399463223), test loss: 53.2809612751\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (35.1771697998,38.0840056126), test loss: 30.3464379072\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (163.610595703,63.2897983624), test loss: 52.9122135162\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (23.8685188293,37.93870726), test loss: 33.0392252922\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (31.4852294922,63.0392456532), test loss: 60.9032474518\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.2150306702,37.7925844502), test loss: 29.9850624561\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (31.5654411316,62.7882956549), test loss: 54.3009519577\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (13.6289367676,37.6453302771), test loss: 30.2625495672\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (16.8881092072,62.5374346692), test loss: 56.2209049702\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (30.2089805603,37.5041291391), test loss: 34.1093338013\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (59.1468963623,62.2970531353), test loss: 64.9573198318\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (43.4084472656,37.3605422547), test loss: 28.1677182674\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (55.1327095032,62.0535612219), test loss: 51.1873376846\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (32.8726654053,37.2204422885), test loss: 31.6479575634\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (25.3966941833,61.8146331645), test loss: 58.2762726784\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (31.3180618286,37.0823299696), test loss: 30.8910466671\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (37.9570922852,61.5776450782), test loss: 59.3064226151\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (17.6853599548,36.9449702854), test loss: 28.3130047083\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (25.303188324,61.340662128), test loss: 51.5772111416\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (16.0173454285,36.8081957196), test loss: 30.2865040541\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (37.0001411438,61.1035561818), test loss: 54.9408428192\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (23.926525116,36.6723675867), test loss: 31.066668272\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (40.0820083618,60.8683955242), test loss: 59.5840707779\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (22.8164596558,36.5394943274), test loss: 28.4552778244\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (28.5240116119,60.6424242127), test loss: 51.3979915142\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (22.607717514,36.4081995257), test loss: 35.968342638\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (53.5677986145,60.4165365132), test loss: 64.9638065338\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (51.232006073,36.2771186534), test loss: 31.2240673542\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (49.9902534485,60.1882286568), test loss: 59.5112012386\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (15.6736354828,36.1473582845), test loss: 29.3902158737\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (10.7552375793,59.963787815), test loss: 52.2525134087\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (71.3546524048,36.0212786168), test loss: 31.5656799316\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (47.1695632935,59.741201902), test loss: 56.9992536545\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (29.4255409241,35.8927252962), test loss: 34.3256023407\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (46.2557373047,59.5194622938), test loss: 66.0648048401\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (46.7893066406,35.7691105995), test loss: 28.6441862106\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (91.0530700684,59.302868005), test loss: 50.5016317844\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (35.4935417175,35.6433529742), test loss: 39.5478234291\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (39.5520782471,59.0858074338), test loss: 69.8014911652\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (16.37748909,35.5205691954), test loss: 32.0501909733\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (13.2394981384,58.8740119752), test loss: 54.8599786758\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (6.82615852356,35.3977828207), test loss: 28.8054450989\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (12.7192792892,58.6587486847), test loss: 49.8348207951\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (42.6513519287,35.2765296301), test loss: 31.8134666681\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (94.8625335693,58.4524484451), test loss: 58.7111963272\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (5.9907078743,35.1544244719), test loss: 31.3761838913\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (29.8594970703,58.2401822778), test loss: 54.397134304\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (21.5297641754,35.0331176909), test loss: 31.3833072186\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (32.0271110535,58.0333299461), test loss: 55.229964447\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (13.8725299835,34.9146924953), test loss: 30.2822336912\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (44.6793899536,57.8314849418), test loss: 56.9608799934\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (21.1133499146,34.7951415819), test loss: 30.7066236019\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (58.2574882507,57.628345082), test loss: 53.6010482788\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (15.2505683899,34.6776474278), test loss: 29.6000584245\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (28.03855896,57.4297047103), test loss: 49.5920951366\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (21.1770076752,34.561938923), test loss: 30.535866797\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (39.9268264771,57.2320178961), test loss: 54.3595764637\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (13.5516862869,34.447808247), test loss: 30.7368597031\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (29.0718765259,57.0353342106), test loss: 52.6135690689\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (20.2697696686,34.3335609643), test loss: 29.5912864685\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (12.9743003845,56.8391590647), test loss: 49.5459158421\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (13.7294979095,34.2205990173), test loss: 31.4706535101\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (13.5093212128,56.6442551135), test loss: 55.9540816307\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (12.2769069672,34.1090209843), test loss: 31.2135303974\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (73.6373214722,56.4565134242), test loss: 53.173299551\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (62.5665664673,33.9986976031), test loss: 39.2469485283\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (145.778045654,56.2680523675), test loss: 66.9764823914\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (19.1479682922,33.8891508386), test loss: 32.5516576052\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (52.7620887756,56.0805724444), test loss: 56.3536790848\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (43.4280204773,33.781707455), test loss: 32.2437173843\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (44.5411911011,55.8971579043), test loss: 53.4245054245\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (20.9524326324,33.6750938407), test loss: 31.2911855221\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (26.0760612488,55.7124773468), test loss: 50.9104744911\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (38.486869812,33.5685336155), test loss: 32.4716321468\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (51.6499252319,55.5298170953), test loss: 55.5078276634\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (80.7869186401,33.4643512929), test loss: 32.461226964\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (120.523796082,55.3497862417), test loss: 53.4566497803\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (3.68664741516,33.3594221086), test loss: 31.1122329712\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (17.873052597,55.1711732277), test loss: 50.2103552341\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (40.9009132385,33.2568973954), test loss: 32.1955357313\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (71.4093170166,54.9978374821), test loss: 56.4540543556\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (40.4166221619,33.1560255835), test loss: 31.9421905041\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (51.2417068481,54.8218648746), test loss: 52.4626637459\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (56.2488746643,33.0550523999), test loss: 37.2509884834\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (51.7792205811,54.6495077308), test loss: 60.6982013702\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (30.3458042145,32.9558556231), test loss: 33.2661215544\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (29.2428283691,54.4767974393), test loss: 58.9996492386\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (16.7703742981,32.8567848373), test loss: 32.2597441196\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (21.6114044189,54.3059338123), test loss: 55.2536506653\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (13.8432092667,32.758775861), test loss: 31.0275306702\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (21.6789054871,54.1385692173), test loss: 50.4802071095\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (10.8091259003,32.661860643), test loss: 33.438243866\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (27.579454422,53.9714520945), test loss: 58.1069107056\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (13.238117218,32.5655236638), test loss: 30.3637566566\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (19.4046421051,53.8080912076), test loss: 51.7150472641\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (25.0170841217,32.4709488529), test loss: 30.7339159966\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (133.521209717,53.6453452607), test loss: 54.7761669159\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (16.5070343018,32.3779576264), test loss: 31.5285926342\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (12.3775424957,53.483013029), test loss: 55.504184866\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (27.4109802246,32.2843673836), test loss: 30.7226127625\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (68.938041687,53.3217154788), test loss: 52.9085161209\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (23.0715827942,32.1922706741), test loss: 30.4296396255\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (34.0648498535,53.1611657509), test loss: 52.9033676147\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (17.6124095917,32.1011950739), test loss: 36.9348102331\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (38.7736244202,53.0048965755), test loss: 67.8905851364\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (19.2688407898,32.0100101454), test loss: 29.7094779015\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (31.463716507,52.8487987244), test loss: 51.0440181732\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (28.9345054626,31.9212968282), test loss: 32.2131062984\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (29.0659656525,52.6951022134), test loss: 56.0317687988\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (28.6339950562,31.8320951566), test loss: 30.4185949802\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (25.224445343,52.5425573223), test loss: 54.9043700218\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (16.6296825409,31.7449297337), test loss: 30.5604962826\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (27.5210056305,52.3902068611), test loss: 51.8961352348\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (12.6239414215,31.6575909841), test loss: 30.7734227061\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (27.593460083,52.238526234), test loss: 55.1429316521\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (22.4264316559,31.5709727002), test loss: 31.6566182613\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (55.2824401855,52.0889772612), test loss: 55.535297966\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (10.2928009033,31.4863291801), test loss: 30.3371195316\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (14.7789640427,51.9425896486), test loss: 51.229015255\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (26.9717521667,31.4010155769), test loss: 47.7567122936\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (49.098449707,51.7965658471), test loss: 87.8591091156\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (22.5248737335,31.3173781187), test loss: 40.5721767426\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (14.1284866333,51.6507046664), test loss: 70.9692875862\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (8.79810714722,31.2345824977), test loss: 35.816447258\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (14.9535455704,51.5078900925), test loss: 58.0279985428\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (31.8604125977,31.1522140651), test loss: 32.3434754372\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (15.9921245575,51.3644461996), test loss: 56.0248582363\n",
      "run time for single CV loop: 2078.52397895\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 6\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (311.540405273,inf), test loss: 213.207393265\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (411.784240723,inf), test loss: 255.532064819\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (55.7178421021,64.3142899361), test loss: 40.0667804718\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (106.992095947,92.9669808025), test loss: 60.5259699821\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (28.8371543884,54.528874609), test loss: 41.3108949661\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (55.5123062134,82.4003811998), test loss: 70.6812345505\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (104.819313049,51.3188489761), test loss: 37.0736770153\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (274.063537598,78.9863730075), test loss: 60.1788455963\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (10.788854599,49.6292992604), test loss: 40.338163662\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (13.2926511765,77.1042596965), test loss: 66.2784573555\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (62.7044563293,48.6166021669), test loss: 38.8797847748\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (64.3418579102,75.9422957924), test loss: 68.7685335159\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (37.862613678,47.9115749997), test loss: 38.3952225685\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (153.380599976,75.1927733968), test loss: 58.6239553452\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (22.0153083801,47.4091093682), test loss: 40.2051229477\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (26.64427948,74.5995677109), test loss: 65.6712516785\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (21.8807144165,47.0296075904), test loss: 38.9183734894\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (62.6394004822,74.1624503269), test loss: 62.2991668701\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (86.6904067993,46.7230616102), test loss: 40.1221451283\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (75.5315551758,73.7810205782), test loss: 59.928401947\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (11.8716163635,46.4375553446), test loss: 40.795892334\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (35.1235084534,73.4532836815), test loss: 68.1596339226\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (47.7185325623,46.2043302605), test loss: 37.4978221893\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (96.6025390625,73.1802485912), test loss: 61.8115235329\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (38.273979187,45.9899055966), test loss: 39.3857690811\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (41.4435195923,72.9091587876), test loss: 65.7840486526\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (33.0801200867,45.8186157352), test loss: 39.5464803219\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (37.8188056946,72.7065637685), test loss: 70.2627568245\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (31.9711208344,45.6537344582), test loss: 38.0426429749\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (58.4190979004,72.50602833), test loss: 59.4515634537\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (49.1926841736,45.4962867399), test loss: 39.1488358498\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (71.3468093872,72.3051971367), test loss: 64.7968657494\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (61.0283432007,45.3653333473), test loss: 38.7184892654\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (104.162643433,72.150884001), test loss: 61.9154607773\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (21.374753952,45.221074145), test loss: 37.8838302612\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (22.4045982361,71.9506721176), test loss: 57.2006567001\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (55.8481369019,45.0980687246), test loss: 40.3767520905\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (69.4071044922,71.7760570771), test loss: 65.8281421661\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (41.498008728,44.9729119134), test loss: 38.013068819\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (48.0717010498,71.6131451568), test loss: 60.2924267769\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (116.709060669,44.8619409707), test loss: 38.2464305401\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (149.250915527,71.4469940031), test loss: 57.8727275848\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (30.4811515808,44.7420202552), test loss: 38.8609791756\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (48.3406105042,71.2751524518), test loss: 67.5290083885\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (19.7006111145,44.6336424226), test loss: 35.3238032341\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (37.0575942993,71.1038402966), test loss: 56.5965836525\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (40.5127601624,44.5147336974), test loss: 38.5233262062\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (57.604927063,70.9280576623), test loss: 63.7234801292\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (9.02103424072,44.3982404984), test loss: 37.8501671791\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (10.320186615,70.7504164149), test loss: 62.994229126\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (45.0427703857,44.2824180145), test loss: 36.2507789135\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (44.1018867493,70.5482605199), test loss: 56.0332553387\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (40.6951522827,44.1726110768), test loss: 40.8914705276\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (59.4934082031,70.3542074811), test loss: 65.9836736202\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (35.9348144531,44.0540626647), test loss: 37.7511482239\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (49.3465385437,70.1453920037), test loss: 60.4977098465\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (34.6094818115,43.9341033845), test loss: 36.9447687149\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (75.693321228,69.9297042006), test loss: 54.5943271637\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (12.8106956482,43.8181396825), test loss: 37.6093244553\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (22.882598877,69.7194065661), test loss: 62.6637742043\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (45.308807373,43.6929995712), test loss: 33.8741150379\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (45.4680557251,69.4804535062), test loss: 54.6042673111\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (11.8774299622,43.5679751437), test loss: 37.1359205246\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (20.7937488556,69.2444474157), test loss: 60.5345221519\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (77.0237503052,43.4432768337), test loss: 35.5513088703\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (102.544906616,69.0116371673), test loss: 62.1968243599\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (64.4687652588,43.3148491052), test loss: 34.9634502888\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (114.563354492,68.7620543548), test loss: 52.6740343094\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (24.1933841705,43.1779062575), test loss: 35.9399879456\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (30.9759273529,68.501196508), test loss: 59.6928652763\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (21.8978271484,43.0450287342), test loss: 35.4651156425\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (65.0890808105,68.2385479486), test loss: 56.6122303963\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (31.0238456726,42.9015729145), test loss: 34.9233551025\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (58.2394676208,67.9669709371), test loss: 52.2863842964\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (31.2114181519,42.7566160256), test loss: 38.3199745655\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (42.4434623718,67.6903116872), test loss: 66.7630515099\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (75.1051940918,42.6104787171), test loss: 34.3611898422\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (163.278427124,67.4130042296), test loss: 57.6064991951\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (33.2566833496,42.458684286), test loss: 36.7596396685\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (74.7463684082,67.1304701215), test loss: 61.5445249557\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (14.037730217,42.3020627331), test loss: 34.7737791061\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (34.9179153442,66.8428877708), test loss: 60.0467658043\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (65.1847991943,42.1464073591), test loss: 31.6695969105\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (101.397392273,66.5572544751), test loss: 50.2459022522\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (23.4314651489,41.9852127126), test loss: 34.4106356144\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (36.0845108032,66.2667533815), test loss: 56.9487658501\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (39.5204124451,41.8228662129), test loss: 34.3698776722\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (63.4107551575,65.9724357877), test loss: 55.0100774765\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (43.1878356934,41.659976888), test loss: 32.7337434292\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (187.333175659,65.6858547887), test loss: 50.114230442\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (11.7920284271,41.4944913033), test loss: 35.2869767189\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (15.2648153305,65.3896995681), test loss: 57.2073908806\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (19.4627380371,41.3301124848), test loss: 32.3437523842\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (27.0086898804,65.099614449), test loss: 51.2548643112\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (77.214302063,41.1656920617), test loss: 33.3956425667\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (54.6553115845,64.8100850041), test loss: 50.955230999\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (21.4806842804,40.996987882), test loss: 33.5905778408\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (64.4885864258,64.519144819), test loss: 59.3868370056\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (35.6649208069,40.8285162536), test loss: 31.0043703079\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (64.446105957,64.2311067558), test loss: 53.3968325615\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (13.9354381561,40.6602161856), test loss: 34.0850843191\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (11.1454763412,63.9459855178), test loss: 58.2091389656\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (30.8543376923,40.4955194764), test loss: 34.092905426\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (44.6967010498,63.6675376248), test loss: 61.6699104309\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (39.2990341187,40.3277627767), test loss: 31.8482158661\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (73.6643447876,63.3866754355), test loss: 49.9024512291\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (43.0943145752,40.1619919391), test loss: 35.7912582397\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (43.2704086304,63.1097590136), test loss: 60.4989743233\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (32.710483551,39.9989980015), test loss: 32.6420198917\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (75.3112945557,62.8396470018), test loss: 55.0143187523\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (28.4165267944,39.8336106683), test loss: 32.966760087\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (31.9030132294,62.5642007124), test loss: 49.85277071\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (53.7567520142,39.6736430929), test loss: 33.4127066851\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (46.4264450073,62.2971271187), test loss: 57.0126667023\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (22.5017032623,39.5125603879), test loss: 29.8826386929\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (31.4687576294,62.0338038128), test loss: 49.4330778122\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (52.3119049072,39.3539177377), test loss: 32.0391355515\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (54.4086532593,61.7690110832), test loss: 54.0100786209\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (22.7394332886,39.197642367), test loss: 32.3578789711\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (27.9867935181,61.5116456066), test loss: 59.0737979889\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (35.0090141296,39.043444377), test loss: 31.3584098101\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (31.6378860474,61.2535282311), test loss: 50.3541254997\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.2016067505,38.8863191518), test loss: 32.2347255707\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (20.757232666,60.9961273751), test loss: 55.5821452141\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (20.1890525818,38.7341605273), test loss: 35.5361354828\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (27.206697464,60.7475051482), test loss: 62.5809891701\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.8268737793,38.5814208155), test loss: 31.5414323807\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (24.5630054474,60.4973201064), test loss: 50.5247132301\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (22.9505691528,38.4321446576), test loss: 37.5472358227\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (26.9469871521,60.2528700678), test loss: 65.7813707352\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (10.3353967667,38.2839720518), test loss: 30.3108391762\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (18.2714557648,60.013226102), test loss: 49.1567198753\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.3802995682,38.1368224), test loss: 35.3400538921\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (40.5781211853,59.7732396445), test loss: 57.3487309456\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (37.0192909241,37.9918904438), test loss: 30.3490381002\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (87.5485687256,59.5385499173), test loss: 53.9962003708\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (42.0031089783,37.8494178493), test loss: 30.3188891888\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (29.1101741791,59.3039904784), test loss: 50.3149687767\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (21.1050567627,37.7086302822), test loss: 31.6861339569\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (23.7844238281,59.0730760987), test loss: 54.4165556908\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (31.2638816833,37.5668960825), test loss: 31.3857382774\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (33.3198051453,58.844940062), test loss: 51.5674880981\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (31.8438034058,37.431576094), test loss: 32.2320605516\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (64.0928421021,58.6223834408), test loss: 51.7765317917\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (36.9681091309,37.294680499), test loss: 32.4213692904\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (38.635635376,58.3983025002), test loss: 54.6540727139\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (21.150302887,37.1590271004), test loss: 31.0679412842\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (16.0550270081,58.1743887651), test loss: 50.9684498787\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (13.3992042542,37.0260656985), test loss: 31.5841558456\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (32.5444717407,57.9588775491), test loss: 49.5397084236\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.9085617065,36.8944705956), test loss: 39.9144956112\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (33.5266685486,57.7444149402), test loss: 76.3895231247\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (55.6572494507,36.7625471282), test loss: 29.2841744423\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (65.6607894897,57.5295327846), test loss: 49.2070729256\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (7.95352268219,36.6356003191), test loss: 33.7385682106\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (35.6673164368,57.3244084886), test loss: 58.200699234\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (10.2111434937,36.5077709957), test loss: 32.6939623833\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (15.2961845398,57.1173392364), test loss: 57.9140622616\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (25.2288780212,36.3803257669), test loss: 30.7413933992\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (30.7314243317,56.9109230304), test loss: 48.612338829\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (16.2970638275,36.2582433215), test loss: 31.8913261414\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (14.340180397,56.7132374073), test loss: 53.3423986435\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (12.205368042,36.1353833873), test loss: 32.2916828156\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (45.8924980164,56.5131094041), test loss: 52.0261200905\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (3.00445103645,36.0133689147), test loss: 32.2882610321\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (11.1371917725,56.3140044546), test loss: 49.912330246\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (27.8938903809,35.8955010204), test loss: 33.327567625\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (28.5443992615,56.122766582), test loss: 56.1378580093\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.3027067184,35.7775831501), test loss: 29.6758726597\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (14.4007892609,55.9309080316), test loss: 48.9056321144\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (42.6385574341,35.6604185232), test loss: 31.0398155689\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (32.4562911987,55.7391446924), test loss: 52.5833110809\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (35.9981384277,35.5460276161), test loss: 30.5976653576\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (69.5537109375,55.5510486896), test loss: 54.9409096718\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (38.8870162964,35.4318511803), test loss: 30.9489406109\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (42.5229530334,55.3658207951), test loss: 49.8007014275\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (11.024728775,35.3175450151), test loss: 31.6618714809\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (26.107925415,55.1813695107), test loss: 55.326831007\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (27.9747543335,35.207647002), test loss: 31.1337825298\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (60.1134529114,55.0018210836), test loss: 51.105350399\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (21.2493591309,35.0976314375), test loss: 31.4487487793\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (35.6114730835,54.8239796185), test loss: 49.4966175079\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (21.8445396423,34.9870492474), test loss: 33.2193264008\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (42.2438583374,54.6462677995), test loss: 57.0795257568\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (46.3866729736,34.8801680207), test loss: 35.531714201\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (121.968833923,54.4722896302), test loss: 60.2410470963\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (11.1450920105,34.7741353173), test loss: 31.3349261999\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (13.5000209808,54.3008268125), test loss: 48.6408827782\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (38.9953956604,34.6683523537), test loss: 36.2466697693\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (35.2228164673,54.1283640533), test loss: 62.2064334393\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (19.9798431396,34.5643041499), test loss: 30.2992418528\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (77.2860031128,53.9597650247), test loss: 49.5585334778\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (13.3798208237,34.4620541257), test loss: 32.1668314457\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (24.9422950745,53.7931010967), test loss: 53.5315401077\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (11.0687255859,34.3603987152), test loss: 31.5866533279\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (18.8760890961,53.6275259208), test loss: 51.7811756134\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (37.907043457,34.2600862232), test loss: 31.1718225002\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (21.7696456909,53.461013889), test loss: 49.3547716141\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (18.8769130707,34.161131898), test loss: 32.8905076504\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (16.9154624939,53.3003051724), test loss: 53.0772049904\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (17.2802238464,34.0619765142), test loss: 45.7539502621\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (30.2098388672,53.1398091296), test loss: 80.0809249878\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (20.982093811,33.9634489175), test loss: 32.2186882019\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (20.654548645,52.9785098509), test loss: 49.7348370552\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (15.8317127228,33.8691425457), test loss: 33.8441023111\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (20.6256141663,52.8257623164), test loss: 57.7392808914\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.39372634888,33.7724504419), test loss: 29.8279970169\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (15.3972301483,52.6702655944), test loss: 49.0379735947\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (10.600856781,33.6764963157), test loss: 38.5753176212\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (16.7832336426,52.5132423833), test loss: 65.6502479553\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (37.3398361206,33.5851756811), test loss: 31.541135025\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (77.1613388062,52.3661420811), test loss: 55.2650781631\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (13.2290649414,33.4917154766), test loss: 32.4895896435\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (19.2328910828,52.2139559671), test loss: 50.6282306671\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (48.793422699,33.3997999102), test loss: 31.1007900238\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (61.126625061,52.0630046832), test loss: 53.497845602\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (25.5898590088,33.309725384), test loss: 35.2002675533\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (35.0364341736,51.9172897321), test loss: 55.4861136436\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (50.1988105774,33.220343484), test loss: 39.5628136158\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (52.5477981567,51.7707611485), test loss: 64.5241378784\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (31.8785686493,33.1302500464), test loss: 36.2287543535\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (27.4459114075,51.6234034506), test loss: 62.7072229385\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (16.9741897583,33.0440033473), test loss: 30.6791258812\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (15.7937088013,51.4804653391), test loss: 50.2880831718\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.18502616882,32.9561175397), test loss: 31.8226800442\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (23.0311832428,51.3379974175), test loss: 53.6973932266\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (11.0006809235,32.8686939671), test loss: 33.8052259207\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (25.8806190491,51.1951997773), test loss: 62.0101815701\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (10.2151832581,32.784279828), test loss: 32.0410998821\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (18.3012752533,51.0568678895), test loss: 52.216274929\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (15.5222501755,32.699839341), test loss: 32.1613692284\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (43.0329818726,50.9201377246), test loss: 54.5961423874\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (20.7337799072,32.6140678539), test loss: 37.6002584457\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (36.0209197998,50.7806747128), test loss: 58.689915657\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (26.458147049,32.5323230505), test loss: 32.2064187527\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (59.8433074951,50.6469169251), test loss: 50.3974484444\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (8.90141868591,32.4500952552), test loss: 33.5960489035\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (9.38849258423,50.5140926226), test loss: 53.9243242264\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (23.3290557861,32.366808796), test loss: 39.9513676167\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (13.7547454834,50.3774597601), test loss: 61.784031868\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 7\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (147.870895386,inf), test loss: 199.159858704\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (172.183059692,inf), test loss: 229.959825134\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (40.5457572937,60.6245836439), test loss: 39.13760252\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (88.3089904785,87.6298775415), test loss: 56.502173996\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (38.4340057373,51.4010421183), test loss: 41.4611279488\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (46.1953544617,77.7045108385), test loss: 62.6152439594\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (13.3600234985,48.2759812126), test loss: 36.8706120491\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (31.4985122681,74.2844665499), test loss: 55.7243370056\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (125.32913208,46.7355481949), test loss: 40.3409506798\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (300.881988525,72.6582878883), test loss: 58.1659574509\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (25.5758705139,45.7573304739), test loss: 40.3109177589\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (28.3665370941,71.5643041571), test loss: 61.2854275703\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (58.2225952148,45.0918167325), test loss: 37.3457308292\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (63.2966117859,70.7857435667), test loss: 53.9728821754\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (51.5074729919,44.6141654841), test loss: 40.9710940361\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (53.3323898315,70.2329360397), test loss: 58.0470878601\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (56.6524047852,44.2450246138), test loss: 37.4091743469\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (219.568084717,69.8299612653), test loss: 56.1776295662\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (19.6428375244,43.9422858265), test loss: 38.120027256\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (9.98431015015,69.4568639165), test loss: 54.0048386574\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (56.3663749695,43.7049511608), test loss: 38.8312326431\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (96.1983642578,69.1679804274), test loss: 59.3820933342\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (44.3743667603,43.4881847928), test loss: 36.2948252201\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (62.8559989929,68.9065064261), test loss: 53.9731348038\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (134.584625244,43.310069153), test loss: 39.3685296535\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (120.759811401,68.6754814103), test loss: 57.5234962463\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (50.3529586792,43.1442630619), test loss: 39.137825489\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (123.564842224,68.4698055217), test loss: 60.0215173721\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (40.0835494995,42.9835373706), test loss: 36.9321875572\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (81.6753158569,68.2676605321), test loss: 52.869883728\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (21.2676200867,42.8438514403), test loss: 40.5624339104\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (29.2521495819,68.0867508539), test loss: 58.3174246788\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (31.6943492889,42.7126966783), test loss: 35.3194260597\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (29.4220733643,67.9082576011), test loss: 55.5062181473\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (25.9999103546,42.5961002451), test loss: 37.400014925\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (40.6889610291,67.7534148171), test loss: 52.6102396965\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (36.3198776245,42.4819598849), test loss: 39.2605527878\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (77.6652145386,67.5957471936), test loss: 60.879955864\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (36.7449111938,42.3730661742), test loss: 35.8640779734\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (42.5022964478,67.447307829), test loss: 53.2225804806\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (17.608455658,42.2652586663), test loss: 38.6638623714\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (32.8865585327,67.293343901), test loss: 56.2321050644\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (124.484664917,42.1689150061), test loss: 38.3275410175\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (268.879547119,67.160877613), test loss: 57.9693449974\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (21.1010627747,42.0659383028), test loss: 35.9687352657\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (24.5835590363,67.0112427747), test loss: 51.8304834366\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (50.3826255798,41.9655714116), test loss: 39.3188246727\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (61.1603507996,66.8537098364), test loss: 56.5791872025\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (44.1000823975,41.8695991362), test loss: 33.7358815193\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (47.1989822388,66.7021807299), test loss: 52.5572685242\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (53.7085380554,41.774619952), test loss: 36.4532083511\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (213.039611816,66.5585696816), test loss: 51.0912332535\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (20.4886932373,41.6768977187), test loss: 37.2465981007\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (22.427728653,66.3969159055), test loss: 57.1111357689\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (59.2387199402,41.5848408772), test loss: 34.9783993721\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (98.8519439697,66.2417837448), test loss: 50.6780311584\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (35.6645889282,41.4865636707), test loss: 37.6142067909\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (51.6478614807,66.0791335726), test loss: 54.1858117104\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (128.354064941,41.3920062176), test loss: 36.9992383003\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (111.061416626,65.9125097769), test loss: 56.00580616\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (39.8585968018,41.2930178104), test loss: 35.2310418367\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (95.4321594238,65.7423361376), test loss: 49.8771008015\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (36.289932251,41.1883507409), test loss: 37.9787591934\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (74.3684387207,65.5641527681), test loss: 54.3842895985\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (22.1811695099,41.0850129165), test loss: 33.5988094807\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (32.4715156555,65.3719108098), test loss: 52.086259079\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (22.796705246,40.9783429702), test loss: 35.3235459328\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (17.9529514313,65.168517883), test loss: 48.7135076523\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (24.4614067078,40.8734177749), test loss: 35.9097653866\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (34.443813324,64.9688663232), test loss: 54.2551642656\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (27.0571784973,40.7631802637), test loss: 34.6386636257\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (53.8041496277,64.7577511912), test loss: 50.4760286331\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (33.3905830383,40.6513739516), test loss: 36.3703710556\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (33.8154067993,64.5442845477), test loss: 50.425662899\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (19.036113739,40.5352820558), test loss: 34.3979343891\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (32.8857383728,64.3206564821), test loss: 50.6063951492\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (111.178237915,40.4208546115), test loss: 34.6875558138\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (206.362884521,64.1013039201), test loss: 49.0116182327\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (16.636800766,40.2993682863), test loss: 36.1437374115\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (15.5865221024,63.870949725), test loss: 51.6245987892\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (42.5491600037,40.1760762403), test loss: 31.5727214336\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (52.4520111084,63.631574343), test loss: 47.0574184418\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (40.7586860657,40.0522392779), test loss: 33.9956878185\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (39.9765052795,63.3909166864), test loss: 46.7190598488\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (46.3681488037,39.9261923396), test loss: 34.25532341\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (190.602203369,63.1525215167), test loss: 51.1573107243\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (17.7054481506,39.795996106), test loss: 31.622791338\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (23.8186073303,62.9032686005), test loss: 44.9395687103\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (78.8871459961,39.6673969688), test loss: 35.4933123112\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (106.203483582,62.6570474288), test loss: 48.1534340382\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (23.6301441193,39.5331376159), test loss: 32.6320029259\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (29.9271392822,62.4061715256), test loss: 48.5982435703\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (110.175842285,39.4004909989), test loss: 32.3629171848\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (81.3763656616,62.1543594683), test loss: 45.0488328934\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (21.1173648834,39.2635507904), test loss: 33.6743256092\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (56.1822624207,61.9023738471), test loss: 48.4615669727\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (27.2044830322,39.1238778562), test loss: 29.3784155369\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (53.7665596008,61.6512604591), test loss: 44.2751476288\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (21.8266716003,38.9847969556), test loss: 32.4887277603\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (34.0149345398,61.4017682), test loss: 45.3418284893\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (16.335193634,38.8432265169), test loss: 32.9837620735\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (11.5136814117,61.1497067374), test loss: 50.1466135979\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (35.4745521545,38.7039689123), test loss: 30.1845435143\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (70.5647583008,60.9050152847), test loss: 43.7746563911\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (16.3644809723,38.561923224), test loss: 36.3384793282\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (31.1634235382,60.6588246049), test loss: 51.3203918457\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (26.8771457672,38.4208040434), test loss: 31.4645054817\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (28.4213676453,60.4169177756), test loss: 46.8169811726\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.6840858459,38.2778960777), test loss: 32.4088200092\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (37.7458686829,60.17294361), test loss: 47.9005948067\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (86.513381958,38.1372486833), test loss: 32.5555362701\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (162.996536255,59.9360506968), test loss: 49.2295871258\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (10.4565439224,37.9944676817), test loss: 28.7330948353\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (7.7749171257,59.698558779), test loss: 44.025998497\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (35.0330314636,37.8532322853), test loss: 32.1049545765\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (43.2444076538,59.4622556596), test loss: 45.2549971104\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (37.9761009216,37.7130192531), test loss: 32.2836637497\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (32.536895752,59.2287636778), test loss: 48.3417771816\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (36.4176597595,37.5733245441), test loss: 29.3032289505\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (166.160766602,59.0003329175), test loss: 43.1029063702\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (16.0044517517,37.4326800939), test loss: 32.4381808519\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (26.3048419952,58.7681284752), test loss: 43.5093885899\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (61.935874939,37.2957308716), test loss: 30.2796073437\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (82.1960296631,58.5423846772), test loss: 44.8671931744\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (17.0217094421,37.1575187872), test loss: 30.2002176762\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (22.0919075012,58.3183699028), test loss: 43.3180447578\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (74.436164856,37.0216906008), test loss: 31.7552650452\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (51.2165527344,58.0952209203), test loss: 46.5802534103\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.5379619598,36.8848759061), test loss: 29.0836185932\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (38.1751022339,57.8728908554), test loss: 44.3520067215\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (17.789352417,36.7483348462), test loss: 31.9890054226\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (35.4375839233,57.6536887081), test loss: 44.5835925102\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (16.0519771576,36.6148053004), test loss: 33.1556209564\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (28.475227356,57.439475404), test loss: 52.4118407249\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (14.9809246063,36.4808200428), test loss: 29.2464581013\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (10.803440094,57.2247322412), test loss: 42.5099277496\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (51.224937439,36.3495811227), test loss: 31.8766744375\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (126.690887451,57.0152454746), test loss: 44.4861836433\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (13.5991373062,36.217373357), test loss: 35.0544973373\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (30.7592258453,56.8044717913), test loss: 54.6441562653\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (23.4540805817,36.0878205509), test loss: 29.4612804413\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (30.0376529694,56.5983321808), test loss: 41.717026329\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (15.0934944153,35.9584458958), test loss: 31.0589623928\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (37.086025238,56.3925881584), test loss: 46.7234266281\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (73.0755767822,35.8314941965), test loss: 31.0078541756\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (142.321868896,56.1927549167), test loss: 50.0376499176\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.6448049545,35.7038467583), test loss: 30.3384609699\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (13.1702070236,55.9919234574), test loss: 44.2984732628\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (28.7352294922,35.5781572073), test loss: 30.7513818741\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (38.8140106201,55.7910965842), test loss: 46.0052018642\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (32.763004303,35.4543395517), test loss: 29.3648579121\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (26.2207736969,55.5941304208), test loss: 42.7608571053\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (28.2177734375,35.3318808959), test loss: 31.1007444143\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (151.191864014,55.4021398206), test loss: 43.8473701477\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (16.804561615,35.2094650643), test loss: 28.2781900167\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (26.4457397461,55.2077505954), test loss: 43.3646191597\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (50.4741439819,35.0896580137), test loss: 30.8219900846\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (66.9937515259,55.0165772895), test loss: 44.7073052883\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.8835668564,34.9701806053), test loss: 29.7092479706\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (21.8737030029,54.8273501142), test loss: 43.8377280235\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (53.8019599915,34.8526485358), test loss: 29.3162463188\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (36.3492507935,54.6394789552), test loss: 44.5700608253\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (10.1612844467,34.7353333665), test loss: 30.7063838482\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (28.7076759338,54.4532208114), test loss: 43.6073381901\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.6809501648,34.6187788735), test loss: 30.5100823402\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (20.4654464722,54.2703549141), test loss: 45.1177160263\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (10.2166938782,34.5046129436), test loss: 30.7326480627\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (18.5345191956,54.0896682215), test loss: 47.3113209248\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (15.8161373138,34.3905311536), test loss: 30.5504794121\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (9.41045188904,53.908954049), test loss: 43.128947258\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (55.6902198792,34.2786244407), test loss: 28.6411873341\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (143.107025146,53.7331104779), test loss: 44.5576540947\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (13.3620243073,34.1666158497), test loss: 32.1369157791\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (30.4117641449,53.5577672336), test loss: 46.1151986122\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (23.5576190948,34.056969918), test loss: 30.466752553\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (33.0139503479,53.3853026554), test loss: 44.5954793453\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (15.4246253967,33.9474063872), test loss: 32.6498999119\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (33.1889762878,53.2117583683), test loss: 51.0681288242\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (62.2648696899,33.8395749742), test loss: 34.5476322174\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (126.35370636,53.0432214591), test loss: 53.7978604317\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (14.5114269257,33.7320058995), test loss: 31.1690558195\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (18.9196777344,52.8757180981), test loss: 45.9928922176\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (19.4863471985,33.6261576731), test loss: 30.4923975945\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (36.0795974731,52.7088355559), test loss: 44.3189637661\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (31.4036102295,33.5218486097), test loss: 32.8695447206\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (25.0841369629,52.5435412499), test loss: 45.6472426414\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (22.9738063812,33.4182934108), test loss: 28.7532883167\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (134.36114502,52.3810862747), test loss: 44.7785629272\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (18.7925491333,33.3151090962), test loss: 30.924057579\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (25.938035965,52.2179819086), test loss: 43.4517776489\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (46.1735992432,33.21396199), test loss: 31.6985255241\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (64.6008911133,52.0582786557), test loss: 47.6206768036\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (12.4878883362,33.1137441402), test loss: 30.569350481\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (23.29945755,51.9003645262), test loss: 44.9363156796\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (46.9559936523,33.0145117416), test loss: 31.2999104023\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (32.7117271423,51.741916463), test loss: 43.7403598309\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (13.7238483429,32.9154446298), test loss: 31.6804456711\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (25.2371025085,51.584350111), test loss: 47.0842485428\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (7.37349414825,32.8169513909), test loss: 32.4506577969\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (13.7517623901,51.4305332457), test loss: 45.5315149307\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (7.29454517365,32.7209769012), test loss: 34.642580986\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (15.3884458542,51.2795017518), test loss: 53.1609643459\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (15.3530845642,32.6251682268), test loss: 29.2368163109\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (9.13799095154,51.128357194), test loss: 45.0672400475\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (53.2477645874,32.5303711768), test loss: 32.9824420452\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (126.307510376,50.9790024665), test loss: 47.4259288788\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (13.2722139359,32.4351379295), test loss: 36.1254345655\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (30.8414344788,50.830153403), test loss: 53.1413226128\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (22.33984375,32.34206417), test loss: 31.1078221798\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (32.0503768921,50.6848071601), test loss: 45.3913984776\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (16.4464073181,32.2495198157), test loss: 34.2639879227\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (28.9914188385,50.5400651782), test loss: 49.9834996223\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (47.3156356812,32.1572321351), test loss: 32.6545287132\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (89.7998123169,50.3974350154), test loss: 49.4604587555\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (12.7623167038,32.0647888349), test loss: 31.4835994005\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (21.4513130188,50.2557445584), test loss: 45.3761773586\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (14.279378891,31.9725710192), test loss: 32.6278444529\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (38.1723594666,50.114058927), test loss: 47.7518262863\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (28.2147026062,31.8823881092), test loss: 31.3072129726\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (22.8823699951,49.9753859587), test loss: 48.1287887096\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (20.2675380707,31.7928420626), test loss: 32.8657385826\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (116.126724243,49.8388968739), test loss: 48.8032808304\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (21.2596855164,31.7030932028), test loss: 31.2469931602\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (29.1140613556,49.7004376773), test loss: 45.5629014015\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (43.1396331787,31.6142849201), test loss: 36.5820742607\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (60.6153068542,49.563328872), test loss: 57.3902155876\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (12.1857671738,31.5265634773), test loss: 32.8678552628\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (22.3637809753,49.4290608398), test loss: 44.4251144409\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (40.5379104614,31.440391253), test loss: 32.8076913834\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (26.879283905,49.2959595254), test loss: 47.6412532806\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (17.4489212036,31.3543198856), test loss: 32.4715929031\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (25.6136417389,49.1626336562), test loss: 46.4338775635\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.39426231384,31.2678820666), test loss: 31.9979471684\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (13.889415741,49.030408868), test loss: 45.9419080734\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.68086576462,31.1832006391), test loss: 33.4613539696\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (13.7754735947,48.900236112), test loss: 57.1693211079\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (12.7856788635,31.0994566529), test loss: 30.8025194407\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (11.4498081207,48.7717041759), test loss: 43.9487767696\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (50.9994735718,31.0170174649), test loss: 32.506302166\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (110.040237427,48.6449299681), test loss: 48.3282004356\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 8\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (302.221862793,inf), test loss: 217.743582153\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (390.562103271,inf), test loss: 268.56598053\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (57.5278282166,62.4373188219), test loss: 47.0762422562\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (66.0953521729,88.9659969521), test loss: 79.2127016068\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (49.0146255493,52.653740231), test loss: 46.2634237289\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (118.84148407,78.6764845867), test loss: 87.5206922531\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (19.4467258453,49.3387878237), test loss: 45.430436039\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (67.9304199219,75.1492381236), test loss: 78.0012573242\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.454832077,47.6596747341), test loss: 48.0981133461\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (61.6706695557,73.3495971255), test loss: 88.233912468\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (13.4221916199,46.6293730154), test loss: 44.5870018959\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (28.1634292603,72.2285488205), test loss: 76.9712324142\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (42.3588180542,45.9244477091), test loss: 47.0324432373\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (45.0699920654,71.4562719692), test loss: 78.8908050537\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (55.3242835999,45.4386680133), test loss: 46.8288578033\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (120.304885864,70.9705566503), test loss: 84.1465475082\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (31.9392261505,45.026883889), test loss: 41.9197894096\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (31.3227901459,70.5031856178), test loss: 73.338202858\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (25.2594165802,44.69111819), test loss: 46.9309806824\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (17.7148246765,70.1152691329), test loss: 85.2045297623\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (19.4905204773,44.4219948694), test loss: 45.0321091652\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (76.1077651978,69.8074483373), test loss: 78.3981409073\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (44.3591156006,44.193064116), test loss: 44.8018565178\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (40.4492034912,69.5314464255), test loss: 76.5522043228\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (17.7717247009,43.9896150347), test loss: 46.2817873001\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (29.4110107422,69.284455752), test loss: 80.843463707\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (35.1605834961,43.8077524232), test loss: 43.2068383217\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (40.2956581116,69.0803870313), test loss: 74.8969100952\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (97.0951385498,43.6516558225), test loss: 45.8748661041\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (128.368026733,68.8830143775), test loss: 81.1964995384\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (24.9520988464,43.4940495774), test loss: 44.3280086517\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (25.9237060547,68.6788898977), test loss: 83.0190248489\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (57.6426277161,43.3662410255), test loss: 44.1970246315\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (106.775588989,68.5100903478), test loss: 76.2339580536\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (15.7437086105,43.228900524), test loss: 46.443065834\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (43.0337753296,68.3277750311), test loss: 84.5812988281\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (21.0583934784,43.0993804491), test loss: 42.8026817322\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (24.9166870117,68.1512041199), test loss: 73.9817121506\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (77.9060668945,42.9862842938), test loss: 45.2594922066\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (65.8483886719,67.9868551869), test loss: 76.7011857986\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (33.6278839111,42.8643409386), test loss: 45.179736805\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (69.1456298828,67.8189803133), test loss: 83.1321996689\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (11.4216098785,42.7445960729), test loss: 40.0051589489\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (30.1510658264,67.6511170028), test loss: 70.6127351761\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (39.2291603088,42.6283780361), test loss: 46.4984984398\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (80.9900817871,67.4846238982), test loss: 85.35962677\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (51.421005249,42.516457812), test loss: 44.4441862583\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (97.9703140259,67.3203504455), test loss: 78.8922977448\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (28.9685020447,42.4012395932), test loss: 44.1305636406\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (43.6653289795,67.1417069801), test loss: 76.2171827316\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (19.039899826,42.2849189347), test loss: 45.9497180462\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (21.0206871033,66.9413664632), test loss: 82.4291273117\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (51.9564208984,42.1720208511), test loss: 41.3886875629\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (50.0459861755,66.7421569724), test loss: 73.5647008896\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (26.3944091797,42.0574226852), test loss: 45.1016708374\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (28.96134758,66.5418793431), test loss: 81.626491642\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (37.3004570007,41.939759729), test loss: 44.3034328461\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (61.2288894653,66.3308815202), test loss: 84.7052484512\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (45.6082992554,41.8206146297), test loss: 43.7835665703\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (84.0712585449,66.1176430808), test loss: 75.7098924637\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (34.4622001648,41.6974348508), test loss: 45.4769735813\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (50.4855079651,65.8947671427), test loss: 82.747032547\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (56.4971847534,41.5735526896), test loss: 40.3819973946\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (72.6332855225,65.6675802199), test loss: 69.3336384773\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (21.7429733276,41.4436867102), test loss: 42.8935934067\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (63.0006446838,65.4304792391), test loss: 72.4387623787\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (108.731033325,41.3183860801), test loss: 42.2130845547\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (206.46711731,65.1966663091), test loss: 76.6385679245\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (19.7956104279,41.1854530437), test loss: 38.4312850952\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (54.1336097717,64.9521700717), test loss: 65.7268655777\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (17.2097263336,41.0483233491), test loss: 43.5595399857\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (38.4837265015,64.695715003), test loss: 78.5315986633\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (47.1102600098,40.9094583285), test loss: 40.3633874416\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (58.3317108154,64.4342637826), test loss: 69.4664720535\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (37.954208374,40.7678838625), test loss: 41.4611614227\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (50.7142219543,64.1687831763), test loss: 70.7420052528\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (57.5308074951,40.6263222478), test loss: 42.0153295994\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (71.5458450317,63.8997042763), test loss: 73.9482114792\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (32.7178611755,40.4792627679), test loss: 36.25190382\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (172.2371521,63.6320419236), test loss: 63.6684114456\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (31.2562618256,40.3296007973), test loss: 42.7389517784\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (34.9976539612,63.3539778967), test loss: 76.9669430733\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (15.9024753571,40.1801866843), test loss: 39.2521790028\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (8.0262594223,63.075550789), test loss: 73.2372934341\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (92.6105880737,40.0293802282), test loss: 40.3559677601\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (72.6329345703,62.7951608567), test loss: 69.9045776367\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (22.3756599426,39.8760621989), test loss: 40.6970837831\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (19.3977069855,62.5166125466), test loss: 71.1909999847\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (29.5036449432,39.7187961092), test loss: 38.2299685001\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (31.1294059753,62.234664363), test loss: 68.1372852325\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (78.7858734131,39.5633988376), test loss: 40.2253491402\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (69.4038238525,61.9542780085), test loss: 69.0353842735\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (34.44348526,39.4069320062), test loss: 38.5512075424\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (38.3112792969,61.6745668251), test loss: 75.0912997723\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (33.837436676,39.2487713213), test loss: 36.9707027197\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (68.3369293213,61.3996146824), test loss: 63.6966841698\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (14.2139673233,39.0879429861), test loss: 40.8481554985\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (16.5946903229,61.122919217), test loss: 76.3022856712\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (36.3493461609,38.9289278364), test loss: 36.9897620201\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (50.5231437683,60.8497783636), test loss: 66.0140491486\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (22.178024292,38.771030529), test loss: 41.0910657883\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (30.3167114258,60.5802572997), test loss: 74.1906723022\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (22.3230686188,38.6125295413), test loss: 40.7405863047\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (29.8528614044,60.3129081826), test loss: 79.3786434174\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (26.4295158386,38.4538968883), test loss: 34.7953807354\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (25.5816383362,60.0478101371), test loss: 64.7593895912\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (39.8142929077,38.2972238822), test loss: 40.4414757252\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (60.9902572632,59.7867652341), test loss: 77.104882431\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (23.956243515,38.1421029539), test loss: 40.6063234329\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (28.4710979462,59.5288591223), test loss: 75.9498105049\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (18.6411628723,37.9869913171), test loss: 40.7587491512\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (38.2603607178,59.2743565766), test loss: 75.782447958\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (18.4646682739,37.8326812837), test loss: 38.1470043182\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (31.1590480804,59.0233113043), test loss: 69.2188728333\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.358915329,37.6789964772), test loss: 35.0757378101\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (38.1673049927,58.7726967312), test loss: 62.0968463898\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.1304168701,37.5283339387), test loss: 39.5405208588\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (37.4228973389,58.52660279), test loss: 69.9959733963\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (33.4303894043,37.3789413575), test loss: 35.6999994755\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (41.4381790161,58.2850590787), test loss: 68.4785932541\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (31.8196163177,37.2317960159), test loss: 41.7199976921\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (67.994178772,58.0500966754), test loss: 78.2986620903\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (8.87218856812,37.0838278222), test loss: 38.6880959511\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (12.0320663452,57.8123928998), test loss: 73.2985105515\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (20.1402339935,36.9385249354), test loss: 35.2503929138\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (24.0528335571,57.5776136269), test loss: 62.9608045578\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (23.6878967285,36.7955320019), test loss: 39.6958710909\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (21.85496521,57.3475322856), test loss: 70.7156673908\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (34.8000564575,36.6537985509), test loss: 37.0992795944\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (27.0127620697,57.1215616692), test loss: 71.2132618904\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (12.9422073364,36.5131338936), test loss: 33.0568340302\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (10.296459198,56.8956906473), test loss: 60.4810537338\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (21.14453125,36.3746106011), test loss: 38.8562768459\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (36.5661277771,56.6768706865), test loss: 73.5489320755\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (49.6517868042,36.2389521694), test loss: 34.7968012333\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (61.4172821045,56.4596005094), test loss: 62.783552742\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (7.87552452087,36.1031911013), test loss: 38.850571394\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (16.0955295563,56.2438799061), test loss: 68.3037586212\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (40.3102798462,35.9702371448), test loss: 36.1007661819\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (83.9916381836,56.0313800772), test loss: 67.114909935\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (12.3639860153,35.8377092148), test loss: 33.3143973589\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (19.205078125,55.8190171576), test loss: 60.4981600761\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (35.1585464478,35.7080555715), test loss: 38.1272481918\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (35.8836135864,55.6117426238), test loss: 73.5519089699\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (50.2284469604,35.5801305957), test loss: 35.1596922636\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (33.229095459,55.4076824691), test loss: 70.6985705376\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (22.3320560455,35.4524357448), test loss: 36.9933316708\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (17.1534671783,55.2041006883), test loss: 66.784214592\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (11.4331130981,35.3267888305), test loss: 37.2499607086\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (33.2881011963,55.0050863284), test loss: 72.4304412365\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (15.8442907333,35.2030285695), test loss: 34.1667375326\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (28.7491436005,54.8085438039), test loss: 62.3194046021\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (41.980846405,35.0809988243), test loss: 39.3866552353\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (60.7928924561,54.6164396127), test loss: 73.3576471806\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (3.69379425049,34.9585311336), test loss: 38.7180232048\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (10.5706768036,54.4228417071), test loss: 81.2004031658\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (10.1698493958,34.8389995713), test loss: 33.2051619053\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (16.3781108856,54.2324596708), test loss: 62.0225177765\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (21.838848114,34.7217452989), test loss: 38.4871075153\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (52.3103637695,54.0466363874), test loss: 72.4624189377\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (38.3677520752,34.605870062), test loss: 33.8448983431\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (80.3349227905,53.864230002), test loss: 62.2642234802\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (19.9445400238,34.4898305243), test loss: 37.2561489105\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (42.0227813721,53.6811924597), test loss: 67.6698050499\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (21.4127883911,34.3759609725), test loss: 36.1015165806\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (41.7190322876,53.5004035412), test loss: 69.2612985611\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (24.1814365387,34.2643289462), test loss: 33.5962033272\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (37.8126831055,53.3232663011), test loss: 62.5468551636\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (38.3296432495,34.1537646166), test loss: 45.5545316696\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (43.2850456238,53.1489576718), test loss: 83.3324872017\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (16.9751091003,34.0434359551), test loss: 36.7296864986\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (41.8600845337,52.9754646172), test loss: 69.9699418068\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (37.7534408569,33.9359555355), test loss: 37.2763691902\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (66.9287338257,52.8044518953), test loss: 66.8345518112\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (34.1242446899,33.8305938826), test loss: 35.3624506712\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (79.3559112549,52.6386447304), test loss: 67.5497463226\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (16.5067081451,33.7248982905), test loss: 35.5638732433\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (21.0928344727,52.4718069952), test loss: 61.8469747543\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (19.991476059,33.6201732747), test loss: 36.9098434448\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (30.2954845428,52.3060627218), test loss: 65.1428560257\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (27.3168449402,33.5175539322), test loss: 32.9739338398\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (69.7847900391,52.1428649924), test loss: 64.6012727737\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (45.9122314453,33.4173905273), test loss: 33.9871728897\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (53.5672874451,51.982752725), test loss: 59.6679985046\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (2.63809967041,33.3170132426), test loss: 36.3981229544\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (9.91753101349,51.8243197657), test loss: 70.428316021\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (24.6866588593,33.2173863602), test loss: 34.6476429939\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (38.1954421997,51.6691152956), test loss: 62.4201793671\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (54.1672935486,33.1201642426), test loss: 40.3080197334\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (51.4633865356,51.5128745945), test loss: 70.2333831787\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (61.6944923401,33.0246823971), test loss: 34.4966994286\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (51.0431480408,51.3598238604), test loss: 65.4438502312\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (16.6557922363,32.9295971813), test loss: 32.1706201077\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (12.2621049881,51.2095287309), test loss: 58.5105690956\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (22.3570079803,32.8341757446), test loss: 37.7743673563\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (24.9792156219,51.058441337), test loss: 74.0295050144\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (19.0253238678,32.7405974687), test loss: 35.8842820168\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (23.7979049683,50.9081054477), test loss: 69.3681894302\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (36.60887146,32.6498247971), test loss: 36.1288460255\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (23.4378871918,50.7611939026), test loss: 65.3583331585\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (19.9603652954,32.5589793713), test loss: 34.8749073505\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (42.838470459,50.6169687032), test loss: 64.1480800629\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (23.1035861969,32.4681335399), test loss: 33.2287098408\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (22.1616020203,50.4734904839), test loss: 58.2714593887\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (28.9983272552,32.3786027209), test loss: 37.9605912209\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (60.3681564331,50.3305920468), test loss: 64.9009930611\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (13.6269598007,32.290904376), test loss: 37.3003173828\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (13.4331741333,50.1899321581), test loss: 78.650250721\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (24.23645401,32.203839812), test loss: 38.8659691334\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (35.5166931152,50.051776491), test loss: 74.4151364326\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (18.5803165436,32.1167154303), test loss: 36.1862457991\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (13.8374347687,49.9135984296), test loss: 72.5698786259\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (27.2869873047,32.0311423661), test loss: 34.9000174522\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (25.2618503571,49.7763519286), test loss: 64.4042261124\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (13.817735672,31.9470483777), test loss: 37.8917182922\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (17.2846641541,49.6417382753), test loss: 67.5249907494\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (16.1824798584,31.8639674364), test loss: 37.5358086586\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (29.2191867828,49.5096880794), test loss: 73.0663395882\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.3383779526,31.7805471814), test loss: 33.3688634872\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (30.9091110229,49.3775468721), test loss: 59.3386374474\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (7.28426837921,31.6978828019), test loss: 37.0556816339\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (17.2970066071,49.2450561386), test loss: 71.7068822861\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (13.0302352905,31.6173235296), test loss: 34.8958731651\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (34.7553329468,49.1155008213), test loss: 61.9297284126\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (38.42628479,31.5377593891), test loss: 36.9615266323\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (77.6082000732,48.9889041777), test loss: 66.2617315292\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (35.8753929138,31.4582466232), test loss: 43.2897171021\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (105.057884216,48.8634268766), test loss: 82.5269794464\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (10.6126728058,31.3791998732), test loss: 33.4242209435\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (24.945438385,48.7367489667), test loss: 58.5608373642\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (16.9078483582,31.3012994079), test loss: 36.9456628799\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (10.3118495941,48.6105815522), test loss: 70.3950675964\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (28.864074707,31.2246887917), test loss: 36.8263157368\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (24.8547210693,48.487510736), test loss: 73.2121473312\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (32.4984474182,31.1482673691), test loss: 40.7818862915\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (29.9440345764,48.3661980375), test loss: 72.6349481106\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (23.0265083313,31.0727326863), test loss: 36.1068148613\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (29.1085319519,48.24420324), test loss: 70.2340559006\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (21.7801609039,30.9976735882), test loss: 34.2498924732\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (109.818481445,48.1244111563), test loss: 59.6502840996\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 9\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (302.034057617,inf), test loss: 209.550705719\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (390.698608398,inf), test loss: 248.240919113\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (38.3324966431,60.4175222988), test loss: 43.071563673\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (59.0692710876,83.1765407696), test loss: 65.1373558998\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (55.6957015991,50.8377037864), test loss: 45.9878367424\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (71.787399292,73.027371254), test loss: 79.4204539299\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (121.825317383,47.6435207132), test loss: 41.2725196838\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (242.303131104,69.6834934966), test loss: 66.9181884766\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (12.3136177063,45.9919597631), test loss: 42.6122333527\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (11.433180809,67.8981579318), test loss: 70.1599401474\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (43.9457015991,45.0026799534), test loss: 44.0413414001\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (39.3452148438,66.8107689964), test loss: 77.0567169189\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (41.3005371094,44.329982155), test loss: 42.201031971\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (43.5174331665,66.1099000133), test loss: 64.1804134369\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (58.7347335815,43.8413861836), test loss: 43.3693710804\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (88.914932251,65.5623250517), test loss: 72.312506485\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (28.6946678162,43.4726240728), test loss: 43.3122429848\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (56.9846343994,65.1539395413), test loss: 69.5280535698\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (58.3259391785,43.1639156597), test loss: 43.1614461899\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (47.5364379883,64.7939175968), test loss: 64.7917099953\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (21.213432312,42.8945363313), test loss: 44.9678911209\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (37.6535186768,64.5028090027), test loss: 75.3884293556\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (9.1112947464,42.6718326564), test loss: 42.6792966843\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (19.6167297363,64.2600444645), test loss: 69.0349157333\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (55.6161766052,42.481543848), test loss: 42.1700474739\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (43.9633369446,64.0361943181), test loss: 69.6876278877\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (31.5507984161,42.3159947059), test loss: 44.8410484314\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (61.891960144,63.8544571912), test loss: 79.3264955521\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (17.8441104889,42.1581142369), test loss: 41.6714849472\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (44.9168205261,63.6763297204), test loss: 66.6763469696\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (42.1845359802,42.0147126712), test loss: 43.4821905136\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (44.1431808472,63.506985879), test loss: 73.0117328644\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (32.1124801636,41.8961862726), test loss: 44.3222010612\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (46.4586181641,63.3785156273), test loss: 71.3385396957\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (45.6086769104,41.7738000989), test loss: 41.0173920631\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (83.6777038574,63.2263771276), test loss: 62.2982465744\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (51.1666488647,41.6628398381), test loss: 43.3660734177\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (193.290115356,63.099365457), test loss: 71.0176298141\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (18.4711780548,41.5561959984), test loss: 41.529721117\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (18.4138431549,62.9596584335), test loss: 66.4398084641\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (39.7020950317,41.4622210847), test loss: 41.6451863766\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (56.9829025269,62.8398701546), test loss: 63.0345740318\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (70.7873840332,41.3606064622), test loss: 44.4084120274\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (65.8750457764,62.7068190329), test loss: 76.2937021255\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (8.4321937561,41.2610868431), test loss: 39.6786839485\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (31.7146148682,62.5805340968), test loss: 64.782746315\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (49.9035263062,41.164847414), test loss: 41.9669811249\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (77.0570297241,62.4590194523), test loss: 68.8715927124\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (32.3613548279,41.0673912104), test loss: 42.9318659782\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (71.8665847778,62.3302368642), test loss: 76.7390031815\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (44.7640914917,40.9776122751), test loss: 41.2860739708\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (54.806728363,62.2097805112), test loss: 63.4061496735\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (28.6191139221,40.8836648752), test loss: 42.6560741425\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (37.2028274536,62.0859004163), test loss: 72.7060171127\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (17.3318309784,40.7871164635), test loss: 42.2858397961\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (46.5503158569,61.9553492414), test loss: 67.7264433861\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (12.7907400131,40.7013800627), test loss: 41.7419888496\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (24.741027832,61.8408183267), test loss: 62.0040393353\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (34.2669219971,40.6063598529), test loss: 42.9928107262\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (57.9810791016,61.7033542326), test loss: 71.4342288971\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (12.1144561768,40.5141335337), test loss: 41.438331604\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (24.5239486694,61.5559123216), test loss: 64.4206990242\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (20.6006507874,40.4215553238), test loss: 40.2467935562\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (12.7035179138,61.4101832146), test loss: 64.6959527969\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (26.847366333,40.3298583843), test loss: 40.7716631889\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (31.603805542,61.2592417968), test loss: 70.5372643471\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (16.5865077972,40.2286414926), test loss: 38.4575448036\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (24.7755012512,61.0938834903), test loss: 61.9880822659\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (31.0396900177,40.1311251183), test loss: 41.1478983879\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (66.1873779297,60.9322091693), test loss: 67.5750280142\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (43.882183075,40.0257201728), test loss: 41.8796644688\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (64.9601135254,60.7611017709), test loss: 67.5507174015\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (31.7607765198,39.9202029636), test loss: 38.7796004772\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (24.3288402557,60.5859550113), test loss: 60.07152071\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (34.1067886353,39.8141404944), test loss: 41.477781105\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (40.9973754883,60.4094555383), test loss: 69.1526283264\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.0374040604,39.7049634774), test loss: 40.9736952782\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (26.7743301392,60.2305377115), test loss: 66.0027219772\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (12.0053977966,39.5903640061), test loss: 39.0590424061\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (22.2542877197,60.038461903), test loss: 57.8658052444\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (38.4855651855,39.4812096806), test loss: 40.8794611931\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (76.7093811035,59.8592748777), test loss: 67.6808943748\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (44.5693702698,39.361362108), test loss: 36.9787129402\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (56.0847625732,59.6561916871), test loss: 59.4551227093\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (43.554889679,39.2453484977), test loss: 39.0938239098\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (55.7229347229,59.4615308493), test loss: 62.5575800896\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (84.3171463013,39.1247855489), test loss: 38.9158538818\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (102.113365173,59.2625450915), test loss: 67.1396270752\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (39.3938369751,39.0048867401), test loss: 38.0008874893\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (72.8680496216,59.06214495), test loss: 57.9293382645\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (26.1463718414,38.8768524148), test loss: 39.0453670502\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (33.2501296997,58.8487211131), test loss: 65.380994606\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (25.6369285583,38.7510790731), test loss: 39.243392086\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (63.1142578125,58.6412096209), test loss: 62.9797369003\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (32.7021484375,38.6180066442), test loss: 37.2587114334\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (83.6591949463,58.4259229596), test loss: 55.8540923119\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.8159103394,38.4870513367), test loss: 40.1373983145\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (19.5368747711,58.2157606595), test loss: 68.8261207581\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (54.0195541382,38.3534122195), test loss: 38.2931288004\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (97.2383346558,57.9995273276), test loss: 60.6800780296\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (43.8194274902,38.2202129523), test loss: 36.9564533234\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (89.1889572144,57.7902186304), test loss: 56.9711933136\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (37.1526489258,38.0811409881), test loss: 36.8972404003\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (43.1569595337,57.5681172644), test loss: 64.043077755\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (25.700094223,37.9474583029), test loss: 34.9975035191\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (34.4738998413,57.3612837133), test loss: 58.0798490524\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (27.9246292114,37.8063628413), test loss: 37.2646274567\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (24.4598884583,57.1386073353), test loss: 61.1361045837\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (42.1402587891,37.6710968799), test loss: 37.1222081184\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (36.9937210083,56.9287768228), test loss: 59.3668200493\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (28.9219093323,37.5304270453), test loss: 35.7665075302\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (38.1688804626,56.7125677637), test loss: 54.3983659744\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (88.6524810791,37.396191882), test loss: 37.6743523598\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (103.829376221,56.5052873605), test loss: 61.0216561317\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (23.6308288574,37.2543912021), test loss: 37.3595591068\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (22.8127784729,56.2864585211), test loss: 61.6059593201\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (37.8186569214,37.1178468175), test loss: 36.1777994156\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (32.5534210205,56.0779606342), test loss: 54.0368233681\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (19.8201370239,36.9747845909), test loss: 37.9569104195\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (24.610370636,55.8612402894), test loss: 65.3614301682\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (22.629617691,36.837999153), test loss: 37.7024976254\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (35.0974502563,55.6586069822), test loss: 67.4919762611\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (42.0967521667,36.6970050931), test loss: 36.0894161224\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (59.1858215332,55.4444417876), test loss: 59.5376008034\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (24.8440189362,36.5608510899), test loss: 41.1514226913\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (32.5123023987,55.2438770974), test loss: 76.275778532\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (31.6308288574,36.4201094282), test loss: 35.851649642\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (59.483745575,55.0326918846), test loss: 56.9282392502\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (73.2359161377,36.2868660227), test loss: 38.2976752758\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (144.512588501,54.8378555224), test loss: 63.7862318039\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (6.35039186478,36.1474019349), test loss: 35.6463330746\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (18.0504550934,54.6282792966), test loss: 57.0332948685\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (42.9458045959,36.0150713672), test loss: 34.6768508911\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (38.4610328674,54.4331903068), test loss: 52.9208616257\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (22.4627227783,35.8792058168), test loss: 35.1764805317\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (34.115146637,54.231255751), test loss: 58.3629132271\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (42.8669319153,35.7492165966), test loss: 34.1189209461\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (24.4589805603,54.0392167659), test loss: 53.7844284058\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (13.3780593872,35.6157195138), test loss: 34.504297781\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (23.2803382874,53.8388712751), test loss: 51.6805883884\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (46.7931518555,35.4874119516), test loss: 36.4176363945\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (31.2679824829,53.6495074972), test loss: 66.1299462318\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (6.8390622139,35.3543834887), test loss: 32.6331276894\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (12.9368944168,53.4513970632), test loss: 54.6360931396\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (13.920674324,35.2281617176), test loss: 35.7263761759\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (20.5534534454,53.2682701778), test loss: 62.118383503\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (18.1088371277,35.0976597491), test loss: 34.5460769653\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (31.1123161316,53.0732735834), test loss: 56.7437130928\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (12.4791107178,34.9737855421), test loss: 35.7770938396\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (38.0723686218,52.8939757362), test loss: 55.9717751503\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (2.95452785492,34.8454673621), test loss: 34.4356919289\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (13.8145017624,52.7030635911), test loss: 57.7431138039\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (30.4194068909,34.7239576835), test loss: 33.9608255148\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (47.1039924622,52.5268940925), test loss: 54.7096255302\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.3656520844,34.5998993713), test loss: 35.8151054859\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (13.4044065475,52.3425430374), test loss: 53.7249699593\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (13.8906068802,34.4805829905), test loss: 34.7500286818\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (57.9355316162,52.1691161869), test loss: 58.9778159618\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (28.5299606323,34.3596187746), test loss: 32.705753088\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (148.595932007,51.9894192088), test loss: 53.2873793602\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (10.0260753632,34.2434484888), test loss: 39.1405543327\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (16.3974571228,51.8197400962), test loss: 61.794022274\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (16.1432933807,34.1255963116), test loss: 33.3419033527\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (17.7699966431,51.6421889631), test loss: 58.6083432198\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (23.0553627014,34.0116291089), test loss: 36.0975456238\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (18.6157684326,51.4754086411), test loss: 60.0687502861\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.37528514862,33.8951519925), test loss: 34.2807271242\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (28.5972232819,51.3009605649), test loss: 56.914893055\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (40.8952178955,33.784174394), test loss: 39.1625590086\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (80.3363800049,51.1400800438), test loss: 67.0590312481\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (27.7479057312,33.6698424647), test loss: 34.6644691467\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (52.723777771,50.9688892328), test loss: 51.8233922005\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (23.288028717,33.5614004745), test loss: 36.4194043636\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (42.4584121704,50.8115920111), test loss: 62.1933877945\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (18.2061576843,33.4499551311), test loss: 35.6847841263\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (26.3750534058,50.6454241977), test loss: 57.4163913727\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (15.7741279602,33.3432169939), test loss: 38.855128336\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (28.3145370483,50.4904367909), test loss: 59.0089762688\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (17.6489315033,33.2358547304), test loss: 33.87071805\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (30.2696418762,50.3303833373), test loss: 58.1972593307\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (32.4104652405,33.1318196452), test loss: 35.3289591074\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (26.1311454773,50.1777662468), test loss: 60.0971709251\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (10.5062675476,33.0268329005), test loss: 35.0035119057\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (5.77413845062,50.019449076), test loss: 56.385695076\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (51.5648574829,32.926079872), test loss: 35.8197736263\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (52.8179626465,49.8739648636), test loss: 61.7828723907\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (18.8029651642,32.8240468246), test loss: 34.5796710014\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (24.2331256866,49.7194490784), test loss: 52.096208477\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (19.5524253845,32.7250295936), test loss: 33.9502566338\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (25.4078826904,49.573998016), test loss: 59.1024230003\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (27.4819602966,32.624870477), test loss: 34.5835635185\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (50.9898223877,49.4220322981), test loss: 55.8197704315\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (40.8403701782,32.5276957038), test loss: 38.4966452122\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (49.9951095581,49.2801016907), test loss: 55.7316870689\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (15.7393474579,32.4294858745), test loss: 34.7246424675\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (19.5164775848,49.1319286743), test loss: 60.6178387642\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (12.9835968018,32.3352418817), test loss: 34.3230566025\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (17.2554740906,48.9942975726), test loss: 55.3061053276\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (12.3313331604,32.2394002604), test loss: 35.2190052509\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (22.6262054443,48.850387396), test loss: 56.422218132\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (16.4590454102,32.1465909134), test loss: 34.1824054241\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (31.7515678406,48.7142537894), test loss: 58.4768602371\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (32.2676620483,32.0542385358), test loss: 33.1403078556\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (66.5048446655,48.5753843265), test loss: 52.7512251854\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (17.9063243866,31.9632063843), test loss: 34.8025402069\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (30.1403846741,48.4411111581), test loss: 58.2803620338\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (48.1353912354,31.8731826121), test loss: 33.9821516037\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (54.6961517334,48.3038421558), test loss: 54.1304489136\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (26.4941902161,31.784455362), test loss: 35.5773476124\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (20.487197876,48.174757082), test loss: 53.560241127\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (20.8773422241,31.6969175993), test loss: 34.5373925686\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (32.5625762939,48.0405607402), test loss: 55.78149786\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (24.8207569122,31.6104884813), test loss: 37.654470396\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (15.0263900757,47.9124125201), test loss: 61.6290482521\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (24.4461212158,31.5238833624), test loss: 35.1265427589\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (30.4523124695,47.7794983347), test loss: 52.2583044052\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (18.866235733,31.4386553158), test loss: 33.7038734913\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (34.8729476929,47.6540773125), test loss: 57.4914028168\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (22.6610107422,31.3542913368), test loss: 35.0594026566\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (26.9259624481,47.5254135474), test loss: 59.7832464218\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (29.287147522,31.2714437964), test loss: 35.3380476952\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (68.2699432373,47.4030541993), test loss: 56.4139531136\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (22.8567676544,31.1885387094), test loss: 33.4506582022\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (48.1623153687,47.2774033149), test loss: 58.1532519341\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (30.1806201935,31.1066903398), test loss: 49.3193099976\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (28.6696186066,47.1559969083), test loss: 76.3192256927\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (33.1944274902,31.026533983), test loss: 39.6299060822\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (68.5516052246,47.0343415075), test loss: 66.2996286392\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (21.8152599335,30.9463822008), test loss: 34.3332781315\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (11.2840709686,46.915043073), test loss: 54.6828399658\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (33.7279319763,30.8680077536), test loss: 36.7823302269\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (24.4985122681,46.7949243175), test loss: 55.1300260544\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (25.410074234,30.7897376792), test loss: 36.3255863667\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (49.7957839966,46.6799304558), test loss: 60.1735984325\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (59.2513847351,30.7136359599), test loss: 34.7108355999\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (58.2258796692,46.5620207994), test loss: 52.760639286\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (23.930147171,30.6369528587), test loss: 36.9019880772\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (34.1921005249,46.4477016005), test loss: 62.7421322823\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (28.9071025848,30.5614268876), test loss: 35.7400897026\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (22.6973075867,46.330605091), test loss: 64.3852802753\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 10\n",
      "Defining train net\n",
      "Defining test net\n",
      "Defining solver\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (258.453857422,inf), test loss: 210.712439346\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (314.702941895,inf), test loss: 254.272243118\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (43.4010238647,62.4494261312), test loss: 40.4949772358\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (67.7601013184,92.38271943), test loss: 65.2046115875\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (20.2979602814,52.8135749717), test loss: 43.312571907\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (30.962562561,82.0321560259), test loss: 74.6015396118\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (12.1086597443,49.5385514615), test loss: 39.8843339443\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (24.8877391815,78.3355767727), test loss: 68.1018255711\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (55.4188270569,47.9374136872), test loss: 39.8090167046\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (82.6588439941,76.6083806117), test loss: 64.2121534348\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (19.7573318481,46.9301306629), test loss: 43.7361248016\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (74.4820098877,75.4600251886), test loss: 76.1890541077\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (29.1504173279,46.1981339734), test loss: 39.4171893597\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (45.3948669434,74.6360690691), test loss: 67.812768364\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (47.6555023193,45.7027412206), test loss: 39.4668438435\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (59.8025016785,74.0808930051), test loss: 63.9265964031\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (9.73325157166,45.2849591974), test loss: 43.6314204693\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (46.4760742188,73.6199437976), test loss: 76.1655797005\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (34.4825744629,44.9730799362), test loss: 38.8794644356\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (27.6339511871,73.2789566156), test loss: 66.1660761833\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (36.0389709473,44.7124110247), test loss: 38.9408373833\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (40.7329750061,72.9772041984), test loss: 63.188732338\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (34.6206550598,44.5036388924), test loss: 42.4432174206\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (63.4193115234,72.7151095698), test loss: 75.2669280052\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (9.93836784363,44.3008523711), test loss: 37.2651596308\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (25.3001441956,72.458780353), test loss: 66.1624430656\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (26.6791400909,44.1117481564), test loss: 38.7070515156\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (29.1215858459,72.2145913252), test loss: 69.1568889618\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (30.5278816223,43.9613369646), test loss: 42.7584849358\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (65.3040924072,72.0386725604), test loss: 77.2551023483\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (94.9379348755,43.8104352293), test loss: 37.4765322685\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (195.4871521,71.8540438097), test loss: 66.1054657936\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (21.7085227966,43.6704858432), test loss: 38.0424965858\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (67.415687561,71.674379256), test loss: 67.4296465397\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (81.8071899414,43.5465523479), test loss: 40.0899032116\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (108.888839722,71.5119043743), test loss: 74.1583835602\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (22.4476699829,43.4266740816), test loss: 35.9021986008\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (24.2114658356,71.3421348755), test loss: 63.7924113274\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (30.0901126862,43.3064192359), test loss: 38.1505520821\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (92.2939910889,71.1743996658), test loss: 67.1764226913\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (53.3115997314,43.1886620919), test loss: 39.9148252964\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (53.1895141602,70.9979124585), test loss: 75.1619322777\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (30.3189563751,43.0782278176), test loss: 35.8796234608\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (57.7469291687,70.8527490312), test loss: 63.8820223808\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (33.6766662598,42.9714069752), test loss: 38.2128434181\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (69.8670959473,70.7078222116), test loss: 67.1050227165\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (63.1946411133,42.8629273199), test loss: 39.7502332687\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (77.2181396484,70.5364179153), test loss: 73.713051796\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (51.3896522522,42.7546672167), test loss: 34.6720752716\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (80.7561950684,70.3773362319), test loss: 61.1097952843\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (79.7900772095,42.6567944218), test loss: 39.1749287605\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (63.3877487183,70.2135036386), test loss: 69.1785930634\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (35.5585899353,42.5405506072), test loss: 38.8714709282\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (43.4206008911,70.0173661015), test loss: 73.6707025528\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (24.025188446,42.430892839), test loss: 34.3625828743\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (24.1484680176,69.8241240054), test loss: 59.8626928329\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (46.2076797485,42.314581064), test loss: 38.9462337494\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (63.396068573,69.6233782456), test loss: 68.4390485764\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (28.399061203,42.1978179039), test loss: 38.1916039944\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (44.4524688721,69.4268323958), test loss: 70.2994513988\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (44.434463501,42.0780656213), test loss: 33.8271986961\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (186.869476318,69.2162774751), test loss: 57.579983139\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (42.657913208,41.960280533), test loss: 37.5789320469\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (80.7247848511,69.0014329198), test loss: 66.1887903214\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (31.0585098267,41.8328444004), test loss: 37.3320588589\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (88.8601150513,68.7734872153), test loss: 64.4642993927\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (31.6939239502,41.6958729692), test loss: 33.3693302393\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (40.5166931152,68.5314348379), test loss: 56.8358200073\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (24.9437828064,41.5639445836), test loss: 37.5184281349\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (47.0348510742,68.2992125774), test loss: 68.3204103708\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (35.3494606018,41.4211938992), test loss: 36.8752607822\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (63.8537597656,68.0526899968), test loss: 63.063723278\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (50.498210907,41.282401635), test loss: 32.860234499\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (54.3272514343,67.8069219619), test loss: 54.9898580551\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (28.1074371338,41.136556497), test loss: 35.7338340759\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (34.7846603394,67.5516965009), test loss: 63.8928664207\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (31.0955944061,40.992613384), test loss: 35.7975936413\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (48.2792739868,67.2909795272), test loss: 61.8524489403\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.5381126404,40.8396000947), test loss: 32.0089491844\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (21.9900131226,67.0180670895), test loss: 53.2903613329\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (40.0252685547,40.6817978034), test loss: 35.9350943565\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (72.5103607178,66.7395400455), test loss: 62.3816288471\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (14.5946073532,40.5285919312), test loss: 35.1529480457\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (41.7997093201,66.4714332863), test loss: 59.3666126251\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (64.1258773804,40.3697444322), test loss: 33.0871108532\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (168.028625488,66.1964242031), test loss: 59.0704721451\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (40.1946029663,40.2081017042), test loss: 36.3385273933\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (38.6756057739,65.9105384014), test loss: 61.9449571609\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (11.2717914581,40.047360225), test loss: 33.0901438236\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (14.8299922943,65.6314611734), test loss: 57.0334751129\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (72.3849945068,39.8863318523), test loss: 32.6472059727\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (66.1513900757,65.3494884904), test loss: 53.1295207977\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (46.1806602478,39.7202854787), test loss: 35.4617756367\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (57.4651603699,65.0625320965), test loss: 61.2002807617\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (75.5587310791,39.5564301678), test loss: 32.3295851707\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (174.240875244,64.7831069951), test loss: 55.7529264927\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.1293964386,39.3904090313), test loss: 32.117309761\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (34.27318573,64.5040480795), test loss: 53.2743402481\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (20.5876216888,39.2282153332), test loss: 35.057404995\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (46.5288848877,64.2324240735), test loss: 61.2398180485\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (23.1639785767,39.0638650613), test loss: 32.2009049892\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (26.7502479553,63.9516710824), test loss: 55.0905374527\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (57.7342567444,38.9031771517), test loss: 34.7560279846\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (64.7274017334,63.6810435752), test loss: 59.6213388443\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (7.88523101807,38.7418359948), test loss: 34.8561327934\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (26.6968402863,63.4091606986), test loss: 62.5823132038\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (5.49693489075,38.5781236531), test loss: 32.3842518806\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (15.6105098724,63.1384684895), test loss: 57.9483151913\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (31.6582183838,38.4212450205), test loss: 31.6606759548\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (41.0002746582,62.8765142278), test loss: 54.1055196762\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (14.4979801178,38.2594893119), test loss: 33.5751963139\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (31.7997112274,62.6119351837), test loss: 60.5241855621\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (25.1573143005,38.1035313139), test loss: 29.8452060223\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (12.3959083557,62.3548266873), test loss: 52.2470956802\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (36.7852630615,37.9482980498), test loss: 30.1413285732\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (168.371353149,62.1001559865), test loss: 52.5102415085\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (23.5326271057,37.7962368083), test loss: 32.8749952316\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (32.2277565002,61.8459771517), test loss: 59.6133095741\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.359079361,37.6436592301), test loss: 29.6271528959\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (30.9320831299,61.5913783262), test loss: 53.8377178192\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (11.9796848297,37.4901998525), test loss: 29.8579535246\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (16.5328483582,61.3382590129), test loss: 55.8119683266\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (30.0424156189,37.3431984655), test loss: 33.7512590885\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (52.9147872925,61.0957299454), test loss: 63.6244031906\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (40.8564529419,37.1938634399), test loss: 27.9826962471\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (50.5108299255,60.8507718392), test loss: 50.2017064095\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (31.1241531372,37.0482706325), test loss: 31.758898735\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (22.2820224762,60.6112300076), test loss: 58.6789100647\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (33.5965194702,36.9050351997), test loss: 30.6266686201\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (42.6893119812,60.3744985062), test loss: 57.6711029053\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (17.0447177887,36.7625227613), test loss: 28.1717280388\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (22.4550228119,60.1378227687), test loss: 50.8951368809\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (14.6900691986,36.6208757017), test loss: 30.1907114029\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (36.4638595581,59.9013219813), test loss: 54.3348223686\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (23.9115467072,36.4804466961), test loss: 30.7781628609\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (38.4552154541,59.6676214766), test loss: 58.3473731041\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (22.8271846771,36.343307246), test loss: 28.5495490074\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (29.7445545197,59.4435039385), test loss: 50.9390887737\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (22.0092353821,36.2079299831), test loss: 34.8347037315\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (53.5556526184,59.2197733378), test loss: 66.2367757797\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (52.3977127075,36.0727124333), test loss: 30.9023955822\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (56.2665519714,58.9944802733), test loss: 57.2283643723\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.8990077972,35.9391559545), test loss: 29.2530287743\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (12.7750244141,58.7732654299), test loss: 51.7442185402\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (66.9420166016,35.8092114461), test loss: 32.0716336727\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (47.3689193726,58.5540862821), test loss: 56.325402832\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (31.7483310699,35.6773995767), test loss: 33.7937376022\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (43.5713882446,58.3363929904), test loss: 66.6779090881\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (48.5168609619,35.5505568699), test loss: 28.6156248093\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (85.5113372803,58.1239384042), test loss: 50.1431241989\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (36.5364837646,35.4218069709), test loss: 38.6604887009\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (34.022693634,57.9114908204), test loss: 72.9249106407\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (15.2486562729,35.2960849539), test loss: 32.7123624325\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (14.0607662201,57.7048330033), test loss: 54.1999225616\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (6.21617650986,35.1721013932), test loss: 29.0377986908\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (10.3214302063,57.495401403), test loss: 50.2441996098\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (40.631477356,35.0504701077), test loss: 31.400787735\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (95.6837158203,57.2947587161), test loss: 57.9198197365\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (6.04615974426,34.9286354865), test loss: 31.319617033\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (29.0361709595,57.0881196676), test loss: 54.3230488777\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (22.9234809875,34.8083326294), test loss: 30.6304196835\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (35.5544433594,56.8877989375), test loss: 55.1968019485\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (14.1817712784,34.6906673401), test loss: 30.2891200066\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (47.0018577576,56.6919451293), test loss: 56.2467618942\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (21.7396316528,34.5727091994), test loss: 30.7861935616\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (58.4218292236,56.4956174844), test loss: 53.7803308487\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (16.6305389404,34.4568141366), test loss: 30.1548352242\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (30.077419281,56.3041718089), test loss: 50.651619482\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (19.1105823517,34.3427980702), test loss: 30.7950553179\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (38.0090942383,56.1134777086), test loss: 54.4213190556\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (12.8866491318,34.2307539791), test loss: 30.8773173332\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (26.7974185944,55.9244888), test loss: 52.5092391014\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (21.475610733,34.1183205186), test loss: 29.8726597786\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (16.4156208038,55.7353791413), test loss: 49.9012859344\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (16.2355575562,34.0077930847), test loss: 31.6304023027\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (16.1555366516,55.5486204662), test loss: 56.369008255\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.3198776245,33.8986765615), test loss: 31.2946989536\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (67.9501876831,55.3688152269), test loss: 52.9862454414\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (66.3202285767,33.7907543375), test loss: 37.9416769981\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (146.562438965,55.1880142208), test loss: 68.7060722351\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (18.7464523315,33.6838788487), test loss: 32.7054070234\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (55.6370697021,55.0090345128), test loss: 56.3150613785\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (42.0443000793,33.5788298153), test loss: 33.302359724\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (43.7888641357,54.8336918956), test loss: 55.3847710609\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (21.8154830933,33.4751063389), test loss: 31.393911314\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (25.8648223877,54.6574538042), test loss: 51.3066828728\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (35.8034591675,33.3713769256), test loss: 33.3960317135\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (53.5912475586,54.483427038), test loss: 55.7655197144\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (79.463142395,33.2698858776), test loss: 32.2341430664\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (122.119232178,54.3116661503), test loss: 52.8751537323\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (3.92006444931,33.1681911088), test loss: 31.2284999371\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (15.5002403259,54.1418720273), test loss: 51.2552983284\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (37.8889541626,33.0685062044), test loss: 32.475449276\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (67.7922515869,53.9770226257), test loss: 56.413629055\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (38.7892990112,32.9708377165), test loss: 31.7638458729\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (49.3880348206,53.8101091079), test loss: 51.0292907715\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (56.4613265991,32.873119128), test loss: 35.0371723652\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (50.5146026611,53.6467809816), test loss: 58.6158676147\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (31.4709968567,32.7766981271), test loss: 33.6254148006\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (29.4814567566,53.4822843855), test loss: 60.9525022507\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (17.8053665161,32.6810711246), test loss: 31.89620471\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (22.4525184631,53.320639009), test loss: 54.7237879753\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (14.3977308273,32.5863281648), test loss: 31.5435708046\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (22.2547149658,53.1622504318), test loss: 51.9159714699\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (12.0209741592,32.492731061), test loss: 33.1570187569\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (29.8932418823,53.0038538169), test loss: 58.2915383339\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (14.5067005157,32.3999202396), test loss: 30.5173992634\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (19.7329978943,52.8499024564), test loss: 51.104642868\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (24.1097888947,32.3082649842), test loss: 30.6204175472\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (127.617004395,52.6953840603), test loss: 54.8609838486\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (15.3177890778,32.2187928017), test loss: 31.7540387154\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (13.6556539536,52.5423216562), test loss: 55.3257784367\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (26.7280673981,32.1285496664), test loss: 30.5735261917\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (60.0767974854,52.3899187), test loss: 53.680241394\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (19.7356204987,32.0399458503), test loss: 31.8821549416\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (36.0899009705,52.2383523986), test loss: 54.2381536484\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (21.4868927002,31.9527310591), test loss: 39.3059286118\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (46.4439697266,52.0915470293), test loss: 71.149263382\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (19.3327941895,31.8645703351), test loss: 30.0603025913\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (29.9781913757,51.9433743986), test loss: 51.1380208969\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (30.0466060638,31.7795845745), test loss: 34.0093717098\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (24.7780418396,51.7990178888), test loss: 60.0803621292\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (29.7665596008,31.6940119062), test loss: 30.7547217369\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (26.5137710571,51.6556679422), test loss: 54.8981259346\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (14.3057346344,31.6102044266), test loss: 30.5898174047\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (22.0552864075,51.5116901017), test loss: 53.1226049423\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (14.2993307114,31.5267554663), test loss: 30.9519289255\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (29.6630210876,51.3692397633), test loss: 56.4570851803\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (21.8966064453,31.4434816555), test loss: 31.5650913715\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (51.6134033203,51.2281304252), test loss: 55.2506381989\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (9.58975601196,31.3626987649), test loss: 30.2753709316\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (15.2593231201,51.0908300618), test loss: 51.5252375603\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (24.0387802124,31.2813219023), test loss: 44.7028856754\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (48.4575843811,50.9540184337), test loss: 89.7000926971\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (23.2446556091,31.2011296257), test loss: 41.0457766056\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (15.2556905746,50.8164103638), test loss: 71.1333106995\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (7.50432777405,31.1222558829), test loss: 34.2739181995\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (14.8942089081,50.6826925646), test loss: 57.081543541\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (30.696762085,31.0432896746), test loss: 33.1582983971\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (19.7955780029,50.5473360648), test loss: 56.8338240623\n",
      "run time for single CV loop: 2070.72940803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:124: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:126: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Deign Net Architecutre and Run Caffe\n",
    "exp_name = 'Exp14_MC'\n",
    "cohort = 'ADNI1'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'CT'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "multi_label = False\n",
    "start_MC=1\n",
    "n_MC=1\n",
    "MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "start_fold = 6\n",
    "n_folds = 10\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "niter = 60000\n",
    "batch_size = 256\n",
    "\n",
    "pretrain = False\n",
    "multimodal_autoencode = False\n",
    "load_pretrained_weights = False\n",
    "\n",
    "if Clinical_Scale in ['BOTH','ADAS13_DX'] :\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "# dr: Dropout rate, lr: learning rate of each modality, tr: loss-weight for each task\n",
    "hype_configs = {\n",
    "                'hyp1':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':10,'HC_CT_ff':25,'COMB_ff':10,\n",
    "                                       'ADAS_ff':10,'MMSE_ff':10,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},\n",
    "    \n",
    "    \n",
    "                'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':10,\n",
    "                                       'ADAS_ff':10,'MMSE_ff':10,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}}\n",
    "       \n",
    "    \n",
    "                }\n",
    "\n",
    "CV_perf_MC = {}\n",
    "for mc in MC_list:\n",
    "    CV_perf_hype = {}\n",
    "    for hype in hype_configs.keys():    \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "        tr = hype_configs[hype]['tr']\n",
    "        solver_configs = hype_configs[hype]['solver_conf']\n",
    "        \n",
    "        if hype in ['hyp1','hyp2']:\n",
    "            HC_snap = 20000 #20000 for ADNI2 5000 for ADNI1\n",
    "            CT_snap = 20000 #20000 for ADNI2 5000 for ADNI1\n",
    "            pre_hype = 'hyp2'\n",
    "        else:\n",
    "            print 'unknown hyp config'\n",
    "            \n",
    "            print hype, pre_hype,HC_snap,CT_snap\n",
    "            \n",
    "        CV_perf = {}\n",
    "        start_time = time.time()\n",
    "        for fid in fid_list:            \n",
    "            print ''\n",
    "            print 'MC # {}, Hype # {}, Fold # {}'.format(mc, hype, fid)\n",
    "            snap_prefix = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}'.format(mc,fid,exp_name,hype,modality)\n",
    "\n",
    "            train_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/train_C688.txt'.format(mc,fid)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "\n",
    "            train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "            with open(train_filename_txt, 'w') as f:\n",
    "                    f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "                    \n",
    "                    \n",
    "            print 'Defining train net'\n",
    "\n",
    "            # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "            if pretrain:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_train.prototxt'.format(mc,fid)\n",
    "                with open(train_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(train_filename_txt, batch_size, node_sizes, modality)))            \n",
    "            else:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(train_net_path, 'w') as f:            \n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "                            \n",
    "            print 'Defining test net'\n",
    "\n",
    "            if pretrain:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_test.prototxt'.format(mc,fid)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(test_filename_txt, batch_size, node_sizes,modality)))\n",
    "            else:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            # Define Solver\n",
    "            print 'Defining solver'\n",
    "            solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "            with open(solver_path, 'w') as f:\n",
    "                f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, snap_prefix)))\n",
    "\n",
    "#             ### load the solver and create train and test nets\n",
    "            #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "            #solver = caffe.get_solver(solver_path)\n",
    "            solver = caffe.NesterovSolver(solver_path)\n",
    "            \n",
    "            if load_pretrained_weights:                    \n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_HC_CT_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                snap_path = baseline_dir + net_name.format(mc,fid,cohort,pre_hype,HC_snap,CT_snap)\n",
    "                print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "                solver.net.copy_from(snap_path)\n",
    "\n",
    "            #run caffe\n",
    "            results = run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode)\n",
    "            CV_perf[fid] = results\n",
    "            \n",
    "        print('run time for single CV loop: {}'.format(time.time() - start_time))\n",
    "\n",
    "        CV_perf_hype[hype] = CV_perf\n",
    "        \n",
    "CV_perf_MC[mc] = CV_perf_hype\n",
    "\n",
    "# pickleIt(CV_perf_MC, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.27777777778\n",
      "5.56666666667\n"
     ]
    }
   ],
   "source": [
    "#Time for training \n",
    "#HC_CT fold 9,10, number of iterations: 40k, two hyper-params\n",
    "#start time: 14:47\n",
    "#end time: 15:31\n",
    "#runtime_mean = (13+31)/4\n",
    "#print runtime_mean\n",
    "\n",
    "tx=670/4 #time for 10k iters\n",
    "itx=4 # num of 10k iters\n",
    "hx=2 #hyp choices\n",
    "fx=5 #k-folds\n",
    "mx=5 #mc-folds\n",
    "\n",
    "num_hrs = (tx*itx*hx*fx*mx)/(3600.0)\n",
    "print num_hrs\n",
    "\n",
    "time_for_single_model = num_hrs*36/60\n",
    "print time_for_single_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABgAAAAJwCAYAAABPg6q5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02f8c448d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "#niter = 30000\n",
    "test_interval = 500\n",
    "n_mc = len(CV_perf_MC.keys())\n",
    "\n",
    "for m, mc in enumerate(CV_perf_MC.keys()): \n",
    "    CV_perf_hype = CV_perf_MC[mc]\n",
    "    \n",
    "    for hype in CV_perf_hype.keys(): \n",
    "        CV_perf = CV_perf_hype[hype]\n",
    "        n_CV_configs = len(CV_perf)\n",
    "        pid = m*n_CV_configs + 1\n",
    "        \n",
    "        for f, fid in enumerate(fid_list):  \n",
    "            train_loss_list = CV_perf[fid]['train_loss']\n",
    "            test_loss_list = CV_perf[fid]['test_loss']\n",
    "            \n",
    "            for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "                #plt.figure()\n",
    "                ax1 = plt.subplot(n_mc,n_CV_configs,pid)            \n",
    "                ax1.plot(arange(niter), train_loss, label='train',linewidth='.5',alpha=0.25)\n",
    "                ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test_{}'.format(hype), linewidth='3')                            \n",
    "                ax1.set_xlabel('iteration')\n",
    "                ax1.set_ylabel('loss')\n",
    "                ax1.set_title('fid: {}, hyp: {}, Test loss: {:.2f}'.format(fid, hype, test_loss[-1]))\n",
    "                ax1.legend(loc=1)\n",
    "                #ax1.set_ylim(0,100)\n",
    "            pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp11'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'R_HC'\n",
    "batch_size = 256\n",
    "encoding_layer = 'code'\n",
    "weight_layers = 'code'\n",
    "multi_task = False\n",
    "\n",
    "if modality in ['L_HC', 'R_HC']:\n",
    "    snap_iter = 10000\n",
    "    \n",
    "    if modality == 'R_HC':\n",
    "        output_node_size = 16471\n",
    "    else: \n",
    "        output_node_size = 16086\n",
    "        \n",
    "    hype_configs = {\n",
    "                   'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':8000,'out':output_node_size},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "\n",
    "elif modality in ['CT']:  \n",
    "    snap_iter = 100000\n",
    "    \n",
    "    hype_configs = {\n",
    "                    'hyp1':{'node_sizes':{'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-4, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "elif modality in ['HC_CT']:  \n",
    "    snap_iter = 20000\n",
    "    hype_configs = {\n",
    "                 'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':500,'out_HC':16086,'out_CT':686},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "hype = 'hyp1'\n",
    "pretrain_preproc = 'ae_preproc_sparse_HC_10k_CT_100k_{}'.format(hype)\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "\n",
    "# Save either L or R HC encodings. Turn on CT + CS save only for one of these. \n",
    "# (Also decide if you want to copy CT raw scores or encodings)\n",
    "save_encodings = True\n",
    "save_scores = False\n",
    "CT_encodings = True\n",
    "\n",
    "for cohort in ['inner_train', 'inner_test','outer_test']:\n",
    "    data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "    test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)       \n",
    "    input_nodes = ['X_{}'.format(modality)]\n",
    "    #input_nodes = ['X_L_HC','X_CT']\n",
    "    net_file = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "    with open(net_file, 'w') as f:\n",
    "        f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "\n",
    "    test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "    with open(test_filename_txt, 'w') as f:\n",
    "        f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "    sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "    if modality == 'CT':\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "    else:\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "        \n",
    "    print 'Check Sparsity for model: {}'.format(model_file)\n",
    "    \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    \n",
    "    encodings_hdf_file = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,pretrain_preproc)\n",
    "    \n",
    "    if save_encodings:    \n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        input_data.create_dataset(input_nodes[0],data=encodings)           \n",
    "        input_data.close()\n",
    "        \n",
    "    if save_scores:\n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        if not CT_encodings: #copy raw scores for CT instead of encodings\n",
    "            CT_data = load_data(data_path, 'X_CT','no_preproc')                    \n",
    "            input_data.create_dataset('X_CT', data=CT_data)    \n",
    "            \n",
    "        adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "        mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "        input_data.create_dataset('y', data=adas_scores)           \n",
    "        input_data.create_dataset('y3', data=mmse_scores)    \n",
    "        input_data.create_dataset('dx_cat3', data=dx_labels)\n",
    "        input_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting TSNE embeddings \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "exp_name = 'Exp11'\n",
    "#preproc = 'no_preproc'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "Clinical_Scale = 'ADAS13_DX' \n",
    "batch_size = 256\n",
    "snap_interval = 2000\n",
    "snap_start = 2000\n",
    "encoding_layer = 'ff3'\n",
    "weight_layers = 'ff3'\n",
    "cohort = 'outer_test'\n",
    "multi_task = False\n",
    "\n",
    "modality = 'HC_CT'\n",
    "snap_end = 21000\n",
    "\n",
    "\n",
    "hype_configs = {\n",
    "            'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-2}},\n",
    "}\n",
    "\n",
    "hype = 'hyp1'\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "tr = hype_configs[hype]['tr']\n",
    "\n",
    "data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "test_net_path = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)       \n",
    "#input_node = 'X_{}'.format(modality)\n",
    "\n",
    "net_file = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)\n",
    "with open(net_file, 'w') as f:\n",
    "    #f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "\n",
    "test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "with open(test_filename_txt, 'w') as f:\n",
    "    f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "n_snaps = len(np.arange(snap_start,snap_end,snap_interval))\n",
    "tsne_encodings = {}\n",
    "net_weights = {}\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    print sn, snap_iter\n",
    "    model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    net_weights[snap_iter] = results['wt_dict']\n",
    "    print encodings.shape\n",
    "\n",
    "    adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "    mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "    dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "    tsne_model = TSNE(n_components=2, random_state=0, init='pca')\n",
    "    tsne_encodings[snap_iter] = tsne_model.fit_transform(encodings) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "plot_encodings = True\n",
    "plot_wts = False\n",
    "\n",
    "color_scores = dx_labels\n",
    "cm = plt.get_cmap('RdYlBu') \n",
    "marker_size = 100\n",
    "marker_size = (np.mod(color_scores+1,2)+1)*50\n",
    "\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    plt.subplot(np.ceil(n_snaps/2.0),2,sn+1)\n",
    "    if plot_encodings:\n",
    "        plt.scatter(tsne_encodings[snap_iter][:,0],tsne_encodings[snap_iter][:,1],c=color_scores,s=marker_size,cmap=cm)\n",
    "    if plot_wts:\n",
    "        plt.imshow(net_weights[snap_iter][weight_layers])\n",
    "        \n",
    "    plt.title(snap_iter)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp14_MC 1 CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.41457441240152471, 0.59366819497634937, 0.33179112122949272, 0.3316428777623231, 0.5137019015498554, 0.50717700225252516, 0.54352629764335392, 0.41801835017393979, 0.37073456103369695, 0.5264017673957313]\n",
      "ADAS mse: [65.420054252759243, 41.702868179076425, 79.77888152022453, 93.333641846724532, 57.308446537228711, 46.716154802941588, 67.289127830161846, 46.109212522428457, 84.625820800835058, 54.284464187945701], rmse: [8.0882664553511869, 6.457775791948527, 8.9319024580558715, 9.6609337978647041, 7.5702342458624559, 6.8349217115444407, 8.2029950524282196, 6.7903764639693183, 9.1992293590732395, 7.3677991413953263]\n",
      "ADAS means: 0.455123648642, 63.656867248, 7.91044344775\n",
      "\n",
      "MMSE corr: [0.34037000489554792, 0.53675009013268138, 0.31596062151379872, 0.33635096013375576, 0.56125543551034329, 0.45000779768837762, 0.52946457175829509, 0.45427346774319444, 0.49238595375169358, 0.55367053542263078]\n",
      "MMSE mse: [122.89515382016961, 79.424097934631078, 128.40245371393428, 133.84961957131875, 81.69826738740214, 90.084262856678691, 116.56745063995747, 59.818355286707181, 142.61837928988552, 63.459631007844713], rmse: [11.085808667849612, 8.9120198571721705, 11.33148064967391, 11.569339634193421, 9.0387093872633244, 9.4912729839931735, 10.796640710885839, 7.7342326889425292, 11.942293719796274, 7.9661553467055057]\n",
      "MMSE means: 0.457048943855, 101.881767151, 9.98679536465\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 10, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.01}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 6, 2: 7, 3: 7, 4: 6, 5: 9, 6: 8, 7: 6, 8: 6, 9: 7, 10: 9}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.42971499068098701, 0.59366819497634937, 0.33179112122949272, 0.3316428777623231, 0.5137019015498554, 0.50778320613744576, 0.56006917114881205, 0.41801835017393979, 0.37073456103369695, 0.5264017673957313]\n",
      "ADAS mse: [65.110377262942222, 41.702868179076425, 79.77888152022453, 93.333641846724532, 57.308446537228711, 46.223194919981829, 64.507431573861766, 46.109212522428457, 84.625820800835058, 54.284464187945701], rmse: [8.0691001519960217, 6.457775791948527, 8.9319024580558715, 9.6609337978647041, 7.5702342458624559, 6.7987642200610123, 8.0316518583577672, 6.7903764639693183, 9.1992293590732395, 7.3677991413953263]\n",
      "ADAS means: 0.458352614209, 63.2984339351, 7.88777674886\n",
      "\n",
      "MMSE corr: [0.34954525084039217, 0.53675009013268138, 0.31596062151379872, 0.33635096013375576, 0.56125543551034329, 0.4384041095621698, 0.55504915382864306, 0.45427346774319444, 0.49238595375169358, 0.55367053542263078]\n",
      "MMSE mse: [124.41842738273139, 79.424097934631078, 128.40245371393428, 133.84961957131875, 81.69826738740214, 90.622620572303887, 123.39817613385549, 59.818355286707181, 142.61837928988552, 63.459631007844713], rmse: [11.154300846881053, 8.9120198571721705, 11.33148064967391, 11.569339634193421, 9.0387093872633244, 9.5195914078443451, 11.108473168435681, 7.7342326889425292, 11.942293719796274, 7.9661553467055057]\n",
      "MMSE means: 0.459364557844, 102.771002828, 10.0276596707\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 10, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.01}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 10, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.01}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 7, 2: 7, 3: 7, 4: 6, 5: 9, 6: 8, 7: 7, 8: 6, 9: 7, 10: 9}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.41457441240152471, 0.59366819497634937, 0.33179112122949272, 0.3316428777623231, 0.5137019015498554, 0.50717700225252516, 0.54352629764335392, 0.40826659035440516, 0.37073456103369695, 0.5264017673957313]\n",
      "ADAS mse: [65.420054252759243, 41.702868179076425, 79.77888152022453, 93.333641846724532, 57.308446537228711, 46.716154802941588, 67.289127830161846, 47.764150100978867, 84.625820800835058, 54.284464187945701], rmse: [8.0882664553511869, 6.457775791948527, 8.9319024580558715, 9.6609337978647041, 7.5702342458624559, 6.8349217115444407, 8.2029950524282196, 6.9111612700745786, 9.1992293590732395, 7.3677991413953263]\n",
      "ADAS means: 0.45414847266, 63.8223610059, 7.92252192836\n",
      "\n",
      "MMSE corr: [0.34037000489554792, 0.53675009013268138, 0.31596062151379872, 0.33635096013375576, 0.56125543551034329, 0.45000779768837762, 0.52946457175829509, 0.50292748760350237, 0.49238595375169358, 0.55367053542263078]\n",
      "MMSE mse: [122.89515382016961, 79.424097934631078, 128.40245371393428, 133.84961957131875, 81.69826738740214, 90.084262856678691, 116.56745063995747, 59.556977424105952, 142.61837928988552, 63.459631007844713], rmse: [11.085808667849612, 8.9120198571721705, 11.33148064967391, 11.569339634193421, 9.0387093872633244, 9.4912729839931735, 10.796640710885839, 7.7173167243612566, 11.942293719796274, 7.9661553467055057]\n",
      "MMSE means: 0.461914345841, 101.855629365, 9.98510376819\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 10, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.01}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 10, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 10, 'En2': 2000, 'CT_ff': 25, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 10, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 6, 2: 7, 3: 7, 4: 6, 5: 9, 6: 8, 7: 6, 8: 9, 9: 7, 10: 9}\n"
     ]
    }
   ],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "#exp_name = 'Exp11'\n",
    "#niter = 10000\n",
    "#modality = 'CT'\n",
    "#start_fold = 1\n",
    "#n_folds = 1\n",
    "#fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "#preproc = 'no_preproc'\n",
    "#batch_size = 256\n",
    "snap_interval = 4000\n",
    "snap_start = 4000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "#Clinical_Scale = 'ADAS13'\n",
    "\n",
    "#MC_list = np.arange(1,11,1)\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "idx = 0  \n",
    "df_perf_dict_adas = {}\n",
    "df_perf_dict_mmse = {}\n",
    "df_perf_dict_adas_tuned = {}\n",
    "df_perf_dict_mmse_tuned = {}\n",
    "df_perf_dict_opt = {}\n",
    "\n",
    "for mc in MC_list:\n",
    "    print exp_name, mc, modality\n",
    "\n",
    "    for hype in hype_configs.keys():      \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "\n",
    "        for fid in fid_list:\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "            #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "            #with open(test_filename_txt, 'w') as f:\n",
    "            #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                    f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC']\n",
    "                elif modality == 'CT':\n",
    "                    f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_CT']\n",
    "                elif modality == 'HC_CT':\n",
    "                    f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "                elif modality == 'HC_CT_unified_hyp1':\n",
    "                    f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_HC_CT']\n",
    "                else:\n",
    "                    print 'Wrong modality'\n",
    "\n",
    "            #print 'Hype # {}, MC # {}, Fold # {}, Clinical_Scale {}'.format(hype, mc, fid, Clinical_Scale)\n",
    "            data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            if Clinical_Scale == 'ADAS13':\n",
    "                act_scores = load_data(data_path, 'adas_bl','no_preproc')\n",
    "            elif Clinical_Scale == 'MMSE': \n",
    "                act_scores = load_data(data_path, 'adas_m12','no_preproc')\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                act_scores_adas13 = load_data(data_path, 'adas_bl','no_preproc')\n",
    "                act_scores_mmse = load_data(data_path, 'adas_m12','no_preproc')\n",
    "            else:\n",
    "                print 'unknown clinical scale'\n",
    "\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort)\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            sub.call([\"cp\", baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort), baseline_dir + \n",
    "                      'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)])\n",
    "            #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "            #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "\n",
    "            if Clinical_Scale in ['ADAS13', 'MMSE']:                \n",
    "                multi_task = False\n",
    "                cs_list = ['opt']\n",
    "                iter_euLoss = []\n",
    "                iter_r = []        \n",
    "                iter_pred_scores = []\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = np.squeeze(results['X_out'])            \n",
    "                    iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                    iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "                fold_r[config_idx] = np.array(iter_r)\n",
    "                fold_act_scores[fid] = act_scores\n",
    "                fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                multi_task = True\n",
    "                cs_list = ['adas','mmse']\n",
    "                iter_euLoss_adas13 = []\n",
    "                iter_r_adas13 = []        \n",
    "                iter_pred_scores_adas13 = []\n",
    "                iter_euLoss_mmse = []\n",
    "                iter_r_mmse = []        \n",
    "                iter_pred_scores_mmse = []\n",
    "\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = results['X_out']   \n",
    "                    encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                    encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                    iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                    iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                    iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                    iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "                fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "                fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "                fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "                fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "                fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "                fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "                fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "\n",
    "    \n",
    "    if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "        fold_perf_dict = {'fold_r':fold_r,'fold_euLoss':fold_euLoss,'fold_act_scores':fold_act_scores,'fold_pred_scores':fold_pred_scores}\n",
    "    else: \n",
    "        fold_perf_dict = {'fold_r_adas13':fold_r_adas13,'fold_r_mmse':fold_r_mmse,'fold_euLoss_adas13':fold_euLoss_adas13,'fold_euLoss_mmse':fold_euLoss_mmse,\n",
    "                         'fold_act_scores_adas13':fold_act_scores_adas13,'fold_act_scores_mmse':fold_act_scores_mmse,\n",
    "                          'fold_pred_scores_adas13':fold_pred_scores_adas13,'fold_pred_scores_mmse':fold_pred_scores_mmse}\n",
    "        \n",
    "    opt_metric = 'euLoss' #euLoss or corr or both\n",
    "    #task_weights = {'ADAS':1,'MMSE':0} \n",
    "    save_multitask_results = False\n",
    "    CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/output/'\n",
    "    save_path = '{}{}_{}_NN_{}'.format(CV_model_dir,exp_name,mc,modality)\n",
    "    \n",
    "    # Create dictionaries of perfromance based on differnt task weights \n",
    "    for key, task_weights in {'adas':{'ADAS':1,'MMSE':0},'mmse':{'ADAS':0,'MMSE':1},'both':{'ADAS':1,'MMSE':1}}.items():\n",
    "        print 'Weights Tuned for: {}'.format(key)                              \n",
    "        results = compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path)  \n",
    "\n",
    "        # populate the perf dictionary for 10 MC x 10 folds\n",
    "        model_choice = 'APANN'    \n",
    "        for f, fid in enumerate(fid_list): \n",
    "            if key in ['adas','mmse']:\n",
    "                r_valid = results['{}_r'.format(key)][f]\n",
    "                MSE_valid = results['{}_mse'.format(key)][f]\n",
    "                RMSE_valid = results['{}_rmse'.format(key)][f]\n",
    "\n",
    "                if key == 'adas':\n",
    "                    df_perf_dict_adas_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                elif key == 'mmse':\n",
    "                    df_perf_dict_mmse_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                else: \n",
    "                    print 'unknown key'\n",
    "\n",
    "            elif key == 'both':                    \n",
    "                for cs in cs_list:            \n",
    "                    r_valid = results['{}_r'.format(cs)][f]\n",
    "                    MSE_valid = results['{}_mse'.format(cs)][f]\n",
    "                    RMSE_valid = results['{}_rmse'.format(cs)][f]\n",
    "\n",
    "                    if cs == 'adas':\n",
    "                        df_perf_dict_adas[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                    elif cs == 'mmse':\n",
    "                        df_perf_dict_mmse[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}                    \n",
    "                    else: \n",
    "                        print 'unknown key'\n",
    "\n",
    "\n",
    "            else: #single task dictionary\n",
    "                df_perf_dict_opt[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "\n",
    "            idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.2195444572928871"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbkAAAJoCAYAAABLB+y9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02f9607790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.scatter(fold_act_scores_mmse[2], fold_pred_scores_mmse['hyp1_2'][3],s=50)\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "\n",
    "np.sqrt(85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results at: /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/output/\n"
     ]
    }
   ],
   "source": [
    "#Save df style dictionaly for seaborn plots\n",
    "cohort = 'ADNI1'\n",
    "update = 0\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_bl_up_{}.pkl'.format(exp_name, cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_m12_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse,df_perf_dict_path)\n",
    "\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_bl_tuned_up_{}.pkl'.format(exp_name,cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas_tuned,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_m12_tuned_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse_tuned,df_perf_dict_path)\n",
    "\n",
    "\n",
    "print 'saving results at: {}'.format(CV_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 25)\n",
    "n_rows = n_folds\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "plot_perf = False\n",
    "\n",
    "if multi_task:\n",
    "    euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "    corrs = [fold_r_adas13,fold_r_mmse]\n",
    "else:\n",
    "    euLosses = [fold_euLoss]\n",
    "    corrs = [fold_r]\n",
    "    \n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        \n",
    "        if plot_perf:\n",
    "            plt.figure(fid)\n",
    "            plt.subplot(n_rows,n_cols,1)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "            plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "            plt.legend()\n",
    "            plt.subplot(n_rows,n_cols,2)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "            plt.title('correlation, fid: {}'.format(fid))\n",
    "            plt.legend(loc=2)\n",
    "\n",
    "print preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path):\n",
    "    fold_r_dict = {}\n",
    "    fold_euLoss_dict = {}\n",
    "    fold_act_scores_dict = {}\n",
    "    fold_pred_scores_dict = {}\n",
    "\n",
    "    if multi_task:\n",
    "        fold_r_dict['ADAS'] = fold_perf_dict['fold_r_adas13']\n",
    "        fold_r_dict['MMSE'] = fold_perf_dict['fold_r_mmse']\n",
    "        fold_euLoss_dict['ADAS'] = fold_perf_dict['fold_euLoss_adas13']\n",
    "        fold_euLoss_dict['MMSE'] = fold_perf_dict['fold_euLoss_mmse']\n",
    "        fold_act_scores_dict['ADAS'] = fold_perf_dict['fold_act_scores_adas13']\n",
    "        fold_act_scores_dict['MMSE'] = fold_perf_dict['fold_act_scores_mmse']\n",
    "        fold_pred_scores_dict['ADAS'] = fold_perf_dict['fold_pred_scores_adas13']\n",
    "        fold_pred_scores_dict['MMSE'] = fold_perf_dict['fold_pred_scores_mmse']\n",
    "        \n",
    "        NN_results = computePerfMetrics(fold_r_dict, fold_euLoss_dict, fold_act_scores_dict, fold_pred_scores_dict, opt_metric, hype_configs, \n",
    "                                        Clinical_Scale,task_weights)\n",
    "        adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "        mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "        adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "        mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "        adas_rmse = NN_results['opt_ADAS']['CV_RMSE'].values()\n",
    "        mmse_rmse = NN_results['opt_MMSE']['CV_RMSE'].values()\n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['opt_ADAS']['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['opt_ADAS']['actual_CV_scores']\n",
    "        \n",
    "        print 'ADAS corr: {}'.format(adas_r)\n",
    "        print 'ADAS mse: {}, rmse: {}'.format(adas_mse,adas_rmse)\n",
    "        print 'ADAS means: {}, {}, {}'.format(np.mean(adas_r),np.mean(adas_mse),np.mean(adas_rmse))\n",
    "        print ''\n",
    "        print 'MMSE corr: {}'.format(mmse_r)\n",
    "        print 'MMSE mse: {}, rmse: {}'.format(mmse_mse,mmse_rmse)\n",
    "        print 'MMSE means: {}, {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse),np.mean(mmse_rmse))\n",
    "        print ''\n",
    "        print 'opt_hyp: {}'.format(opt_hyp)\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        \n",
    "        return {'adas_r':adas_r, 'adas_mse':adas_mse, 'adas_rmse':adas_rmse, 'mmse_r':mmse_r, 'mmse_mse':mmse_mse, 'mmse_rmse':mmse_rmse,\n",
    "               'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        NN_results = computeSingleTaskPerfMetrics(fold_perf_dict, opt_metric, hype_configs)\n",
    "        opt_r = NN_results['opt_r'].values()        \n",
    "        opt_mse = NN_results['opt_mse'].values()        \n",
    "        opt_rmse = NN_results['opt_rmse'].values()        \n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['actual_CV_scores']\n",
    "        print 'opt corr: {}'.format(opt_r)\n",
    "        print 'opt mse: {}'.format(opt_mse)\n",
    "        print 'means: {}, {}'.format(np.mean(opt_r),np.mean(opt_mse))\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        print ''\n",
    "    \n",
    "        return {'opt_r':opt_r, 'opt_mse':opt_mse, 'opt_rmse':opt_rmse,'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    \n",
    "\n",
    "\n",
    "    if save_multitask_results:\n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')        \n",
    "        save_path = save_path + '_' + st + '.pkl' \n",
    "        print 'saving results at: {}'.format(save_path)\n",
    "        output = open(save_path, 'wb')\n",
    "        pickle.dump(NN_results, output)\n",
    "        output.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = np.squeeze(results['predicted_CV_scores'][7])\n",
    "b = np.squeeze(results['actual_CV_scores'][7])\n",
    "\n",
    "print a , b                               \n",
    "print stats.pearsonr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    \n",
    "     # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "\n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    opt_rmse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "\n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    opt_rmse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "\n",
    "    print 'Clinical_Scale: {}'.format(Clinical_Scale)\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        print 'This function does not work for single clinical scale models. Use the code from the next cell'\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "\n",
    "        fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "        fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "        fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "        fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "\n",
    "        for fid in fid_hype_map.keys():\n",
    "            r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "            r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "            r_perf_array = np.array(fid_r_perf[fid])\n",
    "            euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "            euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "            euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "            if opt_metric == 'euLoss':\n",
    "                h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "            else:\n",
    "                h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "\n",
    "            opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "            opt_snap[fid] = snp\n",
    "\n",
    "            opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "            opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "            opt_rmse_adas[fid] = np.sqrt(2*euLoss_perf_array_adas[h,snp]) #RMSE\n",
    "            opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "            opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "            opt_rmse_mmse[fid] = np.sqrt(2*euLoss_perf_array_mmse[h,snp]) #RMSE\n",
    "\n",
    "            actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "            opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "            actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "            opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "        opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'CV_RMSE':opt_rmse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "        opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'CV_RMSE':opt_rmse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_snap':opt_snap, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "def computeSingleTaskPerfMetrics(fold_perf_dict,opt_metric,hype_configs):\n",
    "    corrs = fold_perf_dict['fold_r']\n",
    "    euLosses = fold_perf_dict['fold_euLoss']\n",
    "    act_scores = fold_perf_dict['fold_act_scores']\n",
    "    pred_scores = fold_perf_dict['fold_pred_scores']\n",
    "\n",
    "    \n",
    "    NN_multitask_results = {}\n",
    "   \n",
    "    snap_array = np.arange(snap_start,niter+1,snap_start)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = {}\n",
    "    opt_mse = {}\n",
    "    opt_rmse = {}\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "    actual_scores = {}\n",
    "    opt_pred_scores = {}\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "        # if want to find best hyp from mse values\n",
    "        if opt_metric == 'euLoss':\n",
    "            h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        else:\n",
    "            h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "            \n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        r = r_perf_array[h,snp]        \n",
    "        opt_r[fid] = r\n",
    "        opt_mse[fid] = 2*eu_loss\n",
    "        opt_rmse[fid] = np.sqrt(2*eu_loss)\n",
    "        #print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)        \n",
    "        opt_snap[fid] = snp\n",
    "        opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "        actual_scores[fid] = fold_act_scores[fid]\n",
    "        opt_pred_scores[fid] = fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp]\n",
    "\n",
    "    #print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r.values()), np.mean(opt_mse.values()))\n",
    "    #print opt_r\n",
    "    #print opt_mse\n",
    "    return {'opt_r':opt_r,'opt_mse':opt_mse,'opt_rmse':opt_rmse,'opt_hype':opt_hype,'opt_snap':opt_snap, 'actual_CV_scores':actual_scores,'predicted_CV_scores':opt_pred_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "\n",
    "model_file = 'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-05-06-17-14-49.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {4:0,5:1}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_ADAS13_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(5.64658243068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update fold performance for multitask\n",
    "print NN_results['opt_ADAS'].keys()\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "#model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl'\n",
    "model_file_soa = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-23-52.pkl'\n",
    "print 'current state of art: ' + model_file_soa\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file_soa,'rb'))\n",
    "\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "#load old file\n",
    "# model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-02-48.pkl'\n",
    "# print 'updated fold result file: ' + model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "idx_pairs = {1:1,3:3}\n",
    "CV_data = update_multifold_perf(boxplots_dir + model_file_soa, NN_results, idx_pairs)\n",
    "\n",
    "print \"\"\n",
    "print 'updated CV_data'\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "#print 'saving results at: {}'.format(save_name)\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(CV_data, output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():        \n",
    "        for idx in idx_pairs.keys():            \n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "\n",
    "    return CV_data\n",
    "\n",
    "def update_multifold_perf(saved_perf_file, new_perf_dict, idx_pairs):    \n",
    "    perf_metrics = ['CV_r', 'actual_CV_scores', 'CV_MSE', 'predicted_CV_scores']    \n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        if key == 'opt_hype':\n",
    "            for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                CV_data[key][idx_key] = new_perf_dict[key][idx_val]\n",
    "                \n",
    "        else:\n",
    "            for pm in perf_metrics:\n",
    "                for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                    CV_data[key][pm][idx_key] = new_perf_dict[key][pm][idx_val]\n",
    "    \n",
    "    return CV_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "model_files = ['Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl',             \n",
    "             'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-05-23-07-31-29.pkl']\n",
    "\n",
    "#'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-03-19-29-16_Fold9_10.pkl'\n",
    "\n",
    "Clinical_Scale = 'ADAS'\n",
    "perf_array = []\n",
    "for mf in model_files:\n",
    "    CV_data = pickle.load(open(boxplots_dir + mf,'rb'))['opt_{}'.format(Clinical_Scale)]        \n",
    "    perf_array.append(CV_data['CV_r'].values())\n",
    "\n",
    "print (np.max(np.array(perf_array),axis=0))\n",
    "print np.mean(np.max(np.array(perf_array),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "\n",
    "\n",
    "save_detailed_results = True\n",
    "multi_task = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results    \n",
    "    if not multi_task:\n",
    "        NN_results = {}\n",
    "        NN_results['CV_MSE'] = opt_mse\n",
    "        NN_results['CV_r'] = opt_r\n",
    "        NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "        NN_results['actual_CV_scores'] = actual_scores\n",
    "        NN_results['tid_snap_config_dict'] = opt_hype\n",
    "        print opt_r\n",
    "        print opt_mse\n",
    "        \n",
    "    else:\n",
    "        NN_results = NN_multitask_results\n",
    "        print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "        print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "        print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "        print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])\n",
    "        \n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_BOTH_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    for modality in modalities:\n",
    "        in_data = load_data(in_data_path, 'Fold_{}_{}'.format(fid,modality), preproc)         \n",
    "\n",
    "        # HDF5 is pretty efficient, but can be further compressed.\n",
    "        comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "        with h5py.File(out_data_path, 'a') as f:      \n",
    "            if modality == 'X_R_CT': #Fix the typo            \n",
    "                modality = 'X_CT'            \n",
    "\n",
    "            f.create_dataset('{}'.format(modality), data=in_data, **comp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold1_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold1_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp14_ADNI1_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold2_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold2_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp14_ADNI1_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold3_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold3_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp14_ADNI1_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold4_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold4_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp14_ADNI1_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold5_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold5_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp14_ADNI1_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold6_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold6_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp14_ADNI1_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold7_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold7_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp14_ADNI1_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold8_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold8_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp14_ADNI1_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold9_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold9_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp14_ADNI1_ADAS13_NN_valid_MC_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold10_train_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/caffe_input/CV_Exp14_ADNI1_NN_OuterFold_MC_1_fold10_valid_InnerFold_1.h5\n",
      "/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/CV_Exp14_ADNI1_ADAS13_NN_valid_MC_1.h5\n"
     ]
    }
   ],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp14'\n",
    "exp_name_out = 'Exp14_MC'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "modalities = ['X_L_HC','X_R_HC','X_R_CT','adas_bl','adas_m12','dx']\n",
    "dataset = 'ADNI1'\n",
    "preproc = 'no_preproc' #binary labels\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/'\n",
    "\n",
    "for mc in np.arange(1,2,1):\n",
    "    for fid in np.arange(1,11,1):\n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_train_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_valid_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_ADAS13_NN_valid_MC_{}.h5'.format(exp_name,dataset,mc)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name_out)\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC: fid[1:4] 8000; fid[5,6] 6000, fid[7,8,9]: 8000 fid 10: 6000\n",
    "CT: fid[1:5] 12000 fid[6:10] 6000\n",
    "\n",
    "#spawn net\n",
    "ADNI_FF_train_hyp1_CT.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "cohort = 'ADNI2'\n",
    "modality = 'HC_CT'\n",
    "pretrain_snap_HC = 20000 #ADNI1: 5000\n",
    "pretrain_snap_CT = 20000 #ADNI1: 5000\n",
    "n_folds = 10\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/' \n",
    "#Custom snaps per fold\n",
    "#HC_iter = [8000,8000,8000,8000,6000,6000,8000,8000,8000,6000]\n",
    "#CT_iter = [12000,12000,12000,12000,12000,6000,6000,6000,6000,6000]\n",
    "#mc=1\n",
    "hc_hyp = 'hyp1'\n",
    "ct_hyp = 'hyp1'\n",
    "pretrain_hyp = 'hyp2' #This is hyp generated using optimal combination of hc and ct hyps. Prototxt file is saved with this hyp extension\n",
    "\n",
    "exp_name = 'Exp11_MC'\n",
    "\n",
    "for mc in np.arange(6,11,1):\n",
    "    for fid in np.arange(1,n_folds+1,1):\n",
    "        print 'fid: {}'.format(fid)\n",
    "        #for AE_branch in ['CT','R_HC','L_HC']:\n",
    "        for AE_branch in ['CT','HC']:\n",
    "            print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "            if AE_branch == 'L_HC':\n",
    "                params_FF = ['L_ff1', 'L_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'R_HC':\n",
    "                params_FF = ['R_ff1', 'R_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'HC':\n",
    "                params_FF = ['L_ff1','R_ff1']\n",
    "                AE_iter = pretrain_snap_HC\n",
    "                hyp = hc_hyp\n",
    "            elif AE_branch == 'CT':\n",
    "                #params_FF = ['ff1', 'ff2']\n",
    "                params_FF = ['ff1']\n",
    "                AE_iter = pretrain_snap_CT\n",
    "                hyp = ct_hyp\n",
    "                #fid for pretain is 1 because it's same definition for all the folds.\n",
    "                #Only use this during 1 of the modalities to avoid overwritting\n",
    "                print 'Spawning new net'\n",
    "                pretrain_net = caffe.Net(baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,pretrain_hyp,modality), caffe.TRAIN)\n",
    "            else:\n",
    "                print 'Wrong AE branch'\n",
    "\n",
    "            # conv_params = {name: (weights, biases)}\n",
    "            conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "            for conv in params_FF:\n",
    "                print 'target {} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "            # Review AE net params \n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hyp,AE_branch)\n",
    "            #model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "            model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hyp,AE_branch,AE_iter) \n",
    "\n",
    "            AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "            #params_AE = ['encoder1', 'code']\n",
    "            params_AE = params_FF #if you are using pretrained NN\n",
    "\n",
    "            # fc_params = {name: (weights, biases)}\n",
    "            fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "            for fc in params_AE:\n",
    "                print 'pretrained {} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "            #transplant net parameters\n",
    "            for pr, pr_conv in zip(params_AE, params_FF):\n",
    "                conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "                conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "            save_net = True\n",
    "            if save_net:\n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_{}_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                save_path = baseline_dir + net_name.format(mc,fid,cohort,pretrain_hyp,modality,pretrain_snap_HC,pretrain_snap_CT)\n",
    "                print \"Saving net to \" + save_path\n",
    "                pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_data['opt_ADAS']['CV_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Wxh = 0.5\n",
    "Whh = -1.0\n",
    "hb = -1.0\n",
    "x0 = 9\n",
    "x1 = 4\n",
    "x2 = -2\n",
    "\n",
    "h0 = 1/(1+exp(-Wxh*x0+hb))\n",
    "h1 = 1/(1+exp(h0*Whh - Wxh*x1 + hb))\n",
    "h2 = 1/(1+exp(h1*Whh - Wxh*x2 + hb))\n",
    "\n",
    "print h0,h1,h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
