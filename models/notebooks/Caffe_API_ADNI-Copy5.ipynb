{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import caffe\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "# Some defs to load data and extract encodings from trained net\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    #print net.blobs.items()[0]\n",
    "    #print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        #print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        #print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        #print net.blobs[input_node].shape\n",
    "        \n",
    "        data_layers.append(net.blobs[input_node])    \n",
    "        #print 'X_list shape: {}'.format(X_list[i].shape)\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "    \n",
    "     \n",
    "    net.reshape()            \n",
    "    #print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        #print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1, concat_param=dict(axis=1))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_CT_SpecCluster_dyn,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT_SpecCluster_dyn, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT_unified(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_HC_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_HC_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_HC_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_HC_CT,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_HC_CT, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=4, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas, n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT_SpecCluster_dyn, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "#     n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,n.ff1, concat_param=dict(axis=1))\n",
    "    #n.concat = L.Concat(n.X_L_HC,n.X_R_HC,n.X_CT, concat_param=dict(axis=1))\n",
    "    \n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=lr['COMB']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.dropC1 = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2) #This is done automatically by caffe! \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        #n.ff2_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2ADAS13 = L.ReLU(n.ff2_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        #n.ff2_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2MMSE = L.ReLU(n.ff2_MMSE, in_place=True)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        n.ff1_DX = L.InnerProduct(n.ff3, num_output=node_sizes['DX_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1DX = L.ReLU(n.ff1_DX, in_place=True)\n",
    "        \n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_DX = L.InnerProduct(n.ff1_DX, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.DX_loss = L.SoftmaxWithLoss(n.output_DX, n.dx_cat3,loss_weight=tr['DX'])  \n",
    "    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_CT, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.dropC1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "        \n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_R_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_L_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))       \n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='HC_CT': #multimodal AE - experimental stage\n",
    "        HC_node_split = 0.8\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_L_HC = L.InnerProduct(n.X_L_HC, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.9*node_sizes['En1'])))        \n",
    "        n.NLinEn_L_HC = L.ReLU(n.encoder_L_HC, in_place=True)\n",
    "        \n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_CT = L.InnerProduct(n.X_CT, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.1*node_sizes['En1'])))        \n",
    "        n.NLinEn_CT = L.ReLU(n.encoder_CT, in_place=True)\n",
    "        \n",
    "        #Concat         \n",
    "        n.encoder1 = L.Concat(n.encoder_L_HC,n.encoder_CT, concat_param=dict(axis=1))\n",
    "        \n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers (or common encoder layers for multimodal) \n",
    "    \n",
    "#     n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.Sigmoid(n.encoder2, in_place=True)\n",
    "    #code layer\n",
    "#    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='xavier'))  \n",
    "    \n",
    "#     n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "    \n",
    "    #Decoder layers\n",
    "    if modality == 'CT':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)                    \n",
    "    \n",
    "    elif modality =='R_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    elif modality =='HC_CT':        \n",
    "        n.decoder_L_HC = L.InnerProduct(n.code, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_L_CT = L.ReLU(n.decoder_L_HC, in_place=True)\n",
    "        n.decoder_CT = L.InnerProduct(n.code, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_CT = L.ReLU(n.decoder_CT, in_place=True)\n",
    "        \n",
    "        n.output_L_HC = L.InnerProduct(n.decoder_L_HC, num_output=node_sizes['out_HC'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_HC = L.SigmoidCrossEntropyLoss(n.output_L_HC, n.X_L_HC)\n",
    "        \n",
    "        n.output_CT = L.InnerProduct(n.decoder_CT, num_output=node_sizes['out_CT'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_CT = L.EuclideanLoss(n.output_CT, n.X_CT)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    return n.to_proto()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.96 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode):\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 20\n",
    "    \n",
    "    if multimodal_autoencode:\n",
    "        train_loss_HC = zeros(niter)\n",
    "        test_loss_HC = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_loss_CT = zeros(niter)\n",
    "        test_loss_CT = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "        for it in range(niter):            \n",
    "            solver.step(1)  # SGD by Caffe \n",
    "            train_loss_HC[it] = solver.net.blobs['loss_HC'].data\n",
    "            train_loss_CT[it] = solver.net.blobs['loss_CT'].data\n",
    "            \n",
    "            if it % test_interval == 0:                        \n",
    "                t_loss_HC = 0\n",
    "                t_loss_CT = 0                                \n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()   \n",
    "                    t_loss_HC += solver.test_nets[0].blobs['loss_HC'].data\n",
    "                    t_loss_CT += solver.test_nets[0].blobs['loss_CT'].data\n",
    "                \n",
    "                test_loss_HC[it // test_interval] = t_loss_HC/(test_iter)\n",
    "                test_loss_CT[it // test_interval] = t_loss_CT/(test_iter)\n",
    "                print 'HC Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_HC[it], np.sum(train_loss_HC)/it, test_loss_HC[it // test_interval])\n",
    "                print 'CT Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_CT[it], np.sum(train_loss_CT)/it, test_loss_CT[it // test_interval])\n",
    "                \n",
    "        perf = {'train_loss':[train_loss_HC, train_loss_CT],'test_loss':[test_loss_HC,test_loss_CT]}\n",
    "    else: \n",
    "        \n",
    "        #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "        # losses will also be stored in the log\n",
    "        test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "        if not multi_task:\n",
    "            train_loss = zeros(niter)\n",
    "            test_loss = zeros(int(np.ceil(niter / test_interval)))  \n",
    "\n",
    "        else: \n",
    "            if multi_label:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_DX_loss = zeros(niter)\n",
    "                test_DX_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "            else:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_MMSE_loss = zeros(niter)\n",
    "                test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "\n",
    "        #output = zeros((niter, batch_size))\n",
    "        #solver.restore()\n",
    "        #the main solver loop\n",
    "        for it in range(niter):\n",
    "            #solver.net.forward()\n",
    "            solver.step(1)  # SGD by Caffe    \n",
    "            # store the train loss\n",
    "            if not multi_task:\n",
    "                train_loss[it] = solver.net.blobs['loss'].data        \n",
    "            else: \n",
    "                if multi_label:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_DX_loss[it] = solver.net.blobs['DX_loss'].data\n",
    "                else:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "            # store the output on the first test batch\n",
    "            # (start the forward pass at conv1 to avoid loading new data)\n",
    "            #solver.test_nets[0].forward()\n",
    "            #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "            # run a full test every so often\n",
    "            # (Caffe can also do this for us and write to a log, but we show here\n",
    "            #  how to do it directly in Python, where more complicated things are easier.)\n",
    "            if it % test_interval == 0:        \n",
    "                t_loss = 0\n",
    "                t_ADAS13_loss = 0\n",
    "                t_MMSE_loss = 0\n",
    "                t_DX_loss = 0\n",
    "                correct = 0\n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()                \n",
    "                    if not multi_task:\n",
    "                        t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                        if multi_label:\n",
    "                            correct += sum(solver.test_nets[0].blobs['output'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "                    else: \n",
    "                        if multi_label:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_DX_loss += solver.test_nets[0].blobs['DX_loss'].data\n",
    "                            correct += sum(solver.test_nets[0].blobs['output_DX'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "\n",
    "                        else:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "                if not multi_task:\n",
    "                    test_loss[it // test_interval] = t_loss/(test_iter)\n",
    "                    if multi_label:\n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                    print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                        it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval], test_acc[it // test_interval])\n",
    "                else:\n",
    "                    if multi_label:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_DX_loss[it // test_interval] = t_DX_loss/(test_iter) \n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'DX Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                            it, train_DX_loss[it], np.sum(train_DX_loss)/it, test_DX_loss[it // test_interval],test_acc[it // test_interval])\n",
    "                    else:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_MMSE_loss[it // test_interval] = t_MMSE_loss/(test_iter)             \n",
    "\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "        if not multi_task:\n",
    "            perf = {'train_loss':[train_loss],'test_loss':[test_loss],'test_acc':[test_acc]}\n",
    "        else:\n",
    "            if multi_label:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_DX_loss],'test_loss':[test_ADAS13_loss,test_DX_loss],'test_acc':[test_acc]}\n",
    "            else:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "                \n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,snap_prefix):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    \n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    #s.solver_type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 100000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 4000\n",
    "    s.snapshot_prefix = snap_prefix\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MC # 1, Hype # hyp1, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold1/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (29.2352142334,inf), test loss: 35.6544854164\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.39219975471,inf), test loss: 3.48717322946\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (67.3396759033,25.574990098), test loss: 36.0582522869\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (6.16956329346,2.35996081722), test loss: 3.01429992616\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (91.7916259766,25.5472960799), test loss: 48.6253270149\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (7.7107334137,2.35738070512), test loss: 3.23568522036\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (10.1386566162,25.5631060826), test loss: 33.2086153269\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.3161945343,2.3530996405), test loss: 3.33171541691\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (26.7836170197,25.5892250822), test loss: 33.1575743914\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.42301559448,2.34491119076), test loss: 3.19782031178\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (30.2324810028,25.5951436401), test loss: 36.6716616392\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.37496089935,2.34359788471), test loss: 3.01803417206\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (34.0474090576,25.6002796003), test loss: 33.9083992481\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.77468276024,2.34200945832), test loss: 3.3064493835\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (23.1440525055,25.5479659918), test loss: 34.5322431087\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.24976837635,2.3391946096), test loss: 3.07474809885\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (32.6526794434,25.5610223328), test loss: 31.2688186407\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.30112445354,2.33917821366), test loss: 3.20693484545\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (65.5809631348,25.5619116435), test loss: 37.6084807396\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.47309494019,2.34008230426), test loss: 3.36642881036\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (35.7480163574,25.5149548695), test loss: 32.7216195822\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.35685992241,2.33895741104), test loss: 3.03373740315\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (16.2253837585,25.4907539364), test loss: 34.2472047806\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.75638628006,2.33833708611), test loss: 3.06101547182\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (7.94702529907,25.486919311), test loss: 35.6297590733\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.68468856812,2.3368412223), test loss: 3.31203156114\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (13.8063087463,25.4746682734), test loss: 34.8813118935\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.7849855423,2.33540336064), test loss: 3.43429096937\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (20.3102970123,25.4753497964), test loss: 35.9170058727\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.67327404022,2.33440220604), test loss: 3.26446747184\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (18.062084198,25.4556915576), test loss: 35.6754377604\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.478944182396,2.33255317241), test loss: 3.10510354638\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (18.0329551697,25.4350266263), test loss: 41.3743402004\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.51998901367,2.33184986114), test loss: 3.35701183379\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (38.7437820435,25.4340266882), test loss: 30.5742302895\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.16562223434,2.33169079125), test loss: 3.16866303086\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (25.8512001038,25.4084447776), test loss: 36.9917512894\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.23912513256,2.3308033464), test loss: 3.28433398455\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (20.7847423553,25.382172802), test loss: 33.6044055223\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.35105085373,2.32995971108), test loss: 3.10210311115\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (14.6832284927,25.3808077853), test loss: 35.1428343773\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.89032840729,2.32927311049), test loss: 3.09427832365\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (28.8428001404,25.3674896805), test loss: 37.2504039288\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.5507581234,2.32818909082), test loss: 3.25475226641\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (6.40970611572,25.3552743415), test loss: 33.3807822704\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.37750816345,2.32705132437), test loss: 3.33712310344\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (21.0617218018,25.3460001335), test loss: 38.6016393661\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.75488519669,2.32579198693), test loss: 3.40623609424\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (18.638710022,25.3237382637), test loss: 37.6332182884\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.61453866959,2.32452871084), test loss: 3.07282996774\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (19.3769836426,25.314353261), test loss: 40.4999892235\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.89040493965,2.32413200179), test loss: 3.04133196026\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (59.1585845947,25.3017267463), test loss: 37.8417189598\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.35604453087,2.32376599711), test loss: 3.27395398021\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (17.2951374054,25.2786881445), test loss: 50.2613031387\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.40992927551,2.32285363748), test loss: 3.40722985268\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (22.2252845764,25.260254326), test loss: 33.2388286591\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.18749976158,2.32209048177), test loss: 3.1747204423\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (10.8775615692,25.2514555557), test loss: 33.6533007145\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.734942376614,2.32067867255), test loss: 3.33965476155\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (12.7634792328,25.237949163), test loss: 35.9871315479\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.997424900532,2.31996638011), test loss: 3.03009489775\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (20.3851623535,25.2271010298), test loss: 33.279633522\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.90352571011,2.31897705127), test loss: 3.19033865333\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (26.7412796021,25.2092740725), test loss: 34.9902703285\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.72586965561,2.31784482496), test loss: 3.39369497001\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (10.7481060028,25.1938375239), test loss: 34.7094713211\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.61648702621,2.3168725663), test loss: 3.24159997106\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (9.92997932434,25.1829445397), test loss: 37.9651272297\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.09996128082,2.31632431112), test loss: 3.30141471624\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (16.5702419281,25.1641412251), test loss: 31.9068596125\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.11594295502,2.31548202448), test loss: 3.23353329897\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (46.2997245789,25.143979793), test loss: 37.7690447807\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.98092699051,2.3145347559), test loss: 3.35334093273\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (32.1681060791,25.1365258416), test loss: 30.9632211924\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.828947603703,2.31357921417), test loss: 3.1161155045\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (13.4187030792,25.1208791883), test loss: 35.9377011299\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.62954545021,2.31284567381), test loss: 3.22841108143\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (20.6845302582,25.1075080813), test loss: 32.1754364014\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.05098295212,2.31177926691), test loss: 2.91615711451\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (11.3774175644,25.0937888802), test loss: 32.198445034\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.87227725983,2.31059866402), test loss: 3.08077889979\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (37.0779495239,25.0785033178), test loss: 37.8883874893\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.499984622,2.30958508422), test loss: 3.62741248012\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (24.9048480988,25.0660106361), test loss: 33.0615129471\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.51966357231,2.30884746196), test loss: 3.31142869592\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (23.4728736877,25.0502327456), test loss: 40.1405350208\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.56570529938,2.30849260621), test loss: 3.27145087421\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (61.2449913025,25.0328217072), test loss: 48.5650044441\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.83166742325,2.30770775881), test loss: 3.33770958483\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (4.33742713928,25.0174768708), test loss: 38.6749019384\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.46644878387,2.30680221422), test loss: 3.17375177741\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (11.096619606,25.0077794497), test loss: 31.3248314857\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.87190830708,2.30573294061), test loss: 3.16843467355\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (35.8347930908,24.9949926914), test loss: 36.4678343773\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.97042894363,2.30492308078), test loss: 3.40252191722\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (13.701883316,24.9808209261), test loss: 33.0534828901\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.18012499809,2.30387554546), test loss: 2.92575156093\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (9.55463409424,24.9659685562), test loss: 35.3871498585\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.27488160133,2.30296779154), test loss: 3.22307584584\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (20.987903595,24.9532068253), test loss: 33.2995193481\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.02625405788,2.30202269463), test loss: 3.20329779387\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (41.3606262207,24.941884917), test loss: 32.9118044376\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.0523443222,2.30165178344), test loss: 3.51797167659\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (15.8810758591,24.9240412112), test loss: 37.1332096815\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.499919831753,2.30082576897), test loss: 3.33501966\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (23.2203464508,24.905803133), test loss: 34.8395038128\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.23592472076,2.2998770617), test loss: 3.06713774651\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (36.1810760498,24.8959708789), test loss: 45.3627402306\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.57043576241,2.29906215147), test loss: 3.17613843679\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (15.4774513245,24.8839904417), test loss: 30.8543262005\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.38948976994,2.29820212764), test loss: 3.13469039202\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (17.1481151581,24.8701022095), test loss: 37.2368324757\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (6.48537015915,2.29731358371), test loss: 3.5022995472\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (10.7274589539,24.8570035706), test loss: 33.2439317942\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.820591449738,2.29623712279), test loss: 3.14308857918\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (25.6755104065,24.8427727755), test loss: 41.7884518623\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.14941716194,2.29547323891), test loss: 3.35545374453\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (40.7493286133,24.8299118207), test loss: 33.3273309946\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.96176671982,2.29465732259), test loss: 3.34496631026\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (19.6101722717,24.8149307349), test loss: 32.3190965652\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.627475440502,2.29400337419), test loss: 3.15094797015\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (42.1566009521,24.7984233309), test loss: 35.5502256155\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.91450643539,2.2932644673), test loss: 3.50090951025\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (10.8423337936,24.7858419615), test loss: 36.9060329914\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.29185247421,2.29231204603), test loss: 3.18469109535\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (25.3005580902,24.7745084692), test loss: 41.3343233585\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.59462618828,2.29152232614), test loss: 3.04745236039\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.3873462677,24.7621988156), test loss: 31.1635267258\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.2305598259,2.29062359033), test loss: 3.15250122547\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (9.84009933472,24.7469721398), test loss: 41.4099930763\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.7156894207,2.28960120139), test loss: 3.6222004354\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (19.8249168396,24.7337653573), test loss: 32.1846559525\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.19059491158,2.28874244104), test loss: 2.99484755695\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (17.7558555603,24.7206440459), test loss: 39.8464167118\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.912301898003,2.28793939882), test loss: 3.38535319865\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (30.05818367,24.7070183817), test loss: 34.2595141411\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.412798196077,2.28732897215), test loss: 2.9654037118\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (13.7553853989,24.6908876154), test loss: 44.7863656998\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.30816173553,2.28666354879), test loss: 3.23420320749\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (15.3756942749,24.6750365252), test loss: 33.3208504677\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.12415528297,2.28571291844), test loss: 3.39530956149\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.9791297913,24.6632823262), test loss: 33.1497106075\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.91951084137,2.284758778), test loss: 3.23732179999\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (14.5555953979,24.6516736875), test loss: 40.2740136147\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.72911053896,2.28405866329), test loss: 2.89324825406\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (19.8763122559,24.6383349454), test loss: 34.1962544441\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.12739849091,2.28308021343), test loss: 3.23304059505\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (32.8399543762,24.6240107934), test loss: 34.8956532478\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.63042426109,2.28224048298), test loss: 3.18258818388\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (11.9378547668,24.6112730871), test loss: 33.901491189\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.15451741219,2.28145421204), test loss: 3.20179795325\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.0326805115,24.5976016666), test loss: 36.5600105286\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.24047756195,2.28066221302), test loss: 3.64081071615\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (23.599855423,24.5832556632), test loss: 33.1041104794\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.25170564651,2.27998784508), test loss: 2.94381465614\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (40.5908279419,24.5686748167), test loss: 37.0463421822\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.52959823608,2.27918676777), test loss: 3.10609913468\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (32.4806098938,24.5567211664), test loss: 33.7963533401\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.01189684868,2.27823507367), test loss: 3.41967732608\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold2/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (36.4555435181,inf), test loss: 31.0455755472\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.05936408043,inf), test loss: 2.66556924284\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (22.2429714203,26.1705134845), test loss: 34.3774644852\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.561423420906,2.43137791154), test loss: 3.53165228963\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (34.1173400879,26.1657588196), test loss: 29.2950405359\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.48356485367,2.43571087284), test loss: 3.30401361585\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (4.31630897522,26.1914555114), test loss: 33.0096444368\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.78568911552,2.43133728861), test loss: 3.63007484674\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (5.88625144958,26.205600233), test loss: 29.1807070732\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.4421710968,2.42948754794), test loss: 3.55567136109\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (30.1450500488,26.2338391586), test loss: 37.2522631526\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.21717023849,2.42884725983), test loss: 2.89401450455\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (8.48553276062,26.1734076111), test loss: 32.4034918785\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.48107910156,2.42260389491), test loss: 3.73405703306\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (22.3620147705,26.1721451962), test loss: 38.5688255787\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.42549657822,2.42349091127), test loss: 2.84879375994\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (17.0585384369,26.1633363106), test loss: 30.5055196762\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.62570774555,2.42187520572), test loss: 3.81126332879\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (13.9388751984,26.125017087), test loss: 32.1215539455\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.20881700516,2.42076928036), test loss: 2.64548016787\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (52.961063385,26.1090343858), test loss: 35.7091629982\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.83488881588,2.42014703638), test loss: 3.80138156414\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (31.0089244843,26.0981591771), test loss: 39.2792961836\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.47808790207,2.41928701994), test loss: 2.51177130938\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (15.1922769547,26.090427726), test loss: 34.7343929291\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.82192802429,2.41759443903), test loss: 3.5721234858\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (19.7402496338,26.0788528063), test loss: 29.0979642391\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.06410181522,2.41640920817), test loss: 3.29661902189\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (51.2269706726,26.0485311019), test loss: 38.1547430754\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.12140679359,2.41454334934), test loss: 3.49224007726\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (6.92876434326,26.038444984), test loss: 28.9291027546\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (5.48529958725,2.41370911914), test loss: 3.83140540123\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (29.1930274963,26.0320208925), test loss: 34.9516033649\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.38325834274,2.41301194873), test loss: 2.85657019913\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (29.0579090118,25.9941021582), test loss: 33.3747484684\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.94551980495,2.41162001988), test loss: 3.83042067289\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (29.2162799835,25.9745264279), test loss: 33.9533367634\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.05956554413,2.41071035039), test loss: 2.49117931128\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (33.1819801331,25.9645362011), test loss: 33.6013588428\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (6.42513656616,2.40929398275), test loss: 3.80779175162\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (16.4122619629,25.9546230961), test loss: 34.4443416834\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.62254273891,2.40831975451), test loss: 2.59809808135\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (28.3624839783,25.9395007579), test loss: 34.1336698055\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.05936217308,2.40681922178), test loss: 3.55757709742\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (26.5443458557,25.9116613615), test loss: 30.238147068\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.74092340469,2.40529505201), test loss: 2.58781853318\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (15.3461561203,25.9041576254), test loss: 38.7605908871\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.60989296436,2.40428298775), test loss: 3.5820048213\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (52.1890182495,25.8879423875), test loss: 35.4868279457\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.2835713625,2.40354980401), test loss: 3.54220167398\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (13.4312496185,25.8604101527), test loss: 35.8228516102\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.63277101517,2.40276553566), test loss: 3.09496738613\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (13.5262966156,25.8458428135), test loss: 29.8571507454\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.33622002602,2.40176584965), test loss: 3.65052158237\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (28.3703327179,25.8346803164), test loss: 35.4709278822\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.34715616703,2.4002007634), test loss: 2.72517617941\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (18.2705726624,25.8219247929), test loss: 32.2994954586\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.744736850262,2.39919225616), test loss: 3.79129621387\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (49.2935791016,25.8056432788), test loss: 33.3890946865\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.65501832962,2.39786355815), test loss: 2.76710868776\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (16.5049686432,25.7844433047), test loss: 31.9287984848\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.75043022633,2.39676445986), test loss: 3.89038539529\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (43.2705535889,25.7726529292), test loss: 31.4885551929\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (4.7898144722,2.39560448354), test loss: 2.6651615411\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (32.4163284302,25.752930185), test loss: 35.8118957996\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.589273929596,2.39467150848), test loss: 3.57632504404\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (38.5815238953,25.7297612237), test loss: 30.6296654701\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.67155599594,2.39384789672), test loss: 3.41026227474\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (12.3311233521,25.7175650524), test loss: 32.9740405083\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.53205823898,2.39291734907), test loss: 3.61117545366\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (5.06706047058,25.7031701769), test loss: 28.884749794\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.64456582069,2.39183054449), test loss: 3.49579998851\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (33.6903533936,25.6906162663), test loss: 34.8638443947\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.76426458359,2.39075030636), test loss: 2.95666619837\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (8.36498451233,25.6710595356), test loss: 34.6369959354\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.54644191265,2.38929179095), test loss: 3.67513058186\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (16.3923664093,25.6566047851), test loss: 37.7706068039\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.74391412735,2.38851956079), test loss: 2.76508510709\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (31.9811134338,25.6404314366), test loss: 30.6149830103\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.82206130028,2.38722307038), test loss: 3.91644604802\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (17.9493236542,25.623564894), test loss: 32.1587376595\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.20958614349,2.38647338794), test loss: 2.67174067795\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (45.1071128845,25.6050934469), test loss: 32.5963678837\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.59784603119,2.38542045885), test loss: 3.77499596477\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (34.7330245972,25.5903869331), test loss: 37.1918976784\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.24661386013,2.38440653963), test loss: 2.63236523569\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (14.7517490387,25.5784001586), test loss: 35.144401741\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.13531136513,2.38340165714), test loss: 3.51947731376\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (42.3497085571,25.5633030061), test loss: 31.5349477291\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.98518252373,2.38222864805), test loss: 3.15393943787\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (47.7990455627,25.5443547349), test loss: 38.557816124\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.57493233681,2.38105616214), test loss: 3.5084677279\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (11.0803785324,25.5307460409), test loss: 30.0077057838\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (6.96058177948,2.38020372345), test loss: 3.97712105811\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (31.554227829,25.5156802469), test loss: 37.2934432983\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.31826758385,2.37918371596), test loss: 3.21648811102\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.6764755249,25.4967814148), test loss: 31.9888991833\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.4510859251,2.37826649552), test loss: 3.84433594942\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (22.6992645264,25.4790047409), test loss: 36.2473141909\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.44410204887,2.37721116056), test loss: 2.43781626523\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (34.9672393799,25.4638703133), test loss: 36.9430314302\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (6.65544843674,2.37606945133), test loss: 3.89102718234\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (14.073094368,25.4525430629), test loss: 36.3391324997\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.23868060112,2.37530451646), test loss: 2.63323283195\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (22.4979629517,25.4359583589), test loss: 34.6356785536\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.55098342896,2.37395310427), test loss: 3.57638577223\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (23.4453601837,25.4173010108), test loss: 31.0861608028\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.24467635155,2.37279522931), test loss: 2.58223523945\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (18.515499115,25.4055638487), test loss: 39.8442767143\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.68457233906,2.37190272886), test loss: 3.55384765863\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (56.3739585876,25.387639411), test loss: 33.5487100124\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.79068470001,2.37099488133), test loss: 3.47334921956\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (23.6972885132,25.3713730226), test loss: 37.0294447899\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.11026787758,2.37020335124), test loss: 3.58862369061\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (12.2577400208,25.3551888872), test loss: 28.3061926126\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.7490696907,2.36921774421), test loss: 3.76000003517\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (11.2565374374,25.339922842), test loss: 35.4632484436\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.48615908623,2.36788451202), test loss: 2.78866504431\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (16.7698097229,25.32912469), test loss: 32.5239786625\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.28495901823,2.36712305305), test loss: 3.85540999174\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (34.0105857849,25.3118360902), test loss: 35.0647212267\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.92325258255,2.3658641354), test loss: 2.73403570652\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (15.9541559219,25.2948386101), test loss: 32.10817976\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.92757081985,2.36482802128), test loss: 3.88998372257\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (33.9471931458,25.2843357972), test loss: 31.4532192707\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (4.2057056427,2.36396816171), test loss: 2.61452175379\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (21.4525680542,25.2644783254), test loss: 36.8703928709\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.524314641953,2.36292720512), test loss: 3.57029182315\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (29.2389431,25.247272769), test loss: 30.375347662\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.79487276077,2.36208065917), test loss: 2.97540780902\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (11.6063318253,25.234826696), test loss: 34.4788089037\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.44536471367,2.36120920969), test loss: 3.5865719527\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (9.41962623596,25.2184735563), test loss: 28.4819688797\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.6146800518,2.36009772489), test loss: 3.43810972273\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (21.4635505676,25.2067607717), test loss: 33.6198060513\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.29571425915,2.35922589721), test loss: 2.88002934456\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (36.9444236755,25.1898298495), test loss: 32.5384143353\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.22246348858,2.35799961303), test loss: 3.72817769349\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (19.6703071594,25.1731995604), test loss: 37.6246301651\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.77213740349,2.35708347543), test loss: 2.77736156285\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (33.694278717,25.1610487713), test loss: 33.5886951923\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.21440625191,2.35612367999), test loss: 4.03659260273\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (20.7545909882,25.1441699517), test loss: 32.0770758152\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.8365778923,2.35521088346), test loss: 2.71330396533\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (42.6510848999,25.1273271582), test loss: 31.5395394802\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.95555210114,2.35422676082), test loss: 3.62383359075\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (40.2224006653,25.1146350754), test loss: 31.8887370348\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.702392578125,2.35337134625), test loss: 2.68558790684\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (13.6368770599,25.1000752797), test loss: 34.6325559139\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.25025784969,2.35231920306), test loss: 3.54799437523\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (33.3673934937,25.0866960032), test loss: 31.2893564224\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.83717775345,2.3512970809), test loss: 3.16789914668\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (38.5369644165,25.0704338096), test loss: 36.5590141773\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.09419107437,2.35028814496), test loss: 3.59840511978\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.3395929337,25.0545367763), test loss: 30.7236358166\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.45219373703,2.34931979577), test loss: 3.85791099966\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (11.8907432556,25.0431151299), test loss: 35.2542346239\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.31810355186,2.34847349463), test loss: 3.10839780271\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (26.136302948,25.0257564571), test loss: 31.9132472515\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.63884782791,2.3475594119), test loss: 3.77040125728\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold3/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (48.0931510925,inf), test loss: 34.1157515526\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.69938516617,inf), test loss: 3.09830097556\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (24.118144989,26.165207778), test loss: 33.0518473387\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.77290439606,2.42816049886), test loss: 3.7602191925\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (99.3111495972,26.0970470142), test loss: 36.337693882\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.1203622818,2.4223146755), test loss: 3.83342673928\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (12.9714393616,26.0718812315), test loss: 31.7537261248\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.36213159561,2.41842296922), test loss: 3.68953918219\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (19.0683345795,26.0834771971), test loss: 31.6278973579\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.45968961716,2.40773831262), test loss: 3.6878407836\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (25.018404007,26.0805639203), test loss: 36.824731493\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (8.06838703156,2.4097174454), test loss: 2.8353913337\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (30.7418403625,26.0639790083), test loss: 30.9201133728\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (6.24924516678,2.40497095455), test loss: 4.16865912676\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (42.8486404419,26.0302998671), test loss: 33.1880033493\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.60220980644,2.40331641333), test loss: 3.18278119266\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (29.0968608856,26.0156020817), test loss: 32.1907884121\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.39843094349,2.40276628964), test loss: 4.04947271347\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (5.00054931641,26.0008416388), test loss: 32.4028164864\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.61731660366,2.4022446299), test loss: 3.29327422976\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (11.1754779816,25.9683966463), test loss: 29.6989272118\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.952354669571,2.40143214954), test loss: 3.62988618612\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (36.9619216919,25.9443195991), test loss: 30.5468815327\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.80485618114,2.40080748061), test loss: 3.574138394\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (29.299495697,25.9449341841), test loss: 34.0151573181\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.50103569031,2.39935342735), test loss: 3.86569509804\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (60.5709228516,25.932569039), test loss: 31.2473916531\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.114392519,2.39781094309), test loss: 3.96789869219\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (28.0155067444,25.9147710254), test loss: 34.7791052341\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.41789102554,2.39657107396), test loss: 2.9865567565\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (41.3191337585,25.8944834609), test loss: 30.1942781925\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.14743256569,2.39434563467), test loss: 3.91847076416\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (39.5974998474,25.8746493295), test loss: 31.829042244\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.37841701508,2.3934944428), test loss: 2.95797989368\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (41.3473587036,25.8601212162), test loss: 31.1718265533\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.0613861084,2.3927679923), test loss: 3.80197004676\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (25.0558891296,25.8416144999), test loss: 35.4315804482\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (5.14530658722,2.39247430295), test loss: 3.08941248059\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (74.6858978271,25.8192191903), test loss: 34.514641571\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.63990831375,2.39174923995), test loss: 3.55602980554\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (12.6678848267,25.8029638224), test loss: 31.1913607121\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.01498794556,2.39085322679), test loss: 3.5328997016\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (20.2936859131,25.7909366879), test loss: 30.2946772575\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.11342477798,2.38847137871), test loss: 3.89458912015\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (25.1750717163,25.776141166), test loss: 32.1678523064\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (7.92292118073,2.38820767008), test loss: 3.81016459763\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (33.9342346191,25.762993461), test loss: 33.9058128119\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (5.58057403564,2.38633152943), test loss: 3.28784805536\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (32.8665771484,25.7401272089), test loss: 33.6440835476\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.72489380836,2.38514241326), test loss: 3.87086899877\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (29.5013008118,25.724632947), test loss: 36.2823529482\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.48492562771,2.38426222827), test loss: 2.85224544108\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (4.5389957428,25.7091585144), test loss: 28.7276723862\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.55570495129,2.38354952712), test loss: 3.84207816124\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (11.0239048004,25.6874037076), test loss: 33.5078401089\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.973054587841,2.38262997938), test loss: 3.16079341471\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (41.4479522705,25.6691645018), test loss: 31.442056179\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.29444551468,2.38183889251), test loss: 3.72063271403\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (26.3197822571,25.6584390299), test loss: 37.5132840633\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.63053917885,2.38063503988), test loss: 3.04997692853\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (61.2497673035,25.6440987846), test loss: 29.5350811481\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.06321644783,2.37949237183), test loss: 3.99164047837\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (21.6011047363,25.6277669639), test loss: 30.3736872196\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.44840407372,2.37843522056), test loss: 4.00655978918\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (48.842414856,25.610048358), test loss: 33.9561173201\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.00049114227,2.37679950711), test loss: 3.18725289106\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (40.1058158875,25.5932723405), test loss: 34.1716616154\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.30204677582,2.37591499296), test loss: 3.8992906034\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (36.7522621155,25.5769425801), test loss: 34.7743888378\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.97660446167,2.37505013888), test loss: 2.74287165403\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (19.1065788269,25.5592067989), test loss: 33.7034628391\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.75575637817,2.37443419799), test loss: 3.92996224165\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (51.131690979,25.5401010238), test loss: 33.9051032066\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.09752893448,2.37363969457), test loss: 3.27606229186\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (11.8661212921,25.525036388), test loss: 30.2690639019\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.33980703354,2.37264536279), test loss: 3.75038325787\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (21.8402252197,25.5116012142), test loss: 34.2792163372\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.81917178631,2.37097556333), test loss: 3.40381964445\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (27.4319248199,25.497116737), test loss: 30.0635591507\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (7.08770847321,2.37040984721), test loss: 3.77658467889\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (37.3956375122,25.4827465774), test loss: 30.7523342609\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.51264572144,2.36897751208), test loss: 3.92313238978\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (25.8205471039,25.4631163652), test loss: 32.0685590267\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.70871353149,2.36794384526), test loss: 3.80850285292\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (28.7644348145,25.4468344583), test loss: 33.1244952202\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.44134259224,2.36694630264), test loss: 3.79552827477\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (3.68914556503,25.4322824534), test loss: 35.2998014212\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.29413664341,2.36617213759), test loss: 3.11264392436\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (11.3093290329,25.4145231435), test loss: 32.0578668118\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.992823839188,2.3653270491), test loss: 3.97994551659\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (48.3922462463,25.3965214677), test loss: 31.674007678\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.17161798477,2.3644742209), test loss: 3.14038444459\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (26.1888599396,25.383637291), test loss: 36.0218078613\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.01588201523,2.36338592745), test loss: 3.99230661988\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (58.8005409241,25.3692629433), test loss: 32.833540678\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.97525048256,2.36230100028), test loss: 3.15707069933\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (23.4141349792,25.3539879596), test loss: 28.9353970528\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.8258190155,2.36125902313), test loss: 3.89211322069\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (47.4495353699,25.3376894653), test loss: 30.9708793163\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.72529006004,2.3599310417), test loss: 3.68682560027\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (32.8505630493,25.3208053805), test loss: 29.896061635\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.06457448006,2.35903309142), test loss: 4.02402280867\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (32.9818229675,25.3059216031), test loss: 30.8370388508\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.93885421753,2.35817234252), test loss: 3.94508075118\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (18.9867649078,25.2893260323), test loss: 37.0046766758\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.74230480194,2.3574514217), test loss: 2.98652743548\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (39.8693237305,25.2714752671), test loss: 29.8795552731\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.81949961185,2.3565974797), test loss: 4.09384925365\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.044878006,25.2569098367), test loss: 31.7338712931\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.45446848869,2.35562866986), test loss: 2.87859388888\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (21.221326828,25.2426883764), test loss: 31.1342274427\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.41359865665,2.35423636348), test loss: 3.95887014866\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (32.4404716492,25.2295408498), test loss: 32.4668130875\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (6.42995405197,2.35358214023), test loss: 3.1581125468\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (38.1326828003,25.214737562), test loss: 29.4395545244\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.87029504776,2.35230309964), test loss: 3.65957064629\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (23.2955913544,25.1966013135), test loss: 31.8481627464\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.75649142265,2.35133618294), test loss: 3.66359681487\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (28.4225730896,25.1809436839), test loss: 31.1471585751\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.15034663677,2.35039396686), test loss: 3.98878598213\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (3.12597727776,25.1666107543), test loss: 30.7556745529\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.08175826073,2.34956380281), test loss: 4.02135934234\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (11.3632574081,25.1501724949), test loss: 35.9593039513\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.96991533041,2.34873255619), test loss: 3.33013673723\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (48.4727172852,25.13328163), test loss: 31.7302191615\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.98087453842,2.34787070829), test loss: 3.85690003633\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (26.7611045837,25.1195071471), test loss: 41.1612066269\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.51389455795,2.34685437319), test loss: 2.8940959394\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (56.9926872253,25.1058583031), test loss: 28.5812375069\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.81290602684,2.34583538205), test loss: 4.04075084925\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (35.0895080566,25.0904978071), test loss: 33.1157208443\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.32475042343,2.34478791948), test loss: 3.37357649803\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (46.1723594666,25.0751635221), test loss: 31.2684233665\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.56251478195,2.34362068745), test loss: 3.80534792542\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (24.8559265137,25.0587920156), test loss: 31.808789587\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.933906197548,2.34272678818), test loss: 3.09868603647\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (31.7995834351,25.0444178132), test loss: 29.1787264109\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.96097230911,2.34189486269), test loss: 3.84366864562\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (18.7539787292,25.0287279993), test loss: 33.7346662521\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (4.74730539322,2.34114693774), test loss: 4.00971333981\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (40.9263153076,25.0115943844), test loss: 33.7951714993\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.81394314766,2.34025537681), test loss: 3.4308011502\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.7846069336,24.9973557152), test loss: 34.0816979408\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.64758515358,2.33930166691), test loss: 3.81961698532\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.0471973419,24.9836011989), test loss: 36.2814844131\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.38799130917,2.33808401022), test loss: 3.02098884881\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (35.3578414917,24.970360868), test loss: 29.674971199\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (5.87750434875,2.33740784104), test loss: 4.06483048797\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (35.8648796082,24.9556543081), test loss: 33.377612257\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.60660529137,2.33623118729), test loss: 3.1863776207\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (20.8132286072,24.9383762685), test loss: 29.7951884985\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.68482351303,2.33527621785), test loss: 3.82476029396\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (26.8478126526,24.9235359064), test loss: 36.1962076187\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.00191795826,2.33436083777), test loss: 3.20025653243\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (2.78432393074,24.9101173324), test loss: 29.6600706339\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.875201880932,2.33356188139), test loss: 3.81130655408\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (8.70567417145,24.8935662234), test loss: 32.4008787155\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.949025213718,2.33273370614), test loss: 3.93485640287\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (47.67345047,24.878018389), test loss: 31.6639542818\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.9803404808,2.33190593359), test loss: 3.80095783472\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold4/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (41.4292907715,inf), test loss: 32.9991604328\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (4.4704875946,inf), test loss: 3.00299296826\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (20.0658340454,26.3995496359), test loss: 33.0051593065\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.50041770935,2.48379056233), test loss: 3.49160580933\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (79.4959869385,26.4742137764), test loss: 44.2358338356\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.44570541382,2.48476992159), test loss: 3.89709370732\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (13.6208190918,26.4242459118), test loss: 32.7305073261\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.70670843124,2.47241937064), test loss: 3.56485259235\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (38.1349105835,26.4713410567), test loss: 33.1183781385\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.24327945709,2.46714630545), test loss: 3.67864403725\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (10.6523370743,26.4623115554), test loss: 36.726601696\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.858941137791,2.46706005536), test loss: 2.84480990469\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (57.2794570923,26.4473468866), test loss: 33.5554500222\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.99983119965,2.4661263412), test loss: 3.70645135641\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (12.8455286026,26.4327269017), test loss: 29.8868815422\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.3182387352,2.46641338538), test loss: 2.73503181338\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (14.4060935974,26.4159352081), test loss: 37.8178845882\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.43645381927,2.46445330105), test loss: 3.6377340436\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (23.0299377441,26.4297100237), test loss: 42.3316599846\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.01332437992,2.46498849992), test loss: 3.29137373418\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (25.8195037842,26.3878610624), test loss: 32.9781757832\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.1522333622,2.46406627835), test loss: 3.28718798757\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (16.7652664185,26.3515329262), test loss: 33.8038584471\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.89694559574,2.46250958049), test loss: 3.7000109762\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (32.93201828,26.3544207773), test loss: 30.5292672157\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.16942071915,2.46041508933), test loss: 3.64872451425\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (16.5940551758,26.3326666061), test loss: 38.0504279375\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.24276709557,2.45882679284), test loss: 3.75945958495\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (13.6450033188,26.3329306669), test loss: 32.1320098877\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.04606378078,2.45875894293), test loss: 2.95844011158\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (39.3176231384,26.298467201), test loss: 34.6096957326\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.18333482742,2.45698931218), test loss: 3.64679603577\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (14.7467727661,26.2961083486), test loss: 35.6006229162\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.60129761696,2.45708206117), test loss: 2.94777675867\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (3.89730334282,26.2816074837), test loss: 30.8407544136\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.94183671474,2.45554614312), test loss: 3.63661938608\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (47.9719657898,26.258445797), test loss: 32.3652607918\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.22873783112,2.45495241038), test loss: 3.07602094412\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (30.6738090515,26.2371434757), test loss: 34.5300105095\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.99567890167,2.45365259078), test loss: 3.57909876108\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (37.6535568237,26.2224520679), test loss: 33.9871922493\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.29162287712,2.45211000849), test loss: 2.92660064846\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (30.3842163086,26.2123877486), test loss: 30.5746278286\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.74491405487,2.45122734839), test loss: 3.87279219031\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (37.9567832947,26.1980225531), test loss: 32.5953642368\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.50458812714,2.45013776777), test loss: 3.80146042109\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (11.8042793274,26.1820351386), test loss: 32.7668034434\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.1169230938,2.44913986449), test loss: 2.86330499649\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (26.530960083,26.1666117329), test loss: 36.3690541267\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.02203440666,2.44830008141), test loss: 3.75114969015\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (33.690700531,26.1539835299), test loss: 34.5581935883\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.254214465618,2.44707634269), test loss: 2.85675610602\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (29.911441803,26.1402095676), test loss: 35.6540485144\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.806374192238,2.44643391356), test loss: 3.82990241051\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (34.3704795837,26.1120682324), test loss: 31.7748815536\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.8476858139,2.44533902184), test loss: 3.22390395999\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (18.161447525,26.104345771), test loss: 32.7111655235\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.35383653641,2.44425077571), test loss: 3.47349247336\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (25.9097862244,26.0874884316), test loss: 34.8150874376\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.07500743866,2.44268469625), test loss: 3.11217255592\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (21.3967094421,26.0768892039), test loss: 29.3853412628\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.99967098236,2.44215006572), test loss: 3.45997117758\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (41.2668228149,26.0610227956), test loss: 33.3077162743\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.84801387787,2.44090990124), test loss: 3.91046687067\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (24.1329421997,26.0410824704), test loss: 31.537027061\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.3023443222,2.43991493345), test loss: 3.80336292982\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (28.7950248718,26.0336979496), test loss: 33.1791254759\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.41675686836,2.43922466988), test loss: 3.73803693056\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (57.1315612793,26.0125365243), test loss: 42.3779487133\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.08505129814,2.43824214002), test loss: 3.00468091667\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (48.1185531616,25.9996318938), test loss: 39.1398457527\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.867577195168,2.43769733481), test loss: 3.79771878719\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (8.91976833344,25.9766101853), test loss: 29.8812477827\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.37232375145,2.43626034269), test loss: 2.78075947762\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (21.8726520538,25.9664449668), test loss: 33.9332763433\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.17688894272,2.43476719231), test loss: 3.58348767757\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (18.6318645477,25.954346566), test loss: 34.2608079195\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.6983833313,2.43410590851), test loss: 3.0560441494\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (34.5301017761,25.9393771821), test loss: 30.0871175766\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.76562285423,2.43303068008), test loss: 3.47629437447\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (21.6142292023,25.9240426037), test loss: 33.2529554367\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.946956276894,2.43221635028), test loss: 3.74881987572\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (21.2879619598,25.9077719667), test loss: 29.0637199402\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.20802891254,2.43110259175), test loss: 3.82341965437\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (68.2988128662,25.9015584871), test loss: 33.0644090652\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (5.86814403534,2.43057327728), test loss: 3.96613472104\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (28.5633296967,25.8795348267), test loss: 31.3031948566\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.55354261398,2.42958524488), test loss: 2.86896253526\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (21.0641956329,25.8595099003), test loss: 34.0146178007\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.57137060165,2.42842783972), test loss: 3.66656689644\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (7.5673995018,25.8482997465), test loss: 35.2728065968\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.20257174969,2.42736623552), test loss: 2.79088277221\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (14.9829444885,25.8321426387), test loss: 31.2421552181\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.2129034996,2.42613460441), test loss: 3.90015462041\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (31.0879878998,25.8233081645), test loss: 34.5568519115\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.88777959347,2.42539484109), test loss: 3.10588880181\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (39.0392150879,25.8041434066), test loss: 32.9424018621\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.75770258904,2.42431479227), test loss: 3.4833578229\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (23.3973808289,25.7924710735), test loss: 36.4397818565\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.79877781868,2.42361426371), test loss: 3.54116818905\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.899980545,25.7776065094), test loss: 30.6159829378\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.80652403831,2.4225429496), test loss: 3.49681851864\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (42.3018188477,25.7614223597), test loss: 34.8274802208\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.79282283783,2.42174970392), test loss: 3.96599404216\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (21.7286758423,25.7461854579), test loss: 32.0340195656\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.864982068539,2.42079398209), test loss: 2.87131522\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (39.674747467,25.7310733694), test loss: 35.1626131296\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.01714849472,2.41952879235), test loss: 3.64581733346\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.1948661804,25.7184496334), test loss: 34.7300797939\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.42684936523,2.41870103978), test loss: 2.75708218664\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (24.7598800659,25.7046720126), test loss: 32.5180862188\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.662917673588,2.41772097197), test loss: 3.7059029758\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (11.8690214157,25.6909119459), test loss: 31.7894521475\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.33109688759,2.41678151118), test loss: 3.10902504027\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (30.3571548462,25.6764427932), test loss: 31.8216397643\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.932890117168,2.41585745525), test loss: 3.74477674663\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (29.6279373169,25.6627395192), test loss: 33.4630261421\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.22034692764,2.4148952876), test loss: 3.15998741239\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (40.2909164429,25.6502531685), test loss: 30.9473040581\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (5.64046669006,2.41421969647), test loss: 3.33058716655\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (93.0722503662,25.6320576058), test loss: 45.1401731491\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (7.15332221985,2.41328322614), test loss: 4.09168016016\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.2821483612,25.6185519118), test loss: 31.1026041508\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.72448611259,2.41227179215), test loss: 3.70745992661\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (31.915845871,25.6046929925), test loss: 31.4218332052\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.75578999519,2.41097888394), test loss: 3.75665504336\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (10.5762367249,25.5916690642), test loss: 34.9476711273\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.64422416687,2.41021151837), test loss: 2.87900767028\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (30.0343208313,25.5783013443), test loss: 31.8954980135\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.98672282696,2.40924444414), test loss: 3.71020827293\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (18.9519863129,25.5612577113), test loss: 30.7683103085\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.68109869957,2.40833716841), test loss: 2.80670056045\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (20.8764152527,25.5516211566), test loss: 36.9526638985\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.47145748138,2.40755248446), test loss: 3.63685376346\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (14.1933803558,25.5359363814), test loss: 36.8381002426\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.795503020287,2.40665091708), test loss: 3.25122735202\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (17.3894023895,25.5213722294), test loss: 36.9158194065\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.47030830383,2.40593865206), test loss: 3.34902662039\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (20.54778862,25.5040160691), test loss: 32.9108989239\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.12245881557,2.40479952349), test loss: 3.70943640172\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (43.7118225098,25.4929151997), test loss: 28.7968519449\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (7.19287586212,2.40377225494), test loss: 3.61011816561\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.3796463013,25.4800121378), test loss: 35.7630624771\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.840448379517,2.40283845989), test loss: 3.81458533853\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (11.070558548,25.4659759148), test loss: 31.247056067\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.13354170322,2.40197167706), test loss: 2.99463700652\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (37.9365844727,25.4540684559), test loss: 36.0733991861\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (4.12785053253,2.40117268741), test loss: 3.67061502934\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (9.4452791214,25.4378151426), test loss: 34.2117836475\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.2907166481,2.40025341866), test loss: 3.00286782831\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (4.37736701965,25.4286751029), test loss: 31.3033309221\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.30697250366,2.39945574262), test loss: 3.77650195658\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (41.9235076904,25.4121799927), test loss: 33.024114871\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.20313215256,2.39854518898), test loss: 3.15295116007\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.4042186737,25.3952589081), test loss: 32.7558083534\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.0369412899,2.39758611291), test loss: 3.4914984256\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (29.3260650635,25.3847136726), test loss: 38.0861599922\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.58703875542,2.39663935559), test loss: 3.09315443039\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (17.3346061707,25.3691034774), test loss: 29.7508290768\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.85808992386,2.39560917124), test loss: 4.06345470548\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold5/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (32.5331039429,inf), test loss: 36.1271865368\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.70001935959,inf), test loss: 2.87111891806\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (62.0936889648,25.5276921391), test loss: 35.9997071266\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.58262252808,2.48129194468), test loss: 3.58033272326\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (20.2895050049,25.3658766985), test loss: 37.7547712326\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.47389650345,2.47809515634), test loss: 3.35717356652\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (14.2051792145,25.538441409), test loss: 34.0719465017\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.10487794876,2.47949081696), test loss: 3.79612806439\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (37.7875938416,25.5408803865), test loss: 31.1953052044\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.12857210636,2.46926764381), test loss: 3.60766590238\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (13.2829627991,25.5212105905), test loss: 36.1775363207\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.309398680925,2.46580463176), test loss: 2.85350493193\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (8.05329418182,25.5043132902), test loss: 31.7793703556\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.1329638958,2.46614882819), test loss: 3.73481169343\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (13.5486593246,25.4494742006), test loss: 33.015977633\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.46668958664,2.46390286092), test loss: 2.85379412472\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (17.9211845398,25.4483211779), test loss: 38.4246688604\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.54170715809,2.46306781513), test loss: 3.49704811573\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (69.544960022,25.4484733039), test loss: 43.9918634415\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.79234910011,2.46395129091), test loss: 3.08591988087\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (18.2218055725,25.3882064874), test loss: 39.5087449551\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.41043567657,2.46233598873), test loss: 3.64536598623\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (23.8631649017,25.3910548399), test loss: 30.4123263836\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.19820332527,2.46266157254), test loss: 2.7952344656\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (14.8703107834,25.3927252151), test loss: 34.4791868448\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.897419810295,2.45917109535), test loss: 3.82048404813\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (18.8612060547,25.3719075862), test loss: 37.2269267559\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.36171460152,2.45796192319), test loss: 3.45017146468\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (14.5718812943,25.3723286401), test loss: 35.1725925922\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.32635641098,2.4577636209), test loss: 3.19986243546\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (39.624130249,25.3311539998), test loss: 43.2220838547\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.805811882019,2.4555665017), test loss: 3.65450566411\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (18.9735412598,25.3107845429), test loss: 36.260934186\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (5.28323364258,2.45451532764), test loss: 2.90224576294\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (9.86544704437,25.3146107977), test loss: 29.6670188189\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.851166009903,2.45452771345), test loss: 3.57063183784\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (10.759226799,25.2763234681), test loss: 32.8621333361\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.924028992653,2.45302438125), test loss: 2.79734659791\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (28.0408554077,25.2598439874), test loss: 32.5803469896\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.7831902504,2.45226021482), test loss: 3.4860217452\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (7.81335687637,25.2575231904), test loss: 35.6788086891\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.00069475174,2.45148029258), test loss: 2.74431940317\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (18.4308757782,25.2402239162), test loss: 35.4518632412\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.13068842888,2.44929867681), test loss: 3.70778116584\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (24.0393867493,25.23720549), test loss: 31.4095406294\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.19493865967,2.44881702897), test loss: 3.49163558781\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (56.3005104065,25.2113732826), test loss: 38.7858840466\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.16185355186,2.44741902977), test loss: 3.74162970781\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (18.0286979675,25.1854656938), test loss: 34.9769060135\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.86658287048,2.4461970781), test loss: 3.89371909499\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (33.7375335693,25.1821152444), test loss: 37.5715735912\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.18128418922,2.4457303847), test loss: 3.10701285005\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (40.3918952942,25.1573800832), test loss: 32.2059090614\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.9337644577,2.4445133573), test loss: 3.67304825783\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (13.3418283463,25.1350676684), test loss: 38.0483109951\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.97077703476,2.44365501666), test loss: 2.82074198723\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (23.7667655945,25.1339394616), test loss: 31.62091012\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.66036939621,2.44309086516), test loss: 3.59362646341\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (8.13649940491,25.1134133845), test loss: 32.3514563084\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.29713153839,2.44144310148), test loss: 2.91819705367\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (22.9656562805,25.1067112799), test loss: 32.6190679312\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.878131270409,2.44029290158), test loss: 3.48893710375\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (24.2652454376,25.0856749302), test loss: 31.5252198696\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.75998735428,2.43927892636), test loss: 3.35709130466\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (45.0801925659,25.0648331723), test loss: 35.9086639166\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.22205209732,2.43808515505), test loss: 4.13979830146\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (41.1317901611,25.0560982906), test loss: 30.2465527534\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.86297893524,2.43741843842), test loss: 3.67615423799\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (57.5789222717,25.0361956231), test loss: 39.3569410801\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.34320211411,2.43651941924), test loss: 3.25115962029\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (20.1622962952,25.0118253453), test loss: 35.1447988033\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.34795188904,2.43568330422), test loss: 3.65427479148\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (13.0680923462,25.0072251761), test loss: 36.7057744026\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.06156945229,2.43509220847), test loss: 3.00770941377\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (31.4769859314,24.993725054), test loss: 30.7512628555\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.07241785526,2.43339985778), test loss: 3.48427965045\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (13.3190135956,24.9789877467), test loss: 31.6767411709\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.326862752438,2.43216990635), test loss: 2.80194467008\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (7.68133449554,24.9641771225), test loss: 34.4266071558\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.09807133675,2.43138611622), test loss: 3.67143694162\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (12.9741811752,24.9429768743), test loss: 30.6865834713\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.3633980751,2.43021454782), test loss: 2.70939424038\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (17.6962089539,24.9306466839), test loss: 41.1183036089\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.54800832272,2.42931053391), test loss: 3.70491712391\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (70.7133636475,24.9188163606), test loss: 37.9173128605\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.81324422359,2.4287544808), test loss: 3.88970090747\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (17.1350364685,24.8938180839), test loss: 40.8313406467\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.32817387581,2.42768993756), test loss: 3.24528933167\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (23.0807876587,24.8835595963), test loss: 36.6243598461\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.13115644455,2.42709966777), test loss: 3.5489905864\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (14.4857950211,24.8727186145), test loss: 36.3450051546\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.88370847702,2.42553554666), test loss: 2.73469148576\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (17.12059021,24.8568722616), test loss: 35.494224143\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.39001262188,2.42455712402), test loss: 3.50361474752\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (14.1092891693,24.8470236138), test loss: 33.4299057961\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.2144973278,2.42385015476), test loss: 2.9367328465\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (38.2998847961,24.8250048832), test loss: 39.612809515\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.80149936676,2.4225510546), test loss: 3.52496708632\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (19.4612407684,24.8088750312), test loss: 31.8462756634\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.94314527512,2.42159844505), test loss: 3.02601183057\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (9.29198169708,24.800568583), test loss: 32.9381465197\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.828419923782,2.42101674936), test loss: 3.7022452414\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (9.974193573,24.7781415588), test loss: 31.4439642906\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.954190135002,2.41991745772), test loss: 3.31118967533\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (26.7495880127,24.7635903013), test loss: 36.0116009712\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.75254583359,2.41906843175), test loss: 3.80880607963\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (7.34920310974,24.7536424856), test loss: 31.5482066154\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.92123091221,2.41821437579), test loss: 3.63571411967\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (17.4918212891,24.7386783142), test loss: 39.1421164036\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.13705170155,2.41685030125), test loss: 2.94481082559\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (23.758146286,24.7295209015), test loss: 32.3306948662\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.17038726807,2.41612096215), test loss: 3.59543474913\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (51.5874671936,24.7109970174), test loss: 33.303613925\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.0831515789,2.41502516278), test loss: 2.82446057796\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (17.4995880127,24.6925739004), test loss: 33.6544052124\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.79543709755,2.41400300955), test loss: 4.05064339042\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (32.2526397705,24.6833687383), test loss: 32.430056572\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.18813300133,2.41331095615), test loss: 2.86629106104\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (38.5586929321,24.6649652699), test loss: 33.1154811382\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.9322283268,2.41229257731), test loss: 3.58065550923\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (12.8678245544,24.647842525), test loss: 37.8301593781\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.79260182381,2.41141918111), test loss: 3.37419603467\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (23.4552669525,24.639979824), test loss: 34.9627356768\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.63161468506,2.41068227415), test loss: 3.87798776627\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (8.74837303162,24.6234065903), test loss: 30.6067235231\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.13137292862,2.40946565557), test loss: 3.7126814574\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (22.5856151581,24.613518845), test loss: 34.9892589808\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.909673392773,2.40847953717), test loss: 3.15738818645\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (22.7054672241,24.5967272307), test loss: 31.8663181782\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.70304536819,2.40754433824), test loss: 3.64342842996\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (43.8052215576,24.5801092466), test loss: 39.6644130468\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.18292653561,2.40652396674), test loss: 3.21299276948\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (39.9922714233,24.5693616488), test loss: 28.3717425585\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (4.7510137558,2.40577032109), test loss: 3.53154805899\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (53.1994628906,24.5528373415), test loss: 40.9942399025\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (5.10694122314,2.40489851617), test loss: 3.03417388797\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (20.1917552948,24.5343418881), test loss: 35.7057409286\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.27294540405,2.40405672718), test loss: 3.51418583989\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (12.2598342896,24.5258551761), test loss: 31.3200292587\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.02425837517,2.40333851351), test loss: 2.89331571162\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (26.8306045532,24.5125343594), test loss: 33.3132353783\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.03741061687,2.40205934161), test loss: 3.65923870802\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.2269802094,24.4990256446), test loss: 30.1892351151\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.344809353352,2.40102362171), test loss: 3.52566533685\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (7.35655879974,24.4854105989), test loss: 36.0415984631\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.05060958862,2.40021770955), test loss: 3.30285543203\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.5919818878,24.4682753038), test loss: 34.3892615795\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.27925252914,2.39919889132), test loss: 3.54934717417\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (17.6940460205,24.456068989), test loss: 45.493423748\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.55088925362,2.39834500991), test loss: 2.96965551972\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (71.342048645,24.443934705), test loss: 37.5666353464\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.82565736771,2.39766520605), test loss: 3.82919549942\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (16.1650505066,24.4244812849), test loss: 43.3707478523\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.25039768219,2.39670731527), test loss: 3.02203053832\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (22.2620391846,24.4136273281), test loss: 33.3490396023\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.08031582832,2.3960223993), test loss: 3.56126789451\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (13.9409275055,24.4019927909), test loss: 33.4554678917\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.874287903309,2.394776369), test loss: 2.85690406263\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (15.8606748581,24.3877892091), test loss: 38.0846082687\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.42308878899,2.39387639042), test loss: 3.42650513053\n",
      "run time for single CV loop: 1274.26010799\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold1/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (32.5082015991,inf), test loss: 34.7906497478\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.27716732025,inf), test loss: 3.38260190189\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (68.7562408447,26.4861468291), test loss: 36.388361311\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.79072856903,2.41994636792), test loss: 3.03146150708\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (97.2197570801,26.4686038802), test loss: 44.4824846268\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (7.3715596199,2.41879074475), test loss: 3.11020278931\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (11.7065906525,26.4691656308), test loss: 32.6129582882\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.93667793274,2.41480218822), test loss: 3.34289243817\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.5417327881,26.4834745317), test loss: 32.6452428579\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.61929035187,2.40621480107), test loss: 3.11413283348\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (30.4141292572,26.489613285), test loss: 35.5928659439\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.17388248444,2.40534864618), test loss: 3.04732574821\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (34.6917610168,26.4961968271), test loss: 33.9080711603\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.47982811928,2.40419978145), test loss: 3.25046405196\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (23.4983901978,26.4426938754), test loss: 33.2943588257\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.06288766861,2.40173712661), test loss: 3.03536126912\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (34.2941894531,26.4596586013), test loss: 31.3234659195\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.55998420715,2.40159581444), test loss: 3.17136534452\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (64.3843612671,26.4565069927), test loss: 37.494609642\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.51019287109,2.40199204186), test loss: 3.24629166424\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (38.0408058167,26.4130107155), test loss: 32.5522947311\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.40045881271,2.40116348446), test loss: 3.0740673393\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (18.9368896484,26.3884541044), test loss: 33.9815214157\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.76009702682,2.40063272894), test loss: 2.95236915052\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (9.47890472412,26.3819163962), test loss: 35.6701015472\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.15286135674,2.39917239598), test loss: 3.38364233375\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (12.699719429,26.3679060535), test loss: 34.1510926008\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.1697602272,2.39768052329), test loss: 3.34933865666\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (22.6299781799,26.368655629), test loss: 34.9053617001\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.62577724457,2.39695097026), test loss: 3.28611326814\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (20.6253738403,26.3485260225), test loss: 35.0675742626\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.522700786591,2.39535467985), test loss: 3.05574088693\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (20.1038208008,26.329157423), test loss: 39.4935921192\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.26766610146,2.39470620177), test loss: 3.27895315289\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (37.0691490173,26.3263641421), test loss: 30.4535509586\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.53704810143,2.39436854305), test loss: 3.11150864661\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (26.6861400604,26.2998763304), test loss: 36.1623786926\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.27689826488,2.39337739737), test loss: 3.19745915234\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (22.1435012817,26.2733057278), test loss: 33.3554820776\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.58954668045,2.39266857655), test loss: 3.05168590546\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (17.793182373,26.2704732853), test loss: 34.3101324558\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.1297249794,2.39193156036), test loss: 2.99580960274\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (29.490901947,26.2543654554), test loss: 35.6888805866\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.93196368217,2.3907303565), test loss: 3.29591715336\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (6.84046649933,26.2419309261), test loss: 32.6323566437\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.26981472969,2.38971806877), test loss: 3.2431466639\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (21.1920127869,26.2321419642), test loss: 37.2467019081\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.84831166267,2.38856303516), test loss: 3.43177921474\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (18.7654762268,26.2088749052), test loss: 36.9941405296\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.95649552345,2.38734569881), test loss: 2.98638023138\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (18.0489578247,26.1998258962), test loss: 39.3003372192\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.85988020897,2.38689698071), test loss: 3.04588941932\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (55.2264900208,26.1862620012), test loss: 36.6943043232\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.48092150688,2.3864433715), test loss: 3.15208677948\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (20.2912425995,26.1633999671), test loss: 48.259367466\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.66139423847,2.3855986203), test loss: 3.28951715529\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (24.765750885,26.1444314375), test loss: 32.7483205318\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.29495310783,2.38490187585), test loss: 3.15160054564\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (10.317899704,26.1342787556), test loss: 32.6065850258\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.717837452888,2.38343803025), test loss: 3.19504899383\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (13.9420080185,26.1196707515), test loss: 36.2193357468\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.894993126392,2.38275758552), test loss: 3.08637500405\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (19.253074646,26.1087276458), test loss: 32.7555208683\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.94593703747,2.38189112814), test loss: 3.07379492819\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (27.1101455688,26.090068481), test loss: 34.0080360889\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.84925746918,2.38087170495), test loss: 3.42250362039\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (10.5038633347,26.0747938676), test loss: 33.2418406487\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.82621049881,2.37992442849), test loss: 3.15123282075\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (11.3855514526,26.0637028413), test loss: 37.0776918411\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.38794183731,2.37927121392), test loss: 3.23072735965\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (16.5907936096,26.0444853142), test loss: 31.8733422279\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.06625294685,2.37842586018), test loss: 3.17699150741\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (48.1798744202,26.0238860189), test loss: 37.0453396797\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.26016902924,2.37758844612), test loss: 3.26528692245\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (31.4452209473,26.0161355226), test loss: 30.9291266441\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.822804689407,2.37664852569), test loss: 3.0749550283\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (13.5937395096,25.9989446798), test loss: 34.9357644558\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.55633401871,2.37587305511), test loss: 3.1459004581\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (20.1698799133,25.9855068295), test loss: 32.1569551945\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.05773699284,2.37494742787), test loss: 2.94242933393\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (12.6895389557,25.9715030732), test loss: 31.5492250443\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.01202297211,2.37383167973), test loss: 2.98312112838\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (38.1847076416,25.9559035305), test loss: 36.510930109\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.42989754677,2.3728410784), test loss: 3.61857971251\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (29.0135765076,25.9434871432), test loss: 32.4191104412\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.45122170448,2.37210760883), test loss: 3.22543534338\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (24.2698879242,25.927447324), test loss: 38.9655605793\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.48248386383,2.37171650216), test loss: 3.26738301665\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (70.7657623291,25.9098664383), test loss: 44.8457357407\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.93592453003,2.37096488328), test loss: 3.22838319242\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (4.88684844971,25.8941266836), test loss: 37.888070941\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.14632415771,2.37014379388), test loss: 3.12525603771\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (11.110414505,25.8834801822), test loss: 31.3534115314\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.25359606743,2.36904893214), test loss: 3.12929750085\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (35.4172935486,25.8698150222), test loss: 35.6463470459\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.62615060806,2.36827568236), test loss: 3.32723766267\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (13.4391899109,25.8560805976), test loss: 32.874348259\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.79652202129,2.36733285338), test loss: 2.95351225138\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (9.55048465729,25.8404111894), test loss: 34.5283519745\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.4443962574,2.366478291), test loss: 3.0608713448\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (23.6956939697,25.8278175096), test loss: 32.957521534\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.12505292892,2.36556154173), test loss: 3.24234627485\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (40.8630332947,25.8163140823), test loss: 32.0618806839\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.09962105751,2.36515061252), test loss: 3.40052988529\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (16.4552764893,25.7982311247), test loss: 36.2619250774\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.485598027706,2.36434491915), test loss: 3.36576963067\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (27.3424835205,25.7797605941), test loss: 34.5961218119\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.1363067627,2.3634636884), test loss: 2.99212564975\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (37.7834510803,25.7695820091), test loss: 44.3879845619\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.00702881813,2.36267841313), test loss: 3.17383393645\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.7746086121,25.756847706), test loss: 30.9080086231\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.22645103931,2.36181113142), test loss: 3.08056890965\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (17.7344532013,25.7427565936), test loss: 36.1356372356\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (6.29491043091,2.36101930169), test loss: 3.40252383649\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (11.3810157776,25.7294411715), test loss: 32.878205657\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.944746255875,2.36000242267), test loss: 3.15548315644\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (26.0972003937,25.7150876675), test loss: 39.5215107918\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.06862688065,2.35925306102), test loss: 3.23477574438\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (37.3981742859,25.7021083324), test loss: 32.9940773726\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.30852937698,2.3584621928), test loss: 3.33529052436\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (19.9403800964,25.6873370814), test loss: 31.8899650097\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.644314646721,2.35779891133), test loss: 3.02407866418\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (43.570728302,25.6704064669), test loss: 34.6637444496\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.69032001495,2.35710289655), test loss: 3.41861638129\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (12.9857940674,25.6577497631), test loss: 36.451376605\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.4236137867,2.35621094986), test loss: 3.07129015028\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (26.4670944214,25.645945285), test loss: 39.219595933\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.99388873577,2.35540734176), test loss: 3.01498501301\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.5497074127,25.6331283426), test loss: 31.0259209156\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.16516828537,2.35456432449), test loss: 3.1008428216\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (9.23669528961,25.6180206137), test loss: 39.6182673931\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.52676486969,2.35362218783), test loss: 3.54582574666\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (20.830909729,25.604468146), test loss: 32.3720685244\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.45904946327,2.35278179055), test loss: 2.97686995268\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (17.6920394897,25.5913546016), test loss: 39.1582519293\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.925931572914,2.3519933729), test loss: 3.32520486712\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (25.5543441772,25.577792756), test loss: 34.5938535213\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.476897448301,2.35139225787), test loss: 2.97948573232\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (18.0750617981,25.561645729), test loss: 43.374047184\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.49271297455,2.35074113467), test loss: 3.11141000986\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (15.1853160858,25.5453815741), test loss: 32.5802866459\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.17095112801,2.34984378145), test loss: 3.37535987496\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.8619785309,25.5334520256), test loss: 32.2609324455\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.8800983429,2.34889403806), test loss: 3.12271925807\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (15.4840211868,25.5214299106), test loss: 39.5466475248\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.66499966383,2.34821366118), test loss: 2.91601548791\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (19.4028167725,25.5078209304), test loss: 34.4173738003\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.12466287613,2.34731151217), test loss: 3.15960848331\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (36.1092910767,25.4935669091), test loss: 33.6885052681\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.81212663651,2.34652614361), test loss: 3.13692016006\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.8614368439,25.4805312512), test loss: 33.6088457108\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.19287657738,2.34574370491), test loss: 3.16125358641\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.4221668243,25.4667888934), test loss: 35.7915225983\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.48187565804,2.34494996566), test loss: 3.51711247563\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (21.1670246124,25.4525578659), test loss: 33.2139282227\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.16816616058,2.34428330124), test loss: 2.9796851337\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (43.3644332886,25.4375286344), test loss: 36.3631724834\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.56724214554,2.34351838091), test loss: 2.97615612149\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (35.2412490845,25.4254620094), test loss: 33.2883755207\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.966786623001,2.34261498103), test loss: 3.40600404143\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold2/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (38.0894241333,inf), test loss: 30.5980803013\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.70154690742,inf), test loss: 2.64774388969\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (23.7028656006,27.2288174353), test loss: 34.8956368923\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.605305671692,2.49971223354), test loss: 3.48657580018\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (36.7226791382,27.194759536), test loss: 29.4034458876\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.3848862648,2.50424225843), test loss: 3.13753240108\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (4.69546031952,27.2236127284), test loss: 33.3354027748\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.81232666969,2.49989029938), test loss: 3.58692984581\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (4.88536739349,27.2291203346), test loss: 29.1094018459\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.68387269974,2.49807261993), test loss: 3.48733665049\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (25.4223613739,27.2553979744), test loss: 37.2517814279\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.11066675186,2.49801940358), test loss: 2.92561160624\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (7.34937810898,27.2023429089), test loss: 32.5153849125\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.45301008224,2.49237060298), test loss: 3.65308139324\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (24.7213058472,27.1993752654), test loss: 37.004197073\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.7329185009,2.49342524271), test loss: 2.79532181174\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (19.8447418213,27.1895699029), test loss: 30.8714911938\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.77393853664,2.49153384055), test loss: 3.70343304276\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (13.1392498016,27.1560463324), test loss: 31.6446967602\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.26685190201,2.49062965678), test loss: 2.62734562159\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (54.0462188721,27.1363121036), test loss: 35.7527072191\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.10849285126,2.49006314543), test loss: 3.70594416857\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (32.6839904785,27.12530208), test loss: 38.4539755106\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.63963270187,2.48932701739), test loss: 2.46833606362\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (15.1145305634,27.116541089), test loss: 35.0031282902\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.09784221649,2.48765113366), test loss: 3.54493108392\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (21.9615631104,27.10483761), test loss: 29.3198469162\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.10776758194,2.4868526525), test loss: 3.16726208627\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (53.6700439453,27.0761436917), test loss: 36.9807451963\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.99802231789,2.48529417488), test loss: 3.48292851895\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (6.45331048965,27.0649238098), test loss: 28.5283982754\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (5.68127441406,2.48453697884), test loss: 3.73561408073\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (27.7127895355,27.0574772256), test loss: 34.9929608822\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.47070407867,2.4838562123), test loss: 2.82426556349\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (30.4411277771,27.0221894), test loss: 33.2847072124\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.92861199379,2.48263079094), test loss: 3.75693864226\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (30.6984024048,27.0010478732), test loss: 33.9892403603\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.96605604887,2.48184408084), test loss: 2.47878158391\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (33.6797904968,26.9893889811), test loss: 33.3160574198\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (5.89391803741,2.48050704117), test loss: 3.7371665895\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (20.8125457764,26.9798561468), test loss: 34.6652773142\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.5679320097,2.4797241273), test loss: 2.57823284864\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (28.7933082581,26.9649392128), test loss: 34.4890074253\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.61352682114,2.47850809469), test loss: 3.52359502316\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (29.6714820862,26.9381463811), test loss: 29.7340680599\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.46971988678,2.47721542243), test loss: 2.53321064711\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (17.3438873291,26.9302513376), test loss: 38.9150167465\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.75703263283,2.47625372756), test loss: 3.56987438202\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (55.4920806885,26.9139020688), test loss: 33.5005704403\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.24179959297,2.47560660016), test loss: 3.39057293534\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (14.6218204498,26.8871301744), test loss: 36.2531817913\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.79013478756,2.47489946578), test loss: 3.09841046631\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (13.1847438812,26.8724921273), test loss: 30.0319152355\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.55031371117,2.47407505177), test loss: 3.54206681848\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (31.6858348846,26.8597471235), test loss: 35.4183158398\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.60352361202,2.4726000483), test loss: 2.6774664551\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (18.0974025726,26.8475313494), test loss: 32.6795376301\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.02066683769,2.4717433016), test loss: 3.7234801352\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (50.2616195679,26.831705706), test loss: 32.5793843269\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.87525892258,2.47061252285), test loss: 2.78894531429\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (16.5287399292,26.8109952004), test loss: 32.3712601185\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.70530200005,2.46966851017), test loss: 3.72038986087\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (48.1853179932,26.7992855226), test loss: 31.0891112804\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (4.36966228485,2.4685282297), test loss: 2.64420272708\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (35.7374649048,26.7793978351), test loss: 36.4448018551\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.83739387989,2.46773580433), test loss: 3.5408313334\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (42.4777832031,26.7565875892), test loss: 30.5227675915\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.49669599533,2.46700027999), test loss: 3.18785372972\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (14.5116767883,26.7443305355), test loss: 33.3562614918\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.7450671196,2.46618369458), test loss: 3.57856622636\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (4.47377204895,26.7286304976), test loss: 28.6232557774\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.84190678596,2.46515230316), test loss: 3.42327450514\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (32.2745552063,26.7168169782), test loss: 35.0270878792\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.71007633209,2.46423236656), test loss: 2.99374527633\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (7.68232631683,26.6974203299), test loss: 34.6448191643\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.77271878719,2.4629411742), test loss: 3.58344032764\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (17.8658599854,26.6832144159), test loss: 36.5747105122\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.55198454857,2.46228248536), test loss: 2.72475745082\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (33.4746398926,26.6675857037), test loss: 30.8035464287\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.04958868027,2.46104356787), test loss: 3.82221725583\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (17.6633586884,26.6506996461), test loss: 31.8202125072\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.15932750702,2.46037781872), test loss: 2.66731351018\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (48.1872406006,26.6324811784), test loss: 32.9518728256\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.9042327404,2.45944095793), test loss: 3.68038642406\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (35.4510574341,26.6179273897), test loss: 36.9836271286\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.28884005547,2.45852307983), test loss: 2.57788265049\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (17.4915599823,26.6054444833), test loss: 35.43766222\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.05937087536,2.45757330555), test loss: 3.49864439666\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (40.9643440247,26.5907890194), test loss: 31.7995779037\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.05182218552,2.45656825874), test loss: 3.0552475661\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (53.140335083,26.5723841397), test loss: 37.4991988659\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.34418344498,2.45555708383), test loss: 3.48205227554\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (9.31051921844,26.5585783405), test loss: 29.0201396465\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (7.28584241867,2.45477339098), test loss: 3.89011020064\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (31.8294620514,26.5442421988), test loss: 36.7359925747\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.38685822487,2.45379733213), test loss: 3.20295008719\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (13.7105960846,26.5254556876), test loss: 32.3221932888\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.42400765419,2.4529812767), test loss: 3.76756176949\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (23.6023025513,26.5076492532), test loss: 36.5238709927\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.35818743706,2.45202610197), test loss: 2.46236240268\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (37.4421386719,26.4925226919), test loss: 35.8130412817\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (6.20956134796,2.45092700595), test loss: 3.81436365247\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (19.2250919342,26.4811306951), test loss: 36.3309566021\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.29800283909,2.45027491829), test loss: 2.61902785897\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (19.5180244446,26.4652951119), test loss: 34.7736212254\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.11730790138,2.4490877481), test loss: 3.54665477276\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (25.688167572,26.4467477135), test loss: 30.3590261459\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.36630380154,2.44805713692), test loss: 2.53593437672\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (19.8729438782,26.4351259137), test loss: 39.834974575\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.63186168671,2.44720219903), test loss: 3.55125448704\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (59.0936012268,26.4180318922), test loss: 31.8279391766\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.9806137085,2.44637483553), test loss: 3.3179149121\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (26.3844490051,26.4018521958), test loss: 37.7795869827\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.23835849762,2.4456646552), test loss: 3.53330918849\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (12.8931770325,26.3856550683), test loss: 28.2754068851\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.97010564804,2.44478778184), test loss: 3.63700706065\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (12.5307703018,26.3704727822), test loss: 35.8918875694\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.83852899075,2.44351200057), test loss: 2.76997997761\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (17.5453147888,26.3597678867), test loss: 32.7757996082\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.535420417786,2.44286350471), test loss: 3.73861913681\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (35.0187301636,26.3432126605), test loss: 34.1585820198\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.08491587639,2.44173637594), test loss: 2.76220099032\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (16.8045158386,26.3266365055), test loss: 32.7236079693\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.87134945393,2.4408019857), test loss: 3.72509804964\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (37.2447166443,26.3159291277), test loss: 31.257834053\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (4.14607429504,2.43997229143), test loss: 2.59112595916\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (23.2907657623,26.2968981395), test loss: 37.3067847013\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.797602415085,2.43902838851), test loss: 3.54010709226\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (32.7826461792,26.2798397219), test loss: 30.3156587601\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.57615017891,2.43826185539), test loss: 2.77207562029\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (14.2296524048,26.2674079616), test loss: 34.8722901821\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.67203807831,2.43746831722), test loss: 3.5520934999\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (8.78598976135,26.2510995527), test loss: 27.87121768\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.77354645729,2.43641021147), test loss: 3.35439297706\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (21.7103099823,26.2396632603), test loss: 34.0759549856\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.28722763062,2.43566266935), test loss: 2.93321123123\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (37.2965011597,26.2232157278), test loss: 32.7883719206\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.23424482346,2.43453871492), test loss: 3.65048735142\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (20.5684051514,26.2071955355), test loss: 36.6890868187\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.39032936096,2.43370870065), test loss: 2.75935281217\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (34.7183303833,26.1949115664), test loss: 33.7422800541\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.44299769402,2.43279611459), test loss: 3.95103493333\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (19.9385681152,26.1785649271), test loss: 31.7256883621\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.75603151321,2.43196140212), test loss: 2.70592186451\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (44.6660766602,26.1620076489), test loss: 31.9532043934\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (4.13578224182,2.43106397134), test loss: 3.54040521383\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (40.1788520813,26.1493675692), test loss: 31.8429701567\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.631021261215,2.43027904669), test loss: 2.60203840733\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (15.0850028992,26.1350794084), test loss: 35.0612341404\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.32265806198,2.42929315291), test loss: 3.51686777472\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (31.2080307007,26.1219368292), test loss: 31.5672235489\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.95412826538,2.42840242103), test loss: 3.06266025901\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (45.3157958984,26.1061139092), test loss: 35.6496562481\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.96094548702,2.42748906959), test loss: 3.58161502481\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (8.40566253662,26.0907653418), test loss: 29.5460922718\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.79323983192,2.42658233715), test loss: 3.77918086946\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (12.3725624084,26.0793701574), test loss: 35.2537060976\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.6459530592,2.42579571299), test loss: 3.12388156056\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (27.9251117706,26.0624170524), test loss: 32.2604179859\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.863956213,2.42496485944), test loss: 3.65402015746\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold3/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (50.7121200562,inf), test loss: 33.2127252102\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.15931606293,inf), test loss: 3.00640460253\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (24.2287864685,27.3248679256), test loss: 32.8269558907\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.63266038895,2.51140446764), test loss: 3.72490897775\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (101.551651001,27.2518925371), test loss: 34.6599600315\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.84165477753,2.50670389208), test loss: 3.64668028653\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (16.7969932556,27.2265832485), test loss: 31.1487148046\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.7562353611,2.5032200369), test loss: 3.65834304392\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (23.8468437195,27.2292320026), test loss: 31.2499641418\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.39743471146,2.49301178925), test loss: 3.53413428068\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (27.1099967957,27.2260964359), test loss: 35.9438668728\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (8.15905094147,2.49533840146), test loss: 2.80398319811\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (30.3747520447,27.2157506595), test loss: 31.2538183689\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (5.98228025436,2.49154094253), test loss: 4.04122079015\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (44.5901031494,27.1797964212), test loss: 32.0713378429\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.89467906952,2.49030895434), test loss: 3.03705808222\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (27.5532112122,27.1664967886), test loss: 31.5650790215\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.3449409008,2.48956305531), test loss: 3.96322576404\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (5.04182910919,27.1506392307), test loss: 31.9380383015\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.64437508583,2.48890915028), test loss: 3.15306505561\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (12.9672956467,27.120565361), test loss: 29.59518013\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.959947109222,2.48832655539), test loss: 3.61897121072\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (37.9616470337,27.0943888175), test loss: 30.677089119\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.79074311256,2.48785311983), test loss: 3.43723418564\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (29.9124011993,27.0936640467), test loss: 34.5707843304\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.54510974884,2.4865917051), test loss: 3.82219638824\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (60.1494674683,27.0788460217), test loss: 30.3643070221\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.22479891777,2.48510606749), test loss: 3.86481004506\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (28.8397064209,27.0615278092), test loss: 34.7270442724\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.45581436157,2.48413751102), test loss: 2.9381758064\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (42.2084846497,27.0411950175), test loss: 30.7299256325\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.32039403915,2.48222229756), test loss: 3.78449771404\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (41.1089897156,27.0217588674), test loss: 31.1644521236\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.84966170788,2.48152828699), test loss: 2.87253021449\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (45.4291610718,27.0074992566), test loss: 31.3576250076\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.65261077881,2.48074598196), test loss: 3.70630387366\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (25.3784790039,26.9883969138), test loss: 34.7350021839\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (5.0585193634,2.48055359838), test loss: 2.9668571651\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (76.0549163818,26.9650665011), test loss: 34.15287323\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.43577075005,2.47992623915), test loss: 3.4953768909\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (15.8613357544,26.9484933649), test loss: 30.8564958572\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.49895238876,2.47914218917), test loss: 3.37785505056\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (25.0840892792,26.9348851847), test loss: 29.988882637\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.1616666317,2.47687479923), test loss: 3.81626222432\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (27.3224697113,26.9198971588), test loss: 31.5584425211\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (8.01177978516,2.47673024935), test loss: 3.68431335092\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (32.6582489014,26.9072391745), test loss: 33.3225335121\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (5.28019666672,2.47512677667), test loss: 3.30759964585\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (35.4885482788,26.8835623252), test loss: 33.7957417965\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.95262956619,2.47413041354), test loss: 3.67230389714\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (28.4062862396,26.8682355194), test loss: 34.9768491268\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.31479942799,2.47328317833), test loss: 2.8493450284\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (4.19421243668,26.8523547495), test loss: 29.154013443\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.57028520107,2.47258138607), test loss: 3.71831656694\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (13.1215543747,26.8312772195), test loss: 32.4542517662\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.968141674995,2.47177922665), test loss: 3.09459211528\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (43.3217468262,26.8118579453), test loss: 32.1460543633\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.28016972542,2.47108658747), test loss: 3.66503919363\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (27.8868331909,26.8004684338), test loss: 38.0035923481\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.72324323654,2.47003017195), test loss: 2.92418877035\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (61.3120918274,26.7847583589), test loss: 29.5596467972\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.19433403015,2.4689459815), test loss: 4.04696389437\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (23.345790863,26.7685609405), test loss: 30.00446558\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.46325516701,2.46805303747), test loss: 3.79051062167\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (49.7669219971,26.7505788321), test loss: 33.5459988117\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.05703473091,2.46658619462), test loss: 3.21295207888\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (41.615146637,26.7337278613), test loss: 34.4188214302\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.76134634018,2.46583073469), test loss: 3.69544096589\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (39.9476699829,26.7174633853), test loss: 34.1890820026\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.58565759659,2.46498467734), test loss: 2.73662228882\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (20.0468826294,26.6993624408), test loss: 33.5781979561\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.7330160141,2.46446303246), test loss: 3.7911190927\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (51.0294113159,26.6794318301), test loss: 33.5426251888\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.92521643639,2.46374521346), test loss: 3.09053904414\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (14.935459137,26.6641750177), test loss: 30.3464101791\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.9305062294,2.46284058391), test loss: 3.65867400169\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (26.4097061157,26.6496578972), test loss: 33.4618414879\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.02467155457,2.46126731811), test loss: 3.17794710994\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (28.7970409393,26.6351116413), test loss: 30.420068264\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (7.09010887146,2.46082520113), test loss: 3.76018688679\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (35.4229888916,26.62082954), test loss: 29.9829241276\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.1438794136,2.45957022469), test loss: 3.79820837975\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (28.7444248199,26.600435315), test loss: 31.6116974354\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.90585374832,2.4586562379), test loss: 3.7122969985\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (27.1648864746,26.5843679434), test loss: 32.0125628948\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.25143551826,2.45770897849), test loss: 3.64599896967\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (3.47837400436,26.5693260183), test loss: 34.685085845\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.31094646454,2.45697553552), test loss: 3.01532091796\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (13.5050001144,26.5517793015), test loss: 32.2082945824\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.972018957138,2.45624770403), test loss: 3.8700517118\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (50.9722061157,26.5331100094), test loss: 31.4745882273\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.17078638077,2.45546561415), test loss: 3.03599250615\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (27.4988422394,26.5197285045), test loss: 36.6455199242\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.13920474052,2.454484064), test loss: 3.8867822051\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (59.5293998718,26.5043622537), test loss: 32.068717432\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.08792972565,2.4534562165), test loss: 3.10704357624\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (24.7226486206,26.4889270887), test loss: 29.2797904968\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.89813446999,2.4525608564), test loss: 3.78862918019\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (48.2597160339,26.4725029147), test loss: 30.6369606018\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.7184779644,2.45136805764), test loss: 3.52015867829\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (34.2634773254,26.4555480116), test loss: 29.6784005642\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.39905881882,2.45056899863), test loss: 3.89742987156\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (35.4278030396,26.4405379751), test loss: 30.4274961472\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.53981208801,2.44973085292), test loss: 3.78463988304\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (19.7215938568,26.4236041559), test loss: 36.5432081699\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.73950386047,2.44910126763), test loss: 2.94513301253\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (39.3993682861,26.4050208041), test loss: 30.4028510094\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.66097283363,2.44832423568), test loss: 3.86977899075\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (13.6034469604,26.3904314975), test loss: 31.1967708588\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.17751955986,2.44744952562), test loss: 2.83132138252\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (25.4859046936,26.3754941482), test loss: 31.1775362492\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.70291960239,2.44613378763), test loss: 3.77286897004\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (32.938331604,26.3620167044), test loss: 32.0815000534\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (6.32285881042,2.44556862828), test loss: 3.03688834906\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (36.8478393555,26.3471989439), test loss: 29.7932712078\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.52754044533,2.44443433829), test loss: 3.65909955502\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (25.6520576477,26.328384083), test loss: 31.4828759193\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.97061347961,2.44356699964), test loss: 3.44243674874\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (26.5114173889,26.312985105), test loss: 30.3211397648\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.971244812012,2.44264945993), test loss: 3.93139023781\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (3.02623653412,26.2982095047), test loss: 30.265410924\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.14647960663,2.44181610259), test loss: 3.83064254522\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (13.7060489655,26.2817885099), test loss: 35.0847853661\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.9985871315,2.4410204463), test loss: 3.34321633279\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (51.2388153076,26.2643318847), test loss: 32.2283929586\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.00370931625,2.4401714024), test loss: 3.70786744356\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (27.5780754089,26.2500221582), test loss: 41.7330394268\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.66237950325,2.43919032361), test loss: 2.82553069293\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (57.8064727783,26.2356270432), test loss: 28.7589848995\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.88717699051,2.43815173908), test loss: 3.9376535058\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (36.893157959,26.2201156839), test loss: 32.2912353992\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.5572681427,2.43716647873), test loss: 3.17974414229\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (46.8523864746,26.2045725385), test loss: 31.7391463757\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.52480626106,2.43605138356), test loss: 3.70386125445\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (26.6526126862,26.188091252), test loss: 31.4173886776\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.10797524452,2.43519126472), test loss: 2.95083162934\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (34.151260376,26.1735791741), test loss: 29.5325057983\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.51752758026,2.43432717969), test loss: 3.81355935037\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (19.4494400024,26.1575071459), test loss: 32.5449145317\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (4.7332983017,2.43358660907), test loss: 3.83315679133\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (40.6274719238,26.1398170292), test loss: 33.5040989399\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.67666065693,2.43269269224), test loss: 3.33606894016\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.6026277542,26.1253926649), test loss: 34.2322197437\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.47942173481,2.43176641326), test loss: 3.67446740866\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (18.98686409,26.1110527648), test loss: 35.1921006918\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.70516598225,2.43056867033), test loss: 2.85773454905\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (35.2158317566,26.0974056163), test loss: 30.3237780094\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (5.71635723114,2.42988491579), test loss: 3.89349218607\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (34.8889770508,26.0825657764), test loss: 32.5531847477\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.25490307808,2.42875551787), test loss: 3.08754498363\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (23.1025505066,26.0646958913), test loss: 30.0663081646\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.91452121735,2.42782320495), test loss: 3.68325101733\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (24.56690979,26.0499675739), test loss: 34.4052632332\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.82969802618,2.42689936797), test loss: 3.08526639342\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (2.81744122505,26.0362054752), test loss: 30.0267484665\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.928939402103,2.42609427721), test loss: 3.75293884277\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (10.9278869629,26.0195677426), test loss: 31.4744523525\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.00688159466,2.42526782639), test loss: 3.8049492538\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (50.9464645386,26.0034376123), test loss: 31.4324081421\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.93587112427,2.42442448415), test loss: 3.76199589372\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold4/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (43.1947631836,inf), test loss: 32.5344569206\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.92657089233,inf), test loss: 2.90066405535\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (21.7120838165,27.4734888124), test loss: 33.4226520538\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.65196228027,2.5608372311), test loss: 3.39195676148\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (86.1528320312,27.507062011), test loss: 40.7081474304\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.20694637299,2.5578449443), test loss: 3.63895003796\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (14.0266189575,27.4578043124), test loss: 32.7446638584\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.47226667404,2.54814606901), test loss: 3.49398029447\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (38.2208862305,27.4922672887), test loss: 33.0189903736\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.32111525536,2.54186213537), test loss: 3.50423448086\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (11.4378499985,27.4812928135), test loss: 36.4767105579\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.931030750275,2.54199064987), test loss: 2.82978334725\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (60.6773605347,27.4724857856), test loss: 34.3180914164\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.69626617432,2.54218012348), test loss: 3.56077851653\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (12.6438789368,27.4487191177), test loss: 29.1104912519\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.08052754402,2.5418883013), test loss: 2.69556963742\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (16.5181045532,27.4394208473), test loss: 38.2174176931\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.53838801384,2.54058643848), test loss: 3.52469359636\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (25.9547615051,27.4463233699), test loss: 40.6793938637\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.02045154572,2.54070283699), test loss: 3.09382707775\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (27.4218673706,27.4087162212), test loss: 33.7495821238\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.16860151291,2.54014835194), test loss: 3.20374604464\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (16.4534225464,27.3704613175), test loss: 33.1800619364\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.94374537468,2.53866070813), test loss: 3.49783092141\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (32.6421852112,27.3702164246), test loss: 31.1782574654\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.01102590561,2.5364313257), test loss: 3.66134500206\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (18.4851531982,27.3507127499), test loss: 38.2415915251\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.38424861431,2.53516391239), test loss: 3.56870241761\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (12.9469585419,27.3482471883), test loss: 31.8862252712\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.1032834053,2.53503096101), test loss: 2.943991597\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (39.187664032,27.3165796188), test loss: 34.9735771179\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.2789182663,2.53387860546), test loss: 3.45324190259\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (16.3988113403,27.3114268051), test loss: 34.5566533804\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.89611768723,2.53392406012), test loss: 2.89206092954\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (4.44306850433,27.2982687669), test loss: 31.2785967827\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.11965084076,2.53243958602), test loss: 3.49240546227\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (45.3585968018,27.2751326621), test loss: 31.255312252\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.27222037315,2.53188247148), test loss: 2.94234672785\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (28.7897644043,27.2515534869), test loss: 35.5771234751\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.89593839645,2.53054981719), test loss: 3.5219178915\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (37.7811164856,27.2387762022), test loss: 33.6653151035\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.27644872665,2.52937910196), test loss: 2.81419291496\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (31.5853996277,27.2259800634), test loss: 31.147116673\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.74880933762,2.52830796732), test loss: 3.9309683919\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (41.651134491,27.2130318788), test loss: 32.1621669292\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.63894367218,2.52754989547), test loss: 3.57429929376\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (11.8330821991,27.1956332736), test loss: 31.9831193089\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.91622924805,2.52660646488), test loss: 2.92453449965\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (25.4119510651,27.1800767475), test loss: 36.3308489799\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.89430570602,2.52602286532), test loss: 3.52012898326\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (34.8097076416,27.1671843172), test loss: 33.7469407082\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.366396844387,2.52488204794), test loss: 2.84051549733\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (32.1079483032,27.1521970674), test loss: 36.3143543243\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.999498605728,2.52422542728), test loss: 3.6413374424\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (37.5208702087,27.1251910069), test loss: 31.3053968906\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.77564811707,2.52336421942), test loss: 3.0643865943\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (21.6390457153,27.1159406291), test loss: 33.0847284079\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.79128861427,2.52228729468), test loss: 3.3838765651\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (25.5952949524,27.0989192097), test loss: 33.569443512\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.45444250107,2.52087257735), test loss: 3.01393147111\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.1386108398,27.0877557798), test loss: 30.240781641\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.89826965332,2.52037965496), test loss: 3.47783579826\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (41.6729469299,27.0719236195), test loss: 33.2167236805\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.29439020157,2.51935184744), test loss: 3.79421816319\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (25.1803340912,27.0518922345), test loss: 31.7831020832\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.14930582047,2.51854090911), test loss: 3.68510900438\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (33.4551315308,27.0432927019), test loss: 32.3321083784\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.8481900692,2.51785034983), test loss: 3.53897932172\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (59.2004318237,27.0232687223), test loss: 40.0306300163\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.12971949577,2.51706510467), test loss: 2.9138278082\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (52.8973617554,27.0089981757), test loss: 39.8836437702\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.16498732567,2.51648354254), test loss: 3.59601610899\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (10.0532398224,26.9862357585), test loss: 29.3534732342\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.70691514015,2.51528329671), test loss: 2.72406178415\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (25.4056015015,26.9749159002), test loss: 34.5422065258\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.29378247261,2.51380927359), test loss: 3.46940969825\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (19.0134239197,26.9621201545), test loss: 32.8836041451\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.54919958115,2.51322738048), test loss: 2.93808661699\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (34.1584358215,26.9478512544), test loss: 31.0922585726\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.39639091492,2.51236955946), test loss: 3.4954333365\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (25.449054718,26.9307138869), test loss: 32.734066391\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.901765227318,2.51159198279), test loss: 3.53022176027\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (24.0194187164,26.9157552792), test loss: 29.8547944069\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.3577902317,2.51070153072), test loss: 3.73180019855\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (68.4758911133,26.9075079333), test loss: 32.26945467\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (5.88000249863,2.51012534926), test loss: 3.72748488188\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (31.2937355042,26.8862261838), test loss: 31.3117949009\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.85785937309,2.50931240607), test loss: 2.85891841054\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (20.6474990845,26.8656218368), test loss: 34.574191761\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.44679927826,2.50822991064), test loss: 3.49100068212\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (10.3330793381,26.8533886517), test loss: 36.0990605831\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.51086235046,2.50725305952), test loss: 2.87353802025\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.6234893799,26.8377706853), test loss: 31.8294975877\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.65658068657,2.50612112821), test loss: 3.75306398869\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (30.5689697266,26.8276288401), test loss: 33.3337699413\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.12951016426,2.50544608734), test loss: 2.98642330766\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (41.7891578674,26.8094172858), test loss: 33.0848612547\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.69500076771,2.50459430792), test loss: 3.49790219665\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (23.8719272614,26.7964364814), test loss: 33.9287528992\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.790057837963,2.50392795288), test loss: 3.26361422539\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (12.9929084778,26.7817755129), test loss: 31.8443382978\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.09582805634,2.50295612702), test loss: 3.42012291849\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (42.1050186157,26.7654553656), test loss: 33.4215457201\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.07050323486,2.50221966488), test loss: 3.60285302699\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (23.2017288208,26.7492551921), test loss: 32.2111433864\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.15159082413,2.50133111428), test loss: 2.83198965788\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (44.9579429626,26.734904005), test loss: 35.1492708683\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.07564926147,2.50024125794), test loss: 3.45890227556\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (10.7635631561,26.7208223921), test loss: 33.8454266548\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.33344697952,2.499388413), test loss: 2.76957135648\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (25.2744369507,26.7074943133), test loss: 33.2187828302\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.879609942436,2.49860868368), test loss: 3.58730817437\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (14.8487892151,26.6930832964), test loss: 30.9267284155\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.54758739471,2.49772112294), test loss: 3.06707641482\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (34.4041938782,26.6783377018), test loss: 32.2870953321\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.05357468128,2.49694632928), test loss: 3.58777795136\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (29.1948814392,26.6644389059), test loss: 32.6576727867\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.95246243477,2.49605973704), test loss: 3.03001738936\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (42.7409820557,26.6510758879), test loss: 31.8219581127\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (5.89258384705,2.495429891), test loss: 3.27139211893\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (99.9221878052,26.6336123913), test loss: 41.4154389381\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (7.00790596008,2.49462586361), test loss: 3.76485342979\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (21.4633789062,26.6190058683), test loss: 31.1330765486\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.8308224678,2.49368050473), test loss: 3.59809267521\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (35.2368621826,26.6049542978), test loss: 31.1172484398\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.72562336922,2.49247809874), test loss: 3.57040907145\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (10.7130832672,26.5915678653), test loss: 34.6133077145\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.689015269279,2.4917631924), test loss: 2.87301544547\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (32.6473922729,26.5779518222), test loss: 32.6795736551\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.46845722198,2.49092813355), test loss: 3.56853504777\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (19.1604423523,26.5607853424), test loss: 30.1718613625\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.48496103287,2.49012419851), test loss: 2.731676054\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (21.9011993408,26.5503049782), test loss: 37.7316754818\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.57565641403,2.48936728923), test loss: 3.50957057476\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (13.9217624664,26.5352323247), test loss: 35.9289053917\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.684910178185,2.48859132633), test loss: 3.03157447278\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (21.7773284912,26.5200467701), test loss: 36.6448124409\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.3538506031,2.48790189699), test loss: 3.2582179606\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (23.9924163818,26.5025197254), test loss: 31.8562423706\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.17399024963,2.48690471928), test loss: 3.49378345311\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (41.6194763184,26.49071715), test loss: 29.6304588079\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (6.78082180023,2.48590296777), test loss: 3.64891287684\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.9731235504,26.4773592657), test loss: 34.8898167133\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.737280249596,2.48503757706), test loss: 3.64355564415\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (11.6515693665,26.4636672335), test loss: 31.1314692259\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.15483665466,2.48430324154), test loss: 2.98004677594\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (41.333404541,26.4505018508), test loss: 35.9404040575\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (4.52531909943,2.4835601481), test loss: 3.48890648782\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (7.40086841583,26.4347391664), test loss: 33.385562849\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.66281366348,2.48277831303), test loss: 2.96550020576\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (4.41790151596,26.4244624107), test loss: 31.6480427742\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.43219852448,2.48196349695), test loss: 3.62430023253\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (41.1491088867,26.4081517901), test loss: 32.0183511734\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.2800219059,2.48114889398), test loss: 2.96975838244\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (9.55573272705,26.3906853229), test loss: 33.3349699497\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.0197789669,2.48023453205), test loss: 3.41962704957\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (30.502368927,26.3792723357), test loss: 38.0254275322\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.67431616783,2.47935794876), test loss: 2.97095957994\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (16.7589149475,26.3636540009), test loss: 30.8771328688\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.87164640427,2.47838462348), test loss: 4.1296115458\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold5/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (32.819858551,inf), test loss: 35.4578790188\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.11166214943,inf), test loss: 2.77482684851\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (61.3931655884,26.4432968807), test loss: 36.2115331173\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.50034809113,2.54830593574), test loss: 3.54129198194\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (20.1456546783,26.2664288146), test loss: 36.632396698\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.34442567825,2.54477051109), test loss: 3.19235514402\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (15.0086231232,26.4190301302), test loss: 33.4679060459\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.93694329262,2.54733199283), test loss: 3.75708770156\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (41.4835739136,26.4242887324), test loss: 30.4608686924\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.23729968071,2.53718675169), test loss: 3.50209486485\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (15.5188732147,26.3937118041), test loss: 35.3997117281\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.568141877651,2.53367758336), test loss: 2.83172684014\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (8.33090019226,26.3809612679), test loss: 31.8473654032\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.77790737152,2.53454795177), test loss: 3.64378027916\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (14.8913860321,26.3284578744), test loss: 32.1218725443\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.75592041016,2.53277944548), test loss: 2.80249801278\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (18.8984184265,26.3236177257), test loss: 38.8044442177\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.74244225025,2.53159533603), test loss: 3.48813381195\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (64.407043457,26.3257935379), test loss: 40.891362381\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.98162221909,2.53257643343), test loss: 2.83333130777\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (21.2362442017,26.2688088274), test loss: 39.2691009998\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.5619020462,2.5309204877), test loss: 3.55233817697\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (26.2050228119,26.2657126974), test loss: 29.3654533148\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.23506164551,2.53118032707), test loss: 2.70096542239\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (14.9810943604,26.2693984251), test loss: 34.3449136257\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.973651647568,2.52775296185), test loss: 3.77702425718\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (24.4000587463,26.2460665834), test loss: 37.6326263905\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.50832366943,2.52636420578), test loss: 3.39799806476\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (14.5933799744,26.2452596185), test loss: 34.2294304848\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.13792324066,2.52626976548), test loss: 3.20661683679\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (39.8155326843,26.2064085437), test loss: 41.562566185\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.99393761158,2.52436428369), test loss: 3.50277838707\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (18.3575305939,26.1850330819), test loss: 34.9003242493\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (5.73350000381,2.52314834772), test loss: 2.9032463342\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (10.3635263443,26.1881446986), test loss: 29.8233959675\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.983120679855,2.52307314538), test loss: 3.48805394769\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (11.3613433838,26.1525941171), test loss: 31.7860642433\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.921344220638,2.521615098), test loss: 2.73258800507\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (26.7045593262,26.1329842668), test loss: 32.6840672016\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.79417514801,2.5207851871), test loss: 3.43797594905\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (9.12031364441,26.1322171793), test loss: 35.5294895172\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.53448462486,2.5200855996), test loss: 2.66419680715\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (19.251499176,26.1140058128), test loss: 35.4735127211\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.21797525883,2.51783059492), test loss: 3.6907669425\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (26.6501045227,26.1088670984), test loss: 30.5079518557\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.30215239525,2.51741206381), test loss: 3.42210095227\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (58.7655563354,26.0854888737), test loss: 36.7565088749\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.04418158531,2.51615978547), test loss: 3.74503592253\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (20.8040943146,26.0591515077), test loss: 33.111128521\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.3659260273,2.51496971524), test loss: 3.64955670834\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (34.736125946,26.0547467915), test loss: 36.5023376465\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.28128457069,2.51440524271), test loss: 3.03906182349\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (40.9006652832,26.0316392618), test loss: 32.346057272\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.22920322418,2.51318489021), test loss: 3.55574151874\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (13.7702827454,26.0084321652), test loss: 37.3690371513\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.42117929459,2.51232390234), test loss: 2.75574313998\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (24.4921665192,26.0072883853), test loss: 31.5025973082\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.02524471283,2.51177800672), test loss: 3.50951371193\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (8.04311466217,25.9866426995), test loss: 31.5483590126\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.67113399506,2.51011460011), test loss: 2.85869120359\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (23.1937789917,25.9781128217), test loss: 32.7666046143\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.885376155376,2.50897916586), test loss: 3.47504834533\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (25.6629562378,25.9589865544), test loss: 30.97558465\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.99135041237,2.50805973094), test loss: 3.25324475765\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (48.9774017334,25.938108921), test loss: 34.8297789097\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.70286846161,2.5069409741), test loss: 4.0183193475\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (42.2221908569,25.9282263688), test loss: 29.5691095114\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.46753692627,2.50616036689), test loss: 3.55057997108\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (57.5177993774,25.9097563388), test loss: 38.5539343596\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.36749649048,2.50531863583), test loss: 3.19428953826\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (19.9005889893,25.8851168613), test loss: 34.8307136536\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.15560054779,2.50444333147), test loss: 3.51101790071\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (13.588303566,25.8795309041), test loss: 35.8866249561\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.92087078094,2.50390779458), test loss: 2.90866341889\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (34.8796730042,25.8664996842), test loss: 30.7106866598\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.18719899654,2.50219162704), test loss: 3.42427025437\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (15.4126605988,25.8501121522), test loss: 30.8176311255\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.570075213909,2.50090603947), test loss: 2.72945069671\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (7.97699928284,25.8358924369), test loss: 34.4806804895\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.78045463562,2.50012816007), test loss: 3.64136297107\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (14.1960725784,25.8149726598), test loss: 29.7704815865\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.6038441658,2.49897621332), test loss: 2.62157507837\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (18.7223834991,25.8015760387), test loss: 41.1255322695\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.6786134243,2.49793666428), test loss: 3.72503587008\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (65.398651123,25.7904078921), test loss: 35.45261693\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.02303218842,2.49737373183), test loss: 3.59501925409\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (19.952922821,25.7658291215), test loss: 39.6030886173\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.48468708992,2.49626761015), test loss: 3.17999497652\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (25.4284934998,25.7539651685), test loss: 35.5883261204\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.13989830017,2.49561287064), test loss: 3.42243433893\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (14.7035961151,25.7438343168), test loss: 35.5214919567\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.909773290157,2.49403681421), test loss: 2.66153722107\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (21.8612518311,25.7268933307), test loss: 37.0856638908\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.48626208305,2.49298325281), test loss: 3.41231582761\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (14.2176818848,25.7167022901), test loss: 32.3222651005\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.07639217377,2.49226911108), test loss: 2.87802467346\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (39.2653808594,25.6953110238), test loss: 38.2965068817\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.980027675629,2.49101231709), test loss: 3.43115690053\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (18.7270450592,25.6784317914), test loss: 30.7519322872\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (5.37436914444,2.48996813768), test loss: 2.96229135096\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (9.52097034454,25.6699419967), test loss: 33.1610035181\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.938791394234,2.48932545336), test loss: 3.65340369642\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (10.4722356796,25.6482644238), test loss: 30.4164936543\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.943975925446,2.48819243784), test loss: 3.18681232333\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (25.6585788727,25.6322454271), test loss: 35.6545226574\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.78680610657,2.48728275983), test loss: 3.76169459224\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (8.79102897644,25.6229152509), test loss: 31.1943600178\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.43882274628,2.48642560161), test loss: 3.48858071268\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (18.5074501038,25.6073254591), test loss: 38.4314063072\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.17515289783,2.48500493108), test loss: 2.88978759646\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (26.3516654968,25.5970557645), test loss: 32.2455831051\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.26259994507,2.48426389798), test loss: 3.49393531084\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (53.7931671143,25.5795051499), test loss: 31.2385309219\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.941366434097,2.48317698027), test loss: 2.82526320815\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.9992218018,25.5605475489), test loss: 32.4254977226\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.32937479019,2.48213223146), test loss: 3.89663345814\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (33.3464431763,25.5507275406), test loss: 31.4996711731\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.25607013702,2.4813597956), test loss: 2.80284343362\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (39.6261062622,25.5329993187), test loss: 33.8486449718\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.21211743355,2.48029929147), test loss: 3.5459507525\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (12.9697570801,25.5150548233), test loss: 37.2346728802\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.22575831413,2.4793873172), test loss: 3.21944018602\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (24.3836460114,25.5070787383), test loss: 34.655012846\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.94051074982,2.47862326658), test loss: 3.81999590993\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (8.69854927063,25.4903530291), test loss: 29.7146641254\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.42195034027,2.47736671826), test loss: 3.62040896118\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (22.7673454285,25.4792142697), test loss: 34.2538975954\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.917944610119,2.47635292682), test loss: 3.15177068114\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (24.2175750732,25.4632584562), test loss: 31.9037373543\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.98473954201,2.47541683865), test loss: 3.53721993864\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (47.3233108521,25.4463944207), test loss: 37.7820914507\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.59210169315,2.47439275048), test loss: 3.08010780215\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (40.7988739014,25.4347903052), test loss: 28.4153902054\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (4.27689886093,2.47353838762), test loss: 3.45397381485\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (53.8434791565,25.4190212087), test loss: 39.5628134727\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (5.25286960602,2.47265429987), test loss: 2.91104314625\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (19.8648777008,25.400053308), test loss: 35.4940890789\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.99332261086,2.47175306506), test loss: 3.42067103088\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (12.4900922775,25.3908525259), test loss: 30.2969810963\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.8794580698,2.47102376095), test loss: 2.72767954469\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (29.7129917145,25.3777992075), test loss: 33.1837002516\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.15810370445,2.46971195767), test loss: 3.65006569624\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (15.4594039917,25.3631583045), test loss: 29.2518171787\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.575409889221,2.46864153447), test loss: 3.42354685664\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (7.64907550812,25.3497828357), test loss: 35.5310193777\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.74377298355,2.46782147227), test loss: 3.33383643031\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.6915254593,25.332757631), test loss: 34.146556139\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.49582147598,2.46679765928), test loss: 3.41035658717\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (18.8650398254,25.319715374), test loss: 44.6160456181\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.70968937874,2.46585259932), test loss: 2.9585604012\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (66.2782211304,25.3079855138), test loss: 36.6536636353\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.03453588486,2.46516351762), test loss: 3.63148425221\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (18.7539539337,25.2885781586), test loss: 41.2514168739\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.45301938057,2.46417221873), test loss: 2.87969275415\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (24.5328350067,25.2766268528), test loss: 32.4420498848\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.0446600914,2.46343793074), test loss: 3.49471647739\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (14.1842069626,25.2654152791), test loss: 32.6028847218\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.889951944351,2.46217969847), test loss: 2.75963692665\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (20.0515213013,25.2504026547), test loss: 39.4534948826\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.48200047016,2.46123105561), test loss: 3.41381329894\n",
      "run time for single CV loop: 1309.50396585\n",
      "\n",
      "MC # 2, Hype # hyp1, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold1/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (30.3202877045,inf), test loss: 34.1963336945\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.5838727951,inf), test loss: 3.0190092355\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (44.2223587036,26.5638825488), test loss: 27.0558187962\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.0426440239,2.52646663505), test loss: 2.90097115636\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (9.8790140152,26.5425070357), test loss: 28.9083676815\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.18779826164,2.5252467373), test loss: 2.92081930041\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (34.9515151978,26.607750223), test loss: 27.2937619686\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.41794204712,2.51481579645), test loss: 3.10733599067\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.5975761414,26.6144212147), test loss: 32.3511875629\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.812754690647,2.51185555306), test loss: 2.96614958942\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (17.825668335,26.598100331), test loss: 29.2070784569\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.44559049606,2.50927906115), test loss: 3.24928678274\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (25.240196228,26.6147931902), test loss: 31.6217679977\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.91101455688,2.50916463526), test loss: 3.51605142653\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (44.7588500977,26.5733357786), test loss: 30.3254349709\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.56702232361,2.50973900628), test loss: 3.10466525257\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (6.14610624313,26.5555450239), test loss: 30.6404095411\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.03656673431,2.5082101512), test loss: 3.20012570024\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (31.8371810913,26.5410377433), test loss: 32.1748968601\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.6065557003,2.50558999341), test loss: 2.91858340576\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (25.0621128082,26.5372417608), test loss: 29.3761361122\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.05937480927,2.50448918859), test loss: 3.37063105702\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (22.2397899628,26.5177478966), test loss: 32.4892243385\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.28301596642,2.50304055233), test loss: 2.84492584616\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (30.075345993,26.5264427653), test loss: 28.6245489836\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.22616100311,2.50284300999), test loss: 3.35301735997\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (31.2271385193,26.483597743), test loss: 33.4677821159\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.89435482025,2.50256469902), test loss: 3.08463076651\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (32.1764450073,26.4754781425), test loss: 30.8428408384\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.33198976517,2.50051490318), test loss: 3.34643389583\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (7.47218513489,26.4555627379), test loss: 36.9715118408\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.26835298538,2.49920147641), test loss: 3.00430339277\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (43.1811027527,26.4447331985), test loss: 29.9666473866\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.90117251873,2.49765907787), test loss: 3.08105255961\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (29.1664581299,26.4311503318), test loss: 30.0480574131\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.13425850868,2.49681125798), test loss: 3.15562357008\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (64.9189453125,26.4276417212), test loss: 44.9228723526\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.46371364594,2.49625922728), test loss: 3.17009968758\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (15.5921850204,26.3884471895), test loss: 28.7039480209\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.71627378464,2.49486199993), test loss: 3.04457234144\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (9.35505962372,26.3821350095), test loss: 29.8135294914\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.03634166718,2.49413442647), test loss: 3.25726811886\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (12.4516201019,26.3692923818), test loss: 32.6457052708\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.907476902008,2.49219536844), test loss: 3.27015784383\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (11.698759079,26.3529626097), test loss: 34.3598196983\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.13803672791,2.49125964247), test loss: 3.1060656935\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (25.752658844,26.3447295272), test loss: 31.3347211838\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.04844009876,2.49026594701), test loss: 3.47574979067\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (19.83568573,26.3279805754), test loss: 31.0366503\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.63264465332,2.48929492075), test loss: 3.12630546242\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (10.4197874069,26.3045745662), test loss: 33.4231366634\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.68051636219,2.48857248353), test loss: 3.20080752373\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (27.3488540649,26.2974759649), test loss: 32.2136909485\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.96380519867,2.48716073212), test loss: 2.8445833385\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (27.931930542,26.2829014971), test loss: 30.0932590723\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.940544605255,2.4861480374), test loss: 3.3076084435\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (51.4423294067,26.2684577881), test loss: 40.3366791964\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.57632255554,2.48526811202), test loss: 2.77270642668\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (18.3494529724,26.2585570872), test loss: 27.8168556452\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (4.75979280472,2.48433201644), test loss: 3.19853718281\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (46.1534576416,26.2412446796), test loss: 33.4279580593\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.12370753288,2.4833745267), test loss: 3.06674321592\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (20.0615444183,26.2188260818), test loss: 29.9623624325\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.66790938377,2.48247909472), test loss: 3.24786817729\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (27.3199691772,26.2104142989), test loss: 33.1260330677\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.87520122528,2.48113289605), test loss: 3.04849794507\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (15.7740192413,26.1947631215), test loss: 27.8202206135\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (5.62833595276,2.48012038288), test loss: 3.00552187562\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (25.6262207031,26.1809220825), test loss: 30.0034220457\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.11877155304,2.47901823067), test loss: 2.88938116431\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (21.9035568237,26.1712431023), test loss: 29.4824194431\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.544561088085,2.4779135587), test loss: 3.48148989379\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (18.2261314392,26.1530893871), test loss: 32.7991971016\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.0242023468,2.47731351502), test loss: 3.01919286251\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (9.99085426331,26.1354747471), test loss: 29.0734782696\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.2302942276,2.47641636838), test loss: 3.28542080522\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (17.5798873901,26.1245397289), test loss: 32.0410339594\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.67996358871,2.47511409816), test loss: 3.22593098357\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (19.701128006,26.1099972919), test loss: 31.3042734146\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.28741884232,2.47399955369), test loss: 3.1728385359\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (16.2753696442,26.0969372015), test loss: 32.2603087902\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.55143237114,2.47285495948), test loss: 3.44837487191\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (8.42748832703,26.0860569514), test loss: 32.6981902599\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.57194566727,2.47211149501), test loss: 3.21768785119\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (17.8294277191,26.0678108462), test loss: 35.3860264301\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.18997454643,2.47132032605), test loss: 3.34017621279\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (6.87245702744,26.0533645526), test loss: 31.7460715294\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.92277526855,2.47016472909), test loss: 2.66094931364\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (23.6553134918,26.0391895216), test loss: 29.8521513224\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.22568869591,2.46907971643), test loss: 3.30244748592\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (44.6008644104,26.024768594), test loss: 33.8009525299\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.43160462379,2.46790457411), test loss: 3.06766694188\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (18.3433246613,26.0118972332), test loss: 33.2225124836\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.83913516998,2.46698945504), test loss: 3.71437874436\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (15.3749599457,26.0015323795), test loss: 47.4522157669\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.720864534378,2.46609224879), test loss: 3.35239740312\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (38.7414207458,25.9808524215), test loss: 30.2841376066\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (5.63487195969,2.46531609075), test loss: 3.15837668777\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (40.9897232056,25.968891461), test loss: 39.7137925148\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.46015191078,2.46421276433), test loss: 3.17404514551\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (19.6205615997,25.9569002023), test loss: 28.3459252357\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (7.87686920166,2.46323434029), test loss: 2.87177541554\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (28.5051021576,25.9406656283), test loss: 31.2093543768\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.56199407578,2.46197703836), test loss: 2.9365852356\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (11.044002533,25.9284819565), test loss: 27.3357740879\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.67911624908,2.4610862547), test loss: 3.23624503613\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (19.1997318268,25.9150328674), test loss: 31.8584513187\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.57479071617,2.46027219876), test loss: 3.04461309314\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (9.7595500946,25.8979576433), test loss: 29.7436742544\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.941695332527,2.45926745583), test loss: 3.22032558918\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (29.5543956757,25.8865850511), test loss: 31.6274124622\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (6.39976215363,2.45818882557), test loss: 3.25190164149\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (39.5326881409,25.8753053346), test loss: 30.5251079321\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (5.18334722519,2.45722094094), test loss: 3.13525186479\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (15.4206380844,25.8590540906), test loss: 38.4689074516\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.441654503345,2.4561122081), test loss: 3.2045513466\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (16.8009376526,25.8473340295), test loss: 37.4181628227\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.58054614067,2.45513862814), test loss: 2.84392663538\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (21.1990184784,25.8326403622), test loss: 29.1389190435\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.803711533546,2.45432381046), test loss: 3.26959627271\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (22.8904762268,25.8165214816), test loss: 33.0297235727\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.38528585434,2.45346808259), test loss: 2.79993417859\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (8.55473995209,25.8055657388), test loss: 27.5812357306\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.59168171883,2.45230963102), test loss: 3.2723711431\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (29.179271698,25.7934795175), test loss: 35.9896848679\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.20483875275,2.45144891136), test loss: 3.07983638346\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (28.8369865417,25.7780750534), test loss: 30.4157275558\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.64184617996,2.4503873546), test loss: 3.23835673928\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (33.4569206238,25.7672774511), test loss: 33.0341429234\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.72417736053,2.44937375334), test loss: 3.14219698608\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (24.2110824585,25.7514959425), test loss: 26.867274332\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.67519330978,2.44863529211), test loss: 2.90297678113\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (14.3590011597,25.7357361929), test loss: 30.2330383301\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.17061448097,2.44777390872), test loss: 3.03624696136\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (52.37707901,25.7256693773), test loss: 26.4098041058\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.64434278011,2.44662764885), test loss: 2.94534643292\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (32.3688583374,25.7128517821), test loss: 35.0094854355\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.93438744545,2.44556683766), test loss: 3.07746386528\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (20.7856674194,25.6986605985), test loss: 29.464521718\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.18618226051,2.44457413605), test loss: 3.26936225295\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (36.145942688,25.6865026482), test loss: 36.2975509167\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.17159152031,2.44365143715), test loss: 3.68788779676\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (16.4641609192,25.6717205263), test loss: 33.82827425\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.0097899437,2.44294130099), test loss: 3.20982525647\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (20.1008682251,25.6580610307), test loss: 31.8661328793\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (5.37819910049,2.4420233839), test loss: 3.33887263685\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.6294498444,25.6456002566), test loss: 31.2289200306\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.83608102798,2.44095562631), test loss: 3.27718092203\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (36.1390686035,25.6339192795), test loss: 32.0803283691\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.38269805908,2.43990568743), test loss: 3.27874206305\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (25.4565982819,25.6199071879), test loss: 32.640437603\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.03561890125,2.4388984262), test loss: 2.77015603781\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (9.94942092896,25.608425519), test loss: 29.6910055399\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.01325559616,2.43807322476), test loss: 3.35767637789\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (24.7477474213,25.5926258422), test loss: 30.5328274727\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.30883145332,2.43733849222), test loss: 3.01513361931\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (36.6818580627,25.5808923872), test loss: 29.812211132\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.30143690109,2.43621796793), test loss: 3.23119110167\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (14.7114171982,25.5678874139), test loss: 38.6091573238\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.637789130211,2.43526922187), test loss: 3.07758657038\n",
      "\n",
      "MC # 2, Hype # hyp1, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold2/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (34.7910881042,inf), test loss: 32.2621516705\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.18307995796,inf), test loss: 2.87351926789\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (8.75812625885,25.4496407621), test loss: 31.3115407467\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.80208730698,2.44814837936), test loss: 3.55614949167\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (28.2466621399,25.4153716391), test loss: 31.7615088224\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.19753074646,2.44059968489), test loss: 2.73889541328\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (26.0020637512,25.490292944), test loss: 31.1403122425\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.34471750259,2.42782793007), test loss: 3.63399881721\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (35.6375961304,25.4606490472), test loss: 35.7821895123\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.37246322632,2.42453659542), test loss: 3.37885299474\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (14.2878761292,25.377789811), test loss: 32.1095667362\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.97781944275,2.42217732178), test loss: 3.73216205835\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (39.1742019653,25.3910910872), test loss: 34.4652095795\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.94782543182,2.42063956098), test loss: 3.82371878326\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (43.1451339722,25.3670389754), test loss: 32.3483464718\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.77299594879,2.42315820893), test loss: 2.98567585945\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (10.1153764725,25.3401053292), test loss: 35.4871339798\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.46210980415,2.4230711421), test loss: 3.75705708861\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (5.37769842148,25.3603627996), test loss: 34.9539034367\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.88883531094,2.41989496489), test loss: 2.72032561451\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (22.3911838531,25.3529964004), test loss: 33.7525532722\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.782860040665,2.41794659934), test loss: 3.95739645958\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (21.2157745361,25.3084314967), test loss: 34.3123661518\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.74141085148,2.41578037224), test loss: 2.65647042096\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (19.18409729,25.2949110831), test loss: 31.834493041\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.0937542691827,2.41476651327), test loss: 3.88769835234\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (19.4962310791,25.2708301784), test loss: 33.7065309286\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.845881462097,2.41572078211), test loss: 2.89528502524\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (12.3579502106,25.2508907155), test loss: 32.0697342157\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.84386301041,2.41519303816), test loss: 3.81143246293\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (13.9303588867,25.2610607703), test loss: 32.797819972\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.38279867172,2.41343130937), test loss: 2.74524902552\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (12.614233017,25.2550204071), test loss: 31.7608106613\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.62863087654,2.41207667253), test loss: 3.91325773001\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (22.8342857361,25.2251631304), test loss: 31.5933357239\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.35472917557,2.4101682489), test loss: 2.66956998706\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (26.3725395203,25.207150281), test loss: 30.7380133867\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.6948223114,2.40918761879), test loss: 3.72723790705\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (31.4219245911,25.1816531489), test loss: 31.4391008854\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.2346663475,2.40939450684), test loss: 3.43173387945\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (17.4311103821,25.1619613662), test loss: 30.6769314766\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.34971904755,2.40846831879), test loss: 3.81209014654\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (34.3314819336,25.1647817545), test loss: 32.3928301811\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.88733625412,2.4070666696), test loss: 3.88491304517\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (19.6660003662,25.1591382221), test loss: 32.5477124214\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.51534152031,2.40615895546), test loss: 3.17948356867\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (17.6017856598,25.1371218194), test loss: 35.6867898941\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.75828909874,2.40445856537), test loss: 3.88987277448\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (48.7952270508,25.1197352369), test loss: 38.4352772713\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.30457341671,2.40348612413), test loss: 3.06994195879\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (11.9030303955,25.0936666731), test loss: 33.2609293461\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.75828051567,2.40315630777), test loss: 3.95312976837\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (19.4547157288,25.0742426186), test loss: 34.4924767971\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (5.01400613785,2.40231096201), test loss: 2.57883258164\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (26.8848762512,25.0702792056), test loss: 31.7525405884\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.97512423992,2.40085374086), test loss: 4.39823855162\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (34.3695640564,25.0646764973), test loss: 31.9793829441\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (5.93694400787,2.40012049387), test loss: 2.94688944668\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (17.6034698486,25.047051668), test loss: 32.7729118347\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.04835009575,2.39870423348), test loss: 4.24273033142\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (17.9728660583,25.031307103), test loss: 32.2247195721\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.51007986069,2.39787193843), test loss: 2.96947363019\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (25.2561664581,25.0088319199), test loss: 34.1581119776\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.9043610096,2.39731391606), test loss: 3.80625920892\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (10.0866127014,24.9879082873), test loss: 31.0963265896\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.89577102661,2.39619245201), test loss: 2.74748003781\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (18.7983398438,24.9797222523), test loss: 31.7373200417\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.30650615692,2.39497845028), test loss: 3.63850169778\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (22.6549015045,24.9712515869), test loss: 33.683064127\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.33586025238,2.39398994239), test loss: 3.45055847317\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (29.3738479614,24.9571625268), test loss: 31.9028117657\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.12839543819,2.39309866391), test loss: 4.117441535\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (12.5268859863,24.9440295578), test loss: 33.2052098513\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.14704918861,2.39219659883), test loss: 3.84346907437\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (21.1672267914,24.9236515276), test loss: 35.2405041218\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.09180617332,2.39154765778), test loss: 3.13156289458\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (7.85811901093,24.9037455853), test loss: 32.5573943853\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.65765810013,2.3903890069), test loss: 3.72415333092\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (18.6792106628,24.8924131414), test loss: 36.025288415\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.26109337807,2.38930012701), test loss: 2.77979307175\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (60.4856491089,24.8815958336), test loss: 33.9091957092\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.58531236649,2.38822153106), test loss: 3.95963460207\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (29.4056415558,24.8676264266), test loss: 33.8928710699\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.3959107399,2.38745441817), test loss: 2.53172736466\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (5.40017461777,24.856942991), test loss: 32.0885447502\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.43064296246,2.38663291428), test loss: 3.95185692906\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (44.087890625,24.8382980137), test loss: 33.4229050636\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.03525674343,2.38597496206), test loss: 2.77032930255\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (20.4794979095,24.8210327316), test loss: 32.3421474457\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.44323253632,2.38471333879), test loss: 3.91628634632\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (16.8644924164,24.8072738518), test loss: 37.2830311298\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.37514412403,2.38361822205), test loss: 2.78409865499\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (19.6928024292,24.7930606251), test loss: 32.2731883287\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.03043413162,2.38258970901), test loss: 3.82428320646\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (19.9205760956,24.7799913031), test loss: 33.9689764023\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.70806670189,2.38182761641), test loss: 2.84949441105\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (10.2635593414,24.7706617908), test loss: 30.1209946394\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.733840405941,2.38108609303), test loss: 3.53626462817\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (28.1826820374,24.751885206), test loss: 32.8391085148\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.52055191994,2.38045673178), test loss: 3.54819174707\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (37.5412979126,24.7375764855), test loss: 31.3147042751\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.86977505684,2.37915463446), test loss: 3.85428994894\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (14.2183551788,24.7242484963), test loss: 38.6526697636\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.567135810852,2.37814252336), test loss: 3.61292348504\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (5.65597772598,24.7086199798), test loss: 34.0941477299\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.77701592445,2.3770124236), test loss: 3.55962981731\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.0218935013,24.694520336), test loss: 33.4545469761\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.23867297173,2.3762354826), test loss: 4.26153575778\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (34.7412872314,24.6849144658), test loss: 34.401699543\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.52226614952,2.37557804348), test loss: 2.91986068487\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (33.8773193359,24.6668240599), test loss: 41.0555487156\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (5.30323314667,2.37497098086), test loss: 3.95428502262\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (35.2334785461,24.6533765867), test loss: 33.5064768314\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.47862386703,2.37372552805), test loss: 2.58340033591\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (8.34085655212,24.6422678805), test loss: 37.7847828865\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.28198873997,2.37277003772), test loss: 3.87797688842\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (46.6147689819,24.6264528857), test loss: 30.8757493258\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.75914692879,2.3715521949), test loss: 2.6372433424\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (48.1264839172,24.6115346775), test loss: 33.8361939669\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.81130480766,2.37077662517), test loss: 4.25970130563\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (34.6271743774,24.5999118315), test loss: 33.084871912\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.95571947098,2.37007808555), test loss: 2.86547782868\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (59.4719009399,24.5822590031), test loss: 38.4170101643\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.10205364227,2.36938718951), test loss: 3.79522807598\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (19.8560371399,24.5701916968), test loss: 32.9251728058\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.06382369995,2.36838161634), test loss: 2.92763408124\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (17.9085025787,24.5606326027), test loss: 34.8908371925\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.0025062561,2.36744431019), test loss: 3.49022871852\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (32.613609314,24.5448820242), test loss: 31.097290659\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.40801620483,2.36620631316), test loss: 3.49323220104\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (22.6814937592,24.5290188469), test loss: 36.2185846567\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.07783746719,2.36537427441), test loss: 3.68654606342\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (17.625585556,24.5169128918), test loss: 37.9667814255\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.491897165775,2.36459883899), test loss: 3.93228130192\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (17.3327140808,24.4987332965), test loss: 32.3678112984\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.71326065063,2.36397962066), test loss: 4.00305165648\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (8.34248733521,24.4881595442), test loss: 33.4371886253\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.05841732025,2.36301202325), test loss: 4.01006732881\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (36.7024383545,24.4790001683), test loss: 34.1729005337\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (6.22665596008,2.3620849192), test loss: 3.01023932248\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (9.05372810364,24.4643329029), test loss: 36.7843760729\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.25801563263,2.36090959934), test loss: 3.83676766157\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (6.47374725342,24.4492673023), test loss: 35.3623529434\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.36056470871,2.36009280282), test loss: 2.74049044997\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (63.5614395142,24.4362436846), test loss: 35.5349699497\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.43959212303,2.35933644195), test loss: 4.16964712739\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (28.8682174683,24.4175248549), test loss: 29.5071857929\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.13941931725,2.35862685491), test loss: 2.7279553473\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (2.34494161606,24.4073799096), test loss: 33.3961597919\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (6.58211708069,2.35773441931), test loss: 3.92841241062\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.3993473053,24.3978358524), test loss: 34.4955052853\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.09980559349,2.3567444767), test loss: 2.80181177855\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (46.1165924072,24.3846188596), test loss: 31.064393568\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.8019630909,2.35571062637), test loss: 3.78631818891\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.246884346,24.371059553), test loss: 33.6278589725\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.1680598259,2.35483483555), test loss: 2.90997832566\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (50.4922218323,24.3558583042), test loss: 36.9139942646\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.72496533394,2.35411265981), test loss: 3.82228804827\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (13.195315361,24.337944015), test loss: 34.2870491028\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.36238431931,2.3533474109), test loss: 2.77487568855\n",
      "\n",
      "MC # 2, Hype # hyp1, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold3/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (44.5953598022,inf), test loss: 29.3864472389\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.56206679344,inf), test loss: 2.99855194092\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (64.8063201904,26.0652050128), test loss: 49.2303767204\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.73705244064,2.42603550497), test loss: 4.11977935135\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (40.7731132507,25.9334505799), test loss: 28.2900845051\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.43716800213,2.42141233529), test loss: 3.66969408691\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (28.1637001038,25.8449059219), test loss: 35.1205956221\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.99167954922,2.41519178005), test loss: 3.88363108635\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.0984344482,25.908467258), test loss: 36.8992866516\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.61753702164,2.41131849439), test loss: 3.62221558541\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (12.3533382416,25.8842682339), test loss: 35.0437702656\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.04662275314,2.4072919398), test loss: 2.73440276831\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (32.3590774536,25.9024083014), test loss: 32.426146698\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.47493743896,2.40594891012), test loss: 4.02882518172\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (6.92096614838,25.8795577305), test loss: 35.0108495712\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.82991266251,2.40311463986), test loss: 3.2628862679\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (25.9950866699,25.8514210424), test loss: 31.9376435995\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.98400735855,2.40181695852), test loss: 3.55804658234\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (19.9128742218,25.8467404868), test loss: 27.5842666864\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.73168456554,2.40211432082), test loss: 2.86414889097\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (7.70711898804,25.8315046377), test loss: 32.4676031351\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.60905480385,2.40174007778), test loss: 4.02310005724\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (24.8717689514,25.8017345537), test loss: 29.1158444881\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.22799110413,2.40127471923), test loss: 3.65936558247\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (32.2434272766,25.7718921954), test loss: 32.6877715588\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.93223714828,2.40091097077), test loss: 3.6776116997\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (19.0580711365,25.7637324248), test loss: 32.331647706\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.62772488594,2.39974620476), test loss: 3.64039537311\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (28.6329650879,25.753171103), test loss: 33.6202524662\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.24109911919,2.39709790919), test loss: 2.89198999107\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (11.6290502548,25.7371572813), test loss: 32.3453445435\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.922894120216,2.39659780882), test loss: 3.62880305052\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (31.2535400391,25.7288264205), test loss: 29.5220616817\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.43181598186,2.39526452703), test loss: 2.90517998636\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (18.1313171387,25.7097139796), test loss: 32.8189684153\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.66334629059,2.39348414821), test loss: 3.86347930729\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (22.6911869049,25.6894062459), test loss: 28.1206365108\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.02043139935,2.39293073689), test loss: 2.76899415255\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (25.2926063538,25.6821123356), test loss: 32.7089396715\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.78743505478,2.39233141955), test loss: 3.78661862016\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (68.4811706543,25.6625149427), test loss: 35.4563393116\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.53061032295,2.39193639281), test loss: 3.95334541202\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (31.1374893188,25.6396747638), test loss: 33.676187849\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.79991173744,2.39135353875), test loss: 3.29632072151\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (21.9393424988,25.6135947117), test loss: 32.1972814083\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.29058980942,2.39033334818), test loss: 3.71217077374\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (39.0431900024,25.6059021443), test loss: 34.3110325336\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.693017721176,2.38895983747), test loss: 3.03663380444\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (6.54515743256,25.586812024), test loss: 31.7521825314\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.78750753403,2.38759407767), test loss: 3.66322814822\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (45.4171142578,25.5773863783), test loss: 30.263933754\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.73340368271,2.38665534745), test loss: 3.1563715905\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (16.3106040955,25.5596782049), test loss: 33.1967566252\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.06695365906,2.38509184827), test loss: 3.7254922688\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (19.9520225525,25.5388914395), test loss: 27.6721150398\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.785950422287,2.38410948593), test loss: 2.75926750004\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (10.326374054,25.5277587934), test loss: 33.0508923054\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.81081700325,2.38342088385), test loss: 3.96970194876\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (13.1928730011,25.5100362676), test loss: 30.0443725109\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.524050593376,2.38274470443), test loss: 4.17097622156\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.4418449402,25.4909569435), test loss: 33.086751461\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.14919400215,2.38200010559), test loss: 2.95997719765\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (15.1468715668,25.4696585009), test loss: 30.8888651609\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.10852205753,2.38128063733), test loss: 4.10095470548\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (10.5584917068,25.4562185982), test loss: 30.3888579845\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.35395240784,2.38054288737), test loss: 2.91936229765\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (22.5860462189,25.4417857076), test loss: 31.132165122\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.95977663994,2.37870801224), test loss: 3.68856414557\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (15.3151140213,25.4259827609), test loss: 32.5590485573\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.587434589863,2.37795020936), test loss: 2.86724060178\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (27.5217323303,25.4135528406), test loss: 32.851058054\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.02750682831,2.37673148045), test loss: 3.5622672677\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (11.631231308,25.395388759), test loss: 29.8872885942\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.36157822609,2.37558003794), test loss: 3.51957546622\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (47.5627975464,25.3807888685), test loss: 35.8179350853\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.78554499149,2.37474371587), test loss: 4.16099355817\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (9.16104793549,25.3650552045), test loss: 29.5338239193\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.42914891243,2.37401439724), test loss: 3.75004795492\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (27.9318275452,25.3520620838), test loss: 36.4369483948\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.777411282063,2.37337325754), test loss: 3.05442619324\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (12.2591123581,25.3294787689), test loss: 32.6670541048\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.888323605061,2.37256922384), test loss: 3.79140038192\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (44.2543487549,25.3116145168), test loss: 28.8640080929\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.07830953598,2.37168263798), test loss: 2.91762478948\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (31.9644184113,25.2984187648), test loss: 32.174194622\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.900326967239,2.37046079112), test loss: 3.72974148989\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (9.2325592041,25.2812423571), test loss: 28.8067313194\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.69048750401,2.36939333265), test loss: 2.90037068129\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (19.2710876465,25.2692218255), test loss: 31.7858416557\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.26795685291,2.36837536213), test loss: 3.798112607\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (38.7427864075,25.2533591097), test loss: 29.0168323994\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.29155111313,2.3672712149), test loss: 3.54636156857\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (28.8095645905,25.2365141769), test loss: 33.3261992931\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.05712509155,2.36624640925), test loss: 3.94062399864\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (10.1929111481,25.2205303825), test loss: 29.4752217293\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.59878396988,2.36550073417), test loss: 3.81091840267\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.4520320892,25.207451173), test loss: 33.6086988449\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.12782406807,2.36480899523), test loss: 3.14596609473\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (23.3543624878,25.1877569484), test loss: 30.9669763088\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.596221983433,2.36386900225), test loss: 3.77637607455\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (16.7158088684,25.1703211841), test loss: 29.4989446163\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.408639132977,2.36316636774), test loss: 3.41366789341\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (6.22640132904,25.1546410967), test loss: 30.3705297232\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (5.60581588745,2.36233247887), test loss: 3.61522725821\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (15.9701690674,25.1414608553), test loss: 30.3424607277\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.909252882,2.36089434442), test loss: 3.0842800349\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (14.1959323883,25.1261335221), test loss: 33.361052084\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.864583492279,2.3600749801), test loss: 3.55188038945\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.8512382507,25.1125870698), test loss: 29.8723214149\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.89162731171,2.35906350405), test loss: 3.70490605831\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (9.13647842407,25.0975702836), test loss: 34.6521045208\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.0769405365,2.35797899654), test loss: 3.84585250616\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (38.7343978882,25.0811621194), test loss: 34.754451561\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.33805036545,2.35710268671), test loss: 3.8700463146\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (15.4214420319,25.0695567441), test loss: 35.0452616215\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.99326372147,2.35645724095), test loss: 2.845135355\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (16.9068107605,25.0519463701), test loss: 39.8321093082\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.792814850807,2.35559581414), test loss: 3.96426108479\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (7.29337453842,25.0349692053), test loss: 30.479282856\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.58377289772,2.35490131343), test loss: 2.95588485599\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (25.8567085266,25.0163596315), test loss: 30.4788805008\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.59055614471,2.35393513865), test loss: 3.52420275211\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (39.4856948853,25.0045744285), test loss: 30.9541271687\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.81932985783,2.35288852605), test loss: 2.81905463338\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (52.3014831543,24.9895521361), test loss: 32.2589586735\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.03288662434,2.35184509396), test loss: 3.88289211988\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (17.7079563141,24.9751071507), test loss: 28.4669221878\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.602141737938,2.35092184058), test loss: 3.7363966614\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (53.1770706177,24.9620965715), test loss: 34.4996182919\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.97396254539,2.34991041004), test loss: 3.4952970475\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (35.6287612915,24.943580789), test loss: 39.8333825111\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.79430615902,2.34889278165), test loss: 3.67083350122\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (12.8696556091,24.9314623523), test loss: 34.8795001507\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.35012245178,2.34821681671), test loss: 2.84018576145\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (25.2219791412,24.9148357254), test loss: 29.8856301069\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.09170007706,2.34737369926), test loss: 3.74345098734\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (32.7898483276,24.8995027107), test loss: 31.9524215698\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.863790929317,2.34657514858), test loss: 3.01079674363\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (69.3023834229,24.8831129445), test loss: 37.9384851933\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (6.27490186691,2.34592026008), test loss: 3.57901313901\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.1243896484,24.8676025486), test loss: 28.5082150459\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.53070068359,2.34503775454), test loss: 2.74263569117\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (21.6315441132,24.8544811617), test loss: 31.4031307697\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.90501785278,2.34377600222), test loss: 3.73349274397\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (21.9666099548,24.8391674833), test loss: 30.1512349606\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.51160168648,2.34293724885), test loss: 3.7584089458\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.08045578,24.8272588081), test loss: 33.4410921812\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.21954059601,2.34197276408), test loss: 3.19347776473\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (32.193359375,24.8109296257), test loss: 31.8540416241\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.39145517349,2.3409410151), test loss: 3.66624139547\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (22.9042434692,24.7977210063), test loss: 35.439288187\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.803388834,2.34016147947), test loss: 3.11120874286\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (22.6985969543,24.7818745614), test loss: 30.355276227\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.9335539341,2.33941011602), test loss: 3.653999722\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.6633548737,24.7688466314), test loss: 42.3601285934\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.509593486786,2.33863930485), test loss: 3.19413411617\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (8.33803939819,24.7509813507), test loss: 33.1219005585\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.71823227406,2.33786035559), test loss: 3.50502299666\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (11.1320524216,24.7351853335), test loss: 28.731169486\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.2517156601,2.33704241999), test loss: 2.76751067042\n",
      "\n",
      "MC # 2, Hype # hyp1, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold4/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (50.7449836731,inf), test loss: 32.1992021561\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.37093496323,inf), test loss: 3.15612413883\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (18.0187778473,26.4328773909), test loss: 33.7829789639\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.26294255257,2.38284820896), test loss: 3.80600428879\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (4.75634527206,26.4450406039), test loss: 31.3200227737\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.29528701305,2.37990896454), test loss: 2.97499676943\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (48.5715026855,26.5493498875), test loss: 35.2468645573\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.5313886404,2.37874279775), test loss: 4.02460006177\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (23.5058822632,26.5252855582), test loss: 35.0502537251\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.18732738495,2.3744271009), test loss: 3.88812670112\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (15.9218435287,26.5293243609), test loss: 35.1093811989\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.42999649048,2.37501849726), test loss: 3.4306740433\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (35.594078064,26.4684439674), test loss: 39.2778201103\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.2418718338,2.37124904275), test loss: 3.94372653961\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (25.5750026703,26.4914024036), test loss: 35.0467358112\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.06253433228,2.37070255983), test loss: 3.03456693888\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (96.9166412354,26.4702561031), test loss: 59.7394646645\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.18586826324,2.37121430609), test loss: 4.46746416092\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (30.4031410217,26.4360024062), test loss: 34.358330512\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.06654667854,2.37049462147), test loss: 3.16127621531\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (5.93848323822,26.4196762654), test loss: 32.7578535795\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.70649528503,2.36856298135), test loss: 3.87494705319\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (53.4792098999,26.4109684546), test loss: 34.4296916962\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.84724116325,2.36641019705), test loss: 3.08963395953\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (25.2548141479,26.4092510073), test loss: 32.914506793\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.26083374023,2.36548102816), test loss: 3.77669891119\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (11.5572910309,26.3768645574), test loss: 30.1410207272\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.919960319996,2.36384113997), test loss: 3.08965090513\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (30.0044670105,26.3585757073), test loss: 35.5478764057\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (0.683304309845,2.36322207104), test loss: 4.41263324618\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (3.1095392704,26.3495904067), test loss: 30.0379194736\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.728911519051,2.36231019877), test loss: 4.03441174626\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (31.2165393829,26.3247451175), test loss: 35.36067698\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.92284917831,2.36189272823), test loss: 3.36614890695\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (16.2118148804,26.3011590812), test loss: 33.6651566982\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.27683603764,2.3606191536), test loss: 4.12238989472\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (28.7734527588,26.2946275512), test loss: 35.4023883343\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.691794872284,2.35852334718), test loss: 3.13386741281\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (16.861831665,26.2866684865), test loss: 31.2580261469\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (0.476461589336,2.35791795183), test loss: 3.97468647957\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (14.2238807678,26.263974727), test loss: 38.96528368\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.765568494797,2.35672666664), test loss: 3.22026395202\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (12.2152481079,26.2485111902), test loss: 33.5829935551\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.925312757492,2.35569698162), test loss: 4.34544425011\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (34.5876312256,26.2344339002), test loss: 31.9892362118\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.960983097553,2.35462780328), test loss: 3.3413603425\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (33.1092300415,26.2110890702), test loss: 33.3511863708\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.874163210392,2.35405800406), test loss: 3.75411168933\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (46.6129951477,26.1910045675), test loss: 33.0410746574\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.20254659653,2.3530313234), test loss: 3.02510893047\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (53.6563262939,26.1804728872), test loss: 39.7814350128\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.40042304993,2.35225357655), test loss: 4.01616877913\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (12.7698287964,26.1684800709), test loss: 35.7405633926\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.881009459496,2.35084412813), test loss: 3.87771030068\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (17.5859241486,26.1528871353), test loss: 35.432135725\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.95427036285,2.35009722613), test loss: 3.50662664771\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (9.28479576111,26.128875105), test loss: 30.6735210419\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.19914698601,2.34879464236), test loss: 4.0652295053\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (16.0793323517,26.1192676801), test loss: 40.3914642334\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.54686391354,2.34767589196), test loss: 3.13676942587\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (15.5398845673,26.1001251782), test loss: 31.2260446548\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.55789136887,2.34719183303), test loss: 4.00024524331\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (79.3667831421,26.0828832792), test loss: 50.2334352016\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (6.95995044708,2.34630507519), test loss: 3.23539164066\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (14.1908731461,26.0688484286), test loss: 33.4496124983\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.90008354187,2.34511238805), test loss: 3.97091513276\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (4.41689538956,26.0544843277), test loss: 31.7506239414\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.05053591728,2.34398767736), test loss: 3.13147066832\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (47.0357475281,26.0421141252), test loss: 33.3935646057\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.48808121681,2.34302270587), test loss: 3.75929039717\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (56.5707702637,26.0248784396), test loss: 30.2831488371\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.7116856575,2.34191119797), test loss: 3.16317703128\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (19.3719825745,26.0056074149), test loss: 37.4054547787\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.23195385933,2.34103003153), test loss: 4.01542979479\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (31.6112594604,25.9955227858), test loss: 30.9561837196\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.22415924072,2.34018854055), test loss: 4.06212838888\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (40.5208358765,25.9739208313), test loss: 37.092746067\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.76832461357,2.33919232864), test loss: 4.10936513543\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (22.5595779419,25.9552266521), test loss: 32.87902565\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.881758749485,2.33834065703), test loss: 4.17532021999\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (19.6935920715,25.9460625166), test loss: 35.4843655825\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.86904656887,2.33692711097), test loss: 3.20361146033\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (34.0331039429,25.9321891874), test loss: 31.7004620075\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (4.87648296356,2.33609475584), test loss: 4.04280258417\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (52.9388618469,25.9155012689), test loss: 39.4295801163\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.91800451279,2.33508979186), test loss: 3.26467289329\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (52.5018157959,25.898091068), test loss: 35.0361419678\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.61589229107,2.33409888659), test loss: 4.43404209018\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (28.2687282562,25.8854111107), test loss: 33.1117637157\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.49960947037,2.33315419467), test loss: 3.30131587982\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (22.0362319946,25.8663148443), test loss: 32.4624454498\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.35537743568,2.3323964374), test loss: 3.73001232743\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (29.953754425,25.8485424033), test loss: 31.1117584705\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.31206297874,2.33142339959), test loss: 3.11305770278\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (6.38255882263,25.8365600386), test loss: 35.0186348438\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.38850283623,2.33050218084), test loss: 4.0569752872\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (8.60866928101,25.8233656246), test loss: 36.3465113163\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.25327706337,2.32947429036), test loss: 3.9546613574\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (30.0794353485,25.8105650664), test loss: 35.5595940113\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (6.05268383026,2.32858378761), test loss: 4.32661241591\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (10.8877096176,25.7908315462), test loss: 30.04289217\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (4.41799926758,2.32749142586), test loss: 4.12476084232\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (17.3593883514,25.7786077701), test loss: 37.2302159786\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.84123969078,2.32647044883), test loss: 3.1173517406\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (23.6808204651,25.7613052214), test loss: 31.8571520329\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.66679048538,2.32569083247), test loss: 3.99740612507\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (16.6823616028,25.7416904763), test loss: 34.8227397919\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.471694976091,2.32471667047), test loss: 3.61713377535\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (3.18279623985,25.7326309775), test loss: 32.2090568066\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.82756662369,2.32381944483), test loss: 4.01997687221\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (10.2455043793,25.7167963205), test loss: 32.396842289\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.04163956642,2.32278669946), test loss: 3.18769525886\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (24.7465362549,25.7043133966), test loss: 34.4683132172\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.02103900909,2.32193876021), test loss: 3.6763853848\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (47.648727417,25.6868080453), test loss: 34.4010027409\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.49131011963,2.32091671807), test loss: 3.1180208981\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (8.21855354309,25.6725263227), test loss: 33.4115262985\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.71231794357,2.32003719529), test loss: 4.20434494913\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (36.3590316772,25.6600102484), test loss: 32.1564044476\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (4.40087556839,2.31922445715), test loss: 4.0819301188\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (15.6659460068,25.6414182539), test loss: 39.8418259144\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.02042245865,2.31831230194), test loss: 4.2331153065\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (9.03190994263,25.6261295423), test loss: 30.0745061874\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.43406510353,2.31745013059), test loss: 4.13555211127\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (31.9005184174,25.6138344895), test loss: 36.2380194187\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.19868659973,2.31623281951), test loss: 3.23335037827\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (23.213098526,25.6013519381), test loss: 31.4035552502\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.17028415203,2.31531056904), test loss: 3.96003866792\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (24.9177780151,25.5844792295), test loss: 37.5634086132\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.03714489937,2.31439849738), test loss: 3.26312637925\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (18.5845813751,25.5689647493), test loss: 32.0944229603\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.27484440804,2.3135185908), test loss: 4.18193222284\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (23.9131202698,25.5562334868), test loss: 32.9838310719\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.00332999229,2.31255887345), test loss: 3.51081824899\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (16.7323970795,25.538930407), test loss: 31.4466290474\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.40413403511,2.31181479037), test loss: 3.6963885814\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (27.0557670593,25.5240630928), test loss: 30.5256706238\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.52293205261,2.31087680243), test loss: 3.11466717124\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (7.72854232788,25.5109381164), test loss: 35.2851244211\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (5.87789916992,2.30996149561), test loss: 4.04737916887\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (22.4290542603,25.499738949), test loss: 33.5525115013\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (5.13673877716,2.30908386943), test loss: 3.78099352121\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (36.6398468018,25.4852891202), test loss: 34.949643898\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.97533512115,2.30803720407), test loss: 4.28252899945\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (17.3828010559,25.4690460781), test loss: 30.133105135\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.31333982944,2.3070758921), test loss: 4.13910569251\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (15.1375846863,25.4559176771), test loss: 35.8284828663\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.95208859444,2.30622589441), test loss: 3.10048600435\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (24.3121299744,25.4394994337), test loss: 31.2015628338\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.54406404495,2.3053152708), test loss: 4.00162469596\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (31.3194179535,25.4239696836), test loss: 36.5802559614\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (4.94086742401,2.3045468029), test loss: 3.53294717073\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (35.6661262512,25.4125434991), test loss: 31.8804128647\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.04181194305,2.30359259031), test loss: 4.05900219083\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (32.6413574219,25.3988996506), test loss: 32.7636631489\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.9065772295,2.30265452215), test loss: 3.2911449492\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (31.7402534485,25.3853937382), test loss: 34.8840178967\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.93713212013,2.30180462992), test loss: 3.73025098145\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (18.5182647705,25.3685561601), test loss: 31.2079627991\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.69854998589,2.3008516788), test loss: 3.16360577345\n",
      "\n",
      "MC # 2, Hype # hyp1, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold5/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (49.2720870972,inf), test loss: 34.0516546726\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.74446415901,inf), test loss: 3.15594932735\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (23.7761688232,26.7885805216), test loss: 38.4508241177\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.82493305206,2.48432842338), test loss: 4.01482050717\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (53.6131019592,26.6838404152), test loss: 39.1290807247\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.0751285553,2.47490015247), test loss: 3.54442421943\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (20.1798667908,26.6517255785), test loss: 34.1177659512\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.25589001179,2.46844227619), test loss: 3.87172679603\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (7.72814416885,26.6741914957), test loss: 33.0794898033\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.56514799595,2.46561870518), test loss: 3.68996005654\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (47.0296936035,26.7010547345), test loss: 35.9983131409\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.03858232498,2.46421535851), test loss: 3.0884181574\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (26.2243671417,26.7009512808), test loss: 34.608324337\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.00535535812,2.46180773053), test loss: 3.93601707816\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (17.1678371429,26.655092701), test loss: 36.3371237755\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.61835062504,2.45767351138), test loss: 3.24314837754\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (11.610118866,26.6231259187), test loss: 35.3265812516\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.73262524605,2.45793375921), test loss: 3.74001294971\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (15.7853336334,26.6168751143), test loss: 31.6989925385\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.15737473965,2.45627163693), test loss: 3.1551246196\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (4.38547134399,26.61055179), test loss: 31.5468845844\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.76506137848,2.45701137608), test loss: 3.86844809651\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (20.1006813049,26.5756631142), test loss: 34.3391210079\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.27486622334,2.45568922169), test loss: 3.59655930996\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (15.8951234818,26.5424152437), test loss: 35.7555532455\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.20972108841,2.45458685142), test loss: 3.85355494916\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (17.2497634888,26.5421845687), test loss: 35.932400322\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (5.61599779129,2.45377114842), test loss: 3.72692393064\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (7.03097915649,26.5297730473), test loss: 35.5628447056\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.06189751625,2.45198925748), test loss: 3.07806090117\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (13.5228281021,26.523844067), test loss: 37.2895369768\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.802513659,2.45137635173), test loss: 3.88447434902\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (19.9400520325,26.5050146872), test loss: 34.2844001293\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.3716404438,2.4495995607), test loss: 3.17645438313\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (43.9403076172,26.4828397921), test loss: 36.5749845266\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.76027905941,2.44800764309), test loss: 3.81854833364\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (26.8676452637,26.4630495436), test loss: 34.8919907093\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.71226763725,2.44735091488), test loss: 3.14039085209\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (28.1741409302,26.4542058004), test loss: 33.1700523853\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.50490736961,2.4463901678), test loss: 3.89884832501\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (47.1394538879,26.4353527753), test loss: 46.0631064415\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.24441552162,2.44581610907), test loss: 4.04181234241\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (26.1941356659,26.4111115844), test loss: 41.0934687614\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.79915750027,2.4450124475), test loss: 3.9210814029\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (26.5837631226,26.3910321821), test loss: 35.6071606636\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.32719659805,2.44382298298), test loss: 3.85869464278\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (55.9973258972,26.3815722809), test loss: 40.1285932064\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.08131122589,2.4426257684), test loss: 2.81689758897\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (7.82423877716,26.3693609991), test loss: 33.8716300964\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.59495353699,2.44168952974), test loss: 3.86808822155\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (22.864944458,26.3581071183), test loss: 34.6898927689\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.88675045967,2.44039700531), test loss: 3.04485078901\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (9.22796440125,26.3368513618), test loss: 37.3453390598\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.83529353142,2.43883947913), test loss: 4.1786007911\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (25.9790554047,26.3171794517), test loss: 34.3067276478\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.93429374695,2.43805473801), test loss: 3.00104389936\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (16.7907981873,26.3035691503), test loss: 34.2068574905\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.052339077,2.43693246932), test loss: 4.06487738788\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (9.00613975525,26.2912468343), test loss: 33.7317194939\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.49733376503,2.43639149263), test loss: 3.84362649322\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (38.9128227234,26.2696411391), test loss: 34.9810511589\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.93714773655,2.43535484703), test loss: 3.22896569967\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (33.3157348633,26.2485360824), test loss: 35.0550721169\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (4.22663974762,2.43452530523), test loss: 4.1569029212\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (19.8645076752,26.2365398797), test loss: 36.0332089424\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.83264029026,2.43343776437), test loss: 2.94169094861\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (16.3261947632,26.2244747425), test loss: 35.6635784626\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.80391478539,2.43222152136), test loss: 3.9195745945\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (12.4993801117,26.2117567945), test loss: 37.7796306133\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.740192055702,2.43135105542), test loss: 3.04091748297\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (39.0731811523,26.1973802647), test loss: 35.3562894821\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.63073301315,2.43016872065), test loss: 3.64627479017\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (5.04608726501,26.1775057154), test loss: 32.6161478996\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.27104866505,2.42881148336), test loss: 3.12573376596\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (21.4772567749,26.1599791942), test loss: 33.5503550053\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.986656367779,2.42796217702), test loss: 4.0214753747\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (15.0860614777,26.1498117889), test loss: 36.9004494667\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.51744127274,2.42719605201), test loss: 3.67693561316\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (67.0808105469,26.1339986669), test loss: 52.4562046528\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.13952541351,2.42642166924), test loss: 3.53502855003\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (25.5870246887,26.1134594601), test loss: 34.1544576168\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.64455389977,2.42553973439), test loss: 3.88381786942\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (21.3192386627,26.0957425764), test loss: 35.0798517942\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.61942815781,2.42442163879), test loss: 2.84344238043\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (30.5748119354,26.0839893483), test loss: 36.1341704369\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.44608104229,2.42338324421), test loss: 3.97427191734\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (4.5001912117,26.0705753779), test loss: 34.3605385065\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.82615017891,2.42245585682), test loss: 3.11333317459\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (33.670917511,26.0588033388), test loss: 32.6566165924\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.88247680664,2.4214860125), test loss: 3.83503608704\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (66.6935577393,26.0424320485), test loss: 32.5644450188\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.0171251297,2.42015597241), test loss: 3.65584719181\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (13.9148292542,26.0232413313), test loss: 35.218821764\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.51915740967,2.41916016474), test loss: 3.83719256818\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (10.1263484955,26.0093580448), test loss: 35.679215765\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (6.91119098663,2.41832626234), test loss: 3.94941769242\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (10.9212265015,25.9953990668), test loss: 35.4534041405\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.08916473389,2.41743577574), test loss: 3.19529007077\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (15.7859697342,25.9771834205), test loss: 34.1101188779\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.87568724155,2.4165210983), test loss: 3.95171437263\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (10.1721763611,25.9592654444), test loss: 33.5543384075\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.430824637413,2.41561635965), test loss: 3.35827180892\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (12.19099617,25.9461766351), test loss: 35.0386972427\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.64878082275,2.41471537396), test loss: 3.85109576583\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (23.3971366882,25.9340828884), test loss: 34.0321513653\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.66636931896,2.4135565715), test loss: 3.16245729327\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (15.9533729553,25.9209544874), test loss: 35.8381148338\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.933988034725,2.41263983521), test loss: 3.75722621381\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (24.3019371033,25.907374987), test loss: 36.7479709387\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.2325398922,2.41163404325), test loss: 3.507269907\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (10.5133924484,25.8894750842), test loss: 35.8650546789\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.12181830406,2.41045466689), test loss: 3.87718693018\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (47.0726013184,25.8747706552), test loss: 33.101485014\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.53348600864,2.40957915969), test loss: 3.99667993188\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (23.2876625061,25.8618979226), test loss: 40.2527290821\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.22041606903,2.40880184816), test loss: 3.04264568388\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (44.0722122192,25.8475121575), test loss: 37.4761903286\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.632992863655,2.40791535274), test loss: 3.99458011389\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (43.1349411011,25.8300022554), test loss: 34.2374236107\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.811612665653,2.40708588695), test loss: 3.20320138633\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (9.65365028381,25.8120472547), test loss: 34.7782040834\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.4988181591,2.40604730295), test loss: 3.71063188314\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (52.434589386,25.8016851364), test loss: 32.5365797997\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.704236745834,2.40504669103), test loss: 3.15508846939\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (48.7754554749,25.7877208865), test loss: 31.3581889153\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.13412427902,2.40417401109), test loss: 3.89093193412\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (14.3468809128,25.7752755801), test loss: 34.7343406677\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.349794059992,2.40324948357), test loss: 3.64307821989\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (56.6145019531,25.7604594424), test loss: 35.8145214081\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.19255137444,2.40210492038), test loss: 3.99953219146\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (38.8224182129,25.7426136248), test loss: 41.0564957142\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.809191763401,2.40110506948), test loss: 3.82117584348\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.5959911346,25.729189073), test loss: 38.8938082218\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.44862627983,2.40035225997), test loss: 3.04197940677\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (25.1001663208,25.7153407037), test loss: 32.9368651152\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.63368618488,2.39942889872), test loss: 3.94227686226\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (22.0374107361,25.6992358096), test loss: 34.2907492638\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.03324198723,2.39858356366), test loss: 3.12614025772\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (41.9428329468,25.6829453823), test loss: 36.4258813381\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (5.51793718338,2.39777613116), test loss: 3.85831982493\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (12.2884101868,25.6691585003), test loss: 34.1615683079\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.34816598892,2.39685372951), test loss: 3.00361657739\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (16.7970104218,25.6573820793), test loss: 31.8032959938\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.60801553726,2.39572929891), test loss: 3.95009666681\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (25.7367362976,25.6443335151), test loss: 35.9536543846\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.98223733902,2.39492435025), test loss: 3.85541615635\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.9079799652,25.6310278382), test loss: 37.3434555531\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (4.24414920807,2.39400610811), test loss: 3.76394548416\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (49.2209320068,25.615703631), test loss: 35.1520267725\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.00547266006,2.3928310693), test loss: 3.86186149716\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (30.2922706604,25.6008422916), test loss: 36.5249135017\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.566814184189,2.39200839758), test loss: 3.08922265172\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (18.1053161621,25.587828384), test loss: 37.5353398323\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.12748539448,2.39118387828), test loss: 3.84130157828\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (18.3562355042,25.5741475231), test loss: 47.2475576401\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.841387987137,2.39035996918), test loss: 3.34033231437\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (26.7065734863,25.5583366338), test loss: 33.5585801125\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.68147754669,2.38958624659), test loss: 3.67010352015\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (18.6758899689,25.5415815829), test loss: 34.3985367298\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.60325539112,2.38854002532), test loss: 3.02880582213\n",
      "run time for single CV loop: 1274.59100103\n",
      "\n",
      "MC # 2, Hype # hyp2, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold1/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (31.2179374695,inf), test loss: 33.804545927\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.09244084358,inf), test loss: 2.94566226602\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (42.5649414062,27.3567732511), test loss: 27.3722434759\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.02279400826,2.59165276402), test loss: 2.95657344162\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (11.5277051926,27.3224895205), test loss: 28.521096611\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.46047353745,2.59222116137), test loss: 2.8095482409\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (37.077960968,27.3749057678), test loss: 27.6661496639\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.38012254238,2.58077014089), test loss: 3.223615098\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.9842014313,27.3833199369), test loss: 32.1770327091\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.949611485004,2.57877980891), test loss: 2.85607892275\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (17.6393947601,27.3694355526), test loss: 29.0929445267\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.55328893661,2.57615513232), test loss: 3.2776655674\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (25.3837394714,27.3835095525), test loss: 31.1063343525\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.92226612568,2.57572034377), test loss: 3.427212888\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (44.4469146729,27.3440921032), test loss: 29.8649745464\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.59936976433,2.57631356772), test loss: 3.13709124327\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (6.84897136688,27.3244469846), test loss: 30.2568965912\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.06269216537,2.57490631684), test loss: 3.13944184184\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (31.425201416,27.3060266173), test loss: 31.7373536587\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.86731159687,2.57205060322), test loss: 2.95945097208\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (27.5581283569,27.3039277713), test loss: 29.5498425007\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.98087596893,2.57119436558), test loss: 3.3065500021\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (23.2535171509,27.2836110417), test loss: 31.907693243\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.39220547676,2.56967231846), test loss: 2.77648594081\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (29.9128074646,27.2915189858), test loss: 28.8171696424\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.40455389023,2.56932700966), test loss: 3.26995170414\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (28.0424251556,27.2484507736), test loss: 33.500919342\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.84705018997,2.56903800569), test loss: 3.01512242556\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (30.8080368042,27.2390461181), test loss: 30.8224352598\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.29687380791,2.56711987319), test loss: 3.3060356915\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (8.37404060364,27.2175695368), test loss: 36.4908372402\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.28296029568,2.56574304152), test loss: 2.93721153438\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (42.3445701599,27.2070284982), test loss: 30.1384670973\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.17465400696,2.56433481967), test loss: 3.13834882379\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (27.0125370026,27.193028378), test loss: 29.3934642315\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.95861577988,2.56344961964), test loss: 3.01428463161\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (65.6342544556,27.1888802093), test loss: 41.1491687775\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.55320358276,2.56284652629), test loss: 3.09606671929\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (17.7019538879,27.1489528874), test loss: 28.3298803329\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.17335414886,2.56159534906), test loss: 2.90569880605\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (11.491979599,27.1412585143), test loss: 31.1451127052\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.14940667152,2.56079550737), test loss: 3.38198339343\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (12.0774564743,27.1268153429), test loss: 32.0989435196\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.885233879089,2.55895776288), test loss: 3.18551023006\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (12.1731090546,27.1104445402), test loss: 33.393355298\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.16002082825,2.55805205227), test loss: 3.14466022253\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (27.6548633575,27.1019474435), test loss: 30.9177369118\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.13677835464,2.557027397), test loss: 3.31717690825\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (19.0383338928,27.0838415514), test loss: 30.3620201826\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.64768457413,2.5560668786), test loss: 3.15685996264\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (8.18719005585,27.0594194073), test loss: 33.2591563225\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.74596786499,2.55544727516), test loss: 3.13855102658\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (30.6493034363,27.0512999343), test loss: 31.7478515148\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.99280011654,2.55400504168), test loss: 2.80401205719\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (27.0882377625,27.0354985358), test loss: 30.2347255707\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.979877591133,2.55311622691), test loss: 3.25250648856\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (51.8955307007,27.0208416938), test loss: 37.7519617796\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.33454942703,2.55228001652), test loss: 2.74698956162\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (21.2997131348,27.0101265792), test loss: 27.9612943172\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (5.1684346199,2.5512672807), test loss: 3.1388009131\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (44.6288375854,26.9917439739), test loss: 32.9969176769\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.27462005615,2.55036293773), test loss: 2.98562401086\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (19.6956787109,26.9687672809), test loss: 29.6150114059\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.88088011742,2.54960057111), test loss: 3.22591159046\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (27.7957305908,26.9585423105), test loss: 32.2497220993\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.06814479828,2.54824784382), test loss: 2.94215613306\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (15.8638648987,26.9426322445), test loss: 28.348289299\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (5.57341432571,2.54735323236), test loss: 3.0489248991\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (27.0254859924,26.9284844727), test loss: 29.3884563446\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.64063024521,2.54628829477), test loss: 2.77681671381\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (21.4150161743,26.9173254299), test loss: 30.1750641346\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.575761437416,2.54513656587), test loss: 3.54774811566\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (19.3406600952,26.8990940824), test loss: 31.9796575069\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.08106803894,2.54459613974), test loss: 2.92733432055\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (10.3565750122,26.8804595324), test loss: 29.0654799938\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.0404586792,2.54379118047), test loss: 3.2995601356\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (17.2900333405,26.8677222813), test loss: 30.9231862545\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.72603321075,2.54248379268), test loss: 3.15806764066\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (18.4281921387,26.8529690214), test loss: 31.0988159657\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.38574361801,2.54149685234), test loss: 3.22132782638\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (16.6367912292,26.8394874301), test loss: 31.3460861206\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.36971950531,2.54037344075), test loss: 3.27696776092\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (9.65619468689,26.8275429912), test loss: 32.3055105448\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.84960460663,2.53961314266), test loss: 3.14271912277\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (19.9753513336,26.8088327135), test loss: 35.193184948\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.29180765152,2.53886560599), test loss: 3.2477940172\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (8.25074958801,26.7932952936), test loss: 31.300359416\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.9727191925,2.53778264288), test loss: 2.64663648009\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (21.7558307648,26.7776393691), test loss: 29.9432642937\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.9161067009,2.5367011494), test loss: 3.23808628321\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (47.1689224243,26.7631251745), test loss: 33.1196859837\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.03950333595,2.53564697206), test loss: 3.01698555052\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (19.1641101837,26.749707034), test loss: 32.1189747095\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.61078190804,2.53472680202), test loss: 3.57245854139\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.4312076569,26.7384667126), test loss: 46.1059276581\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.796947002411,2.53382355863), test loss: 3.20073098242\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (37.2793922424,26.7169358954), test loss: 30.2176104784\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (4.99922418594,2.53309010492), test loss: 3.14758620262\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (41.9893302917,26.7042584101), test loss: 39.1047148705\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.68372774124,2.53206505157), test loss: 3.05767917335\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (21.1176948547,26.6906906331), test loss: 28.3460770607\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (7.60714626312,2.53108288307), test loss: 2.90401085913\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (28.9559001923,26.674349861), test loss: 30.3756543159\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.4134850502,2.52991197541), test loss: 2.83710561097\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (9.04013252258,26.661554623), test loss: 27.3003191471\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (5.03674221039,2.52903689973), test loss: 3.33894603848\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (18.8998298645,26.6471692935), test loss: 31.2582202435\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.60519790649,2.52823403208), test loss: 2.91644676626\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.9279384613,26.6292472039), test loss: 29.7578282833\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.996878504753,2.52728924018), test loss: 3.20781800747\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (30.0951690674,26.616919294), test loss: 30.8247580528\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (6.11714601517,2.52621129255), test loss: 3.14658929259\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (37.540763855,26.6042814668), test loss: 30.185666275\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (5.24517774582,2.52531134642), test loss: 3.18908475637\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (15.6795377731,26.5879732653), test loss: 37.6964053154\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.466579139233,2.52426303747), test loss: 3.13403054029\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (17.929019928,26.5755689494), test loss: 36.8399812698\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.63679468632,2.52327732646), test loss: 2.84675996006\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (22.2929992676,26.5599167612), test loss: 29.2521410227\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.759398579597,2.52249937333), test loss: 3.20285065174\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (25.6103515625,26.5431123382), test loss: 32.3967382908\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.47230672836,2.52170783193), test loss: 2.7686367631\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (9.02572441101,26.5309730111), test loss: 27.7173862934\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.01998996735,2.52056342623), test loss: 3.21699887812\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (26.7378768921,26.5177504517), test loss: 35.0541922569\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.30768871307,2.51976156699), test loss: 3.01414317787\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (31.0251083374,26.5021416659), test loss: 30.2441452026\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.98798513412,2.51876360567), test loss: 3.22663174272\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (37.5480651855,26.4905958698), test loss: 32.3588637352\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.75471639633,2.51772052439), test loss: 3.06294390857\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (23.6072864532,26.4740078655), test loss: 27.1921511412\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.63558077812,2.51702111236), test loss: 2.93267456293\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (14.6747989655,26.4574646382), test loss: 29.5460968494\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.12847185135,2.5162262263), test loss: 2.89979992211\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (54.4646873474,26.4461539743), test loss: 26.8820152521\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.43046689034,2.51507746994), test loss: 3.0386438489\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (31.8023986816,26.4322401299), test loss: 34.3389057636\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.26437878609,2.51408072839), test loss: 2.9661529243\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (20.9910888672,26.4176074111), test loss: 29.7549903393\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.11673164368,2.51311001273), test loss: 3.27272932529\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (36.1361274719,26.404464507), test loss: 35.0171192646\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.22296524048,2.51218305536), test loss: 3.57033120096\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (17.4970283508,26.3887699829), test loss: 33.0906041265\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.77918946743,2.51149375468), test loss: 3.2291259557\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (20.8667907715,26.3741160917), test loss: 31.2197399139\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (5.66326713562,2.51062607614), test loss: 3.2203299582\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (17.1178436279,26.3601405795), test loss: 31.0741921186\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.88914966583,2.50954978291), test loss: 3.30092227757\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (34.0268058777,26.3474936898), test loss: 32.071243\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.30431127548,2.50855259961), test loss: 3.20418367684\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (27.1428852081,26.3328982695), test loss: 32.0823131084\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.41586863995,2.50758360212), test loss: 2.71225299239\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (9.4345998764,26.3204455777), test loss: 29.8820259094\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.02158367634,2.5067430784), test loss: 3.26790773273\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (23.5442619324,26.3035848253), test loss: 30.2476965427\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.48535609245,2.50603444218), test loss: 2.9509829402\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (39.7934875488,26.2908950186), test loss: 30.0230340481\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.18061113358,2.50495107384), test loss: 3.17113476992\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (14.8353757858,26.2765429944), test loss: 37.5315258503\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.659141242504,2.50401617349), test loss: 3.00811433196\n",
      "\n",
      "MC # 2, Hype # hyp2, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold2/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (35.1419448853,inf), test loss: 31.7682047367\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.77132725716,inf), test loss: 2.79215827584\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (8.43518447876,26.3752103875), test loss: 31.7807494164\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.74929690361,2.52285673976), test loss: 3.50175326467\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (28.4814987183,26.3357757725), test loss: 30.8079144478\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.36747384071,2.51635207418), test loss: 2.62617909312\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (26.6553153992,26.4100376451), test loss: 31.7324069023\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.31890082359,2.50275759508), test loss: 3.63331859112\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (34.7646446228,26.3879039212), test loss: 34.5162420273\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.51570224762,2.50002942199), test loss: 3.24128544629\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (14.8706245422,26.3081570658), test loss: 31.9182790041\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.18214941025,2.49809331197), test loss: 3.67162386775\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (42.2080802917,26.3186210816), test loss: 33.7930599689\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.83454465866,2.49630498602), test loss: 3.73331770897\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (41.6922607422,26.2948784373), test loss: 32.3422360897\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.76065647602,2.49891816532), test loss: 3.01772433817\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (11.4607067108,26.2646421252), test loss: 35.5760816574\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.61610651016,2.49908777592), test loss: 3.64091937542\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (5.33312225342,26.2826863985), test loss: 34.2073539734\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.07652425766,2.49587986547), test loss: 2.70198173821\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (21.8950004578,26.2777080747), test loss: 34.3868148327\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.775357186794,2.4941632973), test loss: 3.81693144441\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (22.0458908081,26.2355414351), test loss: 33.938690567\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.83750748634,2.49222967421), test loss: 2.63810627759\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (20.4011039734,26.2215956804), test loss: 31.9465847492\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.160632565618,2.49119413784), test loss: 3.80457197726\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (19.9668960571,26.1987501783), test loss: 32.851782918\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.878370344639,2.49209995352), test loss: 2.88292494118\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (12.6049156189,26.1766852529), test loss: 32.1836695194\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.84226751328,2.49168705347), test loss: 3.7239046514\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (13.7073612213,26.1844604103), test loss: 31.7704704285\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.41546058655,2.48995207632), test loss: 2.68249631673\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (11.8727340698,26.1793981183), test loss: 31.9844298363\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.61335909367,2.48885401619), test loss: 3.87538175881\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (22.3238258362,26.1511063108), test loss: 30.9006160736\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.10611605644,2.4871100333), test loss: 2.58104994893\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (22.7820911407,26.1338157651), test loss: 31.3677986145\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.76649820805,2.48614251045), test loss: 3.6675639838\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (34.7838363647,26.1098895268), test loss: 31.081213665\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.48752212524,2.48631504541), test loss: 3.28104246557\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (18.612197876,26.0889959828), test loss: 30.8964205265\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.12747812271,2.48545818267), test loss: 3.75005311966\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (34.8385925293,26.0896873713), test loss: 31.6500342369\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.06471681595,2.48406474829), test loss: 3.81991072148\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (20.0480422974,26.0841564261), test loss: 32.4866834164\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.58379006386,2.48339480187), test loss: 3.23146462888\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (16.9012546539,26.0629410698), test loss: 35.5095754623\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.77053308487,2.48188208583), test loss: 3.71126364022\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (48.9422225952,26.0468159953), test loss: 37.3352884293\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.53064978123,2.48089179214), test loss: 2.99840946198\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (13.2106275558,26.0220986586), test loss: 33.3348530054\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.5756200552,2.48054458143), test loss: 3.83365485072\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (21.5229892731,26.002119094), test loss: 34.2309942245\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (5.45985555649,2.47976661265), test loss: 2.54775277674\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (25.9793987274,25.9967855492), test loss: 31.9796588659\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.26556634903,2.47828619564), test loss: 4.37954676151\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (33.5178451538,25.9907096803), test loss: 31.3398118019\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (5.64720821381,2.47774670571), test loss: 2.9178649202\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (19.8260135651,25.973449483), test loss: 32.8841283321\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.995975375175,2.47652377891), test loss: 4.05473995805\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.2956581116,25.9585032533), test loss: 31.7732149124\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.80889701843,2.47568732816), test loss: 2.82908525765\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (27.2013130188,25.9375722158), test loss: 34.688903904\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.92091846466,2.47515300042), test loss: 3.69846379757\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (11.9862804413,25.916527549), test loss: 30.4771234512\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.04519987106,2.47405644791), test loss: 2.67297744453\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (19.6524772644,25.9076795743), test loss: 32.2345984459\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.28163254261,2.47282474881), test loss: 3.61794239283\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (20.6073226929,25.8988124485), test loss: 32.9573813438\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.85302066803,2.47203249894), test loss: 3.30143677592\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (30.2991008759,25.8843233263), test loss: 31.7867954731\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.52906131744,2.47129131009), test loss: 3.99085803032\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (12.4986896515,25.8715071812), test loss: 32.6566185951\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.24689590931,2.47040593481), test loss: 3.70461474061\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (24.057636261,25.8527275271), test loss: 35.2321958065\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.34132432938,2.46980752258), test loss: 3.06778785288\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (9.31784248352,25.8331863459), test loss: 32.3228138447\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.64146351814,2.4686730343), test loss: 3.58304422796\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (19.5796775818,25.8216319584), test loss: 36.0147062302\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.97695446014,2.46756612545), test loss: 2.75140659362\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (63.2788619995,25.8108075082), test loss: 34.2007712126\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.94162893295,2.46667214492), test loss: 3.80786687136\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (27.13489151,25.7961317084), test loss: 33.5759981394\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.41545414925,2.46598574004), test loss: 2.50293574929\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (5.44807434082,25.7853872941), test loss: 32.4512266874\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.47228860855,2.46520329338), test loss: 3.8033970356\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (46.8753585815,25.7679118851), test loss: 34.1508250237\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.3790268898,2.46460787443), test loss: 2.68962945938\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (21.8599147797,25.7512361897), test loss: 32.6407422543\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.87673687935,2.46338708576), test loss: 3.80485689342\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (18.4218635559,25.7376006549), test loss: 36.6352388382\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.747423172,2.46228729176), test loss: 2.70105375648\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (21.4196052551,25.7235569609), test loss: 32.5346339226\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.17962574959,2.46137746434), test loss: 3.73452344537\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (20.3914413452,25.7102865774), test loss: 32.3545868874\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.48385214806,2.46069266285), test loss: 2.70049008727\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (11.2634620667,25.7004566436), test loss: 30.6572080135\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.812347531319,2.459984562), test loss: 3.49354056716\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (27.3282470703,25.6824304678), test loss: 33.0911005497\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.75370717049,2.45941748309), test loss: 3.34317328632\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (41.0117034912,25.6686353425), test loss: 31.7129039288\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.86302232742,2.45816252169), test loss: 3.77844515443\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (16.103225708,25.6555922316), test loss: 37.949348402\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.676970660686,2.45714713224), test loss: 3.47587104142\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (5.61459350586,25.6402779037), test loss: 34.06420784\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.61018490791,2.45612714779), test loss: 3.5383307755\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.9491710663,25.6263746827), test loss: 32.2669331551\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.57418036461,2.45542328766), test loss: 4.00582212508\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (32.1486167908,25.6162642565), test loss: 33.8785976887\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.565294981,2.45477572495), test loss: 2.85987340808\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (33.9993476868,25.5986602953), test loss: 40.6114470005\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.98406553268,2.45421754947), test loss: 3.76420506239\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (34.2191772461,25.5853331098), test loss: 33.22853055\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.42216444016,2.45304236224), test loss: 2.53313914537\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (9.40791797638,25.5746227479), test loss: 39.3411729813\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.25574612617,2.45208514961), test loss: 3.72776088715\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (46.7339248657,25.5593751529), test loss: 30.2720115662\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.03643465042,2.4509694432), test loss: 2.63893698156\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (46.3274307251,25.5447758546), test loss: 33.1807575703\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.78203773499,2.45023150116), test loss: 4.04411891699\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (35.0061569214,25.5329749133), test loss: 32.8813345909\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.99780893326,2.44954235182), test loss: 2.75104554892\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (61.0499725342,25.5154450026), test loss: 36.7838084221\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.90487742424,2.44889855356), test loss: 3.66186763048\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (22.0324249268,25.5033470042), test loss: 32.2667399883\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.40129113197,2.44794648603), test loss: 2.79205195606\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (20.6601810455,25.4940122233), test loss: 35.7632015228\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.06970024109,2.44702722957), test loss: 3.45849666297\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (35.3284797668,25.4789249653), test loss: 31.0785552263\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.24312949181,2.4458709876), test loss: 3.40999461114\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (27.2339096069,25.4636268143), test loss: 36.6168038845\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.43178796768,2.44507856426), test loss: 3.6569272697\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (18.4213638306,25.4514846773), test loss: 36.6046501637\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.557079017162,2.44430215366), test loss: 3.69214659333\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (20.1862297058,25.4333605367), test loss: 32.465492177\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.74450087547,2.44371981591), test loss: 3.82497123778\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (10.9694156647,25.4225368656), test loss: 33.209690237\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.2942609787,2.44277470901), test loss: 3.86202682555\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (37.0702400208,25.4134113236), test loss: 34.0095917702\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (5.97861480713,2.44189072034), test loss: 3.01697718799\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (9.69243812561,25.3992345681), test loss: 36.8163640022\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.48878240585,2.44080336396), test loss: 3.72605827153\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (5.60417175293,25.384705069), test loss: 34.2930872917\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.47569179535,2.44000066299), test loss: 2.75207282603\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (58.1353492737,25.3718575797), test loss: 35.200535202\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.51436853409,2.43923470433), test loss: 3.96221554875\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (31.2505836487,25.3531291992), test loss: 29.2910300255\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.08324623108,2.43853265305), test loss: 2.60211977959\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (2.01785778999,25.3425317565), test loss: 34.1958150864\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (7.56210803986,2.4376279294), test loss: 3.86310444772\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (19.9659996033,25.3326845181), test loss: 33.4370069981\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.80856609344,2.43666900128), test loss: 2.7110030055\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (47.1179122925,25.3196889612), test loss: 31.2818454504\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.7163848877,2.43570560681), test loss: 3.72485698462\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.6871681213,25.3065851988), test loss: 32.0607788563\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.30622410774,2.43484197382), test loss: 2.82270807475\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (53.4033699036,25.2918214505), test loss: 36.244144392\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.87965631485,2.43410735512), test loss: 3.71407079101\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (13.7361021042,25.2738384805), test loss: 33.9799435616\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.97109103203,2.43334579799), test loss: 2.63336448669\n",
      "\n",
      "MC # 2, Hype # hyp2, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold3/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (47.5477371216,inf), test loss: 28.8200160503\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.99998116493,inf), test loss: 2.90646591187\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (65.8871917725,27.2954302921), test loss: 46.5874569893\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.05567789078,2.49866702586), test loss: 3.92567832172\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (39.7079391479,27.1426084242), test loss: 27.7732702732\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.80335271358,2.49424067152), test loss: 3.52660749555\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (28.5839157104,27.0527516201), test loss: 35.3159928799\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.95494842529,2.48872770675), test loss: 3.82464759946\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (31.9032707214,27.1041767422), test loss: 37.0129243851\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.78512024879,2.4848206332), test loss: 3.48700451106\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (12.7215681076,27.0744593879), test loss: 34.1388829708\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.21119213104,2.48099648576), test loss: 2.76764222234\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (32.5444107056,27.0893697197), test loss: 32.8905799627\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.31036520004,2.4797706362), test loss: 3.97533756495\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (6.28695678711,27.0653044384), test loss: 34.0494494438\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.61557531357,2.47759704791), test loss: 3.20650855899\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (26.2919216156,27.0392091634), test loss: 32.5596110821\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.55501580238,2.47693130304), test loss: 3.52047204971\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (21.0013771057,27.0326049022), test loss: 27.3646977425\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.96829783916,2.47710526832), test loss: 2.80580271482\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (9.84571266174,27.0194048513), test loss: 32.9712267876\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.98575520515,2.47695021004), test loss: 3.97208892107\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (26.970998764,26.9874462478), test loss: 28.981346941\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.5763528347,2.47641921169), test loss: 3.51996131241\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (34.5264968872,26.958063723), test loss: 32.6554811478\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.65668582916,2.47631608225), test loss: 3.53104379177\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (21.790813446,26.9492355543), test loss: 32.9184647083\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.61762857437,2.47521251192), test loss: 3.51384620667\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (31.2895927429,26.9362187439), test loss: 33.2770092487\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.17939031124,2.4725935588), test loss: 2.86526556015\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (13.8281326294,26.9192198263), test loss: 33.5669264317\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.966038942337,2.47211413747), test loss: 3.56279903054\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (29.1537475586,26.9089164937), test loss: 29.2127805233\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.67266476154,2.4710302984), test loss: 2.86290643215\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (13.8420534134,26.8917932267), test loss: 34.2476356506\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.1164958477,2.46951373688), test loss: 3.98509914279\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (24.5714302063,26.870318996), test loss: 27.4701239109\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.06759262085,2.46908319207), test loss: 2.6531865567\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (31.5757789612,26.8656329782), test loss: 33.0089186668\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.81411504745,2.46858256532), test loss: 3.80167596638\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (63.7491111755,26.8445627687), test loss: 33.3538533688\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.72040581703,2.46820205279), test loss: 3.66596271992\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (28.8948326111,26.8231763234), test loss: 32.8946801662\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.92664575577,2.46774053804), test loss: 3.33934731483\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (24.7170772552,26.7968786552), test loss: 32.8739220142\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.39811491966,2.46674074107), test loss: 3.61440464258\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (37.0814971924,26.7883088193), test loss: 33.9448661327\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.62904715538,2.46536558859), test loss: 2.96766506732\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (6.47149658203,26.7675553759), test loss: 32.4648049831\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.10139393806,2.46395911208), test loss: 3.62465417981\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (44.09790802,26.7575082367), test loss: 31.0934691429\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.60304546356,2.463004774), test loss: 3.21193621457\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (18.1658401489,26.7409864556), test loss: 33.6758258581\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.14020633698,2.46157028077), test loss: 3.69783965051\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (23.1434288025,26.7194072702), test loss: 27.1869431496\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.669431567192,2.46066880948), test loss: 2.66156815588\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (10.3480405807,26.7099612133), test loss: 33.2659879684\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.98427820206,2.45999455789), test loss: 3.97564085722\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (15.0590286255,26.6914759709), test loss: 29.4275671482\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.568565368652,2.45927983087), test loss: 4.00994954109\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (19.8874340057,26.6727696619), test loss: 33.3564462185\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.23384582996,2.45854470114), test loss: 2.96496506929\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (18.0961666107,26.651718846), test loss: 31.7567002058\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.03828573227,2.45786070846), test loss: 3.95819867849\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (12.4770345688,26.6384419968), test loss: 29.6512112617\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.62023496628,2.4570857889), test loss: 2.88180092871\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (19.9990406036,26.6230495133), test loss: 31.8990476608\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.00926828384,2.45529056129), test loss: 3.61797678173\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (17.6637535095,26.6069262494), test loss: 32.0963009834\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.776284337044,2.45451578282), test loss: 2.8456929177\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (27.6123428345,26.594335399), test loss: 34.0063498974\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.32413291931,2.45338437142), test loss: 3.56618307829\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (11.2106742859,26.5764184726), test loss: 29.7331724167\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.20856761932,2.4523316907), test loss: 3.45622518063\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (50.5202102661,26.5625753837), test loss: 34.6728086472\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.24098324776,2.45157624141), test loss: 4.02767572403\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (10.56042099,26.5468495573), test loss: 29.2622392654\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.52709555626,2.45080470484), test loss: 3.63470721245\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (28.0931625366,26.534135386), test loss: 35.9160893917\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.785464823246,2.45020476184), test loss: 2.98312764764\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (13.7770671844,26.5120014701), test loss: 33.4852053642\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.71554172039,2.44940653827), test loss: 3.72131005228\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (43.8091506958,26.4937451934), test loss: 28.5951380253\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.15009760857,2.4485414608), test loss: 2.87815820277\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (30.1676139832,26.4803383501), test loss: 32.7658435345\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.906924843788,2.44735426532), test loss: 3.65011166334\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (10.9117336273,26.4624558793), test loss: 27.9725270748\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.7797396183,2.4462864331), test loss: 2.86939868629\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (18.6427459717,26.4505246832), test loss: 33.1801763058\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.47568070889,2.44529842577), test loss: 3.88212411404\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (44.6295280457,26.4350355238), test loss: 28.8579258919\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.7371468544,2.4442760925), test loss: 3.39672600627\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (34.7385559082,26.4185663689), test loss: 33.4358873844\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.06391334534,2.44330430177), test loss: 3.84306752384\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (8.06781768799,26.4023820954), test loss: 28.7872754574\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.94144248962,2.44254573199), test loss: 3.70595959425\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (14.719959259,26.3897692041), test loss: 33.1714352131\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.26482200623,2.44184589408), test loss: 3.09203007519\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (26.2738265991,26.3702151542), test loss: 32.0005232811\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.660304963589,2.44087342369), test loss: 3.67335775793\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (18.6934547424,26.3525402514), test loss: 28.8111462116\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.378793269396,2.44016652147), test loss: 3.33192373514\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (6.1603178978,26.3371958584), test loss: 30.8977790833\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (5.10662126541,2.43932956049), test loss: 3.57106015682\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (16.419883728,26.3232797274), test loss: 29.6928915977\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.97577190399,2.4378817608), test loss: 2.96478990912\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (15.6544504166,26.3078845741), test loss: 34.417903614\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.27892720699,2.4370673288), test loss: 3.55831123292\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (18.5521240234,26.2940242492), test loss: 29.7196962357\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.02756357193,2.43609375938), test loss: 3.59460175633\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (10.5789613724,26.2792420796), test loss: 34.1848659992\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.23138093948,2.435058115), test loss: 3.87670319974\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (42.007019043,26.262746413), test loss: 33.5870635033\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.52264428139,2.43422554712), test loss: 3.63553710878\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (17.7212867737,26.2516072259), test loss: 34.1067135334\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.66487383842,2.4335832425), test loss: 2.84906294346\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (21.4897155762,26.233929573), test loss: 38.9358361244\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.884976863861,2.43270961023), test loss: 3.74049419761\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (8.32894992828,26.2168862025), test loss: 29.6404577255\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.76636171341,2.43201445831), test loss: 2.90320285857\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (23.7155380249,26.1983365379), test loss: 31.3628109455\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.6023015976,2.43105954172), test loss: 3.50066457391\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (41.3244895935,26.1862029133), test loss: 31.3605947495\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.84428715706,2.43002728978), test loss: 2.72597955465\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (57.1445884705,26.1705566732), test loss: 33.7832602501\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.926125407219,2.42897709984), test loss: 3.97796806097\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (21.3203525543,26.1559872204), test loss: 27.6933071613\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.01245367527,2.42808848987), test loss: 3.62776858211\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (53.9162712097,26.1430812893), test loss: 34.5375332355\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.15503311157,2.42712353601), test loss: 3.48954122663\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (36.956817627,26.1244603011), test loss: 40.3167519093\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.0929145813,2.42615875887), test loss: 3.54271774888\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.7587814331,26.1124762481), test loss: 33.7559494495\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.56172275543,2.42548565891), test loss: 2.84989972413\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (26.4531993866,26.0958483836), test loss: 30.7055583954\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.35866260529,2.42465386427), test loss: 3.6807320416\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (41.8700790405,26.0804868163), test loss: 32.5388139725\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.13224959373,2.42384404707), test loss: 2.96866868138\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (66.9379348755,26.0639064187), test loss: 37.2460348606\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (5.82186174393,2.42320646194), test loss: 3.50158849955\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.3683547974,26.0483641319), test loss: 28.0738041878\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.87484025955,2.42233146166), test loss: 2.66149724424\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (22.9046134949,26.0346709858), test loss: 32.2700422525\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.68977999687,2.42107563228), test loss: 3.73887188435\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (20.9014358521,26.019129805), test loss: 29.3387942314\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.72359263897,2.4202531366), test loss: 3.67119053602\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.6697502136,26.0068085842), test loss: 33.5804276943\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.2501885891,2.41931396399), test loss: 3.22962731123\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (31.5908050537,25.9906106108), test loss: 32.66839571\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.25956058502,2.41832943757), test loss: 3.59481855333\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (22.136133194,25.9772902119), test loss: 34.8698350906\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.855371952057,2.41757637426), test loss: 3.05695595443\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (25.5785217285,25.9617271064), test loss: 31.6477286816\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (4.40826320648,2.41683891851), test loss: 3.58335985541\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (21.4381809235,25.948524603), test loss: 40.9538080215\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.676255702972,2.4160608817), test loss: 3.05203438699\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (11.0566625595,25.930583907), test loss: 33.7142413139\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.95341277122,2.41529292757), test loss: 3.49962601662\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.39501094818,25.9143481694), test loss: 28.5493813515\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.22820806503,2.41447156678), test loss: 2.69874418676\n",
      "\n",
      "MC # 2, Hype # hyp2, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold4/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (52.1346168518,inf), test loss: 31.6796219826\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.87106513977,inf), test loss: 3.06671670675\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (17.8103199005,27.4664201369), test loss: 34.1186367512\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.3965473175,2.46906331265), test loss: 3.74056956172\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (5.70992565155,27.4714142735), test loss: 31.4959625244\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.15839529037,2.46741796201), test loss: 2.84948775172\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (48.1536521912,27.5728671447), test loss: 35.8927651405\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.53619384766,2.46652265972), test loss: 3.93803564608\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (24.4502372742,27.5436639003), test loss: 33.4037574768\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.11014962196,2.46205469656), test loss: 3.69161795378\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (14.9624109268,27.5458250578), test loss: 34.48636868\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.55101442337,2.46306996386), test loss: 3.45705654621\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (38.2048187256,27.4875721471), test loss: 37.964717865\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.6136071682,2.46019151316), test loss: 3.71419301927\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (26.8344326019,27.5084761093), test loss: 34.5318562269\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.77743530273,2.45920288023), test loss: 3.00870454907\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (99.8613891602,27.4898762113), test loss: 53.5503025055\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.23081302643,2.45943033605), test loss: 4.14678139687\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (26.2376747131,27.4540178003), test loss: 34.5032371044\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.38998603821,2.4589867123), test loss: 3.05591429472\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (6.05499315262,27.4359081903), test loss: 32.753329134\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.9358587265,2.45738795519), test loss: 3.73872147501\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (56.4088973999,27.424928994), test loss: 33.7861492157\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.54294168949,2.45516821005), test loss: 3.01410280466\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (22.8947982788,27.4227149647), test loss: 33.5839098454\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.52156114578,2.45455305756), test loss: 3.69675611258\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (11.803278923,27.3904053603), test loss: 30.1686388016\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.985663890839,2.453169185), test loss: 3.05399646759\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (29.7579727173,27.3719374468), test loss: 35.2362024784\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (0.70270460844,2.45269072902), test loss: 4.2560815841\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (2.97389435768,27.3625812716), test loss: 29.1591786385\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.787966907024,2.45162262273), test loss: 3.79011941552\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (32.2975997925,27.3383386378), test loss: 35.1207631826\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.14209270477,2.45145703694), test loss: 3.36897683144\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (15.883515358,27.314535864), test loss: 32.8565723419\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.31506490707,2.45033502012), test loss: 3.85943335593\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (32.3347549438,27.3065375084), test loss: 34.8777798176\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.67533415556,2.44830780283), test loss: 3.06107419133\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (17.7432537079,27.2964686358), test loss: 31.5552331448\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (0.672719836235,2.44780553534), test loss: 3.76976699233\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (13.3981657028,27.2750139565), test loss: 37.619346571\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.818950772285,2.4468891036), test loss: 3.18962250352\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (11.577829361,27.2577427447), test loss: 33.4021752357\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.869954586029,2.44595363797), test loss: 4.13786923885\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (38.1111335754,27.244064155), test loss: 31.4519124985\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.992787897587,2.444744235), test loss: 3.28996891975\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (33.3358306885,27.2203581451), test loss: 33.9644643545\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.10104608536,2.44418154149), test loss: 3.65558001399\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (49.2351074219,27.199793068), test loss: 33.6397006512\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.29884767532,2.44314385833), test loss: 2.89721133709\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (58.4960861206,27.1888968206), test loss: 40.2697321892\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.82774949074,2.44241399683), test loss: 3.9474960506\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (13.8157806396,27.1757528628), test loss: 34.631460619\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.814805150032,2.44095489529), test loss: 3.67892377377\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (18.9018592834,27.1597824954), test loss: 35.1555927753\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.95307040215,2.4402826769), test loss: 3.50200772882\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (11.1849594116,27.1349150358), test loss: 30.3323890686\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.13279724121,2.43910135833), test loss: 3.86223476231\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (17.0297393799,27.1253994036), test loss: 39.2776591778\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.68691658974,2.43788623917), test loss: 3.14923954606\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (15.4144277573,27.1056382758), test loss: 31.6183381081\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.53444433212,2.43737967817), test loss: 3.77463323474\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (86.253326416,27.0887865473), test loss: 46.0456694603\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (6.75007629395,2.43652118281), test loss: 3.10553449988\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (17.9121322632,27.0734431806), test loss: 33.5475008726\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.2845556736,2.43543402072), test loss: 3.83160922527\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (3.62879371643,27.0572178643), test loss: 31.2277438164\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.36992120743,2.4342680085), test loss: 3.03425415456\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (49.7287826538,27.045299), test loss: 33.8637945652\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.64024829865,2.43338217755), test loss: 3.71190279126\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (60.413734436,27.0270320436), test loss: 30.6586604118\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.48727929592,2.43235420276), test loss: 3.14531023502\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (23.7756614685,27.0076629983), test loss: 37.3445046425\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.64284825325,2.43150566784), test loss: 3.92225604057\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (32.2686920166,26.9968009039), test loss: 29.6651933193\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.38584685326,2.43058115069), test loss: 3.8420394063\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (36.7367248535,26.9748969183), test loss: 35.9052601576\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.90734696388,2.42961631246), test loss: 4.05876017213\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (23.8158226013,26.9558791065), test loss: 31.7515244007\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.755550026894,2.42883349597), test loss: 3.92627379894\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (24.1616210938,26.9452558764), test loss: 35.1019752502\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.87263906002,2.42742341182), test loss: 3.09464915991\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (32.8362541199,26.9305775339), test loss: 32.3986365795\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (4.67683553696,2.42663144843), test loss: 3.89215497971\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (55.0416984558,26.9136200765), test loss: 39.336854887\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.99163007736,2.42571902797), test loss: 3.27149633169\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (55.4244308472,26.895539148), test loss: 34.1139178276\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.03240442276,2.42479636705), test loss: 4.14567655325\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (26.1047286987,26.8821412374), test loss: 32.7280024052\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.72558879852,2.42379784804), test loss: 3.20649282336\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (20.0225410461,26.8630373765), test loss: 32.9255661488\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.4353107214,2.42303979983), test loss: 3.6339402914\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (28.9278717041,26.8445340364), test loss: 31.4805505276\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.39528799057,2.42211288358), test loss: 3.00173845887\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (9.19742202759,26.8314295674), test loss: 36.1723151207\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.59899914265,2.42120571239), test loss: 4.00628894567\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (10.1327152252,26.8180255057), test loss: 36.0934409142\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.32595849037,2.42020112093), test loss: 3.75015223026\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (29.4805297852,26.8045975002), test loss: 35.4805435181\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (5.6980919838,2.41937014728), test loss: 4.27841130197\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.1362247467,26.7839718488), test loss: 29.7557888031\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (4.96380233765,2.41834096544), test loss: 3.91698439121\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (17.5344696045,26.7716371027), test loss: 36.3686822414\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.78347158432,2.41729775037), test loss: 3.10445038378\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (24.9388046265,26.7535997036), test loss: 31.8907515049\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.65585517883,2.41650844673), test loss: 3.78364512324\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (19.1216506958,26.7341030809), test loss: 34.3686721802\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.510708451271,2.41559813066), test loss: 3.47622759938\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (3.12369537354,26.723865964), test loss: 32.6698096752\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.1272778511,2.41474082849), test loss: 3.84848634601\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (10.6750173569,26.7072275236), test loss: 32.1793100834\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.04906368256,2.41371017108), test loss: 3.14368115962\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (28.3497486115,26.6942627379), test loss: 34.7512691021\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.99403166771,2.41291645613), test loss: 3.59556164742\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (52.3894271851,26.6761547723), test loss: 32.8440001011\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.20590782166,2.41197781347), test loss: 3.01488347948\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (7.25618028641,26.6613525036), test loss: 33.4370276928\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.0602478981,2.41111370544), test loss: 4.15423029065\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (37.0004196167,26.648607105), test loss: 32.2745377541\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (4.57846450806,2.41025228675), test loss: 3.8823572576\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (18.5468940735,26.6297300093), test loss: 39.0361857891\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.12913298607,2.40938114698), test loss: 4.10296958685\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (10.3023071289,26.6134539969), test loss: 29.7329892635\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.58782720566,2.40855742454), test loss: 3.90360730886\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (32.8209991455,26.6003985172), test loss: 35.8356363297\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.30074238777,2.40734735042), test loss: 3.16773428917\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (25.2395896912,26.587372271), test loss: 31.8091645241\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.10327005386,2.40646711775), test loss: 3.76443161964\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (27.748878479,26.5698130265), test loss: 37.38691082\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.35075283051,2.40559575571), test loss: 3.22335954607\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (18.5788764954,26.5538295135), test loss: 32.5923214436\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.32602357864,2.40475908007), test loss: 3.95686160922\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (25.4744262695,26.5405846961), test loss: 32.2707056046\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.12746214867,2.40376654624), test loss: 3.38075869083\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (16.2792015076,26.5232799842), test loss: 31.9558908939\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.44680786133,2.4030608584), test loss: 3.60552925169\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (25.7869129181,26.5076206858), test loss: 30.4740119934\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.69554018974,2.40215600839), test loss: 3.00256271362\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (8.04054737091,26.4936297498), test loss: 36.1572899342\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (6.63618040085,2.40126357821), test loss: 4.00196475685\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (23.9256668091,26.4817541682), test loss: 32.2401172638\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (5.29745197296,2.40041165105), test loss: 3.57965555191\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (39.644985199,26.4669936085), test loss: 34.7594978809\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.94774866104,2.39944378147), test loss: 4.19378945231\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (18.8747711182,26.4497765945), test loss: 30.0850340843\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.19174039364,2.39850142305), test loss: 3.92224778235\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (17.4210681915,26.4365727654), test loss: 35.5314277172\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.46501851082,2.397641747), test loss: 3.0761511445\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (22.3870315552,26.4194451101), test loss: 31.5157881737\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.45208668709,2.39673382221), test loss: 3.80052006245\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (35.045211792,26.4033768162), test loss: 36.1980476141\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (4.67115783691,2.39599803706), test loss: 3.30770845413\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (40.549369812,26.3914319524), test loss: 32.2504889965\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.09271502495,2.39507189487), test loss: 3.88872644007\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (30.7669181824,26.3767860423), test loss: 32.5055384159\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.15367984772,2.39413791358), test loss: 3.2512860477\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (31.4557342529,26.3624908828), test loss: 35.6694479465\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.14284038544,2.39332991216), test loss: 3.63239794374\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (19.0812568665,26.3452392021), test loss: 31.1117661476\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.98530912399,2.39245170468), test loss: 3.07931610346\n",
      "\n",
      "MC # 2, Hype # hyp2, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold5/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (50.3150482178,inf), test loss: 32.9836903572\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.06968951225,inf), test loss: 3.08829136193\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (23.0783290863,27.6730986791), test loss: 38.4268188953\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.80223369598,2.5405524711), test loss: 3.96964060068\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (56.7201118469,27.5697857428), test loss: 39.446181345\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.40578198433,2.53351311284), test loss: 3.37708250731\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (22.1892147064,27.5321102192), test loss: 33.7081064939\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.19626104832,2.5273814593), test loss: 3.82724152803\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (9.17656707764,27.5482037965), test loss: 33.0268558979\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.00310325623,2.52506472662), test loss: 3.55989072621\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (48.116230011,27.5752465106), test loss: 35.1049754143\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.04558420181,2.52357549044), test loss: 3.15136776119\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (29.3092536926,27.5735413679), test loss: 34.3945798874\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.983158051968,2.52167260164), test loss: 3.82609372735\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (16.6903438568,27.531405807), test loss: 35.3903938055\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.78246212006,2.51799340353), test loss: 3.2214952141\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (11.4411315918,27.4976051331), test loss: 35.0642507911\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.78604412079,2.51838787562), test loss: 3.65543000996\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (14.6900148392,27.4929052252), test loss: 30.939774847\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.22848212719,2.51674296586), test loss: 3.06681744307\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (4.89140844345,27.4856220857), test loss: 31.6053646564\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.88147509098,2.51740224519), test loss: 3.83556770086\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (17.7957687378,27.4498749796), test loss: 33.4673000336\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.21858274937,2.51592110595), test loss: 3.39424294233\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (17.3342018127,27.4178469628), test loss: 35.3730717182\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.56006455421,2.51516189502), test loss: 3.79097106457\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (19.6312923431,27.4154131775), test loss: 35.3513380051\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (5.81580305099,2.51431426808), test loss: 3.59703565389\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (8.16737556458,27.4023048356), test loss: 35.2006233215\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.37689971924,2.51244470091), test loss: 3.02692970932\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (14.5295639038,27.3950526892), test loss: 37.7259034634\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.973509788513,2.51183297965), test loss: 3.7380628705\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (20.5280323029,27.3762847596), test loss: 33.5179001808\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.39215636253,2.51018196067), test loss: 3.07931812406\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (42.7504386902,27.3548049167), test loss: 35.5764181137\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.635257601738,2.50875122809), test loss: 3.81868485808\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (26.6277160645,27.3335190437), test loss: 33.8419981003\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.89420866966,2.50811728485), test loss: 2.99258665144\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (34.0234260559,27.3256917543), test loss: 32.784397316\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.65086579323,2.50709720005), test loss: 3.88929202557\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (49.966632843,27.3064090911), test loss: 42.9405952454\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.29980432987,2.5064871797), test loss: 3.81138056219\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (30.3857192993,27.2825550727), test loss: 41.0934368134\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.8900911808,2.50568363496), test loss: 3.82977802753\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (27.8584098816,27.2613770212), test loss: 35.6410029888\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.26780509949,2.5045208163), test loss: 3.70463370085\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (62.1048583984,27.2510855501), test loss: 39.6243807793\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.44115567207,2.50341923298), test loss: 2.78792898655\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (7.89987850189,27.237585921), test loss: 33.346298933\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.6640048027,2.5023821789), test loss: 3.79789406061\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (24.309595108,27.2259069346), test loss: 33.8040102959\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.81960368156,2.50125035439), test loss: 2.97375564277\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (8.293217659,27.2045915797), test loss: 36.8838609695\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.68589949608,2.49971306199), test loss: 4.16189652085\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (28.2503681183,27.1846410952), test loss: 33.3730360031\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (4.55589294434,2.49907233441), test loss: 2.89768847823\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (17.1941280365,27.1712122794), test loss: 33.4622780561\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.4152970314,2.49794218508), test loss: 4.0591899097\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (9.03968811035,27.1582092306), test loss: 32.9357696533\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.68136417866,2.49732574195), test loss: 3.69300679564\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (40.5371170044,27.1367688632), test loss: 34.9395742416\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.28414106369,2.49629956206), test loss: 3.21018596292\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (34.8486709595,27.1152224766), test loss: 34.7977537632\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (4.0596036911,2.49550441085), test loss: 3.91872479916\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (21.5698165894,27.1027047112), test loss: 35.7328709602\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.89381635189,2.49451590674), test loss: 2.92637734413\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (19.972776413,27.0897918194), test loss: 35.6078561306\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.90413403511,2.49326725966), test loss: 3.8294441998\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (13.4412374496,27.0763961003), test loss: 37.0575875759\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.665770053864,2.49240572931), test loss: 2.9653340131\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (37.0641326904,27.0621045508), test loss: 35.3532187939\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.90560054779,2.49136658116), test loss: 3.62986164689\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (5.02244234085,27.041965623), test loss: 32.0365603924\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.30909204483,2.49003288708), test loss: 3.10049495697\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (22.8913764954,27.0241644564), test loss: 33.388466835\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.986450195312,2.48924173417), test loss: 3.93382984996\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (15.8512763977,27.0138350613), test loss: 35.6349600315\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.62656784058,2.4884707636), test loss: 3.52554212809\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (67.6085128784,26.99785596), test loss: 50.1450916767\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.42277622223,2.48768203978), test loss: 3.45550785661\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (28.8600006104,26.977713656), test loss: 34.2388699055\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.69576573372,2.48684264428), test loss: 3.73782358766\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (25.3339252472,26.9589001568), test loss: 34.7242089272\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.47661805153,2.4857215871), test loss: 2.83720370531\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (31.9333229065,26.9470135282), test loss: 36.8276027203\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.54255390167,2.48477271497), test loss: 3.8848379612\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (3.51694774628,26.932473806), test loss: 33.3829221725\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.9440997839,2.48382633857), test loss: 3.06078630984\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (32.9379959106,26.9206299684), test loss: 32.698527813\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.73878479004,2.48293652907), test loss: 3.8554459691\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (68.8465881348,26.9042374375), test loss: 32.1178835869\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.73507499695,2.48170018402), test loss: 3.50624052286\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (14.1268587112,26.8843040405), test loss: 34.6100130558\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.44085717201,2.48073036176), test loss: 3.7936642468\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (8.4504365921,26.8709253122), test loss: 33.9974192142\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (7.19351243973,2.47994240999), test loss: 3.79109295011\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.7633342743,26.8564301733), test loss: 34.9596878529\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.25839018822,2.47902790326), test loss: 3.15523908734\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (16.8224086761,26.8384051598), test loss: 33.9589141369\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.23030757904,2.47812434585), test loss: 3.7994291842\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.7007198334,26.8200987932), test loss: 33.0772463322\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.431379437447,2.47724413018), test loss: 3.20607354194\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (14.1459341049,26.8065596628), test loss: 34.383499527\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.91379880905,2.47639430647), test loss: 3.76488200128\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (24.3848304749,26.793876566), test loss: 33.2397192955\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.81830191612,2.47525556269), test loss: 3.05625637174\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (16.9387550354,26.7802696462), test loss: 35.7648216724\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.35579621792,2.47439876739), test loss: 3.7638399899\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (28.4089565277,26.766588273), test loss: 35.7098648548\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.26353621483,2.47345662855), test loss: 3.35441005528\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (10.7193698883,26.7483060202), test loss: 35.0186305523\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.12911963463,2.47233865246), test loss: 3.88127616942\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (48.5867729187,26.7332960326), test loss: 32.2261481762\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.84776425362,2.47150612743), test loss: 3.78039600551\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (24.8834838867,26.7202562083), test loss: 38.9361612797\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.15692639351,2.47072131588), test loss: 3.03829445839\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (38.678024292,26.705658488), test loss: 38.4301908493\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.641053855419,2.46986634249), test loss: 3.85516477227\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (42.7388648987,26.6880787718), test loss: 33.3802974224\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.780682861805,2.46903186792), test loss: 3.13418554664\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (7.9990105629,26.6696147945), test loss: 34.4943890095\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.60897684097,2.46804954069), test loss: 3.62292590141\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (51.5869216919,26.6588004481), test loss: 32.2949424744\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.73125821352,2.46709161941), test loss: 3.03523247242\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (53.0372009277,26.6442127796), test loss: 31.9544621468\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.08481371403,2.4662179347), test loss: 3.9650334239\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (15.649646759,26.6316499655), test loss: 34.0375428677\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.554135620594,2.46538342533), test loss: 3.48580127358\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (60.2408676147,26.6165613457), test loss: 35.5344177246\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.37181949615,2.46427622057), test loss: 3.9253099978\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (42.4846801758,26.5984733174), test loss: 39.6132730007\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.801686048508,2.46332522584), test loss: 3.64934771657\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (14.8263053894,26.5848252319), test loss: 37.2802582264\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.69094371796,2.46259585467), test loss: 3.04907006919\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (26.4943962097,26.570720625), test loss: 32.6859737158\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.98570108414,2.46165696329), test loss: 3.8158567071\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (22.914276123,26.5546292057), test loss: 33.5984280586\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.19475030899,2.46082969138), test loss: 3.03406248093\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (40.8047103882,26.5376838981), test loss: 35.9302011728\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (5.00753736496,2.46001396114), test loss: 3.74414279461\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.899055481,26.5239054012), test loss: 33.6124233723\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.35636091232,2.45915843017), test loss: 2.89496331215\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (17.4299697876,26.5113800126), test loss: 31.8456598282\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.80239903927,2.45803244343), test loss: 3.9106108427\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (24.3507804871,26.4980608525), test loss: 34.696957922\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.68810987473,2.45724636369), test loss: 3.7360097751\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.908208847,26.4846081722), test loss: 37.1722352028\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (4.14051818848,2.45638717325), test loss: 3.73321199417\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (48.9995040894,26.4687932401), test loss: 35.082391715\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.80954480171,2.45522666318), test loss: 3.75265046358\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (29.6686534882,26.453973281), test loss: 36.1772274494\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.560470342636,2.45444702668), test loss: 3.06438794732\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (16.7934074402,26.4405053575), test loss: 37.4252598047\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.10765910149,2.45359968154), test loss: 3.74011167288\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (18.059381485,26.426836686), test loss: 47.0238725662\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.760405898094,2.45279298395), test loss: 3.24578118026\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (27.1309776306,26.4107953189), test loss: 33.6319448471\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (4.01389217377,2.45201925534), test loss: 3.62266238332\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (17.2423629761,26.3935796919), test loss: 33.6751581192\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.73745727539,2.45099892509), test loss: 2.92272110581\n",
      "run time for single CV loop: 1310.12842703\n",
      "\n",
      "MC # 3, Hype # hyp1, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold1/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (46.0441856384,inf), test loss: 37.2828406334\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.59906411171,inf), test loss: 2.98910023123\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (36.9370727539,26.6691129336), test loss: 36.6278371572\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.627881765366,2.49006437641), test loss: 3.74125205874\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (52.0690917969,26.5239380589), test loss: 37.2301651239\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.94680035114,2.48342607704), test loss: 2.81433323473\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (5.94942712784,26.5293597372), test loss: 38.2750736713\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.67409992218,2.48099729431), test loss: 4.0238129437\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (23.6140613556,26.5858982506), test loss: 40.8599861145\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.09171056747,2.47798987895), test loss: 3.59835814238\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (49.1379165649,26.5521480225), test loss: 42.8024516106\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.4396982193,2.47073363422), test loss: 3.43695387542\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (14.9654121399,26.4907515231), test loss: 37.9011864662\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.58097064495,2.46824529068), test loss: 3.80611611009\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (15.5995168686,26.4929609218), test loss: 39.7987915993\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.567838907242,2.46744852), test loss: 3.11320053935\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (21.3159599304,26.4629029298), test loss: 34.9761795521\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.07530558109,2.46883706247), test loss: 3.91886619925\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (27.9443778992,26.4320857002), test loss: 36.1968528748\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.88908553123,2.46826245443), test loss: 2.80380452573\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (35.0875320435,26.4383909972), test loss: 35.4637329102\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.41839885712,2.46638762487), test loss: 4.08923224211\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (29.4913597107,26.4397059864), test loss: 37.5109447002\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.1305847168,2.46511426505), test loss: 2.99730090201\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (7.88113069534,26.4048552481), test loss: 39.4725786686\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.46053099632,2.46261637562), test loss: 3.88275893927\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (48.7274856567,26.384541419), test loss: 40.6741492271\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.53947234154,2.46177276163), test loss: 3.12227142751\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (49.8802642822,26.3723952426), test loss: 37.0862543106\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.23430204391,2.46076472976), test loss: 3.95354318321\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (44.6317710876,26.3535522972), test loss: 34.6433471203\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.20222258568,2.46105399299), test loss: 3.45267006755\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (22.0103855133,26.3254958304), test loss: 38.7286560297\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.93541836739,2.46009561055), test loss: 3.91551613808\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (16.8105010986,26.3179460751), test loss: 34.576680994\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.946207880974,2.4581075204), test loss: 3.81707210839\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (7.53204917908,26.3155532894), test loss: 40.9663394928\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.802362918854,2.45727432264), test loss: 3.12199943066\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (43.8301467896,26.2980008725), test loss: 36.4838667154\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.15214300156,2.45594023503), test loss: 3.94269673228\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (23.6338100433,26.2681742417), test loss: 41.2293368816\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.46291863918,2.45504921311), test loss: 2.92899306864\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (16.9114208221,26.258011685), test loss: 32.8171731472\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.4035269022,2.45395522036), test loss: 3.9654060632\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (15.468334198,26.2401622475), test loss: 38.5780026436\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.477165311575,2.45361553065), test loss: 2.9811145246\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (28.4771175385,26.2191498055), test loss: 35.7236253262\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.48582053185,2.4529132796), test loss: 3.86381351948\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (23.876499176,26.2121742771), test loss: 36.1367982864\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.80457782745,2.45160147816), test loss: 3.03852211237\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (32.4433021545,26.2051069777), test loss: 35.9092605114\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.80175495148,2.45068460319), test loss: 3.79834581017\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (23.608335495,26.1821662288), test loss: 34.3129030466\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.0699596405,2.44884463008), test loss: 3.23123756498\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (22.3587551117,26.1630040876), test loss: 40.2739964008\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.55617690086,2.44808996097), test loss: 4.44580190182\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (3.30613088608,26.1505971855), test loss: 35.1336954117\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.952120006084,2.44730989494), test loss: 3.90030350983\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (12.0035400391,26.1318925049), test loss: 39.1325123787\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.26941561699,2.44690918126), test loss: 3.27419918776\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (23.7380161285,26.1145985535), test loss: 38.3597003937\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.54883551598,2.44604753651), test loss: 3.81745225787\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (31.0204143524,26.1044228627), test loss: 40.484229517\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.02844238281,2.44455984643), test loss: 2.90957585573\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (11.4395332336,26.0935968793), test loss: 34.4249127388\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.844162523746,2.44355244379), test loss: 3.91145298481\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (51.241104126,26.0752514443), test loss: 36.627611351\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.73574542999,2.44231452263), test loss: 2.99756240249\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (53.4350280762,26.0571106305), test loss: 35.4599734783\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.84027647972,2.44150510457), test loss: 4.32630863786\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (32.1780319214,26.045834451), test loss: 37.8749239922\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.51687335968,2.44065146602), test loss: 3.20446552038\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (21.6110801697,26.0267931219), test loss: 39.13239851\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.10192680359,2.44009517556), test loss: 3.63176714182\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (14.384557724,26.0082340161), test loss: 36.4220327854\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.25729608536,2.43916859165), test loss: 2.9058700949\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (26.3818721771,25.9977455687), test loss: 35.753038168\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.18696403503,2.43795849611), test loss: 3.74439417124\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (31.5863609314,25.9903412429), test loss: 34.1095946789\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.75854539871,2.43709132366), test loss: 3.47173148394\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (12.0229301453,25.9691173979), test loss: 37.9047171593\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.20269846916,2.43568035568), test loss: 3.94697671235\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (15.1054182053,25.9513805646), test loss: 37.2368033409\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.72419500351,2.43482170362), test loss: 3.88969843388\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (16.9831809998,25.9382764351), test loss: 50.4256555557\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.697876930237,2.43379199755), test loss: 3.17663583159\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (45.7060585022,25.9235535034), test loss: 39.3741970062\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.19113707542,2.43350372869), test loss: 3.97356032431\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (19.338514328,25.9078537775), test loss: 40.9728588581\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.22689819336,2.43247717949), test loss: 2.85522035062\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (12.1806812286,25.8941143791), test loss: 32.4813579798\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.11409080029,2.43144795336), test loss: 3.8957899034\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (37.6947746277,25.8825034303), test loss: 38.1648230553\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (6.92416572571,2.43038593446), test loss: 3.15619690269\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (24.6188049316,25.8649281769), test loss: 36.523822403\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.6463547945,2.42913599133), test loss: 3.88893418312\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (14.9038877487,25.8510126877), test loss: 35.6122123241\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.87126088142,2.42831145718), test loss: 3.08625738323\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (19.7834281921,25.836273153), test loss: 38.6373564243\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.578335404396,2.42755054268), test loss: 3.74121192098\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (43.0390586853,25.8197086924), test loss: 35.5294852257\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.85824942589,2.4269846891), test loss: 3.47072689533\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (10.609577179,25.8052254735), test loss: 37.1550133228\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.14093899727,2.42586745153), test loss: 3.91326243281\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (16.2327728271,25.792938345), test loss: 34.4003386021\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.19376707077,2.42497098925), test loss: 3.89215987921\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (23.5123023987,25.7814021167), test loss: 40.2652408123\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.06649804115,2.4238804053), test loss: 3.23086579442\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (20.2823734283,25.7621078437), test loss: 38.2493901253\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.17950630188,2.42280363058), test loss: 3.76309576035\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (27.7585353851,25.7488314105), test loss: 40.8609030247\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.897093772888,2.42181651768), test loss: 2.86775611192\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (46.2598648071,25.7346739706), test loss: 39.7999689102\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.22981500626,2.42111044897), test loss: 3.99992626011\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (98.5444641113,25.7196521807), test loss: 60.5231088638\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (7.49083185196,2.42056245845), test loss: 3.20640872121\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (6.48629331589,25.7058990185), test loss: 34.3587828159\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.1643435955,2.41960431883), test loss: 3.93911658525\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (20.0650405884,25.694306381), test loss: 39.5142080307\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.7117459774,2.41857858752), test loss: 3.32274547219\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.2901554108,25.6808277475), test loss: 36.1619815111\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.437458992,2.41749474415), test loss: 3.57234482765\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (18.9222869873,25.6627850386), test loss: 38.9314772606\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.01213800907,2.41639352367), test loss: 2.92628709674\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (17.2446479797,25.6500978406), test loss: 41.3431674004\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.42645430565,2.41555511622), test loss: 3.73809199333\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (17.5195503235,25.6350061016), test loss: 35.3658632278\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.45086193085,2.41487471373), test loss: 3.73614852428\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.4066467285,25.6199686682), test loss: 38.1779781342\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.31138658524,2.4141309252), test loss: 4.06724184752\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (37.7257919312,25.6086422314), test loss: 34.1311108112\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.14713478088,2.41324600018), test loss: 3.88713081181\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (12.7145643234,25.5950799884), test loss: 41.2118948936\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.89971399307,2.41226237954), test loss: 2.97870758474\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (20.1507263184,25.582503297), test loss: 35.449703598\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.79098272324,2.41132428111), test loss: 3.95891584158\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (30.4592056274,25.5659481009), test loss: 42.781586647\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.15254616737,2.41025099419), test loss: 2.89604219794\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (15.9143209457,25.552201288), test loss: 35.6269831896\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.18917751312,2.40942767957), test loss: 3.89524512589\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (18.4791755676,25.5381403091), test loss: 37.7733707905\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.36959278584,2.40861961199), test loss: 2.96832674444\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (15.0419301987,25.5218988054), test loss: 37.2201657295\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.39153933525,2.40784792559), test loss: 3.85653143525\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (43.8932342529,25.5111924302), test loss: 35.3635493279\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.883499860764,2.40704090795), test loss: 2.92125672698\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (17.0747451782,25.499713665), test loss: 37.9756832123\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.46823465824,2.40617994394), test loss: 3.53655132353\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (21.8283920288,25.4863129872), test loss: 35.5656968832\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (4.62531423569,2.405196603), test loss: 3.56981130242\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (15.5200681686,25.4684754509), test loss: 36.3943717003\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.25333499908,2.40408046909), test loss: 3.91905660033\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (13.4571933746,25.456990975), test loss: 34.7864789009\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.11223602295,2.40324467486), test loss: 3.72678038478\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.5590171814,25.4424998144), test loss: 38.7546094418\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.677637696266,2.40260677491), test loss: 3.29775418341\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (20.1739120483,25.4276529013), test loss: 37.5560742855\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.77661883831,2.40185036171), test loss: 3.79265700281\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (51.4064292908,25.4151180077), test loss: 46.0828181267\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.97530937195,2.40101306036), test loss: 2.78111153543\n",
      "\n",
      "MC # 3, Hype # hyp1, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold2/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (52.8191833496,inf), test loss: 29.3493136406\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (4.14602470398,inf), test loss: 3.06508055925\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (28.3535003662,27.1199941463), test loss: 30.2357877254\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.51641082764,2.44418161297), test loss: 3.28420382738\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (21.3385429382,27.0664120164), test loss: 29.8144308567\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (0.717354774475,2.437659863), test loss: 3.55890083313\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (31.2352199554,27.1027956769), test loss: 38.1050524712\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.01271390915,2.43852215846), test loss: 3.66894973516\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (20.0954093933,27.08657573), test loss: 32.2553685665\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.3990894556,2.43432610716), test loss: 3.35266083479\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (16.0108737946,27.0762684468), test loss: 29.7110175848\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.6580477953,2.43390223936), test loss: 3.0316288352\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (35.1609039307,27.0126169813), test loss: 33.8364884853\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.49856901169,2.43302461323), test loss: 3.43251363039\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (18.7525177002,27.0063476535), test loss: 36.2862226486\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.23908209801,2.43135159766), test loss: 2.93251349032\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (20.6640262604,26.9660515962), test loss: 35.1639763832\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.29183244705,2.43090480702), test loss: 3.55458627343\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (30.4215984344,26.9282841653), test loss: 29.5593009949\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.92832493782,2.42899418317), test loss: 3.45590106696\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (7.28157615662,26.9337470815), test loss: 32.8207716227\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.87444615364,2.42789120621), test loss: 3.57291571498\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (19.7600059509,26.9307979933), test loss: 29.7692871094\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.88827824593,2.42617102404), test loss: 3.08824387044\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (45.1302604675,26.9284144265), test loss: 29.9404094458\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.80584430695,2.42479441287), test loss: 3.6432302177\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (27.6949806213,26.9031236357), test loss: 28.5799010754\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.21278953552,2.42301719643), test loss: 2.90859798193\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (16.1962509155,26.8916872606), test loss: 32.6154005647\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (6.44036197662,2.42208083894), test loss: 3.8067130208\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (23.8129901886,26.8845483659), test loss: 39.0378278732\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.642536997795,2.42087323413), test loss: 3.65444572568\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (51.1688194275,26.8599524212), test loss: 35.859386301\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.13905572891,2.42090676919), test loss: 2.97606009245\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (9.88599586487,26.8293982715), test loss: 29.5894895315\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.3458404541,2.41992273416), test loss: 3.37552686036\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (24.1918487549,26.8163111269), test loss: 31.4533505201\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.71248269081,2.41829581108), test loss: 2.97185173631\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (25.5617847443,26.7985254049), test loss: 32.1416238546\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.04436063766,2.41721528481), test loss: 3.51127823591\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (45.8809127808,26.7748377934), test loss: 28.3611787796\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.29449379444,2.41579921996), test loss: 3.0538497895\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (16.5838909149,26.7587546456), test loss: 34.5831946373\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.63383960724,2.41501123729), test loss: 4.23193513155\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (10.8203639984,26.7529917595), test loss: 29.4292750359\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.26425695419,2.41362800206), test loss: 3.35260407925\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (27.8480415344,26.7348209718), test loss: 30.3225043297\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.81534433365,2.41288306793), test loss: 3.25924443007\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (16.0852508545,26.714029904), test loss: 28.1840265274\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.4574649334,2.41145796621), test loss: 3.0136736691\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (29.2751502991,26.7065647264), test loss: 30.7032308102\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.04648005962,2.40991158792), test loss: 3.51400723457\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (12.2691984177,26.6951837456), test loss: 30.4514734745\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.601968407631,2.40910912049), test loss: 3.47258095741\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (45.7003288269,26.6776706504), test loss: 33.9740997314\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.03635716438,2.40827357455), test loss: 3.74889290333\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (40.0945892334,26.656447415), test loss: 30.1325532913\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.988165080547,2.40751162053), test loss: 3.70269555449\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (27.2070655823,26.6393835034), test loss: 32.0643607378\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.50282895565,2.40646467904), test loss: 3.19120631218\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (15.7062063217,26.615107325), test loss: 31.2677496433\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.34801244736,2.40574351015), test loss: 3.52004984617\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (9.86310482025,26.5922106534), test loss: 30.860499084\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.80040872097,2.40455064616), test loss: 2.91896042228\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (9.5532245636,26.5815687375), test loss: 34.9244418144\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.00727081299,2.40363387072), test loss: 3.69581542015\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (11.1084671021,26.5727743289), test loss: 31.088588047\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.08635890484,2.40241518938), test loss: 3.07498194575\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (17.3854694366,26.5627600973), test loss: 31.9142929077\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.41247749329,2.40105522381), test loss: 3.43993973136\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (19.3362751007,26.5448283094), test loss: 28.8849751949\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.43607902527,2.39998560135), test loss: 2.88026688993\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (12.7079524994,26.5343245927), test loss: 30.3273271203\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.32305622101,2.39888908566), test loss: 3.46564463079\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (35.8848304749,26.5141719669), test loss: 30.1791866302\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.65267765522,2.39800396842), test loss: 3.51140197515\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.0218343735,26.4948179357), test loss: 34.1811298132\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.07924294472,2.39729593014), test loss: 3.5592143774\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (39.5500907898,26.4820621595), test loss: 30.0470499992\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.834560930729,2.39649328753), test loss: 3.59380547404\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (35.9284744263,26.4641912345), test loss: 32.0430287123\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.44370794296,2.39539966277), test loss: 3.27225427181\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (21.0868759155,26.447662231), test loss: 31.8244189143\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.724313020706,2.39433008808), test loss: 3.5123888135\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (17.8022708893,26.4276693977), test loss: 36.3135860443\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.739778995514,2.39327184303), test loss: 2.94303848743\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (15.3969078064,26.4166691655), test loss: 38.1696840286\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.15005338192,2.39220872091), test loss: 3.48107571006\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (44.4518470764,26.403335059), test loss: 47.5669066429\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.942708849907,2.39132092214), test loss: 3.35214129984\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (64.0431900024,26.388498567), test loss: 45.316153717\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (7.6905798912,2.3905015897), test loss: 3.66375432611\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (7.58387613297,26.3741113015), test loss: 28.8609568596\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.77503311634,2.38927499841), test loss: 2.95917201042\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (4.19944190979,26.3605904484), test loss: 31.7435088873\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.92385029793,2.38825348296), test loss: 3.48630622625\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (21.2195472717,26.3474738517), test loss: 30.3164983749\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.82471895218,2.38733002415), test loss: 3.54966800213\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (52.0762557983,26.3312888268), test loss: 31.5630877972\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.934230685234,2.38625946121), test loss: 3.67522892356\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (15.0642547607,26.3135412115), test loss: 34.4616678476\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.52148675919,2.38552237993), test loss: 3.68380684257\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (50.4341049194,26.2998301046), test loss: 32.3578638077\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (5.24204540253,2.38461089182), test loss: 3.00363341272\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (21.0182247162,26.2796430351), test loss: 35.469851923\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.65229225159,2.38376400828), test loss: 3.5286514163\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (9.97702407837,26.2609340556), test loss: 32.5273235559\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.00277328491,2.38277307448), test loss: 2.95599111319\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (19.9564495087,26.2497201819), test loss: 30.6614619255\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.07771992683,2.38153937103), test loss: 3.50519534647\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (25.9663505554,26.2373641931), test loss: 31.375106287\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.55471360683,2.38058571586), test loss: 3.15196586251\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (8.58411979675,26.2219594744), test loss: 31.4436880112\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.26666998863,2.37957942326), test loss: 3.50330889225\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (27.4355373383,26.208073385), test loss: 31.1436980724\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.41239905357,2.37879122726), test loss: 3.16017965376\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.606303215,26.1945955743), test loss: 29.7073040009\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.86569941044,2.37774210952), test loss: 3.40553683937\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.3182754517,26.1784310593), test loss: 30.7659236431\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.432186543941,2.37695896782), test loss: 3.60734987259\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (23.5995731354,26.160646208), test loss: 31.0729095221\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.87833714485,2.37602474256), test loss: 3.65574519932\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (13.6873035431,26.1460483551), test loss: 29.4177621841\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.856588721275,2.37496706728), test loss: 3.62371760905\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (30.78424263,26.1311392897), test loss: 30.307545197\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.3013086319,2.37418863323), test loss: 2.96859390736\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (6.28190326691,26.113460059), test loss: 32.4744615555\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.87148845196,2.37321253971), test loss: 3.61001648903\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.9485797882,26.0967671647), test loss: 31.9461227655\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.66263926029,2.37228118144), test loss: 3.05948882401\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (25.9490222931,26.0835553627), test loss: 30.1998775244\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.02589416504,2.37121967487), test loss: 3.58654569089\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (40.3881874084,26.0675073983), test loss: 36.1132202148\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.20495605469,2.37040690582), test loss: 3.338485232\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (51.7343940735,26.0530125841), test loss: 30.9375070095\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.94840478897,2.36943034339), test loss: 3.39378053844\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (5.95263242722,26.039398978), test loss: 33.2833498001\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.0753724575,2.36850550495), test loss: 3.16062403023\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (17.2452468872,26.0273691781), test loss: 31.4406345606\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (0.984440684319,2.36746784046), test loss: 3.34961885214\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (27.5726203918,26.0142044908), test loss: 31.4272452354\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.29830503464,2.36651955467), test loss: 3.68643803895\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (23.2493171692,25.9968309454), test loss: 30.4193530083\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.78545999527,2.36563481913), test loss: 3.7498852849\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (30.8754730225,25.9828479352), test loss: 31.8421014071\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.95969152451,2.36474821263), test loss: 3.52351360321\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (21.180562973,25.9639820188), test loss: 29.8151181698\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.51395308971,2.36380703123), test loss: 3.01331980526\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (25.5000915527,25.946184731), test loss: 32.8390858173\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.76813483238,2.36296100304), test loss: 3.72006027699\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (37.7508506775,25.9328634026), test loss: 31.9171020508\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.83073210716,2.36197790096), test loss: 2.90877533555\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (48.8532028198,25.9189888722), test loss: 33.7291394949\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (4.56110143661,2.36100567698), test loss: 3.75853192806\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (29.7480354309,25.9063463167), test loss: 32.8743185759\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.10542845726,2.35992808349), test loss: 3.21271262765\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (25.6426467896,25.8917550908), test loss: 31.3442468166\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.90789604187,2.35899202194), test loss: 3.55766645074\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (17.0012435913,25.8796131498), test loss: 31.2207690716\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.48620796204,2.35797347773), test loss: 3.17230044603\n",
      "\n",
      "MC # 3, Hype # hyp1, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold3/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (32.0877342224,inf), test loss: 33.4704784393\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.47450733185,inf), test loss: 3.24928236306\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (20.5169639587,25.5067783849), test loss: 30.9034260988\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.74161243439,2.46818156713), test loss: 2.94988043308\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (27.6916694641,25.3811643828), test loss: 31.1191277742\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.49718046188,2.46857162133), test loss: 2.75931985676\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (25.5073394775,25.397168086), test loss: 30.2551889181\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.9636349678,2.45566615168), test loss: 3.18798507452\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (22.6381721497,25.3750720434), test loss: 31.9985289574\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.16784572601,2.4509416877), test loss: 3.25289700925\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (43.6165122986,25.3401733483), test loss: 32.1225091934\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.17611217499,2.44675750369), test loss: 3.17447541058\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (13.9862709045,25.3714536227), test loss: 36.0817418337\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.66916704178,2.44725643971), test loss: 3.25087453574\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (41.8580551147,25.4088914289), test loss: 33.1109962225\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (5.01512861252,2.44714534245), test loss: 2.82766695917\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (32.0740890503,25.4012348124), test loss: 33.5777844429\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.58420479298,2.44764047167), test loss: 3.31665400863\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (5.64960956573,25.4073640024), test loss: 33.8329245567\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.3076210022,2.44624466759), test loss: 2.72117324471\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (23.0147895813,25.4055616852), test loss: 31.2987167358\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.95889186859,2.44442490071), test loss: 3.43906805813\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (20.4856452942,25.4040764777), test loss: 31.8453625202\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.78734993935,2.4430374297), test loss: 3.10333974957\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (13.8899717331,25.3770506619), test loss: 32.6346047878\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.04234552383,2.44083001174), test loss: 3.15953454375\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (40.0283126831,25.3502642579), test loss: 34.7402071953\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.94768190384,2.4394864951), test loss: 3.13047412634\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (46.5168151855,25.3085216929), test loss: 30.3297109127\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.97078227997,2.43922932625), test loss: 2.95012950897\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (29.3253688812,25.2684388395), test loss: 31.1872015476\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.05275201797,2.43899843551), test loss: 2.77102629542\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (19.0736961365,25.261915555), test loss: 30.866761446\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.79174304008,2.43690699873), test loss: 3.16610876024\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (24.0555877686,25.2537090302), test loss: 32.3454270363\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.899513483047,2.43551407806), test loss: 3.16551975608\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (11.8825817108,25.2358687901), test loss: 33.1056766987\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.93280696869,2.43373477226), test loss: 3.21146202087\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (5.6704416275,25.2367471604), test loss: 33.7985728264\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (5.53175640106,2.43281996372), test loss: 3.39164264798\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (25.0075912476,25.2313798103), test loss: 36.6142028332\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.02996516228,2.43227300881), test loss: 2.91194781959\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (63.0149993896,25.2116035207), test loss: 42.9966231346\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (5.7924580574,2.43250042379), test loss: 3.22413612008\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (23.4252510071,25.206008799), test loss: 33.34763937\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.41661596298,2.43130925412), test loss: 2.71659463942\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (27.5668144226,25.18911636), test loss: 32.2303299427\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.79082798958,2.42990940981), test loss: 3.34825026989\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (26.3596343994,25.1668015229), test loss: 31.7574314117\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.26018857956,2.42843384262), test loss: 3.05647910237\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (24.7840881348,25.1346988023), test loss: 32.3221896172\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.00190234184,2.42676229956), test loss: 3.31212416887\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (26.3677577972,25.1096193057), test loss: 35.1795505524\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.235059201717,2.42556659148), test loss: 3.11998058558\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (14.5098009109,25.08506212), test loss: 32.0751056194\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.9476287365,2.42507981178), test loss: 3.03211809993\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (26.0459957123,25.0674666719), test loss: 31.0880844593\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.21585464478,2.42472923848), test loss: 2.84731970429\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (31.1567420959,25.0629349175), test loss: 31.5791813374\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.28304219246,2.4230337895), test loss: 3.14999628663\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (27.507270813,25.0551842783), test loss: 31.9250991344\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.771872580051,2.42179624911), test loss: 2.86749359965\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (26.2109298706,25.0401064652), test loss: 33.5258665323\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.34741020203,2.42037303674), test loss: 3.16600136161\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (9.29699420929,25.0304736094), test loss: 32.5468831539\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.536043465137,2.41950353677), test loss: 3.33419340998\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (40.1733207703,25.0175729078), test loss: 34.9483019352\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.39556288719,2.41906331448), test loss: 3.12521097362\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (34.5778121948,24.9954708796), test loss: 35.5920053959\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.55609512329,2.41843090244), test loss: 3.21217070818\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (24.3090171814,24.9793444169), test loss: 32.9982296467\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.91393399239,2.41725518646), test loss: 2.68639709949\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (17.0070533752,24.9548759816), test loss: 36.7057853222\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.27398896217,2.41586709291), test loss: 3.19866077304\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (48.2681694031,24.9321101556), test loss: 31.2416725636\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.17135906219,2.41434807124), test loss: 2.98394011259\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (17.7271308899,24.9101479109), test loss: 32.4556174278\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.617311477661,2.41310641912), test loss: 3.4668168366\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (43.3792610168,24.8961941674), test loss: 32.2614014626\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.12335181236,2.41231501137), test loss: 3.07392082214\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (8.60215950012,24.8813255543), test loss: 31.9887687206\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.740802705288,2.41154880831), test loss: 3.04427404404\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (22.7378005981,24.8662672507), test loss: 33.6194168091\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.43266439438,2.4108881702), test loss: 2.79963347912\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (26.43708992,24.8587970855), test loss: 30.8930763721\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.95739924908,2.40951583915), test loss: 3.1057235837\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (37.8247795105,24.8478386671), test loss: 35.022598505\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.88778781891,2.40859682786), test loss: 2.89311366379\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (20.0179920197,24.8317646072), test loss: 31.762519908\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.838395833969,2.40727250898), test loss: 3.21909793615\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (45.3357620239,24.8184585818), test loss: 32.5675570488\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.15905761719,2.40634429448), test loss: 3.37747822404\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (49.2486724854,24.7980755719), test loss: 36.3505035877\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.6643037796,2.40554454655), test loss: 3.15174187422\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.1603393555,24.7719589362), test loss: 35.1403611183\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.669493198395,2.40458018029), test loss: 3.46131001413\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (25.8490562439,24.7530850098), test loss: 35.1663033962\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.44197654724,2.40347749827), test loss: 2.62458218336\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (16.7148513794,24.7350818295), test loss: 37.9971111298\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.762744903564,2.40230047741), test loss: 3.25984818935\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (49.7467842102,24.7182682129), test loss: 33.2740572453\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.0458419323,2.40102227113), test loss: 2.83692533374\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (42.6545372009,24.7045996425), test loss: 33.1591061115\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.42605471611,2.40000274101), test loss: 3.54437417388\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (33.4477920532,24.6929299071), test loss: 35.043571043\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.39547646046,2.3990136184), test loss: 3.17525298595\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.4084348679,24.6799164594), test loss: 33.814892149\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.772973060608,2.39829280239), test loss: 3.13966335356\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (6.88288021088,24.6641785041), test loss: 31.4902971745\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.78070306778,2.39769659929), test loss: 2.81166266799\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (20.0270118713,24.6550080334), test loss: 29.9199712515\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.768856287,2.39651848813), test loss: 3.0435783118\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (28.1208972931,24.640369235), test loss: 31.7057213306\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.94632363319,2.395425256), test loss: 3.09897306263\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (39.7879714966,24.6202683107), test loss: 29.8702586174\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (5.44988393784,2.3942285252), test loss: 3.27145209312\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (21.1941013336,24.6001725859), test loss: 42.5403409958\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.50957918167,2.3931227068), test loss: 3.08257403672\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (18.620174408,24.5784512387), test loss: 32.3627572536\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.989777565,2.39230383652), test loss: 3.11943625212\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.279712677,24.5577062349), test loss: 35.4559614182\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.69071912766,2.39137625787), test loss: 3.39291154742\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (39.0841064453,24.5445802366), test loss: 36.5542923927\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.97419095039,2.3904692353), test loss: 2.87373721898\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.0736598969,24.5310790063), test loss: 41.4736087322\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.8902759552,2.38941844647), test loss: 3.3076392591\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (6.6765460968,24.5173255884), test loss: 33.1166660786\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.52505159378,2.38807321312), test loss: 3.01868655384\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (17.7707500458,24.5052547442), test loss: 32.4831710577\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.72287714481,2.38716185087), test loss: 3.35948140621\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (10.1912097931,24.4927773901), test loss: 32.4985284328\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.03033900261,2.38632199381), test loss: 3.05512093306\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.4861488342,24.4786231206), test loss: 33.7443471432\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.894947648048,2.3856168427), test loss: 3.13939683735\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (10.4813785553,24.4631988365), test loss: 31.5707841396\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.68504524231,2.3849197473), test loss: 2.84638270438\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (46.2941169739,24.4487526091), test loss: 29.0941660404\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.60764086246,2.38369942566), test loss: 3.03805063367\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (10.8337125778,24.4283868506), test loss: 33.5988553762\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (6.20337772369,2.38260209323), test loss: 3.0924349606\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (12.8704538345,24.4068482422), test loss: 36.5387761116\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.811340749264,2.38131092058), test loss: 3.32340664864\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (18.9726047516,24.3902319577), test loss: 36.1144992352\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.35684370995,2.38039338679), test loss: 3.13780815601\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (38.2848777771,24.3739202857), test loss: 33.2443090916\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (5.47758626938,2.37962650044), test loss: 3.11279658675\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (21.1536884308,24.3580445334), test loss: 34.1016434193\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.571683347225,2.37882074636), test loss: 3.27018502951\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.23365879059,24.3463028257), test loss: 33.838888979\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.48211765289,2.37782100658), test loss: 2.88986237645\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (28.2421894073,24.3352549013), test loss: 33.2621137619\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (6.60419607162,2.37680308527), test loss: 3.45142341852\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (45.2766532898,24.3220570419), test loss: 32.0972873211\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.33874607086,2.37567064475), test loss: 2.8402479127\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (17.3175315857,24.3087365505), test loss: 34.3753069878\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.96365475655,2.37486791114), test loss: 3.34860432148\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (10.7286090851,24.294777247), test loss: 33.64187603\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.19525909424,2.37399753039), test loss: 3.19883811176\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (37.9520072937,24.2778748986), test loss: 32.2203725815\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.33471882343,2.37323469827), test loss: 3.1244846344\n",
      "\n",
      "MC # 3, Hype # hyp1, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold4/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (39.6508483887,inf), test loss: 36.2741219997\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (4.58590984344,inf), test loss: 2.96922911406\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (55.7364425659,26.3510348868), test loss: 35.8556090355\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.3870306015,2.46704254091), test loss: 3.49854876101\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (36.1428108215,26.2540581408), test loss: 31.6528574467\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.98140192032,2.46692142829), test loss: 3.69951860905\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (17.2858543396,26.3159845084), test loss: 33.3139068127\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.01700925827,2.4635695164), test loss: 3.82196134925\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (33.0052337646,26.3138219975), test loss: 31.9955163479\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.71890127659,2.45638805821), test loss: 3.82209553719\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (20.2162895203,26.328885241), test loss: 35.6407640934\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.755148589611,2.45274076603), test loss: 3.05948112905\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (30.312669754,26.2822304751), test loss: 33.3043203115\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.22061133385,2.45361938621), test loss: 3.617864573\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (35.5065917969,26.2607245054), test loss: 38.4051176071\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.03219485283,2.45366113929), test loss: 3.03995470703\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (21.6344490051,26.2534019248), test loss: 30.0310741425\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.183191895485,2.45139001895), test loss: 3.63186244071\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (32.6345291138,26.2167786867), test loss: 34.5560551167\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.495243102312,2.45209893131), test loss: 2.90474172235\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (13.651468277,26.1951800756), test loss: 34.4632573128\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.71957397461,2.45185839893), test loss: 3.40897397399\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (4.11404895782,26.2007882341), test loss: 33.9231340408\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.13950037956,2.44946225919), test loss: 2.83568092585\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (4.71474409103,26.1740254776), test loss: 32.8966214657\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.20261931419,2.44793538503), test loss: 3.61272467375\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (16.9550094604,26.1753759418), test loss: 30.967685771\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.746838450432,2.44652104036), test loss: 3.80754206777\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (51.005531311,26.153065516), test loss: 34.924572134\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.47164189816,2.44502060588), test loss: 3.76596812308\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (17.330783844,26.1309974562), test loss: 33.2348625183\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.78076338768,2.44480888339), test loss: 3.88851408362\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (35.9325218201,26.1183307711), test loss: 36.9413889647\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.89918613434,2.44420044741), test loss: 3.08983590901\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (20.4830150604,26.0955017038), test loss: 31.346106267\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.72094368935,2.44335429705), test loss: 3.66478749514\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (17.4541721344,26.0808297584), test loss: 33.4854760885\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.380995154381,2.44218623134), test loss: 3.00589389503\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (31.9547138214,26.0684783693), test loss: 32.4279114246\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.49762427807,2.44105120895), test loss: 3.66386738718\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (41.4345283508,26.0560318929), test loss: 32.3558381557\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.08042001724,2.43973106208), test loss: 2.94507233202\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (15.9614009857,26.0491117315), test loss: 32.5117033958\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.3226852417,2.43827288672), test loss: 3.47124925852\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (8.74711322784,26.0208239079), test loss: 29.9783663273\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.59831929207,2.43702765487), test loss: 3.36899144351\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (16.375213623,26.0079967141), test loss: 36.7427548409\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.46103048325,2.43668701138), test loss: 4.31301643252\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (34.9384536743,25.9927982405), test loss: 33.6342999458\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.35413169861,2.43552838436), test loss: 4.1884441942\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (36.0076675415,25.9736230233), test loss: 35.2094737053\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.86837291718,2.43472982224), test loss: 3.11937812269\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (44.4592132568,25.9543738735), test loss: 33.6048135757\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.74393439293,2.43405191494), test loss: 3.5576254189\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (30.9011955261,25.9450484654), test loss: 41.4470983028\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.68304443359,2.43267725409), test loss: 2.84747836292\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (19.1768379211,25.9313284253), test loss: 30.3866867065\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.09714746475,2.43105490166), test loss: 4.06122583151\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (20.7491931915,25.9179987054), test loss: 33.9232069969\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.65406966209,2.43031413082), test loss: 2.94511905015\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (43.7094497681,25.9011248788), test loss: 32.4232495308\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.589327394962,2.42917557315), test loss: 3.54677285552\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (58.0261726379,25.8870581019), test loss: 36.4528032303\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.87028837204,2.42830686534), test loss: 3.42489118576\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (13.3964633942,25.8663288799), test loss: 32.7049312115\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.03530883789,2.42746069764), test loss: 3.80594630539\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (34.6815567017,25.8521006808), test loss: 30.4227267742\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.04252481461,2.42677322743), test loss: 3.60306305587\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (11.9100799561,25.8341754998), test loss: 33.7298275948\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.46793246269,2.42558651268), test loss: 3.78607396483\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (42.1854019165,25.8215831205), test loss: 37.3158754349\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.73464274406,2.42439204617), test loss: 3.80687608719\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (15.9866981506,25.8087050587), test loss: 38.1054805756\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.14655613899,2.42321971865), test loss: 2.7736087203\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (32.1621856689,25.7973959359), test loss: 32.1975340128\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.72239756584,2.4221854673), test loss: 3.64861083031\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (21.0379543304,25.7790069304), test loss: 34.5353957176\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.822173655033,2.42104937857), test loss: 2.90626246333\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (26.8616523743,25.7628342211), test loss: 33.0948815823\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.6891336441,2.42052244931), test loss: 3.79857479334\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (3.46965456009,25.7502909383), test loss: 33.0077606201\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.42688024044,2.41955323437), test loss: 3.14445566013\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (10.7090492249,25.7339273676), test loss: 32.9777643204\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.11511301994,2.41850146967), test loss: 3.42848863304\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (23.3225860596,25.7135209706), test loss: 31.4348562717\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.0439350605,2.41769379315), test loss: 2.83798967004\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (7.42436504364,25.7047449818), test loss: 34.1589660168\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (5.51863861084,2.41662468196), test loss: 3.66801683903\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (24.1309547424,25.692754862), test loss: 31.0504908562\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.17929458618,2.41518215903), test loss: 3.77945356965\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (21.8159370422,25.6773442151), test loss: 34.9038413048\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.90328001976,2.41431547532), test loss: 3.1696739316\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (20.3029708862,25.6603782676), test loss: 33.6742463589\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.04418611526,2.41338641628), test loss: 3.67258055806\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (24.5702075958,25.6468508962), test loss: 37.169756937\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.18931436539,2.41244527044), test loss: 2.86470036507\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (23.1507797241,25.6341464782), test loss: 30.6860283375\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.81553268433,2.41160344285), test loss: 3.70884299278\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (33.2910766602,25.6155307782), test loss: 33.657766819\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.72595191002,2.41093098333), test loss: 2.98612359762\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (26.1403846741,25.5996996118), test loss: 31.1240588188\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.937854886055,2.40985645668), test loss: 3.46522687674\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (32.8770904541,25.5893361494), test loss: 35.3716640472\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (7.24039363861,2.40845187086), test loss: 3.02668791711\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (14.3401498795,25.5743703337), test loss: 34.1345390797\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.764731407166,2.40757062001), test loss: 3.36839371324\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (19.8405685425,25.5634405017), test loss: 34.0515678883\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.2167198658,2.40668249551), test loss: 4.14162762165\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (22.1578865051,25.5463283184), test loss: 35.860236454\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.940676629543,2.40542578737), test loss: 3.84851711541\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (18.7440471649,25.5300577915), test loss: 31.1297740459\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.775023043156,2.40464474465), test loss: 3.90784044266\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (35.4334487915,25.5195101096), test loss: 37.5924651146\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (4.58085632324,2.40404042549), test loss: 2.87902029753\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (53.3991546631,25.5023114515), test loss: 31.4270856142\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.55147957802,2.40300519919), test loss: 3.64730799794\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (18.748714447,25.4860022238), test loss: 38.6578930855\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.28674578667,2.40208269065), test loss: 3.06128534377\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (14.0143680573,25.4741651569), test loss: 30.791566062\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.63855278492,2.40078201283), test loss: 3.58030245006\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.697306633,25.462220324), test loss: 36.6763274193\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.41192054749,2.39994042293), test loss: 2.88761817217\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (30.9048976898,25.4497528602), test loss: 34.5025693417\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (6.17155790329,2.39890024467), test loss: 3.74680556059\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (19.2208461761,25.429851819), test loss: 34.3025066853\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.32670795918,2.39794467026), test loss: 2.90915809274\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (17.5709686279,25.4190485593), test loss: 35.9955742836\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.67081391811,2.39710042743), test loss: 3.63109469861\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (18.3616218567,25.4061785569), test loss: 33.8922757149\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.782929241657,2.39624389404), test loss: 3.91591293514\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (13.4855594635,25.3878152924), test loss: 43.6698344231\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.2713419199,2.39543175994), test loss: 3.61095119715\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.888584137,25.3741588678), test loss: 33.8203732967\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.25987529755,2.39464880733), test loss: 3.5066216886\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (23.8347740173,25.363216323), test loss: 38.0915622234\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.10875749588,2.39320539819), test loss: 2.95794085264\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (26.4237709045,25.3498714876), test loss: 30.1146095276\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (5.78927850723,2.39235398007), test loss: 3.73870640993\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (52.5247039795,25.3369745693), test loss: 35.423848629\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.03353047371,2.39140177712), test loss: 3.06663934886\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (10.1185274124,25.3205471713), test loss: 30.2069387674\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (4.15850543976,2.39046701827), test loss: 3.62546313107\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (16.5431098938,25.3080454945), test loss: 36.0278941631\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.786857008934,2.38956013566), test loss: 2.95348542631\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (68.5136032104,25.2936840554), test loss: 51.1124754906\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (4.58462047577,2.38897485707), test loss: 3.73946580887\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (24.1525821686,25.278834675), test loss: 38.8832890511\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.459208101034,2.38803099624), test loss: 3.63046448231\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.55286693573,25.2653138218), test loss: 33.7333412886\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.12593650818,2.38706786244), test loss: 3.83442894816\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (15.4231185913,25.2514412824), test loss: 31.6747307777\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.47803330421,2.38586656537), test loss: 3.82013399601\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (16.3475418091,25.2404226318), test loss: 35.8751124859\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.37777853012,2.38492133479), test loss: 3.07941073179\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.8595657349,25.2267722188), test loss: 34.2952745438\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (4.61754512787,2.38392167315), test loss: 3.70549240708\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (18.0131721497,25.210417229), test loss: 38.7179182053\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.32937800884,2.38302259784), test loss: 2.95302468836\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (21.6733093262,25.1991178333), test loss: 28.6534890652\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.79229450226,2.38234736097), test loss: 3.51798263192\n",
      "\n",
      "MC # 3, Hype # hyp1, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold5/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (36.5168838501,inf), test loss: 32.9614873409\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.63380241394,inf), test loss: 2.87545172274\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (16.6549110413,25.4252912116), test loss: 31.1501462698\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.13888907433,2.4419033038), test loss: 3.52334444523\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (23.7980995178,25.4174512396), test loss: 32.1162040234\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.63507723808,2.43505821142), test loss: 2.87871635258\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (25.9463214874,25.4814117037), test loss: 33.5017115116\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.70264363289,2.4251773292), test loss: 3.77656350732\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (13.7573299408,25.5270158997), test loss: 31.3915025711\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.99033653736,2.42277942903), test loss: 3.59331402481\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (10.8727579117,25.4797952911), test loss: 30.6120956421\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.963339030743,2.42072137433), test loss: 3.86060758233\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (39.1256942749,25.4950473582), test loss: 32.1892154694\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (4.18378639221,2.4211968827), test loss: 3.68562633097\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (21.0632572174,25.4409540894), test loss: 30.9386918068\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.05364942551,2.42141773928), test loss: 3.07675939798\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (26.6892929077,25.4132568256), test loss: 31.8795581341\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.599062561989,2.42103606266), test loss: 3.64712331295\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (29.6442470551,25.4187381982), test loss: 32.7648929596\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.08235049248,2.41794017793), test loss: 2.76380988061\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (13.6214160919,25.4200518464), test loss: 28.1718148708\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.32866048813,2.4175268105), test loss: 3.96143008471\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (19.4745082855,25.3959032293), test loss: 32.9530237198\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.96401667595,2.41550429056), test loss: 2.73846337795\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (27.1661682129,25.3883480844), test loss: 26.6549070835\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.24376773834,2.4148690881), test loss: 3.64145009518\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (25.4199142456,25.355372696), test loss: 36.390913868\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.55201530457,2.41472557411), test loss: 2.99287493974\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (16.9601020813,25.3348913693), test loss: 27.7303221226\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.26161372662,2.41409629284), test loss: 3.50289208889\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (21.9892215729,25.3246143044), test loss: 34.5238909245\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.66455364227,2.41173481495), test loss: 2.81175615489\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (8.60067081451,25.3231051131), test loss: 31.3326701641\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (6.50224637985,2.41140337558), test loss: 3.60881929398\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (29.9768981934,25.3002471206), test loss: 32.6910007954\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.31461787224,2.4095898807), test loss: 2.82481588125\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (13.0351295471,25.289743188), test loss: 30.7127147555\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.40989661217,2.40879452334), test loss: 3.74050311446\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (33.7137489319,25.269055863), test loss: 31.0490742683\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.33914637566,2.40868911546), test loss: 3.63750523329\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (28.1884021759,25.2420237407), test loss: 29.7802357912\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.25124776363,2.40766538046), test loss: 3.81486487389\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (25.7551994324,25.2360089582), test loss: 31.5699961662\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.08446502686,2.40590268547), test loss: 3.75271626115\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (21.151222229,25.2287094655), test loss: 30.7770332336\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.32163333893,2.40532961764), test loss: 3.12135500014\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (29.6234951019,25.2063350568), test loss: 32.5474509239\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.68227910995,2.40357550891), test loss: 3.64133118838\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (8.04748344421,25.1998440185), test loss: 33.1671186924\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.63477468491,2.40305442982), test loss: 2.78660494685\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (37.3048286438,25.1760154201), test loss: 28.6978005409\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.06872963905,2.40248029177), test loss: 3.84461874962\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (18.4230480194,25.1554490486), test loss: 32.2828395605\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.35260295868,2.40158045143), test loss: 2.75584224463\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (28.3994960785,25.1480334338), test loss: 27.6164298296\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.14691853523,2.40017648484), test loss: 3.68128473759\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (28.3942508698,25.1378538275), test loss: 34.6775705338\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.29336261749,2.399466054), test loss: 3.00424892902\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (23.137676239,25.119330469), test loss: 26.7383730888\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.02367448807,2.39805950319), test loss: 3.6218485117\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (17.5991020203,25.1079325452), test loss: 33.8722005844\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.14399957657,2.39736814944), test loss: 2.86364895403\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (35.4454536438,25.0875912175), test loss: 32.0646178961\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (0.840229213238,2.39661178519), test loss: 3.53398575783\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (21.7946472168,25.0703039432), test loss: 34.4608486176\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.58994781971,2.39584784066), test loss: 2.82508247793\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (43.0347824097,25.0590106531), test loss: 30.0945510149\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.45235776901,2.394449927), test loss: 3.68187463284\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (28.2521820068,25.0487884258), test loss: 33.4988571644\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.09381699562,2.39373194145), test loss: 3.58990939856\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (28.751493454,25.0318393415), test loss: 30.6156121492\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.08432006836,2.39249967838), test loss: 3.98927523494\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (30.2619819641,25.0193971726), test loss: 33.4548355103\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.32379293442,2.39167473486), test loss: 3.73799777627\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (18.9594993591,25.000931166), test loss: 31.1237646341\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.83034420013,2.3909735713), test loss: 3.16117800474\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (21.9285888672,24.9813229017), test loss: 30.4459712505\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.91069352627,2.39011183039), test loss: 3.66976276934\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (39.6642684937,24.9728499948), test loss: 32.8555222988\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.16149520874,2.38872052864), test loss: 2.87906132042\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (26.8124141693,24.9614153373), test loss: 29.5755804062\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.96755242348,2.38807382911), test loss: 3.73513528109\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (30.3324184418,24.9449036827), test loss: 35.3189188957\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (5.78990364075,2.38694494555), test loss: 2.9973641634\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (23.8636760712,24.9324814053), test loss: 27.1000735283\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.59766483307,2.38606149919), test loss: 3.66555597484\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (24.5424804688,24.9143136515), test loss: 34.502615571\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.02874422073,2.3853060584), test loss: 2.97790665478\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (14.106092453,24.8977158905), test loss: 26.7524652243\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.85568463802,2.38449468008), test loss: 3.61245609522\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (20.5941772461,24.8864730512), test loss: 33.4757246494\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.6970808506,2.38317291799), test loss: 2.90348562896\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (26.4741325378,24.876180633), test loss: 30.3010370255\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.68634748459,2.38250778661), test loss: 3.37377033234\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (24.335735321,24.8594472087), test loss: 34.314968729\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.99608564377,2.38140554829), test loss: 3.02509233057\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.6029434204,24.8470108722), test loss: 30.4626583099\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.14180850983,2.38055149579), test loss: 3.66053379476\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (18.716003418,24.8298804623), test loss: 30.6226219654\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.38330304623,2.37980363562), test loss: 3.67395722866\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (25.1150970459,24.8121229842), test loss: 29.5077764511\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.00823545456,2.37892722853), test loss: 3.8838873446\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (7.89236450195,24.8017545075), test loss: 31.7994454861\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.26460766792,2.37772668625), test loss: 3.87056121826\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (28.5056304932,24.7907493344), test loss: 30.7025690794\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.774291574955,2.37698106323), test loss: 3.04884584844\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (46.6804733276,24.7754299726), test loss: 33.0803365707\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.46912312508,2.37593321461), test loss: 3.66518707871\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (13.306804657,24.7636814282), test loss: 33.80890975\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.17635524273,2.37514178162), test loss: 2.77920629382\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (21.2025356293,24.7451491857), test loss: 26.7108234167\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.720442891121,2.3742909278), test loss: 3.77836855054\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (25.2005462646,24.7292326776), test loss: 32.5573186874\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.62552475929,2.37351180377), test loss: 2.77248583734\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (20.5866928101,24.7191127486), test loss: 25.1451137066\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.87772929668,2.37235904147), test loss: 3.7155962646\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (23.4569168091,24.7071778163), test loss: 34.2645931959\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.66791009903,2.37156974942), test loss: 2.93979957998\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (44.2336730957,24.6931497295), test loss: 30.3647615433\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.18189477921,2.37059842439), test loss: 3.63850663304\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (18.8072547913,24.6794847868), test loss: 36.9365701675\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.723890721798,2.36975998919), test loss: 2.86925107539\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (29.9544353485,24.6630172056), test loss: 32.2752804756\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.614341795444,2.36892379037), test loss: 3.47556477189\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (5.26326942444,24.6483469344), test loss: 32.0463150501\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.26511716843,2.36819981226), test loss: 2.83362733126\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (16.4922428131,24.6360234726), test loss: 29.7675508738\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.81108021736,2.36707092496), test loss: 3.6170732975\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (24.1882400513,24.6254003049), test loss: 30.5882146358\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.764648199081,2.36624681474), test loss: 3.65749529302\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (26.5968894958,24.6113830666), test loss: 29.4080994368\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.35007846355,2.36534584316), test loss: 3.99735133648\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (21.8481445312,24.5975945593), test loss: 37.1656096458\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.5500254631,2.36446164207), test loss: 3.83758673668\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (23.1142444611,24.5830030445), test loss: 35.1052976608\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.83863902092,2.36368867166), test loss: 3.21868712604\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (7.43959379196,24.5661735037), test loss: 29.6143811226\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.46871590614,2.36292406037), test loss: 3.66627512574\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (21.0950984955,24.5556818268), test loss: 32.757548213\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.3318631649,2.36179473805), test loss: 2.83016852736\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.6251735687,24.5446979202), test loss: 27.7611229897\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.840014338493,2.36103474099), test loss: 3.75208191276\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (57.4768638611,24.531644123), test loss: 32.6249590635\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.19914245605,2.36013361537), test loss: 2.90249456763\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (21.54164505,24.5176921683), test loss: 31.1905864239\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.64650475979,2.35926044796), test loss: 3.71159489453\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.0482845306,24.5019702624), test loss: 38.0742398262\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.3196914196,2.35848633774), test loss: 3.0595440805\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (9.34080600739,24.4870851168), test loss: 26.7954666615\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.88791489601,2.35771204175), test loss: 3.73804928064\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (26.3007354736,24.4765056333), test loss: 33.788267231\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.98086565733,2.35665099263), test loss: 2.8992334038\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.1170167923,24.4652169111), test loss: 29.4166561604\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.4710842371,2.3559181336), test loss: 3.37547155619\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (60.7100372314,24.4523135293), test loss: 32.4016591311\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.2922103405,2.35497630178), test loss: 2.83275091648\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (19.8284187317,24.4384740394), test loss: 33.5020531178\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.01961445808,2.35415175437), test loss: 3.59700870514\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (14.8922748566,24.4235654223), test loss: 30.2818109035\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.50404763222,2.35339577573), test loss: 3.67238189876\n",
      "run time for single CV loop: 1274.95152307\n",
      "\n",
      "MC # 3, Hype # hyp2, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold1/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (49.252281189,inf), test loss: 36.775707531\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.12108421326,inf), test loss: 2.9301408112\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (38.1215400696,27.5062776384), test loss: 36.6804077625\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.76861166954,2.5463311733), test loss: 3.68958826661\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (53.621055603,27.3539235668), test loss: 36.2189626694\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.19186401367,2.53980155277), test loss: 2.74777887464\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (7.04219961166,27.3492253388), test loss: 38.9986465454\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (5.16199874878,2.53710770947), test loss: 4.12765656114\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (27.932598114,27.4034476789), test loss: 40.9519863129\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.97493100166,2.53433382089), test loss: 3.53203487098\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (50.6962089539,27.3747524693), test loss: 42.2909667492\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.87568831444,2.52812334585), test loss: 3.47176620066\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (11.7237167358,27.3150910023), test loss: 37.4635872841\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.47349488735,2.52621514181), test loss: 3.6742210865\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (15.7556838989,27.3183198123), test loss: 39.4247590542\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.598979592323,2.52511979781), test loss: 3.11315177083\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (20.0188026428,27.2902818362), test loss: 35.0092168808\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.10942268372,2.52632239443), test loss: 3.79593037367\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (26.5531044006,27.2576335974), test loss: 35.7670641899\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.96984779835,2.52577714659), test loss: 2.8077798903\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (35.1943473816,27.2606956849), test loss: 35.1869525433\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (5.92175102234,2.52384521582), test loss: 4.00400564075\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (32.1438369751,27.2617582339), test loss: 36.6665041924\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.0709605217,2.52289982951), test loss: 2.98020750433\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (6.50248146057,27.2301186572), test loss: 38.8085001945\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.22655558586,2.52076613449), test loss: 3.84207152724\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (53.0512962341,27.2094814151), test loss: 38.9343858719\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.01673364639,2.52018903202), test loss: 3.00813283324\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (49.4646682739,27.1972051236), test loss: 36.625759387\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.47831535339,2.51901140692), test loss: 3.90269497037\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (43.8397750854,27.1798594092), test loss: 34.3492073536\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.23191022873,2.5193838284), test loss: 3.34175435305\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (21.6081371307,27.151938653), test loss: 38.5007574797\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.04079985619,2.51851278712), test loss: 3.88609058261\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (17.3484649658,27.1426199978), test loss: 34.1311319828\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.930475950241,2.51659350455), test loss: 3.70385849774\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (7.66135120392,27.1384296866), test loss: 40.3501414776\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.762738525867,2.51594648761), test loss: 3.17967995405\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (46.4144172668,27.1225546978), test loss: 36.6572183609\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.22549843788,2.51479995418), test loss: 3.84840241075\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (21.4058456421,27.0932383982), test loss: 40.59340868\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.51387548447,2.5141197797), test loss: 2.94884252548\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (18.974029541,27.084169156), test loss: 32.9086220741\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.60824334621,2.51302598106), test loss: 3.83356640339\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (16.4832839966,27.067019863), test loss: 37.670010376\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.405936092138,2.51280982153), test loss: 2.9713185817\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (29.318862915,27.0450862596), test loss: 35.4354957581\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.49917197227,2.51217172608), test loss: 3.77018070221\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (28.1706409454,27.0366671468), test loss: 35.4119949341\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.91348552704,2.51092266671), test loss: 2.93781312406\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (30.7657623291,27.0299531215), test loss: 36.0528595448\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.62008285522,2.51020851388), test loss: 3.84541455507\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (23.8891563416,27.0081623353), test loss: 33.9041268826\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.97572374344,2.50863261506), test loss: 3.17594064772\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (23.5592765808,26.9893064964), test loss: 39.3993028641\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.81280374527,2.50804731263), test loss: 4.36095992923\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (3.20124483109,26.9769457326), test loss: 34.7165272713\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.980885803699,2.50718887783), test loss: 3.79006130099\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (12.8766326904,26.9591131434), test loss: 38.6489271641\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.24990320206,2.50693209246), test loss: 3.32226720452\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (25.9535427094,26.94166053), test loss: 37.9459797382\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.69651269913,2.50621403398), test loss: 3.69280237556\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (33.7901077271,26.9302912739), test loss: 39.8478971958\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.8525826931,2.50481777585), test loss: 2.90629265606\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (11.9431409836,26.919845351), test loss: 34.5545937061\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.75729739666,2.50400052625), test loss: 3.83169013262\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (52.871509552,26.9020641539), test loss: 35.8847319126\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.60110712051,2.50292430398), test loss: 3.01019430161\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (48.433303833,26.8841939064), test loss: 34.8796111107\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.80152988434,2.50225900592), test loss: 4.1612909317\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (30.8074531555,26.8735726566), test loss: 37.3192756176\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.52856683731,2.50144902327), test loss: 3.10589553714\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (23.7622890472,26.8555078394), test loss: 38.8985262394\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.11090230942,2.50101727472), test loss: 3.55852536559\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (14.868514061,26.8366627011), test loss: 35.9893529415\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.4067363739,2.50020856328), test loss: 2.81829652488\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (27.5204238892,26.8246953068), test loss: 35.594686079\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.64944720268,2.4990908599), test loss: 3.75550553203\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (30.3173103333,26.817792961), test loss: 34.0352725506\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.94839143753,2.49838863326), test loss: 3.36876721978\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (12.294549942,26.7975078012), test loss: 37.4187202454\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.18695902824,2.49716028097), test loss: 3.99781859219\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (15.1100730896,26.7804245092), test loss: 35.7843795776\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.04873657227,2.49645003942), test loss: 3.77389435768\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (16.7527084351,26.7674921527), test loss: 48.8541492462\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.686681032181,2.49543683368), test loss: 3.16269775629\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (47.6640281677,26.7525077974), test loss: 39.579011631\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.59330832958,2.49522844095), test loss: 3.81585967541\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (20.7903079987,26.736860767), test loss: 40.3232051849\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.31714177132,2.49432846223), test loss: 2.86152955294\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (11.309305191,26.7226009626), test loss: 32.488928175\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.26978588104,2.49336936994), test loss: 3.85192052126\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (40.9166946411,26.7116123304), test loss: 37.1808248997\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (6.79445123672,2.49249218234), test loss: 3.1060395211\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (25.5630874634,26.6938270205), test loss: 36.0619864464\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.73162412643,2.49139838805), test loss: 3.84116036892\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.0689258575,26.6802344509), test loss: 35.1372117043\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.14933252335,2.49061323277), test loss: 3.06793579757\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (17.9251441956,26.6658879686), test loss: 38.6354477882\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.587615966797,2.48992943573), test loss: 3.69202396274\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (36.7634735107,26.6496528378), test loss: 36.2288009644\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.05741119385,2.48946027965), test loss: 3.34418094456\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (12.9733905792,26.6349993656), test loss: 36.9301450253\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.19795680046,2.48848093442), test loss: 3.89984842241\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (15.9272871017,26.6219498654), test loss: 33.8946474552\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.13870668411,2.48761431723), test loss: 3.8299066335\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (26.8962402344,26.610775421), test loss: 39.8106133461\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.21473789215,2.48668571113), test loss: 3.3080512464\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (20.5936775208,26.5916818038), test loss: 38.0765160084\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.32774400711,2.48577145213), test loss: 3.65293121934\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (29.1884288788,26.5790481356), test loss: 40.504262495\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.9510820508,2.48485928129), test loss: 2.8912411809\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (48.8503303528,26.5651159702), test loss: 38.6220926285\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.3575155735,2.48421287792), test loss: 3.84239398241\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (103.687095642,26.5501122805), test loss: 53.8159358025\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (6.9182882309,2.48372459684), test loss: 3.11976807117\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (7.83950233459,26.5359313888), test loss: 34.1768129826\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.28127503395,2.48287806952), test loss: 3.83022944927\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (18.4637298584,26.5238431594), test loss: 38.3277850151\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.86012220383,2.48192997802), test loss: 3.32686859965\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (19.7960147858,26.5106968476), test loss: 36.4740175247\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.56603813171,2.48099034303), test loss: 3.58952210546\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (20.3345108032,26.4930797382), test loss: 37.9431776047\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.974146962166,2.48002330755), test loss: 2.82705438435\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (19.6112480164,26.4805196102), test loss: 41.0972680569\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.33656525612,2.47922111639), test loss: 3.7813632369\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (16.0536613464,26.4654810671), test loss: 34.6724647522\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.34101676941,2.47862331795), test loss: 3.62347575426\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (22.6165962219,26.450421437), test loss: 37.910663414\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.20473623276,2.47797094921), test loss: 3.94912546277\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (42.5211448669,26.4391309848), test loss: 33.6362078667\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.16531133652,2.47716234932), test loss: 3.73333697617\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.604434967,26.4253413942), test loss: 40.8287615776\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.67126703262,2.47626920007), test loss: 3.03785286248\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (20.4516716003,26.4126037327), test loss: 35.5823832512\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.90489137173,2.47544098284), test loss: 3.86987738013\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (32.9880981445,26.396004721), test loss: 42.1818063259\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.03426396847,2.47447195008), test loss: 2.90826847255\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (14.0189876556,26.3827325387), test loss: 35.2806823492\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.11433005333,2.47371173336), test loss: 3.78876329064\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (17.1551456451,26.3690146876), test loss: 37.1613811493\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.30121994019,2.47299201055), test loss: 2.92839092314\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (16.0842189789,26.3525977272), test loss: 37.22585783\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.99024057388,2.47231597957), test loss: 3.75405023098\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (45.1537895203,26.3414371228), test loss: 34.9994632244\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.940117955208,2.47156005794), test loss: 2.84429873228\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (18.0441360474,26.3298331851), test loss: 38.3126661301\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.90412533283,2.47076759622), test loss: 3.51847089529\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (21.6400432587,26.3167288195), test loss: 35.3319271088\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (4.41500759125,2.46990390205), test loss: 3.46976994872\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (14.9291200638,26.2989135675), test loss: 36.3748065948\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.11676025391,2.46891136876), test loss: 3.88436431289\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.7633285522,26.2875522551), test loss: 34.2095164299\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.80131578445,2.46811782529), test loss: 3.59536014795\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.9448719025,26.2731988354), test loss: 38.5629527092\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.685090124607,2.46752692627), test loss: 3.35599757731\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (23.2730407715,26.258234182), test loss: 37.5350637436\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.936548829079,2.46685012184), test loss: 3.64466496706\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (58.9355964661,26.245580036), test loss: 44.9798669815\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.16108584404,2.46609803821), test loss: 2.824124071\n",
      "\n",
      "MC # 3, Hype # hyp2, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold2/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (55.5265731812,inf), test loss: 29.1212008476\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.73931956291,inf), test loss: 3.00570990443\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (30.5461502075,28.0094010987), test loss: 30.8536230087\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.72588539124,2.49650533432), test loss: 3.23878067732\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (23.9908485413,27.9586050668), test loss: 30.0353694916\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (0.929656744003,2.49002243578), test loss: 3.43744131029\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (33.2197799683,27.9956233435), test loss: 37.8472931862\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.09526634216,2.49083089862), test loss: 3.63309160471\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (20.2717781067,27.9820010314), test loss: 31.8087340593\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.44702589512,2.48626898222), test loss: 3.26294078827\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (15.4561443329,27.9802002671), test loss: 29.3433963299\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.66974210739,2.48615436383), test loss: 3.03953620195\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (38.3773956299,27.9221219887), test loss: 33.8864746094\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.67727732658,2.48579251609), test loss: 3.31226485968\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (19.7640113831,27.9193770027), test loss: 36.2778400302\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.31129419804,2.48431177325), test loss: 2.9489728272\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (19.9166927338,27.8748144978), test loss: 35.4336715221\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.20123672485,2.48389524782), test loss: 3.4211060524\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (31.0151119232,27.8376111925), test loss: 28.6338662148\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.884102642536,2.48213880695), test loss: 3.38772990257\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (9.09662246704,27.8403656429), test loss: 33.2717963219\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.89972805977,2.48121339865), test loss: 3.50679515004\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (20.9842987061,27.8331964296), test loss: 29.1167058945\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.83488368988,2.47957669356), test loss: 3.07320639491\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (49.3078575134,27.8318159984), test loss: 30.6787600517\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.70008182526,2.47854582197), test loss: 3.59881258011\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (28.4208259583,27.806186398), test loss: 27.7809805393\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.24072694778,2.47714197099), test loss: 2.87844013572\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (17.5097141266,27.7943099477), test loss: 32.0974017024\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (6.71388149261,2.47631428834), test loss: 3.82728478909\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (29.7189903259,27.7867460074), test loss: 38.6826189041\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.727150022984,2.47510324642), test loss: 3.50630166531\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (57.6782913208,27.7638781112), test loss: 35.6547934532\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.50656151772,2.47523343527), test loss: 2.94015668631\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (10.0816164017,27.7339941397), test loss: 30.1048276663\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.2765212059,2.47435173315), test loss: 3.27978464067\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (24.7441253662,27.7196628763), test loss: 30.9142869949\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.90655863285,2.47281880958), test loss: 2.93910528719\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (25.430393219,27.7030837811), test loss: 32.5864143491\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.06891036034,2.47196873262), test loss: 3.4057483077\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (43.0900802612,27.6790417953), test loss: 27.6875283241\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.1758159399,2.47076106274), test loss: 3.03837194443\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (17.4865703583,27.662081681), test loss: 34.4126580715\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.79939723015,2.47021690151), test loss: 4.01933157444\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (14.5928201675,27.6553427844), test loss: 28.9517380714\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.65310096741,2.46890592455), test loss: 3.27677296996\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (30.0043048859,27.6375220695), test loss: 31.2124276638\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (4.07010364532,2.46834950613), test loss: 3.23327816725\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (16.8945617676,27.6151202365), test loss: 28.0780101299\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.62887132168,2.4670215215), test loss: 2.91494156122\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (30.6561336517,27.605975542), test loss: 31.4514034271\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.1486749649,2.46552214705), test loss: 3.49300999045\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (14.4854373932,27.5952150379), test loss: 29.7989422798\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.539511084557,2.46480543752), test loss: 3.3637224555\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (46.9625740051,27.5790547705), test loss: 33.9564347267\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.07549905777,2.46410188401), test loss: 3.72756250203\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (43.9360961914,27.5582122103), test loss: 29.7436011791\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.23989260197,2.46349406619), test loss: 3.50093481541\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (27.9779624939,27.5412086971), test loss: 31.9658256531\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.49696213007,2.46252125676), test loss: 3.1961945653\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (13.6166744232,27.5176600434), test loss: 31.9870669365\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.17300343513,2.46190119952), test loss: 3.3935470283\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (8.4999332428,27.4944264324), test loss: 30.4658094645\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.8193769455,2.46083406181), test loss: 2.89294693768\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (14.3869285583,27.4823627431), test loss: 37.1068258762\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.50418281555,2.46003566891), test loss: 3.68632594943\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (11.4822235107,27.4728185918), test loss: 30.3507102013\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (0.989129245281,2.45888952569), test loss: 3.04246985912\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (16.8663063049,27.4624196736), test loss: 32.4087765217\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.11258292198,2.45774091587), test loss: 3.4309550941\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (19.7133483887,27.4431689875), test loss: 28.3957853317\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.34794580936,2.45680485716), test loss: 2.81764856279\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (13.8443698883,27.4326220724), test loss: 30.8071018338\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.53413081169,2.45578670245), test loss: 3.45442103148\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (37.3838043213,27.412638623), test loss: 29.9902991295\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.918017745018,2.45499064953), test loss: 3.37991734147\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.1506681442,27.3935057926), test loss: 34.4859489918\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.47777891159,2.45436104043), test loss: 3.4908557415\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (37.9774208069,27.3810428299), test loss: 29.921195507\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.841442286968,2.45363230691), test loss: 3.45900254548\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (32.2050247192,27.3629436391), test loss: 32.1644339561\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.79624581337,2.45261494131), test loss: 3.35431591272\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (20.5380401611,27.3465549527), test loss: 33.5204154253\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.666052877903,2.45165537355), test loss: 3.38770171404\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (19.2554950714,27.3263917894), test loss: 34.6686855555\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.714016973972,2.45075581003), test loss: 2.89976462126\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (17.7813777924,27.3150917623), test loss: 39.2215036869\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.940993905067,2.44977795687), test loss: 3.41577341557\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (45.2296562195,27.3002713662), test loss: 45.5435817719\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.986873447895,2.44898970792), test loss: 3.22940621972\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (71.2438964844,27.2852853051), test loss: 44.3286091328\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (7.48824548721,2.4482810114), test loss: 3.5492266655\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (10.0952453613,27.2700884017), test loss: 28.3672005653\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.82890748978,2.44717420704), test loss: 2.89882113338\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (3.59086942673,27.2554639719), test loss: 32.1106477737\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.04129385948,2.44621411143), test loss: 3.49401113987\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (20.3838729858,27.2428223366), test loss: 30.1267981052\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.96241235733,2.44539829265), test loss: 3.41263874173\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (49.7329559326,27.2269151226), test loss: 31.6200119019\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.822153568268,2.44444224298), test loss: 3.73481462598\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (15.7139987946,27.2095576129), test loss: 32.9330100536\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (4.0396399498,2.44380822907), test loss: 3.56105048656\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (48.6121139526,27.1955189879), test loss: 33.0323837757\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (5.27597808838,2.44293989292), test loss: 2.97327332795\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (25.5996742249,27.1755853896), test loss: 35.0818157673\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.68686306477,2.44218237596), test loss: 3.41479846835\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (11.1758785248,27.1562110657), test loss: 31.7727001905\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.29905533791,2.44129311659), test loss: 2.90624783337\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (22.5091438293,27.1438228929), test loss: 31.7266090393\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.843562066555,2.44009519313), test loss: 3.4081612438\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (29.1013641357,27.1317033722), test loss: 30.352406311\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.52617537975,2.43924032281), test loss: 3.10096370578\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (9.14446258545,27.1158889674), test loss: 32.2427453518\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.56233763695,2.43831213131), test loss: 3.46784780025\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (23.7809734344,27.1017623308), test loss: 30.4228946686\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.18364715576,2.43758313342), test loss: 3.07391754389\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.711856842,27.0883805076), test loss: 30.4478271008\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.04044771194,2.43656273512), test loss: 3.37905679047\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.3056678772,27.0724626804), test loss: 30.5794481754\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.335814595222,2.43582946551), test loss: 3.50325652361\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (23.9612121582,27.0549095068), test loss: 32.6489821434\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.85114073753,2.43494594), test loss: 3.6190877974\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (14.9797115326,27.0406371994), test loss: 29.133210206\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.00348174572,2.43395360202), test loss: 3.43037735224\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (32.5382423401,27.0264255241), test loss: 29.9401834965\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.11868143082,2.43326642429), test loss: 2.96751053929\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (5.68161010742,27.0092127216), test loss: 33.211321044\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.84415102005,2.43240507722), test loss: 3.52354841828\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.0688686371,26.9924219577), test loss: 31.3365255117\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.59034967422,2.43159750624), test loss: 3.00015908778\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (27.6986618042,26.9790408075), test loss: 31.2991578579\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.05964851379,2.43061057457), test loss: 3.54137205482\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (38.0605926514,26.9630172039), test loss: 34.8218479156\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.38016819954,2.42986642548), test loss: 3.22805991471\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (50.2502479553,26.9479548709), test loss: 32.3154578567\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.31552481651,2.42897490189), test loss: 3.32965068817\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (7.50292062759,26.9339155952), test loss: 32.9430672646\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.77173948288,2.42815115136), test loss: 3.05822927952\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (19.4188899994,26.9216324387), test loss: 32.9595721006\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.40609622002,2.42717240413), test loss: 3.30690618157\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (25.9151115417,26.9087009121), test loss: 30.4591975212\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.02245235443,2.42632512718), test loss: 3.56737312376\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (23.6882781982,26.8913457991), test loss: 30.7400588989\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.98193120956,2.42554653348), test loss: 3.69817894697\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (35.731300354,26.8782003466), test loss: 30.9604635477\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (4.44869709015,2.4247546739), test loss: 3.39166720212\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (19.0248260498,26.8594597934), test loss: 29.7226595879\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.54944252968,2.42390580334), test loss: 3.0014202714\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (27.7256660461,26.8418357406), test loss: 33.4922493935\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.66543340683,2.42315673849), test loss: 3.53072091937\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (43.7823753357,26.8285573749), test loss: 31.2195331097\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.91541862488,2.42228012533), test loss: 2.85067529082\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (50.6946907043,26.8140409083), test loss: 33.4802675247\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (4.4227514267,2.42139413798), test loss: 3.65071703792\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (31.1473197937,26.8011971107), test loss: 31.1379436255\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.44632291794,2.42042756232), test loss: 3.14636301696\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (25.8483924866,26.7859789566), test loss: 31.692673254\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (4.15885162354,2.41961157465), test loss: 3.50924276114\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (16.3780784607,26.7736349282), test loss: 30.1092905998\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.69886231422,2.41867102683), test loss: 3.0829590857\n",
      "\n",
      "MC # 3, Hype # hyp2, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold3/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (33.7656097412,inf), test loss: 33.3576874733\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.13166928291,inf), test loss: 3.22928175032\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (22.9241294861,26.6127902017), test loss: 31.0911703587\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.1158952713,2.54796274489), test loss: 2.96007416248\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (27.2630729675,26.5002414742), test loss: 31.2182378292\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.39885377884,2.55003322512), test loss: 2.64342949092\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (25.7965736389,26.5277049723), test loss: 30.4940901041\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.00410032272,2.53801484909), test loss: 3.17408046722\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.6318969727,26.5158585764), test loss: 31.3824530602\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.96463608742,2.53392618291), test loss: 3.16294974387\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (44.4540367126,26.4935098216), test loss: 31.7623373032\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.17462491989,2.53061164845), test loss: 3.23436422944\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (14.8420801163,26.5337660367), test loss: 34.8873798847\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.94619297981,2.53128787558), test loss: 3.11302010417\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (41.4267807007,26.5722557622), test loss: 33.1941100836\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.71310853958,2.53044867737), test loss: 2.7766464889\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (29.610080719,26.558233481), test loss: 34.0218514681\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.74279606342,2.53054717493), test loss: 3.21542210579\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (7.22888755798,26.5603383876), test loss: 33.1195290089\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.25572109222,2.5292450245), test loss: 2.69059452415\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (22.8305912018,26.5512287813), test loss: 31.565559864\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.05491781235,2.52710982724), test loss: 3.40872857571\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (20.9862480164,26.5465408939), test loss: 31.1182818413\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.83277368546,2.52600660081), test loss: 3.0625657469\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (13.1427640915,26.5180303773), test loss: 32.6247975826\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.2286863327,2.52403672844), test loss: 3.1157074213\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (43.927406311,26.4932174704), test loss: 33.4926651001\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.92763400078,2.52264061991), test loss: 3.12333947122\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (46.4234542847,26.4539058783), test loss: 30.8202210903\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.21950268745,2.52258611931), test loss: 2.9583329916\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (29.3914375305,26.4168267532), test loss: 31.2605544567\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.99898386002,2.52263362094), test loss: 2.65430442989\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (24.1226139069,26.4112247996), test loss: 30.9131603241\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.86011815071,2.52063840662), test loss: 3.15025911629\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (27.5328502655,26.4047913288), test loss: 31.6872141361\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.843984246254,2.51943362731), test loss: 3.07527487427\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (8.91171264648,26.3881289436), test loss: 32.0909398556\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.49592161179,2.51788160125), test loss: 3.28167898357\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (4.69746541977,26.389319951), test loss: 32.8038745403\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (5.9082365036,2.51694965363), test loss: 3.26346178353\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (28.3622760773,26.3819133411), test loss: 36.6301126003\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.21249139309,2.51622548196), test loss: 2.84766067863\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (70.0852355957,26.3607934817), test loss: 41.2880827904\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (5.68838024139,2.51638861026), test loss: 3.10058739781\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (25.1909637451,26.352851446), test loss: 32.7291930676\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.89385986328,2.51538941369), test loss: 2.67622887492\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (27.2374973297,26.3327998784), test loss: 32.5422162056\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.24847149849,2.51398894664), test loss: 3.27410771251\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (26.5844230652,26.3113827239), test loss: 30.821145916\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.69321918488,2.5128538984), test loss: 3.02259526253\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (24.3096370697,26.2816938186), test loss: 32.3972473621\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.85259890556,2.51144946436), test loss: 3.24960052967\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (27.6757278442,26.258489464), test loss: 33.6424224854\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.371310770512,2.51027198052), test loss: 3.10663815439\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (14.5841083527,26.2362207337), test loss: 31.7513307095\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.12293481827,2.50983661907), test loss: 3.05132015944\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (26.4743309021,26.2197656617), test loss: 30.2186721802\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.4581360817,2.50967808899), test loss: 2.69348487556\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (32.2722663879,26.2145997337), test loss: 31.7712428093\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.52026093006,2.50802077656), test loss: 3.14771072268\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (27.8682498932,26.2063311771), test loss: 31.442618227\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.888391673565,2.50691595813), test loss: 2.77010023296\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (28.8827209473,26.1905982649), test loss: 31.7197159529\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.27350771427,2.50561946279), test loss: 3.22489494979\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (8.6918964386,26.1806942303), test loss: 31.8919621944\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.595121443272,2.50475255289), test loss: 3.21128399521\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (39.3045806885,26.1666079891), test loss: 34.8114640713\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.52664351463,2.50431348212), test loss: 3.08657704592\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (33.6482696533,26.1431239023), test loss: 35.2714922428\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.92712831497,2.50375203471), test loss: 3.10558965206\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (27.1317958832,26.1271105808), test loss: 32.3203811169\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.98468470573,2.5028103227), test loss: 2.65013546944\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (18.7035751343,26.1033622799), test loss: 37.2246120453\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.70679926872,2.50162060284), test loss: 3.08139505386\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (50.217414856,26.0823788685), test loss: 30.5056399822\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.35937476158,2.500365116), test loss: 2.92783069462\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (17.1882514954,26.0620010081), test loss: 32.6382762432\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.660369217396,2.49928479779), test loss: 3.381266886\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (37.4174385071,26.048950578), test loss: 31.5591907501\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.40936374664,2.49854094013), test loss: 2.96676295698\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (7.92516565323,26.0350257032), test loss: 31.7674995661\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.932717204094,2.49784600323), test loss: 3.06536729932\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (25.1178436279,26.0196853104), test loss: 31.7300499916\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.51262927055,2.49732329705), test loss: 2.65896762311\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (30.0775127411,26.0107112502), test loss: 30.5189468384\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.88687157631,2.49597310732), test loss: 3.10699193776\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (34.6237945557,25.9987797988), test loss: 34.3530391693\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.87337470055,2.49517923735), test loss: 2.76633813977\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (20.4472846985,25.9819588729), test loss: 30.6121432066\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.83332824707,2.49402305767), test loss: 3.23721108437\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (48.8185882568,25.9688055089), test loss: 32.0057632923\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (5.0329246521,2.4931638196), test loss: 3.23514307737\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (52.5256958008,25.9484420793), test loss: 34.6313834667\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.90156114101,2.49245576724), test loss: 3.1011975646\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (21.6009750366,25.9230939452), test loss: 35.3271328926\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.590917527676,2.49168109682), test loss: 3.26216941178\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (27.5525608063,25.9051995497), test loss: 35.2959736824\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.51076483727,2.49075899282), test loss: 2.62152450383\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (19.9137191772,25.8879296618), test loss: 38.2496552944\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.733073294163,2.48969457401), test loss: 3.14482119083\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (54.9711341858,25.8722607092), test loss: 32.4556857824\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.75651621819,2.48863108046), test loss: 2.79826326072\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (46.7205276489,25.8592891161), test loss: 32.7337227345\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.9519931078,2.48773917346), test loss: 3.42452357411\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (36.011592865,25.8477482627), test loss: 32.9723471165\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.58860111237,2.48676842933), test loss: 3.1013094902\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.2594089508,25.834241027), test loss: 33.6649322033\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.823688030243,2.48607637243), test loss: 3.14902929068\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (7.86880016327,25.8177234617), test loss: 30.5416991234\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.90359556675,2.48560270938), test loss: 2.68510731161\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (21.1019058228,25.8070638713), test loss: 30.0452118158\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.30579519272,2.4844772122), test loss: 3.043154037\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (29.6887512207,25.7921298574), test loss: 31.3475574017\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.18627810478,2.48354394919), test loss: 2.94167436808\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (38.6406860352,25.7721324519), test loss: 29.6769548416\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (5.71764135361,2.48252954545), test loss: 3.27263914943\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (22.1678771973,25.7534270614), test loss: 41.9630368233\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.67279493809,2.48152034534), test loss: 3.00501615703\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (18.6019287109,25.7325217709), test loss: 31.8677314043\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.9411277771,2.48080701076), test loss: 3.11329434514\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (18.6581287384,25.7124816331), test loss: 35.5160612106\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.87837636471,2.48000882137), test loss: 3.21189819574\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (45.5675086975,25.6999735858), test loss: 36.9990977049\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.6814494133,2.47921729868), test loss: 2.8927715987\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (18.373041153,25.6867140008), test loss: 43.2910530567\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.91940104961,2.47825525138), test loss: 3.17595476508\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (7.10025215149,25.6731758422), test loss: 32.1464169979\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.38376426697,2.47703733991), test loss: 3.0466409564\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (17.1418418884,25.6607349613), test loss: 32.5506740093\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.67625641823,2.47619127182), test loss: 3.26498293281\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (11.7598590851,25.6477170411), test loss: 31.3070576191\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.2679028511,2.47536368826), test loss: 2.95550787151\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.0035839081,25.6332065196), test loss: 33.1194432497\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.757703781128,2.47471936779), test loss: 3.1155292958\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (11.0121860504,25.6171202075), test loss: 30.6233980179\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.86541128159,2.47414789664), test loss: 2.73848829865\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (49.6135253906,25.6020608271), test loss: 29.4449985504\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.42301762104,2.47300288492), test loss: 3.05836920738\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (10.5495147705,25.5827040495), test loss: 33.0669012547\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (5.7892127037,2.47210131245), test loss: 2.97364186943\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (14.2159423828,25.5620874555), test loss: 34.8164111614\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.740238010883,2.47096465994), test loss: 3.25956332982\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (16.7211894989,25.5464882556), test loss: 34.8285221577\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.25408506393,2.47009882184), test loss: 3.03865375519\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (40.4289016724,25.5307572084), test loss: 32.8560003281\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (5.43503761292,2.46940141645), test loss: 3.11970000863\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (23.3746891022,25.5151090775), test loss: 34.0157844067\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.671635150909,2.46869281522), test loss: 3.12160578966\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (11.1343050003,25.5034013537), test loss: 34.2667629004\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.8486187458,2.46776755908), test loss: 2.95111944079\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (28.7483177185,25.4917774779), test loss: 33.3630386829\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (6.27345371246,2.46679996959), test loss: 3.27399425507\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (45.0321846008,25.4782176447), test loss: 31.3131186008\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.56396746635,2.46576469978), test loss: 2.87204789072\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (16.1816577911,25.4644348424), test loss: 33.6724453449\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.91111183167,2.46503456804), test loss: 3.27562846541\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (11.7462024689,25.4502005557), test loss: 32.4634249687\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.53425645828,2.46419079035), test loss: 3.09148095399\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (39.1443252563,25.4333208391), test loss: 32.2529927254\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.58418726921,2.46351090032), test loss: 3.087712726\n",
      "\n",
      "MC # 3, Hype # hyp2, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold4/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (40.3906326294,inf), test loss: 36.1743911266\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.99057364464,inf), test loss: 2.88686118722\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (54.5464935303,27.1984726458), test loss: 36.4402894974\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.49078655243,2.51891728795), test loss: 3.45816684365\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (38.2868499756,27.0946862702), test loss: 31.6239547729\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.85769033432,2.51942571908), test loss: 3.50765110254\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (20.0943660736,27.1589277326), test loss: 33.36842103\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.2853269577,2.51652435189), test loss: 3.80400788188\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (34.5814819336,27.141147915), test loss: 31.1909003735\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.81056213379,2.5087252879), test loss: 3.69964566827\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (22.4160614014,27.1548851844), test loss: 34.8982069969\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.734685599804,2.50573053194), test loss: 3.07443248034\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (30.4152526855,27.1138323347), test loss: 33.7998674393\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.37073421478,2.5069512452), test loss: 3.53712126613\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (39.2850379944,27.0862636649), test loss: 37.3655243874\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.10032343864,2.50702054814), test loss: 2.95635656714\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (21.607837677,27.0775314736), test loss: 30.6603118896\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.288081586361,2.50479625532), test loss: 3.60649477541\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (35.7005996704,27.0447338092), test loss: 34.0546977997\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.589692294598,2.50556373473), test loss: 2.84467858672\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (14.3573951721,27.0191135451), test loss: 34.9359911442\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.33248376846,2.50555994471), test loss: 3.34631127715\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (3.89447212219,27.0228898667), test loss: 33.4608991146\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.28524971008,2.50308365067), test loss: 2.75609890819\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (3.75855350494,26.9939985627), test loss: 33.2777662754\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.42528676987,2.50147647995), test loss: 3.63942807913\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (17.196187973,26.9949214778), test loss: 30.583259964\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.852978467941,2.50043837195), test loss: 3.70018848777\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (49.14635849,26.9709065108), test loss: 34.7127289295\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.23309421539,2.49893149132), test loss: 3.78720175922\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (15.4836845398,26.9473223153), test loss: 32.482947731\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.60506868362,2.498861636), test loss: 3.71874221861\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (30.9654006958,26.935151825), test loss: 36.4344577312\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.03439664841,2.49834968059), test loss: 3.08032738045\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (19.5796222687,26.9117425993), test loss: 31.8271696329\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.96302103996,2.49755436915), test loss: 3.57803511024\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (17.6437339783,26.8945223071), test loss: 33.3347930431\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.453943222761,2.49643828474), test loss: 2.92801942825\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (32.008026123,26.8829351339), test loss: 32.5144099236\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.45856332779,2.49536902873), test loss: 3.58672241271\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (41.3585357666,26.8670594917), test loss: 31.6242803097\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.12079906464,2.49409617089), test loss: 2.92149299383\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (16.9672145844,26.8588724982), test loss: 33.0589135647\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.49924874306,2.49282996061), test loss: 3.46757904887\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (9.9790353775,26.8313491075), test loss: 29.3366920471\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.64538824558,2.49166448058), test loss: 3.28666182756\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (17.1001739502,26.8159006555), test loss: 36.0964729309\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.47695207596,2.49147456492), test loss: 4.20915752649\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (35.7511177063,26.8002331717), test loss: 32.6337748528\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.46653544903,2.49032197379), test loss: 4.09786712229\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (35.5804748535,26.7805331664), test loss: 34.6498733044\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.07428717613,2.48959076051), test loss: 3.15016823858\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (45.4163513184,26.7604973856), test loss: 33.953142643\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.95402336121,2.48904817553), test loss: 3.44823776484\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (31.4381370544,26.7492591114), test loss: 41.0814991474\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.75905621052,2.4877246348), test loss: 2.83060030937\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (18.6132717133,26.7335297272), test loss: 30.8785033703\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.10734653473,2.48612109943), test loss: 3.99144024253\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (21.4667663574,26.7200714288), test loss: 32.9859447956\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.62807846069,2.48553340142), test loss: 2.9082680732\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (43.9410896301,26.7016577531), test loss: 32.5271379709\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.43955886364,2.48449108329), test loss: 3.56837281585\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (53.2981262207,26.6858545185), test loss: 33.8848879337\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.78768825531,2.48374369898), test loss: 3.21956008673\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (13.8765878677,26.6656706657), test loss: 33.1953266621\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.09910738468,2.48293496028), test loss: 3.78987654448\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (33.1553535461,26.6500202942), test loss: 29.9482664585\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.8296456337,2.48232171758), test loss: 3.45594234467\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (9.70792770386,26.6302896937), test loss: 33.5125066996\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.47561705112,2.48122969856), test loss: 3.75697293282\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (46.3339920044,26.6170811984), test loss: 36.8905014038\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.17787313461,2.4800748693), test loss: 3.69597881436\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (15.1767835617,26.6027524612), test loss: 37.4631065369\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.01139783859,2.47898280167), test loss: 2.78484960794\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (31.0343952179,26.5896511294), test loss: 32.8156399965\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.03607797623,2.47806297169), test loss: 3.56826549768\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (22.8599338531,26.5705260887), test loss: 33.0240344524\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.737076759338,2.47702492954), test loss: 2.89423814714\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (25.8426151276,26.5538496993), test loss: 32.9005275249\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.09501266479,2.47661272104), test loss: 3.70011789501\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (3.42257165909,26.5396878144), test loss: 32.1299743176\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.55839645863,2.4756543068), test loss: 3.05550034791\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (11.6250371933,26.5224189021), test loss: 33.293583107\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.12326431274,2.47468160594), test loss: 3.41554059386\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (25.4977684021,26.5016192003), test loss: 31.0475171089\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.11561965942,2.47394356376), test loss: 2.76747218966\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (8.27228164673,26.4908203684), test loss: 35.1310369968\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (6.11142492294,2.47294644239), test loss: 3.72431032658\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (24.8350086212,26.477159589), test loss: 30.1518285751\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.49269914627,2.47157169022), test loss: 3.69814643264\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (20.4637393951,26.4612250113), test loss: 34.4772808075\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.80419325829,2.4707836796), test loss: 3.20188408792\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (20.2301445007,26.4430901507), test loss: 33.9067056179\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.21788358688,2.46998270663), test loss: 3.56433115304\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (23.3605613708,26.4282624897), test loss: 36.3200069427\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.41536235809,2.4690823663), test loss: 2.87341210544\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (25.9397239685,26.414684386), test loss: 30.9123650551\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.00323343277,2.46827294649), test loss: 3.61216158569\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (32.9671936035,26.3957567895), test loss: 32.7637367249\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.94299483299,2.46768934976), test loss: 2.96947297007\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (26.6779670715,26.3780456835), test loss: 31.3915025711\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.870071053505,2.46668052331), test loss: 3.41359354854\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (33.0635948181,26.3660745093), test loss: 33.9223305702\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (6.73406505585,2.46528803731), test loss: 2.95168864131\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (15.433681488,26.3505638048), test loss: 34.450169611\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.793360948563,2.46449633711), test loss: 3.35478870869\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (20.7335605621,26.3378970493), test loss: 33.3079580784\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.19224619865,2.46371801954), test loss: 3.95203130543\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (24.3524532318,26.3195448557), test loss: 34.8120460987\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.968328595161,2.46253991592), test loss: 3.82178334147\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (21.2822742462,26.3029325456), test loss: 30.6317464352\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.933433413506,2.46182811969), test loss: 3.71423385739\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (36.7462158203,26.290780644), test loss: 37.5854113102\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (4.67319202423,2.46125482875), test loss: 2.85295263529\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (51.9543380737,26.2725899432), test loss: 31.7884387493\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.60073900223,2.46028890547), test loss: 3.56920045018\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (18.7529430389,26.2551257558), test loss: 37.4835856438\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.43680930138,2.45941441575), test loss: 3.00949462056\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (12.7592334747,26.2422832963), test loss: 31.168594265\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.77678704262,2.45818181583), test loss: 3.48493072391\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (18.196893692,26.2285431546), test loss: 36.7757708788\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.43968105316,2.45738469609), test loss: 2.86869751215\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (31.0813789368,26.2150522377), test loss: 35.2556474209\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (6.02475070953,2.4564278902), test loss: 3.72146122456\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (23.0435657501,26.1945959734), test loss: 33.4035874367\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.27841448784,2.45554305558), test loss: 2.85879774392\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (20.9014472961,26.1821699636), test loss: 36.4722560406\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.90586578846,2.45474173785), test loss: 3.6652726531\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (17.3080291748,26.1681692679), test loss: 32.9511773586\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.700073719025,2.45392848206), test loss: 3.74850301743\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (16.2240867615,26.1495201908), test loss: 42.6171962738\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.33447313309,2.45318634787), test loss: 3.54883023798\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (14.9100036621,26.1343691957), test loss: 33.708262825\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.4279923439,2.45247036425), test loss: 3.40941504836\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (29.5756187439,26.1218848805), test loss: 37.4280056953\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.30511713028,2.45106284087), test loss: 2.90504422188\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (26.9472293854,26.1075383473), test loss: 30.7184631348\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (5.54181241989,2.45025933555), test loss: 3.6556332767\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (56.527469635,26.0939103026), test loss: 33.9445588112\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.3193821907,2.44941572639), test loss: 3.0108484596\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.9838685989,26.0757600229), test loss: 30.5719747543\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (4.86513233185,2.44851197898), test loss: 3.58006743193\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (16.359336853,26.062308817), test loss: 35.1634135723\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.797897040844,2.44763827645), test loss: 2.93215137124\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (70.5590438843,26.0475304987), test loss: 48.2363871098\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (4.67507553101,2.44710354185), test loss: 3.65016012788\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (26.6137142181,26.0312245341), test loss: 37.8486557961\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.60050702095,2.44621114882), test loss: 3.46511672735\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.70651435852,26.0163688163), test loss: 33.8854625702\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.2663807869,2.4452959217), test loss: 3.82472859323\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.7041683197,26.0016545817), test loss: 31.4933982372\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.91988873482,2.44413119623), test loss: 3.71570993066\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (18.0277900696,25.9891711481), test loss: 35.3890334129\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.09282469749,2.44325294055), test loss: 3.12277194858\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.9369525909,25.9742582179), test loss: 34.4301618099\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (4.7082157135,2.44231345323), test loss: 3.58521553278\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (18.3650588989,25.9571456693), test loss: 38.0468093872\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.17489814758,2.4414491078), test loss: 2.93572307527\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (21.4630947113,25.9447529453), test loss: 29.1844837189\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (4.03065872192,2.44082276403), test loss: 3.42294754386\n",
      "\n",
      "MC # 3, Hype # hyp2, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold5/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (37.0151901245,inf), test loss: 32.4973623276\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.37721514702,inf), test loss: 2.84259538054\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (16.0128669739,26.3187160783), test loss: 30.9528881788\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.04639863968,2.52062264273), test loss: 3.43217712045\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (26.7323455811,26.292562052), test loss: 32.0481925011\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.8793053627,2.51652128401), test loss: 2.74261835515\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (27.2413539886,26.3571652191), test loss: 32.6270813465\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.3158826828,2.50684769695), test loss: 3.77067677379\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (12.718252182,26.4026292593), test loss: 31.3868319035\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.91727507114,2.50527151357), test loss: 3.41397798657\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (12.5559196472,26.3598452362), test loss: 30.3671937704\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.925912439823,2.50399172221), test loss: 3.83675683439\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (42.9973602295,26.37635039), test loss: 31.5063589096\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (4.26201486588,2.50430352433), test loss: 3.55163559914\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (20.3342494965,26.3216400659), test loss: 30.639802146\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.0682311058,2.50418665908), test loss: 3.08474336863\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (30.0745582581,26.2931433998), test loss: 31.6793054581\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.500363707542,2.50426428371), test loss: 3.52310380042\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (29.1360740662,26.2943350185), test loss: 32.2097071171\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.35199332237,2.50115762946), test loss: 2.74357286692\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (12.2575511932,26.2970234547), test loss: 28.6222830772\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.04306936264,2.50093501032), test loss: 3.77694611549\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (22.9303741455,26.2748466715), test loss: 32.7979240894\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.01799726486,2.49953538368), test loss: 2.72050214112\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (32.2270050049,26.2678544933), test loss: 26.5526082039\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.5306892395,2.49876133377), test loss: 3.54384400249\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (23.7529506683,26.2360853483), test loss: 35.505271244\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.647742033,2.49854385964), test loss: 2.83263283819\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (16.776632309,26.212968689), test loss: 27.3068811655\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.13598656654,2.49803531785), test loss: 3.42430966198\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (21.4882392883,26.2017758671), test loss: 33.9404096603\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.67704761028,2.49560009135), test loss: 2.69821592569\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (7.67288446426,26.2005887585), test loss: 31.5956056595\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (6.28952026367,2.49537369541), test loss: 3.53623110652\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (33.1013336182,26.1785777897), test loss: 32.4822096348\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.4072072506,2.49378952425), test loss: 2.70080590248\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (13.6063232422,26.1691318279), test loss: 30.563681519\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.61912107468,2.49286727486), test loss: 3.69741382003\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (34.1334686279,26.1469181745), test loss: 30.706888485\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.49143576622,2.49255581464), test loss: 3.38952431679\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (29.055437088,26.1202050922), test loss: 29.8438628197\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.26994824409,2.49165649872), test loss: 3.77184690833\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (29.3363647461,26.1133331054), test loss: 30.8498783588\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.94964718819,2.48985665819), test loss: 3.60021125376\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (18.5640029907,26.1046551208), test loss: 30.7205198765\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.44582176208,2.48933946613), test loss: 3.13792120516\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (29.6115055084,26.0841474009), test loss: 31.9964689255\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.89442777634,2.48780673321), test loss: 3.47894745022\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (8.01842975616,26.0766361699), test loss: 32.4929647446\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.66428661346,2.48721051656), test loss: 2.75851473212\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (39.1400680542,26.0529822956), test loss: 28.7200814009\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.29308462143,2.48654181847), test loss: 3.64870072007\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (18.8615989685,26.031972875), test loss: 32.3655636311\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.30627155304,2.48574610076), test loss: 2.70893287957\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (29.73828125,26.0222546337), test loss: 27.5320062876\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.86349284649,2.48426505887), test loss: 3.6063783586\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (24.6957969666,26.0124776261), test loss: 34.0952349186\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.52132534981,2.48367092903), test loss: 2.8815742299\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (24.7537002563,25.9944773948), test loss: 26.8176060915\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.08463001251,2.48247201878), test loss: 3.55716161132\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.5846366882,25.9830485326), test loss: 33.2209133148\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.75728917122,2.48173508339), test loss: 2.72571102381\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (38.2222824097,25.962808709), test loss: 31.7860165596\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.05615472794,2.48092086776), test loss: 3.42817968726\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (21.4395923615,25.9440680656), test loss: 34.7186515331\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.43625330925,2.48020675463), test loss: 2.72351805568\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (45.3014755249,25.9324618653), test loss: 30.0085963011\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.45222997665,2.47880814028), test loss: 3.6669265151\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (26.6294898987,25.9217882986), test loss: 33.5297581673\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.28483057022,2.47821713142), test loss: 3.38110790849\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (29.4766464233,25.9055117551), test loss: 30.6670664072\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.14881515503,2.47714191748), test loss: 3.94145185947\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (34.3826141357,25.8932198567), test loss: 32.4274586678\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.75224375725,2.47630987817), test loss: 3.57468790412\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (22.0924301147,25.8741521101), test loss: 30.9347328424\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.14673137665,2.4755722087), test loss: 3.14795126319\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (20.9984512329,25.8543153888), test loss: 30.7652297974\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.86263728142,2.47480296201), test loss: 3.52426890731\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (39.807888031,25.8448563223), test loss: 32.3333974838\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.21062636375,2.47341877143), test loss: 2.82894782424\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (23.9025688171,25.8329907004), test loss: 30.1594420671\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.15738844872,2.47286836162), test loss: 3.58736374378\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (30.7756614685,25.8170909044), test loss: 34.8478919029\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (6.04714727402,2.47191613338), test loss: 2.96820512414\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (26.4846687317,25.8044010834), test loss: 26.7860227108\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.59947538376,2.47102001694), test loss: 3.55709941089\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (27.6492176056,25.7864918452), test loss: 33.8905814886\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.22108268738,2.47028227312), test loss: 2.83049225956\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (16.0987033844,25.7688890138), test loss: 26.7723505735\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.88246333599,2.46952888772), test loss: 3.53662528992\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (23.1001873016,25.756823131), test loss: 32.8315290928\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.98835992813,2.46822664814), test loss: 2.76794579774\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (26.1987247467,25.7463721943), test loss: 30.4438208103\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.77991127968,2.46767980532), test loss: 3.36180324256\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (23.5207481384,25.7297689494), test loss: 34.471831131\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.12866353989,2.46671818815), test loss: 2.95131532103\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (16.8540401459,25.7175371457), test loss: 30.3217905045\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.97761321068,2.46585415342), test loss: 3.63569753766\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (18.9143600464,25.7000839525), test loss: 30.5845235348\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.42001342773,2.46510316087), test loss: 3.41906781197\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (29.0062026978,25.6820808858), test loss: 29.4479414463\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.93304085732,2.46430822492), test loss: 3.84988932908\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (8.25657463074,25.6710373231), test loss: 31.289816761\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.57299256325,2.46314018405), test loss: 3.68165464401\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (27.5697288513,25.6595411521), test loss: 30.4208331585\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.853998363018,2.46248211145), test loss: 3.11599203646\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (46.5591163635,25.6448310837), test loss: 31.9450665951\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.45636677742,2.4615538327), test loss: 3.51763757765\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (12.0428562164,25.632587779), test loss: 33.2219340324\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.13442361355,2.46076645356), test loss: 2.76134574413\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (21.5920562744,25.6141782801), test loss: 26.8427746773\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.720710098743,2.45992824124), test loss: 3.60659580231\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (29.21824646,25.5979697793), test loss: 32.2337742805\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.56874608994,2.45921755661), test loss: 2.75156445503\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (25.3031368256,25.5868520492), test loss: 24.8040496588\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.22571444511,2.45807432173), test loss: 3.5579773277\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (23.6970977783,25.5748079843), test loss: 33.663381052\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.745789170265,2.45738268282), test loss: 2.8594689399\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (47.4212417603,25.5609502347), test loss: 29.4319716454\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.91979956627,2.45651821582), test loss: 3.54809600115\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (19.4234809875,25.5470577657), test loss: 36.497388792\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.699256300926,2.45566993504), test loss: 2.76201508939\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (32.6153717041,25.5308293211), test loss: 32.4326254129\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.736276745796,2.45486988187), test loss: 3.42614199519\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (5.3182091713,25.5150348642), test loss: 31.7404555321\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.38312745094,2.45417744269), test loss: 2.71887468994\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (20.9777946472,25.5025252134), test loss: 29.7597336054\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.00129675865,2.45308699414), test loss: 3.59976524413\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (24.3320140839,25.4916063197), test loss: 30.4922895908\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.804981827736,2.45236088512), test loss: 3.48729770184\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (29.9733009338,25.4777834437), test loss: 29.1975053787\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.13768625259,2.45153962934), test loss: 3.94050427675\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (23.8815307617,25.464043276), test loss: 37.2701053619\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.77096009254,2.45067533561), test loss: 3.66472751498\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (27.5941982269,25.449081669), test loss: 34.8426084995\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.04114449024,2.44991080636), test loss: 3.24062881023\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (5.89453125,25.4319221013), test loss: 29.4486592293\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.62831056118,2.4491955827), test loss: 3.51006779671\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (28.787437439,25.4212473622), test loss: 32.0084288597\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.61515903473,2.44811923543), test loss: 2.75875896811\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (12.5825119019,25.4096044375), test loss: 27.8298959255\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.768366038799,2.44741364278), test loss: 3.61665893793\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (58.6383285522,25.3968817562), test loss: 33.2745804787\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.98151373863,2.44658949799), test loss: 2.98736878335\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (24.917263031,25.3826796024), test loss: 31.4379654408\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.90350437164,2.44574840737), test loss: 3.59461705089\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (14.670340538,25.3669989466), test loss: 37.5582900047\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.49791884422,2.44499206144), test loss: 2.94179869592\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (7.5589632988,25.3514664847), test loss: 26.6985808611\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.81528902054,2.44425119705), test loss: 3.62841982841\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (29.1934280396,25.3404446349), test loss: 33.1694384575\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.06662225723,2.44322100763), test loss: 2.75915970653\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.9528503418,25.3288247709), test loss: 29.3648211956\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.39358687401,2.44256210008), test loss: 3.36984473467\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (63.3728942871,25.316022916), test loss: 31.9373043537\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.08966040611,2.4416937508), test loss: 2.79008663595\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (22.3222351074,25.3021131477), test loss: 32.9633457661\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.30637168884,2.44089177777), test loss: 3.57577373087\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (13.7231063843,25.2869452142), test loss: 30.2136352062\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.42478513718,2.44014117792), test loss: 3.45696125925\n",
      "run time for single CV loop: 1309.10469198\n",
      "\n",
      "MC # 4, Hype # hyp1, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold1/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (33.5674057007,inf), test loss: 33.1125967503\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.44744300842,inf), test loss: 3.04817076623\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (13.846736908,25.6062672844), test loss: 27.7041079998\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.39675569534,2.45969417408), test loss: 3.51405727267\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (31.3271751404,25.5795736697), test loss: 30.0232641697\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.42895495892,2.45564625914), test loss: 2.9130360648\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (31.1868171692,25.6793513165), test loss: 29.2228638411\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.01558923721,2.44529065983), test loss: 3.81970221102\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (15.0402679443,25.6647271413), test loss: 32.5554107189\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.776597559452,2.44530471218), test loss: 3.82699664831\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (43.4927787781,25.6390724078), test loss: 30.6112992764\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.23300075531,2.44124907424), test loss: 3.80958885252\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (21.1999015808,25.6054025296), test loss: 32.6699515343\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.62611722946,2.44143537592), test loss: 4.12285956442\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (3.02715802193,25.6005469522), test loss: 32.1715530396\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.863151729107,2.4419979672), test loss: 3.04103957415\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (29.5405502319,25.5574809722), test loss: 32.2566053391\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.798688411713,2.44136091801), test loss: 3.88638272285\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (13.3193855286,25.5424255111), test loss: 31.4693430424\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.47268223763,2.43927920053), test loss: 2.66353257596\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (4.24595928192,25.544685243), test loss: 30.307693696\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.27679347992,2.43848597176), test loss: 3.71059510112\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (12.901720047,25.5368756022), test loss: 33.9658844709\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (6.62878990173,2.43665034373), test loss: 3.04685934857\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (37.6545410156,25.5094506894), test loss: 29.0453663349\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.72633552551,2.4347134384), test loss: 3.64709483385\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (12.2766933441,25.495826188), test loss: 36.6320916176\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.926774740219,2.43333563647), test loss: 3.0608284086\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (18.894985199,25.48123892), test loss: 28.9132555008\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.24451971054,2.43329479542), test loss: 3.51279799938\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (16.9506874084,25.4565124852), test loss: 29.7993072987\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.37218785286,2.43292673485), test loss: 3.20679865479\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (23.3828449249,25.4478836573), test loss: 31.4956835747\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.49012374878,2.43114594198), test loss: 3.79239328802\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (9.31791496277,25.4407892935), test loss: 38.2340132236\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.97107309103,2.43010132355), test loss: 3.73230377436\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (21.4505386353,25.4331225379), test loss: 32.7634210587\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.51038169861,2.42869144662), test loss: 3.34764180928\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (14.6574821472,25.4093993771), test loss: 30.8559492111\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.7768368721,2.42721237051), test loss: 3.99370195866\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (26.9487552643,25.3965570429), test loss: 31.876432991\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.293898105621,2.4266043965), test loss: 2.93767189085\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (7.9410777092,25.3754910156), test loss: 28.8357887506\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.89754462242,2.42613020412), test loss: 3.84252966046\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (22.4553127289,25.3584699511), test loss: 31.3089492798\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.18860530853,2.4250378811), test loss: 2.74308143258\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (18.4472045898,25.3547217208), test loss: 30.9835589409\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.850265145302,2.42396752317), test loss: 3.83283038735\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (22.7151756287,25.3422089451), test loss: 32.5364504337\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.668514192104,2.42269914101), test loss: 3.03384019732\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (13.6601295471,25.3245645126), test loss: 31.6935760021\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.9002045393,2.42116752971), test loss: 3.64478658438\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (20.4667778015,25.3076156002), test loss: 30.5930645466\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.93124139309,2.42077185172), test loss: 2.93549909517\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (36.1371688843,25.3009242168), test loss: 29.819328928\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.04684257507,2.41991121083), test loss: 3.63155579269\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (9.74588108063,25.2733438011), test loss: 30.9824618816\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.945444345474,2.41958747481), test loss: 3.67047539949\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (14.2592334747,25.2619219993), test loss: 28.2541921377\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.97952055931,2.41790296791), test loss: 3.81211281419\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (57.1599845886,25.2559658627), test loss: 33.2927289724\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.60615825653,2.41689548376), test loss: 3.85666887313\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (20.7737083435,25.2433425822), test loss: 31.0083297253\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.3776267767,2.41591390335), test loss: 3.05733564124\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (27.1883773804,25.2218938191), test loss: 31.8050418615\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.06186127663,2.41455870577), test loss: 3.77922371626\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (17.7553367615,25.2113931473), test loss: 36.3829257488\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.95840454102,2.41364392589), test loss: 2.886140576\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (9.46849250793,25.1930369441), test loss: 28.8620553017\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.37386870384,2.41286418213), test loss: 3.84749783278\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (34.1129837036,25.1800111342), test loss: 31.8676381111\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.42972135544,2.41204430991), test loss: 2.76721457839\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (15.1677837372,25.167314242), test loss: 30.0042562962\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (7.17892456055,2.4111084176), test loss: 3.78084821105\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (15.3980121613,25.1557237444), test loss: 35.3416345596\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.40857362747,2.41027417646), test loss: 2.96555528641\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (4.73480415344,25.1444981182), test loss: 32.2601671219\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.58534669876,2.40859453308), test loss: 3.76556736231\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (28.9266014099,25.1273350521), test loss: 30.7115324974\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.15087139606,2.40796835251), test loss: 2.9866648525\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (9.74133586884,25.1130215153), test loss: 27.8666866541\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.50691699982,2.40699300611), test loss: 3.68919966221\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (35.8669891357,25.0972888759), test loss: 31.042934227\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.33864021301,2.40627766272), test loss: 3.53554171622\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (7.98782444,25.0804475017), test loss: 27.6325031757\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.7760668993,2.40543531936), test loss: 3.82535045445\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (29.8964366913,25.0747888675), test loss: 32.9772836685\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.05869102478,2.40411711671), test loss: 3.81737886667\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (32.3555984497,25.0620622413), test loss: 30.2505755186\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.76879310608,2.40341483047), test loss: 3.0645860076\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (47.3451156616,25.0456676725), test loss: 35.9183042049\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.625508546829,2.40196456836), test loss: 3.80708672553\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (18.3419208527,25.0323288582), test loss: 31.2580101013\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.16977095604,2.40094670873), test loss: 2.79625421613\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (72.4294052124,25.0194277214), test loss: 38.3771566868\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.60457968712,2.40052013285), test loss: 4.10792388916\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (40.4595108032,25.0010408572), test loss: 33.0758995056\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (5.38889551163,2.39965618358), test loss: 2.92615676969\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (34.1565971375,24.9898597134), test loss: 29.2290224552\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.08055853844,2.39829629596), test loss: 3.8199611187\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (12.9512529373,24.9769934268), test loss: 31.9234920502\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.93889021873,2.39755303028), test loss: 3.00550523102\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (19.3306694031,24.9666080785), test loss: 28.2223338366\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.72913360596,2.39648975365), test loss: 3.76700164378\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (19.8090114594,24.949261011), test loss: 31.8772934437\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.3662352562,2.39541535835), test loss: 2.85280969441\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (39.5401153564,24.9376999965), test loss: 27.9939445257\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.66266441345,2.39450517392), test loss: 3.64473347962\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (21.1361618042,24.9227261957), test loss: 30.9950036287\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.07575774193,2.39369801343), test loss: 3.55844761133\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (33.9686279297,24.9072873229), test loss: 27.9341967344\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.74870228767,2.39297479569), test loss: 3.81733902395\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (25.52891922,24.8973649736), test loss: 31.547127986\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.854706048965,2.39168427059), test loss: 4.02285116315\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (11.7225399017,24.8854617301), test loss: 30.1111685276\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.655081629753,2.3908329861), test loss: 3.10001634955\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (39.5459899902,24.8721481771), test loss: 31.1707667351\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.74552428722,2.38971648406), test loss: 3.89671671987\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (18.9661312103,24.8573839945), test loss: 33.869340229\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.23864126205,2.38886313339), test loss: 2.90593956709\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (3.51193976402,24.8451420204), test loss: 29.1577498198\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.437269389629,2.38813941618), test loss: 3.92390573919\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (27.454076767,24.8284045054), test loss: 35.7628623962\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.898800671101,2.3873238443), test loss: 2.99197783768\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.0750751495,24.8164699599), test loss: 29.1437260866\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.56827807426,2.38625694185), test loss: 3.78555164933\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (4.04396772385,24.8055293482), test loss: 33.9204953194\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.79593396187,2.38545204506), test loss: 2.97255911231\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (13.3639259338,24.7930168771), test loss: 28.5324518204\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (5.46493625641,2.38436472528), test loss: 3.59771112204\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (31.44115448,24.7782965606), test loss: 30.4207375526\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (4.3357257843,2.38327400826), test loss: 2.98329790533\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.7009391785,24.7659121735), test loss: 35.0977082014\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.31595563889,2.38238286482), test loss: 3.66140754372\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (19.6859016418,24.75288808), test loss: 32.5607559681\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.65364265442,2.38162044302), test loss: 3.72761008739\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (14.5253047943,24.73689069), test loss: 29.6774102449\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.08753728867,2.38081343007), test loss: 3.76675826013\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (15.9445524216,24.7248844901), test loss: 31.9158499718\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.28577065468,2.37975836899), test loss: 4.03606554866\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.719581604,24.7150042197), test loss: 34.3870315075\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.691909253597,2.3788526682), test loss: 2.91580122411\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (18.7362327576,24.7034453575), test loss: 29.9955551147\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.43182229996,2.37786524822), test loss: 3.95036091208\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (14.1152849197,24.6878156243), test loss: 30.5029881477\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.60828602314,2.37681563333), test loss: 2.80353212059\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (21.9766807556,24.6759156271), test loss: 27.5524410725\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.214000701904,2.37597140005), test loss: 3.84063289762\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.53730583191,24.6615455957), test loss: 32.8739604473\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.92825889587,2.37526302472), test loss: 2.97818358541\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (22.6156692505,24.6482723709), test loss: 28.8129519939\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.16748976707,2.37428721442), test loss: 3.76843435168\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.18009758,24.6375673045), test loss: 33.0415834904\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.924591422081,2.37335584114), test loss: 3.09978624284\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (22.3470077515,24.6254226934), test loss: 27.412030983\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.655371189117,2.37239668216), test loss: 3.54927626848\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (11.2124156952,24.6128320109), test loss: 30.8203249454\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.89775335789,2.37129030104), test loss: 2.92965946198\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (20.165719986,24.5988379811), test loss: 30.1907164574\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.08759951591,2.37060928076), test loss: 3.73969482183\n",
      "\n",
      "MC # 4, Hype # hyp1, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold2/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (41.3674926758,inf), test loss: 34.2654577255\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (4.41644906998,inf), test loss: 2.75079855919\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (43.0483474731,25.9649949226), test loss: 32.951334095\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.72455203533,2.4830707224), test loss: 3.52836828232\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (30.153673172,25.898675107), test loss: 33.4581302643\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.5422334671,2.48746304628), test loss: 2.85495684743\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (12.4664716721,25.9892438407), test loss: 33.2963631868\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.22106635571,2.47425523573), test loss: 3.81901416183\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (15.8977336884,25.9591962686), test loss: 36.3523392677\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.16780972481,2.4728919463), test loss: 3.87502933145\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (43.4201774597,25.9651311435), test loss: 35.9259860516\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.44622468948,2.47012027681), test loss: 3.78462637067\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (17.1085281372,25.9068979287), test loss: 35.8238345623\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.26700162888,2.46953214094), test loss: 4.17476193607\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (2.94623160362,25.9473406972), test loss: 35.1974224567\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.998491764069,2.47029779542), test loss: 2.73964552283\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (44.173740387,25.8778155124), test loss: 33.6596819878\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.61611974239,2.46934558664), test loss: 4.04374096394\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (15.0290641785,25.8799851027), test loss: 35.6110747337\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.95807862282,2.47016770278), test loss: 2.5226122573\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (58.6666145325,25.8578578144), test loss: 33.1473793983\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.82160604,2.46640872031), test loss: 4.0097512424\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (22.3561668396,25.8643505485), test loss: 34.2679401875\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.25953006744,2.4654429375), test loss: 2.90339100957\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (41.2945785522,25.837811321), test loss: 37.3211262941\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.07637786865,2.46348706353), test loss: 3.74944426417\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (52.079864502,25.8232714705), test loss: 34.073474884\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (4.40448284149,2.46250851495), test loss: 2.86417252868\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (15.8393964767,25.8102306714), test loss: 34.9171263456\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.56945466995,2.46234259946), test loss: 3.56618802249\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (22.6473312378,25.7721835166), test loss: 33.8341492176\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.41721940041,2.4617067451), test loss: 3.02796962559\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (49.9315452576,25.7854059408), test loss: 34.9459573746\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.46884691715,2.46090090607), test loss: 3.79628995061\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (27.2200469971,25.7520605423), test loss: 36.8116744518\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.36537218094,2.45907230851), test loss: 3.85640587956\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (28.0757598877,25.7572360598), test loss: 38.2979707956\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (5.29336357117,2.45794345008), test loss: 3.36144426763\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (18.5069007874,25.7253138196), test loss: 36.4449579716\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.39069342613,2.45638656337), test loss: 3.86307811737\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (23.394865036,25.7184776812), test loss: 38.0001923084\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.600577831268,2.45567972581), test loss: 2.90742283762\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (20.6827011108,25.6960031204), test loss: 34.4448691368\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.00763356686,2.45505627378), test loss: 4.07520390153\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (30.0085372925,25.678369), test loss: 31.9487197876\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.68870997429,2.45504385489), test loss: 2.51974083632\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (32.1050338745,25.6729830741), test loss: 33.0050750494\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.84458327293,2.45302905698), test loss: 3.88226530552\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (13.6604471207,25.6576600069), test loss: 35.1085664272\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.572741031647,2.45202557544), test loss: 2.75126855075\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (14.5878505707,25.6440128252), test loss: 35.7380008936\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.31546926498,2.45069565007), test loss: 3.82748624086\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (13.9961662292,25.622696425), test loss: 36.5394672394\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.98706412315,2.44996727458), test loss: 3.19525670409\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (12.5939483643,25.6222357571), test loss: 31.9368664742\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.30318784714,2.44956056109), test loss: 3.63215727806\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (10.1861343384,25.5899877534), test loss: 41.3454385757\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.01966333389,2.44865256542), test loss: 3.68763547689\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (19.7509346008,25.5827902366), test loss: 34.3943505526\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (5.27986621857,2.44813550443), test loss: 3.94557074606\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (6.78209209442,25.5625462033), test loss: 37.1871925831\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.03462934494,2.44654735021), test loss: 3.76695940197\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (36.2314682007,25.5580873299), test loss: 33.4586627007\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (6.80097293854,2.4456235662), test loss: 3.19901009649\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (34.7485656738,25.5377523784), test loss: 37.4968789101\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.86959266663,2.44421063317), test loss: 3.9350251317\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (12.5290899277,25.5237586916), test loss: 44.8415574789\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.1930590868,2.44330699252), test loss: 2.64249485731\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (32.7898101807,25.5116660349), test loss: 38.5575371265\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.09673690796,2.44271792652), test loss: 4.08718888164\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (14.9772872925,25.4879743766), test loss: 31.1703595638\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.15016222,2.44210240331), test loss: 2.57733289003\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (58.3632202148,25.4836504568), test loss: 39.6004430056\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.74840211868,2.44119730148), test loss: 4.00426633358\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (17.0998935699,25.4630561142), test loss: 38.7265431404\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.56557500362,2.43983694528), test loss: 2.73743585795\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (45.510433197,25.4578714368), test loss: 35.6107929707\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.95787000656,2.43873943057), test loss: 3.62089511752\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (17.2444343567,25.4360246766), test loss: 34.3487551689\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.70424056053,2.43763155761), test loss: 3.05880221725\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (12.8799037933,25.4258289192), test loss: 32.3224510193\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.490087747574,2.43697578772), test loss: 3.77965628207\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (9.26539325714,25.4072062575), test loss: 34.1876158714\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.98923909664,2.43619110307), test loss: 3.66779438257\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (24.5345153809,25.3925248401), test loss: 32.5512497663\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.26633703709,2.43566740125), test loss: 3.76996941268\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (18.1312160492,25.3820843056), test loss: 34.0938197613\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.18484306335,2.43422195188), test loss: 3.8219011873\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (39.365196228,25.3694019015), test loss: 33.4270499706\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (5.57405853271,2.43337456544), test loss: 2.93600596339\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (15.7047252655,25.3556223959), test loss: 37.7564648151\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.05578756332,2.43213137168), test loss: 3.9568013981\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (49.9620780945,25.3384754202), test loss: 35.9012564659\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.09951424599,2.43134161423), test loss: 2.8457153514\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (77.2215499878,25.3321799656), test loss: 34.8649429321\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (6.62628173828,2.43074138742), test loss: 4.11547976732\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (42.4270782471,25.3080053981), test loss: 38.0907195091\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.939231216908,2.4298106612), test loss: 2.84231165797\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (8.58454036713,25.2983063027), test loss: 35.0266639233\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.39472389221,2.42899741645), test loss: 3.90977829695\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (6.27085781097,25.2813050384), test loss: 36.1108541012\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.16938829422,2.42784070917), test loss: 2.79098179042\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (22.8506736755,25.2726185991), test loss: 33.5504508972\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.45811986923,2.42675278071), test loss: 3.54426084757\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (14.762512207,25.2549497256), test loss: 36.1976415157\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.02426719666,2.42561547351), test loss: 2.94189064503\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (18.1408023834,25.2424143032), test loss: 38.1695162058\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.49497008324,2.42488776163), test loss: 3.59096912742\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (35.962600708,25.2292798725), test loss: 36.9616723537\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.14230775833,2.42403421617), test loss: 3.73134333193\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (7.57729101181,25.2105944776), test loss: 34.0006395578\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.67754304409,2.42344690482), test loss: 3.78499367237\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (7.92844772339,25.2014700462), test loss: 35.5088161945\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.66460871696,2.42249936904), test loss: 4.07957271785\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (16.1386299133,25.185385006), test loss: 41.9637801647\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.853609144688,2.42139358964), test loss: 2.85096299052\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (55.0839538574,25.1766204029), test loss: 37.2936453342\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.20670628548,2.4203308965), test loss: 3.93489215374\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (17.4013252258,25.1588307396), test loss: 36.090241003\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.908733606339,2.41931684308), test loss: 2.9102409333\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (23.8901405334,25.1481245423), test loss: 33.5941105366\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.58632850647,2.41866865991), test loss: 4.13402550519\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (43.6134033203,25.1311032765), test loss: 33.6814622879\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.70752739906,2.41781298818), test loss: 2.8607912153\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (16.4074077606,25.1172946078), test loss: 34.53022542\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.2082490921,2.41722893732), test loss: 3.8471255362\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (19.1963863373,25.1059365237), test loss: 34.6778454781\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.35907411575,2.41594847034), test loss: 2.9135384053\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (21.3125934601,25.0934381265), test loss: 33.5221449375\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.6636235714,2.41504187322), test loss: 3.4267214179\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (13.2126617432,25.0802075335), test loss: 33.4650632858\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.51086306572,2.4140208053), test loss: 2.91462325156\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (16.3978404999,25.0642533729), test loss: 36.8505827427\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.35922408104,2.41324980372), test loss: 3.78225802183\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (12.2950534821,25.0550902214), test loss: 47.8243954659\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.879961252213,2.41245614147), test loss: 4.32178043649\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (15.5345163345,25.0358462889), test loss: 34.3684971333\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.30265498161,2.41175215581), test loss: 3.75101031959\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (7.5186419487,25.026371696), test loss: 34.4655924797\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.83089351654,2.41089491817), test loss: 3.90807034075\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (50.3219146729,25.0109470769), test loss: 37.4055848122\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.08840465546,2.40987136785), test loss: 2.76595570445\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.6154537201,25.0011975861), test loss: 35.6250889063\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.851614356041,2.40887756995), test loss: 4.03226665556\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (42.0878753662,24.985239418), test loss: 38.2415292263\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.16010022163,2.4078856437), test loss: 2.76908212304\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.2897014618,24.9731731964), test loss: 34.7150399208\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.43518328667,2.40715643212), test loss: 3.95755832195\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (26.0923080444,24.9598716188), test loss: 36.9595285416\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.778510034084,2.40627196746), test loss: 2.85576747358\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (34.1762733459,24.9445255204), test loss: 33.7697703838\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.19648885727,2.40570452733), test loss: 3.75718335807\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.8818378448,24.9344088433), test loss: 37.065116787\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.30881929398,2.40478355149), test loss: 2.8781285584\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (8.11804771423,24.9197420961), test loss: 37.5153321981\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.23496651649,2.40382652323), test loss: 3.47180340886\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (8.55923461914,24.9094785714), test loss: 35.7054065704\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.79618239403,2.40278151872), test loss: 3.07994916737\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (54.8703079224,24.8944368461), test loss: 33.9258644581\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.38641119003,2.40190086888), test loss: 4.23674683571\n",
      "\n",
      "MC # 4, Hype # hyp1, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold3/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (50.7289733887,inf), test loss: 30.2376842976\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.91543698311,inf), test loss: 2.69962599427\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (23.596282959,26.09888904), test loss: 31.054829812\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.12951016426,2.53129435396), test loss: 3.81683830619\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (29.0585308075,26.0405891979), test loss: 27.3004253387\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.58471560478,2.52481334989), test loss: 2.56948292255\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (14.6793937683,26.0301747386), test loss: 31.2037574768\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.798998475075,2.51431181073), test loss: 3.96062352657\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (16.2693920135,26.1007714329), test loss: 29.1949874401\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.72073662281,2.5156469948), test loss: 3.39764602333\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (50.1147232056,26.0477371214), test loss: 36.9772359371\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.34806632996,2.51185687268), test loss: 4.1835864082\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (18.4485378265,26.024185214), test loss: 28.6308845043\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.13286542892,2.51286864698), test loss: 3.77706383467\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (7.80434322357,25.9951954196), test loss: 35.4764184475\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.20419025421,2.51135836375), test loss: 3.34696062803\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (37.0688362122,26.003654175), test loss: 30.0332046032\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.01632475853,2.51239128358), test loss: 3.61941179037\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (11.6593694687,25.9571770048), test loss: 35.3355363369\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.30395460129,2.51036408334), test loss: 2.86044070125\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (30.3040008545,25.9659962564), test loss: 32.5323476315\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.29626631737,2.5085971406), test loss: 3.70880488157\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (33.3729705811,25.9556224142), test loss: 34.2465121984\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.32617902756,2.50725592348), test loss: 2.69068846852\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (23.4948654175,25.9319576126), test loss: 32.0831612349\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.978351712227,2.505451053), test loss: 3.62113699913\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (15.9534034729,25.9145097019), test loss: 31.161254549\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.852279126644,2.50408256583), test loss: 2.70672929883\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (53.9412193298,25.9010595914), test loss: 35.4160684347\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.51946473122,2.50387544824), test loss: 3.85950730443\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (22.2893753052,25.8795990843), test loss: 27.5812334299\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.509195327759,2.50290631714), test loss: 3.29402164817\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (45.2195549011,25.8683140069), test loss: 32.1640252113\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.27147841454,2.5013964717), test loss: 4.16722614169\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (15.6573886871,25.863126571), test loss: 29.1318031788\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.802737295628,2.50069451415), test loss: 3.55134899318\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (16.0428390503,25.8465014384), test loss: 33.8913966656\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.22868347168,2.49934230848), test loss: 3.35106013417\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (14.6222629547,25.827808611), test loss: 32.5794490337\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.6291384697,2.49801551297), test loss: 3.39880126417\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (31.6264648438,25.8111659944), test loss: 37.4358121872\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.48819351196,2.49710895866), test loss: 3.06892385185\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (18.1031322479,25.7990201302), test loss: 28.345125246\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.19781947136,2.49681187892), test loss: 3.6192553401\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (15.2403812408,25.7733254328), test loss: 33.1501109123\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.633497715,2.49559965795), test loss: 2.85333406925\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (32.5506248474,25.7686689197), test loss: 29.0988893986\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (5.90548038483,2.49456901873), test loss: 3.5761166662\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (32.2756500244,25.7581823961), test loss: 30.3030961037\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.3467707634,2.49335180727), test loss: 2.80909646451\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (40.7092590332,25.7432280633), test loss: 32.0641378164\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.76512467861,2.49200201852), test loss: 3.72336426377\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (24.2380447388,25.7221764711), test loss: 28.3737896442\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.79411125183,2.49130389222), test loss: 2.70020508319\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (12.7124652863,25.7089524137), test loss: 30.4847054958\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.72012555599,2.49032349749), test loss: 3.85397785902\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (11.7236785889,25.6979512131), test loss: 28.0984602928\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.99934649467,2.48984197395), test loss: 3.23912716806\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (11.9245262146,25.6750388538), test loss: 32.9924429417\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.49492645264,2.48865157194), test loss: 4.19875797331\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (23.988319397,25.6719338877), test loss: 28.2277117729\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.66830861568,2.48754425475), test loss: 3.47168070227\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (25.8941402435,25.6571616866), test loss: 35.9645737648\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.51569080353,2.48634541905), test loss: 3.28812937438\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (63.808013916,25.645686015), test loss: 32.7035517693\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.99505329132,2.48534297688), test loss: 3.50756623447\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (21.954957962,25.6237787364), test loss: 38.6984299183\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.27342498302,2.48405659444), test loss: 3.00355880111\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (41.9001121521,25.6171825289), test loss: 34.465865612\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.10979700089,2.48381017618), test loss: 3.86674670875\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (100.591506958,25.5977583066), test loss: 43.6798069954\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (6.6918926239,2.48285226567), test loss: 2.89296616316\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (21.6751117706,25.5861554959), test loss: 28.9997766018\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.60791778564,2.48171294429), test loss: 3.56948959827\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (21.698764801,25.5712848737), test loss: 29.8078580618\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.67672395706,2.48065212255), test loss: 2.84984680116\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.9995536804,25.5588394656), test loss: 31.006077981\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.41365909576,2.47960165877), test loss: 3.83320859671\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (9.24633026123,25.5403915863), test loss: 27.7930157661\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.4753947258,2.47859308693), test loss: 2.59543228447\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (6.46545314789,25.5268274223), test loss: 32.4278060913\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.50610351562,2.47753710787), test loss: 3.89941363633\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (20.6455936432,25.515098732), test loss: 27.9524723053\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.37965154648,2.47695568652), test loss: 3.2835940972\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (48.151763916,25.4962612315), test loss: 33.6723704815\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.94899833202,2.47581381991), test loss: 4.18206355572\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (12.4154949188,25.4893836767), test loss: 28.7505978584\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (6.11970472336,2.47516769932), test loss: 3.66897610128\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (26.0663414001,25.4756394046), test loss: 35.8583329678\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (6.39541912079,2.47397845502), test loss: 3.30135249794\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (6.62961959839,25.4643930167), test loss: 33.1689916134\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.8835067749,2.47286240237), test loss: 3.70860800147\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (18.604637146,25.4435768676), test loss: 35.4565430641\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.42689919472,2.4718433226), test loss: 2.84342958927\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (20.9992370605,25.434518832), test loss: 28.8082454205\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.23757719994,2.47110113484), test loss: 3.60706544518\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (29.1392593384,25.4186635573), test loss: 31.1621656895\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.79252767563,2.47035639885), test loss: 2.82840644419\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (11.6312046051,25.4031314085), test loss: 28.756836319\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.02377700806,2.46940652372), test loss: 3.62931270599\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (23.1823196411,25.3924232386), test loss: 34.3866569042\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.14841985703,2.46825380108), test loss: 2.75709240139\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (28.4715003967,25.3804875166), test loss: 31.5027812481\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.07129025459,2.46720499604), test loss: 3.70278918147\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (17.2850456238,25.3675338907), test loss: 27.4526050091\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.487649917603,2.46625137183), test loss: 2.56939966381\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.5494937897,25.3491009759), test loss: 32.5102501869\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (6.92481231689,2.46528201942), test loss: 4.13130896688\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.525894165,25.3415221242), test loss: 32.7512223721\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.404092311859,2.46464440607), test loss: 3.56048237681\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (50.6663322449,25.3230578911), test loss: 45.329518652\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (5.58261489868,2.46378118134), test loss: 4.28632225096\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (4.60655450821,25.3138059732), test loss: 28.4210854053\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.28103923798,2.46279638627), test loss: 3.65016767383\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.2117977142,25.2978787765), test loss: 33.76893785\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.35314679146,2.4616850981), test loss: 3.34801704288\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (20.5816383362,25.2890394773), test loss: 32.1830018044\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.26065731049,2.46078480017), test loss: 3.53669965267\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (25.797454834,25.269963572), test loss: 35.930234623\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.79255247116,2.45966900208), test loss: 2.91648635268\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (26.0188407898,25.2601061456), test loss: 29.4900019169\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.41695904732,2.4588681876), test loss: 3.55551476777\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (34.005241394,25.2453297964), test loss: 32.7809498787\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.690150022507,2.45800131403), test loss: 2.90888050199\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (51.5759010315,25.2314176522), test loss: 29.650969696\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.263235569,2.45707706982), test loss: 3.61991585493\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (10.7749347687,25.2192873024), test loss: 32.8919219017\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.41725933552,2.45626362298), test loss: 2.80091888011\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.481842041,25.2074055523), test loss: 34.9755736828\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.08171463013,2.45524352308), test loss: 3.70585581064\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (12.9367828369,25.19558387), test loss: 29.0572788715\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.0712556839,2.45424124721), test loss: 2.67534313202\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (35.1539840698,25.1774131916), test loss: 30.629543066\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.45671713352,2.45314974921), test loss: 4.11354023814\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (47.5043678284,25.1692919265), test loss: 29.2139565468\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.31384587288,2.45249356591), test loss: 3.55160323977\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (16.0139751434,25.1525985015), test loss: 36.2673818111\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.16793584824,2.45160402829), test loss: 3.82841459513\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (22.0875892639,25.1406688975), test loss: 29.8945147991\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.87621331215,2.45085473069), test loss: 3.52918294668\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (40.3801956177,25.128040038), test loss: 37.2363343\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.21482014656,2.44968832848), test loss: 3.12494260371\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (26.893535614,25.1182857474), test loss: 30.3463756084\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.4432643652,2.44884297896), test loss: 3.57467144132\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (8.23581695557,25.1027617359), test loss: 36.6436648607\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.9403860569,2.44785843617), test loss: 2.89544317722\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (11.8015241623,25.0888076607), test loss: 27.9253359795\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.48927736282,2.4469837971), test loss: 3.68554381132\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (34.0135421753,25.0781697309), test loss: 36.7963506222\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.60492897034,2.44624478848), test loss: 2.89675197601\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (28.0797080994,25.0634717163), test loss: 30.0006235838\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.42875099182,2.44535864628), test loss: 3.6475130558\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (13.6937770844,25.0516319603), test loss: 30.5541408062\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.56074929237,2.44438993411), test loss: 2.75837111473\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.3517837524,25.0375914455), test loss: 29.5979159832\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (4.21880531311,2.44339136985), test loss: 4.10002765954\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (28.9777565002,25.0290953116), test loss: 30.8235288858\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.68451595306,2.44250523024), test loss: 3.26356613338\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (15.9300680161,25.0106580237), test loss: 31.8151786327\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.64867591858,2.44140507286), test loss: 4.22271546125\n",
      "\n",
      "MC # 4, Hype # hyp1, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold4/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (41.9484405518,inf), test loss: 32.9466690063\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.87161684036,inf), test loss: 3.3303704679\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (33.9571914673,26.3540870357), test loss: 29.3580330849\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.671194553375,2.51434145117), test loss: 3.05662826002\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (37.7331924438,26.2276128418), test loss: 30.8413633347\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.56050300598,2.5148379918), test loss: 3.25934014618\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (5.43925189972,26.2985970068), test loss: 29.522213006\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.68933606148,2.50887341603), test loss: 3.34279996157\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (6.98148775101,26.2884495312), test loss: 32.1344213486\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.99698829651,2.50440787783), test loss: 3.21270740628\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (24.947101593,26.2776549436), test loss: 31.9895239353\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.787867605686,2.50112644455), test loss: 2.8590906918\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (12.7773618698,26.232319352), test loss: 33.9923500061\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.17394709587,2.49749147685), test loss: 3.19481483102\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (21.5021514893,26.2352280606), test loss: 36.0881131887\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.41814947128,2.49850805289), test loss: 2.99173926115\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (20.3268260956,26.2318315619), test loss: 30.4595394611\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.61673861742,2.4969706515), test loss: 3.3588621676\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (19.9136123657,26.2048154048), test loss: 31.6747768402\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.60196948051,2.49687427065), test loss: 2.92028800845\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (32.1018295288,26.1803638234), test loss: 33.1458212852\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.96148061752,2.49624131674), test loss: 3.13523885012\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (73.7811813354,26.1744766105), test loss: 44.0721275806\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.46937966347,2.49536341832), test loss: 2.80624427199\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (21.3535842896,26.1517592512), test loss: 29.7660978317\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.79806530476,2.49295620223), test loss: 3.25263439417\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (35.2362556458,26.1493158992), test loss: 33.9634840727\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.47560167313,2.49150570984), test loss: 3.17427093387\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (27.8020038605,26.1261430945), test loss: 31.8639724731\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.38695907593,2.49033684952), test loss: 3.30302256644\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (15.1706171036,26.110738135), test loss: 32.5107037306\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (6.45067405701,2.4895294896), test loss: 3.27507613003\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (39.6006584167,26.1047481001), test loss: 34.3880673647\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (5.07014989853,2.48892794476), test loss: 2.81204966009\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (50.2087860107,26.082686447), test loss: 31.6673458576\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.61101913452,2.48844463275), test loss: 3.26256971359\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (15.5159873962,26.0568965954), test loss: 29.7177447319\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.25049698353,2.48729081317), test loss: 2.85043234378\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (21.8657016754,26.0508760872), test loss: 31.2474212408\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.3925921917,2.48539634629), test loss: 3.27871365845\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (8.48500823975,26.0338667608), test loss: 36.3908352375\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.35932350159,2.48494701547), test loss: 2.92749152184\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (32.0678024292,26.020581457), test loss: 29.4177984238\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.49364638329,2.48313013125), test loss: 3.06074410677\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (8.2759475708,25.9982266055), test loss: 31.4826775551\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.60457897186,2.48195966407), test loss: 2.9246440202\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (24.2991294861,25.9932091465), test loss: 31.8179392338\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.15520060062,2.48128134982), test loss: 3.26736524105\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (15.3083171844,25.9732055762), test loss: 32.1655774593\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.17509508133,2.48062025976), test loss: 3.23936939538\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (29.2839679718,25.9545696864), test loss: 33.3973274946\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.81650829315,2.48022308218), test loss: 3.17788099647\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (14.7977581024,25.9396699568), test loss: 35.4651676416\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.06827926636,2.47921871638), test loss: 3.05284975767\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (21.5688266754,25.9250787974), test loss: 35.0396121502\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.05239295959,2.47753805302), test loss: 2.59392176867\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (17.1916160583,25.911359016), test loss: 30.7093473434\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.568866193295,2.47631556746), test loss: 3.16102509499\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (58.678817749,25.8975452461), test loss: 34.8352573872\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.30951237679,2.47506649518), test loss: 3.17429760695\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (23.6490631104,25.8796119345), test loss: 32.0828217983\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.1216622591,2.47428112356), test loss: 3.33639259338\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (33.3098754883,25.8660543659), test loss: 32.5749941826\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.44503772259,2.47305591389), test loss: 3.28645181805\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (21.4559516907,25.8508068372), test loss: 29.0342616558\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.50661277771,2.47262112231), test loss: 2.89268972576\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (14.431804657,25.829542952), test loss: 33.1131361485\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.1241645813,2.47189396648), test loss: 2.99618447721\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (18.2613353729,25.8215685879), test loss: 29.6395770073\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.42176580429,2.47087058652), test loss: 3.34200007617\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (13.8880329132,25.8060294241), test loss: 33.166325736\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.21320223808,2.4698461094), test loss: 3.27009701431\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (20.5798168182,25.7918262974), test loss: 32.6807611227\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.63447546959,2.46876525877), test loss: 3.11543848515\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (8.43671226501,25.7724673789), test loss: 33.771717\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.21676969528,2.46731709971), test loss: 3.23420001864\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (11.4231643677,25.7606827677), test loss: 36.5376318932\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.77354311943,2.46671194903), test loss: 3.02807561457\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (6.0865316391,25.747844021), test loss: 29.3432652473\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.15886068344,2.46578823937), test loss: 3.34752831906\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (12.4213695526,25.7294578636), test loss: 33.289807415\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.989001452923,2.46500112288), test loss: 2.98372452706\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (9.62890911102,25.7127712252), test loss: 31.1525999069\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.48018693924,2.4640606491), test loss: 3.07773823738\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (6.82277870178,25.7004164379), test loss: 33.6487177372\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.11475086212,2.46317745246), test loss: 2.8026720643\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (23.5254631042,25.6848557501), test loss: 29.3066103697\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.55160951614,2.46197199996), test loss: 3.13446049094\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (19.967212677,25.6723815924), test loss: 32.0756135464\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.33216142654,2.46082903772), test loss: 3.19912573695\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (32.3238296509,25.6574419061), test loss: 31.298133707\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.79468727112,2.45976283053), test loss: 3.23420921266\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (13.9040985107,25.6427429445), test loss: 31.7191536188\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.988868296146,2.45876119874), test loss: 3.28546800613\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (22.2742538452,25.6305537974), test loss: 38.3862233162\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.96994304657,2.45793437485), test loss: 2.93690363169\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (18.4088420868,25.6137509624), test loss: 36.6166593552\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.08045697212,2.45735645775), test loss: 3.25432630181\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (29.771440506,25.5965837649), test loss: 30.7734695911\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.614438652992,2.45638697426), test loss: 2.82405579388\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (26.2640686035,25.5847549652), test loss: 31.1282648563\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.66506719589,2.45500736636), test loss: 3.21463828683\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (19.3336410522,25.5723985402), test loss: 34.691435194\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.13905477524,2.45434920118), test loss: 2.95051426888\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (67.9648590088,25.5568571894), test loss: 30.6107953072\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.36562514305,2.45297563172), test loss: 3.04454228282\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (15.3845787048,25.5405530116), test loss: 31.3976006508\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.46722054482,2.45192888561), test loss: 2.82693238258\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (27.1324005127,25.5293493533), test loss: 29.2289614439\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.17196941376,2.4512209228), test loss: 3.1306553185\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (22.3425197601,25.5142900093), test loss: 34.7692399025\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.08953046799,2.45042339789), test loss: 3.30334723294\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (66.4175033569,25.4976762319), test loss: 45.3291614532\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (7.2253780365,2.44966011179), test loss: 3.33804484308\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (24.3314056396,25.4847479206), test loss: 34.9005177021\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (6.30829334259,2.44877938219), test loss: 3.09347002208\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (30.9688720703,25.4704344539), test loss: 33.1067643642\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.47648191452,2.44753794562), test loss: 2.71662970185\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (34.9586639404,25.4564645993), test loss: 32.5409366131\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (5.00986385345,2.44655087858), test loss: 3.32784725428\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.8317050934,25.4418237143), test loss: 32.3068090677\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.90338110924,2.44535014507), test loss: 3.0344702512\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (34.0139503479,25.4278471511), test loss: 31.7434007645\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.66863954067,2.44459503468), test loss: 3.30726758838\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.9864139557,25.4143110693), test loss: 32.3186528206\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.764098405838,2.44355279042), test loss: 3.2328904897\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (30.5751667023,25.3995492758), test loss: 27.7934448242\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.713529527187,2.44284553685), test loss: 2.88721157908\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (3.97950553894,25.3835695101), test loss: 33.2108674049\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.62086284161,2.44205799907), test loss: 3.00301633179\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (39.0512733459,25.3723495692), test loss: 29.8822471619\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.00827431679,2.4410720214), test loss: 3.36859063506\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (63.1042556763,25.3587885032), test loss: 32.7727818012\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.47012281418,2.44011044657), test loss: 3.52380596399\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (35.0939483643,25.3449655206), test loss: 32.4064509153\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.19426631927,2.43915295488), test loss: 3.34713802934\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (19.2190322876,25.32895983), test loss: 35.3101076126\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.632040739059,2.43801927134), test loss: 3.11238309145\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (40.5274429321,25.3157072855), test loss: 34.436631155\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.10370969772,2.43720640752), test loss: 2.95443003774\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.0526418686,25.3042591421), test loss: 29.3921129227\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.57323217392,2.43640601304), test loss: 3.21656830907\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.7466802597,25.2877586068), test loss: 33.5723681927\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.314973056316,2.43562859353), test loss: 3.02423168719\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (37.1275787354,25.2729999071), test loss: 30.23565135\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.72913837433,2.4347126427), test loss: 3.06760350168\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (4.85995578766,25.2604912839), test loss: 33.3751671791\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (5.96915721893,2.4338462225), test loss: 2.96789055467\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (18.8832969666,25.2458564548), test loss: 30.6760033607\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.40734136105,2.43279528195), test loss: 2.98772304356\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (17.9451026917,25.2335384026), test loss: 31.9952214241\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.38174819946,2.43181065071), test loss: 3.41977759004\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (35.4636535645,25.2198911749), test loss: 32.2312690496\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.88505065441,2.43082938709), test loss: 3.31715477109\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (20.2210884094,25.2060417619), test loss: 31.7819396019\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.77737641335,2.43001082479), test loss: 3.29648038149\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (26.9428806305,25.193255705), test loss: 33.7102890968\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.245725989342,2.42919922155), test loss: 2.88161660433\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (15.7165594101,25.1787107204), test loss: 36.3253731251\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.33578801155,2.42849153405), test loss: 3.3510230124\n",
      "\n",
      "MC # 4, Hype # hyp1, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold5/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (34.2157897949,inf), test loss: 33.1832849979\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.58834314346,inf), test loss: 3.22504435182\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (36.9112625122,26.1779163404), test loss: 30.5707163811\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.684930443764,2.45280756846), test loss: 3.54824857116\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (49.1981811523,26.181982877), test loss: 30.6988297701\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.06063985825,2.44886714639), test loss: 3.47361954451\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (62.3968658447,26.252654379), test loss: 37.1295479774\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.17852067947,2.44622706516), test loss: 3.73561519384\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (20.8614311218,26.2508006806), test loss: 32.5903917789\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.47767734528,2.44159880404), test loss: 3.53952645361\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (32.1788406372,26.2523522638), test loss: 29.8099219799\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.86566066742,2.4382659925), test loss: 3.03247747123\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (18.1037368774,26.2124790525), test loss: 34.9465639114\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.02883028984,2.43530280458), test loss: 3.70089194775\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (8.84842586517,26.2210979085), test loss: 31.5983799934\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.07086658478,2.43554181337), test loss: 2.52132148147\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (45.7098808289,26.192162999), test loss: 32.4014651537\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.50469827652,2.43479044019), test loss: 3.7698641777\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (5.77704906464,26.1524877584), test loss: 30.09779706\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.94443392754,2.4342139236), test loss: 2.85747792721\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (37.5214347839,26.160471104), test loss: 34.2446534157\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.24308633804,2.4337387673), test loss: 3.8838939786\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (9.11406040192,26.1443913757), test loss: 31.8117725372\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (0.966154932976,2.43152994294), test loss: 2.77144284248\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (18.5955734253,26.1320134348), test loss: 30.2101161003\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.85398626328,2.42998650951), test loss: 3.73837601542\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (16.1906280518,26.1050975367), test loss: 28.332173872\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (4.90030574799,2.428360699), test loss: 2.66375555247\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (26.5913257599,26.1009584865), test loss: 30.9294118166\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.69165420532,2.42777830661), test loss: 3.66245743036\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (26.2849216461,26.0840441091), test loss: 31.1310755253\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.96138429642,2.42708474565), test loss: 3.48611111343\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (17.3180961609,26.0509015971), test loss: 32.7414169312\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.19609880447,2.42603465578), test loss: 3.3188025564\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (27.1176223755,26.049835982), test loss: 32.9126317024\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.2932779789,2.42499264969), test loss: 3.67919423878\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (28.5284576416,26.034232698), test loss: 31.9765253782\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.65698027611,2.42349182431), test loss: 2.9700031817\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (17.75756073,26.0210466708), test loss: 31.3178663969\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.56469607353,2.42228194112), test loss: 3.89628067613\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (10.3658313751,25.998089363), test loss: 28.2326322079\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.78134942055,2.42080087332), test loss: 2.54095081985\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (18.5794143677,25.9903655237), test loss: 32.9619203806\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.922238886356,2.41999306013), test loss: 3.75606689751\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (38.6627388,25.976217426), test loss: 33.3705854416\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.6874819994,2.41952270188), test loss: 2.83975580931\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (28.3904037476,25.9492319194), test loss: 33.0966506958\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.60742807388,2.41839622386), test loss: 4.08951091468\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (29.6441326141,25.9429242166), test loss: 28.767254734\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.09591913223,2.41750534863), test loss: 2.73734962344\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (52.1322402954,25.9289426961), test loss: 31.5312049866\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.30950045586,2.416295958), test loss: 3.68457012773\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (22.0432319641,25.9140514563), test loss: 30.2116271257\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.659968614578,2.41476132676), test loss: 3.19735614657\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (21.4299583435,25.8922781613), test loss: 37.2302589417\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.746906399727,2.41353840138), test loss: 3.63436225653\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (21.3364219666,25.8829999362), test loss: 34.4316568613\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.19820022583,2.41276223092), test loss: 3.80320693254\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (57.8297958374,25.8710177852), test loss: 45.3480560303\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (4.0514793396,2.41224433594), test loss: 2.97469384968\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (77.6242141724,25.8490332933), test loss: 41.3699233055\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (8.31123638153,2.41143466443), test loss: 3.9411518991\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (8.23012924194,25.8340058637), test loss: 32.4036667228\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.13049697876,2.41023651043), test loss: 2.62477808893\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (18.4265823364,25.8219525293), test loss: 30.263807869\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.85858201981,2.40897470702), test loss: 3.77520724535\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (26.1211471558,25.8080539266), test loss: 31.9174954891\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.88735437393,2.4075498112), test loss: 2.81417165995\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (40.1237411499,25.7882059321), test loss: 30.8631023169\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.32085990906,2.40665580242), test loss: 3.68076441288\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (23.0703430176,25.777975512), test loss: 31.459594202\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.57685983181,2.40553138728), test loss: 2.85249241292\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (29.6392669678,25.7657839375), test loss: 33.487212801\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (0.251328140497,2.40493360375), test loss: 3.58729760051\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (16.0132369995,25.7455375812), test loss: 31.4845321655\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.19563603401,2.40439958046), test loss: 3.40997395515\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (11.6531887054,25.7300452766), test loss: 30.0374161959\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.9939841032,2.40303848536), test loss: 3.70458279252\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (4.73785495758,25.7165902673), test loss: 32.1238598824\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.40550899506,2.40198500052), test loss: 3.58321404159\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (15.0675182343,25.7037722779), test loss: 30.7746144772\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.64016914368,2.40068643939), test loss: 2.98260759115\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (27.6566162109,25.6850664157), test loss: 33.1536817074\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.30154871941,2.3996482436), test loss: 3.72782138586\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (28.564119339,25.6736824496), test loss: 32.5535533309\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.23522233963,2.39859902311), test loss: 2.47911254168\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (18.5380725861,25.6599135067), test loss: 40.4334044456\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.528872847557,2.39787456425), test loss: 3.91513326764\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (39.5571670532,25.6414125427), test loss: 36.3894142151\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.07457995415,2.39728403396), test loss: 2.90138018727\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (20.5454864502,25.6256635837), test loss: 31.2992318869\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.52030730247,2.3960196935), test loss: 3.79147335291\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (52.1750106812,25.6130300679), test loss: 30.6891722679\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.21993851662,2.3949861874), test loss: 2.79144623578\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (55.4754905701,25.6008283652), test loss: 29.8612232924\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.85187971592,2.39391688432), test loss: 3.81731343269\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (56.2457046509,25.5832500645), test loss: 28.4932655334\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.92800450325,2.39273049136), test loss: 2.92017453611\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (12.1380672455,25.569330486), test loss: 31.0263542652\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (6.85071802139,2.39195792341), test loss: 4.03916609883\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (40.5437698364,25.5583367915), test loss: 30.6943798065\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (4.59761619568,2.39106216823), test loss: 3.57261517048\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (19.0140609741,25.5388535952), test loss: 39.5817406654\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.07369875908,2.39031051228), test loss: 3.03716416657\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (9.45876312256,25.5237876313), test loss: 35.0001172543\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.88023519516,2.38927134722), test loss: 3.76877576709\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (19.8968200684,25.5120855763), test loss: 32.1803221226\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.30111646652,2.3881211421), test loss: 2.69155467749\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (26.3580551147,25.4992886345), test loss: 33.7232207537\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.78400540352,2.38718237655), test loss: 3.78336688876\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (6.42908763885,25.4818666498), test loss: 29.1918298364\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.0699057579,2.38598709707), test loss: 2.89353347123\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (16.5269126892,25.468275161), test loss: 34.228149271\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.48167848587,2.3851543477), test loss: 3.92138687372\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (14.2222251892,25.457645506), test loss: 30.3498747349\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.57006025314,2.38432779849), test loss: 3.07003123164\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (33.1580848694,25.4386871072), test loss: 31.2660090923\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.80714058876,2.38356975362), test loss: 3.65815345794\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (12.3164749146,25.4229556722), test loss: 28.5788455486\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.43586111069,2.38252538292), test loss: 2.8359271273\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (24.8978805542,25.4117954882), test loss: 30.8313185692\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.20842838287,2.38143125305), test loss: 3.53986642063\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (30.0392856598,25.3989359136), test loss: 31.3743654728\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.841150283813,2.38043494349), test loss: 3.48355214596\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.8293066025,25.3824100528), test loss: 32.8228539944\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.15946483612,2.37931391394), test loss: 3.63205576539\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (28.1037693024,25.3691014357), test loss: 32.4640692234\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.73808920383,2.37849200743), test loss: 4.14780208468\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (2.83581161499,25.3571845189), test loss: 32.8886293888\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.806517958641,2.37763876297), test loss: 3.0722073108\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (29.4845428467,25.341315486), test loss: 30.1468239069\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.27782177925,2.37687987282), test loss: 3.90623914599\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.1608810425,25.3247638867), test loss: 32.9109310627\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.05901145935,2.37592376348), test loss: 2.82255384028\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (22.6545009613,25.3142818245), test loss: 29.4889108658\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.07647836208,2.37482958475), test loss: 3.70962287784\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (25.2843704224,25.3016322449), test loss: 31.108270216\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.46377849579,2.37379912834), test loss: 2.87505693436\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (10.0715122223,25.2858206497), test loss: 31.27415452\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.33491611481,2.37281959411), test loss: 3.8547187984\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (17.2085189819,25.2721104029), test loss: 34.6544368267\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.43610048294,2.37198927622), test loss: 3.19730731398\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (7.88159561157,25.2597448162), test loss: 29.8931724906\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.07388067245,2.37107519922), test loss: 3.75490113422\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.0736122131,25.2446700436), test loss: 33.2271985054\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.783056557178,2.37035780253), test loss: 3.44635172486\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (23.9963512421,25.2284931633), test loss: 31.8358832359\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.11232066154,2.36938440764), test loss: 3.85134432018\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (17.0261955261,25.2176810051), test loss: 30.3052371025\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.45864343643,2.36834727772), test loss: 3.95253178477\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (35.86095047,25.2055665914), test loss: 31.4165548086\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (4.84154844284,2.36744200247), test loss: 3.10021832287\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (37.4666824341,25.1897626092), test loss: 31.4314275742\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.92857122421,2.36636977788), test loss: 3.75000634193\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (28.2490463257,25.1769887308), test loss: 32.8363287687\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.59177494049,2.36557903507), test loss: 2.59923958778\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (12.7044057846,25.1636858046), test loss: 29.6923821926\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.95671868324,2.364653656), test loss: 3.85717343986\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (44.5261459351,25.1498072233), test loss: 31.536071682\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.79993891716,2.36392848482), test loss: 2.97468260229\n",
      "run time for single CV loop: 1276.40658307\n",
      "\n",
      "MC # 4, Hype # hyp2, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold1/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (33.2834777832,inf), test loss: 33.0692209721\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.3283495903,inf), test loss: 2.91158091426\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (14.7576446533,26.6181141844), test loss: 28.2030566812\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.82139992714,2.5213909831), test loss: 3.48657968342\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (29.1782035828,26.5841544919), test loss: 29.9874709129\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.39611363411,2.51812406391), test loss: 2.80968209505\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (29.4686565399,26.6783043095), test loss: 29.0934841394\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.927968621254,2.50821414928), test loss: 3.79104423523\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (16.2313652039,26.6616973584), test loss: 31.953836441\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.803124248981,2.50832975868), test loss: 3.73724711835\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (47.2384185791,26.6417412858), test loss: 30.4246278763\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.96040606499,2.5051738624), test loss: 3.84681259394\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (23.8781547546,26.6055155913), test loss: 32.3770711422\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.63742589951,2.50572445645), test loss: 3.93493804634\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (2.58734607697,26.5994042908), test loss: 31.4105216503\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (0.907909095287,2.50573912756), test loss: 2.98486674428\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (33.0074539185,26.5593098366), test loss: 32.5739564896\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.922459363937,2.50541766352), test loss: 3.77562201619\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (12.4558477402,26.5434965248), test loss: 30.8211658955\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.35297119617,2.50340841427), test loss: 2.61497590542\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (3.35476875305,26.5393787268), test loss: 30.4157897949\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.53505754471,2.50229586954), test loss: 3.627078861\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (13.7909040451,26.5344970076), test loss: 33.1581654072\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (6.24417877197,2.50080994625), test loss: 2.9452157855\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (37.0919494629,26.5070036467), test loss: 29.3210563183\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.7406578064,2.49938616857), test loss: 3.62753743529\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (15.2181015015,26.4930061864), test loss: 35.8252940655\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.839424014091,2.49782309743), test loss: 3.02345356643\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (18.6695251465,26.4767585559), test loss: 29.4685954571\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.22012281418,2.49769936751), test loss: 3.4802574873\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (19.6043510437,26.4534127453), test loss: 29.9226925254\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.69064283371,2.49746910781), test loss: 3.06026227474\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (25.8399047852,26.4447734595), test loss: 31.7588364124\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.81007671356,2.49579627894), test loss: 3.78665413558\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (10.1138477325,26.4365106325), test loss: 37.2312312126\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.0916107893,2.4946119942), test loss: 3.64246495068\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (23.9079875946,26.4285660161), test loss: 32.3630773067\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.52629137039,2.49345890125), test loss: 3.3710632503\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (13.7484645844,26.4044815802), test loss: 31.0632925034\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.82757544518,2.49218813611), test loss: 3.83578287363\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (28.222530365,26.3911003881), test loss: 31.6252233028\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.315807193518,2.49150102899), test loss: 2.95595207885\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (7.42928981781,26.3701792866), test loss: 29.2634022236\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.90695393085,2.49100416246), test loss: 3.74142777324\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (23.2312698364,26.3528920074), test loss: 30.8492488861\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.2317814827,2.49001243662), test loss: 2.66574765742\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (20.7016010284,26.3473154594), test loss: 31.3978489399\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.906442165375,2.48883571147), test loss: 3.73818816841\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (26.2774009705,26.3354579748), test loss: 31.8502408028\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.650225996971,2.48774636521), test loss: 2.97394377589\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (13.6970062256,26.317971156), test loss: 31.0346845865\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.85983252525,2.48639543733), test loss: 3.61645517945\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (25.014415741,26.300323297), test loss: 30.0876662254\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.13276290894,2.4860779394), test loss: 2.82295816988\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (36.5035858154,26.2928025616), test loss: 31.0164731026\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.26936101913,2.48516981548), test loss: 3.62043044865\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (10.9169092178,26.2658891973), test loss: 30.7495265484\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.9503262043,2.48486727052), test loss: 3.52211366296\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (18.9103851318,26.2544983131), test loss: 28.024849689\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.21622610092,2.48328491725), test loss: 3.79957802892\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (54.6644096375,26.2466067084), test loss: 31.9987169266\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.54768323898,2.48223402665), test loss: 3.79271491617\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (18.7794647217,26.2338024346), test loss: 30.7765896797\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.24803948402,2.48136289509), test loss: 3.07674034685\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (31.6187953949,26.2129422012), test loss: 31.7675481796\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.10352683067,2.48024879176), test loss: 3.67675972432\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (19.223077774,26.2020467891), test loss: 34.3965463161\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.91838955879,2.47929807487), test loss: 2.85296364129\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (9.40253162384,26.1829913471), test loss: 29.2384309292\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.54097878933,2.47852225885), test loss: 3.73295157552\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (33.0479049683,26.1694245992), test loss: 32.2110989571\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.39526152611,2.47779467129), test loss: 2.68561634123\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (18.8421611786,26.156844852), test loss: 30.8475182056\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (7.69212770462,2.47686630042), test loss: 3.72174522281\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (16.9018440247,26.1449420298), test loss: 33.8132432222\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.36191749573,2.47607194116), test loss: 2.87880470455\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (4.2804851532,26.1334447384), test loss: 32.524416256\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.46798419952,2.47455489661), test loss: 3.7939361155\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (30.7705001831,26.1158726518), test loss: 30.0105472565\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.46980214119,2.47401200365), test loss: 2.84936500788\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (13.5314340591,26.1012111264), test loss: 28.1177634716\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.8658657074,2.47308330455), test loss: 3.67882616222\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (38.1810264587,26.0857336706), test loss: 30.727869308\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.46968436241,2.47237821679), test loss: 3.40704784989\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (9.14752006531,26.0683026345), test loss: 27.472342205\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.97642302513,2.47158349142), test loss: 3.79562563002\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (30.3190765381,26.0613651552), test loss: 31.8131610394\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.52874326706,2.47032103473), test loss: 3.67884167731\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (31.1704463959,26.0488003132), test loss: 29.5635245323\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.84651041031,2.46969624924), test loss: 3.07919138074\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (46.4669876099,26.0326581246), test loss: 33.8465488434\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.486469209194,2.46838992373), test loss: 3.75818561316\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (19.4051551819,26.0184902118), test loss: 30.3167827129\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.21943664551,2.46741812661), test loss: 2.7769090116\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (70.2633743286,26.0051005545), test loss: 36.7611421585\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.690294981,2.46697161833), test loss: 3.92033092082\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (37.8928604126,25.9863247878), test loss: 32.2530040741\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (4.90067195892,2.46621686069), test loss: 2.84314802885\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (39.3076057434,25.974722715), test loss: 29.5808918715\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.09103679657,2.46493089819), test loss: 3.74569531083\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (14.2413358688,25.9607985302), test loss: 30.7954652786\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.69898068905,2.46418890286), test loss: 2.9392331183\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (19.6384391785,25.9501925186), test loss: 28.4239633083\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.6207485199,2.46328117537), test loss: 3.77171553075\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (20.4112167358,25.9320871416), test loss: 31.5030288696\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.16223478317,2.46231493268), test loss: 2.75422596335\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (42.3842430115,25.9198509527), test loss: 28.2282811522\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.4484834671,2.46144973084), test loss: 3.66926414073\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (22.5514698029,25.9041650526), test loss: 30.65191679\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.43213176727,2.46067822245), test loss: 3.3964622736\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (32.7139892578,25.8883381609), test loss: 28.0750356436\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.65736627579,2.46000482945), test loss: 3.81831798851\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (24.6669006348,25.8776888441), test loss: 31.2685271978\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.817985534668,2.45879829732), test loss: 3.86710779071\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (12.3582077026,25.865078224), test loss: 29.3887112141\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.681747198105,2.45800209149), test loss: 3.11305859387\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (43.8355102539,25.8514024199), test loss: 31.5148405075\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.5107653141,2.45698396972), test loss: 3.86199074984\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (22.0350246429,25.8360590937), test loss: 32.3577972412\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.35640716553,2.45624001108), test loss: 2.8298484683\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (3.29008507729,25.8231274603), test loss: 29.3312898159\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.509133577347,2.45550666227), test loss: 3.82749140561\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (30.8172988892,25.8063259089), test loss: 34.4159952164\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.02963423729,2.45477292135), test loss: 2.89994811118\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (12.1402645111,25.7934438515), test loss: 29.3123206615\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.51546144485,2.45377519769), test loss: 3.72071191072\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (3.55743312836,25.7812133965), test loss: 32.8363385677\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.04632306099,2.45296587166), test loss: 2.87529688478\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (14.3695554733,25.7688890641), test loss: 28.9003707886\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (5.2484254837,2.45200512202), test loss: 3.59064124823\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (30.9674720764,25.7534648878), test loss: 30.0984792709\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (4.27165222168,2.45103262602), test loss: 2.91641741991\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (18.3189697266,25.7402122318), test loss: 35.1161761999\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.29827654362,2.45014493871), test loss: 3.72366730422\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (19.8967914581,25.7264234464), test loss: 31.6025743484\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.68979024887,2.44944481925), test loss: 3.55388282537\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (16.4829959869,25.7102645876), test loss: 30.0725556731\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.44868731499,2.44871944063), test loss: 3.71437358856\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (18.8908195496,25.6976773558), test loss: 31.7567936182\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.73153829575,2.44773803454), test loss: 3.89200953543\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (12.228225708,25.686909151), test loss: 33.1634925842\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.769007444382,2.44685004484), test loss: 2.89610079527\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (21.2220573425,25.674876721), test loss: 30.4436595917\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.48196482658,2.44598470914), test loss: 3.86435479522\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (13.1170959473,25.6586754346), test loss: 29.9061823845\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.71106672287,2.44504129306), test loss: 2.76778325438\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (22.6733703613,25.6460330314), test loss: 28.2651916027\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.254759311676,2.44423315091), test loss: 3.79020361602\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (7.69220924377,25.6311127797), test loss: 31.8887687683\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.99426674843,2.44355470643), test loss: 2.88319836557\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (23.2941360474,25.6171771125), test loss: 28.6643030882\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.19859910011,2.44268156218), test loss: 3.69332981408\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (15.5577287674,25.6056541391), test loss: 32.4007745743\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.978772878647,2.44177008398), test loss: 2.96916323006\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (26.0671577454,25.5932393474), test loss: 27.8934553146\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.649508118629,2.44091297762), test loss: 3.59268195629\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (11.5095024109,25.579935428), test loss: 29.8594789982\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.87096965313,2.43990343627), test loss: 2.84743842483\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (26.6495075226,25.5652825154), test loss: 30.6479812622\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.4225025177,2.43929329186), test loss: 3.76211993098\n",
      "\n",
      "MC # 4, Hype # hyp2, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold2/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (42.4600715637,inf), test loss: 34.2872018814\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.82850909233,inf), test loss: 2.62304745317\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (42.9850997925,26.9696809249), test loss: 33.8043375015\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.09847760201,2.54576875067), test loss: 3.45715624094\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (29.3726520538,26.8753787637), test loss: 33.2121300697\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.57523941994,2.54969408256), test loss: 2.72230687737\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (14.1711616516,26.9638359488), test loss: 33.7043605804\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.30675673485,2.53708926584), test loss: 3.74468220472\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (17.5074291229,26.9230735425), test loss: 35.4775704861\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.0572385788,2.53588552526), test loss: 3.70143297911\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (46.420627594,26.9349116763), test loss: 35.8032167912\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.05929231644,2.53362500695), test loss: 3.77796547413\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (18.1606903076,26.8744022373), test loss: 34.9398931026\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.7215102911,2.5333790313), test loss: 3.92645778954\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (3.04631853104,26.9162427689), test loss: 34.8428112507\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.01042115688,2.53379172609), test loss: 2.70790536702\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (44.3205070496,26.8493967933), test loss: 34.2024168015\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.84528958797,2.53293739926), test loss: 3.89215061069\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (16.203704834,26.8481475605), test loss: 34.9229905128\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.3503074646,2.53367176424), test loss: 2.48070803136\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (61.6401863098,26.8243811141), test loss: 33.5207489014\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.61611521244,2.5298666856), test loss: 3.93530341387\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (23.6050891876,26.8284464832), test loss: 33.3115852356\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.47544813156,2.52924131456), test loss: 2.7688796103\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (43.3730239868,26.804090253), test loss: 36.8131942272\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.11207485199,2.52748303248), test loss: 3.65202332139\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (56.5992202759,26.7886294134), test loss: 33.2723195076\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (5.09628582001,2.52649625627), test loss: 2.71582088768\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (14.9935245514,26.7766149692), test loss: 35.1077476501\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.52679681778,2.52630348841), test loss: 3.46574101746\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (25.2136573792,26.7374114683), test loss: 33.7807589531\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.42985892296,2.52560967722), test loss: 2.83174438179\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (48.5584602356,26.7496044902), test loss: 35.5378612041\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.35484313965,2.52484360521), test loss: 3.71962305307\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (25.6284103394,26.7146683876), test loss: 35.7951872349\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.05679178238,2.52304170902), test loss: 3.69116192758\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (27.1980400085,26.7197593143), test loss: 37.2552771568\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.86164522171,2.52210755995), test loss: 3.38512284458\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (17.5206336975,26.6875175812), test loss: 36.5476042271\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.46838498116,2.52069674382), test loss: 3.69921291173\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (26.0298080444,26.6800613569), test loss: 37.6570512295\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.65009188652,2.51991461588), test loss: 2.94705458879\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (18.5790138245,26.6585594378), test loss: 35.0072734833\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.09761059284,2.5193039516), test loss: 3.90913757086\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (32.3242797852,26.639344583), test loss: 31.4942452908\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.96649360657,2.51929874002), test loss: 2.47706119716\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (33.5453910828,26.6336441222), test loss: 33.5377701283\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.89377510548,2.5173515869), test loss: 3.77678393424\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (14.3636541367,26.6162341619), test loss: 34.0274284363\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.787683606148,2.51639807803), test loss: 2.66569815278\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (14.9722137451,26.6032591048), test loss: 36.332450223\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.62232780457,2.51521803748), test loss: 3.73350298405\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (15.6956233978,26.5811544333), test loss: 34.383231163\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.39118289948,2.51459673364), test loss: 3.02212215662\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (13.0198059082,26.581080209), test loss: 32.4085544109\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.56436681747,2.51412171181), test loss: 3.56858959198\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (12.8795127869,26.5490268232), test loss: 40.1985665321\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.15706157684,2.51324684066), test loss: 3.45948457271\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (21.2153396606,26.5406704513), test loss: 34.7622768641\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (5.65240859985,2.51279309049), test loss: 3.83158986866\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (6.30700874329,26.5193573775), test loss: 35.7869921684\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.22040319443,2.5111921872), test loss: 3.62493657172\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (39.5152053833,26.5141809061), test loss: 33.4264099121\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (6.6115231514,2.51040421798), test loss: 3.20696844459\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (33.4964408875,26.4942612346), test loss: 37.7577198029\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.94487571716,2.50909937992), test loss: 3.77071332037\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (14.3178281784,26.4794847266), test loss: 44.2819728613\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.08956873417,2.50818515231), test loss: 2.65968094766\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (34.9350509644,26.4680023377), test loss: 38.4412717819\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.06437492371,2.50767285395), test loss: 3.91682037711\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (15.0252323151,26.4432353165), test loss: 30.9562119961\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.596763134,2.50706305472), test loss: 2.48670323193\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (64.8221054077,26.4387369006), test loss: 39.2973138332\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.87595033646,2.50623428351), test loss: 3.87813481688\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (18.7395706177,26.4170364981), test loss: 37.4936782598\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.03457927704,2.50491710882), test loss: 2.63808731139\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (49.3960380554,26.411789668), test loss: 36.2656665087\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.99561166763,2.50395228091), test loss: 3.54042471647\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (17.4899654388,26.389379183), test loss: 33.3007368088\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.66351079941,2.5029139768), test loss: 2.87632946968\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (13.3255519867,26.3788501018), test loss: 33.0208878517\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.526674509048,2.50227532983), test loss: 3.73716000319\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (8.45686912537,26.3607264115), test loss: 33.8646361828\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.88056254387,2.50153126008), test loss: 3.43962674737\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (26.616985321,26.34486883), test loss: 32.8667143345\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.18847966194,2.50104809737), test loss: 3.66685885489\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (22.40259552,26.3342808467), test loss: 33.6087591648\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.30885887146,2.49969414989), test loss: 3.6233514607\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (38.3030166626,26.3204288044), test loss: 33.192321682\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (5.44020652771,2.49891700803), test loss: 2.99322110862\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (15.3967514038,26.3069433995), test loss: 38.2519479752\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.40941047668,2.49780102385), test loss: 3.80055034012\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (45.3707122803,26.289195916), test loss: 34.998082304\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.95499992371,2.49711310007), test loss: 2.76038095802\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (75.0253295898,26.283049319), test loss: 35.1373244286\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (6.42826223373,2.49652806876), test loss: 3.96475459337\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (46.1382751465,26.2584266802), test loss: 38.9874014854\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.28699159622,2.49563947686), test loss: 2.71985355765\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.5576181412,26.2479057816), test loss: 35.3372282267\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.48128986359,2.49489455127), test loss: 3.77046993971\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (6.50841426849,26.2298098835), test loss: 34.69535923\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.18331623077,2.49375456979), test loss: 2.70105785131\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (25.4173660278,26.2202500606), test loss: 34.2116616488\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.56426620483,2.4927889992), test loss: 3.52214796543\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (15.4925584793,26.2025893873), test loss: 34.8070147514\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.964151203632,2.49171651402), test loss: 2.75666540265\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (19.0644378662,26.1892352676), test loss: 39.2628427744\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.72391748428,2.49101450223), test loss: 3.56072692871\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (35.8472862244,26.1760664169), test loss: 36.7327682972\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.1453435421,2.49022630978), test loss: 3.49164409041\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (9.37448406219,26.1563092654), test loss: 34.4636114359\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.5371465683,2.48964786355), test loss: 3.67806145549\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (8.98566055298,26.146520509), test loss: 35.5391591549\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.1545484066,2.48878003159), test loss: 3.87238846123\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (16.0902061462,26.129459882), test loss: 41.7950803757\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.828402161598,2.487688772), test loss: 2.87135364115\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (60.359413147,26.1202711739), test loss: 38.2319300652\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.94356536865,2.48674470121), test loss: 3.77131709456\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (15.2244224548,26.1018062069), test loss: 35.6365052938\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.819974422455,2.48578354572), test loss: 2.78378687203\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (23.6726112366,26.0905891814), test loss: 34.0649081707\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.83620154858,2.48516307834), test loss: 3.99994333088\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (43.6820449829,26.0735705547), test loss: 33.100820303\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.87018322945,2.48433641953), test loss: 2.76541873813\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (16.57554245,26.058477775), test loss: 34.5267619133\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.06013810635,2.48378011131), test loss: 3.71984573305\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (20.8712348938,26.0465326215), test loss: 34.2184941769\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.69236683846,2.48254816917), test loss: 2.76648864448\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (23.0851860046,26.0329664897), test loss: 34.4357711792\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.54461789131,2.48169876867), test loss: 3.39329852462\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (12.3000707626,26.0196218266), test loss: 33.0752739906\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.17908358574,2.4807413885), test loss: 2.81588332653\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (19.5705432892,26.0029423942), test loss: 36.5205716133\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.69485139847,2.48003969327), test loss: 3.7465043664\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (13.7094917297,25.993603003), test loss: 46.008266449\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.933351755142,2.47926108907), test loss: 3.98520423472\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (16.8029251099,25.9735834161), test loss: 35.2169748783\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.40063893795,2.47857374132), test loss: 3.6360131979\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (9.60423851013,25.9634990527), test loss: 34.2470006466\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.82005023956,2.47778043163), test loss: 3.71311756372\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (49.3327102661,25.9471679718), test loss: 36.2849974155\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.18561935425,2.4767697806), test loss: 2.80813971311\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.3821630478,25.9366277427), test loss: 36.3814969778\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.893691420555,2.47586269941), test loss: 3.87662415504\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (44.7627029419,25.9205045861), test loss: 37.1460010529\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.45429134369,2.47491787641), test loss: 2.6092392832\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.1395549774,25.9076375346), test loss: 35.0645837307\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.10909295082,2.47420236465), test loss: 3.84219231606\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (28.167804718,25.8942131896), test loss: 36.7240438461\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.01960039139,2.47337327219), test loss: 2.73415627182\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (36.907623291,25.8777757853), test loss: 34.3144819975\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.27158164978,2.47281815895), test loss: 3.63342382312\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (13.9496269226,25.8671870477), test loss: 37.9539608479\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.53813421726,2.47194372539), test loss: 2.7944677949\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (9.522315979,25.8515498804), test loss: 39.0660030842\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.27631103992,2.47100316419), test loss: 3.39445736408\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (9.01902008057,25.8407065656), test loss: 35.4793894768\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.68653678894,2.47003117138), test loss: 2.95368374139\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (57.7495727539,25.8250848524), test loss: 34.0363098145\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.81461715698,2.46920815931), test loss: 4.07950129211\n",
      "\n",
      "MC # 4, Hype # hyp2, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold3/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (52.1117172241,inf), test loss: 29.3966565609\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.64896321297,inf), test loss: 2.614433375\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (22.4557685852,26.9487857409), test loss: 31.484238553\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.36103391647,2.58640108293), test loss: 3.74161932766\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (27.6669769287,26.8816078668), test loss: 26.9103810072\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.61963057518,2.58065548566), test loss: 2.50230759382\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (15.2290363312,26.8609292164), test loss: 31.4109830856\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.885014951229,2.57072293542), test loss: 3.9081938982\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (17.571559906,26.9250948406), test loss: 28.4021495342\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.67892980576,2.57221798857), test loss: 3.33189257681\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (50.990486145,26.8754713964), test loss: 36.8470356464\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.36875844002,2.56936002333), test loss: 4.12803785354\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (19.8444766998,26.8526686528), test loss: 28.1597759962\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.64506030083,2.57031459511), test loss: 3.58232644796\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (9.53007411957,26.8216101044), test loss: 34.8628853798\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.4576292038,2.56875975265), test loss: 3.28229083121\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (36.3329238892,26.8317887789), test loss: 29.9927544594\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.26575613022,2.56956032782), test loss: 3.51263020337\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (11.4171981812,26.7841338773), test loss: 34.6240176678\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.40214085579,2.5676732482), test loss: 2.81840539277\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (31.2012004852,26.7900320254), test loss: 31.1339303493\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.80790472031,2.5657338814), test loss: 3.62540766001\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (31.3102378845,26.7781142239), test loss: 33.7166916132\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.3390750885,2.56462248569), test loss: 2.67144760191\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (24.4019012451,26.7568219101), test loss: 31.6175364494\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.964227080345,2.56315451644), test loss: 3.60451544225\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (16.9675312042,26.738650031), test loss: 30.0655814171\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.956750094891,2.56159761187), test loss: 2.64609552026\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (56.2625770569,26.725070913), test loss: 34.5855834007\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.65165805817,2.56136746847), test loss: 3.75415279865\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (23.8006706238,26.704233712), test loss: 27.160685873\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.549441337585,2.56042748872), test loss: 3.16026257277\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (49.0953559875,26.6919837193), test loss: 32.3816085815\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.20298814774,2.55891617256), test loss: 4.09531039\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (18.0814418793,26.6848363171), test loss: 28.1391256332\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.79415923357,2.5580951866), test loss: 3.47694853842\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (16.1190128326,26.6683690695), test loss: 33.4783709049\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.09617495537,2.55700939312), test loss: 3.3546479404\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (15.0409164429,26.6492725116), test loss: 32.4103450298\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.46668589115,2.55568888526), test loss: 3.28161949515\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (35.5609550476,26.6324241765), test loss: 36.6430273533\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.43029403687,2.55480903144), test loss: 3.03951777816\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (19.911655426,26.6201663312), test loss: 28.4736978769\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.42137932777,2.5544052615), test loss: 3.49499156475\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (13.4670953751,26.5940613817), test loss: 32.4781919479\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.59797692299,2.5532080868), test loss: 2.82230172455\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (31.6119308472,26.5884993572), test loss: 28.91806283\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (5.20810699463,2.55207712138), test loss: 3.53858544827\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (33.0874938965,26.5767548514), test loss: 29.4951003551\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.17006015778,2.5509509885), test loss: 2.79821074903\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (41.2627983093,26.5623826685), test loss: 32.160919404\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.92684555054,2.54976961891), test loss: 3.69771876931\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (23.0099544525,26.5407675903), test loss: 27.5293679714\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.67033672333,2.54907405234), test loss: 2.62621422708\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (13.6099710464,26.5273364403), test loss: 30.7724935055\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.86908531189,2.54809072166), test loss: 3.78655203581\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (12.9436073303,26.5166328978), test loss: 27.4787240505\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.77795350552,2.54754083314), test loss: 3.12409685552\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (13.769408226,26.4932743051), test loss: 32.8711868286\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.75373268127,2.54644757311), test loss: 4.11113351285\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (23.3867797852,26.4888356061), test loss: 27.4141114235\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.82311689854,2.5452635688), test loss: 3.36052150726\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (26.7672367096,26.4735359602), test loss: 35.8283764362\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.58160090446,2.54418743454), test loss: 3.31329371035\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (67.1485748291,26.4628928517), test loss: 31.6091237068\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.87036561966,2.54325832313), test loss: 3.48959194422\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (22.3131713867,26.4403968762), test loss: 36.6700068951\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.48980164528,2.54197718084), test loss: 2.97599360347\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (41.7607803345,26.4335723138), test loss: 34.6966595173\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.25813388824,2.54167460884), test loss: 3.70070286095\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (102.374732971,26.4142600837), test loss: 40.0016284943\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (6.14975404739,2.54072312025), test loss: 2.80004982352\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (23.2839279175,26.4018535154), test loss: 29.1353171825\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.77504968643,2.53958276139), test loss: 3.49807125926\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (22.3115463257,26.3860342894), test loss: 28.9603780746\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.94450390339,2.5385037789), test loss: 2.85618397892\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.2576770782,26.373673915), test loss: 31.3288051605\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.37277185917,2.53756453518), test loss: 3.78216774464\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (10.3036794662,26.3547450722), test loss: 26.9616286278\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (5.09228181839,2.53661043837), test loss: 2.54754146338\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (6.43339395523,26.3411673036), test loss: 32.3759315968\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.41593408585,2.53554571779), test loss: 3.87825981379\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (21.4545135498,26.3292357996), test loss: 27.3286084652\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.54291415215,2.53492749085), test loss: 3.13192932904\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (48.1532478333,26.3101192252), test loss: 33.4467597008\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.12776756287,2.53383161646), test loss: 4.08276437223\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (14.7505617142,26.3026426917), test loss: 28.1339967251\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (6.61571502686,2.53313466617), test loss: 3.58714094162\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (26.8624305725,26.2881099881), test loss: 35.1212118626\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (6.14277839661,2.53201987748), test loss: 3.33679146469\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (5.53000307083,26.2770974131), test loss: 32.6518783569\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.6227376461,2.53096133698), test loss: 3.59423441291\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (17.9987640381,26.2557916215), test loss: 34.9941365719\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.40519475937,2.53000551012), test loss: 2.79180788398\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (21.73645401,26.2467171865), test loss: 28.9066874981\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.50598359108,2.52925501426), test loss: 3.50071085393\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (30.5336532593,26.230936815), test loss: 30.5212304592\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (4.03709506989,2.52851260868), test loss: 2.78381590843\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (11.1767063141,26.214881942), test loss: 28.7602704048\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.98277258873,2.52758713675), test loss: 3.58122416735\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (25.7280082703,26.203465703), test loss: 32.8499248981\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.97013926506,2.52640776678), test loss: 2.7047845751\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (28.8885231018,26.1911348855), test loss: 31.9216572762\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.05210506916,2.5254467206), test loss: 3.69878709316\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (19.1453990936,26.1782849371), test loss: 26.7725272179\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.570247709751,2.52454187555), test loss: 2.55039507151\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (7.84196949005,26.1593933555), test loss: 32.0985800266\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (7.21272373199,2.52359449523), test loss: 4.1468565464\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (14.8678531647,26.1516206513), test loss: 32.2703686714\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.431216031313,2.52291900452), test loss: 3.34837626219\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (55.5502548218,26.133106383), test loss: 43.20349226\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (5.34537982941,2.522072359), test loss: 4.14070645869\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (5.04313564301,26.123253918), test loss: 27.8440599442\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.31987333298,2.52109276543), test loss: 3.52173813283\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.1334199905,26.1065237112), test loss: 33.1105523348\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.46470165253,2.5200015252), test loss: 3.41920746565\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (20.6359729767,26.097654174), test loss: 33.0118038177\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.08059310913,2.51912764387), test loss: 3.43524400592\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (27.6499919891,26.0779799251), test loss: 35.1509637833\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.03473138809,2.51804145836), test loss: 2.87101193815\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (29.7746238708,26.0681293624), test loss: 29.3257483482\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.99684333801,2.51719701642), test loss: 3.48984720707\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (36.5503234863,26.0529432833), test loss: 32.4392439842\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.811615347862,2.51626325676), test loss: 2.87138398588\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (52.3287849426,26.0386216697), test loss: 30.0475653172\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.29338884354,2.51530320426), test loss: 3.57017529607\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.1463356018,26.0260809765), test loss: 32.6968920708\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.50897455215,2.51441325586), test loss: 2.79520572126\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (21.2960319519,26.0136596959), test loss: 36.0432966232\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.99469709396,2.51338967862), test loss: 3.67622254491\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (14.1584644318,26.0017984786), test loss: 28.0104414463\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.18511509895,2.51238798353), test loss: 2.60028615594\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (36.1637191772,25.9831706878), test loss: 31.0369057178\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.76220321655,2.51130338817), test loss: 4.04940816164\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (47.7946472168,25.9748652973), test loss: 28.0565454006\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.55123591423,2.5105830591), test loss: 3.47448397577\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (17.0753803253,25.9578470611), test loss: 35.4623632431\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.20606350899,2.50963728656), test loss: 3.79499951005\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (23.9625473022,25.9455738232), test loss: 28.81513834\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.11273527145,2.50884268548), test loss: 3.41376879215\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (40.5856018066,25.9320867362), test loss: 36.029693985\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.20961594582,2.50761679163), test loss: 3.07770198882\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (28.5114860535,25.9220553437), test loss: 30.5253185272\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.31210422516,2.50676638015), test loss: 3.50104978085\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (7.01617097855,25.9062351092), test loss: 35.3642208576\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.98351883888,2.50578047131), test loss: 2.88756658733\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (11.1024894714,25.8919683617), test loss: 27.6256774902\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.79176974297,2.5048731706), test loss: 3.63784797788\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (35.8497505188,25.8810230581), test loss: 36.6777467728\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.89008784294,2.50408141168), test loss: 2.82105826735\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (25.0550422668,25.8660532651), test loss: 30.5560498714\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.76826882362,2.50315629909), test loss: 3.60494595766\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (16.6682167053,25.8538616639), test loss: 29.7292729855\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.90830135345,2.50214763488), test loss: 2.70688175857\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.3218231201,25.8391262943), test loss: 30.0380103588\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (4.17887926102,2.50112063131), test loss: 4.14840451479\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (27.5758323669,25.8305239242), test loss: 30.7906497955\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.84304881096,2.50023162999), test loss: 3.1777187109\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (19.3816719055,25.8115950129), test loss: 32.0444531441\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.55441737175,2.49913478853), test loss: 4.15976672769\n",
      "\n",
      "MC # 4, Hype # hyp2, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold4/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (43.3285942078,inf), test loss: 32.2647773266\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.53537988663,inf), test loss: 3.23924485147\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (38.1347427368,27.2961945238), test loss: 30.0734810352\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.957235097885,2.57019303799), test loss: 3.11111484766\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (41.7031135559,27.1871207873), test loss: 30.6494229794\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.40784883499,2.57150954022), test loss: 3.07858746052\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (6.1594452858,27.2486592542), test loss: 29.4784713507\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.79436612129,2.56660977051), test loss: 3.38728728294\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (7.68639659882,27.2276191706), test loss: 31.8501042366\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.02439308167,2.5624896354), test loss: 3.17126796842\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (25.3959960938,27.2219399415), test loss: 31.7082435131\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.764186143875,2.55990585105), test loss: 2.92016852796\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (13.7966022491,27.1752429334), test loss: 34.2406997204\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.09413051605,2.55695795098), test loss: 3.17709180117\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (22.9235725403,27.1768914771), test loss: 35.2156159401\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.99342763424,2.55807012131), test loss: 2.95156724453\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (21.9000396729,27.1758234992), test loss: 30.7543152332\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.729635238647,2.55614616158), test loss: 3.30063920021\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (18.6038036346,27.1502144334), test loss: 31.1392727375\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.6719878912,2.55604868402), test loss: 2.8948826164\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (32.6045303345,27.1232472615), test loss: 33.307011652\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.13274073601,2.5555105076), test loss: 3.15465374589\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (79.35131073,27.1185013158), test loss: 40.8158714294\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.46399068832,2.55470863733), test loss: 2.74371348321\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (21.7280330658,27.0940791137), test loss: 30.5416883469\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.05361461639,2.55247326931), test loss: 3.4454238832\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (35.3002510071,27.0895675421), test loss: 33.3467353344\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.71802663803,2.55148324966), test loss: 3.11211029887\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (29.2169837952,27.067281145), test loss: 31.8985799789\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.48309469223,2.55056284081), test loss: 3.36495124698\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (14.9339885712,27.0536546957), test loss: 32.0812288284\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (6.61567306519,2.54977460799), test loss: 3.18998901248\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (39.7091407776,27.0462497496), test loss: 34.6772879839\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (5.02381229401,2.54907117152), test loss: 2.80505791605\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (46.4210128784,27.0251049699), test loss: 31.7548461914\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.70444977283,2.54858317211), test loss: 3.24691249728\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (15.4590978622,26.9999683183), test loss: 28.5644303322\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.46181178093,2.54748667533), test loss: 2.7797867626\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (22.6651535034,26.9920814347), test loss: 31.5130609512\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.26669573784,2.54561696565), test loss: 3.25921793878\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (10.3242912292,26.9746299538), test loss: 36.4717554092\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.35778284073,2.54525780097), test loss: 2.87655212283\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (30.1857376099,26.9615481885), test loss: 30.1192562103\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.12565422058,2.54361694691), test loss: 3.17836272717\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (9.57878112793,26.9380552318), test loss: 31.1710476875\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.49305343628,2.54266660171), test loss: 2.88139154315\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (23.1637058258,26.9333688956), test loss: 31.879901123\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.21014428139,2.54187615794), test loss: 3.33297918439\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (14.4439220428,26.9139090548), test loss: 31.752213192\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.96387195587,2.54117658172), test loss: 3.16458475888\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (26.0952072144,26.894682727), test loss: 33.5848442316\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.0926194191,2.54076738174), test loss: 3.17895773053\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (15.4236869812,26.8791141224), test loss: 35.6925229073\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.77357625961,2.53984978022), test loss: 3.01106075346\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (25.6224479675,26.8642856352), test loss: 33.9989250422\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.83341836929,2.53814387366), test loss: 2.60675802231\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (17.6835327148,26.8494452546), test loss: 31.2029428244\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.876355051994,2.53708814082), test loss: 3.13453431726\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (60.257019043,26.8351872768), test loss: 33.94314785\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.31381940842,2.53594877801), test loss: 3.1575738728\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (25.8646888733,26.817861657), test loss: 31.6806770802\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.07000601292,2.53519590366), test loss: 3.29442432523\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (36.2948570251,26.804026479), test loss: 32.2891645908\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.45772099495,2.533965213), test loss: 3.26983985901\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (22.2049217224,26.7884292953), test loss: 29.6383072376\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.70841765404,2.53351669247), test loss: 2.95742520392\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (15.0424423218,26.7672868159), test loss: 32.9845762253\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.72330856323,2.53278377452), test loss: 2.88669941127\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (20.1390991211,26.7591549371), test loss: 29.8156788349\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.76295256615,2.53180739516), test loss: 3.35678822994\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (14.1534042358,26.7422875553), test loss: 32.8699449301\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.1566991806,2.53080831554), test loss: 3.199379915\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (20.5525398254,26.7285271909), test loss: 32.3821882248\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.46777582169,2.52983467061), test loss: 3.15828744471\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (6.35625886917,26.7090126918), test loss: 34.1498551846\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.17787909508,2.52849962133), test loss: 3.19992548227\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (12.1111364365,26.6968543309), test loss: 35.4014725685\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.15149402618,2.52796881679), test loss: 2.9690547049\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (7.11733102798,26.6840531137), test loss: 29.7967668295\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.45931005478,2.52697562675), test loss: 3.28485477716\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (12.7882499695,26.6665489362), test loss: 32.786162138\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.17557907104,2.52619941931), test loss: 2.97321695834\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (7.24054098129,26.6491330228), test loss: 31.5412031889\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.5094704628,2.52531982085), test loss: 3.09486067891\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (7.32278633118,26.6363112336), test loss: 33.8223540306\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.5608215332,2.52450600511), test loss: 2.76688104868\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (23.0972747803,26.6207777451), test loss: 29.6762207747\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.05288720131,2.52332097716), test loss: 3.22476286888\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (19.2384719849,26.6077377496), test loss: 31.9468373775\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.21151947975,2.52235089983), test loss: 3.10163065791\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (33.3468780518,26.5926568019), test loss: 31.0961195946\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.00918674469,2.52142484505), test loss: 3.25189920962\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (12.3042802811,26.5787871214), test loss: 31.3000939846\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.05758929253,2.52044738097), test loss: 3.18814998567\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (26.9424571991,26.5661556843), test loss: 38.6865832806\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.05971181393,2.5196439802), test loss: 2.91270948648\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (22.4568119049,26.5492794155), test loss: 35.9730540276\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.13206982613,2.51909530891), test loss: 3.19419449568\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (31.0712566376,26.5323810399), test loss: 29.4398770094\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.53095138073,2.51817684634), test loss: 2.7543587923\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (26.4963188171,26.520038843), test loss: 31.4618836641\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.76242113113,2.51686580221), test loss: 3.18234217763\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (20.8560256958,26.5071915906), test loss: 34.3144243717\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.11792993546,2.51630000985), test loss: 2.88937411308\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (72.3213195801,26.4920249681), test loss: 30.7311938763\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.66045570374,2.51504158123), test loss: 3.10740220547\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (15.0949735641,26.4753008468), test loss: 31.2155083179\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.35784494877,2.51406972101), test loss: 2.75426700413\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (30.4778842926,26.4642506954), test loss: 29.495227623\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.44804286957,2.51339485152), test loss: 3.20809852481\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (22.8953437805,26.4496242742), test loss: 33.5213920593\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.38354253769,2.51262349778), test loss: 3.18122402132\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (71.8459625244,26.4331039864), test loss: 42.9052016735\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (6.96902084351,2.51189755807), test loss: 3.27438937128\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (27.5325069427,26.4197055631), test loss: 35.1343727589\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (6.57186746597,2.51111580933), test loss: 3.0390104562\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (31.3619422913,26.4051369524), test loss: 32.6650634766\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.65687799454,2.50988568501), test loss: 2.70924822688\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (33.8019218445,26.3911647014), test loss: 33.0702258587\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (4.76938533783,2.50898841884), test loss: 3.37150619626\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (22.5924682617,26.376348933), test loss: 31.7380818844\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.59742105007,2.50790734261), test loss: 2.99373762012\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (34.599105835,26.3626507906), test loss: 31.6369496822\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.073741436,2.50721019932), test loss: 3.26019302011\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (15.8262310028,26.3491849806), test loss: 31.9019769192\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.971527457237,2.50618984823), test loss: 3.16089940518\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (31.5266838074,26.3345038128), test loss: 28.4516117096\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.894268155098,2.50551248815), test loss: 2.93616550863\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (4.78701496124,26.3184209613), test loss: 32.9422149658\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.60160470009,2.50474571201), test loss: 2.90094407797\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (38.5154075623,26.3073799977), test loss: 30.3941788197\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.93314909935,2.50382325375), test loss: 3.36726971269\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (65.3153152466,26.2932307946), test loss: 32.1716575623\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.56360435486,2.50292665289), test loss: 3.42457187474\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (39.3192749023,26.279569991), test loss: 32.1318679571\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.33609676361,2.50207067852), test loss: 3.31450551152\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (19.5906543732,26.2636935816), test loss: 35.3153776169\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.541878461838,2.50101085355), test loss: 3.06651997566\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (36.7959403992,26.2503638756), test loss: 33.6872136593\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.02671003342,2.50027330559), test loss: 2.85824283361\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (10.7469377518,26.2386396897), test loss: 29.8138950825\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.78015422821,2.49947229785), test loss: 3.17541889548\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.7409858704,26.2227353992), test loss: 33.0279113054\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.266180187464,2.49872315827), test loss: 2.99848566651\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (40.5737075806,26.2077159216), test loss: 30.5049830914\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.46333670616,2.49786832454), test loss: 3.05551139116\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (4.65361833572,26.194811832), test loss: 33.8891783714\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (6.70464754105,2.49705792902), test loss: 2.94726995826\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (20.4622631073,26.1803587857), test loss: 31.0240072489\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.79162740707,2.49604869802), test loss: 3.05226359367\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.3589324951,26.1677764798), test loss: 31.3258774281\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.63362240791,2.49517850333), test loss: 3.23592276573\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (39.9329071045,26.1538073341), test loss: 31.9586224318\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.31099224091,2.49427371006), test loss: 3.29556356072\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (22.526222229,26.140656983), test loss: 31.4295621872\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (4.73632001877,2.49346256263), test loss: 3.19051588178\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (22.6481800079,26.1275940531), test loss: 34.0956838131\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.295482486486,2.49267342462), test loss: 2.86098241806\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (16.367767334,26.1130598954), test loss: 36.0524744034\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.57203722,2.49200235958), test loss: 3.27260878384\n",
      "\n",
      "MC # 4, Hype # hyp2, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold5/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (36.9698410034,inf), test loss: 33.1162565231\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.33084273338,inf), test loss: 3.14230338335\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (41.441696167,27.1267503219), test loss: 30.8503081799\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.991897344589,2.52371424019), test loss: 3.51701350212\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (52.6216468811,27.1131695201), test loss: 30.7190134525\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.24913024902,2.52172953382), test loss: 3.3084746778\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (67.6992797852,27.1767932037), test loss: 36.3309417009\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.53720450401,2.51942556859), test loss: 3.70807324648\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.4605293274,27.1681914493), test loss: 32.0986750603\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.46689963341,2.51464787101), test loss: 3.44048521221\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (30.441570282,27.1719602971), test loss: 29.5798160791\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.39202976227,2.51192695324), test loss: 3.10154436231\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (18.5074310303,27.1348899824), test loss: 35.1441879511\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.8916465044,2.50941232474), test loss: 3.57745577693\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (9.67057418823,27.1443212694), test loss: 31.0674175739\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.03417015076,2.50928005665), test loss: 2.5447130084\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (44.1444854736,27.1149138891), test loss: 32.7330294609\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.4341237545,2.50861712406), test loss: 3.65652306676\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (6.71603727341,27.0748199144), test loss: 29.7884686947\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.92017710209,2.50821646936), test loss: 2.80512927175\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (37.3941879272,27.0814371284), test loss: 34.4163059235\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.28769874573,2.50786563731), test loss: 3.79079234004\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (10.2290496826,27.0620815256), test loss: 31.5093751431\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.0429649353,2.50564341325), test loss: 2.72249275148\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (19.4735412598,27.0509866915), test loss: 30.6309991121\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.79901194572,2.5043865705), test loss: 3.77315090299\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (18.3912658691,27.0248128329), test loss: 28.0840052128\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (5.57564735413,2.50306032129), test loss: 2.59409246743\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (29.878446579,27.0202513322), test loss: 30.8642138004\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (5.15201377869,2.50230962882), test loss: 3.64262919426\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (26.2417259216,27.0030606732), test loss: 30.2884006977\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.83966302872,2.50163747119), test loss: 3.32615864873\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (19.6948642731,26.9700313362), test loss: 32.5149778605\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.41894197464,2.50076015868), test loss: 3.28165739775\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (26.4656066895,26.9671884549), test loss: 32.5361573696\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.2846750021,2.499748819), test loss: 3.53631980419\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (28.9080848694,26.9501422972), test loss: 31.6398852229\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.82869434357,2.49826925862), test loss: 3.0272903502\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (17.4549465179,26.9368568342), test loss: 31.678162384\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.73244988918,2.49725210932), test loss: 3.79204452634\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (10.7463903427,26.9141879868), test loss: 27.6268746376\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.85535275936,2.49595492718), test loss: 2.52421143055\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (21.0489616394,26.9068647468), test loss: 33.2620271206\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.931411147118,2.49507694355), test loss: 3.6548248291\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (38.9342308044,26.8919471857), test loss: 32.63942976\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.49576640129,2.49461056942), test loss: 2.76970894337\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (31.6029262543,26.8646770066), test loss: 33.1515564442\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.42109918594,2.49364611798), test loss: 3.94582320154\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (31.5514450073,26.857601839), test loss: 28.8005038261\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.42591333389,2.49283986574), test loss: 2.66763988733\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (50.3465614319,26.8418642678), test loss: 31.228220129\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.29323673248,2.49161744999), test loss: 3.71132746339\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (21.9448623657,26.826801481), test loss: 29.9540171623\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.776939272881,2.49026260921), test loss: 3.05909635425\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (22.0458374023,26.8055437621), test loss: 35.4208938122\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.75443226099,2.48920637501), test loss: 3.58266712427\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (23.8828620911,26.7966305025), test loss: 33.7955949306\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.50317692757,2.48841146665), test loss: 3.69143764079\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (60.0704421997,26.7840058983), test loss: 41.8399033546\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (4.3718380928,2.48788903586), test loss: 2.90038355589\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (85.9227142334,26.7619563969), test loss: 38.9830922127\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (7.84149980545,2.48718536531), test loss: 3.76378533244\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (9.85019111633,26.7458176289), test loss: 31.8942617416\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.23268461227,2.48608360313), test loss: 2.62635386884\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (19.9390945435,26.7321153935), test loss: 30.5159940481\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.73888802528,2.48479437582), test loss: 3.7142508328\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (26.6964874268,26.7183355145), test loss: 31.2396065712\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.92453598976,2.48353184612), test loss: 2.79782715142\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (38.3817939758,26.6983012304), test loss: 31.165219593\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.50597763062,2.48276185843), test loss: 3.62159343362\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (25.2824783325,26.6885314839), test loss: 31.0795246124\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.692122340202,2.4816392251), test loss: 2.76202273816\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (26.4565830231,26.6755714213), test loss: 33.7169686794\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (0.319056272507,2.48101098643), test loss: 3.5412037313\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (14.2507419586,26.6551121532), test loss: 31.3931671619\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.2948102951,2.48053095617), test loss: 3.24228543043\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.1338529587,26.6389786608), test loss: 29.8104415894\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.15942931175,2.47929330762), test loss: 3.67845665216\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (3.83435559273,26.6244701054), test loss: 31.9486876965\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.56050038338,2.47822004276), test loss: 3.46610048413\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (15.7020130157,26.611932366), test loss: 30.596931982\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.61103439331,2.47704703963), test loss: 3.02723340392\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (29.5026893616,26.5930477514), test loss: 33.3071973801\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.13434386253,2.47612152362), test loss: 3.61137113571\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (26.2815799713,26.5822014479), test loss: 31.5942250729\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.16566944122,2.47509448682), test loss: 2.51808998883\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (21.9990692139,26.5683445661), test loss: 40.3929354668\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.631065368652,2.47435868141), test loss: 3.75941798091\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (43.8795852661,26.5497264107), test loss: 35.6996829987\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.44756770134,2.47381421964), test loss: 2.83277515471\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (23.4022960663,26.5334606811), test loss: 31.316335988\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.93598854542,2.47268620398), test loss: 3.73648982644\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (57.1899642944,26.5198394077), test loss: 30.2831202507\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.08156347275,2.47161564926), test loss: 2.73712645322\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (57.0267372131,26.507562441), test loss: 30.0888562202\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.12881231308,2.47066530277), test loss: 3.77281040549\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (55.5747032166,26.4899466774), test loss: 29.6094250679\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.55693340302,2.46957269107), test loss: 2.9406984359\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.0970096588,26.4760617509), test loss: 30.9459185362\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (7.34392023087,2.46885192722), test loss: 4.0251388371\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (40.7192840576,26.4649133376), test loss: 30.6478698015\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (4.82801151276,2.46794411523), test loss: 3.40356975496\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (23.2487335205,26.4457281247), test loss: 37.7788268089\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.18392395973,2.46723314978), test loss: 3.03608660698\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (12.6797657013,26.4300198877), test loss: 34.877688694\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.44806098938,2.46629135792), test loss: 3.64379707873\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (21.3720397949,26.4172184979), test loss: 31.4323314905\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.98726320267,2.46516436306), test loss: 2.70281404853\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (21.9265174866,26.4042383619), test loss: 33.9639332771\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.71241426468,2.46430115589), test loss: 3.66786732078\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (5.86389350891,26.3867925707), test loss: 29.2706704378\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.97756302357,2.46318564961), test loss: 2.94163608253\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (20.2005290985,26.3731566162), test loss: 34.0478415966\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.83027029037,2.46241749189), test loss: 3.81239233613\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (15.4395427704,26.362296767), test loss: 29.9482064724\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.8156170845,2.46158835794), test loss: 2.97791046053\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (30.2105751038,26.3434550533), test loss: 32.0440402031\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.04082512856,2.46087927274), test loss: 3.61006755531\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.4333744049,26.3271312551), test loss: 28.5783539772\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.62257099152,2.45990776608), test loss: 2.71581993401\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (28.3481731415,26.3152085319), test loss: 30.7328126669\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.03094005585,2.45883064205), test loss: 3.52617915869\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (32.0565719604,26.3019862086), test loss: 30.7072546482\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.988193929195,2.45792404068), test loss: 3.35058241189\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (15.1734771729,26.2852467368), test loss: 32.4658926249\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.29208755493,2.45686681426), test loss: 3.62208970934\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (27.5353622437,26.2720812329), test loss: 31.2820567131\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.73699629307,2.45610427023), test loss: 3.86924455762\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (3.14417552948,26.2597040528), test loss: 32.6725328445\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.899224758148,2.45524898845), test loss: 3.0509236902\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (31.3673477173,26.2437655009), test loss: 30.5226998806\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.18911099434,2.45454186801), test loss: 3.78409302235\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.2331771851,26.2267181867), test loss: 32.6194290161\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.14769029617,2.45365739396), test loss: 2.77371361554\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (22.5729370117,26.2154925185), test loss: 29.6942690849\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.17436122894,2.45258136297), test loss: 3.61808739901\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (26.9825305939,26.2024549679), test loss: 30.7830320358\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.33279538155,2.45161870734), test loss: 2.83559870124\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (11.5115375519,26.1864864298), test loss: 31.5230971098\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.72549915314,2.45071686376), test loss: 3.78584566712\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (17.924282074,26.1726855071), test loss: 32.8308134079\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.73403835297,2.4499309711), test loss: 2.99088279605\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (8.95263671875,26.1597798979), test loss: 30.1873780012\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.37979161739,2.44901563803), test loss: 3.7236969173\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.7511253357,26.1446496961), test loss: 32.1079946995\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.748920917511,2.44834378683), test loss: 3.32530489564\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (26.9131736755,26.1280316343), test loss: 31.1641640186\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.19152200222,2.44743644575), test loss: 3.77204664797\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (20.1738948822,26.116500954), test loss: 29.8800087452\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.40309429169,2.44641097684), test loss: 3.76026667356\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (34.2550086975,26.1039387811), test loss: 31.3269934177\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (4.90838241577,2.44556498492), test loss: 3.17809109688\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (39.1266288757,26.0878718418), test loss: 31.7700282097\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.92211127281,2.44456577588), test loss: 3.636539042\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (25.5261383057,26.0748955724), test loss: 32.7097380638\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.44522237778,2.44381154917), test loss: 2.60927397907\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.491484642,26.0614456649), test loss: 30.1906783581\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.25463724136,2.44290523455), test loss: 3.74576314688\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (43.9959411621,26.0470670058), test loss: 30.9774342537\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.96211397648,2.44220121546), test loss: 2.93369803429\n",
      "run time for single CV loop: 1311.0594101\n",
      "\n",
      "MC # 5, Hype # hyp1, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold1/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (49.2031669617,inf), test loss: 33.137855196\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (4.7640581131,inf), test loss: 2.91637392342\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (50.3300247192,26.3966970816), test loss: 36.9990182877\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.93450689316,2.51003970438), test loss: 3.87224809825\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (40.552734375,26.3071682291), test loss: 33.0049064636\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.56568288803,2.50378617468), test loss: 3.4559645772\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (12.8789978027,26.276677848), test loss: 31.5052859306\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (5.80589103699,2.49564863139), test loss: 3.70682136118\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (20.955078125,26.3504499956), test loss: 31.7354215622\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.26419043541,2.49160434118), test loss: 3.67495750785\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (16.4408035278,26.3197882455), test loss: 33.9792451382\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.11199593544,2.48537532636), test loss: 2.77433032393\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (34.2253227234,26.2853888018), test loss: 31.283632493\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.61440563202,2.48214449904), test loss: 3.81189951003\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (31.4353923798,26.2596833165), test loss: 30.4567903519\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.15689587593,2.48176068657), test loss: 2.79574449509\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (31.4581851959,26.2378742079), test loss: 32.0275361776\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.27805149555,2.47942260726), test loss: 3.74924257398\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (16.503446579,26.2218440538), test loss: 31.1034462452\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.9835896492,2.48055066969), test loss: 2.79790506959\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (26.1773376465,26.172918808), test loss: 32.8125079632\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.83478522301,2.47871662296), test loss: 3.65833474398\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (3.48323297501,26.1698273008), test loss: 30.0604608059\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.05713176727,2.47796695763), test loss: 2.71177084446\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (13.7390670776,26.1734141478), test loss: 31.7854717731\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.48595571518,2.47705329358), test loss: 3.71703820825\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (28.3761787415,26.1475877584), test loss: 30.9391924381\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.47411203384,2.47443118185), test loss: 3.83154422641\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (11.3410758972,26.1298463307), test loss: 33.4233714581\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.19846940041,2.47270486019), test loss: 3.00803836286\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (23.4621257782,26.0985929867), test loss: 34.5904021263\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.309902668,2.47167191554), test loss: 3.86135314703\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (35.9419517517,26.0895886385), test loss: 33.696698761\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.27227735519,2.47069108274), test loss: 2.9934812963\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (23.6843528748,26.0724178589), test loss: 32.0738538265\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.98961663246,2.47030787227), test loss: 3.80625548959\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (22.9435844421,26.0367019024), test loss: 32.6824361801\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.651748120785,2.46855618357), test loss: 2.98658362329\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (25.5312423706,26.0370939759), test loss: 31.8142739773\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.0588862896,2.46808050851), test loss: 3.8471671164\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (7.18567991257,26.0192631347), test loss: 31.2533447742\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.25453472137,2.46657698668), test loss: 3.11255117655\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (22.3083839417,26.0023909751), test loss: 30.3793665886\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.45172059536,2.46461648916), test loss: 3.6546824038\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (62.8611755371,25.9885542352), test loss: 32.1109091043\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.99623537064,2.46374898525), test loss: 3.44727173448\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (26.0769920349,25.9583284737), test loss: 29.6640952587\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.35894608498,2.46238334601), test loss: 4.11369472742\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (11.6830234528,25.9520760786), test loss: 30.5798566103\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.71884965897,2.46173599313), test loss: 3.80300007463\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (45.283870697,25.9295050443), test loss: 32.8688438416\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.7997123003,2.46078099927), test loss: 3.02754037976\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (29.0521392822,25.8999594906), test loss: 34.1350977182\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.66961574554,2.45945535336), test loss: 3.8034457624\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (42.36170578,25.8957393336), test loss: 39.6021777153\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.89120447636,2.45886647107), test loss: 2.69579824209\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (17.6290435791,25.8765273512), test loss: 29.5625125408\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.53681492805,2.45716997047), test loss: 3.98922359347\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (20.9874229431,25.8641149832), test loss: 31.9458120346\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.69245648384,2.45593489289), test loss: 2.96669468284\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.7458095551,25.8401909418), test loss: 31.2843879461\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.90929603577,2.45465837296), test loss: 3.87034479678\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (7.8899064064,25.8203248427), test loss: 30.5703257084\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (5.87878036499,2.45350149614), test loss: 3.05784037411\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (9.14700126648,25.8155348027), test loss: 30.0497385025\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.840711832047,2.45297371756), test loss: 3.80154224634\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (33.9183807373,25.7879777973), test loss: 30.9701285601\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.37604594231,2.45185117218), test loss: 3.67749491036\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (21.7192993164,25.7665404406), test loss: 32.2414838791\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.88075685501,2.45088340223), test loss: 3.74191140234\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (6.96067905426,25.7581440891), test loss: 31.3873277903\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.94902467728,2.4498284488), test loss: 3.68192703426\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (18.5012149811,25.7403801782), test loss: 39.5776714444\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (0.998304009438,2.44841105199), test loss: 2.73894793391\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (21.6680431366,25.7312080905), test loss: 31.6763392448\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.60435843468,2.44740526267), test loss: 4.29865709543\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (34.2793426514,25.7035276323), test loss: 29.841710186\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.49156522751,2.44577819363), test loss: 2.71002719402\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.3059215546,25.6910952032), test loss: 31.7432548285\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.49104118347,2.44491749692), test loss: 3.90407931805\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (39.8913345337,25.6792070226), test loss: 33.8588840008\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.99381875992,2.44428211091), test loss: 2.85734010935\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (53.9837493896,25.6540395715), test loss: 30.0132606268\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.947588503361,2.44315161827), test loss: 3.54803917408\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (17.8682041168,25.6383302298), test loss: 30.4223201275\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.18812358379,2.44239749948), test loss: 3.02560115457\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (15.4401388168,25.6222142518), test loss: 28.5537197828\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.74602818489,2.44072891379), test loss: 3.80021935701\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (32.6785049438,25.6087875454), test loss: 37.1684702396\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (6.19851398468,2.44005856229), test loss: 3.67594482899\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (40.7622375488,25.5956356159), test loss: 31.9972023487\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.16117191315,2.43857868834), test loss: 3.18111863136\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (13.3184118271,25.5702606022), test loss: 35.2307257652\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.64807319641,2.4372950726), test loss: 3.74101171494\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (21.989566803,25.5611438398), test loss: 36.1520852566\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.19599485397,2.4365652626), test loss: 2.56246133447\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (65.4124679565,25.5428126091), test loss: 45.7031939507\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.1225476265,2.43560958965), test loss: 4.15396129489\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (37.6140060425,25.5215580888), test loss: 36.0240921021\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.11920022964,2.43476856922), test loss: 2.98982815593\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (12.7450885773,25.5075148248), test loss: 30.6703056574\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.53767466545,2.43392061934), test loss: 3.75228110254\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (16.1391029358,25.4901617009), test loss: 32.9796686172\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.47071933746,2.4322882994), test loss: 3.11761347353\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (15.0712184906,25.4800951479), test loss: 29.9017151117\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.01042759418,2.4313907681), test loss: 3.63315408826\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (11.0733690262,25.4617248833), test loss: 33.8778877258\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.64780139923,2.43006874654), test loss: 3.85312748551\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (14.8044471741,25.4423699151), test loss: 29.8258642673\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.82131266594,2.42900822289), test loss: 3.83238490522\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (8.28587341309,25.4318963385), test loss: 30.9780826092\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.81212449074,2.42819672626), test loss: 3.77165573835\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (11.8719301224,25.4102486822), test loss: 36.8909841537\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.59946966171,2.4272214963), test loss: 2.86911281049\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (20.7616882324,25.3953355462), test loss: 36.208896637\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (4.54381561279,2.42656682806), test loss: 3.87950603962\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.3744163513,25.3791042555), test loss: 33.0054902077\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.48180508614,2.42543754698), test loss: 2.79219817817\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (23.8025150299,25.3632329418), test loss: 29.0794301748\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.99360775948,2.42414123505), test loss: 3.6438480705\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (22.1155433655,25.3530135118), test loss: 32.1673601151\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.25631999969,2.42325711356), test loss: 2.91823335588\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (57.6363105774,25.3325390517), test loss: 35.4285382271\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.42160844803,2.42177795392), test loss: 3.92581849098\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (30.5716018677,25.3171639943), test loss: 37.0758965015\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.18948149681,2.4208858667), test loss: 3.27951467633\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (10.7975616455,25.3021365415), test loss: 31.4032340527\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.78017544746,2.4198622812), test loss: 3.77138568163\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (18.5379600525,25.2822459489), test loss: 30.6896834373\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.708077907562,2.41892323768), test loss: 3.75122610927\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (15.2520275116,25.2680820807), test loss: 34.4103653908\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.392346769571,2.41819189995), test loss: 3.56267613173\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (10.2955245972,25.2517293595), test loss: 34.0346059799\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.33659505844,2.41708862535), test loss: 3.72331813574\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (10.1255083084,25.2384522719), test loss: 34.6801859379\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.76136922836,2.41611592138), test loss: 2.75537643433\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (22.4594955444,25.2265924841), test loss: 32.0787575722\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.757118701935,2.41506244856), test loss: 3.87753690481\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (12.1826725006,25.2060189288), test loss: 31.3091749191\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.63698244095,2.41369467083), test loss: 2.92681585401\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (20.5966796875,25.1931279788), test loss: 31.1049925804\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.54540908337,2.41294721849), test loss: 3.9921797514\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (20.6647834778,25.1766935064), test loss: 33.4163751602\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.157231599092,2.41181069213), test loss: 3.16015561819\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (13.2454910278,25.1594262033), test loss: 29.1596528769\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.0775551796,2.41104992004), test loss: 3.61456582844\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.0966682434,25.1443848415), test loss: 33.6933339596\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.64767479897,2.41013751359), test loss: 3.53750596642\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (24.656539917,25.1286793341), test loss: 29.4256910324\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.05135059357,2.40901777798), test loss: 3.84403227568\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (25.2405948639,25.1175401057), test loss: 30.7295179844\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.95018315315,2.40818434073), test loss: 4.1069827497\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (36.3747215271,25.1012675152), test loss: 32.228921175\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.28560972214,2.40699012944), test loss: 3.29731900692\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.9070796967,25.0837583731), test loss: 34.0762781143\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.12023544312,2.40580553612), test loss: 3.85889926553\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.5417785645,25.0696285896), test loss: 35.489579916\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.15996551514,2.40498905024), test loss: 2.90972327888\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (43.816192627,25.0530478023), test loss: 31.295847249\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.71618533134,2.4038748852), test loss: 3.88455888629\n",
      "\n",
      "MC # 5, Hype # hyp1, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold2/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (44.0128173828,inf), test loss: 31.3332336426\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.9364657402,inf), test loss: 2.98541865945\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (61.00756073,26.6183863378), test loss: 32.4443751335\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.35208845139,2.38909256834), test loss: 3.07514880896\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (85.3891525269,26.5947731118), test loss: 46.8784573555\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (7.99277114868,2.39475665647), test loss: 3.01167207956\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (22.5322418213,26.5256866272), test loss: 30.7207003832\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.39781904221,2.38181015543), test loss: 3.13815757483\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (32.9948234558,26.546622784), test loss: 33.0024311066\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.10151338577,2.3780948704), test loss: 3.03521421552\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (34.67603302,26.5202777364), test loss: 32.0604005337\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.38425540924,2.37494505942), test loss: 2.97121106386\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (41.5822677612,26.5278455475), test loss: 34.5031119347\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.39311027527,2.37514767896), test loss: 3.2024397552\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (18.4834823608,26.4658015274), test loss: 31.5715849638\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.63959550858,2.37204880737), test loss: 2.81144720316\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (18.2428665161,26.4729243011), test loss: 41.5036693096\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.75324904919,2.37341242149), test loss: 3.21955999732\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (26.817445755,26.4630781749), test loss: 36.8380353928\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.00079584122,2.3727771578), test loss: 3.05691193193\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (48.111618042,26.4467544621), test loss: 29.517029357\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.75115132332,2.37356431338), test loss: 2.9050188601\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (20.2590522766,26.4037379909), test loss: 32.0133268833\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.03576087952,2.37105806812), test loss: 2.9970173955\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (34.8186225891,26.4078106451), test loss: 32.8509465218\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (7.64464187622,2.36955758282), test loss: 3.18753265142\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (28.3148097992,26.383400407), test loss: 35.107639432\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.78989768028,2.36798604365), test loss: 2.97850461155\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (17.8711700439,26.3784495117), test loss: 33.8302146912\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.17441034317,2.36728052865), test loss: 2.97554411888\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (68.3627624512,26.3574597218), test loss: 35.695095253\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.06726741791,2.36523408347), test loss: 3.18167120814\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (29.419626236,26.3392224376), test loss: 32.6066157341\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.07106542587,2.3648716297), test loss: 2.81890235841\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (6.90653657913,26.3248014844), test loss: 32.0026822567\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.60769283772,2.36436211849), test loss: 3.55836559534\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (16.2747383118,26.3098578823), test loss: 31.6115634441\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.41923809052,2.36421826295), test loss: 3.00885885209\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (52.914226532,26.2840509461), test loss: 33.2309863567\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.83322167397,2.36276905111), test loss: 3.18537448049\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (30.0710678101,26.2774797533), test loss: 34.4050507069\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.29359650612,2.36167764836), test loss: 2.95357089341\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (22.3425521851,26.2554772225), test loss: 31.7618938923\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.48538923264,2.36000129721), test loss: 3.28016629219\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (27.3563556671,26.247888133), test loss: 34.1316241026\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.875962018967,2.35908896415), test loss: 2.99800730646\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (30.3105888367,26.2268075186), test loss: 33.4752620697\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.03830289841,2.35751090359), test loss: 3.07952726781\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (57.8298835754,26.2117834518), test loss: 32.279263401\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.99244844913,2.35697512879), test loss: 3.10341700017\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (8.36296081543,26.192760423), test loss: 35.4855023384\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.09548199177,2.35604271483), test loss: 2.98367482424\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (26.5423126221,26.1803590987), test loss: 32.754750514\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.79688489437,2.355746518), test loss: 3.23503088653\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (37.8083724976,26.1565530367), test loss: 34.9832543373\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.20781183243,2.35500419556), test loss: 3.02938428074\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (22.3793106079,26.1458469864), test loss: 29.9200315237\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (5.28025722504,2.35408912965), test loss: 3.04200007319\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (23.6659679413,26.1286921428), test loss: 31.4663905621\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.03297543526,2.35220212484), test loss: 2.999957636\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (26.2761917114,26.1178184256), test loss: 30.7154490471\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.17627763748,2.35145857005), test loss: 3.02842389941\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (56.7385025024,26.0999947967), test loss: 32.7328843594\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.51997375488,2.35024944144), test loss: 2.89452836066\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (22.7891960144,26.0821910954), test loss: 30.4749544382\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.28881549835,2.3494120062), test loss: 3.10867958069\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (25.5427742004,26.0659268982), test loss: 44.6607172966\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.54318523407,2.34852551744), test loss: 3.0722231254\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (15.7405281067,26.0568746028), test loss: 32.4410488605\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.488349199295,2.34813186238), test loss: 2.97069401443\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (25.8850593567,26.0351885769), test loss: 34.1116904259\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.56569385529,2.34721042174), test loss: 3.22734316587\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (16.4846744537,26.0179594164), test loss: 31.448092103\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.65357232094,2.34634576403), test loss: 2.7963866204\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (34.2404823303,26.0047664453), test loss: 31.6940897942\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (5.78685379028,2.34471212754), test loss: 3.17357417941\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (22.4937877655,25.9912079348), test loss: 32.8152955055\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.24671435356,2.34399985101), test loss: 2.9806243822\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (18.5004653931,25.9746168129), test loss: 30.5556955814\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.694107174873,2.3428495554), test loss: 2.8708391428\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (19.4975471497,25.960763537), test loss: 32.207337141\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.996882915497,2.34194491888), test loss: 3.10241256654\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (16.8075141907,25.9412783819), test loss: 36.1382914543\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.50384759903,2.3408215338), test loss: 3.1838321209\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (3.55662798882,25.9315386958), test loss: 32.6195626497\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.97135007381,2.34047478685), test loss: 3.3071816355\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (36.0946235657,25.9126983963), test loss: 31.8526626587\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.68741750717,2.33955230288), test loss: 3.0303262651\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (39.6271209717,25.8974103424), test loss: 34.0321611643\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.67677497864,2.33878550248), test loss: 3.23205977082\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (43.5348510742,25.8829773123), test loss: 32.4986348867\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.787749528885,2.33745955337), test loss: 2.78432133794\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (30.244846344,25.8691103186), test loss: 32.9908584118\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.754484951496,2.33662037681), test loss: 3.38265506029\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (21.224647522,25.8530691735), test loss: 30.6187882185\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.681592822075,2.33536090662), test loss: 2.98539402783\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (10.5849676132,25.8399335056), test loss: 32.3586514235\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.47435903549,2.33452243147), test loss: 3.19140642285\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (49.1619338989,25.8220018723), test loss: 34.5194161415\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.19080615044,2.33347708556), test loss: 3.12600896358\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (15.0965881348,25.8093173834), test loss: 34.190454793\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.12353079766,2.33285877322), test loss: 3.4070384264\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (15.7454586029,25.792112496), test loss: 33.4081749439\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.51295113564,2.33200583902), test loss: 2.99188117385\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (30.1919193268,25.7769832864), test loss: 33.3319687843\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.02541863918,2.33139186597), test loss: 3.21637160182\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (11.1245079041,25.7618492674), test loss: 31.6736742973\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.4774851799,2.33032123472), test loss: 3.02931063622\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (28.2773666382,25.7491459804), test loss: 33.0757871866\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.3732585907,2.3292959148), test loss: 2.86130192131\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (16.5797424316,25.7334553762), test loss: 33.6252218723\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.17534422874,2.32811300424), test loss: 3.1904907763\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (44.9442710876,25.7208492396), test loss: 31.089055109\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.86979961395,2.32739968806), test loss: 3.01985008121\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (7.57392835617,25.702190443), test loss: 30.3038214684\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (4.50976085663,2.32629382936), test loss: 2.99585232735\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (18.5664100647,25.689999484), test loss: 35.2376413345\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.871772289276,2.32570090996), test loss: 3.00404422283\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (30.1171531677,25.6761937974), test loss: 29.3851857424\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.478829681873,2.32489255032), test loss: 3.00338732004\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (15.3219852448,25.660879745), test loss: 40.2306712627\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.1040687561,2.32427832451), test loss: 2.91931301504\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (20.9283714294,25.6429926891), test loss: 31.0359360695\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.28472697735,2.32317543181), test loss: 3.12721250057\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.6626386642,25.6316364632), test loss: 32.1909085751\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.892658472061,2.32201959497), test loss: 3.19937958121\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (16.5193576813,25.6157884706), test loss: 39.0450865269\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.52394032478,2.3211513562), test loss: 2.93909023702\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.6193962097,25.6040382941), test loss: 33.8471401215\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.730473875999,2.32040911942), test loss: 3.20075468421\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (45.6548080444,25.5872026945), test loss: 34.6058780193\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.80065536499,2.3192619962), test loss: 2.78209200799\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (14.1002998352,25.5734748759), test loss: 35.6539885521\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.60776996613,2.31852285103), test loss: 3.16291725636\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (3.83643054962,25.5601429222), test loss: 30.9324668884\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.989225685596,2.31777627616), test loss: 3.21331622601\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (40.4463768005,25.5456213707), test loss: 28.4618634701\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.74161207676,2.31709714339), test loss: 2.86392810941\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (30.5462036133,25.5281347483), test loss: 32.4425734282\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.33000183105,2.31606749266), test loss: 3.0318571806\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (38.350944519,25.5168508392), test loss: 31.3731404781\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.44721865654,2.31523656362), test loss: 3.20063916445\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (49.7380523682,25.5013488491), test loss: 34.2308929443\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.01084899902,2.31417713252), test loss: 3.15155774951\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (32.880027771,25.489438593), test loss: 33.9037663937\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.21904373169,2.31333495258), test loss: 3.03896142542\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (14.7649412155,25.4737582686), test loss: 33.9418678284\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.60198402405,2.31224839387), test loss: 3.19309061766\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (27.0703296661,25.4592283831), test loss: 33.4221152067\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.49302124977,2.31151963615), test loss: 2.77161713243\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.8255844116,25.4453927549), test loss: 33.3193080902\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.305893421173,2.31068120974), test loss: 3.35650243759\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (28.3646850586,25.4318792147), test loss: 32.7293150425\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.905948162079,2.31002791757), test loss: 3.07706316859\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.0689744949,25.415081812), test loss: 31.2807930946\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.411531865597,2.3091496039), test loss: 3.45837251544\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (7.04962825775,25.4030733663), test loss: 31.9844157219\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.29094946384,2.30830747423), test loss: 2.97564570904\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (33.8198699951,25.3887416398), test loss: 31.3368419647\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.88635993004,2.30720639451), test loss: 3.16426945329\n",
      "\n",
      "MC # 5, Hype # hyp1, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold3/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (30.8301944733,inf), test loss: 31.4312988758\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.79914665222,inf), test loss: 2.85671952069\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (68.3170318604,26.3558796329), test loss: 32.42628901\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.56693029404,2.5193216812), test loss: 3.5513748616\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (18.5030498505,26.2352584605), test loss: 29.4419835091\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (0.559547305107,2.51441409779), test loss: 3.68597565293\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (14.7900104523,26.2849852068), test loss: 32.7502295017\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.66130828857,2.50587069982), test loss: 3.86271196604\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (59.984287262,26.290797279), test loss: 28.3504923344\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.83656859398,2.5026844527), test loss: 3.70600279272\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (26.7325611115,26.2893167355), test loss: 32.8262337446\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.03670740128,2.4994358714), test loss: 3.12990228236\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (15.1818752289,26.2549202708), test loss: 30.3290854454\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.65769124031,2.49507461498), test loss: 3.77036312819\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (38.0281219482,26.2381027984), test loss: 35.2765934229\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.29871320724,2.49581881442), test loss: 2.91066093445\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (13.5370092392,26.2155649611), test loss: 31.9979132175\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.248535618186,2.49280269279), test loss: 3.85070987344\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (19.7302875519,26.1912245377), test loss: 29.7441840649\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.569439291954,2.49254808775), test loss: 2.84222296476\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (18.155921936,26.1689638), test loss: 33.6979660988\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.862221241,2.49310164301), test loss: 3.59672984779\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (39.3059158325,26.1684307102), test loss: 29.3014888763\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.21047830582,2.49111164958), test loss: 2.67666190863\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (9.62090873718,26.147867961), test loss: 34.5339019299\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.63896989822,2.48977246554), test loss: 3.71182408333\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (35.3641738892,26.1462846459), test loss: 28.8801117897\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.17620491982,2.4890514457), test loss: 3.42844625115\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (14.2020149231,26.1176706888), test loss: 35.5607199669\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.04539704323,2.48622700363), test loss: 3.79032319188\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (26.6031589508,26.1027421241), test loss: 30.1781166077\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.16560506821,2.48581023129), test loss: 3.86841934919\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (38.8649215698,26.090458705), test loss: 34.5102766991\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.35857582092,2.48482920057), test loss: 3.21721771061\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (41.1372299194,26.0649637041), test loss: 30.9982709408\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.75244307518,2.48362480468), test loss: 3.81207257509\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (29.4802398682,26.0478177187), test loss: 30.9095891953\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.16217756271,2.48300048954), test loss: 2.77660748363\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (42.2862510681,26.0393791146), test loss: 31.4107409954\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (0.535169482231,2.48208323603), test loss: 3.75141988695\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (13.1906776428,26.0184292253), test loss: 29.3775977612\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.59184837341,2.48039123516), test loss: 3.11060232818\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (13.7086954117,26.0114949451), test loss: 32.729513979\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (7.03351593018,2.47947296406), test loss: 3.57192545533\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (51.5520591736,25.9948892381), test loss: 27.3845933199\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.15802264214,2.47795695182), test loss: 3.10751357973\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (19.8876457214,25.9748865282), test loss: 33.9987294197\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.6969537735,2.4768375941), test loss: 4.09245547056\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (9.90188598633,25.9603886222), test loss: 29.0201310158\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.19526267052,2.4758967796), test loss: 3.78362308145\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (14.1516933441,25.9433364413), test loss: 32.7087387562\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.20148611069,2.47537865157), test loss: 3.15835019052\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (27.8760223389,25.9227181424), test loss: 32.2134218693\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.12799787521,2.47405661551), test loss: 3.69513459802\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (48.4267120361,25.9142756554), test loss: 40.8738372803\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.77044081688,2.47325537823), test loss: 2.71257140636\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (18.7499542236,25.9006888813), test loss: 30.2809945583\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.77817416191,2.47194306586), test loss: 3.77733970881\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (38.591835022,25.8886525564), test loss: 31.5205679893\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.94257581234,2.47056082763), test loss: 2.82297922075\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (48.2135238647,25.8715773039), test loss: 33.4242634058\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.61268043518,2.46970858469), test loss: 3.48179627359\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (17.6731987,25.8555913114), test loss: 31.095673418\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.31343221664,2.46878152621), test loss: 2.7867169112\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (4.57542276382,25.8417343204), test loss: 32.334119916\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.43065536022,2.46754366108), test loss: 3.82098554373\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (6.41619205475,25.8265716146), test loss: 28.6815264702\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (0.436231404543,2.46712902902), test loss: 3.46389060616\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (37.4143028259,25.806127578), test loss: 33.455482173\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.17677164078,2.4659389846), test loss: 3.79071802497\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (6.4367017746,25.7945061327), test loss: 30.0383056641\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (6.44117355347,2.46489934348), test loss: 3.81698780656\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (16.3980846405,25.7825449616), test loss: 39.6151012421\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.53340649605,2.4639002275), test loss: 2.85316030979\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (15.2526931763,25.7681176663), test loss: 29.6935795307\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.52427506447,2.46254002809), test loss: 3.84933636189\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (36.0424766541,25.7522686616), test loss: 35.2680099964\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.49854755402,2.46140298316), test loss: 2.83952512443\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (26.6433238983,25.7378539096), test loss: 31.8131721497\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (5.66943264008,2.46074006788), test loss: 3.92495265007\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (26.3576278687,25.7240850001), test loss: 30.6128281116\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.0674161911,2.45956623211), test loss: 3.11057597995\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (23.4996376038,25.7071278448), test loss: 32.3805979252\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.38027572632,2.45889775576), test loss: 3.57931792438\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (19.2565422058,25.6883817301), test loss: 27.6418988228\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.742506980896,2.45794152453), test loss: 2.76511675715\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (38.7700271606,25.6770696843), test loss: 33.1897591352\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (6.1235909462,2.45658754738), test loss: 3.75619587302\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (13.2635803223,25.6646769438), test loss: 33.8387984037\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.05663847923,2.45577455859), test loss: 3.64412657022\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (13.6638641357,25.6509299596), test loss: 33.4577443838\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.34127044678,2.45466116956), test loss: 3.12586150169\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (18.41497612,25.6323664862), test loss: 31.1159895897\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.91579532623,2.45334578709), test loss: 3.66606958807\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (41.1386489868,25.6201607137), test loss: 34.2786889553\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.10649394989,2.45248568213), test loss: 2.85702240169\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.9980430603,25.6071105507), test loss: 35.0914324284\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.961212694645,2.45166052393), test loss: 3.88190703988\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (24.8597679138,25.5898307672), test loss: 34.148748827\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.01963019371,2.45088653496), test loss: 2.93710201383\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (28.5010738373,25.5741529598), test loss: 32.6222545624\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.23548007011,2.44997800847), test loss: 3.64173987508\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (22.4431858063,25.5619200665), test loss: 29.3509019375\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.809082150459,2.44866675544), test loss: 2.96387701333\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (25.8956794739,25.5491153185), test loss: 31.0633096933\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (7.41201162338,2.4478800055), test loss: 3.56411035657\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (37.7334747314,25.5359464411), test loss: 30.5374252558\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.25570178032,2.4467190015), test loss: 3.64664088488\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (14.0941028595,25.5183761009), test loss: 32.1840481997\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.75279927254,2.44565757608), test loss: 3.89270153642\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (19.0954170227,25.5055857302), test loss: 32.8110561371\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.17220747471,2.44460296944), test loss: 3.75072555542\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (70.1272277832,25.493571792), test loss: 43.5039130688\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.62838888168,2.4440089291), test loss: 3.37945721447\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (40.3194122314,25.4761419463), test loss: 35.7438491821\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.821610808372,2.44317925856), test loss: 3.75135689974\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (7.65052986145,25.4598224617), test loss: 35.0718250751\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.45716905594,2.44212709156), test loss: 2.85034108758\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.9406356812,25.4485315297), test loss: 30.6082172394\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.28751885891,2.44104440484), test loss: 3.67785943449\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (16.159368515,25.434890843), test loss: 29.9982258797\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.761099636555,2.43993029621), test loss: 2.89474795163\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (7.06128740311,25.4206399402), test loss: 35.2853065968\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.9521522522,2.43890186931), test loss: 3.64598164558\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.0197191238,25.4056224579), test loss: 28.8446875572\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.90938949585,2.43803878907), test loss: 2.73078184426\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (29.4567146301,25.3924988304), test loss: 32.9001419306\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.37053799629,2.43692617815), test loss: 3.71309323609\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (20.2034606934,25.3768081213), test loss: 28.6858163357\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.83577275276,2.43615766848), test loss: 3.61540360451\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (27.2020759583,25.3631836646), test loss: 39.1906555176\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (4.86018610001,2.43562945605), test loss: 3.90293653607\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (20.2633934021,25.3474361526), test loss: 28.5682706833\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (4.98492288589,2.43437724424), test loss: 3.75839082748\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (26.1524085999,25.3358725485), test loss: 36.3230585814\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.22428035736,2.43336559824), test loss: 2.98448451161\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (30.0077190399,25.3231368928), test loss: 31.2185371161\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.73957967758,2.43246360098), test loss: 3.93054640889\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (41.6027565002,25.3087473289), test loss: 35.0161063671\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.16157197952,2.43119305925), test loss: 2.85642002523\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (12.346414566,25.2939697637), test loss: 30.7642911434\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.36070084572,2.43034567697), test loss: 3.89493559599\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (47.0641441345,25.2809215077), test loss: 29.7044101238\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (4.71279239655,2.42943253077), test loss: 2.87832462788\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (61.0216560364,25.2657265666), test loss: 31.4602888107\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (4.72196912766,2.42853273451), test loss: 3.4994666785\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (36.6297950745,25.2512924312), test loss: 29.0210474014\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.38228654861,2.42780344555), test loss: 3.3996829927\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (10.9280614853,25.2379891775), test loss: 32.6059302807\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.43445920944,2.42681932554), test loss: 3.89554623961\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (29.2086372375,25.225376064), test loss: 29.5248450279\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.49230027199,2.42578824865), test loss: 3.7450416103\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (15.130531311,25.2132808906), test loss: 33.310843277\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.862825632095,2.4249004507), test loss: 3.15785160959\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (13.0221452713,25.1981305691), test loss: 31.9431036949\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.61212992668,2.42377392026), test loss: 3.71677715182\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (33.6569366455,25.1847077352), test loss: 34.821609211\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.13412833214,2.42289555558), test loss: 2.84351498485\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (19.195602417,25.1709532022), test loss: 29.2284303665\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.586266756058,2.42196198426), test loss: 3.74159191251\n",
      "\n",
      "MC # 5, Hype # hyp1, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold4/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (31.2674293518,inf), test loss: 35.7361453533\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.77088451385,inf), test loss: 3.25157595277\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (29.0837135315,26.1501421824), test loss: 32.5978348255\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.6877399683,2.45665708187), test loss: 3.69518114924\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (12.1709833145,25.9890670781), test loss: 29.5790627956\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (0.380037873983,2.45410705192), test loss: 3.69293638468\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (4.74269485474,26.0159366643), test loss: 34.5255650997\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.95587277412,2.44460114595), test loss: 3.94731096327\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (20.8343505859,26.022051569), test loss: 32.4736011028\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.1987156868,2.44085255186), test loss: 3.96238833666\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (25.6258239746,26.0450093313), test loss: 36.9344994068\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.36878156662,2.44102162268), test loss: 2.97170988321\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (59.794380188,26.0163149389), test loss: 32.0176697731\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.46046757698,2.43914106069), test loss: 4.09075916409\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (11.942076683,25.9801472004), test loss: 32.3598118782\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.35328292847,2.43999076813), test loss: 2.89014055133\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (15.4174261093,25.9695584172), test loss: 33.1047176361\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.13522911072,2.43965983324), test loss: 3.83766855597\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (19.8996562958,25.9357804364), test loss: 34.1122612\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.24782848358,2.4390052385), test loss: 3.00717011094\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (74.7240142822,25.9037179802), test loss: 46.8662765026\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (7.21495246887,2.43992087763), test loss: 3.82799627036\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (13.9737672806,25.8881307721), test loss: 29.3778355122\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.23080039024,2.43863790523), test loss: 2.82336526811\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (10.338558197,25.8824108399), test loss: 33.0371526957\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.48523199558,2.4362829399), test loss: 3.82430667877\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (8.63285064697,25.8715947239), test loss: 31.5157508373\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.689735591412,2.43455006936), test loss: 3.78503569365\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (8.18196201324,25.8393798404), test loss: 34.708439064\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.1796875,2.43160890242), test loss: 3.33400886059\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (22.5819778442,25.8203968631), test loss: 35.1356561661\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.20013999939,2.43057105959), test loss: 3.81531808078\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (23.2302055359,25.8174461944), test loss: 39.6204608917\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.09994459152,2.43057406285), test loss: 2.83690366447\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (79.985168457,25.8143521408), test loss: 39.6291500092\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.27595329285,2.43097314619), test loss: 4.14174231291\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (44.4601325989,25.7936877609), test loss: 44.9223572731\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.16155040264,2.43077488261), test loss: 3.11486086398\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (9.17479038239,25.7704642545), test loss: 32.6633191824\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.39920091629,2.42962316955), test loss: 3.87246363163\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (26.8499259949,25.7580445824), test loss: 32.9135896683\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.829568982124,2.4274519134), test loss: 3.03625867069\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (36.5268478394,25.7429445114), test loss: 31.9403467894\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (5.48279094696,2.42676660311), test loss: 3.79921622276\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (64.1967697144,25.7250127023), test loss: 29.5681571007\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.85964035988,2.42525910351), test loss: 3.65857032537\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (24.3033752441,25.6993878042), test loss: 34.5256700516\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (4.22871303558,2.4244232933), test loss: 3.91389987767\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (22.0982074738,25.6869251714), test loss: 33.9840410709\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.89279842377,2.42357923085), test loss: 3.88082309961\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (21.0539093018,25.6752213789), test loss: 42.5729640961\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.645989060402,2.42238091949), test loss: 3.04523640871\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (20.7633209229,25.6564819287), test loss: 38.8131109238\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.07055878639,2.42174326051), test loss: 4.04277813435\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (25.653131485,25.6425024703), test loss: 33.7936011553\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.30242347717,2.42089203665), test loss: 3.11044606566\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (18.6711769104,25.6348383636), test loss: 31.5607249975\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.94895100594,2.4194998266), test loss: 3.89840207994\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (14.8202724457,25.6257398167), test loss: 38.5607647419\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.03837347031,2.41911370153), test loss: 2.96751545966\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (35.0685119629,25.610405733), test loss: 32.7977370024\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (6.53887939453,2.41773546104), test loss: 3.93502079248\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (29.2136096954,25.5849782441), test loss: 30.4837840557\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.80165505409,2.41635917144), test loss: 3.19692046642\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (11.7555036545,25.5698529254), test loss: 33.4179797173\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.47212028503,2.41566832181), test loss: 3.9435710907\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (35.0254821777,25.5578188127), test loss: 29.1494696617\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.0339884758,2.41526778673), test loss: 3.95308858752\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (17.578338623,25.5392509639), test loss: 34.6330883503\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.58190178871,2.41454310306), test loss: 3.37212847471\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (4.03804636002,25.5216463037), test loss: 33.1474839687\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.11420118809,2.41342435583), test loss: 3.97066016495\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (7.30250120163,25.5085562945), test loss: 38.9767167807\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (7.01087999344,2.41214545151), test loss: 2.8407191962\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (12.3530540466,25.4980911678), test loss: 37.054447937\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.592007637024,2.41068066157), test loss: 3.85510033369\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (19.9105243683,25.4872373253), test loss: 36.6998473167\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.80068635941,2.40972530091), test loss: 3.2776884079\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (35.6159248352,25.4692126779), test loss: 33.1798223019\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (5.9143447876,2.40888578637), test loss: 3.99005191028\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (7.70270729065,25.4585408055), test loss: 30.9041742802\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (5.69648838043,2.408308232), test loss: 3.07543119192\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (5.46797943115,25.4455947531), test loss: 32.1165530205\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.47359228134,2.40744600375), test loss: 3.89165772796\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (11.228843689,25.4260765797), test loss: 29.5321075916\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.810607552528,2.40653148202), test loss: 3.78720125258\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (22.9725131989,25.4078493339), test loss: 34.7535661221\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.16696810722,2.40568402582), test loss: 3.94271973372\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (8.61595058441,25.3950298198), test loss: 29.8088466644\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.35272157192,2.40487457121), test loss: 3.90716042966\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (10.3603630066,25.3835593264), test loss: 39.1585937977\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.22407770157,2.40371746282), test loss: 2.96060146689\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (33.6204299927,25.3702170458), test loss: 33.7843950748\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.74777722359,2.40250046467), test loss: 3.98282186985\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (23.4403076172,25.3504423009), test loss: 33.8583895206\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.936509072781,2.40124545962), test loss: 3.07184651494\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (23.3585929871,25.3374080835), test loss: 34.4471149445\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.03443431854,2.40040445957), test loss: 4.01380002499\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (11.9709444046,25.3279109786), test loss: 33.1866831779\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.64267849922,2.3996249539), test loss: 3.15061022341\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (16.0663871765,25.3156656122), test loss: 32.5295889378\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.2388677597,2.39905011935), test loss: 3.68174471855\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (36.8824195862,25.2992470656), test loss: 33.4379848957\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.46798944473,2.39815629304), test loss: 3.6202685684\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (31.7100868225,25.2878483377), test loss: 41.6530165672\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.75606870651,2.39729237324), test loss: 4.06061105728\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (7.91438102722,25.2735910486), test loss: 28.9810313702\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.22566413879,2.39616351469), test loss: 4.33357286155\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (18.2919464111,25.2590194429), test loss: 35.5306278229\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.55738830566,2.39502049894), test loss: 3.27062515169\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (18.3448047638,25.2416650524), test loss: 34.6699341297\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.465506255627,2.39408001005), test loss: 3.90664913058\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (15.4336576462,25.2267000967), test loss: 40.305068469\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.79874372482,2.39342375361), test loss: 3.20292016268\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (33.7626075745,25.2142918979), test loss: 30.049488616\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.18966007233,2.39247094847), test loss: 3.92808853388\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (35.2537536621,25.1991349837), test loss: 35.8697936773\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.53585839272,2.39146434729), test loss: 3.12122734189\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (16.8497238159,25.1843896283), test loss: 33.1985014439\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.10609769821,2.39051840622), test loss: 3.88757544756\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (30.7576351166,25.1760185528), test loss: 31.3774358273\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.22282981873,2.38960369536), test loss: 3.06003260911\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (10.2058763504,25.1624803978), test loss: 32.4793519258\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.47191333771,2.3887897568), test loss: 3.89191316962\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (15.3816394806,25.1522022502), test loss: 28.3314118385\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.70537114143,2.38791664886), test loss: 3.91859316826\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (10.8192300797,25.1347276473), test loss: 38.8589277744\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.63812160492,2.38676647449), test loss: 3.91116485\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (25.3417949677,25.1193497974), test loss: 31.8241335869\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.79818892479,2.38596099638), test loss: 3.97507045418\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (23.4243679047,25.1047452042), test loss: 37.1231054306\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.145827621222,2.38495274693), test loss: 3.31068485677\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (21.808177948,25.0903490869), test loss: 31.7420187473\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.873485088348,2.38430720331), test loss: 4.01692736149\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (13.0793476105,25.0746146889), test loss: 34.9595731974\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.17238283157,2.3835194616), test loss: 3.197010988\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (3.96653532982,25.063924849), test loss: 31.4792399645\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.90142011642,2.38252302221), test loss: 3.96514303088\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (55.163734436,25.0492775232), test loss: 36.6260341167\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.49568998814,2.38136445432), test loss: 3.16065793037\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (38.6597709656,25.0390763156), test loss: 31.717542243\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.099588871,2.38034109083), test loss: 3.69189422727\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.195980072,25.0247428698), test loss: 29.9631248951\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.5845618248,2.37933043862), test loss: 3.58678171635\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (25.3513259888,25.0120175857), test loss: 34.2090891123\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.18416011333,2.37865598997), test loss: 4.14108333588\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (14.68663311,25.0007521304), test loss: 29.9756507874\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.13500976562,2.37780184314), test loss: 4.25615257621\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (19.3696403503,24.9862718338), test loss: 35.3315657616\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.51425755024,2.3769820873), test loss: 3.31834797859\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (24.694568634,24.9691591125), test loss: 32.1906890869\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.39806866646,2.37614624523), test loss: 4.20515348911\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (8.5685634613,24.9571269816), test loss: 36.8161521673\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.79938483238,2.37521975957), test loss: 2.93973568082\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (23.6591377258,24.9440688036), test loss: 30.1015602112\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.41165876389,2.37433742564), test loss: 3.89034383297\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (13.7281827927,24.9315911843), test loss: 34.2802474976\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.68885207176,2.37334420921), test loss: 3.07854494452\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (23.9793338776,24.9163532147), test loss: 32.2292670965\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.91402673721,2.37230492687), test loss: 3.83648670316\n",
      "\n",
      "MC # 5, Hype # hyp1, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold5/train_snaps/Exp13_MC_hyp1_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (40.6873321533,inf), test loss: 35.5042615891\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.43860960007,inf), test loss: 3.1669487834\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (27.0876846313,26.7562986488), test loss: 31.400221014\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.466955602169,2.39836133492), test loss: 3.67077190578\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (3.9184718132,26.664161063), test loss: 36.0741741657\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.77977859974,2.39518223332), test loss: 3.69027570486\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (49.6847915649,26.7553676085), test loss: 32.8622943401\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.91571444273,2.38852471283), test loss: 3.99902616739\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (28.7362689972,26.7267240888), test loss: 34.1414399624\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.74339902401,2.38555826094), test loss: 4.1959752202\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (46.3537864685,26.7464200891), test loss: 35.50462327\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.74482131004,2.38234300689), test loss: 3.17612363994\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (43.1379356384,26.6939403896), test loss: 35.822293663\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (5.88256216049,2.38102764505), test loss: 3.87642467618\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (15.870677948,26.6977945713), test loss: 36.5097806215\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.85515737534,2.37995088208), test loss: 2.97427990437\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (18.0580711365,26.6979330033), test loss: 40.9619565487\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.647700965405,2.37998919775), test loss: 4.09361153841\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (24.4163074493,26.6591783675), test loss: 45.418820858\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.961570262909,2.38038458124), test loss: 3.16258991361\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (10.9336023331,26.6271842107), test loss: 31.1577325583\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.74207997322,2.3802353196), test loss: 3.81617123187\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (47.1112747192,26.6260136359), test loss: 33.9770835876\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.7261865139,2.37683424361), test loss: 3.11943377852\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (25.5404701233,26.6136875324), test loss: 31.7671503067\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.8091211319,2.37592036004), test loss: 3.62061921954\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (37.4388847351,26.5903335848), test loss: 34.5380200863\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.76268064976,2.37327311933), test loss: 3.86056230068\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (41.5205879211,26.5758356323), test loss: 32.7767905593\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.36930847168,2.37271469821), test loss: 4.20661019087\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (28.5822277069,26.5631262155), test loss: 34.3733208895\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.822419166565,2.37161296777), test loss: 4.24934181571\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (42.9151153564,26.5426806805), test loss: 34.5058386326\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.57386064529,2.37181558235), test loss: 3.21531293392\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (64.9558944702,26.5196708216), test loss: 34.1384944201\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.72932219505,2.37151218957), test loss: 3.94510935545\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (48.1554069519,26.5068770673), test loss: 45.6743457317\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.80015039444,2.37037333615), test loss: 2.92007318437\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (25.4348068237,26.4885431409), test loss: 33.6680947781\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.42745244503,2.36892237239), test loss: 3.87602053881\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (21.9062747955,26.4765763263), test loss: 35.1630818844\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.18568658829,2.36736939382), test loss: 3.32251962721\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (35.1588973999,26.4496699543), test loss: 35.2743597507\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.56713294983,2.36595710412), test loss: 3.8690120101\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (30.6214084625,26.439831159), test loss: 33.4114425182\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.17482447624,2.36493094911), test loss: 3.10864019692\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (38.2046508789,26.4294869182), test loss: 31.6905165672\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.379019111395,2.36467096482), test loss: 3.62859407067\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (14.5864257812,26.4047120429), test loss: 35.1830123901\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.4025349617,2.36440703818), test loss: 3.86076558232\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (16.0121746063,26.3878130582), test loss: 32.7555139065\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.73261702061,2.36346792228), test loss: 4.03433033228\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (24.2602157593,26.3749588328), test loss: 33.8264779091\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.81976413727,2.36210558635), test loss: 4.12613201141\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (11.4218463898,26.3594809951), test loss: 34.8340604782\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.907059073448,2.36097895088), test loss: 3.12667620331\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (48.0972175598,26.337468616), test loss: 34.3357893467\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.49156987667,2.35933048007), test loss: 3.97046598792\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (27.1557998657,26.3214951107), test loss: 36.5730915546\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.37828278542,2.35855334582), test loss: 2.99610173404\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (35.7632446289,26.307856365), test loss: 31.4282851219\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.08553767204,2.35753228624), test loss: 3.96290749311\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (40.2497634888,26.2880341535), test loss: 33.902907896\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.00507307053,2.3570578875), test loss: 3.1050966382\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (29.9116516113,26.2697777198), test loss: 32.569238472\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.49689984322,2.35642982266), test loss: 3.82342290878\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (7.98305034637,26.257707138), test loss: 36.7666679382\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (5.84393644333,2.35563737017), test loss: 3.11399569958\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (14.7704591751,26.2416894258), test loss: 38.3000919819\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.828802227974,2.35462336213), test loss: 3.58567226529\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (18.0568084717,26.2281460503), test loss: 34.1105593204\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.27908658981,2.35318456732), test loss: 3.86129551977\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (10.7657289505,26.2038041187), test loss: 33.0820304632\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (4.62843704224,2.35206345053), test loss: 3.99031886458\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (17.8817520142,26.1904492078), test loss: 40.1696917057\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.42085003853,2.35092929811), test loss: 4.05868427157\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (52.7303466797,26.1755225813), test loss: 40.3344150543\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.07760226727,2.35036230636), test loss: 3.03483891487\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (101.694915771,26.1574167131), test loss: 40.2711898804\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.66536521912,2.34981765209), test loss: 4.0182726562\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (8.18719673157,26.1425716402), test loss: 36.4589391232\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.31761741638,2.34879537807), test loss: 2.91287185848\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (42.594127655,26.1296459639), test loss: 30.5136293411\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.39102780819,2.34780931158), test loss: 3.97268275023\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (30.71900177,26.1146744902), test loss: 35.2847554684\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.53433585167,2.34701957899), test loss: 3.02268643677\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (12.1718711853,26.0946418813), test loss: 33.6997672081\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.17967605591,2.34562983716), test loss: 3.8541923821\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (12.5088424683,26.0784424476), test loss: 36.2055937767\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.76095795631,2.34470996747), test loss: 3.2651376605\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (12.9989023209,26.0626582381), test loss: 31.0184011936\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.02296853065,2.34371957194), test loss: 3.78315781951\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (14.5253257751,26.045157248), test loss: 33.9100077629\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.330591022968,2.34310564749), test loss: 3.79793811738\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (19.8536987305,26.0275417561), test loss: 32.3611130476\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.619496166706,2.34233205922), test loss: 3.97603855431\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.9017963409,26.0152376467), test loss: 33.150655508\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.8265273571,2.34118321139), test loss: 4.09275016785\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (24.9653320312,26.0019397764), test loss: 37.5623453617\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (8.43546676636,2.34070376617), test loss: 2.92821829915\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (42.0643501282,25.9869204639), test loss: 36.343204689\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.08913111687,2.33934984646), test loss: 3.96619309187\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (16.3200492859,25.9679640849), test loss: 35.6467634678\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.26072907448,2.33835938706), test loss: 2.92582067847\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (25.0812339783,25.9528179163), test loss: 35.0016367912\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.04272460938,2.33743927297), test loss: 3.90869297981\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (46.2259597778,25.9357212407), test loss: 40.2797333717\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (5.8031539917,2.33670395971), test loss: 3.18707442284\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (35.5798912048,25.9170391209), test loss: 32.5527457237\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.69894790649,2.33594350293), test loss: 4.0933916539\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (4.96890974045,25.9054715229), test loss: 33.7429206848\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.44665908813,2.33486131731), test loss: 3.08555825055\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (27.4131298065,25.8907323323), test loss: 31.9920953274\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.2348074913,2.33398779162), test loss: 3.83701203465\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.3061065674,25.8771831191), test loss: 34.3569041252\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (6.04349660873,2.33313688044), test loss: 3.88655822575\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (21.7704620361,25.8595858785), test loss: 33.6960902214\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.15480327606,2.332080506), test loss: 4.02014170587\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (16.5671367645,25.8440445661), test loss: 41.0972582817\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.05042648315,2.3311597429), test loss: 4.0655626297\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (5.00242042542,25.829792396), test loss: 35.0353936434\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.556732535362,2.33023152765), test loss: 3.02399128377\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (25.209897995,25.8117804484), test loss: 35.6442288637\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.6965072155,2.32950772895), test loss: 4.08465912938\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (19.024646759,25.7944572358), test loss: 34.8608522177\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.3742518425,2.32872355619), test loss: 2.94855732322\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.6672296524,25.7822839906), test loss: 34.0621052265\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.2114136219,2.3275439041), test loss: 3.97454279065\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (15.2878313065,25.7680783842), test loss: 34.6697495461\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.837219834328,2.32666801734), test loss: 3.08521860242\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (35.6658859253,25.7540439629), test loss: 32.0284193039\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.61184871197,2.32564162715), test loss: 3.86476084888\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.3661193848,25.7380396398), test loss: 33.4312955856\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.39113664627,2.32478824646), test loss: 3.14366362691\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (38.7941131592,25.723116588), test loss: 31.7667453051\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (5.1322054863,2.32395737127), test loss: 3.74924080372\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (25.5950183868,25.7059001278), test loss: 33.9505589008\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.495143532753,2.32315803934), test loss: 3.94456492662\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (3.83938884735,25.6883401713), test loss: 34.511471796\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.56891965866,2.32237814262), test loss: 3.94415557384\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (49.8417663574,25.6758590795), test loss: 32.8563559055\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.85958981514,2.32137609943), test loss: 4.17644203305\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (26.784992218,25.6609651747), test loss: 35.0485254765\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.73717176914,2.3205057487), test loss: 3.1011562705\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (36.9665985107,25.6477879003), test loss: 36.9465272665\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.4673037529,2.31950617425), test loss: 3.99911947846\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (39.4660720825,25.6309792332), test loss: 33.5194657326\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (5.14888858795,2.31864375625), test loss: 2.95943498015\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (15.2457103729,25.6172957721), test loss: 34.9033782959\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (4.43890190125,2.31776485875), test loss: 4.09691218734\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (17.0188903809,25.6034091556), test loss: 50.0999549866\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.512129426003,2.31699792045), test loss: 3.40605760813\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (20.8437213898,25.5860701523), test loss: 35.548484993\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.858957767487,2.31626389176), test loss: 3.92463994622\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.8416337967,25.5690915233), test loss: 33.9490169525\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.55856752396,2.31550958032), test loss: 3.04008612037\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (35.5354957581,25.5566329757), test loss: 31.3010429382\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.68440234661,2.31435375386), test loss: 3.81970883012\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (24.1740989685,25.5428948114), test loss: 33.4161104679\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (4.73774528503,2.31352450688), test loss: 4.04073677659\n",
      "run time for single CV loop: 1275.26998496\n",
      "\n",
      "MC # 5, Hype # hyp2, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold1/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (52.6310424805,inf), test loss: 32.3251278877\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (4.33051013947,inf), test loss: 2.83224381506\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (50.5407485962,27.2073343935), test loss: 37.0313508511\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.89524126053,2.57137726319), test loss: 3.83014236391\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (39.057182312,27.1276789286), test loss: 31.9586948872\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.06752634048,2.56713755235), test loss: 3.3174197793\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (13.0606546402,27.1132249026), test loss: 31.2807832956\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (6.15974664688,2.55982213092), test loss: 3.66902971864\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.8536148071,27.1671515167), test loss: 31.2535524368\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.21409761906,2.55514735273), test loss: 3.57898255289\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (17.558921814,27.1528820246), test loss: 33.554882288\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.00742483139,2.55023514388), test loss: 2.76967268437\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (34.2387657166,27.1145755095), test loss: 31.2969879866\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.47169327736,2.54706260958), test loss: 3.71255235076\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (33.8423995972,27.0894426927), test loss: 29.9242925167\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.46610951424,2.54687171704), test loss: 2.73911406696\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (33.9656906128,27.0725558873), test loss: 32.1380563498\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.2954518795,2.54447057297), test loss: 3.70263614953\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (16.5621128082,27.0499772712), test loss: 30.7159890175\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.10327458382,2.54544823936), test loss: 2.73957742453\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (28.1420440674,27.006332342), test loss: 32.9643642902\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.917491436,2.54409772701), test loss: 3.56833020449\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (3.48067140579,27.0005498166), test loss: 29.6236242294\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.07839894295,2.54314783495), test loss: 2.63344840705\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (13.8826780319,27.0003449889), test loss: 31.5555670738\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.52898359299,2.54228603525), test loss: 3.73473884463\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (26.2455482483,26.9793218859), test loss: 30.497234726\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.38679647446,2.54010076802), test loss: 3.66039716601\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (12.2351703644,26.9572799757), test loss: 32.7184206486\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.25122117996,2.53849423654), test loss: 3.06283755004\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (24.6107978821,26.9288910603), test loss: 34.1895753622\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.09417700768,2.53768193064), test loss: 3.70985239744\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (32.4892234802,26.9183920037), test loss: 33.4585378408\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.76267671585,2.53665909186), test loss: 2.93170831501\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (22.2751197815,26.8990049189), test loss: 32.1522113323\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.23372125626,2.53639463947), test loss: 3.67906262279\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (25.8209323883,26.8662330905), test loss: 32.1091233253\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.814887285233,2.5348965731), test loss: 2.87650101781\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (23.7194862366,26.8629965619), test loss: 31.7147854328\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (0.995477318764,2.53446888704), test loss: 3.74123077989\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (8.18597126007,26.8462758687), test loss: 30.7919806004\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.93614768982,2.53311911627), test loss: 3.06457728148\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (24.068775177,26.8295655593), test loss: 30.6642565727\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.48392438889,2.53138820006), test loss: 3.6088914454\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (63.1999359131,26.8140855177), test loss: 31.5886547327\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.77570986748,2.53076775245), test loss: 3.36884983778\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (24.5897598267,26.7859361668), test loss: 29.6802474499\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.27641820908,2.52957839083), test loss: 3.99463999867\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (11.9753742218,26.7772232586), test loss: 30.1901787758\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.86520063877,2.5288463479), test loss: 3.68350133598\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (44.3271217346,26.755669219), test loss: 32.437376976\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.90250778198,2.52808764261), test loss: 3.05831678212\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (27.4728984833,26.7259641255), test loss: 34.2913630486\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.67657721043,2.5268053356), test loss: 3.6429910779\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (47.0129776001,26.7200182525), test loss: 39.207861042\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.43074512482,2.52631778873), test loss: 2.68639057577\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (17.9874191284,26.7024561711), test loss: 29.5117939472\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.99163222313,2.52467807343), test loss: 3.88109219074\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (21.3988952637,26.6883168114), test loss: 31.3484546185\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.55022883415,2.52353637911), test loss: 2.8936167419\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (21.6052799225,26.6646907905), test loss: 31.5621662617\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.32510614395,2.52252483225), test loss: 3.80021032095\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (7.41855859756,26.6458096515), test loss: 30.0710438251\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (6.3753619194,2.52132386072), test loss: 2.92306003273\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (9.86217021942,26.6386836264), test loss: 30.1753128052\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.929350078106,2.52078051721), test loss: 3.75590175986\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (35.222240448,26.6128596058), test loss: 30.3875886679\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.65805745125,2.51978035785), test loss: 3.58485135436\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (22.1944026947,26.5899157169), test loss: 32.319894886\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.01703429222,2.51882119138), test loss: 3.68129540086\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (8.86453819275,26.5805668076), test loss: 30.6889255524\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (5.34577560425,2.51784637354), test loss: 3.55526041389\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (20.465467453,26.5639200284), test loss: 39.0720351219\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.03881502151,2.51648879721), test loss: 2.72532647848\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (21.6080131531,26.5531397287), test loss: 31.7911817551\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.76272845268,2.51562390633), test loss: 4.1290289402\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (38.5026473999,26.5265825197), test loss: 28.9445278168\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.80829715729,2.51414811317), test loss: 2.65615697503\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (14.5037727356,26.5135147374), test loss: 31.7773496151\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.45581698418,2.51323936211), test loss: 3.75642983317\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (41.8123092651,26.500718797), test loss: 34.1777719021\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (5.01954555511,2.51268305153), test loss: 2.76662884951\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (51.962928772,26.4763923272), test loss: 30.6158227444\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.04800987244,2.51164388729), test loss: 3.51268872917\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (17.3398456573,26.4588352042), test loss: 29.5774600744\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.37728095055,2.51094096968), test loss: 2.90147618651\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (14.2163658142,26.4431165178), test loss: 28.7753787279\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.86117386818,2.50938744892), test loss: 3.74730409384\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (35.3116912842,26.4291620273), test loss: 36.0132658482\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (6.0351061821,2.50873147419), test loss: 3.5693969667\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (42.4055709839,26.4155133728), test loss: 31.8128108978\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.10960102081,2.50746025648), test loss: 3.17473242283\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (14.1796875,26.3907229596), test loss: 35.1065749645\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.70514321327,2.50623696626), test loss: 3.6098246932\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (21.6943740845,26.380374873), test loss: 35.7077778816\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.27195739746,2.50552061185), test loss: 2.56024990976\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (64.0622558594,26.3624169469), test loss: 44.5588140965\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.06335449219,2.50466219285), test loss: 3.9659514904\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (40.4330787659,26.34099261), test loss: 36.3619737148\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.4264986515,2.50386209743), test loss: 2.90530918241\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (14.3580713272,26.3257977756), test loss: 30.5957295418\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.74220335484,2.50309907668), test loss: 3.65698147416\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (18.8212165833,26.3088073639), test loss: 32.519204855\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.57152748108,2.50155356778), test loss: 2.93535633385\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (15.1708211899,26.2972972517), test loss: 30.238267827\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.99595451355,2.50070184071), test loss: 3.59770395756\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (11.1889467239,26.2795961155), test loss: 33.1695211887\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.40502738953,2.49953202241), test loss: 3.70608491749\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (14.4453887939,26.260027133), test loss: 29.8263471127\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.95356011391,2.49850942586), test loss: 3.7978267014\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (8.69820308685,26.2486583667), test loss: 30.4931733608\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.83280813694,2.49773365556), test loss: 3.63299521208\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (11.1823825836,26.2276699652), test loss: 36.2311723232\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.86681246758,2.49685314374), test loss: 2.83253510594\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.4546432495,26.2117271318), test loss: 35.6931909561\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (4.41959810257,2.4962158765), test loss: 3.73351178765\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.7641401291,26.1955096408), test loss: 32.6448945999\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.35186934471,2.49520749847), test loss: 2.75173040628\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (23.2923240662,26.1791268842), test loss: 29.0110655069\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.27241277695,2.49394814931), test loss: 3.54370257854\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.4474258423,26.1681428387), test loss: 31.6349236965\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.15426635742,2.49313196528), test loss: 2.86858644187\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (59.4720954895,26.1486705419), test loss: 34.8641255379\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.70122122765,2.49179352416), test loss: 3.89153593779\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (31.9623374939,26.1322741199), test loss: 34.5262922287\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.18049621582,2.49090824935), test loss: 3.08310470581\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (11.3974666595,26.1170648979), test loss: 31.5241024971\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.00939846039,2.48995139661), test loss: 3.76591418386\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (20.2810630798,26.0974350268), test loss: 30.2281573772\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.930259585381,2.48904650421), test loss: 3.62036899924\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (16.8963623047,26.0821151975), test loss: 34.3096194267\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.403740465641,2.4883803949), test loss: 3.50907953978\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (12.5781402588,26.0664219011), test loss: 33.6555600643\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.32602238655,2.48736685925), test loss: 3.59138782024\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (9.7865524292,26.0519454818), test loss: 33.9130135059\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.03710055351,2.48639153891), test loss: 2.74356496632\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (22.7110977173,26.0398590966), test loss: 32.030155611\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.755525887012,2.48545524247), test loss: 3.74825029969\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (13.2411146164,26.01976214), test loss: 30.5878632545\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.09225320816,2.48417948306), test loss: 2.87707923055\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (19.9465370178,26.0058206525), test loss: 31.0898689985\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.51276183128,2.48344384131), test loss: 3.86394222677\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (21.5705795288,25.9897673074), test loss: 33.063467741\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.273799359798,2.48237676807), test loss: 3.15167148113\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.1031532288,25.9721176148), test loss: 29.4183328629\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.38229990005,2.4816370671), test loss: 3.54072770476\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.656870842,25.9564312858), test loss: 33.3407320976\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.26576709747,2.4808061099), test loss: 3.36013056636\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (26.418844223,25.9411754622), test loss: 29.505590868\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.30335712433,2.47975438872), test loss: 3.79908704758\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (24.3183441162,25.9288239986), test loss: 29.9853568077\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.00429272652,2.47891195973), test loss: 3.99848954976\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (38.5955810547,25.9131465932), test loss: 32.1491457939\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.38162136078,2.477851845), test loss: 3.27091282606\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.4437198639,25.8952014704), test loss: 33.6323379993\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.21161270142,2.47672110263), test loss: 3.75205962062\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (18.8809776306,25.8805276256), test loss: 34.6524909496\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.23665094376,2.47597731282), test loss: 2.86883734167\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (44.3483047485,25.8644440788), test loss: 30.9004347086\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.8328243494,2.4749300276), test loss: 3.76959485412\n",
      "\n",
      "MC # 5, Hype # hyp2, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold2/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (44.6673736572,inf), test loss: 30.7900631905\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.57578611374,inf), test loss: 2.84857406616\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (59.0733299255,27.8905926716), test loss: 34.424354434\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.3560500145,2.46825449443), test loss: 3.11636763811\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (92.7389984131,27.887819682), test loss: 41.4824913979\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (7.51786136627,2.47370996714), test loss: 2.80996361375\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (25.5123214722,27.8042209225), test loss: 30.3900714874\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.48621964455,2.4621834321), test loss: 3.12786118686\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (33.9751052856,27.8225729979), test loss: 32.2963644981\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.01444911957,2.45760855582), test loss: 2.88350930661\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (34.5207977295,27.7905665147), test loss: 31.2334187031\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.94290351868,2.45512850086), test loss: 2.97815768123\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (43.0733909607,27.8083858149), test loss: 35.0472828865\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.3690237999,2.455676601), test loss: 3.06096961498\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (19.1507873535,27.7430629121), test loss: 30.5195198536\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.52086472511,2.45339997477), test loss: 2.76606308669\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (18.7398796082,27.7563298824), test loss: 42.4021528721\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.12347507477,2.45437367564), test loss: 3.16926507354\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (30.8820858002,27.7442099478), test loss: 35.2041257381\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.00389230251,2.45365632084), test loss: 2.91692389399\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (44.5986061096,27.7328350328), test loss: 30.3898464918\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.84604763985,2.4545520271), test loss: 2.9128341496\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (21.4578437805,27.6870366438), test loss: 31.9444741249\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.08581960201,2.45229448217), test loss: 2.84621905088\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (34.8259429932,27.6908146142), test loss: 32.2442516804\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.88264656067,2.45057910239), test loss: 3.18839663863\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (27.4328231812,27.6643641407), test loss: 33.8810659647\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.4078514576,2.4493791183), test loss: 2.83719426543\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (19.0729217529,27.6616089492), test loss: 33.2180706024\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.23609876633,2.44883036054), test loss: 2.98666291833\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (73.719619751,27.6407243625), test loss: 34.9664142609\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.99572908878,2.44712321933), test loss: 3.12906899452\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (28.6789493561,27.6245896505), test loss: 31.8385164976\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.78365564346,2.44681893203), test loss: 2.75363547504\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (9.61997890472,27.6088149748), test loss: 32.2253358841\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.02493357658,2.44625435951), test loss: 3.37769782841\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (14.9073562622,27.5966126885), test loss: 30.6491998196\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.27540016174,2.44617168644), test loss: 2.89515646547\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (54.0027542114,27.5693116948), test loss: 34.168759346\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.0289068222,2.44492209411), test loss: 3.15373401642\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (29.430103302,27.5642029791), test loss: 32.8358092308\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.2331931591,2.44385834643), test loss: 2.81275342107\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (25.424041748,27.5399053963), test loss: 32.055221796\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.69430685043,2.442382237), test loss: 3.28163009584\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (27.5301513672,27.5341664401), test loss: 33.4765889168\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.923280239105,2.4416220923), test loss: 2.85347799361\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (31.185710907,27.5130336172), test loss: 32.9173743248\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.0366358757,2.44022604201), test loss: 3.1025339216\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (59.4192733765,27.4994803967), test loss: 32.3438390255\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.42623376846,2.43980703197), test loss: 2.94571831822\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (8.45243835449,27.4795192062), test loss: 35.2441296101\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.29067707062,2.43892393971), test loss: 3.01638701558\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (23.9662055969,27.4692799966), test loss: 33.0197693825\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.83658981323,2.43868628958), test loss: 3.11951516569\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (34.9372711182,27.4442171127), test loss: 32.2667909622\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (4.60207080841,2.43803991047), test loss: 2.90300358981\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (25.9325942993,27.4354882665), test loss: 30.3456869602\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (5.59697628021,2.43727161996), test loss: 2.98582360744\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (23.2100067139,27.4156712147), test loss: 30.1680883408\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.09539580345,2.43546913092), test loss: 2.85876515508\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (27.6869754791,27.4062123788), test loss: 30.964106369\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.94884824753,2.43482494666), test loss: 3.04341036677\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (60.3226547241,27.3886216006), test loss: 32.2539877892\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.79158449173,2.43383362073), test loss: 2.74512605667\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (24.5908470154,27.3717821569), test loss: 30.2328890324\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.30146980286,2.43313360535), test loss: 3.10255904198\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (27.1460113525,27.3551903292), test loss: 45.7623355865\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.91594541073,2.43231073152), test loss: 3.01392699182\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (14.5935506821,27.3470813112), test loss: 31.8450824738\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.465577185154,2.43191973485), test loss: 2.93727285862\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (28.5250072479,27.3256420252), test loss: 34.5004343987\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.60279035568,2.43113971394), test loss: 3.09472519755\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (16.9842510223,27.3056548892), test loss: 30.6053659439\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.81548786163,2.43038990107), test loss: 2.73313575387\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (35.8274307251,27.2862468318), test loss: 31.4988380909\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (5.17240715027,2.4288710359), test loss: 3.09673514962\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (23.2974891663,27.2681020627), test loss: 31.4513206482\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.74186050892,2.4283056437), test loss: 2.86997686923\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (19.1440830231,27.2465369238), test loss: 31.2530893803\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.809799611568,2.42733560669), test loss: 2.89227229953\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (21.6628665924,27.2291979787), test loss: 31.4682155609\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.08237278461,2.42656247251), test loss: 2.99722613394\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (20.0073604584,27.2052481315), test loss: 35.8838442802\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.79129457474,2.42559950527), test loss: 3.20695484281\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (4.01789140701,27.1918150021), test loss: 31.9641604424\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.27040719986,2.42523334531), test loss: 3.10390762389\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (34.1730003357,27.1692080646), test loss: 31.4709396362\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.63154959679,2.42442169944), test loss: 3.02579991221\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (43.0305862427,27.1503775932), test loss: 34.3502294064\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.10664653778,2.42376495264), test loss: 3.0805752039\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (42.5065765381,27.1318841652), test loss: 31.8472074509\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.793831765652,2.42256379898), test loss: 2.72878501415\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (32.8968734741,27.1145371849), test loss: 32.617297554\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.802081823349,2.42177762421), test loss: 3.23106884956\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (20.729434967,27.0948340523), test loss: 29.5747200489\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.833226025105,2.42069990138), test loss: 2.88915586174\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (11.3868293762,27.0788962013), test loss: 32.6562668085\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.77622461319,2.41992841613), test loss: 3.17104908824\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (50.5155830383,27.0577659241), test loss: 32.8126560211\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.56751227379,2.41903757717), test loss: 2.91637253165\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (16.0491104126,27.0425993962), test loss: 34.4314516306\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.251295059919,2.41843993811), test loss: 3.47786020637\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (16.8702659607,27.022661154), test loss: 33.0109383106\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.89762866497,2.41768458034), test loss: 2.84689332843\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (29.8313751221,27.0049748409), test loss: 32.6859273434\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.989842414856,2.41715118413), test loss: 3.15062181056\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (11.7890205383,26.9869044839), test loss: 31.5082004547\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.7704410553,2.41620428125), test loss: 2.92204023898\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (28.9057998657,26.9714460903), test loss: 32.1544369936\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.64074254036,2.41520691907), test loss: 2.8262673974\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (16.6806697845,26.9529073458), test loss: 33.9419062138\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.16358184814,2.41415648813), test loss: 3.0756457746\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (48.9688453674,26.9384548489), test loss: 29.8377608299\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (4.43625879288,2.41353710979), test loss: 2.91578662395\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (9.65977191925,26.9171457818), test loss: 30.6066983223\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (5.58343935013,2.41259034795), test loss: 2.94168370962\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (19.1243972778,26.9033342433), test loss: 34.8825377464\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.876778900623,2.41200635239), test loss: 2.89526156783\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (25.9159317017,26.8870832729), test loss: 30.1909560919\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.588729858398,2.41126792465), test loss: 3.01838351488\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.6378211975,26.8704818795), test loss: 38.6059040546\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.04990720749,2.41073098961), test loss: 2.76326707751\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (23.2632675171,26.8499601377), test loss: 30.4393956661\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.43750309944,2.40973195729), test loss: 3.09132758677\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.6384220123,26.8366334616), test loss: 31.6175587654\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.01886129379,2.40862118628), test loss: 2.97100047618\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (17.2535610199,26.8184805616), test loss: 38.5915394306\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.07122182846,2.40786864498), test loss: 2.95128785372\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.5523948669,26.8050981629), test loss: 34.9047715664\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.7161013484,2.40721402422), test loss: 3.05455565453\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (48.0661849976,26.786333168), test loss: 33.132071352\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.41305303574,2.40618863049), test loss: 2.73854897022\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (16.2526016235,26.7712970841), test loss: 35.996070528\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.95980358124,2.40553220842), test loss: 3.0869093895\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (3.10937643051,26.7558601483), test loss: 30.0301193237\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.07143759727,2.4048053837), test loss: 3.00881825089\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (38.2144203186,26.7403405451), test loss: 29.0379290104\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.85046267509,2.40420968015), test loss: 2.88230092525\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (29.8863983154,26.7207950939), test loss: 32.110436821\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.43701601028,2.40326795377), test loss: 2.90568686724\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (38.8718032837,26.7082473888), test loss: 31.4969334602\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.56356537342,2.40250391991), test loss: 3.17770605683\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (50.2016525269,26.6905323112), test loss: 32.4537722111\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.33601117134,2.40151796421), test loss: 3.00037117004\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (32.4832229614,26.6774369975), test loss: 33.4153558731\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.43109107018,2.40074769089), test loss: 3.01211740077\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.5788536072,26.6600364905), test loss: 34.5754387379\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.94328665733,2.39971979723), test loss: 3.07405603528\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (28.681219101,26.6443530153), test loss: 32.9653553247\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.55033791065,2.39902954007), test loss: 2.71378466785\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.8645343781,26.6287706563), test loss: 34.0865272522\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.355138748884,2.39821969905), test loss: 3.30638467669\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (33.1847038269,26.6145091384), test loss: 32.613391304\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.24020957947,2.39758655916), test loss: 2.98786337376\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (12.1317892075,26.5960196798), test loss: 31.5288666725\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.497347056866,2.39675573508), test loss: 3.26006615758\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (8.04341888428,26.583038996), test loss: 31.5326528072\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.40693020821,2.39594117279), test loss: 2.84398298264\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (32.5674057007,26.5667133125), test loss: 30.6837245941\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.30085158348,2.3948740569), test loss: 3.19235437512\n",
      "\n",
      "MC # 5, Hype # hyp2, Fold # 3\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold3/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (31.4293384552,inf), test loss: 31.9953292847\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (3.12375855446,inf), test loss: 2.78510652781\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (67.2869720459,27.2341879725), test loss: 33.3882216454\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.53148603439,2.57877211887), test loss: 3.48634422421\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (20.449848175,27.0756330597), test loss: 29.0299953699\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (0.572929084301,2.57539900613), test loss: 3.48355167508\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (17.4165058136,27.1247975146), test loss: 32.6928327322\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.91166400909,2.56654225445), test loss: 3.80169737935\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (61.887550354,27.1271949328), test loss: 27.9712821245\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.53961539268,2.56355484697), test loss: 3.5600255698\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (27.1771621704,27.1155581197), test loss: 32.4443656445\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.03725719452,2.56118718711), test loss: 3.14881652892\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (16.2743339539,27.0876652927), test loss: 30.7457312107\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.87530064583,2.557276351), test loss: 3.63622211218\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (40.7302246094,27.0721222991), test loss: 34.4180846214\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.79965436459,2.55856680727), test loss: 2.82220892906\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (14.6010951996,27.042104557), test loss: 32.4132207155\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.396861851215,2.55537404656), test loss: 3.81057527363\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (21.2692718506,27.0226429446), test loss: 29.0279589176\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.57869631052,2.55519518093), test loss: 2.78883100152\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (18.9689064026,26.9995078773), test loss: 34.1137887955\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.49035811424,2.55607988777), test loss: 3.50890294909\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (43.5768203735,26.9941014774), test loss: 28.889432621\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.25772428513,2.55401699098), test loss: 2.56393850744\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (9.0214099884,26.9737078451), test loss: 34.4323846579\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.47472405434,2.55274893591), test loss: 3.71492869854\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (32.4444084167,26.9713719047), test loss: 28.5297232151\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.25623178482,2.55249835518), test loss: 3.2510065943\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (15.2589645386,26.9409998423), test loss: 35.0135937214\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.08961725235,2.54987512497), test loss: 3.75841462016\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (25.4011001587,26.9279206984), test loss: 29.6401480675\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.98334693909,2.54961915144), test loss: 3.66518713236\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (37.805480957,26.9137273201), test loss: 34.3892968655\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.78156757355,2.54875761609), test loss: 3.16697728783\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (40.5972900391,26.8873349894), test loss: 31.476074791\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.02420926094,2.54764158175), test loss: 3.66907840967\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (31.1182250977,26.871149927), test loss: 30.6874171257\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.39781808853,2.54718077995), test loss: 2.70876071453\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (40.9338493347,26.8602026562), test loss: 31.7325459003\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (0.455040067434,2.54636390137), test loss: 3.66513912082\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (15.2338180542,26.8378840517), test loss: 28.6612452984\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.37732362747,2.54476911784), test loss: 3.0287040174\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (14.5706958771,26.831701122), test loss: 33.1696453214\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (6.83343601227,2.54409968298), test loss: 3.50695657134\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (51.2902030945,26.8132790868), test loss: 28.0844240665\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.93937253952,2.54282808619), test loss: 3.08595864773\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (21.0164318085,26.7928628355), test loss: 34.0565357924\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.81637310982,2.54182848598), test loss: 4.02989137471\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (10.9889192581,26.7789867777), test loss: 28.5876812935\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.55135345459,2.54095544434), test loss: 3.61873662472\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (13.5203561783,26.7600361034), test loss: 32.4838717937\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.14662837982,2.54054428456), test loss: 3.19754944444\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (26.3806591034,26.738732883), test loss: 32.7113191128\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.06079816818,2.53934513292), test loss: 3.57661478817\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (51.7993583679,26.7305017916), test loss: 40.0830514431\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.19529533386,2.53870967721), test loss: 2.69624531865\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (20.2174472809,26.7140783156), test loss: 30.6104001284\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.67009997368,2.5374900019), test loss: 3.69661871791\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (37.0794868469,26.7018754434), test loss: 30.6118459225\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.20044946671,2.53629180998), test loss: 2.77236653268\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (50.0613670349,26.6854139912), test loss: 33.1341048717\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.46865510941,2.53567245511), test loss: 3.42368294299\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (19.8661308289,26.6674271434), test loss: 29.4161559582\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.6016998291,2.53484818102), test loss: 2.69334935546\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (4.28375005722,26.6536183998), test loss: 32.7212897778\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.41847324371,2.53361860695), test loss: 3.79011177421\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (7.1896905899,26.6385481698), test loss: 28.0242800713\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (0.434296935797,2.53335970967), test loss: 3.31777240336\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (39.9546890259,26.6159471762), test loss: 33.3123580933\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.21759009361,2.53230661257), test loss: 3.75417442024\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (7.10953712463,26.6039731257), test loss: 29.9237742901\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (6.77127122879,2.53131079278), test loss: 3.65645125508\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (17.2567062378,26.5915736012), test loss: 39.4240224838\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.02765369415,2.53051957066), test loss: 2.87065549493\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (14.4068870544,26.5755158423), test loss: 30.1155071735\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.5757920742,2.52928839768), test loss: 3.71492190957\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (35.4864006042,26.5599914713), test loss: 34.485496521\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.71930742264,2.52833672105), test loss: 2.78442612886\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (25.1739177704,26.5452879495), test loss: 31.8724517345\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (5.84845399857,2.52778609171), test loss: 3.86703981161\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (26.6392745972,26.5300599956), test loss: 29.4631233215\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.13194727898,2.52666127972), test loss: 3.06007706225\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (24.8025360107,26.5137491404), test loss: 32.6614594698\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.51972031593,2.52611859522), test loss: 3.53598711789\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (20.7910861969,26.4934916406), test loss: 27.2499936104\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.761617898941,2.52529725861), test loss: 2.64691228867\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (38.8407669067,26.4809122693), test loss: 33.2114527702\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (5.64259624481,2.52397469238), test loss: 3.72217765749\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (14.2373218536,26.4686247753), test loss: 33.8012820721\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.1170592308,2.52331960873), test loss: 3.50207282305\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (14.0323143005,26.4535956729), test loss: 33.3467787266\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.20786428452,2.5223675661), test loss: 3.14419781566\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (20.8937263489,26.4343273453), test loss: 31.3299760103\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.04658007622,2.52119453574), test loss: 3.48596050739\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (46.1699295044,26.4227733516), test loss: 33.8490253448\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.70223760605,2.52044422582), test loss: 2.82456661761\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.8847455978,26.4080187976), test loss: 35.3423244238\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.881541848183,2.51967371814), test loss: 3.68694369793\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (26.3033161163,26.3906684112), test loss: 33.2200230598\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.06383514404,2.51899205639), test loss: 2.86664742529\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (30.0865745544,26.3746768317), test loss: 31.9752196312\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.26982760429,2.51822589813), test loss: 3.59163219035\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (26.2074356079,26.3607317248), test loss: 28.6084056377\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.879258692265,2.51698009282), test loss: 2.8270742178\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (26.0843086243,26.3472044035), test loss: 31.7100823879\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (7.14526367188,2.5162991266), test loss: 3.50727523565\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (38.9591941833,26.3344416467), test loss: 29.8588185549\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.2023537159,2.51532452101), test loss: 3.42930098176\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (14.3944139481,26.3151715622), test loss: 32.2024858713\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.81588721275,2.51436376565), test loss: 3.81069126725\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (20.4009437561,26.3024006121), test loss: 32.3279228449\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.2062599659,2.51337066064), test loss: 3.5939989984\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (70.5291595459,26.2901349623), test loss: 41.7727950096\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.70409560204,2.51285776919), test loss: 3.25164218396\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (43.1403923035,26.2715092136), test loss: 35.6426177979\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.16618156433,2.51208792065), test loss: 3.56862614155\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (7.30704164505,26.2546904915), test loss: 34.8864035606\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.67343330383,2.51110640233), test loss: 2.77870525122\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.7985725403,26.2428093982), test loss: 31.1368656635\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.70471906662,2.51007547338), test loss: 3.571265167\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (15.3250484467,26.2276991206), test loss: 29.2534254074\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.722740888596,2.50903463155), test loss: 2.81146544218\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (7.42851924896,26.2134661111), test loss: 35.0840577602\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.94902658463,2.50809312891), test loss: 3.56363729835\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.541595459,26.1980032072), test loss: 28.3257491589\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.09219288826,2.50731813182), test loss: 2.60518024862\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (33.5336837769,26.1837643066), test loss: 32.634372139\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.13929796219,2.50624991575), test loss: 3.68150276542\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (20.5219535828,26.1680560292), test loss: 28.0371469021\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.58251309395,2.50549148225), test loss: 3.4027110368\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (25.3210372925,26.1538001196), test loss: 39.0514317036\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (4.57601308823,2.5049923111), test loss: 3.76507291198\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (22.843460083,26.1369076754), test loss: 28.3144165516\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (5.26954650879,2.50382422945), test loss: 3.54627002776\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (29.3037509918,26.1249627468), test loss: 35.6454786301\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.97754859924,2.50282724255), test loss: 2.92731505334\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (28.4871749878,26.1112967017), test loss: 31.8204097033\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.52804803848,2.50198980601), test loss: 3.85399104953\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (40.299331665,26.0961785537), test loss: 34.0887277603\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.31499242783,2.50079643381), test loss: 2.80295061469\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (10.9901275635,26.0814172715), test loss: 31.2381218433\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.25517392159,2.50000964314), test loss: 3.77405155301\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (50.7837944031,26.0673251755), test loss: 29.2304376602\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (4.29046535492,2.49910047467), test loss: 2.77884356976\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (62.4057388306,26.051584375), test loss: 32.3566637039\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (4.84374189377,2.4982424945), test loss: 3.43019873202\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (35.6725006104,26.03706212), test loss: 28.1208754063\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.19827973843,2.49755628884), test loss: 3.17519455254\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (13.5204048157,26.022431631), test loss: 32.7720454454\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.66875338554,2.49660939453), test loss: 3.82902585566\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (28.4135875702,26.0089452655), test loss: 28.6272869587\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.70657491684,2.49558390002), test loss: 3.54943292141\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.5183620453,25.9969127021), test loss: 33.0833071709\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.73086887598,2.49477734472), test loss: 3.1605727464\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (14.2908916473,25.9805305076), test loss: 32.3127590656\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.94254112244,2.49369942239), test loss: 3.57623500228\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (35.3898925781,25.9667879585), test loss: 34.2513859749\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.6215364933,2.49286794028), test loss: 2.76687940359\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (21.2749042511,25.9527403774), test loss: 29.7777083874\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.646990835667,2.49194404634), test loss: 3.67083079219\n",
      "\n",
      "MC # 5, Hype # hyp2, Fold # 4\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold4/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (30.7527694702,inf), test loss: 35.4758185863\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.25620698929,inf), test loss: 3.25167759657\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (25.6376571655,27.0730154204), test loss: 33.059233427\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.76169717312,2.5179034816), test loss: 3.69888680577\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (13.6864051819,26.8866498659), test loss: 29.1194500923\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (0.42560133338,2.51634700727), test loss: 3.49924360514\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (5.34600973129,26.8957364734), test loss: 34.0418302774\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.05638790131,2.50744520873), test loss: 3.95109400749\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (25.2817897797,26.8769490592), test loss: 31.4214354992\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.95702397823,2.50332151969), test loss: 3.8350666821\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (23.8519020081,26.8936229473), test loss: 35.9731543541\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.10138511658,2.50425491635), test loss: 3.00218692422\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (60.2503814697,26.8671655924), test loss: 32.1608159781\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.4161336422,2.50288378623), test loss: 3.97794321179\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (9.10811138153,26.834589116), test loss: 31.4988025665\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.30468332767,2.50411366364), test loss: 2.89699395299\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (15.1932001114,26.8339263865), test loss: 32.6738371849\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.01104927063,2.50390384833), test loss: 3.75912543535\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (19.029132843,26.8064475916), test loss: 32.7919252634\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.19211673737,2.50365710658), test loss: 2.95334215164\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (83.4981079102,26.779076942), test loss: 43.9114276886\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.84618663788,2.50479913429), test loss: 3.77420556545\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (13.869260788,26.7646837575), test loss: 28.5526089191\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.99840712547,2.50366503553), test loss: 2.75336292088\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (10.4719562531,26.7581039902), test loss: 32.8659478426\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.97304844856,2.50124220339), test loss: 3.85180829763\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (9.43476963043,26.7455928722), test loss: 30.6387901306\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.696185171604,2.49964974886), test loss: 3.71126684695\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (8.05432128906,26.7113060931), test loss: 34.2808328629\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.18437099457,2.49691815662), test loss: 3.44012209773\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (23.9329299927,26.6878688227), test loss: 35.0882448196\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.51345789433,2.49604099288), test loss: 3.68521816134\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (22.8729419708,26.6827170332), test loss: 38.6379189968\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.14638996124,2.49593142629), test loss: 2.91350241303\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (79.5657272339,26.6791098899), test loss: 37.7664883614\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.16425418854,2.49641319241), test loss: 3.9430783689\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (48.8135070801,26.6596294536), test loss: 44.4713458061\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.44959497452,2.49638988123), test loss: 3.06237241626\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (10.0684719086,26.6380609899), test loss: 32.5404961586\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.5687584877,2.49545919072), test loss: 3.81850534678\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (30.597530365,26.626617268), test loss: 31.9236454487\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.984659433365,2.49348315556), test loss: 2.95244781971\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (37.4025230408,26.6116535446), test loss: 32.0061256647\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (5.27524614334,2.49304978605), test loss: 3.84244185388\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (66.9724273682,26.5959431271), test loss: 28.8436497211\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.97570490837,2.49182195074), test loss: 3.51204901934\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (25.7484283447,26.5708454801), test loss: 34.1849309921\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (4.75595092773,2.49109459101), test loss: 3.92812282592\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (25.7525138855,26.5588861938), test loss: 32.5726511002\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.47296595573,2.49019700383), test loss: 3.76581225395\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (22.2542209625,26.5450904571), test loss: 42.023046875\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.705249786377,2.48900857508), test loss: 3.04145345092\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (26.4942340851,26.5242658634), test loss: 37.7548040152\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.07072806358,2.48846406434), test loss: 3.84870252609\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (27.1976585388,26.5077997366), test loss: 32.1213958502\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.4785451889,2.48768725183), test loss: 3.05667335987\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (20.5862922668,26.4986003746), test loss: 31.5572858334\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.8906801939,2.48635413776), test loss: 3.81799575984\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (18.9858398438,26.4890299035), test loss: 38.676350975\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.14554357529,2.48611785583), test loss: 2.94120920599\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (35.198059082,26.4749346182), test loss: 32.9075596809\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (6.3526597023,2.4850261516), test loss: 3.95931376219\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (32.8305664062,26.4503677063), test loss: 29.3554589748\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.03479790688,2.48392971468), test loss: 3.06187443435\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (10.0969791412,26.4366451068), test loss: 33.1841432095\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.49158608913,2.48332590949), test loss: 3.96935135722\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (33.8420715332,26.4253879073), test loss: 28.6094884872\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.23013877869,2.48297913679), test loss: 3.81076555848\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (19.5213050842,26.4083050168), test loss: 33.9413521051\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.49318397045,2.4823443663), test loss: 3.42095580101\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (4.58507156372,26.3898150341), test loss: 32.7994684696\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.15123975277,2.48129003002), test loss: 3.80530008078\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (8.57959842682,26.3749304053), test loss: 38.6750406742\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (7.54378604889,2.48005512897), test loss: 2.89071595669\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (13.8724727631,26.3618880629), test loss: 37.9936778069\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.733326911926,2.47866478305), test loss: 3.77775819898\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (19.2890472412,26.349495799), test loss: 34.9590868473\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.70775914192,2.47786840879), test loss: 3.23939546943\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (36.7541809082,26.3305353818), test loss: 33.1040128708\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (6.07853984833,2.47719627705), test loss: 3.94433932006\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (6.23099374771,26.3198306798), test loss: 29.7420590878\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (5.96751976013,2.47668812737), test loss: 3.0209947139\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (5.78445625305,26.307928907), test loss: 32.056911087\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.6202609539,2.47596704051), test loss: 3.90454969406\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (11.595334053,26.2896879981), test loss: 28.4061126709\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.871064960957,2.47526351162), test loss: 3.65922309458\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (25.8217182159,26.2716114134), test loss: 34.1717895746\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.10770940781,2.47455802219), test loss: 3.95233708024\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (10.9612083435,26.258911273), test loss: 29.9886101723\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.66224873066,2.47383655225), test loss: 3.78388210237\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (11.9063434601,26.2475311319), test loss: 37.9717946053\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.32564532757,2.47274778414), test loss: 2.96759729385\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (32.2647094727,26.2337264483), test loss: 34.1486545086\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.0570294857,2.47165494736), test loss: 3.84268410206\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (24.3333835602,26.2132549441), test loss: 32.00384655\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.964712262154,2.47051513859), test loss: 3.04339866936\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (25.1908607483,26.1981522227), test loss: 34.0621948957\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.33345079422,2.46971881474), test loss: 3.88948039412\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (13.0998001099,26.1875208874), test loss: 32.0965082407\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.76764643192,2.46897568183), test loss: 3.07442940474\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (14.8688716888,26.1748341723), test loss: 32.5684126139\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.93387758732,2.46848417186), test loss: 3.67687460184\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (33.3416137695,26.15805398), test loss: 32.432127142\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.55223035812,2.4677223219), test loss: 3.49680839777\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (33.3781700134,26.1472609484), test loss: 41.4638917446\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.9362757206,2.46703032751), test loss: 4.08686943352\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (8.12398147583,26.1327568355), test loss: 28.3074602604\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.90592193604,2.46602818633), test loss: 4.26563816071\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (19.9915733337,26.1186316173), test loss: 34.9709270477\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.4882273674,2.46508729516), test loss: 3.3293851018\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (20.1380081177,26.1020324564), test loss: 33.7186957836\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.526745021343,2.46426969785), test loss: 3.79048588276\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (17.6955089569,26.087175187), test loss: 38.3612309694\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.89396166801,2.46367264857), test loss: 3.16159765124\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (33.1150436401,26.0745621937), test loss: 30.033429575\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.68261194229,2.46275361079), test loss: 3.80101557374\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (37.2713050842,26.0587276212), test loss: 34.8244121075\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.89854097366,2.46179770892), test loss: 3.04821099639\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (18.1919898987,26.0423573704), test loss: 33.0731058121\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.2753226757,2.46093454687), test loss: 3.82509375215\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (30.2300834656,26.0329900615), test loss: 30.4600317955\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.21981823444,2.4600965628), test loss: 2.94986931384\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (9.82262134552,26.0186059012), test loss: 32.2373897552\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.63332891464,2.45935927709), test loss: 3.96098096371\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.5756855011,26.0084272631), test loss: 27.7093118429\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.80255889893,2.45865011941), test loss: 3.80116772354\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.0111722946,25.9917675943), test loss: 37.567969203\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.53622889519,2.45767626753), test loss: 3.95229335427\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (23.7519645691,25.9768296509), test loss: 31.4303645134\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.90446424484,2.45702957165), test loss: 3.79214799106\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (23.7748146057,25.9629014038), test loss: 36.3272759914\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.260726362467,2.45614690587), test loss: 3.32915288806\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (22.9451370239,25.9495109759), test loss: 31.6154171944\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.844182491302,2.45556798795), test loss: 3.86958102584\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (13.5958766937,25.9339688488), test loss: 33.923854351\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.93963265419,2.45487861326), test loss: 3.0959295094\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (3.83287572861,25.9231424084), test loss: 31.3528095961\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.23677611351,2.45392563453), test loss: 3.88836310506\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (59.0493927002,25.9072263981), test loss: 35.3849160671\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.23216307163,2.45280488493), test loss: 3.13284870684\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (39.4243087769,25.8957757125), test loss: 31.8701939106\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.10504615307,2.45189282605), test loss: 3.77566707432\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.6766891479,25.8806086354), test loss: 29.377990818\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.83126211166,2.45097622162), test loss: 3.51273423433\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (26.2808876038,25.8673506422), test loss: 33.9040473938\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.55775046349,2.45040136265), test loss: 4.09861475825\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (15.6165380478,25.8562625711), test loss: 29.3166801929\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.0175743103,2.44963203161), test loss: 4.12885253727\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (17.6426391602,25.8427442642), test loss: 34.5167367935\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.49230790138,2.44895080689), test loss: 3.35189630389\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (27.6289596558,25.825902396), test loss: 32.1851595163\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.178855896,2.44826326511), test loss: 3.95476961732\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.7298002243,25.8142987006), test loss: 35.7050549269\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.99160385132,2.44744874994), test loss: 2.93899771571\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (23.2663211823,25.8011281361), test loss: 29.9344116211\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.14869022369,2.44661142615), test loss: 3.8182384789\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.1387729645,25.7890594291), test loss: 33.1430104733\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.69444644451,2.44572313739), test loss: 2.99944988191\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (27.271068573,25.7738199668), test loss: 32.422809267\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.71755838394,2.44476389278), test loss: 3.84776057303\n",
      "\n",
      "MC # 5, Hype # hyp2, Fold # 5\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold5/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (41.8006820679,inf), test loss: 35.5253884315\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (2.88027262688,inf), test loss: 3.10495225191\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (30.2584877014,27.6895341525), test loss: 32.2540755749\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.567938685417,2.48332881755), test loss: 3.64856993258\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (4.4165225029,27.5837015936), test loss: 36.7208382607\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.73256802559,2.47964106336), test loss: 3.51967768669\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (49.7577095032,27.6745006975), test loss: 33.3072306633\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.08507156372,2.47361670794), test loss: 3.96391844153\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (27.6195144653,27.6483672419), test loss: 33.6999758244\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.93073356152,2.47054042971), test loss: 4.09045936465\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (44.9752578735,27.6621546943), test loss: 35.1885067463\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.90593695641,2.46842607738), test loss: 3.19221957326\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (40.2268295288,27.6049169552), test loss: 36.3645588398\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (5.87613582611,2.46751944094), test loss: 3.71027855873\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (16.1655063629,27.6082089282), test loss: 35.528357625\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (5.36490154266,2.46651138092), test loss: 2.93371301591\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (17.9345474243,27.6044745635), test loss: 41.4708229065\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.613402247429,2.46633313728), test loss: 3.92177684903\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (28.7530899048,27.5672112315), test loss: 43.7031033516\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.08234274387,2.46670827448), test loss: 3.01250059009\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (11.8850727081,27.5366318798), test loss: 31.3654996872\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.982006073,2.46665966864), test loss: 3.76028854847\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (51.0855865479,27.5333153632), test loss: 33.4270846844\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.80329847336,2.46325862773), test loss: 2.98720521927\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (24.5930480957,27.520986008), test loss: 32.0345357895\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.68001890182,2.4627085457), test loss: 3.66181955338\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (39.4332389832,27.498055344), test loss: 34.4754309654\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.67689847946,2.4604230988), test loss: 3.72669570446\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (44.1960067749,27.4810993379), test loss: 32.8649528742\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.93050098419,2.46011170547), test loss: 4.12076789737\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (32.5872268677,27.4676131322), test loss: 34.4234070301\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (0.892454147339,2.45893200362), test loss: 4.14225050509\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (43.6390838623,27.4463740748), test loss: 34.4925815582\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.90695524216,2.45924251514), test loss: 3.20030795932\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (66.7571792603,27.422083987), test loss: 34.6472857952\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.1620259285,2.4589261122), test loss: 3.77031062841\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (51.9371986389,27.4096828835), test loss: 45.6729366302\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.19371318817,2.45792245366), test loss: 2.90029496849\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (27.614692688,27.3925655226), test loss: 33.5793793201\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.82078933716,2.45659761099), test loss: 3.80448609591\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (21.6285934448,27.3803355917), test loss: 34.5871632099\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.19911932945,2.45534727334), test loss: 3.22156892419\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (39.0895690918,27.3523718647), test loss: 34.5199717522\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.77479076385,2.45417682449), test loss: 3.77896544933\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (30.9745864868,27.3428855662), test loss: 33.0244587421\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.12401819229,2.45325579685), test loss: 2.97116904259\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (30.3109054565,27.3306896081), test loss: 32.9812808275\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.44152328372,2.45303950932), test loss: 3.61135070324\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (15.3570756912,27.3047138084), test loss: 35.749394846\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.58586382866,2.45273251336), test loss: 3.62532448173\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (15.9773464203,27.2878846061), test loss: 32.9187688351\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.53037798405,2.45190630638), test loss: 3.99386744499\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (25.7114295959,27.2736365537), test loss: 33.2805420399\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.54607629776,2.45059484988), test loss: 3.93087347448\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (12.1629629135,27.258659756), test loss: 34.3889930248\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.881775856018,2.44969264916), test loss: 3.14151308835\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (47.2228164673,27.2370056744), test loss: 34.6701970577\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.26408410072,2.44824164226), test loss: 3.85468946695\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (24.5836524963,27.220723747), test loss: 36.4196761131\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.24088907242,2.44766529866), test loss: 2.95148165226\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (34.4152946472,27.2063858268), test loss: 31.9200203419\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.62588882446,2.44670789677), test loss: 3.84530857801\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (37.5683898926,27.1865024093), test loss: 33.3022868633\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.12227272987,2.44630752632), test loss: 3.00848715603\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (29.8636417389,27.1669827961), test loss: 31.8095371246\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.5600861311,2.44569517971), test loss: 3.76072613597\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (8.46247196198,27.1536265595), test loss: 37.8386693954\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (6.48108768463,2.44497797108), test loss: 3.04955129623\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (15.3319530487,27.1379542691), test loss: 39.2649719715\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.797742486,2.44406128335), test loss: 3.60588746667\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (19.58618927,27.1242450229), test loss: 33.9405779362\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.01829981804,2.44284392181), test loss: 3.68235592842\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (12.6401538849,27.0992952922), test loss: 33.1188038826\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (5.29931306839,2.44191245594), test loss: 3.94265627861\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (17.1255435944,27.0866435122), test loss: 39.954087925\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.66187214851,2.44086504087), test loss: 3.93726323843\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (55.5792121887,27.071401535), test loss: 39.3111689091\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.10517668724,2.44035646184), test loss: 2.94835713506\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (102.407493591,27.0521854274), test loss: 39.3070436001\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.22960424423,2.43973647879), test loss: 3.83473740816\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (10.9527626038,27.0365600303), test loss: 35.9313822269\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.39540290833,2.43871975367), test loss: 2.85337677002\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (45.5687637329,27.0222031411), test loss: 31.0928031445\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.40286850929,2.437671721), test loss: 3.9478309989\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (28.5568389893,27.0069898713), test loss: 34.5564290524\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.56187391281,2.43697198569), test loss: 2.92630547881\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (12.2914924622,26.9867639337), test loss: 33.0874241352\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.20731627941,2.43565224579), test loss: 3.83209601045\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (14.3013954163,26.9705383252), test loss: 34.6134280682\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.6312071085,2.43478576436), test loss: 3.0484141618\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (13.6894226074,26.954445253), test loss: 31.6954832792\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.983717739582,2.43376430275), test loss: 3.76090570092\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.037147522,26.9368318477), test loss: 33.5125540733\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.292650103569,2.43315880807), test loss: 3.62463439107\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (22.1129417419,26.9185406109), test loss: 32.4675735235\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.50418561697,2.43235627877), test loss: 3.89537966847\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (19.405544281,26.905130384), test loss: 32.8537117004\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.81461107731,2.43118305831), test loss: 3.90786530972\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (26.3202114105,26.8911116396), test loss: 36.6645109653\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (8.37562656403,2.43070483544), test loss: 2.91717734337\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (44.2966384888,26.8754540269), test loss: 36.6296797752\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.51931166649,2.4294408822), test loss: 3.8295653522\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (17.2824058533,26.855843635), test loss: 34.9609007835\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.56263804436,2.42850103499), test loss: 2.85306069851\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (28.1731262207,26.8414971865), test loss: 34.5896799803\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.46584272385,2.4276026392), test loss: 3.81197010279\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (47.4844894409,26.8242018941), test loss: 39.860197258\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (5.92054080963,2.42690111857), test loss: 3.01736158431\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (37.5933570862,26.8047114905), test loss: 32.2942322731\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.59734010696,2.42613480396), test loss: 3.89009102583\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (5.42798948288,26.792433892), test loss: 33.2942550659\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.5720448494,2.4250623245), test loss: 2.93760480881\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (27.6367988586,26.7770080646), test loss: 32.3490553379\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.2754380703,2.42417206099), test loss: 3.91200190485\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.2403259277,26.7626886958), test loss: 34.1909785271\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (5.68073272705,2.42339006387), test loss: 3.69519968629\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (24.2572956085,26.7444162432), test loss: 33.4027880192\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.12780177593,2.42241290969), test loss: 4.00383236706\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (19.5336322784,26.7287774702), test loss: 39.8995278358\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.27448320389,2.42155207559), test loss: 3.89409615993\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (4.99087381363,26.7141275857), test loss: 35.6641882181\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.655377149582,2.42061703284), test loss: 2.9610111624\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (24.707775116,26.696126631), test loss: 34.9185325146\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.7321254015,2.41991504112), test loss: 3.98182915449\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (19.4454460144,26.6785716867), test loss: 33.8588303566\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.51039457321,2.41915930902), test loss: 2.88096008897\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (16.4082241058,26.6654621646), test loss: 34.8848029852\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.3591966629,2.41798768632), test loss: 3.87562298775\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (15.6271572113,26.6506010171), test loss: 33.9229542255\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.755061507225,2.41715154718), test loss: 2.99185801744\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (36.3724899292,26.6359958233), test loss: 32.4574455261\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.79797458649,2.41618782484), test loss: 3.84922575057\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.7355079651,26.6190267969), test loss: 32.9203504562\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.34350717068,2.41537406322), test loss: 2.98774210215\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (41.0400924683,26.6043709912), test loss: 32.1723151207\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (4.75531959534,2.41455077031), test loss: 3.77195442319\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (27.5888404846,26.5870109605), test loss: 33.9839931965\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.566081285477,2.41379822082), test loss: 3.69296505451\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (4.31714868546,26.5689202345), test loss: 35.0728397369\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.51477718353,2.41301408309), test loss: 3.90460518003\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (49.5819778442,26.5560203759), test loss: 32.4589082241\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.930379450321,2.41205426886), test loss: 3.96295338869\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (25.3764572144,26.5408509773), test loss: 34.7691502571\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.92144274712,2.41118688831), test loss: 3.14246820807\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (36.7591056824,26.5270153641), test loss: 37.5375749826\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.61145305634,2.41026806789), test loss: 3.86759486794\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (36.4900817871,26.5093138262), test loss: 32.9471363306\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (5.18016576767,2.40944157971), test loss: 2.88622831702\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (15.5779705048,26.4953292501), test loss: 34.0270442486\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (4.81801176071,2.40859300703), test loss: 3.99335150719\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (16.4316082001,26.4809545237), test loss: 48.9251644135\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.539895713329,2.40782994765), test loss: 3.15710210204\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (24.9429531097,26.4635123156), test loss: 35.3150556564\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.982879757881,2.40709478901), test loss: 3.82126168311\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.52754879,26.4464450239), test loss: 33.7587828159\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.90468668938,2.40636580358), test loss: 2.87994092405\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (39.2039833069,26.4333007783), test loss: 31.9911016464\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.71322095394,2.40521533308), test loss: 3.8262111187\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (23.3824539185,26.4191227975), test loss: 32.9600376606\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (4.6021232605,2.40444518103), test loss: 3.89801302552\n",
      "run time for single CV loop: 1310.416924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:124: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:126: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Deign Net Architecutre and Run Caffe\n",
    "exp_name = 'Exp13_MC'\n",
    "cohort = 'ADNI1and2'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'CT'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "multi_label = False\n",
    "start_MC=1\n",
    "n_MC=5\n",
    "MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "start_fold = 1\n",
    "n_folds = 5\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "niter = 40000\n",
    "batch_size = 256\n",
    "\n",
    "pretrain = False\n",
    "multimodal_autoencode = False\n",
    "load_pretrained_weights = True\n",
    "\n",
    "if Clinical_Scale in ['BOTH','ADAS13_DX'] :\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "# dr: Dropout rate, lr: learning rate of each modality, tr: loss-weight for each task\n",
    "hype_configs = {\n",
    "                'hyp1':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':100,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':2,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-4}},  \n",
    "        \n",
    "                 'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':200,'HC_CT_ff':25,'COMB_ff':100,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':2,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-4}}, \n",
    "                    \n",
    "                }\n",
    "\n",
    "CV_perf_MC = {}\n",
    "for mc in MC_list:\n",
    "    CV_perf_hype = {}\n",
    "    for hype in hype_configs.keys():    \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "        tr = hype_configs[hype]['tr']\n",
    "        solver_configs = hype_configs[hype]['solver_conf']\n",
    "        \n",
    "        if hype in ['hyp1','hyp4']:\n",
    "            HC_snap = 20000 #5000 for ADNI1\n",
    "            CT_snap = 20000 #5000 for ADNI1\n",
    "            pre_hype = 'hyp1'\n",
    "        elif hype in ['hyp2','hyp5']:\n",
    "            HC_snap = 8000 #5000 for ADNI1\n",
    "            CT_snap = 28000 #5000 for ADNI1\n",
    "            pre_hype = 'hyp2'\n",
    "        else:\n",
    "            print 'unknown hyp config'\n",
    "            \n",
    "        CV_perf = {}\n",
    "        start_time = time.time()\n",
    "        for fid in fid_list:            \n",
    "            print ''\n",
    "            print 'MC # {}, Hype # {}, Fold # {}'.format(mc, hype, fid)\n",
    "            snap_prefix = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}'.format(mc,fid,exp_name,hype,modality)\n",
    "\n",
    "            train_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/train_C688.txt'.format(mc,fid)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "\n",
    "            train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "            with open(train_filename_txt, 'w') as f:\n",
    "                    f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "            if pretrain:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_train.prototxt'.format(mc,fid)\n",
    "                with open(train_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(train_filename_txt, batch_size, node_sizes, modality)))            \n",
    "            else:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(train_net_path, 'w') as f:            \n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            if pretrain:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_test.prototxt'.format(mc,fid)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(test_filename_txt, batch_size, node_sizes,modality)))\n",
    "            else:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            # Define Solver\n",
    "            solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "            with open(solver_path, 'w') as f:\n",
    "                f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, snap_prefix)))\n",
    "\n",
    "            ### load the solver and create train and test nets\n",
    "            #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "            #solver = caffe.get_solver(solver_path)\n",
    "            #solver = caffe.NesterovSolver(solver_path)\n",
    "            solver = caffe.NesterovSolver(solver_path)\n",
    "\n",
    "            if load_pretrained_weights:                    \n",
    "                #net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_HC_CT_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                #snap_path = baseline_dir + net_name.format(mc,fid,cohort,pre_hype,HC_snap,CT_snap)\n",
    "                \n",
    "                net_name = 'API/data/MC_{}/fold{}/train_snaps/Exp13_MC_{}_CT_iter_80000.caffemodel'\n",
    "                snap_path = baseline_dir + net_name.format(mc,fid,hype)\n",
    "                \n",
    "                print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "                solver.net.copy_from(snap_path)\n",
    "\n",
    "            #run caffe\n",
    "            results = run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode)\n",
    "            CV_perf[fid] = results\n",
    "            \n",
    "        print('run time for single CV loop: {}'.format(time.time() - start_time))\n",
    "\n",
    "        CV_perf_hype[hype] = CV_perf\n",
    "        \n",
    "CV_perf_MC[mc] = CV_perf_hype\n",
    "\n",
    "# pickleIt(CV_perf_MC, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.77777777778\n"
     ]
    }
   ],
   "source": [
    "#Time for training \n",
    "#HC_CT fold 9,10, number of iterations: 40k, two hyper-params\n",
    "#start time: 14:47\n",
    "#end time: 15:31\n",
    "#runtime_mean = (13+31)/4\n",
    "#print runtime_mean\n",
    "\n",
    "tx=260/(4*2) #time for 10k iters\n",
    "itx=4 # num of 10k iters\n",
    "hx=2 #hyp choices\n",
    "fx=5 #k-folds\n",
    "mx=5 #mc-folds\n",
    "\n",
    "num_hrs = (tx*itx*hx*fx*mx)/(3600.0)\n",
    "print num_hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbcAAAJoCAYAAABVztwOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERERERERESk6vw/Lp365x3dE3kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff5980670d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "#niter = 30000\n",
    "test_interval = 500\n",
    "n_mc = len(CV_perf_MC.keys())\n",
    "\n",
    "for m, mc in enumerate(CV_perf_MC.keys()): \n",
    "    CV_perf_hype = CV_perf_MC[mc]\n",
    "    \n",
    "    for hype in CV_perf_hype.keys(): \n",
    "        CV_perf = CV_perf_hype[hype]\n",
    "        n_CV_configs = len(CV_perf)\n",
    "        pid = m*n_CV_configs + 1\n",
    "        \n",
    "        for f, fid in enumerate(fid_list):  \n",
    "            train_loss_list = CV_perf[fid]['train_loss']\n",
    "            test_loss_list = CV_perf[fid]['test_loss']\n",
    "            \n",
    "            for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "                #plt.figure()\n",
    "                ax1 = plt.subplot(n_mc,n_CV_configs,pid)            \n",
    "                ax1.plot(arange(niter), train_loss, label='train',linewidth='.5',alpha=0.25)\n",
    "                ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test_{}'.format(hype), linewidth='3')                            \n",
    "                ax1.set_xlabel('iteration')\n",
    "                ax1.set_ylabel('loss')\n",
    "                ax1.set_title('fid: {}, hyp: {}, Test loss: {:.2f}'.format(fid, hype, test_loss[-1]))\n",
    "                ax1.legend(loc=1)\n",
    "                ax1.set_ylim(0,100)\n",
    "            pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp11'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'R_HC'\n",
    "batch_size = 256\n",
    "encoding_layer = 'code'\n",
    "weight_layers = 'code'\n",
    "multi_task = False\n",
    "\n",
    "if modality in ['L_HC', 'R_HC']:\n",
    "    snap_iter = 10000\n",
    "    \n",
    "    if modality == 'R_HC':\n",
    "        output_node_size = 16471\n",
    "    else: \n",
    "        output_node_size = 16086\n",
    "        \n",
    "    hype_configs = {\n",
    "                   'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':8000,'out':output_node_size},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "\n",
    "elif modality in ['CT']:  \n",
    "    snap_iter = 100000\n",
    "    \n",
    "    hype_configs = {\n",
    "                    'hyp1':{'node_sizes':{'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-4, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "elif modality in ['HC_CT']:  \n",
    "    snap_iter = 20000\n",
    "    hype_configs = {\n",
    "                 'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':500,'out_HC':16086,'out_CT':686},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "hype = 'hyp1'\n",
    "pretrain_preproc = 'ae_preproc_sparse_HC_10k_CT_100k_{}'.format(hype)\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "\n",
    "# Save either L or R HC encodings. Turn on CT + CS save only for one of these. \n",
    "# (Also decide if you want to copy CT raw scores or encodings)\n",
    "save_encodings = True\n",
    "save_scores = False\n",
    "CT_encodings = True\n",
    "\n",
    "for cohort in ['inner_train', 'inner_test','outer_test']:\n",
    "    data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "    test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)       \n",
    "    input_nodes = ['X_{}'.format(modality)]\n",
    "    #input_nodes = ['X_L_HC','X_CT']\n",
    "    net_file = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "    with open(net_file, 'w') as f:\n",
    "        f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "\n",
    "    test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "    with open(test_filename_txt, 'w') as f:\n",
    "        f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "    sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "    if modality == 'CT':\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "    else:\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "        \n",
    "    print 'Check Sparsity for model: {}'.format(model_file)\n",
    "    \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    \n",
    "    encodings_hdf_file = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,pretrain_preproc)\n",
    "    \n",
    "    if save_encodings:    \n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        input_data.create_dataset(input_nodes[0],data=encodings)           \n",
    "        input_data.close()\n",
    "        \n",
    "    if save_scores:\n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        if not CT_encodings: #copy raw scores for CT instead of encodings\n",
    "            CT_data = load_data(data_path, 'X_CT','no_preproc')                    \n",
    "            input_data.create_dataset('X_CT', data=CT_data)    \n",
    "            \n",
    "        adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "        mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "        input_data.create_dataset('y', data=adas_scores)           \n",
    "        input_data.create_dataset('y3', data=mmse_scores)    \n",
    "        input_data.create_dataset('dx_cat3', data=dx_labels)\n",
    "        input_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting TSNE embeddings \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "exp_name = 'Exp11'\n",
    "#preproc = 'no_preproc'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "Clinical_Scale = 'ADAS13_DX' \n",
    "batch_size = 256\n",
    "snap_interval = 2000\n",
    "snap_start = 2000\n",
    "encoding_layer = 'ff3'\n",
    "weight_layers = 'ff3'\n",
    "cohort = 'outer_test'\n",
    "multi_task = False\n",
    "\n",
    "modality = 'HC_CT'\n",
    "snap_end = 21000\n",
    "\n",
    "\n",
    "hype_configs = {\n",
    "            'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-2}},\n",
    "}\n",
    "\n",
    "hype = 'hyp1'\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "tr = hype_configs[hype]['tr']\n",
    "\n",
    "data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "test_net_path = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)       \n",
    "#input_node = 'X_{}'.format(modality)\n",
    "\n",
    "net_file = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)\n",
    "with open(net_file, 'w') as f:\n",
    "    #f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "\n",
    "test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "with open(test_filename_txt, 'w') as f:\n",
    "    f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "n_snaps = len(np.arange(snap_start,snap_end,snap_interval))\n",
    "tsne_encodings = {}\n",
    "net_weights = {}\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    print sn, snap_iter\n",
    "    model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    net_weights[snap_iter] = results['wt_dict']\n",
    "    print encodings.shape\n",
    "\n",
    "    adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "    mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "    dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "    tsne_model = TSNE(n_components=2, random_state=0, init='pca')\n",
    "    tsne_encodings[snap_iter] = tsne_model.fit_transform(encodings) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "plot_encodings = True\n",
    "plot_wts = False\n",
    "\n",
    "color_scores = dx_labels\n",
    "cm = plt.get_cmap('RdYlBu') \n",
    "marker_size = 100\n",
    "marker_size = (np.mod(color_scores+1,2)+1)*50\n",
    "\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    plt.subplot(np.ceil(n_snaps/2.0),2,sn+1)\n",
    "    if plot_encodings:\n",
    "        plt.scatter(tsne_encodings[snap_iter][:,0],tsne_encodings[snap_iter][:,1],c=color_scores,s=marker_size,cmap=cm)\n",
    "    if plot_wts:\n",
    "        plt.imshow(net_weights[snap_iter][weight_layers])\n",
    "        \n",
    "    plt.title(snap_iter)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp13_MC 1 CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.57115037399151725, 0.60591714912320882, 0.56203579839898299, 0.65771045230263314, 0.55327640152266711, 0.63698664054386711, 0.52916225423152774, 0.51220607418496733, 0.51246018343482758, 0.51185015790058119]\n",
      "ADAS mse: [62.140661529244824, 56.291814569613678, 60.262003967876943, 55.641602228289479, 55.415779384452513, 46.765454505668082, 79.074690981492154, 64.808996259097682, 57.637207063345087, 72.642250359774621]\n",
      "ADAS means: 0.565275548563, 61.0680460849\n",
      "\n",
      "MMSE corr: [0.39671273709439919, 0.38354570326976589, 0.47012556706851749, 0.50566180989934884, 0.54153260129983782, 0.53347452073471113, 0.59538955510545333, 0.45308713187163846, 0.53826799558200245, 0.50668135928099023]\n",
      "MMSE mse: [7.1196495628969227, 7.222183825941233, 5.4902179833616307, 6.0963257487858415, 5.4662590944667064, 5.2357930319503527, 5.6313848593741405, 5.7694346881369496, 4.797825274360596, 5.393710711951833]\n",
      "MMSE means: 0.492447898121, 5.82227847812\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 7, 2: 4, 3: 4, 4: 17, 5: 12, 6: 15, 7: 10, 8: 18, 9: 4, 10: 9}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.5744301951208215, 0.61008273954554315, 0.56203579839898299, 0.65771045230263314, 0.55327640152266711, 0.63729648751743473, 0.52916225423152774, 0.51220607418496733, 0.51246018343482758, 0.51185015790058119]\n",
      "ADAS mse: [61.890375680636296, 56.181621395125475, 60.262003967876943, 55.641602228289479, 55.415779384452513, 46.714668710117635, 79.074690981492154, 64.808996259097682, 57.637207063345087, 72.642250359774621]\n",
      "ADAS means: 0.566051074416, 61.026919603\n",
      "\n",
      "MMSE corr: [0.3987738656049466, 0.38148708049145613, 0.47012556706851749, 0.50566180989934884, 0.54153260129983782, 0.532711165618313, 0.59538955510545333, 0.45308713187163846, 0.53826799558200245, 0.50668135928099023]\n",
      "MMSE mse: [7.3801829042798044, 7.3458701653531611, 5.4902179833616307, 6.0963257487858415, 5.4662590944667064, 5.459362241449579, 5.6313848593741405, 5.7694346881369496, 4.797825274360596, 5.393710711951833]\n",
      "MMSE means: 0.492371813182, 5.88305736715\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 9, 2: 9, 3: 4, 4: 17, 5: 12, 6: 14, 7: 10, 8: 18, 9: 4, 10: 9}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.5147259238718811, 0.58320089019725552, 0.56633948832930137, 0.65346574616326381, 0.5468363459920863, 0.63698664054386711, 0.51149981554641577, 0.46888259274772903, 0.51784915804345633, 0.50589090526498282]\n",
      "ADAS mse: [69.357967890927242, 69.554566346180664, 61.970179468403607, 55.931760755183227, 59.890741205535122, 46.765454505668082, 85.539364638932554, 73.880491586570599, 58.903864767023911, 73.43701089385803]\n",
      "ADAS means: 0.55056775067, 65.5231402058\n",
      "\n",
      "MMSE corr: [0.39268997171265024, 0.38568388519658425, 0.46813693036558635, 0.51954760019654289, 0.55063875320441014, 0.53347452073471113, 0.60451657307339757, 0.45432290873529113, 0.57051719882910434, 0.50171748009895722]\n",
      "MMSE mse: [6.9469528940201162, 6.8344292775227187, 5.4316615699269768, 5.8871845897157229, 5.2870542706785892, 5.2357930319503527, 5.3266359452149414, 5.457429468198689, 4.3665495792086135, 5.3509087618036926]\n",
      "MMSE means: 0.498124582215, 5.61245993882\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 11, 2: 12, 3: 6, 4: 6, 5: 19, 6: 15, 7: 12, 8: 5, 9: 16, 10: 9}\n",
      "Exp13_MC 2 CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.56816429811177094, 0.59695146403677279, 0.59147690244871365, 0.56963462057775494, 0.63552099961561104, 0.63672705534745533, 0.52686629044435584, 0.64137929425115936, 0.44527338420430401, 0.50446065409835572]\n",
      "ADAS mse: [61.531741195821212, 63.665632502944348, 61.302903014674008, 47.672512564786174, 47.634596386193245, 59.606752005107452, 72.746971987494376, 51.437015596474673, 75.361640708762778, 65.436603896197582]\n",
      "ADAS means: 0.571645496314, 60.6396369858\n",
      "\n",
      "MMSE corr: [0.57897893672734713, 0.51562053032915423, 0.50148992713319751, 0.43740816399817645, 0.52655141179213438, 0.44084727734193346, 0.37594907226750496, 0.55610063457803582, 0.61672635555157884, 0.41502005375571627]\n",
      "MMSE mse: [5.2074869459743534, 5.7731783625894275, 5.6783701468521821, 5.825028764949316, 4.7850460444583574, 6.3163156805565128, 6.7313470265560875, 5.1592864807738694, 5.4237065125120889, 6.3592524515470696]\n",
      "MMSE means: 0.496469236347, 5.72590184168\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 8, 2: 7, 3: 19, 4: 2, 5: 13, 6: 16, 7: 8, 8: 5, 9: 10, 10: 2}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.56816429811177094, 0.59695146403677279, 0.59147690244871365, 0.55871851071731327, 0.63552099961561104, 0.63719144110702142, 0.52686629044435584, 0.64332265017129198, 0.44527338420430401, 0.50446065409835572]\n",
      "ADAS mse: [61.531741195821212, 63.665632502944348, 61.302903014674008, 47.633560683781013, 47.634596386193245, 59.604931217332187, 72.746971987494376, 50.676515341620004, 75.361640708762778, 65.436603896197582]\n",
      "ADAS means: 0.570794659496, 60.5595096935\n",
      "\n",
      "MMSE corr: [0.57897893672734713, 0.51562053032915423, 0.50148992713319751, 0.41568085066336596, 0.52655141179213438, 0.43984723782373419, 0.37594907226750496, 0.55034634213498801, 0.61672635555157884, 0.41502005375571627]\n",
      "MMSE mse: [5.2074869459743534, 5.7731783625894275, 5.6783701468521821, 5.9274916033780247, 4.7850460444583574, 6.7118949232842757, 6.7313470265560875, 6.2989890354584439, 5.4237065125120889, 6.3592524515470696]\n",
      "MMSE means: 0.493621071818, 5.88967630526\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 8, 2: 7, 3: 19, 4: 15, 5: 13, 6: 3, 7: 8, 8: 7, 9: 10, 10: 2}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.56812550019340557, 0.56188751839651285, 0.58624182848915518, 0.56180064370178828, 0.62951824372054055, 0.62725765831923486, 0.48746432692455416, 0.63668609103746809, 0.43469613815341418, 0.50692120093875326]\n",
      "ADAS mse: [61.885382181282488, 67.954009319306195, 61.724798747695374, 48.654011228143602, 49.444849448484277, 62.407481104163054, 76.053626245121166, 51.691771981885381, 76.567272079264924, 65.814727292056176]\n",
      "ADAS means: 0.560059914987, 62.2197929627\n",
      "\n",
      "MMSE corr: [0.57740350313402455, 0.5193979178836583, 0.49843429224229974, 0.44379873218729465, 0.54181225614731687, 0.43253822540923065, 0.36314636259904493, 0.55252436993231646, 0.61494364352951103, 0.42351749285782542]\n",
      "MMSE mse: [5.2038416119895707, 5.6545064854141422, 5.6095121108666213, 5.7382506422433375, 4.558731760045557, 6.1530638252021053, 6.6884356328154988, 5.1076449809652198, 5.0740422761600099, 6.0771114595119897]\n",
      "MMSE means: 0.496751679592, 5.58651407852\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 6, 2: 15, 3: 17, 4: 3, 5: 3, 6: 11, 7: 19, 8: 3, 9: 13, 10: 6}\n",
      "Exp13_MC 3 CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.63344544297061744, 0.62483167833666775, 0.51595417074424821, 0.64718632297140077, 0.51297304102247243, 0.5665479542010331, 0.51639500979924036, 0.54225243589933736, 0.6725231633050478, 0.50865893065384515]\n",
      "ADAS mse: [44.235897662212537, 47.793012518931192, 71.36181736918239, 57.730318950638591, 69.474790343002709, 64.086930248966809, 68.745965493260584, 60.945457465085887, 62.299290126372476, 58.464682128269217]\n",
      "ADAS means: 0.57407681499, 60.5138162306\n",
      "\n",
      "MMSE corr: [0.61363525679225617, 0.48864191628947273, 0.49426308873333508, 0.4858532341736983, 0.44577280817320697, 0.59418416883579217, 0.4350889134188935, 0.51570918365859675, 0.51005212307412962, 0.39920058147044757]\n",
      "MMSE mse: [4.9755798967908706, 5.7150817621800671, 5.6294528137968616, 5.7895316666845709, 6.1465496620513367, 5.1923457340576267, 5.9098363477363574, 5.5964576602509082, 5.6057136672278096, 6.739416521841826]\n",
      "MMSE means: 0.498240127462, 5.72999657326\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 4, 2: 6, 3: 18, 4: 5, 5: 13, 6: 6, 7: 1, 8: 15, 9: 9, 10: 11}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.63344544297061744, 0.62483167833666775, 0.51251457666661449, 0.64745511817217183, 0.51297304102247243, 0.56381742083244368, 0.51639500979924036, 0.54003661009449044, 0.65925301539513925, 0.50865893065384515]\n",
      "ADAS mse: [44.235897662212537, 47.793012518931192, 71.20528100745932, 57.459897803981725, 69.474790343002709, 64.051604098993053, 68.745965493260584, 60.841404645453423, 62.221320292703389, 58.464682128269217]\n",
      "ADAS means: 0.571938084394, 60.4493855994\n",
      "\n",
      "MMSE corr: [0.61363525679225617, 0.48864191628947273, 0.49197569978535111, 0.47430960871624889, 0.44577280817320697, 0.57384763807242156, 0.4350889134188935, 0.51856034348004321, 0.51137625769720474, 0.39920058147044757]\n",
      "MMSE mse: [4.9755798967908706, 5.7150817621800671, 5.8146841303542729, 6.2318842955232965, 6.1465496620513367, 5.2971639873113041, 5.9098363477363574, 6.0615860301593836, 5.7383199396959084, 6.739416521841826]\n",
      "MMSE means: 0.49524090239, 5.86301025736\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 4, 2: 6, 3: 15, 4: 0, 5: 13, 6: 6, 7: 1, 8: 13, 9: 4, 10: 11}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.63287769974602659, 0.62483167833666775, 0.51595417074424821, 0.64526781113852183, 0.50920600580298936, 0.57650055941932499, 0.52354875796320455, 0.54020208460706232, 0.65606428807770101, 0.50218963701641406]\n",
      "ADAS mse: [52.514386278829612, 47.793012518931192, 71.36181736918239, 64.131201614012639, 72.399162829657811, 67.812128851250904, 75.034668779511094, 62.350713723278083, 65.478725263568805, 73.594818112336455]\n",
      "ADAS means: 0.572664269285, 65.2470635341\n",
      "\n",
      "MMSE corr: [0.60749257796909639, 0.48864191628947273, 0.49426308873333508, 0.49637520337321672, 0.46253963423300537, 0.59021042640497134, 0.43362795696223955, 0.5175589855549344, 0.51275904303937947, 0.39572362311359088]\n",
      "MMSE mse: [4.9239902860419216, 5.7150817621800671, 5.6294528137968616, 5.5149599161452558, 5.9413079955735073, 4.9223392791470149, 5.77410701094104, 5.5406953054447952, 5.5235592629567121, 6.6044060262159165]\n",
      "MMSE means: 0.499919245567, 5.60898996584\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 9, 2: 6, 3: 18, 4: 9, 5: 9, 6: 8, 7: 18, 8: 17, 9: 5, 10: 10}\n",
      "Exp13_MC 4 CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.55728508312681579, 0.51287097223915101, 0.55380193426790347, 0.5770834981216324, 0.54746986474909942, 0.4980230507777727, 0.69459512399237433, 0.60664894411460413, 0.58069933349103597, 0.53304266347084805]\n",
      "ADAS mse: [66.179468714965395, 62.857135100370265, 73.746752693996783, 63.294369853187312, 63.020142197076439, 66.473526833375004, 45.750610155152344, 58.507579641798834, 53.715045641394006, 59.657915728777581]\n",
      "ADAS means: 0.566152046835, 61.320254656\n",
      "\n",
      "MMSE corr: [0.5858694062350247, 0.30637282977661617, 0.54355187504465496, 0.43992358732685177, 0.54537306684952769, 0.53755650264382748, 0.48340433778867697, 0.53432789216272603, 0.47747391899176711, 0.46707585991591677]\n",
      "MMSE mse: [5.3087978528011854, 7.333107403189322, 5.1609857292824017, 6.1294086318049947, 5.7631275766820131, 5.7488349587466905, 6.5292034973489494, 5.8754698218354626, 5.1283417095770085, 5.4454834645082997]\n",
      "MMSE means: 0.492092927674, 5.84227606458\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 5, 2: 2, 3: 6, 4: 14, 5: 4, 6: 17, 7: 9, 8: 12, 9: 8, 10: 3}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.55728508312681579, 0.51287097223915101, 0.55380193426790347, 0.5770834981216324, 0.55020143258804566, 0.4980230507777727, 0.69459512399237433, 0.60233498715787759, 0.58069933349103597, 0.53304266347084805]\n",
      "ADAS mse: [66.179468714965395, 62.857135100370265, 73.746752693996783, 63.294369853187312, 62.921004187583449, 66.473526833375004, 45.750610155152344, 58.485444365209659, 53.715045641394006, 59.657915728777581]\n",
      "ADAS means: 0.565993807923, 61.3081273274\n",
      "\n",
      "MMSE corr: [0.5858694062350247, 0.30637282977661617, 0.54355187504465496, 0.43992358732685177, 0.52651686748594084, 0.53755650264382748, 0.48340433778867697, 0.53757340719558455, 0.47747391899176711, 0.46707585991591677]\n",
      "MMSE mse: [5.3087978528011854, 7.333107403189322, 5.1609857292824017, 6.1294086318049947, 5.9168296600410226, 5.7488349587466905, 6.5292034973489494, 6.0023177229088187, 5.1283417095770085, 5.4454834645082997]\n",
      "MMSE means: 0.49053185924, 5.87033106302\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 5, 2: 2, 3: 6, 4: 14, 5: 4, 6: 17, 7: 9, 8: 15, 9: 8, 10: 3}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.55219296561204978, 0.48684212077576783, 0.53298835189985583, 0.57271033267359217, 0.52210708936909334, 0.51365912087059373, 0.69448263810369182, 0.59605069499433172, 0.56262779153670983, 0.50413855239831007]\n",
      "ADAS mse: [69.896491318279061, 63.599461619402433, 75.832719865929931, 64.880899552151618, 66.076949859106236, 73.867972985695573, 56.349088369826632, 61.637827322494481, 56.528312918117038, 65.198813315997711]\n",
      "ADAS means: 0.553779965823, 65.3868537127\n",
      "\n",
      "MMSE corr: [0.58943853755536801, 0.29640678348017208, 0.54017017034876746, 0.46489861893888584, 0.54788781624326077, 0.53492509199884675, 0.49278124808047408, 0.53721935458699432, 0.4843800738999921, 0.46533044820203429]\n",
      "MMSE mse: [5.0266031770983801, 7.1006468037282131, 4.7656571936459722, 5.7829437387804621, 5.603901268923158, 5.6460871312261744, 6.4114607063112805, 5.7957616883496801, 5.0166490904979915, 5.2643391568671536]\n",
      "MMSE means: 0.495343814333, 5.64140499554\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 1, 2: 11, 3: 13, 4: 3, 5: 14, 6: 12, 7: 8, 8: 18, 9: 1, 10: 12}\n",
      "Exp13_MC 5 CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.59013291413131141, 0.60610647159776243, 0.58668179125515563, 0.62250699996560499, 0.55893956178895154, 0.58485399272535932, 0.54065274419659004, 0.43407493357361354, 0.54511781509715151, 0.59898478735677119]\n",
      "ADAS mse: [54.089994879130167, 51.46967267336948, 59.770245026909514, 61.487389778125014, 51.904405811747942, 68.11244104287789, 63.880031714563643, 77.091711642466208, 68.669590399785761, 56.567197345271389]\n",
      "ADAS means: 0.566805201169, 61.3042680314\n",
      "\n",
      "MMSE corr: [0.58343157617753505, 0.39534098034940407, 0.56198417785141208, 0.57352455247913703, 0.39230746209253603, 0.48792670038666969, 0.48688250947392792, 0.35891797108043072, 0.55688575587988076, 0.49776891516948968]\n",
      "MMSE mse: [5.1010247265901771, 6.9447369713609239, 4.672786597966109, 5.0357504501813697, 6.4197394696497554, 5.858712994400455, 5.5192845851890331, 7.2216735464488337, 6.1052034767078176, 4.8258981635746228]\n",
      "MMSE means: 0.489497060094, 5.77048109821\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 9, 2: 16, 3: 8, 4: 6, 5: 17, 6: 9, 7: 13, 8: 11, 9: 10, 10: 2}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.59013291413131141, 0.61316248356039971, 0.58668179125515563, 0.62250699996560499, 0.55893956178895154, 0.58485399272535932, 0.54065274419659004, 0.43407493357361354, 0.54511781509715151, 0.59898478735677119]\n",
      "ADAS mse: [54.089994879130167, 51.34598672510225, 59.770245026909514, 61.487389778125014, 51.904405811747942, 68.11244104287789, 63.880031714563643, 77.091711642466208, 68.669590399785761, 56.567197345271389]\n",
      "ADAS means: 0.567510802365, 61.2918994366\n",
      "\n",
      "MMSE corr: [0.58343157617753505, 0.40541444026336765, 0.56198417785141208, 0.57352455247913703, 0.39230746209253603, 0.48792670038666969, 0.48688250947392792, 0.35891797108043072, 0.55688575587988076, 0.49776891516948968]\n",
      "MMSE mse: [5.1010247265901771, 7.0881392820112419, 4.672786597966109, 5.0357504501813697, 6.4197394696497554, 5.858712994400455, 5.5192845851890331, 7.2216735464488337, 6.1052034767078176, 4.8258981635746228]\n",
      "MMSE means: 0.490504406085, 5.78482132927\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 9, 2: 8, 3: 8, 4: 6, 5: 17, 6: 9, 7: 13, 8: 11, 9: 10, 10: 2}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.59791857341664378, 0.59040516703716817, 0.54447564910660295, 0.62233053330356003, 0.54892785797905941, 0.57285396550289702, 0.52502250278316021, 0.43142935617565881, 0.51364023321731067, 0.57987051955556235]\n",
      "ADAS mse: [65.311735184681893, 55.695660148925299, 68.485005985367266, 61.680130465000389, 53.34385545933754, 71.127027573692942, 70.567977933238836, 77.496849443778018, 75.777716138368959, 62.212759968104066]\n",
      "ADAS means: 0.552687435808, 66.16987183\n",
      "\n",
      "MMSE corr: [0.59089106781405709, 0.39501817289948798, 0.56345604789346138, 0.57374484511023705, 0.40475455762637519, 0.49842302876343247, 0.50736170720612883, 0.35414329383645998, 0.59466183829477459, 0.51372554966314199]\n",
      "MMSE mse: [4.9530570538344554, 6.8599938509957177, 4.6453195019043898, 4.9585148382706077, 6.1243006147392309, 5.6251023197162739, 5.353189909581352, 7.2206901687328466, 5.5856809582080604, 4.6854686020424952]\n",
      "MMSE means: 0.499618010911, 5.6011317818\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 6, 2: 15, 3: 18, 4: 8, 5: 7, 6: 6, 7: 9, 8: 10, 9: 8, 10: 3}\n",
      "Exp13_MC 6 CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.45216759511640353, 0.6676016388355549, 0.55813312215945998, 0.58549419668896974, 0.57288550737014476, 0.61880756458149866, 0.55835920287006457, 0.57513613997590152, 0.54637602352756631, 0.52610645640456211]\n",
      "ADAS mse: [74.895571596822691, 48.93679943284031, 50.229065160489121, 59.540516555236735, 69.800001091552431, 56.04740249818434, 68.254673203385963, 67.211815310407232, 51.688396783758456, 62.513079753305398]\n",
      "ADAS means: 0.566106744753, 60.9117321386\n",
      "\n",
      "MMSE corr: [0.54006098807608915, 0.58047582876770487, 0.42821344182449683, 0.45503627064666113, 0.60280471424777016, 0.41776191627936426, 0.41523575843103866, 0.50522697447664633, 0.44586573807162255, 0.50147031911788076]\n",
      "MMSE mse: [5.1694220622122637, 4.3433763439704265, 5.8427970781345007, 6.3786403381101699, 5.1772475780551517, 5.968008551688051, 7.1363624979308513, 6.2064365218538207, 6.4907010836729224, 5.2963825124198847]\n",
      "MMSE means: 0.489215194994, 5.8009374568\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 13, 2: 7, 3: 5, 4: 10, 5: 13, 6: 8, 7: 2, 8: 3, 9: 13, 10: 0}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.45216759511640353, 0.6676016388355549, 0.55608308562216546, 0.58549419668896974, 0.57288550737014476, 0.61880756458149866, 0.56405565991664108, 0.57513613997590152, 0.54637602352756631, 0.52767234793733864]\n",
      "ADAS mse: [74.895571596822691, 48.93679943284031, 50.221274269528344, 59.540516555236735, 69.800001091552431, 56.04740249818434, 67.974543393358189, 67.211815310407232, 51.688396783758456, 62.454100584778203]\n",
      "ADAS means: 0.566627975957, 60.8770421516\n",
      "\n",
      "MMSE corr: [0.54006098807608915, 0.58047582876770487, 0.42057994535048032, 0.45503627064666113, 0.60280471424777016, 0.41776191627936426, 0.40600574535369799, 0.50522697447664633, 0.44586573807162255, 0.49270919936672425]\n",
      "MMSE mse: [5.1694220622122637, 4.3433763439704265, 5.9165262584176981, 6.3786403381101699, 5.1772475780551517, 5.968008551688051, 7.6171152189952736, 6.2064365218538207, 6.4907010836729224, 5.5180319187979912]\n",
      "MMSE means: 0.486652732064, 5.87855058758\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 13, 2: 7, 3: 2, 4: 10, 5: 13, 6: 8, 7: 3, 8: 3, 9: 13, 10: 7}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.46777234596393974, 0.64078730312065657, 0.55550127386945891, 0.56806181475871997, 0.55811200186088639, 0.60543205505203235, 0.54406428524747175, 0.54152641099286025, 0.53560239247479691, 0.53725589247047678]\n",
      "ADAS mse: [78.864376138659281, 59.598333204444408, 58.997728738941497, 61.955580601629826, 78.419505925380449, 61.558485694071429, 70.079249263311553, 76.47946212455301, 60.277627859075345, 74.056504735265108]\n",
      "ADAS means: 0.555411577581, 68.0286854285\n",
      "\n",
      "MMSE corr: [0.54154068728309679, 0.59337212213090729, 0.42968078891827299, 0.49300718521180065, 0.6161128673266526, 0.42731729468338275, 0.39612172206008628, 0.50097412786200834, 0.46169342968373506, 0.52123722247888327]\n",
      "MMSE mse: [5.1592980262930395, 4.185806336318648, 5.81778010020476, 6.0130118961530377, 5.0018910462603721, 5.9327440754237504, 6.8921960819908552, 6.1166228104306795, 6.1493106055773357, 5.2357038528250008]\n",
      "MMSE means: 0.498105744764, 5.65043648315\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 18, 2: 16, 3: 16, 4: 9, 5: 9, 6: 18, 7: 12, 8: 10, 9: 19, 10: 8}\n",
      "Exp13_MC 7 CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.42384983340295646, 0.52639254520623291, 0.63187263694836893, 0.62964166128380616, 0.59923129912908879, 0.56920905918153319, 0.55435559785767929, 0.57035627021984048, 0.56640915168837525, 0.56955768799793283]\n",
      "ADAS mse: [69.320470145993454, 64.238524311650806, 53.956804913908975, 59.660926348886015, 54.329474782967701, 61.413736711143123, 60.88059808391538, 62.601361932316038, 61.465129494300349, 63.258760056102574]\n",
      "ADAS means: 0.564087574292, 61.1125786781\n",
      "\n",
      "MMSE corr: [0.40800572296740967, 0.3943353510504683, 0.60861874886996203, 0.53618917849395653, 0.50470630601143462, 0.54378211351216965, 0.44358821501719237, 0.45574264905913092, 0.51109333547436064, 0.51125196304891096]\n",
      "MMSE mse: [6.3786796818499329, 6.8410059799546659, 4.8137034349050127, 5.7886096262718505, 5.1353916969819426, 5.8713849265940663, 5.9368568299764846, 6.559836046589945, 5.4618966323431595, 5.7899437750574201]\n",
      "MMSE means: 0.49173135835, 5.85773086305\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 10, 2: 8, 3: 6, 4: 9, 5: 6, 6: 18, 7: 10, 8: 16, 9: 0, 10: 9}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.42384983340295646, 0.52639254520623291, 0.63141046693300462, 0.62964166128380616, 0.59923129912908879, 0.56920905918153319, 0.55397480036623903, 0.57035627021984048, 0.56640915168837525, 0.56955768799793283]\n",
      "ADAS mse: [69.320470145993454, 64.238524311650806, 53.856444384179632, 59.660926348886015, 54.329474782967701, 61.413736711143123, 60.806555679064452, 62.601361932316038, 61.465129494300349, 63.258760056102574]\n",
      "ADAS means: 0.564003277541, 61.0951383847\n",
      "\n",
      "MMSE corr: [0.40800572296740967, 0.3943353510504683, 0.60843536856757874, 0.53618917849395653, 0.50470630601143462, 0.54378211351216965, 0.44822982822978258, 0.45574264905913092, 0.51109333547436064, 0.51125196304891096]\n",
      "MMSE mse: [6.3786796818499329, 6.8410059799546659, 5.018113261304582, 5.7886096262718505, 5.1353916969819426, 5.8713849265940663, 6.8140209226191892, 6.559836046589945, 5.4618966323431595, 5.7899437750574201]\n",
      "MMSE means: 0.492177181642, 5.96588825496\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 10, 2: 8, 3: 7, 4: 9, 5: 6, 6: 18, 7: 16, 8: 16, 9: 0, 10: 9}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.43469591315832579, 0.50872021166169534, 0.63187263694836893, 0.58962999583038611, 0.60027207018504336, 0.55695395812242809, 0.54352243838580017, 0.55036400785045936, 0.55601166421044046, 0.56677396304818273]\n",
      "ADAS mse: [81.523828557562908, 69.666896711848949, 53.956804913908975, 68.40420668101919, 69.347116190064554, 65.075004650992568, 62.420927369183552, 68.082643176728382, 63.09315216902074, 66.735588178087909]\n",
      "ADAS means: 0.55388168594, 66.8306168598\n",
      "\n",
      "MMSE corr: [0.40712018093375918, 0.39877242792548201, 0.60861874886996203, 0.52903191066448252, 0.53165887458511674, 0.5519309189604612, 0.44015277747087866, 0.47319568183144006, 0.50754528185028036, 0.51509144765344361]\n",
      "MMSE mse: [5.8162674161454992, 6.7263446709916872, 4.8137034349050127, 5.6829652453488428, 4.8949326699464244, 5.3125431922262578, 5.9303279576163517, 6.1718968104850411, 5.3171095273076325, 5.5486050402071951]\n",
      "MMSE means: 0.496311825075, 5.62146959652\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 13, 2: 1, 3: 6, 4: 10, 5: 9, 6: 0, 7: 10, 8: 9, 9: 13, 10: 8}\n",
      "Exp13_MC 8 CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.48285878952213135, 0.56607905666748204, 0.62507271440314127, 0.56884585557340961, 0.49996804740593087, 0.54698909476733737, 0.63220332063231577, 0.66849987010399337, 0.5477164525695124, 0.45914022086176115]\n",
      "ADAS mse: [74.300474008009729, 59.708259501782365, 62.218994262536476, 69.172112212908161, 54.106193615074403, 65.350042734326649, 52.401201960593319, 55.048274995760124, 55.023149235438169, 69.724628924950522]\n",
      "ADAS means: 0.559737342251, 61.7053331451\n",
      "\n",
      "MMSE corr: [0.40458234849364166, 0.54353332490812967, 0.63680510737916629, 0.46446780234636903, 0.39223470211895611, 0.53773456860080826, 0.39377747267690533, 0.61642581338909208, 0.47088278723629079, 0.47014216024516636]\n",
      "MMSE mse: [6.6684217977219706, 5.572640577924413, 4.4878342956396242, 7.3070738894560225, 6.4516714039422975, 5.4842144504736474, 6.5062847666546046, 4.4756909324713172, 6.1555140298555502, 4.6166974105134226]\n",
      "MMSE means: 0.493058608739, 5.77260435547\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 5, 2: 7, 3: 9, 4: 8, 5: 5, 6: 8, 7: 1, 8: 5, 9: 17, 10: 15}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.48285878952213135, 0.56607905666748204, 0.62507271440314127, 0.56884585557340961, 0.49996804740593087, 0.54698909476733737, 0.6326243183468907, 0.66849987010399337, 0.5477164525695124, 0.45914022086176115]\n",
      "ADAS mse: [74.300474008009729, 59.708259501782365, 62.218994262536476, 69.172112212908161, 54.106193615074403, 65.350042734326649, 52.28299369934377, 55.048274995760124, 55.023149235438169, 69.724628924950522]\n",
      "ADAS means: 0.559779442022, 61.693512319\n",
      "\n",
      "MMSE corr: [0.40458234849364166, 0.54353332490812967, 0.63680510737916629, 0.46446780234636903, 0.39223470211895611, 0.53773456860080826, 0.39676615407076138, 0.61642581338909208, 0.47088278723629079, 0.47014216024516636]\n",
      "MMSE mse: [6.6684217977219706, 5.572640577924413, 4.4878342956396242, 7.3070738894560225, 6.4516714039422975, 5.4842144504736474, 6.6827708092191438, 4.4756909324713172, 6.1555140298555502, 4.6166974105134226]\n",
      "MMSE means: 0.493357476879, 5.79025295972\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 5, 2: 7, 3: 9, 4: 8, 5: 5, 6: 8, 7: 19, 8: 5, 9: 17, 10: 15}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.4781037128897489, 0.5639459118395026, 0.62507271440314127, 0.56088035852642137, 0.50440257935328514, 0.5389438432225061, 0.61412978935429852, 0.66136656291556684, 0.51002527540396758, 0.45820789995724048]\n",
      "ADAS mse: [75.166664859649828, 60.124960553416884, 62.218994262536476, 71.361563197821553, 55.417416301139632, 66.050570864342689, 60.840419850672902, 68.173608841364967, 76.553347226268173, 71.598642534575788]\n",
      "ADAS means: 0.551507864787, 66.7506188492\n",
      "\n",
      "MMSE corr: [0.41670336687635723, 0.54590573445533008, 0.63680510737916629, 0.47005198003556059, 0.39580316377906799, 0.53688598376697572, 0.39332687660665494, 0.62679498965543956, 0.44221400693743212, 0.48401129513850688]\n",
      "MMSE mse: [6.5923388050334664, 5.461434300831689, 4.4878342956396242, 6.9775028345939392, 6.0319993503540026, 5.4568964670013296, 6.3976221552155366, 4.3501297138945079, 6.10490999248342, 4.5015601429572563]\n",
      "MMSE means: 0.494850250463, 5.6362228058\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 5, 2: 2, 3: 9, 4: 13, 5: 8, 6: 8, 7: 16, 8: 3, 9: 12, 10: 19}\n",
      "Exp13_MC 9 CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.52682047474995974, 0.55316903554764407, 0.52587116511942622, 0.54205029258016235, 0.61111086381168256, 0.63875240789462695, 0.57920678533109726, 0.52384437030235698, 0.63061877996467197, 0.57774513623257351]\n",
      "ADAS mse: [63.151874079527062, 72.987587336036299, 74.961306753152172, 53.168765005348348, 44.336529831366846, 63.81724907909576, 50.213090669840852, 63.221470008155769, 58.836204997759594, 60.317609711982882]\n",
      "ADAS means: 0.570918931153, 60.5011687472\n",
      "\n",
      "MMSE corr: [0.4892722024045289, 0.58996686025908551, 0.46797794402023263, 0.45223567474804227, 0.52965588443959, 0.60841711129793896, 0.35226832972167366, 0.43469863961346589, 0.50925803837453665, 0.43234312784699036]\n",
      "MMSE mse: [5.2082677554288557, 6.0932865813158088, 6.2345485711192445, 4.7561941902065241, 4.832988411246121, 4.8506850324444768, 7.2322717772394141, 6.2431215151193182, 6.1602270505462178, 6.0841822256571279]\n",
      "MMSE means: 0.486609381273, 5.76957731103\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 10, 2: 7, 3: 9, 4: 10, 5: 12, 6: 7, 7: 15, 8: 14, 9: 9, 10: 17}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.52682047474995974, 0.55316903554764407, 0.52587116511942622, 0.54205029258016235, 0.61111086381168256, 0.63875240789462695, 0.58169498256804653, 0.52384437030235698, 0.63061877996467197, 0.57774513623257351]\n",
      "ADAS mse: [63.151874079527062, 72.987587336036299, 74.961306753152172, 53.168765005348348, 44.336529831366846, 63.81724907909576, 50.100071994994984, 63.221470008155769, 58.836204997759594, 60.317609711982882]\n",
      "ADAS means: 0.571167750877, 60.4898668797\n",
      "\n",
      "MMSE corr: [0.4892722024045289, 0.58996686025908551, 0.46797794402023263, 0.45223567474804227, 0.52965588443959, 0.60841711129793896, 0.35272226848020766, 0.43469863961346589, 0.50925803837453665, 0.43234312784699036]\n",
      "MMSE mse: [5.2082677554288557, 6.0932865813158088, 6.2345485711192445, 4.7561941902065241, 4.832988411246121, 4.8506850324444768, 7.5429299142603075, 6.2431215151193182, 6.1602270505462178, 6.0841822256571279]\n",
      "MMSE means: 0.486654775148, 5.80064312473\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 10, 2: 7, 3: 9, 4: 10, 5: 12, 6: 7, 7: 18, 8: 14, 9: 9, 10: 17}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.46400032475150194, 0.53802529364640472, 0.49205007678616836, 0.52124653354293571, 0.61846998030927958, 0.63250354808750908, 0.57905362393022086, 0.51966532227149465, 0.59872167014389166, 0.57658330644644828]\n",
      "ADAS mse: [70.784441718092964, 75.181874549668095, 78.603892548074526, 54.889679115765986, 45.974275125718862, 64.607129080602277, 51.024682197198885, 67.901458878506702, 68.443475346070187, 66.801467155134489]\n",
      "ADAS means: 0.554031967992, 64.4212375715\n",
      "\n",
      "MMSE corr: [0.51744210565910731, 0.60722766294647634, 0.47786280703235406, 0.46542241835151338, 0.56033410919273285, 0.61914970469129416, 0.35620327594555956, 0.42150140478312781, 0.5031898781376597, 0.4340672767847813]\n",
      "MMSE mse: [4.9120593603898843, 5.878195868131761, 6.0556400233805716, 4.6981182964684365, 4.5458203043061234, 4.6471868633521431, 7.1361681987465468, 6.203056389391322, 6.0268299700024057, 5.9077023830920679]\n",
      "MMSE means: 0.496240064352, 5.60107776573\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 8, 2: 8, 3: 16, 4: 12, 5: 3, 6: 5, 7: 14, 8: 10, 9: 13, 10: 13}\n",
      "Exp13_MC 10 CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.55880361204381024, 0.49759635729059415, 0.52644470335288485, 0.57773590559022769, 0.54971300778501686, 0.57080735240272018, 0.59424234225097583, 0.58615117280662099, 0.5861677922253733, 0.58655408146714105]\n",
      "ADAS mse: [66.918304215605133, 63.856008368931093, 70.922085658103512, 56.928367160773156, 54.280029705523106, 72.231819460190877, 52.986234937041509, 57.907535054499554, 60.809260059068755, 56.151000046087013]\n",
      "ADAS means: 0.563421632722, 61.2990644666\n",
      "\n",
      "MMSE corr: [0.50421919650524594, 0.59595327930148312, 0.45696848728330547, 0.4540098088436732, 0.36326884548522359, 0.57581793551883476, 0.45993618055527685, 0.52252747845264236, 0.41607989623206015, 0.57330076796046248]\n",
      "MMSE mse: [5.5347873880321155, 4.426033610455451, 6.2383133440544043, 7.387483625448902, 6.689808235574926, 5.785570864435452, 5.8087043388639277, 5.0852240486944282, 6.530206375787893, 4.635949128165521]\n",
      "MMSE means: 0.492208187614, 5.81220809595\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 2, 2: 13, 3: 9, 4: 15, 5: 10, 6: 0, 7: 18, 8: 12, 9: 2, 10: 9}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.55880361204381024, 0.4947734324614389, 0.52968197503849668, 0.57773590559022769, 0.54971300778501686, 0.57080735240272018, 0.59061450321272801, 0.58615117280662099, 0.5861677922253733, 0.58655408146714105]\n",
      "ADAS mse: [66.918304215605133, 63.851379812756434, 70.779400417422735, 56.928367160773156, 54.280029705523106, 72.231819460190877, 52.850657430479039, 57.907535054499554, 60.809260059068755, 56.151000046087013]\n",
      "ADAS means: 0.563100283503, 61.2707753362\n",
      "\n",
      "MMSE corr: [0.50421919650524594, 0.59348676434110237, 0.44696600712183487, 0.4540098088436732, 0.36326884548522359, 0.57581793551883476, 0.4554983701679291, 0.52252747845264236, 0.41607989623206015, 0.57330076796046248]\n",
      "MMSE mse: [5.5347873880321155, 4.4904148094335232, 6.4298215718121412, 7.387483625448902, 6.689808235574926, 5.785570864435452, 5.9576709829906669, 5.0852240486944282, 6.530206375787893, 4.635949128165521]\n",
      "MMSE means: 0.490517507063, 5.85269370304\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 2, 2: 14, 3: 8, 4: 15, 5: 10, 6: 0, 7: 16, 8: 12, 9: 2, 10: 9}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.55713997325863474, 0.49103031494239957, 0.51944575032102747, 0.56711565452650903, 0.54069265253914112, 0.56750453530902589, 0.59702196850169731, 0.57069208603975874, 0.56284028309588641, 0.58469892723886951]\n",
      "ADAS mse: [82.850454091665839, 66.256489570019539, 76.408110047053555, 80.156713708440066, 56.449403448959494, 74.748529950885228, 54.328443577132951, 63.218230314732494, 74.187395196307278, 57.864638281737186]\n",
      "ADAS means: 0.555818214577, 68.6468408187\n",
      "\n",
      "MMSE corr: [0.52342720823447964, 0.60387187330772407, 0.45894269540950966, 0.45233507285687202, 0.38209378469720273, 0.58286414788093965, 0.46127764712786185, 0.54885408523380796, 0.42978888050356928, 0.5722666277324735]\n",
      "MMSE mse: [5.4998245431780175, 4.3659497683701627, 6.1164821252395862, 7.1305001783517969, 5.9842927472947487, 5.6517105091229762, 5.660963231089104, 4.9595598219596591, 6.3547136673795803, 4.5926560000636281]\n",
      "MMSE means: 0.501572202298, 5.6316652592\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 4: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 9: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 100, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 200, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.0001}, 'lr': {'HC': 2, 'COMB': 1, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {1: 8, 2: 9, 3: 2, 4: 9, 5: 6, 6: 6, 7: 1, 8: 19, 9: 12, 10: 3}\n"
     ]
    }
   ],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "#exp_name = 'Exp11'\n",
    "niter = 80000\n",
    "# modality = 'HC_CT'\n",
    "start_fold = 1\n",
    "n_folds = 10\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "#preproc = 'no_preproc'\n",
    "#batch_size = 256\n",
    "snap_interval = 4000\n",
    "snap_start = 4000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "#Clinical_Scale = 'ADAS13'\n",
    "\n",
    "MC_list = np.arange(1,11,1)\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "idx = 0  \n",
    "df_perf_dict_adas = {}\n",
    "df_perf_dict_mmse = {}\n",
    "df_perf_dict_adas_tuned = {}\n",
    "df_perf_dict_mmse_tuned = {}\n",
    "df_perf_dict_opt = {}\n",
    "\n",
    "for mc in MC_list:\n",
    "    print exp_name, mc, modality\n",
    "\n",
    "    for hype in hype_configs.keys():      \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "\n",
    "        for fid in fid_list:\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "            #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "            #with open(test_filename_txt, 'w') as f:\n",
    "            #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                    f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC']\n",
    "                elif modality == 'CT':\n",
    "                    f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_CT_SpecCluster_dyn']\n",
    "                elif modality == 'HC_CT':\n",
    "                    f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC','X_CT_SpecCluster_dyn']\n",
    "                elif modality == 'HC_CT_unified_hyp1':\n",
    "                    f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_HC_CT']\n",
    "                else:\n",
    "                    print 'Wrong modality'\n",
    "\n",
    "            #print 'Hype # {}, MC # {}, Fold # {}, Clinical_Scale {}'.format(hype, mc, fid, Clinical_Scale)\n",
    "            data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            if Clinical_Scale == 'ADAS13':\n",
    "                act_scores = load_data(data_path, 'adas','no_preproc')\n",
    "            elif Clinical_Scale == 'MMSE': \n",
    "                act_scores = load_data(data_path, 'mmse','no_preproc')\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                act_scores_adas13 = load_data(data_path, 'adas','no_preproc')\n",
    "                act_scores_mmse = load_data(data_path, 'mmse','no_preproc')\n",
    "            else:\n",
    "                print 'unknown clinical scale'\n",
    "\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort)\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            sub.call([\"cp\", baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort), baseline_dir + \n",
    "                      'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)])\n",
    "            #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "            #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "\n",
    "            if Clinical_Scale in ['ADAS13', 'MMSE']:                \n",
    "                multi_task = False\n",
    "                cs_list = ['opt']\n",
    "                iter_euLoss = []\n",
    "                iter_r = []        \n",
    "                iter_pred_scores = []\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = np.squeeze(results['X_out'])            \n",
    "                    iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                    iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "                fold_r[config_idx] = np.array(iter_r)\n",
    "                fold_act_scores[fid] = act_scores\n",
    "                fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                multi_task = True\n",
    "                cs_list = ['adas','mmse']\n",
    "                iter_euLoss_adas13 = []\n",
    "                iter_r_adas13 = []        \n",
    "                iter_pred_scores_adas13 = []\n",
    "                iter_euLoss_mmse = []\n",
    "                iter_r_mmse = []        \n",
    "                iter_pred_scores_mmse = []\n",
    "\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = results['X_out']   \n",
    "                    encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                    encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                    iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                    iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                    iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                    iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "                fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "                fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "                fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "                fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "                fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "                fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "                fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "\n",
    "    \n",
    "    if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "        fold_perf_dict = {'fold_r':fold_r,'fold_euLoss':fold_euLoss,'fold_act_scores':fold_act_scores,'fold_pred_scores':fold_pred_scores}\n",
    "    else: \n",
    "        fold_perf_dict = {'fold_r_adas13':fold_r_adas13,'fold_r_mmse':fold_r_mmse,'fold_euLoss_adas13':fold_euLoss_adas13,'fold_euLoss_mmse':fold_euLoss_mmse,\n",
    "                         'fold_act_scores_adas13':fold_act_scores_adas13,'fold_act_scores_mmse':fold_act_scores_mmse,\n",
    "                          'fold_pred_scores_adas13':fold_pred_scores_adas13,'fold_pred_scores_mmse':fold_pred_scores_mmse}\n",
    "        \n",
    "    opt_metric = 'euLoss' #euLoss or corr or both\n",
    "    #task_weights = {'ADAS':1,'MMSE':0} \n",
    "    save_multitask_results = False\n",
    "    CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/'  \n",
    "    save_path = '{}{}_{}_NN_{}'.format(CV_model_dir,exp_name,mc,modality)\n",
    "    \n",
    "    # Create dictionaries of perfromance based on differnt task weights \n",
    "    for key, task_weights in {'adas':{'ADAS':1,'MMSE':0},'mmse':{'ADAS':0,'MMSE':1},'both':{'ADAS':1,'MMSE':1}}.items():\n",
    "        print 'Weights Tuned for: {}'.format(key)                              \n",
    "        results = compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path)  \n",
    "\n",
    "        # populate the perf dictionary for 10 MC x 10 folds\n",
    "        model_choice = 'APANN'    \n",
    "        for f, fid in enumerate(fid_list): \n",
    "            if key in ['adas','mmse']:\n",
    "                r_valid = results['{}_r'.format(key)][f]\n",
    "                MSE_valid = results['{}_mse'.format(key)][f]\n",
    "                RMSE_valid = results['{}_rmse'.format(key)][f]\n",
    "\n",
    "                if key == 'adas':\n",
    "                    df_perf_dict_adas_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                elif key == 'mmse':\n",
    "                    df_perf_dict_mmse_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                else: \n",
    "                    print 'unknown key'\n",
    "\n",
    "            elif key == 'both':                    \n",
    "                for cs in cs_list:            \n",
    "                    r_valid = results['{}_r'.format(cs)][f]\n",
    "                    MSE_valid = results['{}_mse'.format(cs)][f]\n",
    "                    RMSE_valid = results['{}_rmse'.format(cs)][f]\n",
    "\n",
    "                    if cs == 'adas':\n",
    "                        df_perf_dict_adas[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                    elif cs == 'mmse':\n",
    "                        df_perf_dict_mmse[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}                    \n",
    "                    else: \n",
    "                        print 'unknown key'\n",
    "\n",
    "\n",
    "            else: #single task dictionary\n",
    "                df_perf_dict_opt[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "\n",
    "            idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold10/train_snaps/Exp13_MC_hyp2_CT_iter_80000.caffemodel'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results at: /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/\n"
     ]
    }
   ],
   "source": [
    "#Save df style dictionaly for seaborn plots\n",
    "cohort = 'ADNI1and2'\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13.pkl'.format(exp_name, cohort,modality)                \n",
    "pickleIt(df_perf_dict_adas,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE.pkl'.format(exp_name, cohort,modality)  \n",
    "pickleIt(df_perf_dict_mmse,df_perf_dict_path)\n",
    "\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_tuned.pkl'.format(exp_name,cohort,modality)                \n",
    "pickleIt(df_perf_dict_adas_tuned,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE_tuned.pkl'.format(exp_name, cohort,modality)  \n",
    "pickleIt(df_perf_dict_mmse_tuned,df_perf_dict_path)\n",
    "\n",
    "\n",
    "print 'saving results at: {}'.format(CV_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 25)\n",
    "n_rows = n_folds\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "plot_perf = False\n",
    "\n",
    "if multi_task:\n",
    "    euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "    corrs = [fold_r_adas13,fold_r_mmse]\n",
    "else:\n",
    "    euLosses = [fold_euLoss]\n",
    "    corrs = [fold_r]\n",
    "    \n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        \n",
    "        if plot_perf:\n",
    "            plt.figure(fid)\n",
    "            plt.subplot(n_rows,n_cols,1)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "            plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "            plt.legend()\n",
    "            plt.subplot(n_rows,n_cols,2)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "            plt.title('correlation, fid: {}'.format(fid))\n",
    "            plt.legend(loc=2)\n",
    "\n",
    "print preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path):\n",
    "    fold_r_dict = {}\n",
    "    fold_euLoss_dict = {}\n",
    "    fold_act_scores_dict = {}\n",
    "    fold_pred_scores_dict = {}\n",
    "\n",
    "    if multi_task:\n",
    "        fold_r_dict['ADAS'] = fold_perf_dict['fold_r_adas13']\n",
    "        fold_r_dict['MMSE'] = fold_perf_dict['fold_r_mmse']\n",
    "        fold_euLoss_dict['ADAS'] = fold_perf_dict['fold_euLoss_adas13']\n",
    "        fold_euLoss_dict['MMSE'] = fold_perf_dict['fold_euLoss_mmse']\n",
    "        fold_act_scores_dict['ADAS'] = fold_perf_dict['fold_act_scores_adas13']\n",
    "        fold_act_scores_dict['MMSE'] = fold_perf_dict['fold_act_scores_mmse']\n",
    "        fold_pred_scores_dict['ADAS'] = fold_perf_dict['fold_pred_scores_adas13']\n",
    "        fold_pred_scores_dict['MMSE'] = fold_perf_dict['fold_pred_scores_mmse']\n",
    "        \n",
    "        NN_results = computePerfMetrics(fold_r_dict, fold_euLoss_dict, fold_act_scores_dict, fold_pred_scores_dict, opt_metric, hype_configs, \n",
    "                                        Clinical_Scale,task_weights)\n",
    "        adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "        mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "        adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "        mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "        adas_rmse = NN_results['opt_ADAS']['CV_RMSE'].values()\n",
    "        mmse_rmse = NN_results['opt_MMSE']['CV_RMSE'].values()\n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['opt_ADAS']['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['opt_ADAS']['actual_CV_scores']\n",
    "        \n",
    "        print 'ADAS corr: {}'.format(adas_r)\n",
    "        print 'ADAS mse: {}'.format(adas_mse)\n",
    "        print 'ADAS means: {}, {}'.format(np.mean(adas_r),np.mean(adas_mse))\n",
    "        print ''\n",
    "        print 'MMSE corr: {}'.format(mmse_r)\n",
    "        print 'MMSE mse: {}'.format(mmse_mse)\n",
    "        print 'MMSE means: {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse))\n",
    "        print ''\n",
    "        print 'opt_hyp: {}'.format(opt_hyp)\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        \n",
    "        return {'adas_r':adas_r, 'adas_mse':adas_mse, 'adas_rmse':adas_rmse, 'mmse_r':mmse_r, 'mmse_mse':mmse_mse, 'mmse_rmse':mmse_rmse,\n",
    "               'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        NN_results = computeSingleTaskPerfMetrics(fold_perf_dict, opt_metric, hype_configs)\n",
    "        opt_r = NN_results['opt_r'].values()        \n",
    "        opt_mse = NN_results['opt_mse'].values()        \n",
    "        opt_rmse = NN_results['opt_rmse'].values()        \n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['actual_CV_scores']\n",
    "        print 'opt corr: {}'.format(opt_r)\n",
    "        print 'opt mse: {}'.format(opt_mse)\n",
    "        print 'means: {}, {}'.format(np.mean(opt_r),np.mean(opt_mse))\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        print ''\n",
    "    \n",
    "        return {'opt_r':opt_r, 'opt_mse':opt_mse, 'opt_rmse':opt_rmse,'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    \n",
    "\n",
    "\n",
    "    if save_multitask_results:\n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')        \n",
    "        save_path = save_path + '_' + st + '.pkl' \n",
    "        print 'saving results at: {}'.format(save_path)\n",
    "        output = open(save_path, 'wb')\n",
    "        pickle.dump(NN_results, output)\n",
    "        output.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predicted_CV_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6f668dc9c41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted_CV_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual_CV_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predicted_CV_scores'"
     ]
    }
   ],
   "source": [
    "a = np.squeeze(results['predicted_CV_scores'][7])\n",
    "b = np.squeeze(results['actual_CV_scores'][7])\n",
    "\n",
    "print a , b                               \n",
    "print stats.pearsonr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    \n",
    "     # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "\n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    opt_rmse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "\n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    opt_rmse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "\n",
    "    print 'Clinical_Scale: {}'.format(Clinical_Scale)\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        print 'This function does not work for single clinical scale models. Use the code from the next cell'\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "\n",
    "        fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "        fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "        fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "        fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "\n",
    "        for fid in fid_hype_map.keys():\n",
    "            r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "            r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "            r_perf_array = np.array(fid_r_perf[fid])\n",
    "            euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "            euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "            euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "            if opt_metric == 'euLoss':\n",
    "                h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "            else:\n",
    "                h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "\n",
    "            opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "            opt_snap[fid] = snp\n",
    "\n",
    "            opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "            opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "            opt_rmse_adas[fid] = np.sqrt(2*euLoss_perf_array_adas[h,snp]) #RMSE\n",
    "            opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "            opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "            opt_rmse_mmse[fid] = np.sqrt(2*euLoss_perf_array_mmse[h,snp]) #RMSE\n",
    "\n",
    "            actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "            opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "            actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "            opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "        opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'CV_RMSE':opt_rmse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "        opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'CV_RMSE':opt_rmse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_snap':opt_snap, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "def computeSingleTaskPerfMetrics(fold_perf_dict,opt_metric,hype_configs):\n",
    "    corrs = fold_perf_dict['fold_r']\n",
    "    euLosses = fold_perf_dict['fold_euLoss']\n",
    "    act_scores = fold_perf_dict['fold_act_scores']\n",
    "    pred_scores = fold_perf_dict['fold_pred_scores']\n",
    "\n",
    "    \n",
    "    NN_multitask_results = {}\n",
    "   \n",
    "    snap_array = np.arange(snap_start,niter+1,snap_start)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = {}\n",
    "    opt_mse = {}\n",
    "    opt_rmse = {}\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "    actual_scores = {}\n",
    "    opt_pred_scores = {}\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "        # if want to find best hyp from mse values\n",
    "        if opt_metric == 'euLoss':\n",
    "            h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        else:\n",
    "            h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "            \n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        r = r_perf_array[h,snp]        \n",
    "        opt_r[fid] = r\n",
    "        opt_mse[fid] = 2*eu_loss\n",
    "        opt_rmse[fid] = np.sqrt(2*eu_loss)\n",
    "        #print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)        \n",
    "        opt_snap[fid] = snp\n",
    "        opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "        actual_scores[fid] = fold_act_scores[fid]\n",
    "        opt_pred_scores[fid] = fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp]\n",
    "\n",
    "    #print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r.values()), np.mean(opt_mse.values()))\n",
    "    #print opt_r\n",
    "    #print opt_mse\n",
    "    return {'opt_r':opt_r,'opt_mse':opt_mse,'opt_rmse':opt_rmse,'opt_hype':opt_hype,'opt_snap':opt_snap, 'actual_scores':actual_scores,'opt_pred_scores':opt_pred_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "\n",
    "model_file = 'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-05-06-17-14-49.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {4:0,5:1}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_ADAS13_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(5.64658243068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update fold performance for multitask\n",
    "print NN_results['opt_ADAS'].keys()\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "#model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl'\n",
    "model_file_soa = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-23-52.pkl'\n",
    "print 'current state of art: ' + model_file_soa\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file_soa,'rb'))\n",
    "\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "#load old file\n",
    "# model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-02-48.pkl'\n",
    "# print 'updated fold result file: ' + model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "idx_pairs = {1:1,3:3}\n",
    "CV_data = update_multifold_perf(boxplots_dir + model_file_soa, NN_results, idx_pairs)\n",
    "\n",
    "print \"\"\n",
    "print 'updated CV_data'\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "#print 'saving results at: {}'.format(save_name)\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(CV_data, output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():        \n",
    "        for idx in idx_pairs.keys():            \n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "\n",
    "    return CV_data\n",
    "\n",
    "def update_multifold_perf(saved_perf_file, new_perf_dict, idx_pairs):    \n",
    "    perf_metrics = ['CV_r', 'actual_CV_scores', 'CV_MSE', 'predicted_CV_scores']    \n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        if key == 'opt_hype':\n",
    "            for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                CV_data[key][idx_key] = new_perf_dict[key][idx_val]\n",
    "                \n",
    "        else:\n",
    "            for pm in perf_metrics:\n",
    "                for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                    CV_data[key][pm][idx_key] = new_perf_dict[key][pm][idx_val]\n",
    "    \n",
    "    return CV_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "model_files = ['Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl',             \n",
    "             'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-05-23-07-31-29.pkl']\n",
    "\n",
    "#'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-03-19-29-16_Fold9_10.pkl'\n",
    "\n",
    "Clinical_Scale = 'ADAS'\n",
    "perf_array = []\n",
    "for mf in model_files:\n",
    "    CV_data = pickle.load(open(boxplots_dir + mf,'rb'))['opt_{}'.format(Clinical_Scale)]        \n",
    "    perf_array.append(CV_data['CV_r'].values())\n",
    "\n",
    "print (np.max(np.array(perf_array),axis=0))\n",
    "print np.mean(np.max(np.array(perf_array),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "\n",
    "\n",
    "save_detailed_results = True\n",
    "multi_task = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results    \n",
    "    if not multi_task:\n",
    "        NN_results = {}\n",
    "        NN_results['CV_MSE'] = opt_mse\n",
    "        NN_results['CV_r'] = opt_r\n",
    "        NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "        NN_results['actual_CV_scores'] = actual_scores\n",
    "        NN_results['tid_snap_config_dict'] = opt_hype\n",
    "        print opt_r\n",
    "        print opt_mse\n",
    "        \n",
    "    else:\n",
    "        NN_results = NN_multitask_results\n",
    "        print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "        print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "        print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "        print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])\n",
    "        \n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_BOTH_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    for modality in modalities:\n",
    "        in_data = load_data(in_data_path, 'Fold_{}_{}'.format(fid,modality), preproc)         \n",
    "\n",
    "        # HDF5 is pretty efficient, but can be further compressed.\n",
    "        comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "        with h5py.File(out_data_path, 'a') as f:      \n",
    "            if modality == 'X_R_CT': #Fix the typo            \n",
    "                modality = 'X_CT'            \n",
    "\n",
    "            f.create_dataset('{}'.format(modality), data=in_data, **comp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp6'\n",
    "exp_name_out = 'Exp6_MC'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "modalities = ['X_L_HC','X_R_HC','X_R_CT','adas','mmse','dx']\n",
    "dataset = 'ADNI1'\n",
    "preproc = 'no_preproc' #binary labels\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "\n",
    "for mc in np.arange(1,3,1):\n",
    "    for fid in np.arange(1,11,1):\n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_train_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_valid_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_ADAS13_NN_valid_MC_{}.h5'.format(exp_name,dataset,mc)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name_out)\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC: fid[1:4] 8000; fid[5,6] 6000, fid[7,8,9]: 8000 fid 10: 6000\n",
    "CT: fid[1:5] 12000 fid[6:10] 6000\n",
    "\n",
    "#spawn net\n",
    "ADNI_FF_train_hyp1_CT.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n"
     ]
    }
   ],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "cohort = 'ADNI2'\n",
    "modality = 'HC_CT'\n",
    "pretrain_snap_HC = 20000 #ADNI1: 5000\n",
    "pretrain_snap_CT = 20000 #ADNI1: 5000\n",
    "n_folds = 10\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/' \n",
    "#Custom snaps per fold\n",
    "#HC_iter = [8000,8000,8000,8000,6000,6000,8000,8000,8000,6000]\n",
    "#CT_iter = [12000,12000,12000,12000,12000,6000,6000,6000,6000,6000]\n",
    "#mc=1\n",
    "hc_hyp = 'hyp1'\n",
    "ct_hyp = 'hyp1'\n",
    "pretrain_hyp = 'hyp2' #This is hyp generated using optimal combination of hc and ct hyps. Prototxt file is saved with this hyp extension\n",
    "\n",
    "exp_name = 'Exp11_MC'\n",
    "\n",
    "for mc in np.arange(6,11,1):\n",
    "    for fid in np.arange(1,n_folds+1,1):\n",
    "        print 'fid: {}'.format(fid)\n",
    "        #for AE_branch in ['CT','R_HC','L_HC']:\n",
    "        for AE_branch in ['CT','HC']:\n",
    "            print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "            if AE_branch == 'L_HC':\n",
    "                params_FF = ['L_ff1', 'L_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'R_HC':\n",
    "                params_FF = ['R_ff1', 'R_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'HC':\n",
    "                params_FF = ['L_ff1','R_ff1']\n",
    "                AE_iter = pretrain_snap_HC\n",
    "                hyp = hc_hyp\n",
    "            elif AE_branch == 'CT':\n",
    "                #params_FF = ['ff1', 'ff2']\n",
    "                params_FF = ['ff1']\n",
    "                AE_iter = pretrain_snap_CT\n",
    "                hyp = ct_hyp\n",
    "                #fid for pretain is 1 because it's same definition for all the folds.\n",
    "                #Only use this during 1 of the modalities to avoid overwritting\n",
    "                print 'Spawning new net'\n",
    "                pretrain_net = caffe.Net(baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,pretrain_hyp,modality), caffe.TRAIN)\n",
    "            else:\n",
    "                print 'Wrong AE branch'\n",
    "\n",
    "            # conv_params = {name: (weights, biases)}\n",
    "            conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "            for conv in params_FF:\n",
    "                print 'target {} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "            # Review AE net params \n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hyp,AE_branch)\n",
    "            #model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "            model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hyp,AE_branch,AE_iter) \n",
    "\n",
    "            AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "            #params_AE = ['encoder1', 'code']\n",
    "            params_AE = params_FF #if you are using pretrained NN\n",
    "\n",
    "            # fc_params = {name: (weights, biases)}\n",
    "            fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "            for fc in params_AE:\n",
    "                print 'pretrained {} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "            #transplant net parameters\n",
    "            for pr, pr_conv in zip(params_AE, params_FF):\n",
    "                conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "                conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "            save_net = True\n",
    "            if save_net:\n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_{}_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                save_path = baseline_dir + net_name.format(mc,fid,cohort,pretrain_hyp,modality,pretrain_snap_HC,pretrain_snap_CT)\n",
    "                print \"Saving net to \" + save_path\n",
    "                pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_data['opt_ADAS']['CV_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print n_snaps/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
