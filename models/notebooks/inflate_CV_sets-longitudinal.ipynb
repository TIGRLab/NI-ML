{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n",
      "/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "#from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import KFold\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import ipyparallel as ipp\n",
    "from functools import partial\n",
    "import collections\n",
    "import tables as tb\n",
    "from math import isnan\n",
    "\n",
    "%matplotlib inline\n",
    "#plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasets\n",
    "cohort = 'ADNI1'\n",
    "exp_name = 'Exp6'\n",
    "atlas = 'SpecCluster'\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "HC_L_data_path = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_left_{}.pkl'.format(cohort,exp_name)\n",
    "HC_R_data_path = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_right_{}.pkl'.format(cohort,exp_name)\n",
    "CT_data_path = baseline_dir + 'CT/civet_out/{}_subject_ROI_CT_dict_{}.pkl'.format(cohort,atlas)\n",
    "ADAS_bl_data_path = baseline_dir + 'CS/{}_BL_PTID_ADAS13_dict.pkl'.format(cohort)\n",
    "ADAS_m12_data_path = baseline_dir + 'CS/{}_m12_PTID_ADAS13_dict.pkl'.format(cohort)\n",
    "CT_unique_ROIs_path = baseline_dir + 'CT/civet_out/ADNI_unique_ROIs_{}.pkl'.format(atlas)\n",
    "DX_data_path = baseline_dir + 'CS/{}_BL_PTID_DX_bl_dict.pkl'.format(cohort)\n",
    "#k-fold indices (from a saved file)\n",
    "#kf_file = \"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_train_valid_KFold_idx.pkl\"\n",
    "#kf_file = \"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_train_valid_KFold_UIDs.pkl\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grab CV data with specific feature columes (independent vars) and specific clinical scale (dependent var)\n",
    "def load_CV_data(sub_list, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, sub_ADAS_dict, sub_MMSE_dict, sub_DX_dict, kf, hdf_file, cohort, min_CT_sampx):\n",
    "    # Decide which input modalities / output variables you want to load and inflate\n",
    "    # Use sub list to verify dictionary entry exists for each in the sub_HC / sub_CT /sub_CS dictionaries\n",
    "    # Grab clinical score for each subject from master_csv table (infile) / or grab it from the sub_CS_dict\n",
    "    # Filter out NANs \n",
    "    # Loop through K-folds (kf_file) and append candidate labels + CT values\n",
    "    # Note: UID = PTID + IID (036_S_0976_I65091)\n",
    "    # cohort = Train, Valid, or Heldout \n",
    "    # Pick any UID to generate roi_list common across all subjects to stay consistents while tranforming dictionary into array\n",
    "    #ordered_roi_list = sub_CT_dict['137_S_0459'][0].keys()\n",
    "    ordered_roi_list = pickle.load( open(CT_unique_ROIs_path, \"rb\" ) )\n",
    "    \n",
    "    # ignore the \"0\" idx along with the 4 missing rois from the mean CT value csv\n",
    "    #ignore_roi_list = [0,29,30,39,40]\n",
    "    \n",
    "    if len(ordered_roi_list) < 100: #AAL\n",
    "        ignore_roi_list = [0] #-1 (and -1+1000) index for the ROIs in the middle\n",
    "    else: #SpecCluster\n",
    "        ignore_roi_list = [-1,999] #-1 (and -1+1000) index for the ROIs in the middle\n",
    "        \n",
    "    for roi in ignore_roi_list:\n",
    "        ordered_roi_list.remove(roi)\n",
    "            \n",
    "\n",
    "    # K-fold list\n",
    "    #kf = pickle.load( open(kf_file, \"rb\" ) )\n",
    "    train_fold_list = []\n",
    "    val_fold_list = []\n",
    "    for train, test in kf:\n",
    "        tmp_train_list = []\n",
    "        for t in train:\n",
    "            tmp_train_list.append(sub_list[t])\n",
    "        train_fold_list.append(tmp_train_list)\n",
    "        \n",
    "        tmp_val_list = []\n",
    "        for t in test:\n",
    "            tmp_val_list.append(sub_list[t])\n",
    "        val_fold_list.append(tmp_val_list)\n",
    "            \n",
    "    if cohort == 'train':        \n",
    "        #train_fold_list= kf['train_UIDs']       \n",
    "        computeTrainingFolds = True\n",
    "        drawSamples = True  #Draw samples or point estimates?\n",
    "        \n",
    "    elif cohort == 'valid':                \n",
    "        #val_fold_list= kf['valid_UIDs']  \n",
    "        computeTrainingFolds = False\n",
    "        computeValidFolds = True\n",
    "        drawSamples = False\n",
    "        \n",
    "    else:        \n",
    "        print 'unknown cohort'    \n",
    "    \n",
    "    runParallel = False\n",
    "    if runParallel: #parallelized version -- doesnt work :-(   \n",
    "        rc = ipp.Client()\n",
    "        #rc.block = False\n",
    "        dview = rc[:]\n",
    "        print dview\n",
    "        dview.push(dict(inflate_fold = inflate_fold))        \n",
    "        dview.push(dict(inflate_subject_samples = inflate_subject_samples))        \n",
    "        mapfunc = partial(inflate_fold, sub_HC_L_dict=sub_HC_L_dict, sub_HC_R_dict=sub_HC_R_dict, sub_CT_dict=sub_CT_dict, \n",
    "                  ordered_roi_list=ordered_roi_list, sub_clinical_scores_dict=sub_clinical_scores_dict)\n",
    "\n",
    "        parallel_result = dview.map_sync(mapfunc, train_fold_list)  \n",
    "        return parallel_result\n",
    "    \n",
    "    else:\n",
    "        fold = 0 \n",
    "        uid_sampx_dict_list = []\n",
    "        \n",
    "        if computeTrainingFolds:             \n",
    "            for train in train_fold_list:  \n",
    "                #train = train_fold_list[-1]\n",
    "                #fold = 10\n",
    "                fold+=1                \n",
    "                print 'Starting train subset'\n",
    "                print 'Staring fold # {}'.format(fold)\n",
    "                uid_sampx_dict = collections.OrderedDict() #keep track of number of samples generated per subject\n",
    "                X_train_PE = []\n",
    "                adas_train_PE = []\n",
    "                mmse_train_PE = []\n",
    "                dx_train_PE = []\n",
    "                #print 'train:{}'.format(train)\n",
    "                for t, tr in enumerate(train):                  \n",
    "                    uid = tr             \n",
    "                    #print uid\n",
    "                    result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, sub_ADAS_dict, sub_MMSE_dict, sub_DX_dict, \n",
    "                                                     ordered_roi_list, drawSamples, min_CT_sampx)\n",
    "                    sub_X = result['sub_X']\n",
    "                    sub_adas = result['sub_adas']\n",
    "                    sub_mmse = result['sub_mmse']\n",
    "                    sub_dx = result['sub_dx']\n",
    "                \n",
    "                    if drawSamples:\n",
    "                        if t == 0:                \n",
    "                            X_train_stack = sub_X\n",
    "                            adas_train_stack = sub_adas\n",
    "                            mmse_train_stack = sub_mmse\n",
    "                            dx_train_stack = sub_dx\n",
    "                        else:\n",
    "                            X_train_stack = np.vstack((X_train_stack,sub_X))\n",
    "                            adas_train_stack = np.concatenate((adas_train_stack,sub_adas))\n",
    "                            mmse_train_stack = np.concatenate((mmse_train_stack,sub_mmse))\n",
    "                            dx_train_stack = np.concatenate((dx_train_stack,sub_dx))\n",
    "                            \n",
    "                        uid_sampx_dict[uid] = len(sub_adas)\n",
    "                         \n",
    "                    else: #collect Point Esimtates\n",
    "                        X_train_PE.append(sub_X)\n",
    "                        adas_train_PE.append(sub_adas)\n",
    "                        mmse_train_PE.append(sub_mmse)\n",
    "                        dx_train_PE.append(sub_dx)\n",
    "                        uid_sampx_dict[uid] = 1\n",
    "                \n",
    "                if not drawSamples:   \n",
    "                    X_train_stack  = np.squeeze(np.array(X_train_PE))\n",
    "                    adas_train_stack  = np.array(adas_train_PE)\n",
    "                    mmse_train_stack  = np.array(mmse_train_PE)\n",
    "                    dx_train_stack  = np.array(dx_train_PE)\n",
    "                \n",
    "                input_data = h5.File(hdf_file, 'a')\n",
    "\n",
    "                input_data.create_dataset('Fold_{}_train_X'.format(fold),data=X_train_stack)\n",
    "                input_data.create_dataset('Fold_{}_train_adas_bl'.format(fold),data=adas_train_stack)\n",
    "                input_data.create_dataset('Fold_{}_train_adas_m12'.format(fold),data=mmse_train_stack)\n",
    "                input_data.create_dataset('Fold_{}_train_dx'.format(fold),data=dx_train_stack)\n",
    "                input_data.close()\n",
    "                                \n",
    "                print 'Ending train subset'\n",
    "                uid_sampx_dict_list.append(uid_sampx_dict)\n",
    "                \n",
    "        #validation by default should be on \"fused features\"                \n",
    "        elif computeValidFolds:    \n",
    "            fold = 0 \n",
    "            for valid in val_fold_list:\n",
    "                fold+=1\n",
    "                print 'Starting valid subset'\n",
    "                X_valid_PE = []\n",
    "                adas_valid_PE = []\n",
    "                mmse_valid_PE = []\n",
    "                dx_valid_PE = []\n",
    "                \n",
    "                for v, val in enumerate(valid):\n",
    "                    #print valid\n",
    "                    uid = val\n",
    "                    result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, sub_ADAS_dict, sub_MMSE_dict, sub_DX_dict, \n",
    "                                                     ordered_roi_list, drawSamples, min_CT_sampx)\n",
    "                    sub_X = result['sub_X']\n",
    "                    sub_adas = result['sub_adas']\n",
    "                    sub_mmse = result['sub_mmse']\n",
    "                    sub_dx = result['sub_dx']\n",
    "                    \n",
    "                    #No drawing samples for validation set\n",
    "                    X_valid_PE.append(sub_X)\n",
    "                    adas_valid_PE.append(sub_adas)\n",
    "                    mmse_valid_PE.append(sub_mmse)\n",
    "                    dx_valid_PE.append(sub_dx)\n",
    "        \n",
    "                print 'Ending valid subset' \n",
    "                if not drawSamples:   \n",
    "                    X_valid_stack  = np.squeeze(np.array(X_valid_PE))\n",
    "                    adas_valid_stack  = np.array(adas_valid_PE)\n",
    "                    mmse_valid_stack  = np.array(mmse_valid_PE)\n",
    "                    dx_valid_stack  = np.array(dx_valid_PE)\n",
    "                \n",
    "                input_data = h5.File(hdf_file, 'a')            \n",
    "                input_data.create_dataset('Fold_{}_valid_X'.format(fold),data=X_valid_stack)    \n",
    "                input_data.create_dataset('Fold_{}_valid_adas_bl'.format(fold),data=adas_valid_stack)\n",
    "                input_data.create_dataset('Fold_{}_valid_adas_m12'.format(fold),data=mmse_valid_stack)\n",
    "                input_data.create_dataset('Fold_{}_valid_dx'.format(fold),data=dx_valid_stack)                \n",
    "                input_data.close()\n",
    "        \n",
    "        else:\n",
    "            print \"unknown set\"\n",
    "           \n",
    "            \n",
    "        #Save uid--> sampx list of dictionaries per fold\n",
    "        f = open(hdf_file+'.pkl', 'wb')\n",
    "        pickle.dump(uid_sampx_dict_list, f)\n",
    "        f.close()\n",
    "        \n",
    "        print 'All folds done!'\n",
    "\n",
    "        \n",
    "def inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, sub_ADAS_dict, sub_MMSE_dict, sub_DX_dict, \n",
    "                            ordered_roi_list, drawSamples, min_CT_sampx):\n",
    "    import numpy as np\n",
    "    import h5py as h5    \n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    from sklearn.cross_validation import KFold\n",
    "    import pickle\n",
    "    import re\n",
    "    import random\n",
    "    import collections\n",
    "    #UID = PTID + IID (PTID:[HC_vols], IID:{ROI:CT})\n",
    "    uid = uid.strip()\n",
    "    #print 'uid: {}'.format(uid)\n",
    "    ptid_re = re.compile('\\d*(_S_)\\d*')\n",
    "    iid_re = re.compile('(?<=I)\\d*')\n",
    "    ptid = re.search(ptid_re, uid).group(0).strip()\n",
    "    #iid = re.search(iid_re, uid).group(0).strip()\n",
    "    missing_data = False\n",
    "    \n",
    "    MC = True\n",
    "    \n",
    "    #print ptid\n",
    "    if ptid in sub_HC_L_dict.keys():\n",
    "        sub_HC_L = np.asarray(sub_HC_L_dict[ptid])\n",
    "        \n",
    "    else: \n",
    "        print \"missing HC_L entry for: {}\".format(uid) \n",
    "        missing_data = True\n",
    "    \n",
    "    if ptid in sub_HC_R_dict.keys():\n",
    "        sub_HC_R = np.asarray(sub_HC_R_dict[ptid])\n",
    "        \n",
    "    else: \n",
    "        print \"missing HC_R entry for: {}\".format(uid)\n",
    "        missing_data = True\n",
    "        \n",
    "    if ptid in sub_CT_dict.keys():\n",
    "        sub_CT_all_rois = sub_CT_dict[ptid][0]            \n",
    "    else: \n",
    "        print \"missing CT entry for: {}\".format(uid) \n",
    "        missing_data = True\n",
    "        \n",
    "    if not missing_data:\n",
    "        sub_ADAS_score = sub_ADAS_dict[ptid]\n",
    "        sub_MMSE_score = sub_MMSE_dict[ptid]\n",
    "        sub_DX_score = sub_DX_dict[ptid]\n",
    "        \n",
    "        sub_CT_sampx_dict = collections.OrderedDict()\n",
    "        if drawSamples:                    \n",
    "            min_sampx = np.min([sub_HC_L.shape[0],sub_HC_R.shape[0],min_CT_sampx])\n",
    "            #print 'min_sampx {}'.format(min_sampx)\n",
    "            \n",
    "#######################\n",
    "            if MC: \n",
    "                #select averaged out samples \n",
    "                MC_mult = np.max([1,int(0.5*(np.min([sub_HC_L.shape[0],sub_HC_R.shape[0]])))]) #Pool for averaged out samples \n",
    "                HC_L_MC_sampx_list = []\n",
    "                HC_R_MC_sampx_list = []\n",
    "\n",
    "                for i in np.arange(min_sampx): #Generate CT samples \n",
    "                    #select samples \n",
    "                    sub_HC_L_sampx_array = random.sample(sub_HC_L, MC_mult)\n",
    "                    sub_HC_R_sampx_array = random.sample(sub_HC_R, MC_mult)\n",
    "                    HC_L_MC_sampx_list.append(np.squeeze(stats.mode(sub_HC_L_sampx_array)[0]))\n",
    "                    HC_R_MC_sampx_list.append(np.squeeze(stats.mode(sub_HC_R_sampx_array)[0]))\n",
    "\n",
    "                sub_HC_L_sampx = np.array(HC_L_MC_sampx_list)\n",
    "                sub_HC_R_sampx = np.array(HC_R_MC_sampx_list)   \n",
    "\n",
    "            else:\n",
    "                #select any samples \n",
    "                sub_HC_L_sampx = random.sample(sub_HC_L, min_sampx)\n",
    "                sub_HC_R_sampx = random.sample(sub_HC_R, min_sampx)\n",
    "#######################\n",
    "\n",
    "            #Draw equal number of samples per roi            \n",
    "            for roi in ordered_roi_list:\n",
    "                #print roi\n",
    "                if roi not in sub_CT_sampx_dict:\n",
    "                    sub_CT_sampx_dict[roi]=[]\n",
    "\n",
    "                sub_CT_roi = np.squeeze(sub_CT_all_rois[roi])\n",
    "                #print sub_CT_roi.shape\n",
    "                if len(sub_CT_roi) >= min_CT_sampx:\n",
    "                    #Do you want averaged out samples or true thickness samples\n",
    "                    if MC: \n",
    "                        #Pool for averaged out samples (dynamic MC_mult eg.total 1000 vertices, 50 min_CT_sampx, then sample 20 vertices and average them)\n",
    "                        MC_mult = int(len(sub_CT_roi)/min_CT_sampx) \n",
    "                        CT_MC_sampx = []\n",
    "                        for i in np.arange(min_sampx): #Generate CT samples \n",
    "                            CT_MC_sampx.append(np.mean(random.sample(sub_CT_roi, MC_mult))) #Average out individual samples\n",
    "\n",
    "                        sub_CT_sampx_dict[roi].append(CT_MC_sampx)      \n",
    "                    else:\n",
    "                        # Draw true samples\n",
    "                        sub_CT_sampx_dict[roi].append(random.sample(sub_CT_roi, min_sampx))  \n",
    "                        #sub_CT_sampx_dict[roi].append(np.mean(sub_CT_roi))\n",
    "                else:\n",
    "                    print \"Wrong value for the min_CT_sampx\"\n",
    "\n",
    "            #Clinical Score            \n",
    "            sub_adas = np.tile(sub_ADAS_score, min_sampx)\n",
    "            sub_mmse = np.tile(sub_MMSE_score, min_sampx)\n",
    "            sub_dx = np.tile(sub_DX_score, min_sampx)\n",
    "        \n",
    "        # Or just collect point esimates (fused labels + mean thickness values)\n",
    "        else:\n",
    "            #select point-estimates\n",
    "            min_sampx = 1\n",
    "            #print sub_HC_L\n",
    "            #print sub_HC_R\n",
    "            sub_HC_L_sampx = stats.mode(sub_HC_L)[0]\n",
    "            sub_HC_R_sampx = stats.mode(sub_HC_R)[0]\n",
    "            \n",
    "            for roi in ordered_roi_list:                \n",
    "                sub_CT_roi = np.squeeze(sub_CT_all_rois[roi])                \n",
    "                sub_CT_sampx_dict[roi] = np.mean(sub_CT_roi)\n",
    "            #Clinical Score            \n",
    "            sub_adas = sub_ADAS_score\n",
    "            sub_mmse = sub_MMSE_score\n",
    "            sub_dx = sub_DX_score\n",
    "            \n",
    "        \n",
    "        # Convert samples or a mean vector to a numpy array   \n",
    "        sub_CT_sampx = np.zeros((min_sampx, len(ordered_roi_list)))\n",
    "        for col, roi in enumerate(ordered_roi_list):\n",
    "            sub_CT_sampx[:,col] = np.asarray(sub_CT_sampx_dict[roi],dtype=float)\n",
    "\n",
    "        #print sub_HC_L_sampx.shape, sub_HC_R_sampx.shape, sub_CT_sampx.shape\n",
    "        sub_X = np.hstack((sub_HC_L_sampx,sub_HC_R_sampx,sub_CT_sampx))\n",
    "        #sub_X = sub_CT_sampx\n",
    "        \n",
    "    else:\n",
    "        sub_X = []\n",
    "        sub_adas = []\n",
    "        sub_mmse = []\n",
    "        sub_dx = []\n",
    "        \n",
    "    return {'sub_X': sub_X, 'sub_adas':sub_adas,'sub_mmse':sub_mmse,'sub_dx':sub_dx}\n",
    "\n",
    "\n",
    "#If you want only inflate training subset (this is used for parallel implementation)\n",
    "def inflate_fold(train,sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict):\n",
    "    import numpy as np\n",
    "    import h5py as h5    \n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    from sklearn.cross_validation import KFold\n",
    "    import pickle\n",
    "    import re\n",
    "    import random\n",
    "    import collections\n",
    "    print 'Starting train subset'\n",
    "    for t, tr in enumerate(train):            \n",
    "        uid = subject_uids[t]\n",
    "        result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict)\n",
    "        sub_X = result['sub_X']\n",
    "        sub_y = result['sub_y']\n",
    "        if t == 0:                \n",
    "            X_train_stack = sub_X\n",
    "            y_train_stack = sub_y\n",
    "        else:\n",
    "            X_train_stack = np.vstack((X_train_stack,sub_X))\n",
    "            y_train_stack = np.concatenate((y_train_stack,sub_y))\n",
    "        \n",
    "    print 'Ending train subset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load dictionaries... \n",
    "sub_HC_L_dict = pickle.load( open(HC_L_data_path, \"rb\" ) )\n",
    "sub_HC_R_dict = pickle.load( open(HC_R_data_path, \"rb\" ) )\n",
    "sub_CT_dict = pickle.load( open(CT_data_path, \"rb\" ) )\n",
    "\n",
    "sub_ADAS_bl_dict = pickle.load( open(ADAS_bl_data_path, \"rb\" ) )\n",
    "sub_ADAS_m12_dict = pickle.load( open(ADAS_m12_data_path, \"rb\" ) )\n",
    "sub_DX_dict = pickle.load( open(DX_data_path, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab common subjects from all feature dictionaries\n",
    "# sub_CS_dict_clean = filter(lambda k: not isnan(sub_CS_dict[k]), sub_CS_dict) #remove NaNs\n",
    "# sub_CS_dict_clean = {k: sub_CS_dict[k] for k in sub_CS_dict if not isnan(sub_CS_dict[k])}\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sub_list = list(set(sub_HC_L_dict.keys()) & set(sub_HC_R_dict.keys()) & set(sub_CT_dict.keys()) \n",
    "                & set(sub_ADAS_bl_dict.keys())& set(sub_ADAS_m12_dict.keys()))\n",
    "\n",
    "print len(set(sub_HC_L_dict.keys())), len(set(sub_HC_R_dict.keys())), len(set(sub_CT_dict.keys())), len(set(sub_ADAS_bl_dict.keys())), len(set(sub_ADAS_m12_dict.keys())) \n",
    "print len(sub_list)\n",
    "print sub_HC_L_dict.values()[0][0].shape, sub_HC_R_dict.values()[0][0].shape\n",
    "\n",
    "sub_DX_dict_long={}\n",
    "for sub in sub_list:\n",
    "    sub_DX_dict_long[sub] = sub_DX_dict[sub]\n",
    "    \n",
    "le = preprocessing.LabelEncoder()\n",
    "y_dx = sub_DX_dict_long.values()\n",
    "le.fit(y_dx)\n",
    "y_dx_int = le.transform(y_dx) \n",
    "y_dx_int[y_dx_int==3]=2\n",
    "y_dx_int[y_dx_int==4]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configs for generating fold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "clinical_scale = 'ADAS13'\n",
    "cohort = 'ADNI1'\n",
    "subset = 'train'\n",
    "exp_name_out = 'Exp14'\n",
    "min_CT_sampx = 65 #exp6:65 #exp11:60\n",
    "output_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/'\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for mc in np.arange(1,2,1):\n",
    "#     exp_setup_file = baseline_dir + 'exp_data/baseline_MC/CV_{}_{}_ADAS13_MC_{}.pkl'.format(exp_name,cohort,mc)\n",
    "#     exp_setup = pickle.load( open(exp_setup_file, \"rb\" ) )\n",
    "#     kf = exp_setup['kf']\n",
    "#     sub_list = exp_setup['common_subs'] #grabbing this from previous cell for longitudinal data\n",
    "#     ordered_roi_list = sub_CT_dict['40817'][0].keys() #keeps order consistent while appending CT cols across subjects   \n",
    "#     save hdf_file for inflated / fused sets    \n",
    "       \n",
    "    kf = StratifiedKFold(y_dx_int, n_folds=10, shuffle=True, random_state=mc)\n",
    "    \n",
    "    out_file = output_dir + 'CV_{}_{}_{}_NN_{}_MC_{}.h5'.format(exp_name_out,cohort,clinical_scale,subset,mc)\n",
    "\n",
    "    CV_inflated_data = load_CV_data(sub_list,sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, sub_ADAS_bl_dict, sub_ADAS_m12_dict, sub_DX_dict_long,\n",
    "                                    kf, out_file, subset,min_CT_sampx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Combine ADNI1 and ADNI2 CV data --> fold by fold concatination. \n",
    "cohort = 'valid'\n",
    "CS_only = False\n",
    "adni1_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp6_ADNI1_ADAS13_NN_{}.h5'.format(cohort)\n",
    "adni2_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp11_ADNI2_ADAS13_NN_{}.h5'.format(cohort)\n",
    "adni1and2_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp14_ADNI1and2_ADAS13_MMSE_NN_{}.h5'.format(cohort)\n",
    "for fold in np.arange(1,2,1):\n",
    "    if not CS_only:\n",
    "        adni1_data_X = load_data(adni1_path, 'Fold_{}_{}_X'.format(fold,cohort))\n",
    "        adni1_data_y = load_data(adni1_path, 'Fold_{}_{}_y'.format(fold,cohort))\n",
    "        adni2_data_X = load_data(adni2_path, 'Fold_{}_{}_X'.format(fold,cohort))\n",
    "        adni2_data_y = load_data(adni2_path, 'Fold_{}_{}_y'.format(fold,cohort))\n",
    "        adni1and2_data_X = np.vstack((adni1_data_X,adni2_data_X))\n",
    "        adni1and2_data_y = np.concatenate((adni1_data_y,adni2_data_y))\n",
    "\n",
    "        print 'fold: {}, X shape:{}, y shape: {}'.format(fold, adni1and2_data_X.shape, adni1and2_data_y.shape)\n",
    "        comb_data = h5.File(adni1and2_path, 'a')            \n",
    "        comb_data.create_dataset('Fold_{}_{}_X'.format(fold,cohort),data=adni1and2_data_X)            \n",
    "        comb_data.create_dataset('Fold_{}_{}_y'.format(fold,cohort),data=adni1and2_data_y)    \n",
    "    else:\n",
    "        adni1_data_y = load_data(adni1_path, 'Fold_{}_{}_y3'.format(fold,cohort))\n",
    "        adni2_data_y = load_data(adni2_path, 'Fold_{}_{}_y3'.format(fold,cohort))        \n",
    "        adni1and2_data_y = np.concatenate((adni1_data_y,adni2_data_y))    \n",
    "        print 'fold: {}, y shape: {}'.format(fold, adni1and2_data_y.shape)\n",
    "        comb_data = h5.File(adni1and2_path, 'a')\n",
    "        comb_data.create_dataset('Fold_{}_y3'.format(fold,cohort),data=adni1and2_data_y)        \n",
    "        \n",
    "    comb_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Combine ADNI1 and ADNI2 CV data --> from API formatted data directly \n",
    "#Only shuffle inner_train subsets\n",
    "import h5py\n",
    "def concatHDFdata(file1,file2,dataset_list,shuffle,save_data,file3):\n",
    "    data1_sampx = load_data(file1,dataset_list[0]).shape[0]\n",
    "    data2_sampx = load_data(file2,dataset_list[0]).shape[0]\n",
    "    comb_sampx = data1_sampx + data2_sampx\n",
    "    \n",
    "    if shuffle:\n",
    "        shuff_idx = np.random.permutation(comb_sampx)\n",
    "    \n",
    "    for dataset in dataset_list:\n",
    "        d1 = load_data(file1,dataset)\n",
    "        d2 = load_data(file2,dataset)\n",
    "        \n",
    "        if d1.ndim > 1:\n",
    "            comb_data = np.vstack((d1,d2))\n",
    "        else: \n",
    "            comb_data = np.concatenate((d1,d2))\n",
    "            \n",
    "        if shuffle:\n",
    "            comb_data_arange = comb_data[shuff_idx]\n",
    "        else:\n",
    "            comb_data_arange = comb_data               \n",
    "        \n",
    "        if save_data:\n",
    "            # HDF5 is pretty efficient, but can be further compressed.\n",
    "            comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "            with h5py.File(file3, 'a') as f:      \n",
    "                f.create_dataset('{}'.format(dataset), data=comb_data, **comp_kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/'\n",
    "dataset_list = ['X_CT_SpecCluster_dyn','X_L_HC','X_R_HC','adas','dx','mmse']\n",
    "save_data = True\n",
    "subset = 'inner_test'\n",
    "shuffle = False #Trying this for MC=2\n",
    "# if subset == 'inner_train':\n",
    "#     shuffle = True\n",
    "# else:\n",
    "#     shuffle = False\n",
    "\n",
    "for mc in np.arange(10,11,1):\n",
    "    for fid in np.arange(6,7,1):\n",
    "        file1 = baseline_dir + 'MC_{}/fold{}/{}/Exp6_MC.h5'.format(mc,fid,subset)\n",
    "        file2 = baseline_dir + 'MC_{}/fold{}/{}/Exp11_MC.h5'.format(mc,fid,subset)\n",
    "        file3 = baseline_dir + 'MC_{}/fold{}/{}/Exp13_MC.h5'.format(mc,fid,subset)\n",
    "\n",
    "        concatHDFdata(file1,file2,dataset_list,shuffle,save_data,file3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# QC the empirical samples\n",
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "def ES_QC(PE_data_X, PE_data_y, ES_data_X, ES_data_y,show_plots):    \n",
    "    #local params\n",
    "    sampx_PE = PE_data_X.shape[0]\n",
    "    sampx_ES = ES_data_X.shape[0]\n",
    "    featx = 686\n",
    "    L_HC_offset = 16086 #12488 #9732 #11076 #11427\n",
    "    R_HC_offset =   #12263 #8953 #10161 #10519\n",
    "    HC_CT_offset = 686\n",
    "    \n",
    "    #compare the mean and stdev\n",
    "#     PE_X_HC_L = np.sum(PE_data_X[:,:L_HC_offset],axis=1)\n",
    "#     PE_X_HC_R = np.sum(PE_data_X[:,L_HC_offset:L_HC_offset+R_HC_offset],axis=1)\n",
    "#     PE_X_CT = PE_data_X[:,L_HC_offset+R_HC_offset:]   \n",
    "    \n",
    "#     ES_X_HC_L = np.sum(ES_data_X[:,:L_HC_offset],axis=1)\n",
    "#     ES_X_HC_R = np.sum(ES_data_X[:,L_HC_offset:L_HC_offset+R_HC_offset],axis=1)\n",
    "#     ES_X_CT = ES_data_X[:,L_HC_offset+R_HC_offset:]   \n",
    "    \n",
    "#     PE_X_HC_L = PE_X_HC_L.reshape(sampx_PE,1)\n",
    "#     PE_X_HC_R = PE_X_HC_R.reshape(sampx_PE,1)\n",
    "#     ES_X_HC_L = ES_X_HC_L.reshape(sampx_ES,1)\n",
    "#     ES_X_HC_R = ES_X_HC_R.reshape(sampx_ES,1)\n",
    "           \n",
    "#     PE_feat = np.hstack((PE_X_HC_L,PE_X_HC_R,PE_X_CT))\n",
    "#     ES_feat = np.hstack((ES_X_HC_L,ES_X_HC_R,ES_X_CT))\n",
    "    PE_feat = PE_data_X\n",
    "    ES_feat = ES_data_X\n",
    "    \n",
    "    print PE_feat.shape, ES_feat.shape\n",
    "    \n",
    "    PE_X_mean = np.mean(PE_feat,axis=0)\n",
    "    ES_X_mean = np.mean(ES_feat,axis=0)\n",
    "    \n",
    "    PE_X_std = np.std(PE_feat,axis=0)\n",
    "    ES_X_std = np.std(ES_feat,axis=0)\n",
    "    \n",
    "    #compare the correlations with the score    \n",
    "    PE_corr = []\n",
    "    ES_corr = []\n",
    "    feat_idx = np.arange(featx)\n",
    "    for col in feat_idx:\n",
    "        PE_corr.append(*zip(stats.pearsonr(PE_feat[:,col],PE_data_y))[0])\n",
    "        ES_corr.append(*zip(stats.pearsonr(ES_feat[:,col],ES_data_y))[0])\n",
    "    \n",
    "    PE_corr = np.squeeze(np.array(PE_corr))\n",
    "    ES_corr = np.squeeze(np.array(ES_corr))\n",
    "    \n",
    "    print 'PE: HC_L mean vol={}, std={}, corr={}'.format(PE_X_mean[0],PE_X_std[0],PE_corr[0])\n",
    "    print 'PE: HC_R mean vol={}, std={}, corr={}'.format(PE_X_mean[1],PE_X_std[1],PE_corr[1])\n",
    "    print 'ES: HC_L mean vol={}, std={}, corr={}'.format(ES_X_mean[0],ES_X_std[0],ES_corr[0])\n",
    "    print 'ES: HC_R mean vol={}, std={}, corr={}'.format(ES_X_mean[1],ES_X_std[1],ES_corr[1])\n",
    "    \n",
    "    if show_plots:\n",
    "        fig, ax = plt.subplots()    \n",
    "        bar_width = 1\n",
    "        opacity = 0.4\n",
    "        error_config = {'ecolor': '0.3'}\n",
    "\n",
    "        plt.subplot(3,1,1)\n",
    "        mean_PE_plt = plt.plot(feat_idx[2:],PE_X_mean[2:],label='PE_mean')\n",
    "        mean_ES_plt = plt.plot(feat_idx[2:],ES_X_mean[2:],label='ES_mean')\n",
    "        diff_mean_plt = plt.bar(feat_idx[2:], PE_X_mean[2:]-ES_X_mean[2:], bar_width,\n",
    "                         alpha=opacity,                     \n",
    "                         yerr=PE_X_std[2:],\n",
    "                         error_kw=error_config,\n",
    "                         label='PE_mean - ES_mean (PE_std)')\n",
    "        plt.ylabel('Stats (mean)')    \n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(3,1,2)\n",
    "        diff_std_plt = plt.bar(feat_idx[2:], PE_X_std[2:]-ES_X_std[2:] , bar_width,\n",
    "                         alpha=opacity,\n",
    "                         color='r',                     \n",
    "                         label='PE_std - ES_std')\n",
    "        plt.legend()\n",
    "\n",
    "        #Correlation\n",
    "        plt.subplot(3,1,3)\n",
    "        r_PE_plt = plt.plot(feat_idx[2:], PE_corr[2:], label='PE_corr')\n",
    "        r_ES_plt = plt.plot(feat_idx[2:], ES_corr[2:], label='ES_corr')\n",
    "        diff_corr_plt = plt.bar(feat_idx[2:], PE_corr[2:] - ES_corr[2:], bar_width,\n",
    "                         alpha=opacity,                     \n",
    "                         label='PE_corr - ES_corr')\n",
    "\n",
    "        plt.xlabel('PE vs ES')\n",
    "        plt.ylabel('Stats (corr)')    \n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "MC = 2\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "#ES_file = baseline_dir + 'caffe_input/CV_Exp2_ADNI1_ADAS13_NN_OuterFold_{}_train_InnerFold_1.h5'.format(fid)\n",
    "ES_file = baseline_dir + 'CV_Exp6_ADNI1_ADAS13_NN_train_MC_{}.h5'.format(MC)\n",
    "PE_file = baseline_dir + 'CV_Exp6_ADNI1_ADAS13_NN_valid_MC_{}.h5'.format(MC)\n",
    "\n",
    "show_plots = True\n",
    "for fid in np.arange(1,2,1):\n",
    "    print fid\n",
    "    PE_data_X = load_data(PE_file,'Fold_{}_X_R_CT'.format(fid))\n",
    "    PE_data_y = load_data(PE_file,'Fold_{}_valid_adas'.format(fid))\n",
    "    #PE_data_y = load_data(PE_file,'Fold_{}_y3'.format(fid))\n",
    "\n",
    "    #ES_data_X_L_HC = load_data(ES_file,'Fold_{}_X_L_HC'.format(fid))\n",
    "    #ES_data_X_R_HC = load_data(ES_file,'Fold_{}_X_R_HC'.format(fid))\n",
    "    #ES_data_X_CT = load_data(ES_file,'Fold_{}_X_R_CT'.format(fid))\n",
    "\n",
    "    #ES_data_X = np.hstack((ES_data_X_L_HC,ES_data_X_R_HC,ES_data_X_CT))\n",
    "\n",
    "    ES_data_X = load_data(ES_file,'Fold_{}_train_SpecCluster_dyn'.format(fid))\n",
    "    ES_data_y = load_data(ES_file,'Fold_{}_train_adas'.format(fid))\n",
    "    #ES_data_y = load_data(ES_file,'Fold_{}_train_y3'.format(fid))\n",
    "\n",
    "    ES_QC(PE_data_X, PE_data_y, ES_data_X, ES_data_y,show_plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# plt.figure()\n",
    "# for r,roi in enumerate(ordered_roi_list):      \n",
    "#     plt.subplot(8,10,r+1)\n",
    "#     plt.hist(sub_roi_mat_sampx[:,r]-sub_roi_mat[:,r])\n",
    "\n",
    "\n",
    "ES_data_X = load_data(ES_file,'Fold_{}_train_SpecCluster_dyn'.format(fid))\n",
    "ES_data_y = load_data(ES_file,'Fold_{}_train_dx'.format(fid))\n",
    "\n",
    "print ES_data_X.shape, ES_data_y.shape\n",
    "\n",
    "df = pd.DataFrame(columns=['roi','dx','CT'])\n",
    "df_concat = pd.DataFrame(columns=['roi','dx','CT'])\n",
    "\n",
    "roi_list = np.random.choice(686, 10)\n",
    "\n",
    "for roi in roi_list: #ES_data_X.shape[1]\n",
    "    df['CT'] = ES_data_X[:,roi]\n",
    "    df['roi'] = np.tile(roi,ES_data_X.shape[0])\n",
    "    df['dx'] = ES_data_y\n",
    "    \n",
    "    df_concat = df_concat.append(df)\n",
    "\n",
    "#plots\n",
    "\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "flatui = [ \"#3498db\", \"#e74c3c\",\"#34495e\", \"#95a5a6\", \"#9b59b6\",\"#2ecc71\",]\n",
    "palette = sns.color_palette(flatui)\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    g = sns.factorplot(x=\"roi\", y='CT', hue='dx', sharey=True, data=df_concat, \n",
    "                       hue_order=['CN','LMCI','AD'], kind=\"box\", palette=palette, size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "for roi in roi_list:\n",
    "    a = df_concat[(df_concat.roi==roi)&(df_concat.dx=='CN')]['CT']\n",
    "    b = df_concat[(df_concat.roi==roi)&(df_concat.dx=='AD')]['CT']\n",
    "\n",
    "    bartlett = scipy.stats.bartlett(a,b)\n",
    "    levene = scipy.stats.levene(a,b)\n",
    "\n",
    "    print bartlett\n",
    "    print levene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py as h5\n",
    "import tables as tb\n",
    "import numpy as np\n",
    "\n",
    "def create_dummy_vars(input_file, dataset, dummy_vars,save_to_file):        \n",
    "    #takes a categorical output variable and changes to binary vectorized form\n",
    "    #dummy_vars lists all the categories \n",
    "    dummy_var_match_list = []                    \n",
    "    categorical_data = load_data(input_file, dataset, 'no_preproc')         \n",
    "    for cat_data_item in categorical_data:        \n",
    "        match_list = dummy_vars == cat_data_item                \n",
    "        if np.sum(match_list) == 1:\n",
    "            match_list = np.argmax(match_list)\n",
    "            dummy_var_match_list.append(match_list)        \n",
    "        else:\n",
    "            print 'no match found for {}'.format(cat_data_item)\n",
    "            \n",
    "    \n",
    "    output_vector = np.array(dummy_var_match_list)  \n",
    "    #print output_vector\n",
    "    #collapse LMCI-->EMCI and smcs-->CN\n",
    "    output_vector[output_vector==2] = 1\n",
    "    output_vector[output_vector==3] = 2\n",
    "    output_vector[output_vector==4] = 2\n",
    "    \n",
    "    if save_to_file:\n",
    "        print 'Adding vectorized dataset to: {}'.format(input_file)\n",
    "        \n",
    "#         hdf_data = h5.File(input_file, mode='a')            \n",
    "#         hdf_data.create_dataset(dataset + '_vectorized', data=output_vector)            \n",
    "#         hdf_data.close()   \n",
    "        \n",
    "        # use table to save data\n",
    "        FILTERS = tb.Filters(complevel=5,complib='zlib')    \n",
    "        fileh = tb.open_file(input_file, mode='a', filters=FILTERS)        \n",
    "        data  = fileh.create_array(fileh.root, dataset + '_cat3', output_vector)\n",
    "        fileh.close()\n",
    "        \n",
    "    else:\n",
    "        print 'output_vector'\n",
    "        print output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create_dummy_vars for categorical Dx        \n",
    "hdf_file = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp11_ADNI2_ADAS13_NN_valid.h5'\n",
    "dataset = 'Fold_1_valid_dx'\n",
    "save_to_file = False\n",
    "dummy_vars = np.array(['AD', 'LMCI', 'EMCI', 'SMC', 'CN'])\n",
    "dummy_var_array = create_dummy_vars(hdf_file, dataset, dummy_vars, save_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "def generateInnerFold(OF_id, fold_name_prefix, out_file_prefix, in_file, n_innerFolds):\n",
    "    import numpy as np\n",
    "    import h5py as h5    \n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    from sklearn.cross_validation import KFold\n",
    "    import pickle\n",
    "    import re\n",
    "    import random\n",
    "    import collections\n",
    "    from sklearn import preprocessing\n",
    "    import tables as tb\n",
    "    \n",
    "    CT_SCALED = False #Scale CT modality to have values between [0,1] per dimension\n",
    "    split_HC_CT = True #Split the HC and CT data layer to allow partitioned model\n",
    "    \n",
    "    # Load data\n",
    "    data = tb.open_file(in_file, 'r')\n",
    "   \n",
    "    X_name = fold_name_prefix + '_X'    \n",
    "    adas_name = fold_name_prefix + '_adas_bl'\n",
    "    mmse_name = fold_name_prefix + '_adas_m12'\n",
    "    dx_name = fold_name_prefix + '_dx'\n",
    "    \n",
    "    #######################\n",
    "#     AAL_name = fold_name_prefix + '_SpecCluster_dyn'\n",
    "#     print in_file, AAL_name\n",
    "#     AAL = data.get_node('/' + AAL_name)[:]\n",
    "    #######################\n",
    "    \n",
    "    X_raw = data.get_node('/' + X_name)[:]\n",
    "    X = X_raw\n",
    "    print X.shape\n",
    "   \n",
    "    adas_scores = data.get_node('/' + adas_name)[:]\n",
    "    mmse_scores = data.get_node('/' + mmse_name)[:]\n",
    "    dx_scores = data.get_node('/' + dx_name)[:]\n",
    "    data.close()\n",
    "        \n",
    "    #Remove ROIs from ignore list       \n",
    "    L_HC_offset = 16086 #12488 #9732 #11076 #11427\n",
    "    R_HC_offset = 16471 #12263 #8953 #10161 #10519\n",
    "    sampx = X_raw.shape[0]\n",
    "    #ignore_list_CT_idx = list(L_HC_offset + R_HC_offset + np.array([0,29,30,37,38]))\n",
    "    #X = np.delete(X_raw, np.s_[ignore_list_CT_idx], 1)\n",
    "    \n",
    "    #Just create single fold (this avoids mixing of samples across the subjects)\n",
    "    if n_innerFolds == 1:\n",
    "        print \"Creating single fold...\"\n",
    "        k=1\n",
    "        train = np.arange(1,int(0.8*sampx),1)\n",
    "        valid = np.arange(int(0.8*sampx),sampx,1)\n",
    "        # Need to shuffle manually since no longer calling KFold\n",
    "        #np.random.shuffle(train)\n",
    "        #np.random.shuffle(valid)\n",
    "        \n",
    "        out_file_train = out_file_prefix + 'train_InnerFold_{}.h5'.format(k)\n",
    "        out_file_valid = out_file_prefix + 'valid_InnerFold_{}.h5'.format(k)\n",
    "        #Save Train\n",
    "        output_data = h5.File(out_file_train, 'a')        \n",
    "        print out_file_train        \n",
    "\n",
    "        X_all = X[train]\n",
    "        X_L_HC = X_all[:,:L_HC_offset]\n",
    "        X_R_HC = X_all[:,L_HC_offset:L_HC_offset+R_HC_offset]\n",
    "        X_CT = X_all[:,L_HC_offset+R_HC_offset:]\n",
    "        if CT_SCALED:\n",
    "            #scaler = preprocessing.MinMaxScaler()\n",
    "            print 'train'\n",
    "            print 'before scaling' + np.mean(X_CT, axis=1)\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "            X_CT_scaler = scaler.fit_transform(X_CT) \n",
    "            print 'after scaling' + np.mean(X_CT_scaler, axis=1)\n",
    "        else:\n",
    "            X_CT_scaler = X_CT\n",
    "\n",
    "        if split_HC_CT:\n",
    "####################            \n",
    "            output_data.create_dataset('Fold_{}_X_L_HC'.format(OF_id),data=X_L_HC)\n",
    "            output_data.create_dataset('Fold_{}_X_R_HC'.format(OF_id),data=X_R_HC)\n",
    "            output_data.create_dataset('Fold_{}_X_R_CT'.format(OF_id),data=X_CT_scaler)\n",
    "####################\n",
    "#             CT_AAL = AAL[train,:]\n",
    "#             output_data.create_dataset('Fold_{}_CT_SpecCluster_dyn'.format(OF_id),data=CT_AAL)\n",
    "        else:\n",
    "            output_data.create_dataset('Fold_{}_X'.format(OF_id),data=X[train])\n",
    "####################\n",
    "        output_data.create_dataset('Fold_{}_adas_bl'.format(OF_id),data=adas_scores[train])\n",
    "        output_data.create_dataset('Fold_{}_adas_m12'.format(OF_id),data=mmse_scores[train])\n",
    "        output_data.create_dataset('Fold_{}_dx'.format(OF_id),data=dx_scores[train])\n",
    "####################\n",
    "        output_data.close()\n",
    "\n",
    "        # Save valid\n",
    "        output_data = h5.File(out_file_valid, 'a')\n",
    "\n",
    "        X_all = X[valid]\n",
    "        X_L_HC = X_all[:,:L_HC_offset]\n",
    "        X_R_HC = X_all[:,L_HC_offset:L_HC_offset+R_HC_offset]\n",
    "        X_CT = X_all[:,L_HC_offset+R_HC_offset:]\n",
    "        if CT_SCALED:\n",
    "            print 'valid'\n",
    "            print 'before scaling' + np.mean(X_CT, axis=1)\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "            X_CT_scaler = scaler.fit_transform(X_CT) \n",
    "            print 'after scaling' + np.mean(X_CT_scaler, axis=1) \n",
    "        else:\n",
    "            X_CT_scaler = X_CT\n",
    "\n",
    "        if split_HC_CT:\n",
    "####################\n",
    "            output_data.create_dataset('Fold_{}_X_L_HC'.format(OF_id),data=X_L_HC)\n",
    "            output_data.create_dataset('Fold_{}_X_R_HC'.format(OF_id),data=X_R_HC)\n",
    "            output_data.create_dataset('Fold_{}_X_R_CT'.format(OF_id),data=X_CT_scaler)\n",
    "####################\n",
    "#             CT_AAL = AAL[valid,:]\n",
    "#             output_data.create_dataset('Fold_{}_CT_SpecCluster_dyn'.format(OF_id),data=CT_AAL)\n",
    "        else:\n",
    "            output_data.create_dataset('Fold_{}_X'.format(OF_id),data=X[valid])\n",
    "####################\n",
    "        output_data.create_dataset('Fold_{}_adas_bl'.format(OF_id),data=adas_scores[valid])\n",
    "        output_data.create_dataset('Fold_{}_adas_m12'.format(OF_id),data=mmse_scores[valid])\n",
    "        output_data.create_dataset('Fold_{}_dx'.format(OF_id),data=dx_scores[valid])\n",
    "####################\n",
    "\n",
    "        output_data.close()\n",
    "        \n",
    "    else: #Sample / Shuffle Data (not a good idea)\n",
    "        kf = KFold(sampx, n_folds=n_innerFolds,shuffle=True)\n",
    "        k=0\n",
    "        for train, valid in kf:\n",
    "            k+=1\n",
    "            out_file_train = out_file_prefix + 'train_InnerFold_{}.h5'.format(k)\n",
    "            out_file_valid = out_file_prefix + 'valid_InnerFold_{}.h5'.format(k)\n",
    "            #Save Train\n",
    "            output_data = h5.File(out_file_train, 'a')\n",
    "            X_all = X[train]\n",
    "            if split_HC_CT:\n",
    "                output_data.create_dataset('Fold_{}_X_L_HC'.format(OF_id),data=X_all[:,:L_HC_offset])\n",
    "                output_data.create_dataset('Fold_{}_X_R_HC'.format(OF_id),data=X_all[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "                output_data.create_dataset('Fold_{}_X_R_CT'.format(OF_id),data=X_all[:,L_HC_offset+R_HC_offset:])\n",
    "            else:\n",
    "                output_data.create_dataset('Fold_{}_X'.format(OF_id),data=X[train])\n",
    "\n",
    "            output_data.create_dataset('Fold_{}_y'.format(OF_id),data=y[train])\n",
    "            output_data.close()\n",
    "\n",
    "            # Save valid\n",
    "            output_data = h5.File(out_file_valid, 'a')\n",
    "            X_all = X[valid]\n",
    "            if split_HC_CT:\n",
    "                output_data.create_dataset('Fold_{}_X_L_HC'.format(OF_id),data=X_all[:,:L_HC_offset])\n",
    "                output_data.create_dataset('Fold_{}_X_R_HC'.format(OF_id),data=X_all[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "                output_data.create_dataset('Fold_{}_X_R_CT'.format(OF_id),data=X_all[:,L_HC_offset+R_HC_offset:])\n",
    "            else:\n",
    "                output_data.create_dataset('Fold_{}_X'.format(OF_id),data=X[valid])\n",
    "\n",
    "            output_data.create_dataset('Fold_{}_y'.format(OF_id),data=y[valid])\n",
    "            output_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirectView [0, 1, 2, 3]>\n"
     ]
    }
   ],
   "source": [
    "#Create inner fold(s) for hyper-param tuning\n",
    "exp_name = 'Exp14'\n",
    "exp_name_out = 'Exp14'\n",
    "cohort = 'ADNI1'\n",
    "#outer_CV_fold_file = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp11_ADNI2_ADAS13_NN_train.h5'\n",
    "#single_CV_fold_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/caffe_input/'\n",
    "runParallel = True\n",
    "\n",
    "single_CV_fold_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/'\n",
    "n_innerFolds = 1\n",
    "rc = ipp.Client()\n",
    "#rc.block = False\n",
    "dview = rc[:]\n",
    "print dview\n",
    "dview.push(dict(generateInnerFold = generateInnerFold))                   \n",
    "outer_folds = np.arange(1,11,1)\n",
    "mc_folds = np.arange(1,2,1)\n",
    "\n",
    "for mc in mc_folds:\n",
    "    outer_CV_fold_file = '{}CV_{}_{}_ADAS13_NN_train_MC_{}.h5'.format(single_CV_fold_dir,exp_name,cohort,mc)\n",
    "    fold_name_prefix_list = []\n",
    "    single_CV_fold_file_list = []\n",
    "    for of in outer_folds:\n",
    "        fold_name_prefix_list.append('Fold_{}_train'.format(of))\n",
    "        single_CV_fold_file_list.append('{}caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_'.format(single_CV_fold_dir,\n",
    "                                                                                               exp_name_out,cohort,mc,of)) \n",
    "        #generateInnerFold(outer_CV_fold_file, str(of), fold_name_prefix, n_innerFolds, single_CV_fold_file)\n",
    "\n",
    "    \n",
    "    if runParallel: #parallelized version:                \n",
    "        mapfunc = partial(generateInnerFold, in_file=outer_CV_fold_file, n_innerFolds=n_innerFolds)\n",
    "        parallel_result = dview.map_sync(mapfunc, outer_folds, fold_name_prefix_list, single_CV_fold_file_list)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p1=0.45\n",
    "p2=0.6\n",
    "p3=0.95\n",
    "\n",
    "c1 = (1-p1)*(1-p2)\n",
    "c2 = (1-p1)*(p2)\n",
    "c3 = (p1)*(1-p3)\n",
    "c4 = (p1)*(p3)\n",
    "\n",
    "print c1,c2,c3,c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this data for computing subject wise performance during outerloop cross-validation + held-out testset\n",
    "# Partition the validation folds\n",
    "from sklearn import preprocessing\n",
    "CV = True\n",
    "L_HC_offset = 16086 #12488 #9732 #11076 #11427\n",
    "R_HC_offset = 16471 #12263 #8953 #10161 #10519\n",
    "\n",
    "subset = 'valid'\n",
    "CS_ONLY = False\n",
    "CT_SCALED = False\n",
    "\n",
    "#baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/'\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_long/'\n",
    "for mc in np.arange(1,2,1):\n",
    "    in_file = 'CV_Exp14_ADNI1_ADAS13_NN_valid_MC_{}.h5'.format(mc)    \n",
    "\n",
    "    if CV:\n",
    "        CV_fused_data = h5.File(baseline_dir + in_file,'a')        \n",
    "        for f in np.arange(10):\n",
    "            X = CV_fused_data['Fold_{}_{}_X'.format(f+1,subset)][:]\n",
    "            \n",
    "            adas = CV_fused_data['Fold_{}_{}_adas_bl'.format(f+1,subset)][:]\n",
    "            mmse = CV_fused_data['Fold_{}_{}_adas_m12'.format(f+1,subset)][:]\n",
    "            dx = CV_fused_data['Fold_{}_{}_dx'.format(f+1,subset)][:]\n",
    "\n",
    "            CV_fused_data.create_dataset('Fold_{}_X_L_HC'.format(f+1),data=X[:,:L_HC_offset])\n",
    "            CV_fused_data.create_dataset('Fold_{}_X_R_HC'.format(f+1),data=X[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "            X_CT = X[:,L_HC_offset+R_HC_offset:]\n",
    "            if CT_SCALED:\n",
    "                #scaler = preprocessing.MinMaxScaler()\n",
    "                scaler = preprocessing.StandardScaler()\n",
    "                X_CT_scaled = scaler.fit_transform(X_CT)\n",
    "                #X_CT_scaled = preprocessing.scale(X_CT)\n",
    "            else:\n",
    "                X_CT_scaled = X_CT\n",
    "            #print np.mean(X_CT,axis=0)        \n",
    "            #print np.mean(X_CT_scaled,axis=0)\n",
    "            CV_fused_data.create_dataset('Fold_{}_X_R_CT'.format(f+1),data=X_CT_scaled)#Typo : R_CT        \n",
    "            CV_fused_data.create_dataset('Fold_{}_adas_bl'.format(f+1),data=adas) \n",
    "            CV_fused_data.create_dataset('Fold_{}_adas_m12'.format(f+1),data=mmse) \n",
    "            CV_fused_data.create_dataset('Fold_{}_dx'.format(f+1),data=dx) \n",
    "\n",
    "        #CV_valid_norm_data.close()\n",
    "        CV_fused_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_CV_fold_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outer_CV_fold_file = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp2_ADNI1_ADAS13_NN_train.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate map between AAL atlas ROI : index based on\n",
    "AAL_roi_map_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL_ROI_IDx'\n",
    "AAL_roi_Name_featID_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL_ROI_Name_featIDx.pkl'\n",
    "ordered_roi_list = sub_CT_dict['40817'][0].keys()\n",
    "\n",
    "\n",
    "ignore_roi_list = [0,29,30,39,40]\n",
    "for roi in ignore_roi_list:\n",
    "    ordered_roi_list.remove(roi)\n",
    "\n",
    "ordered_roi_idx_dict = {}\n",
    "for i,idx in enumerate(ordered_roi_list):\n",
    "    ordered_roi_idx_dict[idx]=i\n",
    "\n",
    "print ordered_roi_idx_dict\n",
    "#data = pd.read_csv(AAL_roi_map_file,delim_whitespace=True)\n",
    "print data.columns\n",
    "\n",
    "data['feature_id'] = data['Ind'].map(ordered_roi_idx_dict) \n",
    "print data\n",
    "#data = data[~np.isnan(data['feature_id'])]\n",
    "#print data\n",
    "\n",
    "roi_name_featIDx_Dict = data.set_index('feature_id')['Name'].to_dict()\n",
    "od = collections.OrderedDict(sorted(roi_name_featIDx_Dict.items())) #order by feature colume index\n",
    "print od\n",
    "\n",
    "#f = open(AAL_roi_Name_featID_file, 'wb')\n",
    "#pickle.dump(od, f)\n",
    "#f.close()\n",
    "\n",
    "#print 'total number of ROIs {}'.format(len(ordered_roi_list))\n",
    "#ignore_roi_list = [0,29,30,39,40]\n",
    "\n",
    "#for roi in ignore_roi_list:\n",
    "#    print 'Ignore ROI index {}'.format(ordered_roi_list.index(roi))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print arr.shape\n",
    "l = [1,2]\n",
    "arr_trunc = np.delete(arr, np.s_[l], 1)\n",
    "print arr_trunc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create inflated HC total vol dataset (no voxel wise features)\n",
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "\n",
    "baseline_dir= '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "in_file = 'HC_CT_inflated_CV_subsets_MC5.h5'\n",
    "out_file = 'total_HC_vol_CT_inflated_CV_subsets_MC5.h5'\n",
    "\n",
    "L_HC_offset=11427\n",
    "R_HC_offset=10519\n",
    "\n",
    "ignore_cols = False\n",
    "for lid in np.arange(6,11,1):\n",
    "    print 'Starting Fold {}'.format(lid)\n",
    "    out_train_X_raw = load_data(baseline_dir + in_file,'Fold_{}_train_X'.format(lid))\n",
    "    out_train_y = load_data(baseline_dir + in_file,'Fold_{}_train_y'.format(lid))\n",
    "\n",
    "    #out_valid_X_raw = load_data(baseline_dir + in_file,'Fold_{}_valid_X'.format(lid))\n",
    "    #out_valid_y = load_data(baseline_dir + in_file,'Fold_{}_valid_y'.format(lid))\n",
    "\n",
    "    #if you want to remove some CT columes (74 connundrum)\n",
    "    if ignore_cols:\n",
    "        ignore_list_CT_idx = list(L_HC_offset + R_HC_offset + np.array([0,29,30,37,38]))\n",
    "        out_train_X = np.delete(out_train_X_raw, np.s_[ignore_list_CT_idx], 1)\n",
    "        #out_valid_X = np.delete(out_valid_X_raw, np.s_[ignore_list_CT_idx], 1)\n",
    "    else:\n",
    "        out_train_X = out_train_X_raw\n",
    "        #out_valid_X = out_valid_X_raw\n",
    "\n",
    "\n",
    "    out_data = h5.File(baseline_dir + out_file,'a')\n",
    "    #Train\n",
    "    out_data.create_dataset('Fold_{}_train_X_L_HC'.format(lid),data=np.sum(out_train_X[:,:L_HC_offset],axis=1))\n",
    "    out_data.create_dataset('Fold_{}_train_X_R_HC'.format(lid),data=np.sum(out_train_X[:,L_HC_offset:L_HC_offset+R_HC_offset],axis=1))\n",
    "    out_data.create_dataset('Fold_{}_train_X_CT'.format(lid),data=out_train_X[:,L_HC_offset+R_HC_offset:])\n",
    "    out_data.create_dataset('Fold_{}_train_y'.format(lid),data=out_train_y)\n",
    "    #Valid\n",
    "    #out_data.create_dataset('Fold_{}_valid_X_L_HC'.format(lid),data=np.sum(out_valid_X[:,:L_HC_offset],axis=1))\n",
    "    #out_data.create_dataset('Fold_{}_valid_X_R_HC'.format(lid),data=np.sum(out_valid_X[:,L_HC_offset:L_HC_offset+R_HC_offset],axis=1))\n",
    "    #out_data.create_dataset('Fold_{}_valid_X_CT'.format(lid),data=out_valid_X[:,L_HC_offset+R_HC_offset:])\n",
    "    #out_data.create_dataset('Fold_{}_valid_y'.format(lid),data=out_valid_y)\n",
    "    out_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sanity checks \n",
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "baseline_dir= '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "in_file1 = 'HC_CT_fused_CV_subsets_C688_valid_test.h5'\n",
    "in_file2 = 'HC_CT_fused_CV_subsets_C688_valid.h5'\n",
    "\n",
    "x1 = load_data(baseline_dir + in_file1,'Fold_1_valid_X')\n",
    "y1 = load_data(baseline_dir + in_file1,'Fold_1_valid_y')\n",
    "\n",
    "x2 = load_data(baseline_dir + in_file2,'Fold_1_valid_X')\n",
    "y2 = load_data(baseline_dir + in_file2,'Fold_1_valid_y')\n",
    "\n",
    "print zip(y1,y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_setup_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/CV_Exp2_ADNI1_ADAS13.pkl'\n",
    "exp_setup = pickle.load( open(exp_setup_path, \"rb\" ) )\n",
    "kf = exp_setup['kf']\n",
    "sub_list = exp_setup['common_subs']\n",
    "train_fold_list=[]\n",
    "val_fold_list=[]\n",
    "for train, test in kf:\n",
    "        tmp_train_list = []\n",
    "        for t in train:\n",
    "            tmp_train_list.append(sub_list[t])\n",
    "        train_fold_list.append(tmp_train_list)\n",
    "        \n",
    "        tmp_val_list = []\n",
    "        for t in test:\n",
    "            tmp_val_list.append(sub_list[t])\n",
    "        val_fold_list.append(tmp_val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Delete hdf5 dataset! (works sometimes -- possibly with py tables datasets)\n",
    "local_root = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/caffe_input/'\n",
    "\n",
    "for fid in range(1,11,1):\n",
    "    file_name = 'CV_Exp12_ADNI1and2_ADAS13_MMSE_NN_OuterFold_{}_valid_InnerFold_1.h5'.format(fid)\n",
    "    dataset = 'Fold_{}_y3'.format(fid)\n",
    "    print local_root + file_name, dataset\n",
    "    with h5.File(local_root + file_name,  \"a\") as f:\n",
    "        f.__delitem__(dataset)\n",
    "        \n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
