{
 "metadata": {
  "name": "",
  "signature": "sha256:5caf112a31e0774c0273aa91036d42dae411a47e99b9b5aa0bc4da3f62c43671"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Basic Imports\n",
      "import numpy as np\n",
      "import h5py as h5\n",
      "from sklearn.externals import joblib\n",
      "import collections\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "from scipy import stats\n",
      "from sklearn.cross_validation import KFold\n",
      "import pickle\n",
      "import re\n",
      "import os\n",
      "import os.path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Has the AAL ROI indices \n",
      "#use top 40,962 to match low-res subjects http://www.bic.mni.mcgill.ca/ServicesSoftware/StatisticalAnalysesUsingSurfstatMatlab\n",
      "atlas_file_L = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/AAL_atlas_left.txt'\n",
      "atlas_file_R = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/AAL_atlas_right.txt'\n",
      "\n",
      "#Subjects:\n",
      "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/CT/civet12_adni2_m00/output/'\n",
      "\n",
      "#Has the CT values \n",
      "\n",
      "subject_file_L = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/test_subject_L.txt'\n",
      "subject_file_R = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/test_subject_R.txt'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read atlas files\n",
      "with open(atlas_file_L) as f:\n",
      "    atlas_data_highRes_L = f.readlines()\n",
      "\n",
      "with open(atlas_file_R) as f:\n",
      "    atlas_data_highRes_R = f.readlines()\n",
      "    \n",
      "atlas_data_list = atlas_data_highRes_L[:40962] + atlas_data_highRes_R[:40962]\n",
      "unique_roi = np.array(list(set(atlas_data_list)),dtype=int)\n",
      "atlas_data = np.array(atlas_data_list,dtype=int)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Grab left and right CT data for a given subject\n",
      "def get_ADNI1_SubjectData(baseline_dir,sub_id):\n",
      "    sub_pre = 'ADNI_'\n",
      "    sub_suf_L = '_native_rms_rsl_tlink_20mm_left.txt'\n",
      "    sub_suf_R = '_native_rms_rsl_tlink_20mm_right.txt'\n",
      "    \n",
      "    subject_file_L = baseline_dir + sub_pre + sub_id + sub_suf_L\n",
      "    subject_file_R = baseline_dir + sub_pre + sub_id + sub_suf_R\n",
      "    \n",
      "    if os.path.isfile(subject_file_L) and os.path.isfile(subject_file_R):\n",
      "    \n",
      "        with open(subject_file_L) as f:\n",
      "            subject_data_list_L = f.readlines()\n",
      "\n",
      "        with open(subject_file_R) as f:\n",
      "            subject_data_list_R = f.readlines()\n",
      "\n",
      "        subject_data = np.array(subject_data_list_L + subject_data_list_R,dtype=float)\n",
      "        msg = True\n",
      "            \n",
      "    else:        \n",
      "        subject_data = 0\n",
      "        msg = False\n",
      "        \n",
      "    return {'subject_data':subject_data, 'success': msg}\n",
      "\n",
      "def get_ADNI2_SubjectData(baseline_dir,sub_id):\n",
      "    sub_pre = sub_id + '/thickness/ADNI_'\n",
      "    sub_suf_L = '_native_rms_rsl_tlink_28.28mm_left.txt'\n",
      "    sub_suf_R = '_native_rms_rsl_tlink_28.28mm_right.txt'\n",
      "    \n",
      "    subject_file_L = baseline_dir + sub_pre + sub_id + sub_suf_L\n",
      "    subject_file_R = baseline_dir + sub_pre + sub_id + sub_suf_R\n",
      "    \n",
      "    if os.path.isfile(subject_file_L) and os.path.isfile(subject_file_R):\n",
      "    \n",
      "        with open(subject_file_L) as f:\n",
      "            subject_data_list_L = f.readlines()\n",
      "\n",
      "        with open(subject_file_R) as f:\n",
      "            subject_data_list_R = f.readlines()\n",
      "\n",
      "        subject_data = np.array(subject_data_list_L + subject_data_list_R,dtype=float)\n",
      "        msg = True\n",
      "            \n",
      "    else:        \n",
      "        subject_data = 0\n",
      "        msg = False\n",
      "        \n",
      "    return {'subject_data':subject_data, 'success': msg}\n",
      "\n",
      "# Create dictionary with roi_id:[thickness values]\n",
      "def get_ROI_CT_dict(unique_roi, subject_data):\n",
      "    roi_CT_dict = collections.defaultdict(list)\n",
      "    for roi in unique_roi:\n",
      "        roi_idx = atlas_data==roi\n",
      "        roi_CT_dict[roi].append(subject_data[roi_idx])\n",
      "        #print str(roi) + ': ' +  str(np.sum(roi_idx))    \n",
      "        \n",
      "    return roi_CT_dict\n",
      "\n",
      "def save_dictionary(_dict,save_path):\n",
      "    f = open(save_path, 'wb')\n",
      "    pickle.dump(_dict, f)\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subject_ROI_CT_dict_filename = '/projects/nikhil/ADNI_prediction/input_datasets/CT/civet12_adni2_m00/ADNI2_subject_ROI_CT_dict.pkl'\n",
      "subject_dirs = os.listdir(baseline_dir)\n",
      "#Dictionary of dictionary --> subject:{roi:CT_vals}\n",
      "subject_ROI_CT_dict = collections.defaultdict(list)\n",
      "subs_missing_data  = []\n",
      "for sub_id in subject_dirs:\n",
      "    result = get_ADNI2_SubjectData(baseline_dir,sub_id)\n",
      "    \n",
      "    # check if subject data exists\n",
      "    if result['success']:\n",
      "        single_ROI_CT_dict = get_ROI_CT_dict(unique_roi,result['subject_data'])\n",
      "        subject_ROI_CT_dict[sub_id].append(single_ROI_CT_dict)\n",
      "    else:\n",
      "        subs_missing_data.append(sub_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# keep log of subs with missing data:\n",
      "sub_missing_data_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/civet12_adni2_m00/bad_subs'\n",
      "with open(sub_missing_data_file, 'w') as f:\n",
      "    for s in subs_missing_data:\n",
      "        f.write(s + '\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ADNI-1 CT imports\n",
      "# Grab all the subject idx\n",
      "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/CT/ADNI1_1.5T_CIVET_1.1.12/thickness/'\n",
      "subject_files = os.listdir(baseline_dir)\n",
      "subject_idx = []\n",
      "for sub in subject_files:\n",
      "    idx = sub.split('_')[1]\n",
      "    subject_idx.append(idx)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate dictionary\n",
      "#Dictionary of dictionary --> subject:{roi:CT_vals}\n",
      "subject_ROI_CT_dict = collections.defaultdict(list)\n",
      "subs_missing_data  = []\n",
      "for sub_id in list(set(subject_idx)):\n",
      "    result = get_ADNI1_SubjectData(baseline_dir,sub_id)    \n",
      "    # check if subject data exists\n",
      "    if result['success']:\n",
      "        single_ROI_CT_dict = get_ROI_CT_dict(unique_roi,result['subject_data'])\n",
      "        subject_ROI_CT_dict[sub_id].append(single_ROI_CT_dict)\n",
      "    else:\n",
      "        subs_missing_data.append(sub_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Save the subject-roi-CT dictionary\n",
      "subject_ROI_CT_dict_filename = '/projects/nikhil/ADNI_prediction/input_datasets/CT/subject_roi_ct_data/ADNI1_subject_ROI_CT_dict.pkl'\n",
      "save_dictionary(subject_ROI_CT_dict,subject_ROI_CT_dict_filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Chech if all ADNI-1 subjects have ROI-CT dataset assoicated with them\n",
      "master_dataframe = '/projects/francisco/data/ADNI/master_fused.pkl'\n",
      "data = pd.read_pickle(master_dataframe)\n",
      "id_image = re.compile('(?<=I)\\d*')\n",
      "ADNI1_subs_with_CT_data = []\n",
      "for uid in data.UID:\n",
      "    img = re.search(id_image, uid).group(0)\n",
      "    if len(subject_ROI_CT_dict[img][0]) == 79:\n",
      "        ADNI1_subs_with_CT_data.append(img)\n",
      "        \n",
      "print len(ADNI1_subs_with_CT_data), len(subject_ROI_CT_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Check the minimun number of vertices per ROI (=min sampling bound = 132 for ADNI1 baseline)\n",
      "no_of_vertices = []\n",
      "for key in single_ROI_CT_dict.keys():\n",
      "    no_of_vertices.append(len(single_ROI_CT_dict[int(key)][0]))\n",
      "    \n",
      "print np.asarray(no_of_vertices).min()     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "132\n"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 122,
       "text": [
        "(698, 22039)"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}