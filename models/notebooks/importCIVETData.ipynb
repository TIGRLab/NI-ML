{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "from sklearn.externals import joblib\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import KFold\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use top 40,962 to match low-res subjects http://www.bic.mni.mcgill.ca/ServicesSoftware/StatisticalAnalysesUsingSurfstatMatlab\n",
    "AAL_style = True\n",
    "JDV_style = False\n",
    "#Subjects:\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/CT/civet_out/'\n",
    "\n",
    "#Regex for parsing PTID and IID \n",
    "ptid_re = re.compile('\\d*(_S_)\\d*')\n",
    "iid_re = re.compile('(?<=I)\\d*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atlas data (number of L + R vertices) 81924\n",
      "# of unique ROIs 79\n"
     ]
    }
   ],
   "source": [
    "# Read atlas files\n",
    "if AAL_style:\n",
    "    atlas_file_L = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/AAL_atlas_left.txt'\n",
    "    atlas_file_R = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/AAL_atlas_right.txt'\n",
    "    with open(atlas_file_L) as f:\n",
    "        atlas_data_highRes_L = f.readlines()\n",
    "\n",
    "    with open(atlas_file_R) as f:\n",
    "        atlas_data_highRes_R = f.readlines()\n",
    "    \n",
    "    atlas_data_list = atlas_data_highRes_L[:40962] + atlas_data_highRes_R[:40962]\n",
    "    atlas_data = np.array(atlas_data_list,dtype=int)\n",
    "    \n",
    "elif JDV_style:\n",
    "    atlas_data_L = pd.read_csv(atlas_file_L,header=3,delim_whitespace=True)\n",
    "    atlas_data_R = pd.read_csv(atlas_file_R,header=3,delim_whitespace=True)\n",
    "    \n",
    "    atlas_data_list = list(atlas_data_L['v0']) + list(atlas_data_R['v0'])\n",
    "    atlas_data = np.array(atlas_data_list,dtype=int)\n",
    "    \n",
    "else:        \n",
    "    atlas_file_L = '/projects/nikhil/ADNI_prediction/input_datasets/CT/left-labels_C375.txt'\n",
    "    #Use same atlas for L & R to preseve symmetry to ROIs\n",
    "    atlas_file_R = '/projects/nikhil/ADNI_prediction/input_datasets/CT/left-labels_C375.txt'\n",
    "\n",
    "    atlas_data_L = np.genfromtxt(atlas_file_L, dtype=int)\n",
    "    atlas_data_R = 1000 + np.genfromtxt(atlas_file_R, dtype=int)#adding big enough offset to sperate L v R ROIs\n",
    "    \n",
    "    atlas_data = np.hstack((atlas_data_L[:40962],atlas_data_R[:40962]))\n",
    "    atlas_data_list = list(atlas_data)\n",
    "        \n",
    "unique_roi = np.array(list(set(atlas_data)))\n",
    "\n",
    "\n",
    "print 'atlas data (number of L + R vertices) {}'.format(len(atlas_data))\n",
    "print '# of unique ROIs {}'.format(len(unique_roi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print unique_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab left and right CT data for a given subject\n",
    "def get_baseline_data(baseline_dir):\n",
    "    print \"nothing to do yet.. need to find mean thickness values from civet\"\n",
    "        \n",
    "    \n",
    "def get_legacy_ADNI1_SubjectData(baseline_dir,sub_id):\n",
    "    sub_pre = 'ADNI_'\n",
    "    sub_suf_L = '_native_rms_rsl_tlink_20mm_left.txt'\n",
    "    sub_suf_R = '_native_rms_rsl_tlink_20mm_right.txt'\n",
    "    \n",
    "    subject_file_L = baseline_dir + sub_pre + sub_id + sub_suf_L\n",
    "    subject_file_R = baseline_dir + sub_pre + sub_id + sub_suf_R\n",
    "    \n",
    "    if os.path.isfile(subject_file_L) and os.path.isfile(subject_file_R):\n",
    "    \n",
    "        with open(subject_file_L) as f:\n",
    "            subject_data_list_L = f.readlines()\n",
    "\n",
    "        with open(subject_file_R) as f:\n",
    "            subject_data_list_R = f.readlines()\n",
    "\n",
    "        subject_data = np.array(subject_data_list_L + subject_data_list_R,dtype=float)\n",
    "        msg = True\n",
    "            \n",
    "    else:        \n",
    "        subject_data = 0\n",
    "        msg = False\n",
    "        \n",
    "    return {'subject_data':subject_data, 'success': msg}\n",
    "\n",
    "def get_ADNI_SubjectData(baseline_dir,sub_id, prefix):\n",
    "    sub_pre = sub_id + '/thickness/{}_'.format(prefix)\n",
    "    sub_suf_L = '_native_rms_rsl_tlink_28.28mm_left.txt'\n",
    "    sub_suf_R = '_native_rms_rsl_tlink_28.28mm_right.txt'\n",
    "    \n",
    "    subject_file_L = baseline_dir + sub_pre + sub_id + sub_suf_L\n",
    "    subject_file_R = baseline_dir + sub_pre + sub_id + sub_suf_R\n",
    "    \n",
    "    if os.path.isfile(subject_file_L) and os.path.isfile(subject_file_R):\n",
    "    \n",
    "        with open(subject_file_L) as f:\n",
    "            subject_data_list_L = f.readlines()\n",
    "\n",
    "        with open(subject_file_R) as f:\n",
    "            subject_data_list_R = f.readlines()\n",
    "\n",
    "        subject_data = np.array(subject_data_list_L + subject_data_list_R,dtype=float)\n",
    "        msg = True\n",
    "            \n",
    "    else:        \n",
    "        subject_data = 0\n",
    "        msg = False\n",
    "        \n",
    "    return {'subject_data':subject_data, 'success': msg}\n",
    "\n",
    "# Create dictionary with roi_id:[thickness values]\n",
    "def get_ROI_CT_dict(unique_roi, subject_data):\n",
    "    roi_CT_dict = collections.defaultdict(list)\n",
    "    for roi in unique_roi:\n",
    "        roi_idx = atlas_data==roi\n",
    "        roi_CT_dict[roi].append(subject_data[roi_idx])\n",
    "        #print str(roi) + ': ' +  str(np.sum(roi_idx))    \n",
    "        \n",
    "    return roi_CT_dict\n",
    "\n",
    "def save_dictionary(_dict,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(_dict, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(subject_ROI_CT_dict[ptid][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not an ADNI dir: civet_adni2_QC.tar.gz\n",
      "Not an ADNI dir: References.txt\n",
      "Not an ADNI dir: QC\n",
      "missing data for subjects: 1\n"
     ]
    }
   ],
   "source": [
    "# ADNI CT imports\n",
    "prefix = 'ADNI2_BL'\n",
    "subject_ROI_CT_dict_filename = baseline_dir + 'adni2/ADNI2_subject_ROI_CT_dict.pkl'\n",
    "subject_dirs_path = baseline_dir + 'adni2/output/'\n",
    "subject_dirs = os.listdir(subject_dirs_path)\n",
    "#Dictionary of dictionary --> subject:{roi:CT_vals}\n",
    "subject_ROI_CT_dict = collections.defaultdict(list)\n",
    "subs_missing_data  = []\n",
    "for sub_dir in subject_dirs:\n",
    "    if sub_dir.split(\"_\")[0] == 'ADNI':\n",
    "        ptid = re.search(ptid_re, sub_dir).group(0).strip()\n",
    "        #print ptid\n",
    "        result = get_ADNI_SubjectData(subject_dirs_path,sub_dir,prefix)\n",
    "\n",
    "        # check if subject data exists\n",
    "        if result['success']:\n",
    "            single_ROI_CT_dict = get_ROI_CT_dict(unique_roi,result['subject_data'])\n",
    "            if len(single_ROI_CT_dict) != len(unique_roi):\n",
    "                print \"something is wrong\"\n",
    "                print sub_dir\n",
    "            subject_ROI_CT_dict[ptid].append(single_ROI_CT_dict)\n",
    "        else:\n",
    "            subs_missing_data.append(ptid) \n",
    "            \n",
    "    else:\n",
    "        print 'Not an ADNI dir: {}'.format(sub_dir)\n",
    "\n",
    "print 'missing data for subjects: {}'.format(len(subs_missing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep log of subs with missing data:\n",
    "sub_missing_data_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/civet_out/adni1/bad_subs'\n",
    "with open(sub_missing_data_file, 'w') as f:\n",
    "    for s in subs_missing_data:\n",
    "        f.write(s + '\\n')\n",
    "        \n",
    "print len(subs_missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ADNI-1 CT imports (legacy)\n",
    "# Grab all the subject idx\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/CT/ADNI1_1.5T_CIVET_1.1.12/thickness/'\n",
    "subject_files = os.listdir(baseline_dir)\n",
    "subject_idx = []\n",
    "for sub in subject_files:\n",
    "    idx = sub.split('_')[1]\n",
    "    subject_idx.append(idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ADNI-1 CT imports (legacy)\n",
    "# Generate dictionary\n",
    "#Dictionary of dictionary --> subject:{roi:CT_vals}\n",
    "subject_ROI_CT_dict = collections.defaultdict(list)\n",
    "subs_missing_data  = []\n",
    "for sub_id in list(set(subject_idx)):\n",
    "    result = get_ADNI1_SubjectData(baseline_dir,sub_id)    \n",
    "    # check if subject data exists\n",
    "    if result['success']:\n",
    "        single_ROI_CT_dict = get_ROI_CT_dict(unique_roi,result['subject_data'])\n",
    "        subject_ROI_CT_dict[sub_id].append(single_ROI_CT_dict)\n",
    "    else:\n",
    "        subs_missing_data.append(sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Chech if all ADNI-1 subjects have ROI-CT dataset assoicated with them\n",
    "master_dataframe = '/projects/francisco/data/ADNI/master_fused.pkl'\n",
    "data = pd.read_pickle(master_dataframe)\n",
    "id_image = re.compile('(?<=I)\\d*')\n",
    "ADNI1_subs_with_CT_data = []\n",
    "for uid in data.UID:\n",
    "    img = re.search(ptid_re, uid).group(0)\n",
    "    if img in subject_ROI_CT_dict.keys():\n",
    "        if len(subject_ROI_CT_dict[img][0]) == len(unique_roi): #688 for spectral clustering, #79 for AAL\n",
    "             ADNI1_subs_with_CT_data.append(img)    \n",
    "        \n",
    "        \n",
    "print len(ADNI1_subs_with_CT_data), len(subject_ROI_CT_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check the minimun number of vertices per ROI (=min sampling bound = 132 for ADNI1 baseline)\n",
    "no_of_vertices = []\n",
    "for key in single_ROI_CT_dict.keys():\n",
    "    no_of_vertices.append(len(single_ROI_CT_dict[int(key)][0]))\n",
    "    \n",
    "print np.asarray(no_of_vertices).min()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subject_ROI_CT_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distribution of the vertices per ROI\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.style.use('ggplot')\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(Counter(atlas_data).values(),bins=100)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 763)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(single_ROI_CT_dict), len(subject_ROI_CT_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save dictionary to the disk\n",
    "cohort = 'ADNI2'\n",
    "atlas = 'AAL'\n",
    "save_CT_dict_path = baseline_dir + '{}_subject_ROI_CT_dict_{}.pkl'.format(cohort,atlas)\n",
    "save_dictionary(subject_ROI_CT_dict,save_CT_dict_path)\n",
    "#sub_CT_dict = pickle.load( open(save_CT_dict_path, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subject_ROI_CT_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-14cff1829a08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mordered_roi_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubject_ROI_CT_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'40817'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mroi_vert_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mroi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mordered_roi_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'subject_ROI_CT_dict' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "ordered_roi_list = subject_ROI_CT_dict['40817'][0].keys()\n",
    "\n",
    "roi_vert_count = {}\n",
    "for roi in ordered_roi_list:\n",
    "    roi_vert_count[roi] = np.sum(atlas_data==roi)\n",
    "ignore_roi_list = [-1]\n",
    "\n",
    "for roi in ignore_roi_list:\n",
    "    ordered_roi_list.remove(roi)\n",
    "\n",
    "print ordered_roi_list\n",
    "#print (ordered_roi_list)\n",
    "for key, val in roi_vert_count.iteritems():\n",
    "    if int(val) < 66:\n",
    "        print key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/CT/civet_out/adni2/'\n",
    "adni2_subs = 'adni2_subject_list'\n",
    "adni2_bpipe_out_ids = 'adni2_ids_clean' #same as civet in ids (so after bpipe QC)\n",
    "adni2_civet_out_ids = 'civet_out_ids_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780 764 763\n"
     ]
    }
   ],
   "source": [
    "with open(baseline_dir + adni2_subs) as f:\n",
    "    s1 = f.read().splitlines()\n",
    "with open(baseline_dir + adni2_bpipe_out_ids) as f:\n",
    "    s2 = f.read().splitlines()\n",
    "with open(baseline_dir + adni2_civet_out_ids) as f:\n",
    "    s3 = f.read().splitlines()\n",
    "\n",
    "print len(s1),len(s2),len(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADNI_002_S_4171_MR_MT1__N3m_Br_20110816094410627_S118013_I250649', 'ADNI_002_S_4213_MR_MT1__N3m_Br_20110910135704514_S121168_I255409', 'ADNI_002_S_4219_MR_MT1__N3m_Br_20110928093601592_S122143_I258694']\n",
      "['ADNI_002_S_4171_MR_MT1__N3m_Br_20110816094410627_S118013_I250649', 'ADNI_002_S_4213_MR_MT1__N3m_Br_20110910135704514_S121168_I255409', 'ADNI_002_S_4219_MR_MT1__N3m_Br_20110928093601592_S122143_I258694']\n",
      "['ADNI_002_S_4171_MR_MT1__N3m_Br_20110816094410627_S118013_I250649', 'ADNI_002_S_4213_MR_MT1__N3m_Br_20110910135704514_S121168_I255409', 'ADNI_002_S_4219_MR_MT1__N3m_Br_20110928093601592_S122143_I258694']\n"
     ]
    }
   ],
   "source": [
    "print s1[:3]\n",
    "print s2[:3]\n",
    "print s3[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "civet_proc_summary = []\n",
    "for sub in s1:\n",
    "    if sub in s2:\n",
    "        bpipe_str = 'pass'\n",
    "        if sub in s3:\n",
    "            civet_str = 'pass'\n",
    "        else:\n",
    "            civet_str = 'fail'\n",
    "    else:\n",
    "        bpipe_str = 'fail'\n",
    "        civet_str = 'fail'\n",
    "    \n",
    "    \n",
    "        \n",
    "    sub_proc_str = '{} {} {}'.format(sub,bpipe_str,civet_str)\n",
    "    civet_proc_summary.append(sub_proc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_file = 'adni2_proc_summary.csv'\n",
    "f = open(baseline_dir + out_file, \"w\")\n",
    "for item in civet_proc_summary:\n",
    "    f.write(\"%s\\n\" % item)\n",
    "    \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
