{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "from sklearn.externals import joblib\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import KFold\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Has the AAL ROI indices \n",
    "\n",
    "#use top 40,962 to match low-res subjects http://www.bic.mni.mcgill.ca/ServicesSoftware/StatisticalAnalysesUsingSurfstatMatlab\n",
    "#atlas_file_L = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/AAL_atlas_left.txt'\n",
    "#atlas_file_R = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/AAL_atlas_right.txt'\n",
    "\n",
    "atlas_file_L = '/projects/nikhil/ADNI_prediction/input_datasets/CT/JDV_Atlas/ctx_roi_1000-dil_civ.L.1D'\n",
    "atlas_file_R = '/projects/nikhil/ADNI_prediction/input_datasets/CT/JDV_Atlas/ctx_roi_1000-dil_civ.R.1D'\n",
    "\n",
    "AAL = False\n",
    "\n",
    "#Subjects:\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/CT/civet12_adni2_m00/output/'\n",
    "\n",
    "#Has the CT values \n",
    "subject_file_L = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/test_subject_L.txt'\n",
    "subject_file_R = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/test_subject_R.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read atlas files\n",
    "if AAL:\n",
    "    with open(atlas_file_L) as f:\n",
    "        atlas_data_highRes_L = f.readlines()\n",
    "\n",
    "    with open(atlas_file_R) as f:\n",
    "        atlas_data_highRes_R = f.readlines()\n",
    "    \n",
    "    atlas_data_list = atlas_data_highRes_L[:40962] + atlas_data_highRes_R[:40962]\n",
    "    \n",
    "else:\n",
    "    atlas_data_L = pd.read_csv(atlas_file_L,header=3,delim_whitespace=True)\n",
    "    atlas_data_R = pd.read_csv(atlas_file_R,header=3,delim_whitespace=True)\n",
    "    \n",
    "    atlas_data_list = list(atlas_data_L['v0']) + list(atlas_data_R['v0'])\n",
    "    \n",
    "unique_roi = np.array(list(set(atlas_data_list)),dtype=int)\n",
    "atlas_data = np.array(atlas_data_list,dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab left and right CT data for a given subject\n",
    "def get_ADNI1_SubjectData(baseline_dir,sub_id):\n",
    "    sub_pre = 'ADNI_'\n",
    "    sub_suf_L = '_native_rms_rsl_tlink_20mm_left.txt'\n",
    "    sub_suf_R = '_native_rms_rsl_tlink_20mm_right.txt'\n",
    "    \n",
    "    subject_file_L = baseline_dir + sub_pre + sub_id + sub_suf_L\n",
    "    subject_file_R = baseline_dir + sub_pre + sub_id + sub_suf_R\n",
    "    \n",
    "    if os.path.isfile(subject_file_L) and os.path.isfile(subject_file_R):\n",
    "    \n",
    "        with open(subject_file_L) as f:\n",
    "            subject_data_list_L = f.readlines()\n",
    "\n",
    "        with open(subject_file_R) as f:\n",
    "            subject_data_list_R = f.readlines()\n",
    "\n",
    "        subject_data = np.array(subject_data_list_L + subject_data_list_R,dtype=float)\n",
    "        msg = True\n",
    "            \n",
    "    else:        \n",
    "        subject_data = 0\n",
    "        msg = False\n",
    "        \n",
    "    return {'subject_data':subject_data, 'success': msg}\n",
    "\n",
    "def get_ADNI2_SubjectData(baseline_dir,sub_id):\n",
    "    sub_pre = sub_id + '/thickness/ADNI_'\n",
    "    sub_suf_L = '_native_rms_rsl_tlink_28.28mm_left.txt'\n",
    "    sub_suf_R = '_native_rms_rsl_tlink_28.28mm_right.txt'\n",
    "    \n",
    "    subject_file_L = baseline_dir + sub_pre + sub_id + sub_suf_L\n",
    "    subject_file_R = baseline_dir + sub_pre + sub_id + sub_suf_R\n",
    "    \n",
    "    if os.path.isfile(subject_file_L) and os.path.isfile(subject_file_R):\n",
    "    \n",
    "        with open(subject_file_L) as f:\n",
    "            subject_data_list_L = f.readlines()\n",
    "\n",
    "        with open(subject_file_R) as f:\n",
    "            subject_data_list_R = f.readlines()\n",
    "\n",
    "        subject_data = np.array(subject_data_list_L + subject_data_list_R,dtype=float)\n",
    "        msg = True\n",
    "            \n",
    "    else:        \n",
    "        subject_data = 0\n",
    "        msg = False\n",
    "        \n",
    "    return {'subject_data':subject_data, 'success': msg}\n",
    "\n",
    "# Create dictionary with roi_id:[thickness values]\n",
    "def get_ROI_CT_dict(unique_roi, subject_data):\n",
    "    roi_CT_dict = collections.defaultdict(list)\n",
    "    for roi in unique_roi:\n",
    "        roi_idx = atlas_data==roi\n",
    "        roi_CT_dict[roi].append(subject_data[roi_idx])\n",
    "        #print str(roi) + ': ' +  str(np.sum(roi_idx))    \n",
    "        \n",
    "    return roi_CT_dict\n",
    "\n",
    "def save_dictionary(_dict,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(_dict, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ADNI-2 CT imports\n",
    "subject_ROI_CT_dict_filename = '/projects/nikhil/ADNI_prediction/input_datasets/CT/civet12_adni2_m00/ADNI2_subject_ROI_CT_dict.pkl'\n",
    "subject_dirs = os.listdir(baseline_dir)\n",
    "#Dictionary of dictionary --> subject:{roi:CT_vals}\n",
    "subject_ROI_CT_dict = collections.defaultdict(list)\n",
    "subs_missing_data  = []\n",
    "for sub_id in subject_dirs:\n",
    "    result = get_ADNI2_SubjectData(baseline_dir,sub_id)\n",
    "    \n",
    "    # check if subject data exists\n",
    "    if result['success']:\n",
    "        single_ROI_CT_dict = get_ROI_CT_dict(unique_roi,result['subject_data'])\n",
    "        subject_ROI_CT_dict[sub_id].append(single_ROI_CT_dict)\n",
    "    else:\n",
    "        subs_missing_data.append(sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep log of subs with missing data:\n",
    "sub_missing_data_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/civet12_adni2_m00/bad_subs'\n",
    "with open(sub_missing_data_file, 'w') as f:\n",
    "    for s in subs_missing_data:\n",
    "        f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ADNI-1 CT imports\n",
    "# Grab all the subject idx\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/CT/ADNI1_1.5T_CIVET_1.1.12/thickness/'\n",
    "subject_files = os.listdir(baseline_dir)\n",
    "subject_idx = []\n",
    "for sub in subject_files:\n",
    "    idx = sub.split('_')[1]\n",
    "    subject_idx.append(idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate dictionary\n",
    "#Dictionary of dictionary --> subject:{roi:CT_vals}\n",
    "subject_ROI_CT_dict = collections.defaultdict(list)\n",
    "subs_missing_data  = []\n",
    "for sub_id in list(set(subject_idx)):\n",
    "    result = get_ADNI1_SubjectData(baseline_dir,sub_id)    \n",
    "    # check if subject data exists\n",
    "    if result['success']:\n",
    "        single_ROI_CT_dict = get_ROI_CT_dict(unique_roi,result['subject_data'])\n",
    "        subject_ROI_CT_dict[sub_id].append(single_ROI_CT_dict)\n",
    "    else:\n",
    "        subs_missing_data.append(sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Chech if all ADNI-1 subjects have ROI-CT dataset assoicated with them\n",
    "master_dataframe = '/projects/francisco/data/ADNI/master_fused.pkl'\n",
    "data = pd.read_pickle(master_dataframe)\n",
    "id_image = re.compile('(?<=I)\\d*')\n",
    "ADNI1_subs_with_CT_data = []\n",
    "for uid in data.UID:\n",
    "    img = re.search(id_image, uid).group(0)\n",
    "    if len(subject_ROI_CT_dict[img][0]) == 903: #79 for AAL\n",
    "        ADNI1_subs_with_CT_data.append(img)\n",
    "        \n",
    "print len(ADNI1_subs_with_CT_data), len(subject_ROI_CT_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check the minimun number of vertices per ROI (=min sampling bound = 132 for ADNI1 baseline)\n",
    "no_of_vertices = []\n",
    "for key in single_ROI_CT_dict.keys():\n",
    "    no_of_vertices.append(len(single_ROI_CT_dict[int(key)][0]))\n",
    "    \n",
    "print np.asarray(no_of_vertices).min()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(Counter(atlas_data_L['v0']).values()) + len(Counter(atlas_data_R['v0']).values())\n",
    "#print np.sort(Counter(atlas_data).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "plt.style.use('ggplot')\n",
    "#np.sort(Counter(atlas_data_L['v0']).values())\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(Counter(atlas_data).values(),bins=50)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(single_ROI_CT_dict), len(subject_ROI_CT_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_CT_dict_path = '/projects/nikhil/ADNI_prediction/input_datasets/CT/subject_roi_ct_data/ADNI1_subject_ROI_CT_dict_JDV.pkl'\n",
    "#save_dictionary(subject_ROI_CT_dict,save_CT_dict_path)\n",
    "sub_CT_dict = pickle.load( open(save_CT_dict_path, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf_file = \"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_train_valid_KFold_UIDs.pkl\"\n",
    "kf = pickle.load( open(kf_file, \"rb\" ) )\n",
    "val_fold_list= kf['valid_UIDs']\n",
    "ordered_roi_list = sub_CT_dict['40817'][0].keys()\n",
    "iid_re = re.compile('(?<=I)\\d*')\n",
    "\n",
    "for uid in val_fold_list[0]:\n",
    "    uid = uid.strip()\n",
    "    iid = re.search(iid_re, uid).group(0).strip()    \n",
    "    sub_CT_all_rois = sub_CT_dict[iid][0]           \n",
    "    sub_CScore = sub_cScores_dict[uid]        \n",
    "    \n",
    "    for roi in ordered_roi_list:              \n",
    "        sub_CT_roi = np.squeeze(sub_CT_all_rois[roi])\n",
    "        \n",
    "        # Mean CT values per ROI\n",
    "        sub_CT_sampx_dict[roi].append(np.mean(sub_CT_roi))\n",
    "                           \n",
    "        sub_CT_sampx = np.zeros((min_sampx, len(ordered_roi_list)))\n",
    "        for col, roi in enumerate(ordered_roi_list):\n",
    "            sub_CT_sampx[:,col] = np.asarray(sub_CT_sampx_dict[roi],dtype=float)\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
