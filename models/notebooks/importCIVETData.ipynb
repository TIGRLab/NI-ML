{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "from sklearn.externals import joblib\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import KFold\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Has the AAL ROI indices \n",
    "\n",
    "#use top 40,962 to match low-res subjects http://www.bic.mni.mcgill.ca/ServicesSoftware/StatisticalAnalysesUsingSurfstatMatlab\n",
    "#atlas_file_L = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/AAL_atlas_left.txt'\n",
    "#atlas_file_R = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/AAL_atlas_right.txt'\n",
    "\n",
    "atlas_file_L = '/projects/nikhil/ADNI_prediction/input_datasets/CT/left-labels_C375.txt'\n",
    "#Use same atlas for L & R to preseve symmetry to ROIs\n",
    "atlas_file_R = '/projects/nikhil/ADNI_prediction/input_datasets/CT/left-labels_C375.txt'\n",
    "\n",
    "AAL_style = False\n",
    "JDV_style = False\n",
    "\n",
    "#Subjects:\n",
    "#baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/CT/civet12_adni2_m00/output/'\n",
    "\n",
    "#Has the CT values \n",
    "#subject_file_L = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/test_subject_L.txt'\n",
    "#subject_file_R = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL/test_subject_R.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81924\n",
      "688\n"
     ]
    }
   ],
   "source": [
    "# Read atlas files\n",
    "if AAL_style:\n",
    "    with open(atlas_file_L) as f:\n",
    "        atlas_data_highRes_L = f.readlines()\n",
    "\n",
    "    with open(atlas_file_R) as f:\n",
    "        atlas_data_highRes_R = f.readlines()\n",
    "    \n",
    "    atlas_data_list = atlas_data_highRes_L[:40962] + atlas_data_highRes_R[:40962]\n",
    "    atlas_data = np.array(atlas_data_list,dtype=int)\n",
    "    \n",
    "elif JDV_style:\n",
    "    atlas_data_L = pd.read_csv(atlas_file_L,header=3,delim_whitespace=True)\n",
    "    atlas_data_R = pd.read_csv(atlas_file_R,header=3,delim_whitespace=True)\n",
    "    \n",
    "    atlas_data_list = list(atlas_data_L['v0']) + list(atlas_data_R['v0'])\n",
    "    atlas_data = np.array(atlas_data_list,dtype=int)\n",
    "    \n",
    "else:        \n",
    "    atlas_data_L = np.genfromtxt(atlas_file_L, dtype=int)\n",
    "    atlas_data_R = 1000 + np.genfromtxt(atlas_file_R, dtype=int)#adding big enough offset to sperate L v R ROIs\n",
    "    \n",
    "    atlas_data = np.hstack((atlas_data_L[:40962],atlas_data_R[:40962]))\n",
    "    atlas_data_list = list(atlas_data)\n",
    "        \n",
    "unique_roi = np.array(list(set(atlas_data)))\n",
    "\n",
    "\n",
    "print len(atlas_data)\n",
    "print len(unique_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n",
      "688\n"
     ]
    }
   ],
   "source": [
    "print len(set(atlas_data_L))\n",
    "print len(set(atlas_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab left and right CT data for a given subject\n",
    "def get_ADNI1_SubjectData(baseline_dir,sub_id):\n",
    "    sub_pre = 'ADNI_'\n",
    "    sub_suf_L = '_native_rms_rsl_tlink_20mm_left.txt'\n",
    "    sub_suf_R = '_native_rms_rsl_tlink_20mm_right.txt'\n",
    "    \n",
    "    subject_file_L = baseline_dir + sub_pre + sub_id + sub_suf_L\n",
    "    subject_file_R = baseline_dir + sub_pre + sub_id + sub_suf_R\n",
    "    \n",
    "    if os.path.isfile(subject_file_L) and os.path.isfile(subject_file_R):\n",
    "    \n",
    "        with open(subject_file_L) as f:\n",
    "            subject_data_list_L = f.readlines()\n",
    "\n",
    "        with open(subject_file_R) as f:\n",
    "            subject_data_list_R = f.readlines()\n",
    "\n",
    "        subject_data = np.array(subject_data_list_L + subject_data_list_R,dtype=float)\n",
    "        msg = True\n",
    "            \n",
    "    else:        \n",
    "        subject_data = 0\n",
    "        msg = False\n",
    "        \n",
    "    return {'subject_data':subject_data, 'success': msg}\n",
    "\n",
    "def get_ADNI2_SubjectData(baseline_dir,sub_id):\n",
    "    sub_pre = sub_id + '/thickness/ADNI_'\n",
    "    sub_suf_L = '_native_rms_rsl_tlink_28.28mm_left.txt'\n",
    "    sub_suf_R = '_native_rms_rsl_tlink_28.28mm_right.txt'\n",
    "    \n",
    "    subject_file_L = baseline_dir + sub_pre + sub_id + sub_suf_L\n",
    "    subject_file_R = baseline_dir + sub_pre + sub_id + sub_suf_R\n",
    "    \n",
    "    if os.path.isfile(subject_file_L) and os.path.isfile(subject_file_R):\n",
    "    \n",
    "        with open(subject_file_L) as f:\n",
    "            subject_data_list_L = f.readlines()\n",
    "\n",
    "        with open(subject_file_R) as f:\n",
    "            subject_data_list_R = f.readlines()\n",
    "\n",
    "        subject_data = np.array(subject_data_list_L + subject_data_list_R,dtype=float)\n",
    "        msg = True\n",
    "            \n",
    "    else:        \n",
    "        subject_data = 0\n",
    "        msg = False\n",
    "        \n",
    "    return {'subject_data':subject_data, 'success': msg}\n",
    "\n",
    "# Create dictionary with roi_id:[thickness values]\n",
    "def get_ROI_CT_dict(unique_roi, subject_data):\n",
    "    roi_CT_dict = collections.defaultdict(list)\n",
    "    for roi in unique_roi:\n",
    "        roi_idx = atlas_data==roi\n",
    "        roi_CT_dict[roi].append(subject_data[roi_idx])\n",
    "        #print str(roi) + ': ' +  str(np.sum(roi_idx))    \n",
    "        \n",
    "    return roi_CT_dict\n",
    "\n",
    "def save_dictionary(_dict,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(_dict, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ADNI-2 CT imports\n",
    "subject_ROI_CT_dict_filename = '/projects/nikhil/ADNI_prediction/input_datasets/CT/civet12_adni2_m00/ADNI2_subject_ROI_CT_dict.pkl'\n",
    "subject_dirs = os.listdir(baseline_dir)\n",
    "#Dictionary of dictionary --> subject:{roi:CT_vals}\n",
    "subject_ROI_CT_dict = collections.defaultdict(list)\n",
    "subs_missing_data  = []\n",
    "for sub_id in subject_dirs:\n",
    "    result = get_ADNI2_SubjectData(baseline_dir,sub_id)\n",
    "    \n",
    "    # check if subject data exists\n",
    "    if result['success']:\n",
    "        single_ROI_CT_dict = get_ROI_CT_dict(unique_roi,result['subject_data'])\n",
    "        subject_ROI_CT_dict[sub_id].append(single_ROI_CT_dict)\n",
    "    else:\n",
    "        subs_missing_data.append(sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep log of subs with missing data:\n",
    "sub_missing_data_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/civet12_adni2_m00/bad_subs'\n",
    "with open(sub_missing_data_file, 'w') as f:\n",
    "    for s in subs_missing_data:\n",
    "        f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ADNI-1 CT imports\n",
    "# Grab all the subject idx\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/CT/ADNI1_1.5T_CIVET_1.1.12/thickness/'\n",
    "subject_files = os.listdir(baseline_dir)\n",
    "subject_idx = []\n",
    "for sub in subject_files:\n",
    "    idx = sub.split('_')[1]\n",
    "    subject_idx.append(idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate dictionary\n",
    "#Dictionary of dictionary --> subject:{roi:CT_vals}\n",
    "subject_ROI_CT_dict = collections.defaultdict(list)\n",
    "subs_missing_data  = []\n",
    "for sub_id in list(set(subject_idx)):\n",
    "    result = get_ADNI1_SubjectData(baseline_dir,sub_id)    \n",
    "    # check if subject data exists\n",
    "    if result['success']:\n",
    "        single_ROI_CT_dict = get_ROI_CT_dict(unique_roi,result['subject_data'])\n",
    "        subject_ROI_CT_dict[sub_id].append(single_ROI_CT_dict)\n",
    "    else:\n",
    "        subs_missing_data.append(sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698 3585\n"
     ]
    }
   ],
   "source": [
    "#Chech if all ADNI-1 subjects have ROI-CT dataset assoicated with them\n",
    "master_dataframe = '/projects/francisco/data/ADNI/master_fused.pkl'\n",
    "data = pd.read_pickle(master_dataframe)\n",
    "id_image = re.compile('(?<=I)\\d*')\n",
    "ADNI1_subs_with_CT_data = []\n",
    "for uid in data.UID:\n",
    "    img = re.search(id_image, uid).group(0)\n",
    "    if len(subject_ROI_CT_dict[img][0]) == 688: #691: #903: #79 for AAL\n",
    "        ADNI1_subs_with_CT_data.append(img)\n",
    "        \n",
    "print len(ADNI1_subs_with_CT_data), len(subject_ROI_CT_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "#Check the minimun number of vertices per ROI (=min sampling bound = 132 for ADNI1 baseline)\n",
    "no_of_vertices = []\n",
    "for key in single_ROI_CT_dict.keys():\n",
    "    no_of_vertices.append(len(single_ROI_CT_dict[int(key)][0]))\n",
    "    \n",
    "print np.asarray(no_of_vertices).min()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(Counter(atlas_data_L['v0']).values()) + len(Counter(atlas_data_R['v0']).values())\n",
    "#print np.sort(Counter(atlas_data).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.style.use('ggplot')\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(Counter(atlas_data).values(),bins=50)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(single_ROI_CT_dict), len(subject_ROI_CT_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_CT_dict_path = '/projects/nikhil/ADNI_prediction/input_datasets/CT/subject_roi_ct_data/ADNI1_subject_ROI_CT_dict_C688.pkl'\n",
    "save_dictionary(subject_ROI_CT_dict,save_CT_dict_path)\n",
    "#sub_CT_dict = pickle.load( open(save_CT_dict_path, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ordered_roi_list = subject_ROI_CT_dict['40817'][0].keys()\n",
    "\n",
    "roi_vert_count = {}\n",
    "for roi in ordered_roi_list:\n",
    "    roi_vert_count[roi] = np.sum(atlas_data==roi)\n",
    "ignore_roi_list = [-1]\n",
    "\n",
    "for roi in ignore_roi_list:\n",
    "    ordered_roi_list.remove(roi)\n",
    "\n",
    "print ordered_roi_list\n",
    "#print (ordered_roi_list)\n",
    "for key, val in roi_vert_count.iteritems():\n",
    "    if int(val) < 66:\n",
    "        print key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(ordered_roi_list)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
