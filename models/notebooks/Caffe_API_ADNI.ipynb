{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import caffe\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "# Some defs to load data and extract encodings from trained net\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    #print net.blobs.items()[0]\n",
    "    #print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        #print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        #print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        #print net.blobs[input_node].shape\n",
    "        \n",
    "        data_layers.append(net.blobs[input_node])    \n",
    "        #print 'X_list shape: {}'.format(X_list[i].shape)\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "    \n",
    "     \n",
    "    net.reshape()            \n",
    "    #print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        #print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1, concat_param=dict(axis=1))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_CT,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT_unified(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_HC_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_HC_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_HC_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_HC_CT,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_HC_CT, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=4, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.adas, n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "    n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "    n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "    n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff2,n.R_ff2,n.ff2, concat_param=dict(axis=1))\n",
    "    #n.concat = L.Concat(n.X_L_HC,n.X_R_HC,n.X_CT, concat_param=dict(axis=1))\n",
    "    \n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.dropC1 = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2) #This is done automatically by caffe! \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        #n.ff2_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2ADAS13 = L.ReLU(n.ff2_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        #n.ff2_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2MMSE = L.ReLU(n.ff2_MMSE, in_place=True)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        n.ff1_DX = L.InnerProduct(n.ff3, num_output=node_sizes['DX_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1DX = L.ReLU(n.ff1_DX, in_place=True)\n",
    "        \n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_DX = L.InnerProduct(n.ff1_DX, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.DX_loss = L.SoftmaxWithLoss(n.output_DX, n.dx_cat3,loss_weight=tr['DX'])  \n",
    "    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_CT, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.dropC1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "        \n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_R_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_L_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))       \n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='HC_CT': #multimodal AE - experimental stage\n",
    "        HC_node_split = 0.8\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_L_HC = L.InnerProduct(n.X_L_HC, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.9*node_sizes['En1'])))        \n",
    "        n.NLinEn_L_HC = L.ReLU(n.encoder_L_HC, in_place=True)\n",
    "        \n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_CT = L.InnerProduct(n.X_CT, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.1*node_sizes['En1'])))        \n",
    "        n.NLinEn_CT = L.ReLU(n.encoder_CT, in_place=True)\n",
    "        \n",
    "        #Concat         \n",
    "        n.encoder1 = L.Concat(n.encoder_L_HC,n.encoder_CT, concat_param=dict(axis=1))\n",
    "        \n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers (or common encoder layers for multimodal) \n",
    "    \n",
    "#     n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.Sigmoid(n.encoder2, in_place=True)\n",
    "    #code layer\n",
    "#    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='xavier'))  \n",
    "    \n",
    "#     n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "    \n",
    "    #Decoder layers\n",
    "    if modality == 'CT':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)                    \n",
    "    \n",
    "    elif modality =='R_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    elif modality =='HC_CT':        \n",
    "        n.decoder_L_HC = L.InnerProduct(n.code, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_L_CT = L.ReLU(n.decoder_L_HC, in_place=True)\n",
    "        n.decoder_CT = L.InnerProduct(n.code, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_CT = L.ReLU(n.decoder_CT, in_place=True)\n",
    "        \n",
    "        n.output_L_HC = L.InnerProduct(n.decoder_L_HC, num_output=node_sizes['out_HC'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_HC = L.SigmoidCrossEntropyLoss(n.output_L_HC, n.X_L_HC)\n",
    "        \n",
    "        n.output_CT = L.InnerProduct(n.decoder_CT, num_output=node_sizes['out_CT'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_CT = L.EuclideanLoss(n.output_CT, n.X_CT)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    return n.to_proto()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 1 µs, total: 11 µs\n",
      "Wall time: 15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode):\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 20\n",
    "    \n",
    "    if multimodal_autoencode:\n",
    "        train_loss_HC = zeros(niter)\n",
    "        test_loss_HC = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_loss_CT = zeros(niter)\n",
    "        test_loss_CT = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "        for it in range(niter):            \n",
    "            solver.step(1)  # SGD by Caffe \n",
    "            train_loss_HC[it] = solver.net.blobs['loss_HC'].data\n",
    "            train_loss_CT[it] = solver.net.blobs['loss_CT'].data\n",
    "            \n",
    "            if it % test_interval == 0:                        \n",
    "                t_loss_HC = 0\n",
    "                t_loss_CT = 0                                \n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()   \n",
    "                    t_loss_HC += solver.test_nets[0].blobs['loss_HC'].data\n",
    "                    t_loss_CT += solver.test_nets[0].blobs['loss_CT'].data\n",
    "                \n",
    "                test_loss_HC[it // test_interval] = t_loss_HC/(test_iter)\n",
    "                test_loss_CT[it // test_interval] = t_loss_CT/(test_iter)\n",
    "                print 'HC Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_HC[it], np.sum(train_loss_HC)/it, test_loss_HC[it // test_interval])\n",
    "                print 'CT Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_CT[it], np.sum(train_loss_CT)/it, test_loss_CT[it // test_interval])\n",
    "                \n",
    "        perf = {'train_loss':[train_loss_HC, train_loss_CT],'test_loss':[test_loss_HC,test_loss_CT]}\n",
    "    else: \n",
    "        \n",
    "        #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "        # losses will also be stored in the log\n",
    "        test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "        if not multi_task:\n",
    "            train_loss = zeros(niter)\n",
    "            test_loss = zeros(int(np.ceil(niter / test_interval)))  \n",
    "\n",
    "        else: \n",
    "            if multi_label:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_DX_loss = zeros(niter)\n",
    "                test_DX_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "            else:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_MMSE_loss = zeros(niter)\n",
    "                test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "\n",
    "        #output = zeros((niter, batch_size))\n",
    "        #solver.restore()\n",
    "        #the main solver loop\n",
    "        for it in range(niter):\n",
    "            #solver.net.forward()\n",
    "            solver.step(1)  # SGD by Caffe    \n",
    "            # store the train loss\n",
    "            if not multi_task:\n",
    "                train_loss[it] = solver.net.blobs['loss'].data        \n",
    "            else: \n",
    "                if multi_label:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_DX_loss[it] = solver.net.blobs['DX_loss'].data\n",
    "                else:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "            # store the output on the first test batch\n",
    "            # (start the forward pass at conv1 to avoid loading new data)\n",
    "            #solver.test_nets[0].forward()\n",
    "            #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "            # run a full test every so often\n",
    "            # (Caffe can also do this for us and write to a log, but we show here\n",
    "            #  how to do it directly in Python, where more complicated things are easier.)\n",
    "            if it % test_interval == 0:        \n",
    "                t_loss = 0\n",
    "                t_ADAS13_loss = 0\n",
    "                t_MMSE_loss = 0\n",
    "                t_DX_loss = 0\n",
    "                correct = 0\n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()                \n",
    "                    if not multi_task:\n",
    "                        t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                        if multi_label:\n",
    "                            correct += sum(solver.test_nets[0].blobs['output'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "                    else: \n",
    "                        if multi_label:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_DX_loss += solver.test_nets[0].blobs['DX_loss'].data\n",
    "                            correct += sum(solver.test_nets[0].blobs['output_DX'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "\n",
    "                        else:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "                if not multi_task:\n",
    "                    test_loss[it // test_interval] = t_loss/(test_iter)\n",
    "                    if multi_label:\n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                    print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                        it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval], test_acc[it // test_interval])\n",
    "                else:\n",
    "                    if multi_label:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_DX_loss[it // test_interval] = t_DX_loss/(test_iter) \n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'DX Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                            it, train_DX_loss[it], np.sum(train_DX_loss)/it, test_DX_loss[it // test_interval],test_acc[it // test_interval])\n",
    "                    else:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_MMSE_loss[it // test_interval] = t_MMSE_loss/(test_iter)             \n",
    "\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "        if not multi_task:\n",
    "            perf = {'train_loss':[train_loss],'test_loss':[test_loss],'test_acc':[test_acc]}\n",
    "        else:\n",
    "            if multi_label:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_DX_loss],'test_loss':[test_ADAS13_loss,test_DX_loss],'test_acc':[test_acc]}\n",
    "            else:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "                \n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,snap_prefix):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    \n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    #s.solver_type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 100000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 5000\n",
    "    s.snapshot_prefix = snap_prefix\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp6_MC'\n",
    "preproc = 'no_preproc'\n",
    "fid = 1\n",
    "mc=1\n",
    "train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "\n",
    "train_data_CT = load_data(train_filename_hdf, 'X_CT',preproc)\n",
    "train_data_y = load_data(train_filename_hdf, 'y','no_preproc')\n",
    "test_data_CT = load_data(test_filename_hdf, 'X_CT',preproc)\n",
    "test_data_y = load_data(test_filename_hdf, 'y','no_preproc')\n",
    "\n",
    "print train_data_CT.shape, train_data_y.shape, test_data_CT.shape, test_data_y.shape\n",
    "np.mean(train_data_y), np.std(train_data_y), np.mean(test_data_y), np.std(test_data_y)\n",
    "\n",
    "ct_corr_train = []\n",
    "ct_corr_test = []\n",
    "for c in np.arange(0,train_data_CT.shape[1],1):\n",
    "    ct_corr_train.append(stats.pearsonr(train_data_CT[:,c],train_data_y)[0])\n",
    "    ct_corr_test.append(stats.pearsonr(test_data_CT[:,c],test_data_y)[0])\n",
    "    \n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(np.mean(train_data_CT, axis=0)-np.mean(test_data_CT, axis=0))\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.array(ct_corr_train) - np.array(ct_corr_test),label='train-test')\n",
    "#plt.plot(ct_corr_test,label='test')\n",
    "plt.legend()\n",
    "\n",
    "mean_diff = np.mean(train_data_CT, axis=0)-np.mean(test_data_CT, axis=0)\n",
    "\n",
    "print mean_diff[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "150*4*100*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MC # 1, Hype # hyp4, Fold # 3\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (295.457092285,inf), test loss: 187.584952164\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (337.27734375,inf), test loss: 381.778025818\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (20.26953125,131.897957771), test loss: 50.8117003918\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (32.2719726562,244.018641157), test loss: 22.0781537056\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (29.3750286102,88.2114954691), test loss: 40.1524298191\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.82234334946,124.116031351), test loss: 3.30216703415\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (19.1672935486,73.3465783666), test loss: 38.9636779785\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.79711723328,83.734405287), test loss: 3.24494508505\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (45.3404579163,65.9035995665), test loss: 35.5116269588\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (8.35338020325,63.54636249), test loss: 3.33826244771\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (32.1508331299,61.3904192989), test loss: 38.8910776615\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.10743618011,51.4301585081), test loss: 3.03178402781\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (10.6981964111,58.3308087079), test loss: 39.793758297\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.63465666771,43.3528645393), test loss: 3.24922650009\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (50.0219459534,56.1858377728), test loss: 37.6975432396\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.90881204605,37.5817592242), test loss: 3.36416820735\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (56.5131072998,54.5365638748), test loss: 33.8171586514\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.26983070374,33.2524856004), test loss: 3.30280048996\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (48.7113342285,53.2322822736), test loss: 39.2321253777\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (6.10473012924,29.8849551409), test loss: 2.89355273098\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (21.0700263977,52.1738106217), test loss: 39.305525732\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.53200721741,27.1886380362), test loss: 3.29273161888\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (30.3188247681,51.2973595871), test loss: 37.0103610039\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.97442674637,24.9848788009), test loss: 3.27879252434\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (39.637714386,50.5470592083), test loss: 34.805118227\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (9.63051795959,23.1482987619), test loss: 3.03330331445\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (32.1268196106,49.8985157475), test loss: 39.3581897736\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.83205962181,21.5922681603), test loss: 2.97251367718\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (60.7851142883,49.3343802688), test loss: 38.9305989742\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.77994680405,20.2584959069), test loss: 3.38713643253\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (11.5751533508,48.8340090151), test loss: 35.4673363686\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.0071849823,19.1026105024), test loss: 3.21951414347\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (71.4246292114,48.3837802756), test loss: 35.093307209\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.74462842941,18.0899831972), test loss: 2.99717045575\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (44.2118835449,47.9697951223), test loss: 38.0446058273\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.83362102509,17.1979448539), test loss: 3.0897962451\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (19.7669696808,47.5858585584), test loss: 37.6264967918\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.19913864136,16.4036802265), test loss: 3.36425477564\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (20.3928222656,47.2216785768), test loss: 33.9448890686\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.47768652439,15.6929993351), test loss: 3.1739795953\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (26.9166793823,46.9014378566), test loss: 36.2625600338\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.58283376694,15.0530770845), test loss: 2.99130907953\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (101.647697449,46.5993491065), test loss: 38.2979548931\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.80555582047,14.4738330807), test loss: 3.20408785641\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (33.5806312561,46.2953928968), test loss: 36.5685251236\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.79908066988,13.9464756337), test loss: 3.29912439585\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (39.7945709229,46.0085852071), test loss: 32.7441447735\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.55486130714,13.4641664691), test loss: 3.15939036608\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (85.97996521,45.7401374148), test loss: 34.8521894455\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.24870538712,13.0231024436), test loss: 2.97014786303\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (31.8172149658,45.4691133287), test loss: 36.6505543232\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.00008654594,12.6163586139), test loss: 3.20655230284\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (56.0242767334,45.2130005365), test loss: 35.8997697353\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.38895559311,12.2404174163), test loss: 3.23315121531\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (54.5189971924,44.9603857932), test loss: 32.1969855785\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.7084197998,11.892026843), test loss: 3.26000233144\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (22.0907669067,44.7109748375), test loss: 34.4409107685\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.6611456871,11.5680711659), test loss: 2.94267590493\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (35.7916488647,44.4678739776), test loss: 35.6863868237\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.31985116005,11.2658861789), test loss: 3.16974027753\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (26.7287712097,44.2238629678), test loss: 34.13473382\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.48190522194,10.9845479102), test loss: 3.30382954776\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (20.5551300049,43.979373797), test loss: 29.6920101643\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.23201131821,10.7204111991), test loss: 3.20444826782\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (56.0946273804,43.7332390278), test loss: 34.0381351948\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.38434553146,10.472861317), test loss: 2.88528510928\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (30.6137428284,43.4928425741), test loss: 35.4126907825\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.05918526649,10.2396807763), test loss: 3.29730548263\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (24.2899837494,43.2534099143), test loss: 32.6369390011\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.09236621857,10.0199512233), test loss: 3.25868513584\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (26.1643371582,43.0064321638), test loss: 30.0577216148\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.84862279892,9.81231309171), test loss: 2.99910757542\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (73.4671173096,42.7589338479), test loss: 33.0727067947\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.67024087906,9.61596423407), test loss: 2.94151646346\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (42.9393577576,42.5122067611), test loss: 33.5402393341\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.23074626923,9.43034159546), test loss: 3.31611057818\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (15.3212795258,42.2560932084), test loss: 30.1013235569\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.23777484894,9.25392880456), test loss: 3.16327771693\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (37.7034225464,42.0069452145), test loss: 29.2782322407\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.12391233444,9.08600810235), test loss: 3.01742049456\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (16.5564231873,41.7528350476), test loss: 30.8077503443\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.41239142418,8.92633064408), test loss: 3.05309652686\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (47.8990478516,41.4994561951), test loss: 32.4153445244\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.49364089966,8.77399315008), test loss: 3.30121054649\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (17.2348899841,41.2444179171), test loss: 28.7652335644\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.34659051895,8.62847598683), test loss: 3.15769973993\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (38.3239746094,40.9915339674), test loss: 29.0275902271\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.0907664299,8.49020468777), test loss: 2.98088065684\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (18.814907074,40.7351828734), test loss: 31.2808904171\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.08359670639,8.3574168976), test loss: 3.13183395863\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (50.3100814819,40.4810684965), test loss: 31.854172945\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.24069666862,8.23058586537), test loss: 3.30786496103\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (24.2805042267,40.2269193524), test loss: 27.4878995419\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.26627898216,8.10872513731), test loss: 3.20239881277\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (32.883605957,39.9785104211), test loss: 27.6926483631\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.65988636017,7.99193964747), test loss: 2.9574074775\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (16.1665840149,39.7269655208), test loss: 29.9114168167\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.45237278938,7.87957103673), test loss: 3.11180447936\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (27.5670547485,39.479761537), test loss: 30.7539391518\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (7.32655239105,7.77204863731), test loss: 3.25222955644\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (15.3817749023,39.2347628407), test loss: 26.3188835859\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.43414366245,7.66834494368), test loss: 3.22146092355\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (12.7839298248,38.9872983694), test loss: 27.5496996403\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.35943984985,7.5686092944), test loss: 2.91483357847\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (16.3550186157,38.7471283887), test loss: 29.9473793745\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.95359683037,7.47230987656), test loss: 3.09202548563\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (18.253162384,38.5071507071), test loss: 28.9152325869\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.35774159431,7.37945894294), test loss: 3.28610090315\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (29.2757606506,38.2717046801), test loss: 25.2048866272\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.04940986633,7.28991609019), test loss: 3.14959724844\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.7742958069,38.0357433872), test loss: 28.1716722965\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.64048624039,7.20324774555), test loss: 2.85813071132\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (9.55525970459,37.8051701262), test loss: 29.1867948055\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.31987047195,7.11990671032), test loss: 3.21699348688\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (22.6238594055,37.5744418508), test loss: 28.1010758638\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (8.19125175476,7.03921124416), test loss: 3.24753081501\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (23.1076011658,37.3468109848), test loss: 25.9833891392\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.70372462273,6.96090233844), test loss: 3.00705470145\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (43.7013702393,37.1206531897), test loss: 27.884706974\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.32358074188,6.88507158036), test loss: 2.94134296775\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (19.4814319611,36.8984148722), test loss: 28.4271851063\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.86832094193,6.81164818023), test loss: 3.24881533086\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (16.5640907288,36.6762218471), test loss: 26.8430305243\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.24846827984,6.74029037227), test loss: 3.14878760129\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (39.7150344849,36.4587107996), test loss: 25.3705896616\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (5.48577880859,6.67163036934), test loss: 3.00320865959\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (24.7719287872,36.2425275267), test loss: 26.5289231777\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.11161994934,6.6045798589), test loss: 2.99480754435\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (21.1650772095,36.0262473386), test loss: 28.8176864862\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.81422448158,6.53957429979), test loss: 3.23075172603\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (17.9163990021,35.8136149488), test loss: 25.5616889715\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.984769761562,6.47629375856), test loss: 3.14322663546\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (45.6596298218,35.604331435), test loss: 25.5122564793\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.60016965866,6.41482984731), test loss: 2.96903418452\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (10.5278873444,35.3948298764), test loss: 27.1928719044\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.656541705132,6.35501463603), test loss: 3.02703502178\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (15.3486700058,35.1869995959), test loss: 28.7489825726\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.17160630226,6.29679129794), test loss: 3.20309204459\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (43.958732605,34.9844456879), test loss: 24.6457771897\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (4.66538524628,6.24045682106), test loss: 3.16457942724\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (19.5700416565,34.7803211864), test loss: 25.1445321083\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.09825015068,6.18537454146), test loss: 2.89775048792\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (19.5516777039,34.579255534), test loss: 26.6328838348\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.30402851105,6.13163797071), test loss: 3.0467098698\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (4.86662721634,34.3792389467), test loss: 28.2456395149\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.40525054932,6.0792066443), test loss: 3.28736285269\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (10.6231002808,34.1820936076), test loss: 23.7163205624\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.96642792225,6.02810465242), test loss: 3.15103670508\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (15.342335701,33.985221062), test loss: 25.6384846687\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.2097132206,5.97815310522), test loss: 2.87241400182\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.0087032318,33.7916332272), test loss: 26.8444890738\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.75375437737,5.9298041195), test loss: 2.99963736236\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (13.8533172607,33.5992176692), test loss: 26.9424558759\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.08674907684,5.88230163299), test loss: 3.21733538508\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.5204048157,33.4070194147), test loss: 23.417494607\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.5678062439,5.83602739895), test loss: 3.04161780477\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.0597600937,33.2166376682), test loss: 26.3600388288\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.6163649559,5.79069887177), test loss: 2.81812495589\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (11.2994747162,33.0293919619), test loss: 26.663732481\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.6401154995,5.74641128184), test loss: 3.13186758608\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (28.7788505554,32.8420509123), test loss: 26.5553810835\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.36241769791,5.7030875938), test loss: 3.21440408826\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (35.9349937439,32.6564329326), test loss: 24.5165709019\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.48134088516,5.66080441394), test loss: 2.9246052742\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (22.322019577,32.4734078155), test loss: 26.1838620186\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.67324018478,5.61962491351), test loss: 2.8849473983\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (12.82218647,32.2898630971), test loss: 27.3678604603\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.61267185211,5.57919608445), test loss: 3.17797358036\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (13.2760820389,32.1082092201), test loss: 25.6742357731\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.24451160431,5.53950462153), test loss: 3.15695919096\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (10.6688051224,31.9274920218), test loss: 24.5077668905\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.31641697884,5.50068301475), test loss: 2.93503737897\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (14.6385784149,31.748600891), test loss: 25.8406667709\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.8264786005,5.46261343364), test loss: 2.92769466937\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (12.7037239075,31.5703607569), test loss: 27.8357484579\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.23315131664,5.42526771696), test loss: 3.16818820536\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (40.8234596252,31.394482476), test loss: 24.0513000607\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.88504469395,5.38901582836), test loss: 3.10489166379\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (10.3999919891,31.2187828367), test loss: 24.9808433771\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.00954961777,5.35324913626), test loss: 2.90017234534\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (21.7148704529,31.0442637534), test loss: 25.6898650527\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.74604463577,5.31826293277), test loss: 3.00286546946\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (13.1061096191,30.8696387409), test loss: 27.9512176514\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.72201645374,5.28383887359), test loss: 3.20522212982\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (12.8603410721,30.6976463695), test loss: 23.8949656248\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.40119981766,5.25012443783), test loss: 3.14104063958\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (7.98809194565,30.5254706949), test loss: 24.9787913322\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.17426252365,5.21694739363), test loss: 2.87499762774\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (21.8775691986,30.3550646185), test loss: 25.8665571213\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.92591238022,5.18457446226), test loss: 3.0026707083\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.88693618774,30.1856184975), test loss: 27.8331589699\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.586735367775,5.15280862483), test loss: 3.28387029469\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (8.53539085388,30.0162730298), test loss: 23.4603886366\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.24377655983,5.12158406577), test loss: 3.13004364371\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (5.66242694855,29.8479473575), test loss: 26.1081762314\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.46801996231,5.09081399869), test loss: 2.83946306556\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (10.1382303238,29.6805299843), test loss: 26.2259061813\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.85226678848,5.06063007236), test loss: 2.93972355127\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (15.2103948593,29.5143115959), test loss: 26.7639719009\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.23570275307,5.03091256469), test loss: 3.17314578295\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (14.8934135437,29.3485171862), test loss: 23.4355017185\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.5764311552,5.00170847067), test loss: 2.95523209572\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (8.92759132385,29.1843072294), test loss: 26.5477627754\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.5570987463,4.97324098201), test loss: 2.7844354406\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (7.40364789963,29.0203846252), test loss: 26.8835454702\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (5.29470348358,4.94513812594), test loss: 3.12131923437\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (5.8423500061,28.8572589676), test loss: 27.2125936985\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.37103819847,4.91741605903), test loss: 3.28403283954\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (22.0713634491,28.6947384536), test loss: 25.0330347061\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.41837263107,4.89015118926), test loss: 2.92263614237\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (9.63402938843,28.5334035208), test loss: 27.4232861519\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.6995511055,4.86332618884), test loss: 2.84123309851\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (6.52408123016,28.3723447072), test loss: 28.1788291454\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.24254751205,4.83685811749), test loss: 3.19570870847\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.06506919861,28.2127563097), test loss: 26.5999428272\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.77091503143,4.8110594085), test loss: 3.23702513874\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (16.3248138428,28.05410971), test loss: 25.5937306404\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.35261726379,4.7854988512), test loss: 2.87062277645\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (18.4187984467,27.8957319312), test loss: 26.1367938519\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.47993254662,4.76037144564), test loss: 2.9243609488\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (7.84668636322,27.7378999941), test loss: 28.6858295441\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.5186085701,4.73556051894), test loss: 3.18877010643\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (24.1547966003,27.5815341615), test loss: 25.1079143047\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.35732936859,4.71111667788), test loss: 3.14458410144\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.94381523132,27.4257290593), test loss: 26.6494164944\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.726400911808,4.6870300941), test loss: 2.82060232162\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (6.57349061966,27.2705449789), test loss: 26.0839330673\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.17543673515,4.66331917662), test loss: 2.98672462702\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (17.9661140442,27.1171311541), test loss: 28.9850174189\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (4.29071521759,4.64009079166), test loss: 3.22804297805\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (14.4721727371,26.9638113477), test loss: 24.8299294233\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.67936372757,4.61711517853), test loss: 3.15495387912\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.36971378326,26.811295745), test loss: 26.0802366257\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.07775640488,4.59437127665), test loss: 2.89192236066\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (3.64026045799,26.6595857715), test loss: 26.5561965466\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.04247164726,4.57197649503), test loss: 2.97780967802\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (5.09885215759,26.5089412894), test loss: 28.9926176548\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.97286224365,4.54990119365), test loss: 3.27816367149\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.87741136551,26.3589646727), test loss: 24.6757613659\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.49552559853,4.5280731626), test loss: 3.1372951895\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (7.05705690384,26.2104108238), test loss: 26.7812823772\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.29356050491,4.50673320857), test loss: 2.85361854732\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (5.8107213974,26.0625384046), test loss: 26.9808927059\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.04746174812,4.48555142233), test loss: 2.94164278507\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (5.28606367111,25.9154197332), test loss: 28.4065703392\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.05747866631,4.46470358158), test loss: 3.23446254134\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (9.03914451599,25.7689460006), test loss: 24.9585777283\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.72694587708,4.44405068808), test loss: 2.94775342047\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (13.0536870956,25.6238920315), test loss: 27.3814530373\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.52885699272,4.42367629007), test loss: 2.77629102468\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (11.0817432404,25.4795656641), test loss: 28.2529989243\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.36585760117,4.40356962663), test loss: 3.10219863653\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (21.918636322,25.3361493101), test loss: 29.0997031212\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.58975195885,4.38375555246), test loss: 3.30134983659\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (7.61541938782,25.1939714698), test loss: 26.735042429\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.42566013336,4.3642976172), test loss: 2.89734596908\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (4.23666381836,25.0523327103), test loss: 28.4067483425\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.6320194006,4.34499033265), test loss: 2.81860038042\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (3.35428166389,24.9117744594), test loss: 29.9123895168\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.16880595684,4.32588258593), test loss: 3.24754053354\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.02954053879,24.7720842795), test loss: 28.1569316387\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.45468473434,4.30703228106), test loss: 3.24305735081\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (3.97770452499,24.6335319512), test loss: 27.6440233231\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.41251349449,4.28837840414), test loss: 2.82118897587\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (9.09586429596,24.4959198104), test loss: 27.4913613319\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.883048415184,4.2699321136), test loss: 2.9224937886\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (7.72477531433,24.3594877832), test loss: 30.6754686832\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.716464877129,4.25186872983), test loss: 3.26883057058\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (5.70036506653,24.2239290705), test loss: 27.513718605\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.615160524845,4.23390805216), test loss: 3.12770142555\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (9.14086055756,24.0893886918), test loss: 28.143752718\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.17007899284,4.21622283121), test loss: 2.85453689098\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (8.29532051086,23.9555838907), test loss: 27.970975256\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.47997403145,4.19868164412), test loss: 2.95511133075\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (7.43984365463,23.8231110935), test loss: 30.7243458748\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.29343450069,4.18131968025), test loss: 3.19519254565\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (2.40832734108,23.6914414271), test loss: 26.6281785011\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.72118639946,4.16417392889), test loss: 3.12783517241\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (7.99708652496,23.5610486114), test loss: 28.1202530384\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.77611637115,4.14727765089), test loss: 2.86255086958\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (5.02630996704,23.4316444336), test loss: 28.0369198799\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.794489741325,4.1306323047), test loss: 2.93446469456\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (3.00999474525,23.303078976), test loss: 31.1454112053\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.11795866489,4.11412453749), test loss: 3.28858283758\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (2.46110773087,23.1755806039), test loss: 26.8027143002\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.16483521461,4.09774549992), test loss: 3.14668217748\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (3.0709695816,23.0490314081), test loss: 28.322710228\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.93953108788,4.08158497966), test loss: 2.81192243397\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.14681386948,22.9237817587), test loss: 28.1426635742\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.508705258369,4.06555334034), test loss: 2.96522201598\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (4.1758480072,22.7994216871), test loss: 30.0035338879\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.24061954021,4.04970838361), test loss: 3.23172423989\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (5.26626682281,22.6763104737), test loss: 26.5936364174\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.55020534992,4.03416083932), test loss: 2.93719836473\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (4.39648675919,22.5540672412), test loss: 28.4232002735\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (3.13371419907,4.01872722045), test loss: 2.77271613181\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (2.30341768265,22.4328402201), test loss: 30.1212434292\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.30881619453,4.00343463472), test loss: 3.13912437558\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.71069812775,22.3125322314), test loss: 30.5889608383\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.45834815502,3.9882882555), test loss: 3.26162799597\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (4.57946491241,22.1934532801), test loss: 29.0694433689\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.26836740971,3.97329688249), test loss: 2.85048642009\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.5087287426,22.0751851392), test loss: 29.6661644459\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.32629156113,3.958449925), test loss: 2.81984808892\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (2.44387888908,21.9582284831), test loss: 31.7279260635\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (4.66570091248,3.94387314945), test loss: 3.28897199929\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.78101873398,21.8422331878), test loss: 29.2135889053\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.2238919735,3.92938852702), test loss: 3.15281179845\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (6.91077232361,21.7271532195), test loss: 29.1908597946\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.72006416321,3.9150637911), test loss: 2.80780075788\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.1001303196,21.6130367855), test loss: 28.7059150696\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.56034636497,3.90085413125), test loss: 2.91377505064\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.77755641937,21.4999330815), test loss: 31.7520577431\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.878497362137,3.88678210904), test loss: 3.26800196022\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.61297225952,21.3879897968), test loss: 28.2358346462\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.86437487602,3.87285516501), test loss: 3.06129129529\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (2.07546281815,21.2768968275), test loss: 29.3310583591\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.37719202042,3.85907324017), test loss: 2.86111635566\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (2.86283063889,21.1670099409), test loss: 28.5348590374\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (4.36273002625,3.84552753145), test loss: 2.90251127183\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 6\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (224.570373535,inf), test loss: 191.230906677\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (358.819946289,inf), test loss: 380.26872406\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (17.0897827148,132.291317282), test loss: 48.4201508522\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (33.8722763062,242.936846127), test loss: 22.4583990097\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (16.910533905,88.5311776233), test loss: 40.8500322104\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.64300847054,123.564020904), test loss: 2.99939656854\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (59.8049087524,73.7384027418), test loss: 40.3497662544\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.36192941666,83.3622011479), test loss: 3.31270867586\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.3402557373,66.3275608118), test loss: 38.6246148586\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.08778738976,63.2569870843), test loss: 3.3625967294\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (46.9305610657,61.8204663021), test loss: 35.4230722904\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.429218411446,51.1941724115), test loss: 3.26263309419\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (80.7244873047,58.8328387051), test loss: 40.0529260635\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.22024583817,43.1501934505), test loss: 2.91395035982\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (14.9958715439,56.6559552644), test loss: 39.5805459023\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.02085876465,37.4041884338), test loss: 3.15969394445\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (52.9876937866,55.0191815779), test loss: 38.9518749237\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.28101730347,33.0953454147), test loss: 3.13480695188\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (40.8124542236,53.7349941372), test loss: 35.4756608486\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.2833199501,29.7454730849), test loss: 3.1612891078\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (98.6784896851,52.6848665487), test loss: 38.7544326782\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (7.17833518982,27.0634509393), test loss: 2.9759745419\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (15.9104499817,51.8085556284), test loss: 39.9157170773\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.62624597549,24.8681552302), test loss: 3.08708929121\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (25.9093399048,51.0536872928), test loss: 38.9173076153\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.50244569778,23.039226867), test loss: 3.25607263744\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (15.9386405945,50.4004101817), test loss: 35.6973244667\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.41636133194,21.4906322891), test loss: 3.08177520186\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (51.736038208,49.837605304), test loss: 34.3345538855\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.7452018261,20.1633245169), test loss: 2.9969982028\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (50.1940078735,49.33810032), test loss: 39.6357234001\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.45294308662,19.0126822915), test loss: 2.87917283177\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (31.0385704041,48.8799769377), test loss: 39.3403526783\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.56660175323,18.0059843088), test loss: 3.22286833823\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (28.485742569,48.4720686491), test loss: 38.0607457161\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.07444095612,17.1171589437), test loss: 3.15035144389\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (27.6259803772,48.100143912), test loss: 33.4449717522\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.56590366364,16.3264817866), test loss: 3.22018456459\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (70.7419433594,47.7469102311), test loss: 37.6086429119\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.33996748924,15.6188176018), test loss: 2.87377024293\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (42.6514587402,47.4241223414), test loss: 37.7764452934\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.456585526466,14.9810716713), test loss: 3.18585255146\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (41.4662094116,47.1120662236), test loss: 37.1381742477\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.93580174446,14.4040594582), test loss: 3.20847616196\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (104.421325684,46.8214463599), test loss: 33.7498209715\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.05880451202,13.8795410217), test loss: 3.0741997987\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (29.2554740906,46.5376207111), test loss: 34.5308600903\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.50786018372,13.4006498873), test loss: 2.91289439201\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (65.0956420898,46.2667963718), test loss: 37.7426906109\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.77682495117,12.960947971), test loss: 2.96915483326\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (12.6841783524,45.996297299), test loss: 36.6292059183\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.22017478943,12.5556404649), test loss: 3.26164338589\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (39.9489059448,45.7348788413), test loss: 35.8980994225\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.24440097809,12.1817430599), test loss: 3.22847804129\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (33.5614280701,45.4781330537), test loss: 31.9095967293\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.21735966206,11.8347371763), test loss: 3.15566197336\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (22.968503952,45.2300058998), test loss: 36.4843496323\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.99960613251,11.5125140832), test loss: 2.90860151052\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (28.7077941895,44.9857426925), test loss: 36.1251161575\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.60146784782,11.2120723785), test loss: 3.09071850777\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (49.6636352539,44.7406890557), test loss: 35.1548138142\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.44845223427,10.9314787897), test loss: 3.05912851989\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (33.3079299927,44.5007245094), test loss: 30.8235846996\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (0.705389976501,10.668524494), test loss: 3.11756169796\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (28.3738327026,44.2627106588), test loss: 33.9225660801\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.46275377274,10.4216863252), test loss: 2.89978406429\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (29.7609081268,44.0204581782), test loss: 35.2737823009\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.49765491486,10.1892673745), test loss: 3.07693728805\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (42.0631713867,43.7801010275), test loss: 34.9408465147\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.27596211433,9.97002161494), test loss: 3.20940495729\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (28.3215713501,43.5329132157), test loss: 31.1585695744\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.79868769646,9.76295427376), test loss: 3.01234469712\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (73.6634979248,43.2890327225), test loss: 30.4272582769\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.33830213547,9.5674490122), test loss: 2.93040230721\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (36.5931816101,43.0411025069), test loss: 33.0299053192\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.554166018963,9.38216908201), test loss: 2.89539063871\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (53.6058387756,42.7938082101), test loss: 34.0324796677\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.85628461838,9.20623842583), test loss: 3.19917768389\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (7.13994932175,42.5391677606), test loss: 32.6137814283\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.801509380341,9.03863771082), test loss: 3.27351156771\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (25.772567749,42.2856974871), test loss: 28.8048464775\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.41274428368,8.87954960722), test loss: 3.24649618566\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (23.102355957,42.0316554238), test loss: 31.1589234352\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.59896600246,8.72759341315), test loss: 2.88862123489\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (32.2319374084,41.7800311281), test loss: 31.63626647\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.66949605942,8.58286406509), test loss: 3.08700140715\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (27.5823402405,41.5265931795), test loss: 32.6984930515\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.54972362518,8.44432413723), test loss: 3.15993634909\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (48.8562049866,41.2737525306), test loss: 28.9644723892\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.19620227814,8.31206791987), test loss: 3.20792581141\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (64.5448532104,41.0238391849), test loss: 29.7433853149\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.6254928112,8.18525338681), test loss: 3.0004275471\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (32.794128418,40.7734889792), test loss: 31.558249855\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.32241368294,8.06373907505), test loss: 2.99500300884\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (31.6525783539,40.5240017984), test loss: 31.9625289917\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.03137660027,7.94702220471), test loss: 3.24973066747\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (14.965265274,40.276464292), test loss: 29.4518113136\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.47236311436,7.83486098205), test loss: 3.12048401237\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (60.3291702271,40.0316010917), test loss: 27.1481980324\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.90894448757,7.72719910118), test loss: 3.04310609102\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (32.3682518005,39.7888695193), test loss: 29.8632993698\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.98552632332,7.62392843353), test loss: 2.87397342026\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (47.1496429443,39.548122568), test loss: 30.4608564615\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.2598361969,7.52458055443), test loss: 3.04270527959\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (22.1485271454,39.3089980713), test loss: 31.4609735012\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.18823480606,7.42859301825), test loss: 3.16506290436\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (13.159992218,39.0689213373), test loss: 27.1930877924\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.06792354584,7.33585716954), test loss: 3.19319335222\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (12.654001236,38.8322970913), test loss: 28.3020073652\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.960896134377,7.24665747568), test loss: 2.89091923386\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (31.4258422852,38.5985441699), test loss: 29.970367384\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.2694041729,7.16033590371), test loss: 3.17638979107\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (22.8189582825,38.3678697218), test loss: 31.337936306\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.82671880722,7.07698924732), test loss: 3.26508973539\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (11.4592819214,38.1377312785), test loss: 27.6345687389\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.06923520565,6.99622425436), test loss: 3.15479960442\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (41.0914230347,37.9123734126), test loss: 27.2340125084\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.68205857277,6.91826194558), test loss: 2.99511755407\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (80.5225067139,37.6895040968), test loss: 28.3273291111\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.87662887573,6.84268091634), test loss: 2.93414434493\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.3184452057,37.4678637294), test loss: 29.4465633392\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.98678731918,6.76947772587), test loss: 3.18690814674\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (33.906211853,37.2486205098), test loss: 29.1732399464\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.66649651527,6.69842761404), test loss: 3.32396779358\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.3648443222,37.0316464928), test loss: 25.4804144859\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.58351194859,6.6294383136), test loss: 3.18275254667\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (63.3838500977,36.8177454184), test loss: 27.7932742119\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.51857471466,6.56265273518), test loss: 2.93634649962\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.9521751404,36.6062992567), test loss: 28.5305864334\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (6.64993095398,6.49815906356), test loss: 3.0314485386\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (31.0960960388,36.3965141675), test loss: 29.4233524561\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.1827044487,6.43526397315), test loss: 3.12098874748\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (21.9708938599,36.189041822), test loss: 26.3765260696\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.01175200939,6.37411008585), test loss: 3.17202565223\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (15.351266861,35.9811094581), test loss: 26.9329860687\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.16926598549,6.31454457986), test loss: 2.94149007052\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (19.6161422729,35.7754321806), test loss: 27.89228971\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.37334990501,6.25684363496), test loss: 3.01247652322\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (21.6827125549,35.5718553049), test loss: 28.9990310192\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.43693614006,6.20051796677), test loss: 3.22548405081\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.0483570099,35.3701971189), test loss: 26.7495829105\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.25906777382,6.145719588), test loss: 3.10735889673\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (16.2931289673,35.1697587825), test loss: 24.7797620773\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.68825888634,6.09229937545), test loss: 2.96987431496\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (10.9495916367,34.9716340831), test loss: 27.4205901146\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.05798649788,6.04031988604), test loss: 2.8629687041\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (23.0757904053,34.7765362088), test loss: 27.6858936787\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.778682470322,5.98958519747), test loss: 3.14606823921\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (3.48774623871,34.5809669355), test loss: 28.4923120499\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.84704768658,5.94016190338), test loss: 3.25631160885\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (15.6045017242,34.3884988472), test loss: 24.5236735344\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.04392039776,5.89184036989), test loss: 3.19167670012\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.3480892181,34.1965400664), test loss: 25.7839595795\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.885286271572,5.84464221796), test loss: 2.8719952181\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (17.08452034,34.0077262801), test loss: 26.404889822\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (4.68603086472,5.79880677004), test loss: 3.0035432592\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (21.5502719879,33.8196941786), test loss: 28.8854989529\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (6.01625919342,5.75410592567), test loss: 3.22889021039\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.59677886963,33.6335406317), test loss: 25.3244258165\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.13183927536,5.710312879), test loss: 3.166183424\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (9.21919059753,33.4478945461), test loss: 25.1963298559\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.5231590271,5.66749509698), test loss: 2.90386452824\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (9.69071388245,33.2635083901), test loss: 26.2938748837\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.01928305626,5.62558886177), test loss: 2.95435046405\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (14.8318881989,33.0796968233), test loss: 27.7664330006\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.14744400978,5.58470324048), test loss: 3.14563435763\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (32.7523117065,32.898613371), test loss: 26.6153138161\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.64899492264,5.54463134529), test loss: 3.17538819611\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.8580551147,32.7170295248), test loss: 23.9554394245\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.16547524929,5.50540698232), test loss: 3.06557566524\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (15.4282264709,32.5371244146), test loss: 26.504330492\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (4.26326274872,5.4670235679), test loss: 2.85088839829\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (3.95295572281,32.3584157079), test loss: 26.4185681343\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.69031763077,5.42944389609), test loss: 2.91115548909\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (18.889465332,32.1819370893), test loss: 28.315512228\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.10379981995,5.39257472252), test loss: 3.18431185484\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.690782547,32.005176527), test loss: 24.4188922405\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.49191749096,5.35650079763), test loss: 3.07025890648\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (6.26309585571,31.8296329253), test loss: 25.8082613945\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.532916784286,5.32104498733), test loss: 2.8515810281\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.85409545898,31.6549836903), test loss: 26.8889964581\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.93275904655,5.28633632914), test loss: 2.99226697981\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (10.6430072784,31.4818041715), test loss: 27.9923978567\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (5.25912237167,5.25240491445), test loss: 3.2077482909\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (24.1035671234,31.3099441591), test loss: 25.5566838026\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (4.80123567581,5.21927702396), test loss: 3.1545893386\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (9.21901130676,31.1377439716), test loss: 24.5714081764\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.55183601379,5.1865716405), test loss: 2.90401423275\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (17.6676292419,30.967073832), test loss: 26.0312695503\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.58715486526,5.15446423013), test loss: 2.82224270105\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.11365795135,30.7960008419), test loss: 26.6615918636\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (5.34125471115,5.12300171201), test loss: 3.0815770328\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (12.2427330017,30.6263869725), test loss: 27.3020082474\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.01083230972,5.09208413259), test loss: 3.28768624365\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (9.23740005493,30.4573462169), test loss: 23.5013983727\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.80169820786,5.06168221528), test loss: 3.14483658075\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (7.56040477753,30.2889910976), test loss: 26.5160552502\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.13245677948,5.03183947649), test loss: 2.80081041455\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (10.4422626495,30.1207681325), test loss: 25.2848733425\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (3.15315771103,5.00251445089), test loss: 2.87237234116\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (13.6809234619,29.9544809419), test loss: 28.0450278759\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.88200414181,4.97370646771), test loss: 3.13827704191\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (8.79039001465,29.7883733179), test loss: 24.7621677876\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.6552823782,4.94530212975), test loss: 3.14083248377\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (11.691781044,29.6231690608), test loss: 25.7003747463\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.273539125919,4.91744100189), test loss: 2.82408291698\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (25.7772464752,29.4584640135), test loss: 25.8461630344\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.63071358204,4.88997501623), test loss: 2.88226086199\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (10.1215648651,29.2941304859), test loss: 27.4844677448\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (3.545535326,4.86298383798), test loss: 3.09069725275\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (12.4092445374,29.1310028662), test loss: 26.2634659767\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.21142482758,4.83652049275), test loss: 3.07545796782\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (16.4484882355,28.9689139582), test loss: 24.0319602489\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (3.61247825623,4.81057434427), test loss: 2.92819964886\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (17.4532814026,28.8069611411), test loss: 26.288877058\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (4.39935970306,4.78490202714), test loss: 2.79933476746\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (10.4691896439,28.6457832421), test loss: 26.2344600677\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.58169412613,4.75961599092), test loss: 2.96105364263\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.918050766,28.4849147696), test loss: 28.5918271542\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (4.17669820786,4.73476970181), test loss: 3.17224415392\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (14.7964391708,28.3248828542), test loss: 24.0258770943\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.43380177021,4.71019884747), test loss: 3.07663469017\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (6.57760381699,28.1658029422), test loss: 26.0166402817\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.82205486298,4.68605177911), test loss: 2.76746996939\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (4.6822719574,28.0067783344), test loss: 27.0102777004\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.17269229889,4.66223081504), test loss: 2.92152474523\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (9.57029151917,27.8490620846), test loss: 27.9159540176\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.51294779778,4.63880694629), test loss: 3.11346088648\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.36183071136,27.6917954712), test loss: 25.5114599705\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.30185031891,4.61568806313), test loss: 3.08066643775\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (6.46772384644,27.5358188449), test loss: 25.084840107\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.34205031395,4.59286570515), test loss: 2.77311709821\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (10.6505031586,27.3800922438), test loss: 27.9644515991\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.87294703722,4.57038258082), test loss: 2.77930507809\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (4.89865970612,27.2255889483), test loss: 27.4305526733\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.2792943716,4.54817373949), test loss: 3.07604625523\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (19.465259552,27.071333572), test loss: 28.3728132725\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.42905962467,4.52629025344), test loss: 3.1867211327\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (26.2209911346,26.9186084981), test loss: 24.3599189758\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.81743717194,4.50481569128), test loss: 3.04195011407\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (4.87454509735,26.7661055869), test loss: 26.6310954094\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.97520583868,4.48364909705), test loss: 2.82577199042\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (10.9654045105,26.6148887069), test loss: 26.221105051\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.20932674408,4.46273087943), test loss: 2.82458518147\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (7.20402002335,26.4639863948), test loss: 29.2498810291\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.34434604645,4.44200157522), test loss: 3.05616973341\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (11.5271873474,26.3141534025), test loss: 26.0824886322\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.732853114605,4.4216170178), test loss: 3.02476777434\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (14.21235466,26.1650368009), test loss: 27.4399138927\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.35701227188,4.40143466589), test loss: 2.77549518645\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (7.37308883667,26.0169173035), test loss: 28.3191716194\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.58156037331,4.38153578448), test loss: 2.87100641429\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (3.26866483688,25.8693638097), test loss: 28.9070750713\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.84729385376,4.36189494235), test loss: 3.08196892291\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.70208644867,25.7229432271), test loss: 28.0223883152\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.878205776215,4.34249623644), test loss: 2.94376567006\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (3.81787443161,25.5773962549), test loss: 25.9517525196\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.46939229965,4.32335300514), test loss: 2.82043292522\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (9.25851821899,25.433074142), test loss: 29.2427267075\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.27147841454,4.30441105924), test loss: 2.73309198022\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (5.40796661377,25.2894353145), test loss: 29.0266403198\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.99461555481,4.28569370323), test loss: 2.97077770233\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (2.63345599174,25.1468279498), test loss: 29.7219053268\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.820988416672,4.26717748071), test loss: 3.23037664592\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (9.01006221771,25.0052244147), test loss: 26.4765754223\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.63675379753,4.2489263602), test loss: 3.0922030881\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (17.0859718323,24.8646940324), test loss: 27.7680436134\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (3.17975902557,4.2309538748), test loss: 2.7705493927\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (5.3245677948,24.7251472713), test loss: 26.869352293\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.7611566782,4.21319846146), test loss: 2.86200428158\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (11.2420167923,24.5865770406), test loss: 30.3885187149\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (3.37176942825,4.19563212044), test loss: 3.07645492703\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (3.77772378922,24.4490172002), test loss: 28.3067250729\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.433838903904,4.17819373384), test loss: 3.12579137683\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.65401172638,24.3122705671), test loss: 28.1464172363\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.17788505554,4.16101425053), test loss: 2.74984498173\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (5.64561462402,24.1768586112), test loss: 29.556065321\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.79085111618,4.14399190583), test loss: 2.80377402902\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (5.04565048218,24.0422582028), test loss: 31.1024702072\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.76358580589,4.12718624535), test loss: 3.09080167413\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (3.75759029388,23.9087053678), test loss: 29.2088728905\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.27373099327,4.11054725729), test loss: 3.05987320244\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (6.7048664093,23.7762257678), test loss: 27.2073719025\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.9254784584,4.09411660958), test loss: 2.87215546966\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.12927532196,23.6448272179), test loss: 29.65352211\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.973480343819,4.07784177539), test loss: 2.73840094805\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (3.99537158012,23.5145814371), test loss: 29.8186855316\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.4646384716,4.06175090784), test loss: 2.78211105019\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.11471796036,23.3853581263), test loss: 31.953545475\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.867959856987,4.04581021737), test loss: 3.07159956396\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (2.08697986603,23.2571598092), test loss: 28.3458688736\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.02245461941,4.03004413613), test loss: 2.99543358982\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (3.84214305878,23.1301193235), test loss: 29.6879340649\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.52114462852,4.01447731795), test loss: 2.76240755767\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (6.93806362152,23.0042855807), test loss: 29.3434940338\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.95520949364,3.99914340194), test loss: 2.94688907415\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (4.49631690979,22.8793821877), test loss: 31.5827644348\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.30840802193,3.98396234269), test loss: 3.14846607745\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.38296127319,22.7556503283), test loss: 29.2370038986\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.29927861691,3.96894603284), test loss: 3.02842658311\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (4.22524738312,22.6328998557), test loss: 29.1967741966\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.475208729506,3.95399604034), test loss: 2.78826308846\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (5.26301002502,22.5112263132), test loss: 30.5351773262\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.07697355747,3.93929154637), test loss: 2.7315242812\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (3.6677839756,22.3906601242), test loss: 31.7325120449\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.57959842682,3.92468636557), test loss: 3.05808357745\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (2.92831683159,22.2711323691), test loss: 32.5872276306\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.49676394463,3.91027384113), test loss: 3.25965756774\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (3.30424022675,22.1525735296), test loss: 28.8237851143\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.45902717113,3.89595390485), test loss: 2.99143675715\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.77549386024,22.0351858136), test loss: 30.7451414108\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.09191465378,3.88182425612), test loss: 2.78435997665\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (8.49114894867,21.9188200722), test loss: 29.5921500206\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.84807485342,3.86780535346), test loss: 2.83949627876\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (2.94717788696,21.8036072004), test loss: 32.7485552788\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.51870465279,3.85394000116), test loss: 3.00205453187\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.44418239594,21.6893315166), test loss: 30.4256664276\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.57835566998,3.84019695021), test loss: 3.01686479449\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (2.28369235992,21.5761386106), test loss: 31.9669077873\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.982439935207,3.82657533512), test loss: 2.72850365937\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 7\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (334.898132324,inf), test loss: 188.23385582\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (332.591949463,inf), test loss: 378.440730286\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (34.6526832581,132.1270347), test loss: 43.6842406273\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (15.3237781525,242.941370014), test loss: 22.941457653\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (36.4608230591,88.1953993192), test loss: 38.7345162868\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.50552368164,123.516464362), test loss: 2.95967549682\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (43.6564674377,73.3022650232), test loss: 38.8175854206\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.96593546867,83.2966482672), test loss: 3.09754768014\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (44.4932823181,65.8453907878), test loss: 37.8086803436\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.58951163292,63.1901835897), test loss: 3.16067778468\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (109.568267822,61.3183537762), test loss: 37.6578677654\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.01110935211,51.1258674585), test loss: 3.15510503054\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (41.1282424927,58.2796779668), test loss: 34.117139864\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.23421978951,43.0798157774), test loss: 3.15864246786\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (44.5471954346,56.1058969732), test loss: 34.9311882019\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.58878326416,37.3333680279), test loss: 3.03732067943\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (19.3217468262,54.4293038635), test loss: 37.8976187706\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.55777072906,33.0223714725), test loss: 2.84612435699\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (27.684009552,53.1262949823), test loss: 38.9354010344\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.83520960808,29.6696584171), test loss: 3.22589035928\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (39.0875205994,52.0704623298), test loss: 38.0572313786\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.882945597172,26.9875777604), test loss: 3.22724131942\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (27.7483024597,51.1725141859), test loss: 36.5819898605\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (0.962943077087,24.7923154348), test loss: 3.22521855235\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (20.9064979553,50.4252405149), test loss: 32.9561338425\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.924836158752,22.963256342), test loss: 3.15413178802\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (110.935768127,49.7864129976), test loss: 35.589853096\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.17795324326,21.4154393651), test loss: 2.95076440275\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (29.9442691803,49.2079286785), test loss: 38.4969669342\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (5.01623153687,20.0881718246), test loss: 2.93780329525\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (20.8091335297,48.700607009), test loss: 37.7074311733\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (6.48091840744,18.937209486), test loss: 3.19757056236\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (49.5095214844,48.2465003364), test loss: 36.2551751614\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.6798555851,17.92961861), test loss: 3.19606195688\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (52.030002594,47.8232624091), test loss: 33.0468319416\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.46873831749,17.0406349917), test loss: 3.05395070016\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (49.1385269165,47.4282412315), test loss: 32.8488495827\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.17322635651,16.2498190609), test loss: 3.05387606621\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (40.7090454102,47.0708481305), test loss: 36.3657829762\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.16974687576,15.5419495999), test loss: 2.89674674273\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (34.2325286865,46.736285365), test loss: 36.3650127411\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (8.99036407471,14.9045849934), test loss: 3.06673400402\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (15.0191774368,46.4210089363), test loss: 36.2929660797\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.50270390511,14.3269572726), test loss: 3.21846487522\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (29.1693611145,46.1209294889), test loss: 35.2352561474\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.32583999634,13.8021463623), test loss: 3.11913115382\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (25.5372962952,45.8212008387), test loss: 31.798102212\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.22012424469,13.3220432518), test loss: 3.13597128838\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (43.435508728,45.5405300786), test loss: 33.0032269478\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.2849187851,12.8820281849), test loss: 3.00462331772\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (102.997589111,45.2634377031), test loss: 34.2051223755\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (5.01370477676,12.4764056484), test loss: 2.84164275229\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (83.6255950928,44.993611163), test loss: 35.6336338997\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.29221558571,12.1017515797), test loss: 3.16397343874\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (24.7112350464,44.7238136316), test loss: 35.3361896276\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.07706451416,11.7548120415), test loss: 3.22924173474\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (63.5828056335,44.462829267), test loss: 33.3520058155\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.66522216797,11.4324835748), test loss: 3.22810139954\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (52.0132522583,44.2012467709), test loss: 29.7836531639\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.595283150673,11.1316372724), test loss: 3.15569957495\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (23.6164360046,43.9448455371), test loss: 31.9617420673\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.873745083809,10.8507870103), test loss: 2.90677457005\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (34.8851623535,43.683816443), test loss: 34.0120278835\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.27350103855,10.5876198228), test loss: 2.94807765782\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (50.3577537537,43.4265328185), test loss: 33.3663487911\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.50361919403,10.340397488), test loss: 3.13579911888\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (21.118938446,43.1664050085), test loss: 32.6558609486\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (0.956108868122,10.1074794692), test loss: 3.17631593943\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (38.6022949219,42.9060772717), test loss: 29.1591514587\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.01914024353,9.88830110657), test loss: 3.11736694425\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (61.7065429688,42.6407376638), test loss: 28.3548556328\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.17611217499,9.6812450463), test loss: 3.06986131221\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (25.6400184631,42.3713676823), test loss: 30.653184557\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.78001618385,9.48518179759), test loss: 2.92969389707\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (24.2200756073,42.1044424154), test loss: 31.8130431652\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.43951368332,9.2995197136), test loss: 3.06695530117\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (26.3830299377,41.835719498), test loss: 32.1113665581\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.00385570526,9.12281931275), test loss: 3.23915184736\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (27.995475769,41.5667138151), test loss: 30.7280781507\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.52617549896,8.95538759655), test loss: 3.23510451615\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (75.1374816895,41.294327292), test loss: 27.2153174639\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.85732769966,8.79602872245), test loss: 3.22587454915\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (29.8608398438,41.0193658701), test loss: 28.1133115768\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.842253923416,8.64386015608), test loss: 3.05954410136\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (34.7614212036,40.7450041788), test loss: 28.4154281139\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.60058689117,8.49874992297), test loss: 2.94614971727\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (7.16309261322,40.4686139613), test loss: 29.5786568165\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (5.47008037567,8.36001943605), test loss: 3.14476430714\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (38.1703834534,40.1949391169), test loss: 30.8938585758\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.30125808716,8.22732988275), test loss: 3.28179969192\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (33.285823822,39.9237796412), test loss: 28.4462829828\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.940046668053,8.10043817831), test loss: 3.30118907988\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (25.9358978271,39.6524459633), test loss: 25.8848006964\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.1109303236,7.97876982526), test loss: 3.21970871836\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (9.62144851685,39.3840499847), test loss: 27.0216011047\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.54981517792,7.86214114042), test loss: 2.90369869173\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (67.0580444336,39.1203353461), test loss: 27.9841899872\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.30455875397,7.75005596635), test loss: 3.05470215678\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (15.499162674,38.8550381658), test loss: 29.1559144497\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.59570932388,7.64228670619), test loss: 3.1440236479\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.7823905945,38.5951344499), test loss: 29.0567520142\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (5.13640594482,7.53853515985), test loss: 3.26671377718\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (12.8673801422,38.338851862), test loss: 26.1677652836\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.40523934364,7.43863573755), test loss: 3.2386007905\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (23.3387737274,38.084864456), test loss: 25.267431283\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.67201805115,7.34251009057), test loss: 3.15103726238\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (34.1077575684,37.8328480156), test loss: 26.0398657799\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.14439058304,7.24974637156), test loss: 2.98161776066\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (17.6822471619,37.5841204368), test loss: 27.1860198975\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.65571498871,7.16025126155), test loss: 3.08664728999\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (20.4579143524,37.3400963679), test loss: 28.2335735798\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (9.06943893433,7.07391915144), test loss: 3.25200680792\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (15.7750997543,37.098393234), test loss: 27.6049391508\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.54402828217,6.99022307612), test loss: 3.31653269529\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (16.4316806793,36.8593219128), test loss: 24.3472315192\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.44336628914,6.90958545048), test loss: 3.21081106216\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.9405527115,36.621510699), test loss: 25.3748925924\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.17407739162,6.83143646632), test loss: 3.06859259754\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (23.7099647522,36.3866996071), test loss: 25.2872876644\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.89788007736,6.75588000499), test loss: 2.95570081174\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (53.013835907,36.154589107), test loss: 26.1497475147\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.29296302795,6.68247502325), test loss: 3.10363511741\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (50.9517822266,35.925259693), test loss: 28.1476398945\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.8486790657,6.61142367964), test loss: 3.22880188227\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (10.0225028992,35.6983565609), test loss: 25.5571235657\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.54636991024,6.54260933191), test loss: 3.27409336269\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.0612154007,35.475587352), test loss: 23.350887537\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.09293341637,6.47594723915), test loss: 3.22373561561\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (24.6319789886,35.2546145674), test loss: 24.2861607075\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.0710170269,6.41113119157), test loss: 2.87681681365\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (6.2297873497,35.0373991754), test loss: 25.1932681561\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.607014536858,6.34822358656), test loss: 2.99389823973\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (17.5430774689,34.8209597249), test loss: 26.3593791962\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.95955634117,6.28710549914), test loss: 3.05896021426\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (38.747253418,34.6089697059), test loss: 26.5075983524\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.12805700302,6.22760017654), test loss: 3.20561154485\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (7.77471733093,34.397976053), test loss: 24.2564004302\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.1537822485,6.16965425404), test loss: 3.21278679967\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (20.9276313782,34.1898125763), test loss: 23.4197265625\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.39999675751,6.11341695708), test loss: 3.07812948674\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (20.8053798676,33.9838084003), test loss: 23.6746409774\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.61888885498,6.05861434452), test loss: 2.97300188392\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.8446273804,33.7788010238), test loss: 24.6084685326\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.86357545853,6.00523099052), test loss: 3.04726014137\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (8.02495384216,33.5773386862), test loss: 25.9078884125\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.95003557205,5.95326150124), test loss: 3.17734377682\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (15.0535583496,33.378118805), test loss: 25.60262146\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.58527219296,5.90243419138), test loss: 3.28864783347\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (21.3982009888,33.1802875107), test loss: 22.7537291288\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.65964865685,5.85307958218), test loss: 3.13590010703\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (35.788356781,32.9845298011), test loss: 23.7598537207\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.71756744385,5.80494761466), test loss: 3.0120293811\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (34.6394042969,32.7889089226), test loss: 23.8065184832\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.52420389652,5.75786418675), test loss: 2.87797138244\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (26.4093551636,32.5956728759), test loss: 24.6444011211\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.68960046768,5.71194446129), test loss: 3.07415879965\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (10.849811554,32.4040629622), test loss: 26.998671484\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (5.47569656372,5.66707661827), test loss: 3.22943785787\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (24.7502670288,32.2141135375), test loss: 24.2443387747\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.12436628342,5.62325075045), test loss: 3.18636643142\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (16.6290855408,32.0273868979), test loss: 22.0633751392\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.708144307137,5.58051324164), test loss: 3.12317956984\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (20.2338562012,31.8412866273), test loss: 23.7248328686\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.21965563297,5.53876417944), test loss: 2.82969430685\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (9.02346992493,31.6573490525), test loss: 24.0082577705\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.57325100899,5.49798501309), test loss: 2.89981651902\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (33.7453308105,31.4752442219), test loss: 25.0939967632\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.59290254116,5.45805906694), test loss: 3.00101960599\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (7.04651975632,31.2931149606), test loss: 25.3168846846\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.51297330856,5.41898840998), test loss: 3.17224701345\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.1828746796,31.11352073), test loss: 22.6057531118\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.57487654686,5.38069242845), test loss: 3.22140570581\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (4.25471591949,30.9354154223), test loss: 22.4727460861\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.08485841751,5.34323165428), test loss: 3.01932234466\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (12.3462905884,30.7590524716), test loss: 23.1719285011\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (3.48002576828,5.3065888343), test loss: 2.94065816253\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.5639266968,30.5835010578), test loss: 23.8691233158\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.95018470287,5.2706871052), test loss: 3.00181679577\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (8.73401260376,30.4090698964), test loss: 24.8966618061\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.29260504246,5.2355102964), test loss: 3.12137874663\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (11.6382198334,30.2366540765), test loss: 24.5425967216\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (7.46632671356,5.2010691424), test loss: 3.26010373831\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (14.9285678864,30.0647503602), test loss: 22.401769495\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.53796386719,5.16717809863), test loss: 3.13641736656\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (12.9057703018,29.8940009544), test loss: 22.9199451923\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.06958150864,5.13407741787), test loss: 2.89090645909\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (9.04292869568,29.7239326293), test loss: 23.965190959\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.05406308174,5.10153399364), test loss: 2.82057149559\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (7.88478469849,29.5547814271), test loss: 24.2738087654\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.35476899147,5.06968580948), test loss: 3.03019261062\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (27.4780082703,29.3871625099), test loss: 26.4051356792\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.91469454765,5.03830721055), test loss: 3.20766220689\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (27.2842674255,29.2202729775), test loss: 23.6571748734\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.73988199234,5.00757011104), test loss: 3.1175243482\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (7.67709445953,29.0547861628), test loss: 21.6129072189\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.17485439777,4.97741027443), test loss: 3.0516312778\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (3.58305835724,28.8905761031), test loss: 23.6594017506\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.788241028786,4.94788084431), test loss: 2.8504400298\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (12.5573596954,28.7271608177), test loss: 23.495994997\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.38932669163,4.9188387589), test loss: 2.91392042935\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (2.76417565346,28.5650675896), test loss: 24.4237014771\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.622668743134,4.89030054236), test loss: 2.90653728545\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.23928833008,28.4033130143), test loss: 25.2829432011\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.17262458801,4.86228947976), test loss: 3.17058155537\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (25.0759277344,28.2434746437), test loss: 22.6004589796\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.85046720505,4.83467505313), test loss: 3.20433781296\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (6.20180416107,28.0837027537), test loss: 22.0622858524\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.26890039444,4.80751738154), test loss: 2.97077719867\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (9.08115673065,27.9254418774), test loss: 23.813444519\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.12960636616,4.78088143611), test loss: 2.85251217037\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (4.32506561279,27.7681442377), test loss: 23.779386425\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.58759617805,4.75465120131), test loss: 3.02154797167\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (10.3986530304,27.6112703899), test loss: 24.8335376263\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.00809574127,4.72886858845), test loss: 3.1049629271\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (5.9825258255,27.4558075375), test loss: 24.3869549751\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.60658550262,4.7034915646), test loss: 3.21955313385\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (10.6176490784,27.3011234264), test loss: 22.4149292231\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.1784594059,4.67844714884), test loss: 3.10500006378\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (15.0420913696,27.1470101812), test loss: 22.7488986015\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.25242686272,4.65388877558), test loss: 2.77140298337\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (12.3999586105,26.9939308747), test loss: 24.3529754639\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.02703213692,4.62973379246), test loss: 2.80786726773\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (23.7864074707,26.8412632135), test loss: 23.954558754\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.7842605114,4.60586245231), test loss: 2.92979102731\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (15.4833717346,26.6897031706), test loss: 26.2617501259\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (3.37023663521,4.58237103307), test loss: 3.23846406937\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (6.8004693985,26.5388345102), test loss: 24.3269895077\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (4.27420330048,4.5592310813), test loss: 3.11179565191\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (12.2528171539,26.389037547), test loss: 21.9611440659\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.990907609463,4.53642866642), test loss: 2.97225991338\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (4.26985836029,26.2405810728), test loss: 24.2499938965\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.369616568089,4.51400066916), test loss: 2.86000406593\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (11.8944206238,26.0926532942), test loss: 24.1842813969\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.974041163921,4.49193981443), test loss: 2.89253668487\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.02107191086,25.9458392229), test loss: 24.6582479\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.57794463634,4.47018822728), test loss: 2.91528482586\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (13.5660848618,25.7998657891), test loss: 25.7724398613\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.996606051922,4.44873125088), test loss: 3.18449013829\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (4.48463249207,25.6545824951), test loss: 22.8155311108\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.70115566254,4.42757464988), test loss: 3.14480121732\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (8.72641181946,25.5103858743), test loss: 22.4428356647\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.40928113461,4.40666631469), test loss: 2.94281767607\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (3.58982181549,25.3671041172), test loss: 24.7131072521\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.91491544247,4.38606056993), test loss: 2.83800180107\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (5.96308135986,25.2248810237), test loss: 24.730264616\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.85101032257,4.36574101533), test loss: 3.01046853215\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (8.92997646332,25.0834529684), test loss: 25.8304796219\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.41022849083,4.34571690718), test loss: 3.12468032837\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (4.20580387115,24.942778967), test loss: 24.8962457657\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.62554359436,4.325938042), test loss: 3.19498057365\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (5.23804855347,24.8032998859), test loss: 22.9188274384\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (4.66133928299,4.30643645992), test loss: 3.08267827034\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (7.97121238708,24.6645817774), test loss: 23.7270028114\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.10768795013,4.28712741304), test loss: 2.81762226522\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (7.09682750702,24.5266949557), test loss: 25.4334724426\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.05898499489,4.26813304832), test loss: 2.79021432698\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (7.80097770691,24.3897427008), test loss: 24.8097485542\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.24986279011,4.24933631792), test loss: 2.91859440207\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (4.45403003693,24.2536048767), test loss: 27.5027056694\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.25672793388,4.23083185818), test loss: 3.26521500796\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (15.8070602417,24.1185275701), test loss: 24.7660039425\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.10238027573,4.21249125382), test loss: 3.07737196982\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (10.421497345,23.9843726639), test loss: 23.1041808605\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.16118335724,4.19441318081), test loss: 2.96503331065\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (4.33085775375,23.8511573341), test loss: 25.5996394157\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.80506259203,4.17654635791), test loss: 2.84528821111\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (3.6173017025,23.7191077485), test loss: 25.3687942505\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.651326239109,4.15897702563), test loss: 2.90613987744\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (4.21600055695,23.5878531632), test loss: 26.1080943108\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.48700928688,4.14159706566), test loss: 3.00142012239\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (3.78469610214,23.4576544731), test loss: 26.3951097965\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.548807859421,4.12441212696), test loss: 3.14491397887\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (3.36409807205,23.3283549721), test loss: 23.987708807\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.75546836853,4.1074758082), test loss: 3.11815023124\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (8.5353603363,23.2001330746), test loss: 24.3135166645\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.31146216393,4.09065631652), test loss: 2.94845963717\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (4.02328681946,23.0728064602), test loss: 26.206111908\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.19406378269,4.07404859711), test loss: 2.84047562927\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (2.83183288574,22.946480572), test loss: 26.3432994843\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.869119524956,4.05766413574), test loss: 2.92976121157\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (2.17439699173,22.821189807), test loss: 27.7056516647\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.40456628799,4.04145727226), test loss: 3.1627699554\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (6.85253238678,22.6967810565), test loss: 26.383380127\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.71806406975,4.02545437142), test loss: 3.2395638749\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.48407411575,22.5732929826), test loss: 23.9647735596\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.53048849106,4.00960660071), test loss: 3.06100705862\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (6.35945892334,22.4509343106), test loss: 25.0933538914\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.29276776314,3.99391261331), test loss: 2.81332241744\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (7.26307964325,22.3293728719), test loss: 26.7553981304\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.21404099464,3.97844116939), test loss: 2.80852472931\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.19280433655,22.2088796279), test loss: 26.0497973919\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.83868122101,3.9631582192), test loss: 2.92371445\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (5.61679172516,22.0892419287), test loss: 28.0591914654\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.65672206879,3.94798601324), test loss: 3.20547725111\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (5.88548660278,21.9706353426), test loss: 25.6969007015\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.92748594284,3.932989364), test loss: 3.11514357924\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (3.08236551285,21.8529660967), test loss: 24.623038435\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.39932632446,3.91816052484), test loss: 2.98716878295\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (8.6382522583,21.7363396552), test loss: 26.7279752731\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.148458004,3.90347880357), test loss: 2.8820299387\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (1.40451693535,21.6207791029), test loss: 26.6200757027\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.519522368908,3.88897974407), test loss: 2.93123946786\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (4.54851770401,21.5061665989), test loss: 27.8957005024\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.651090443134,3.87467289088), test loss: 3.08812493682\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (2.91768336296,21.3924931136), test loss: 27.6072910786\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.65518498421,3.86050010765), test loss: 3.18668728173\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.57253170013,21.2798367038), test loss: 25.036655426\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.584558725357,3.84647898578), test loss: 3.12246717513\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (2.55815720558,21.1680678239), test loss: 25.6454036713\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.70887374878,3.83259830056), test loss: 2.90424394757\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (2.73290538788,21.0573497599), test loss: 27.4536316872\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.39873504639,3.8188291375), test loss: 2.86496949196\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (2.20337224007,20.9475690542), test loss: 26.259186697\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.69768512249,3.805201187), test loss: 2.92644194961\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.71751022339,20.8387441607), test loss: 29.0101737499\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.96682429314,3.79172414403), test loss: 3.19533794224\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (3.50360202789,20.7308881227), test loss: 27.9145216465\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.24070715904,3.77840625222), test loss: 3.30392987728\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (2.49273753166,20.6238433515), test loss: 25.0345698833\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.07224273682,3.76519329728), test loss: 3.0386067152\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 8\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (341.370910645,inf), test loss: 176.25936203\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (318.08001709,inf), test loss: 382.726759338\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (67.2294158936,132.294445702), test loss: 40.5480448246\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (18.5979976654,244.032008728), test loss: 23.4866963387\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (50.357460022,88.9804952345), test loss: 35.3503297329\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.63044953346,124.172797192), test loss: 2.92601699531\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (30.3638954163,74.2823606335), test loss: 35.3564612389\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.13043022156,83.7842381212), test loss: 3.15520368814\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (49.3070907593,66.9071667273), test loss: 35.03180089\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.16561675072,63.5908639635), test loss: 3.23013283908\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (55.1160125732,62.4813305504), test loss: 30.6357901096\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.37927865982,51.4731508193), test loss: 3.20055834055\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (49.6010246277,59.5032836798), test loss: 33.6272352695\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.00794410706,43.392689036), test loss: 3.02111593783\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (20.1909008026,57.3700230464), test loss: 35.1290597677\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.2991399765,37.6206194191), test loss: 3.05567786396\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (23.7438163757,55.7291523136), test loss: 35.9058864594\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.84575438499,33.2920249357), test loss: 3.33151878715\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (57.5860137939,54.4394886093), test loss: 34.1484510422\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.48869478703,29.9243051085), test loss: 3.26941396892\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (67.0100021362,53.3945752467), test loss: 30.3649051905\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.52166032791,27.2303833149), test loss: 3.2137161836\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (95.7900161743,52.5329726431), test loss: 34.2465338707\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.28149425983,25.0249164051), test loss: 2.96129062772\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (19.9370803833,51.7834513884), test loss: 35.0337469578\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.47979259491,23.1869683206), test loss: 3.16022769213\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (31.3237609863,51.1388322862), test loss: 35.1060986042\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.86753356457,21.6314537942), test loss: 3.24962069988\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (21.2446022034,50.5772610032), test loss: 30.9690010071\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.44519329071,20.2982978469), test loss: 3.07573724091\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (90.0467605591,50.0838353861), test loss: 30.6134931564\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.73471450806,19.1429291518), test loss: 2.99622963369\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (20.5264282227,49.6309972399), test loss: 33.6191406012\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.44779157639,18.1320617248), test loss: 2.84477034211\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (44.1756286621,49.2183689566), test loss: 34.0261973858\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.83748841286,17.2392684439), test loss: 3.09935855269\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (29.282951355,48.8348494936), test loss: 33.6859316349\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.59523940086,16.4452228063), test loss: 3.1439447403\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (62.2708091736,48.4901402165), test loss: 29.0980073929\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.87191867828,15.7348939113), test loss: 3.11666507423\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (37.2520370483,48.1536753406), test loss: 31.5415358543\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.29395413399,15.0945027346), test loss: 2.95094844699\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (25.6954917908,47.8329904261), test loss: 33.0873615742\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.696026206017,14.5149564029), test loss: 3.01252011657\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (48.4884567261,47.53112277), test loss: 34.1283549309\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (7.85748720169,13.9883483389), test loss: 3.32651755214\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (14.2825984955,47.2401097823), test loss: 32.2419717789\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.07207536697,13.5067448683), test loss: 3.25354061127\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (15.9049024582,46.958597231), test loss: 28.3809327126\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.94662117958,13.0649027628), test loss: 3.1795035556\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (47.5296783447,46.6884223311), test loss: 31.6893549919\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.39529418945,12.6581121685), test loss: 2.95611336231\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (86.7153625488,46.4289253997), test loss: 32.890990448\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (4.96502304077,12.2823976547), test loss: 3.16628440022\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (56.3234634399,46.1704418955), test loss: 33.2289039612\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.9712677002,11.934082547), test loss: 3.29082372189\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (37.1020965576,45.9144920971), test loss: 28.714218998\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.81368994713,11.6102313691), test loss: 3.11327534616\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (26.1983718872,45.6580703196), test loss: 28.1679489851\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.29018688202,11.3084443486), test loss: 3.0383812964\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (50.5956726074,45.4051251025), test loss: 30.7618994236\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.38859295845,11.0268097523), test loss: 2.87723919749\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (60.477142334,45.1521913675), test loss: 31.5127906799\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (4.14898872375,10.7628376185), test loss: 3.14435971975\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (95.1276855469,44.8988853158), test loss: 31.0294059753\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.897960186,10.5149369719), test loss: 3.21859351993\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (18.4715805054,44.6403553987), test loss: 26.3786469698\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.7872209549,10.2816791598), test loss: 3.17319849432\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (5.81995487213,44.3791622969), test loss: 27.989606142\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.1264295578,10.0620575662), test loss: 2.97007352114\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (54.0647163391,44.1189899747), test loss: 29.7932188511\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.47767901421,9.85442608575), test loss: 3.01658719778\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (48.1522712708,43.8568408561), test loss: 31.3220485687\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (5.34815597534,9.65817847845), test loss: 3.33161104321\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (32.8861579895,43.5939410583), test loss: 29.1302898884\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.624139785767,9.47196156777), test loss: 3.25195058286\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.6195907593,43.3261362703), test loss: 25.258219409\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.75680088997,9.29535614726), test loss: 3.16425696462\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (21.6331329346,43.0569656807), test loss: 27.4095345497\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.76645636559,9.1275264573), test loss: 2.95810259581\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (9.55735015869,42.7892480269), test loss: 29.8348757744\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (5.26710987091,8.9679270521), test loss: 3.17394845188\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (52.4865150452,42.5229461595), test loss: 30.6574394226\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.75958919525,8.81586973184), test loss: 3.30634557605\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (38.8217506409,42.253698279), test loss: 26.382002759\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (5.60136508942,8.67100511875), test loss: 3.20726339221\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (38.5298347473,41.9842186444), test loss: 25.4452464581\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.87260556221,8.53218515965), test loss: 3.06473351717\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (25.0222644806,41.7147998082), test loss: 27.473154974\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.09082508087,8.39953132847), test loss: 2.9132456094\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (28.4130382538,41.4493864822), test loss: 29.4136008024\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.21664762497,8.27243495748), test loss: 3.18524723649\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (17.483921051,41.181547368), test loss: 28.9891983032\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.3527109623,8.15051029428), test loss: 3.2878464818\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (14.6211051941,40.9152304789), test loss: 24.9208162308\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.572101533413,8.03359480028), test loss: 3.25106959343\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (22.0203437805,40.6520367948), test loss: 25.4536957741\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (9.46056365967,7.92172165767), test loss: 3.05172999501\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (7.66364622116,40.3902280178), test loss: 27.7225367308\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.25587368011,7.81382237937), test loss: 3.06458409429\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (22.9494514465,40.1318908941), test loss: 29.7474914551\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.20777225494,7.71003339029), test loss: 3.36084107757\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (46.0179214478,39.8770662478), test loss: 27.1135062695\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.41671848297,7.61019673473), test loss: 3.29492558837\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (32.4969367981,39.6250227167), test loss: 24.3324273348\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.19996404648,7.51393457227), test loss: 3.23088760376\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (24.8569488525,39.3744882812), test loss: 25.5433268785\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.31951236725,7.42112593656), test loss: 3.02359505892\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (12.8181114197,39.125874071), test loss: 28.2843058825\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.47524356842,7.33148690389), test loss: 3.17350186706\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (15.3851642609,38.8802439362), test loss: 29.1972324848\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (5.16255664825,7.24499852718), test loss: 3.37457290888\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (47.5112915039,38.6386928315), test loss: 24.9848897457\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.13747644424,7.16139545639), test loss: 3.267073071\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (24.4081459045,38.398665944), test loss: 24.6410793304\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.67385196686,7.08057681376), test loss: 3.10951033533\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (81.2650985718,38.1633021835), test loss: 26.3091547489\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.57622981071,7.00229950115), test loss: 2.95575729311\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (23.6860198975,37.9282706347), test loss: 27.9112527847\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (4.85150003433,6.92659158364), test loss: 3.19675858617\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (3.51940584183,37.6955412028), test loss: 27.3076568127\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.18317890167,6.8531843572), test loss: 3.31569803655\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (27.0033397675,37.4661501021), test loss: 23.803880167\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.20432400703,6.78198325817), test loss: 3.21226134002\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (23.3563728333,37.2388304877), test loss: 24.4234055042\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.94356417656,6.71297103924), test loss: 3.017542696\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (21.703453064,37.0143024559), test loss: 26.2610188484\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.853358566761,6.64590883477), test loss: 3.05714155734\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (5.286860466,36.7906230168), test loss: 28.2251327038\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.957257390022,6.58082145494), test loss: 3.32120449841\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (49.2923698425,36.570887408), test loss: 25.6684951305\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.33054065704,6.51763954126), test loss: 3.27337863445\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.1656360626,36.3527978282), test loss: 23.3170483351\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.73153948784,6.45638157893), test loss: 3.14872668982\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (25.1800575256,36.1380235179), test loss: 24.3150744438\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.86904478073,6.39674339796), test loss: 2.99145187438\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (41.6006164551,35.9251838335), test loss: 26.3584847689\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (5.2850522995,6.33895140466), test loss: 3.16128798723\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (50.0736541748,35.7143500189), test loss: 27.722516346\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.60804224014,6.28247225037), test loss: 3.3303253293\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (41.5613632202,35.5052482961), test loss: 23.6392041206\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (4.37722396851,6.2275131499), test loss: 3.26071134508\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.2897529602,35.2979184919), test loss: 23.548385191\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.05646967888,6.17393951814), test loss: 3.05545889437\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (15.1819295883,35.0917706464), test loss: 25.1316399574\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.087849617,6.12175784661), test loss: 2.94569014013\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.9559812546,34.8874018838), test loss: 26.4315958023\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.51097548008,6.0708157709), test loss: 3.19481004477\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (18.6036167145,34.6848060523), test loss: 26.5200556993\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (4.60304641724,6.02142902371), test loss: 3.382398054\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.6092348099,34.484103909), test loss: 22.9463137388\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.37032341957,5.97306667892), test loss: 3.20286703408\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (24.5864067078,34.285160131), test loss: 23.5819366455\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.44163155556,5.92582535218), test loss: 2.99633249342\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (46.1227531433,34.0885892003), test loss: 25.1040632725\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.93290090561,5.87979016545), test loss: 3.05317466855\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (14.7198066711,33.8929279017), test loss: 27.1241044998\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.55119156837,5.83476939916), test loss: 3.34738301635\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.02137184143,33.6983158789), test loss: 24.6261835575\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.24679017067,5.7908329647), test loss: 3.30305187702\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (7.9437623024,33.5053057084), test loss: 22.5513382196\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.20587348938,5.74784055204), test loss: 3.124164325\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (5.83804130554,33.3136078475), test loss: 23.9741067886\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (4.37121963501,5.70586166767), test loss: 2.9972903192\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (36.7899932861,33.1244085778), test loss: 25.1362469673\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (3.15628266335,5.66477560401), test loss: 3.1310755074\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (12.0613889694,32.9355252166), test loss: 26.820593977\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.77641439438,5.62463684293), test loss: 3.34435864687\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (27.7779998779,32.7487323575), test loss: 22.8590132236\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.978646159172,5.58528052878), test loss: 3.25959581435\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (23.2593803406,32.5627040135), test loss: 22.843959403\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.88664865494,5.54682458351), test loss: 2.99497099519\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (11.4098930359,32.3775187063), test loss: 24.8245494366\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.23111057281,5.5091534442), test loss: 2.91256864816\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (18.7721595764,32.1936831232), test loss: 25.7156747818\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.31621742249,5.47219180306), test loss: 3.16578111053\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (8.43975257874,32.0108814917), test loss: 25.5289182901\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.882807731628,5.43602153231), test loss: 3.36211701035\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (5.47241401672,31.8292747053), test loss: 22.592822504\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.656310915947,5.40052366333), test loss: 3.21228121221\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (6.37177371979,31.6481784896), test loss: 23.3640461683\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.51249456406,5.36576279081), test loss: 2.95530561507\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (39.0004348755,31.4691735652), test loss: 24.5675084591\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.42147326469,5.33171135116), test loss: 3.04433657527\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (9.90501213074,31.2903188056), test loss: 26.4978975773\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.91217827797,5.2983537364), test loss: 3.30151047707\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (18.0854187012,31.1132260788), test loss: 24.6160289049\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.45177960396,5.26565836411), test loss: 3.29217965305\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (18.7713699341,30.9365619033), test loss: 22.8477724552\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.93881607056,5.23360484841), test loss: 3.13621532917\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (30.9018287659,30.7611932271), test loss: 23.5139906168\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.25513100624,5.20204246274), test loss: 2.96918257177\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (24.3225326538,30.5862999196), test loss: 24.5186115742\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (3.62381887436,5.17107978679), test loss: 3.08923374414\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (15.4752473831,30.41233181), test loss: 26.5611196995\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.74099946022,5.14062119138), test loss: 3.31228983402\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (11.2528543472,30.2392650767), test loss: 23.1823583603\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.80958676338,5.11074279423), test loss: 3.23761492372\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (12.3662815094,30.0666828043), test loss: 23.10762887\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.46640825272,5.08134404675), test loss: 3.03546484709\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (19.4528522491,29.895338878), test loss: 24.2985159397\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (3.66678404808,5.05260284126), test loss: 2.92474440038\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (8.98385238647,29.7244509124), test loss: 25.6944808006\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.57692956924,5.02424504951), test loss: 3.1769916296\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (9.42360973358,29.5546972934), test loss: 25.8923322678\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.95068645477,4.99634634365), test loss: 3.38987641186\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (30.2744750977,29.3861239927), test loss: 22.7387833834\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.92813026905,4.96894896638), test loss: 3.21776353717\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.1840085983,29.2177921965), test loss: 23.5447619915\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.39670681953,4.94198528949), test loss: 2.97761161923\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (7.12984371185,29.0503500949), test loss: 25.1410462379\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.16647815704,4.91545851749), test loss: 3.02569522858\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (6.2986946106,28.8836463896), test loss: 26.7602043629\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.40796518326,4.88933888067), test loss: 3.30235062838\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (6.92950725555,28.7179402755), test loss: 24.3303264141\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.55995750427,4.86367119388), test loss: 3.22686139196\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.94514656067,28.553404075), test loss: 22.6372731686\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.941594839096,4.83837176058), test loss: 3.0854983151\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (6.69939899445,28.3893035409), test loss: 23.9172559023\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.99033176899,4.81350613118), test loss: 2.95989229977\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (15.7431297302,28.2264371298), test loss: 24.6132392883\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.856659770012,4.78895578289), test loss: 3.06383859515\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (13.7889223099,28.0641184306), test loss: 26.9441212654\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.14305329323,4.76482986058), test loss: 3.25840824991\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (8.59741592407,27.9024166786), test loss: 23.5752608299\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.72098922729,4.74103490648), test loss: 3.16082421839\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (7.3056344986,27.7418143889), test loss: 23.0800429821\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.561337649822,4.71755791345), test loss: 2.93931248039\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (9.80840682983,27.582083924), test loss: 24.6635629416\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.681797802448,4.69443879552), test loss: 2.91655226797\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (3.02745389938,27.4231783089), test loss: 26.6223330021\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.05288660526,4.67163181128), test loss: 3.21702515781\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.6210308075,27.2651836826), test loss: 26.7060894966\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.69439578056,4.649174675), test loss: 3.38582675457\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (17.6734161377,27.1083945724), test loss: 24.1175352573\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.61588037014,4.62703594297), test loss: 3.23607179523\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (10.4004945755,26.9523669495), test loss: 24.6068572998\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.14212059975,4.60525534837), test loss: 2.95645161271\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (10.7177915573,26.7974057178), test loss: 25.6556584835\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (5.90023994446,4.58381157999), test loss: 3.0599367857\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.15752315521,26.6433356128), test loss: 28.2829306126\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.19581007957,4.56259997848), test loss: 3.36683080494\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (11.0181932449,26.4902630788), test loss: 25.3786732197\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.62557625771,4.54164426215), test loss: 3.24879116118\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (15.8164997101,26.3381204013), test loss: 24.0374603748\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.03736114502,4.52100867571), test loss: 3.09172950387\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (5.82116985321,26.1869246793), test loss: 24.9151638269\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.22462618351,4.50058758189), test loss: 2.96122742742\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (6.8058013916,26.036666154), test loss: 25.6997854233\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.53971385956,4.48044349127), test loss: 3.06125285625\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (7.46741771698,25.8874472194), test loss: 28.9118779659\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.51471066475,4.46054870466), test loss: 3.36153216362\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (7.07592773438,25.7392249734), test loss: 24.9610174179\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.28433990479,4.44101159566), test loss: 3.21984550059\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (4.03360557556,25.5920646601), test loss: 24.2549023151\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.07342147827,4.42163075277), test loss: 2.95742687732\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.29857730865,25.4460788772), test loss: 25.5572220802\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.00044560432,4.40251046718), test loss: 2.94034126103\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (9.79681777954,25.301140641), test loss: 28.2470149517\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.27267217636,4.38362148928), test loss: 3.25159936547\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (4.93039989471,25.1571806749), test loss: 28.3300503254\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.89493513107,4.36497255401), test loss: 3.45903241485\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (2.62314176559,25.0142025188), test loss: 25.2516629696\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.01523566246,4.34653217219), test loss: 3.23117106557\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (5.36106872559,24.8724277031), test loss: 26.0611854792\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.2667593956,4.32831842701), test loss: 2.99291852713\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (5.36823654175,24.7318131994), test loss: 28.614604187\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.29964649677,4.31035080577), test loss: 3.02905625105\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (2.65734100342,24.5922603645), test loss: 29.7872964859\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.88598656654,4.29257692135), test loss: 3.35133183897\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (3.18677139282,24.4537521722), test loss: 26.6043414593\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.99834179878,4.27502382004), test loss: 3.25778417885\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.68511962891,24.3164838224), test loss: 25.3720598221\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.27843904495,4.25763311959), test loss: 3.10670902729\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (5.01524543762,24.1802257161), test loss: 27.0291760445\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.38223898411,4.24047750035), test loss: 2.93654159009\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (5.23573827744,24.044997486), test loss: 27.0021306992\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.53760480881,4.22350272548), test loss: 3.02911008596\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (4.51385116577,23.9108847369), test loss: 29.6652363777\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.736837387085,4.20669601836), test loss: 3.30552949905\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.48555088043,23.778005861), test loss: 26.5061338902\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.20771467686,4.19009299945), test loss: 3.27669529617\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (3.12479281425,23.6461221593), test loss: 25.645754385\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.03056693077,4.17366407153), test loss: 2.97761352062\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (6.4284157753,23.5154105231), test loss: 27.2389866829\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.45707297325,4.15742580365), test loss: 2.92354961932\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.11093664169,23.3859138085), test loss: 29.9881131649\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.07803463936,4.14137534485), test loss: 3.27237040699\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (13.7891578674,23.257559304), test loss: 30.2047269344\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.05496549606,4.12554803937), test loss: 3.47232101262\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.79522514343,23.1302741024), test loss: 27.0038310051\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (5.51499509811,4.1099047363), test loss: 3.29285044074\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.41257667542,23.0040595916), test loss: 27.5251365185\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.49920618534,4.09439287818), test loss: 3.00426886082\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (3.65933132172,22.8789984539), test loss: 28.3631485462\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.07769298553,4.07901550373), test loss: 3.04698267579\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (7.99375295639,22.7550976474), test loss: 31.3069221497\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.76213371754,4.0638528684), test loss: 3.35725217462\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.44285559654,22.6321833249), test loss: 28.0256903887\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.13515520096,4.04879270248), test loss: 3.25116342157\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (1.99851632118,22.5104124482), test loss: 27.0823363304\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.6682767272,4.03389963265), test loss: 3.11661278009\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (4.91446733475,22.3897142316), test loss: 27.7907263756\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.4994187355,4.01917025009), test loss: 3.00587098002\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (8.32674026489,22.2701442441), test loss: 28.0498853207\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.66918778419,4.00464912123), test loss: 3.04057144523\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.92324638367,22.1516610546), test loss: 31.0522618294\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.19009852409,3.9902274893), test loss: 3.31383449882\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (3.75380587578,22.0342150948), test loss: 27.63746171\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.3173866272,3.97595208108), test loss: 3.28059363365\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.96302223206,21.9178922606), test loss: 26.7903401852\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.04657673836,3.96181232591), test loss: 3.00551833808\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (2.87411737442,21.8025193033), test loss: 28.239278698\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.47163319588,3.94782636817), test loss: 2.94988488108\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.03962850571,21.6881865175), test loss: 31.8817052841\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.17098259926,3.93397176229), test loss: 3.27223448753\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (2.80917596817,21.5748916214), test loss: 31.1100469112\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.18315720558,3.92025077514), test loss: 3.47109994441\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (2.84643292427,21.4626299358), test loss: 27.8803060055\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.590042829514,3.90668471405), test loss: 3.23990044892\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 10\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (350.093261719,inf), test loss: 187.798921204\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (315.093444824,inf), test loss: 382.584249878\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (66.3545379639,130.59191556), test loss: 44.9939621925\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (21.596326828,243.528093515), test loss: 22.8300512314\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (28.1829185486,86.3779762487), test loss: 40.7798422337\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.15274000168,123.880368348), test loss: 2.87543768883\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (24.6868000031,71.3866431147), test loss: 39.9697306633\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.88243770599,83.5789227487), test loss: 3.10063382685\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (45.5219192505,63.8886674392), test loss: 38.7410817146\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (5.07575416565,63.4249352516), test loss: 3.13479370177\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (84.2246780396,59.3793736677), test loss: 35.9306856155\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.57017993927,51.3305160229), test loss: 3.12453810871\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (25.8700466156,56.3418654254), test loss: 38.558244133\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.58743786812,43.2663899623), test loss: 2.96410118043\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (11.7263393402,54.130853379), test loss: 41.4333165169\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.55151557922,37.5066972241), test loss: 2.93386609256\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (21.2283611298,52.4625920317), test loss: 38.9438358307\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.882551431656,33.1848376259), test loss: 3.23105894625\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (75.6663589478,51.1648666714), test loss: 36.528331089\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.6163957119,29.8243602281), test loss: 3.09667823315\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (20.9558391571,50.0877412622), test loss: 35.676561594\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.45076560974,27.1347281981), test loss: 3.19040657282\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (45.4182548523,49.2001626686), test loss: 39.2808263779\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.79387354851,24.9340802731), test loss: 2.91834406555\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (17.2352218628,48.4413585283), test loss: 39.1207320213\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.77350568771,23.1000077447), test loss: 3.05337952971\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (48.9744873047,47.7960661441), test loss: 37.3437085152\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (8.64030170441,21.5494206772), test loss: 3.19447476566\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (35.1410102844,47.2142307629), test loss: 35.3300069332\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.68136835098,20.2187347893), test loss: 3.13942892253\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (26.9109039307,46.6945362761), test loss: 36.412447834\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (5.73561859131,19.0653110744), test loss: 2.93948928416\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (21.1256103516,46.2303339958), test loss: 39.7541242599\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (0.957952857018,18.0550330452), test loss: 2.86425799131\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (17.9739627838,45.8027809365), test loss: 39.9806612968\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.49200415611,17.1636549811), test loss: 3.20136537254\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (28.7070236206,45.4095633381), test loss: 37.6073532104\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (5.2583489418,16.3710188436), test loss: 3.23718438745\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (21.8662242889,45.0444601333), test loss: 34.005581522\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.13439238071,15.6618869897), test loss: 3.2394499898\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (15.664182663,44.6995908398), test loss: 37.4272331238\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.96786260605,15.0227599912), test loss: 2.86893617213\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (65.2844238281,44.3771785668), test loss: 39.321213913\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.01423692703,14.4440329682), test loss: 3.09258995652\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (59.3971481323,44.0708941123), test loss: 37.7100264072\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.48459815979,13.9176465591), test loss: 3.28607904315\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (29.6871623993,43.7740007521), test loss: 34.1718427181\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.72603774071,13.4363244914), test loss: 3.05315832347\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (18.9293174744,43.4842651719), test loss: 33.3632007599\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (6.85361242294,12.9953174412), test loss: 2.96193535924\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (36.8915710449,43.2060836298), test loss: 37.7171519756\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.15616416931,12.5887994051), test loss: 2.81669833362\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (48.580581665,42.9349955547), test loss: 38.1144316673\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.73342323303,12.2134571773), test loss: 3.19608394504\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (14.5741252899,42.671566788), test loss: 35.792220211\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.70591211319,11.8654445308), test loss: 3.11179629266\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (7.17676639557,42.404356877), test loss: 32.6092120171\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.40987110138,11.5422054227), test loss: 3.07270082533\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (35.2050819397,42.1426336315), test loss: 34.8548061371\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.33525133133,11.2405448494), test loss: 2.87280345261\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (26.2081336975,41.8825507704), test loss: 36.2455840111\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.733359158039,10.9588613546), test loss: 2.98539129794\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (16.9467277527,41.6251137977), test loss: 35.5534676075\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.30089092255,10.6948265015), test loss: 3.21922895312\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (36.0467300415,41.3661997899), test loss: 32.7443459034\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.97484612465,10.4470692505), test loss: 3.03858211488\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (10.1378593445,41.1078819202), test loss: 30.6195206165\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (5.2787694931,10.214074894), test loss: 3.11020671129\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (40.1877288818,40.851034635), test loss: 33.2920769691\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.74674940109,9.99472945026), test loss: 2.84432076663\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (19.3159694672,40.591353951), test loss: 34.6078735352\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.62383842468,9.78769691678), test loss: 3.03674563468\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (24.501159668,40.3308622442), test loss: 34.0794580936\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.77356660366,9.59144556898), test loss: 3.1199865967\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (15.1595077515,40.07069794), test loss: 31.0092282295\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.966569602489,9.40561616772), test loss: 3.10192751884\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.5155563354,39.8093523217), test loss: 30.998637104\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.72692108154,9.22918446823), test loss: 2.97031507045\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (19.6843223572,39.5478098537), test loss: 33.5795626163\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.26316809654,9.0614235283), test loss: 2.90573049486\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (22.4610023499,39.2882478103), test loss: 34.2291128635\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.69184923172,8.90214464854), test loss: 3.26492679715\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (14.3341579437,39.0285157138), test loss: 32.584465909\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.65829992294,8.7500371529), test loss: 3.17102111876\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (36.2718391418,38.7707378085), test loss: 29.4706058979\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.18142414093,8.60487581155), test loss: 3.26780196726\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (26.1308345795,38.5159226857), test loss: 31.1530694485\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.29406237602,8.46622716469), test loss: 2.96364968717\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (10.0395908356,38.2616342659), test loss: 33.4756922245\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.44340920448,8.33346536611), test loss: 3.11738035679\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (18.893611908,38.0090436838), test loss: 33.6694487095\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (5.52470970154,8.20649833974), test loss: 3.34065452218\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (36.4103240967,37.7597545089), test loss: 30.7448060513\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.68164408207,8.084648281), test loss: 3.25860428214\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (20.3976364136,37.5120340805), test loss: 30.0653111458\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.35073280334,7.9678723592), test loss: 3.06843519211\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (10.6526994705,37.2700029748), test loss: 31.6498489141\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.78077232838,7.85569656773), test loss: 2.95397883952\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (24.322177887,37.0280026296), test loss: 33.4411497116\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.20020914078,7.74793587656), test loss: 3.25806045383\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (51.7461395264,36.789159198), test loss: 33.1568344116\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.71948385239,7.64420500584), test loss: 3.26634916961\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (18.2575359344,36.553015436), test loss: 28.8048859358\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.960748791695,7.54434128733), test loss: 3.30596880913\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (16.8903923035,36.3203409404), test loss: 29.8851620674\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.92645049095,7.44805529699), test loss: 2.9856018573\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (34.0873184204,36.0893350667), test loss: 32.6486911774\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.8273627758,7.35531163276), test loss: 3.17993157804\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (14.6785783768,35.8621163942), test loss: 33.4740071774\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (5.1994395256,7.26592462148), test loss: 3.43192595243\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (26.7449455261,35.6369028249), test loss: 30.377891326\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.39459371567,7.17956269591), test loss: 3.11145068109\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (13.782749176,35.4149199176), test loss: 28.0786539793\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.93138980865,7.09638214009), test loss: 3.13480376899\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (12.1458044052,35.1953299481), test loss: 30.4516050339\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.24193370342,7.01563059813), test loss: 2.95683590621\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (18.054233551,34.9777814075), test loss: 32.2433903694\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.19296216965,6.93755655571), test loss: 3.23729186952\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (17.4016513824,34.7622505348), test loss: 31.8841805696\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.38854217529,6.86184672394), test loss: 3.21077597141\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (15.551492691,34.5490472144), test loss: 27.910620594\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.579035162926,6.78852244579), test loss: 3.23479519188\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (16.2585601807,34.3388347408), test loss: 28.4804998398\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.05067658424,6.71777016448), test loss: 2.99084182829\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (12.4973526001,34.1307069705), test loss: 30.4596849442\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.61525666714,6.64889060609), test loss: 3.05041750073\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (28.3279247284,33.9255440374), test loss: 31.1639293194\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.38836574554,6.58209370502), test loss: 3.32806664407\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (14.3591346741,33.7224158795), test loss: 29.7607836723\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.20422410965,6.51725344703), test loss: 3.19226762801\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (13.557844162,33.521318248), test loss: 26.8345603466\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.827360987663,6.45423230329), test loss: 3.2726102978\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (14.6234741211,33.3220571368), test loss: 28.7664649725\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.92992353439,6.39304248039), test loss: 3.03052989542\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (27.4558181763,33.1257308993), test loss: 30.1567411423\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.34729266167,6.33349869321), test loss: 3.12542742789\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (9.13781356812,32.9306476558), test loss: 31.8688113689\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.45829224586,6.27561790408), test loss: 3.33092176914\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (25.9263019562,32.7394281224), test loss: 29.1454215288\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.89774727821,6.21925560598), test loss: 3.32129248083\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (21.9922676086,32.5484290581), test loss: 27.9368155479\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.22453188896,6.1644061398), test loss: 3.02680622935\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (22.9771881104,32.3587572159), test loss: 29.3588637829\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.979129850864,6.11090260353), test loss: 2.96318800896\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (20.0724029541,32.1713575413), test loss: 30.3284790993\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.63429188728,6.05882391501), test loss: 3.31392920613\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.1577758789,31.9853406154), test loss: 30.3775048733\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.848530054092,6.00793432373), test loss: 3.30843423009\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (3.91851758957,31.799821613), test loss: 26.4989649296\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.05645704269,5.95839532757), test loss: 3.33928004503\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (54.5063743591,31.6178929974), test loss: 27.1306176186\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.22904622555,5.91003738591), test loss: 3.01607038528\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (22.3566093445,31.4359139882), test loss: 29.5590196133\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.03156852722,5.8629824528), test loss: 3.22196642011\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (37.8654098511,31.2560638872), test loss: 31.7322392464\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (5.13941860199,5.81714632833), test loss: 3.46595484018\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (27.7646903992,31.0774727646), test loss: 28.6058282375\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.2849111557,5.77214772326), test loss: 3.28862247467\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (17.5900173187,30.8997430969), test loss: 27.3025351524\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.67896604538,5.72824551844), test loss: 3.07608833015\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (17.103181839,30.7234339713), test loss: 28.7828473091\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.53109145164,5.68524432197), test loss: 2.9358268559\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (12.0367431641,30.5481401287), test loss: 29.3727686644\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.735341310501,5.64321897994), test loss: 3.28574974835\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (15.0264310837,30.3745622256), test loss: 30.8555817604\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (3.01959443092,5.60230992605), test loss: 3.29927294701\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (11.3835811615,30.2018636384), test loss: 26.4513031006\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.82672286034,5.56214951902), test loss: 3.2380509764\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.2621173859,30.0309264693), test loss: 27.5638321161\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.05408501625,5.52282601982), test loss: 2.99919105917\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.7178592682,29.861022136), test loss: 28.3514066935\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.54986298084,5.48437939204), test loss: 3.14126629531\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (8.91118240356,29.692090546), test loss: 29.6635932446\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.948948264122,5.44673281982), test loss: 3.38238081485\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (14.4054613113,29.5243374792), test loss: 28.3115972996\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.04906058311,5.40984357489), test loss: 3.16254321337\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (19.4138412476,29.3579243943), test loss: 26.1282160044\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.26186323166,5.3737252086), test loss: 3.11529342383\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (14.5069227219,29.1923784629), test loss: 28.2492407322\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.82257032394,5.33834501336), test loss: 2.99425063133\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (25.6034317017,29.0286108207), test loss: 28.8563907623\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.24050331116,5.30360707836), test loss: 3.23657085001\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (24.5086021423,28.8651503289), test loss: 30.9222472191\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.07358360291,5.26957767797), test loss: 3.23456855416\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (15.4000034332,28.7023346533), test loss: 26.6217219353\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.07922315598,5.2361990131), test loss: 3.21416096687\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (12.8439083099,28.5408198921), test loss: 27.4721616745\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.29311656952,5.20346046689), test loss: 2.92386146039\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.42166996,28.3802198567), test loss: 28.6476594925\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.579722940922,5.17126772011), test loss: 3.01402048171\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (7.30476379395,28.2199161458), test loss: 29.8573268414\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.813164711,5.13973060566), test loss: 3.33057902753\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (32.5801010132,28.0615226643), test loss: 29.6075989723\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.7655184269,5.10876557864), test loss: 3.22674168348\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (16.7784309387,27.9032888474), test loss: 25.6763740778\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.56237697601,5.07843624871), test loss: 3.29597504735\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (24.5756320953,27.7463415557), test loss: 27.8013086796\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (4.81310462952,5.0487336294), test loss: 3.04946342111\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (34.538520813,27.590093912), test loss: 28.1463217974\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.73346018791,5.01939151549), test loss: 3.09496112168\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (6.7472486496,27.4342752404), test loss: 31.9325697422\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.89486789703,4.99061074168), test loss: 3.37728336453\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (5.13181734085,27.2793364417), test loss: 28.976506567\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.75175213814,4.96224150264), test loss: 3.28129274249\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (6.2738070488,27.125292611), test loss: 27.3944701672\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.545194268227,4.93437334037), test loss: 3.04215483963\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (11.4776878357,26.9721828271), test loss: 28.756948328\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.92471647263,4.90710530216), test loss: 2.9575347662\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (9.27208423615,26.8197832747), test loss: 30.7223721504\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.5177462101,4.88017898698), test loss: 3.3124215275\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (7.15948152542,26.6683475066), test loss: 31.55052948\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.25067412853,4.85367447243), test loss: 3.44365310073\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (6.70303916931,26.5176842607), test loss: 26.7053938866\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.86259794235,4.82764063131), test loss: 3.37526107728\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (9.23499679565,26.3677963084), test loss: 28.3242340088\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.19799828529,4.80203254839), test loss: 3.00153616667\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (10.5195112228,26.218632579), test loss: 29.2530158997\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.50364887714,4.77679466952), test loss: 3.17392819822\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (15.2681150436,26.0705691873), test loss: 32.5561839104\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.17831897736,4.75198043094), test loss: 3.43308931589\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (8.43127346039,25.9232190477), test loss: 29.8069397449\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.32664489746,4.72753127291), test loss: 3.32949978709\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (20.537481308,25.7768149065), test loss: 27.6489077568\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.47155761719,4.70344859601), test loss: 3.10762944221\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (16.9057044983,25.6313204359), test loss: 30.0945449352\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (3.57929134369,4.67973965926), test loss: 2.91739841998\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (10.5819377899,25.4863650235), test loss: 30.8769127846\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.8808413744,4.65637960552), test loss: 3.27969016433\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.02401161194,25.3423942904), test loss: 32.7081892967\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.25518417358,4.63333493632), test loss: 3.29823831022\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (7.59462928772,25.1993520683), test loss: 27.7347619534\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.18859124184,4.61063463113), test loss: 3.25359647572\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (4.61089229584,25.0568457006), test loss: 29.5882845402\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.577722370625,4.58824540328), test loss: 2.98059417307\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (11.9466896057,24.9155496211), test loss: 28.5835121632\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.53057670593,4.56623754586), test loss: 3.06078766584\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (8.14873600006,24.7750234764), test loss: 30.9267363071\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.46992695332,4.54454874433), test loss: 3.41151258349\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (8.35833644867,24.635426419), test loss: 30.9066685677\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.19439983368,4.52322661185), test loss: 3.22883456051\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (12.7191104889,24.4967719503), test loss: 27.47356987\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.48670256138,4.50211987705), test loss: 3.14497586191\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (5.03109741211,24.358942803), test loss: 30.2237543583\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.935120821,4.48130684127), test loss: 2.96015121341\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.66758060455,24.2218949994), test loss: 29.4634023666\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.22847127914,4.46075126642), test loss: 3.10770970136\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (2.22375917435,24.0857372529), test loss: 33.6653113365\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.61093211174,4.44045886357), test loss: 3.35063157678\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (5.2189078331,23.950526826), test loss: 29.1781614304\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (6.51798248291,4.42052299536), test loss: 3.2702652514\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (7.57291889191,23.8162345212), test loss: 29.7328466415\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.50508499146,4.40078383432), test loss: 2.96326506883\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (12.9183492661,23.6828914461), test loss: 30.7136142254\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.00466275215,4.38127478428), test loss: 2.97538653314\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (4.82034873962,23.5504295968), test loss: 31.5848799229\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.73269629478,4.36205600879), test loss: 3.41955683231\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (5.42997455597,23.4188454363), test loss: 32.2106605053\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.47515010834,4.34308829561), test loss: 3.27458590865\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (5.90698194504,23.2881267225), test loss: 27.9222601414\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.73837041855,4.3243338002), test loss: 3.30973053575\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (4.90413808823,23.1584907937), test loss: 31.2054430485\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.11321723461,4.30584278798), test loss: 3.0199846074\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (2.44446229935,23.0297069377), test loss: 29.8880172253\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.21732878685,4.28755563446), test loss: 3.12483222485\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (14.8879337311,22.9020016751), test loss: 34.6237743378\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.73473763466,4.26949531817), test loss: 3.47013978064\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (9.41511821747,22.7753561376), test loss: 31.8395971298\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.04398870468,4.2516457397), test loss: 3.34211757481\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (3.80782365799,22.6494913045), test loss: 30.4891624928\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.10888338089,4.23404979073), test loss: 3.02177952081\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (5.29124307632,22.5246616613), test loss: 31.3720048428\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.693698167801,4.21660956165), test loss: 2.93729584813\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (13.5149745941,22.4008610705), test loss: 32.5230428219\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.50664353371,4.19940403768), test loss: 3.34339792728\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (2.569024086,22.2778325672), test loss: 35.0567300797\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.38773941994,4.18238020359), test loss: 3.45700350255\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (9.46543884277,22.1558919614), test loss: 29.0565377235\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.16886162758,4.16559992469), test loss: 3.35792647004\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (3.68002009392,22.0349023568), test loss: 30.5732074738\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.82215690613,4.14902814708), test loss: 3.02594518363\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.48050308228,21.9149403247), test loss: 30.5737100601\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (5.35367012024,4.13267333569), test loss: 3.11904583275\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (3.45528364182,21.7958809655), test loss: 35.176362896\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.06874132156,4.11645594836), test loss: 3.50679177344\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (4.20012760162,21.6779166376), test loss: 32.4525700569\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (4.1893787384,4.10044131031), test loss: 3.25025770068\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.1734046936,21.5607710992), test loss: 29.4844750404\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.549583137035,4.08457043275), test loss: 3.05961567461\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (2.47617149353,21.444641123), test loss: 31.6227952003\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.21668887138,4.0688805411), test loss: 2.93412655592\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.70514154434,21.3294809841), test loss: 32.6155148983\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (4.52454710007,4.0533945825), test loss: 3.3239055723\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.68388557434,21.2153020788), test loss: 36.0546012402\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.3879096508,4.03805929423), test loss: 3.36106612086\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.68931150436,21.1021438571), test loss: 31.1337289333\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.29216146469,4.02288754914), test loss: 3.23958647996\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (6.52306175232,20.9899632268), test loss: 31.1382830143\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.6451818943,4.00788276196), test loss: 3.03290806711\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (2.28368854523,20.8786461859), test loss: 30.930009222\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.54307579994,3.99305043129), test loss: 3.05065470785\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (2.8864440918,20.7683017639), test loss: 34.0433409214\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.77692365646,3.9783551307), test loss: 3.44920980036\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.26300001144,20.6589725819), test loss: 33.9263299465\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.29037487507,3.96383829023), test loss: 3.29538754672\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (2.529712677,20.5505788029), test loss: 29.7760287762\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.57189536095,3.94944671701), test loss: 3.24984464049\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.45312356949,20.4431028277), test loss: 32.0584915161\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.76553022861,3.93520782811), test loss: 2.98695927858\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.35265445709,20.3367286178), test loss: 31.1384223461\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.694348812103,3.92109855966), test loss: 3.09619081616\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (1.6552914381,20.231213594), test loss: 37.1081404209\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.91873121262,3.90717437721), test loss: 3.37659694999\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (3.24896454811,20.1266175658), test loss: 32.5724420071\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.898485779762,3.89336862987), test loss: 3.23665371537\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (7.35095596313,20.0229808144), test loss: 31.6940546513\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.74037861824,3.87970031872), test loss: 3.06728840917\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (2.16365838051,19.9201777315), test loss: 33.9021272182\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.53670799732,3.86616895705), test loss: 2.96917163581\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (6.41869592667,19.8183622145), test loss: 34.674295187\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.68624150753,3.85279562838), test loss: 3.42302486151\n",
      "run time for single CV loop: 7989.17183495\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 3\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (273.16418457,inf), test loss: 169.076036835\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (310.112670898,inf), test loss: 352.010662842\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (40.362701416,66.7274130611), test loss: 38.9085567474\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.07815551758,64.3689424767), test loss: 3.18043467999\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (29.4977741241,55.4483884454), test loss: 40.2729872704\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.066801548,33.7196525843), test loss: 3.39976100922\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (18.8141555786,51.611882048), test loss: 39.3330229759\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.68927192688,23.4966914405), test loss: 3.2984159261\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (45.1170425415,49.6988426712), test loss: 35.8688193798\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (8.36039543152,18.3866002947), test loss: 3.43341102302\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (32.7569046021,48.513971501), test loss: 39.3395636082\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.15409803391,15.3165390524), test loss: 3.08570141196\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (10.9320106506,47.683108222), test loss: 40.1301316261\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.71142029762,13.2701282251), test loss: 3.33777431548\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (50.9492721558,47.1437156581), test loss: 38.2802001953\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.11149787903,11.8066086938), test loss: 3.44522559345\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (56.5845413208,46.708476849), test loss: 34.3528733253\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.19030284882,10.7080724635), test loss: 3.40023572445\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (49.1396064758,46.3584582258), test loss: 39.9434884071\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (6.44285058975,9.85380759484), test loss: 2.95259500742\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (21.2721862793,46.073614152), test loss: 39.9297328949\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.43910217285,9.16799374774), test loss: 3.37782637477\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (31.029460907,45.8378591101), test loss: 37.833300209\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.34851741791,8.60954652324), test loss: 3.3287219137\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (41.0401611328,45.6314308326), test loss: 35.6518109322\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (9.90706443787,8.14432397534), test loss: 3.1175098449\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (33.4514122009,45.4512160303), test loss: 40.4807453156\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.92607367039,7.74879708709), test loss: 3.04246464968\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (60.8915023804,45.2973349298), test loss: 39.9447063923\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.8062710762,7.40993307438), test loss: 3.45699810386\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (12.0301609039,45.1604134015), test loss: 36.5500204563\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.05292224884,7.11647171941), test loss: 3.28538882136\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (76.2194976807,45.0368991661), test loss: 36.3158153534\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.65836262703,6.85858759926), test loss: 3.07666485608\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (43.907043457,44.9179068754), test loss: 39.5691843033\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.81331539154,6.63268377572), test loss: 3.14744438231\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (20.6541137695,44.8041077344), test loss: 38.9048577309\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.35012722015,6.43078568313), test loss: 3.40329642892\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (21.0344963074,44.6890517354), test loss: 35.2983602047\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.34658634663,6.25025778884), test loss: 3.26067303717\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (27.7402687073,44.6014504931), test loss: 38.0862518787\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.5173766613,6.08775062706), test loss: 3.06346466541\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (107.938064575,44.5181645903), test loss: 40.2742708206\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.89831876755,5.94073067726), test loss: 3.26297996342\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (39.8204574585,44.4234330018), test loss: 38.1927984238\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.729194641113,5.80680603909), test loss: 3.31539725065\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (42.7583770752,44.3367872137), test loss: 34.5613102913\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.9742295742,5.68383918237), test loss: 3.22105923891\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (93.743560791,44.2604832353), test loss: 37.4351440907\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (6.0030374527,5.5725719517), test loss: 2.9906095624\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (34.8535232544,44.175725537), test loss: 39.055388546\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.04106760025,5.46944603402), test loss: 3.285360533\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (61.1822853088,44.1010245954), test loss: 38.2621473789\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.60343456268,5.37407409339), test loss: 3.25731583536\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (62.9566955566,44.0260353616), test loss: 34.8343024254\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (6.17292404175,5.28580873127), test loss: 3.31128242016\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (27.0990371704,43.9507902751), test loss: 37.9330492496\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.90096497536,5.20358947282), test loss: 2.96350394785\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (37.5211181641,43.88120231), test loss: 38.6695221424\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.4104783535,5.12662640984), test loss: 3.24785804451\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (29.5820503235,43.8078294982), test loss: 37.1238128662\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.38254153728,5.05569684335), test loss: 3.30254361033\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (26.6157112122,43.7336400961), test loss: 32.9310899258\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.32453131676,4.98875476775), test loss: 3.22859476805\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (57.0479888916,43.6573019781), test loss: 38.4103972435\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.15656900406,4.92632806965), test loss: 2.90891917646\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (36.2772674561,43.5889864946), test loss: 39.5734040737\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.2486178875,4.86736322259), test loss: 3.38348984718\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (28.7112121582,43.5216779453), test loss: 36.5453873634\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.34133958817,4.8119646793), test loss: 3.23198421299\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (39.0815353394,43.448429083), test loss: 34.469185257\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.527423679829,4.75958357749), test loss: 3.00950638056\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (89.8749389648,43.3751076091), test loss: 38.6300911903\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.80686330795,4.71001800424), test loss: 2.96285695732\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (53.9141731262,43.3023172764), test loss: 38.1208153725\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.14166593552,4.66365318263), test loss: 3.34186915457\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (26.3715286255,43.2212741393), test loss: 34.1270006657\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.03627705574,4.61948501119), test loss: 3.13295973837\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (51.7115592957,43.1482577794), test loss: 34.4788297176\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.42113518715,4.57739853538), test loss: 3.05438608527\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (22.4973869324,43.0700475194), test loss: 37.0608120918\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.71862864494,4.5377177122), test loss: 3.07295607924\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (61.0867614746,42.9913975912), test loss: 37.0012359142\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.16684961319,4.49964205083), test loss: 3.28114094734\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (25.3466720581,42.9115506689), test loss: 33.3975136757\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.10388207436,4.46317797943), test loss: 3.11873155236\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (32.4941253662,42.8287355193), test loss: 35.3502313614\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.31779813766,4.42908553674), test loss: 2.93909400403\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (21.4368610382,42.7417630476), test loss: 37.8253602982\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.8321890831,4.39636333314), test loss: 3.16010796726\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (67.856628418,42.6531582801), test loss: 36.507889843\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.84632325172,4.36536213263), test loss: 3.23610975295\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (32.0513153076,42.5631305563), test loss: 32.2256916046\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (5.85335636139,4.3356204236), test loss: 3.16991494596\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (26.8524894714,42.4733657493), test loss: 34.3679123878\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.47451114655,4.30712823678), test loss: 2.88077367842\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.1099567413,42.3768959305), test loss: 35.789645195\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.6429336071,4.27960232348), test loss: 3.18369933963\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (35.7735595703,42.2793595873), test loss: 35.5194873333\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (7.8456735611,4.25349149689), test loss: 3.13216506839\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (22.4057865143,42.1770418102), test loss: 31.2180077076\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.10232639313,4.22824887394), test loss: 3.15855170488\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (8.6912651062,42.0673899337), test loss: 34.1307537556\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.61765313148,4.20411174918), test loss: 2.85904062688\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (38.3537330627,41.9548673548), test loss: 34.994971323\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.82851839066,4.18075509274), test loss: 3.19834508002\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (34.6519889832,41.8305794945), test loss: 33.479746151\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.04147791862,4.15815478324), test loss: 3.18493807018\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (34.7654953003,41.7035279707), test loss: 28.7996420622\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (5.17026853561,4.13617847227), test loss: 3.06454772949\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (15.9112949371,41.5706204767), test loss: 33.4283139229\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.36278533936,4.11454448294), test loss: 2.79154296517\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (16.597202301,41.434799802), test loss: 34.6035781384\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.24546074867,4.09409067101), test loss: 3.30525681078\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (37.9695892334,41.2934613587), test loss: 31.9885620117\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (8.28305053711,4.07438614464), test loss: 3.09254124165\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (45.0322113037,41.1463448483), test loss: 29.4099441051\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.75611257553,4.05500796925), test loss: 2.90222386122\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (49.012008667,40.9946352793), test loss: 32.7964285851\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (4.15164852142,4.03632928019), test loss: 2.92156150043\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (23.6943435669,40.8406608902), test loss: 32.1554359436\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.53826713562,4.01814879667), test loss: 3.25752435327\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (34.8083953857,40.6812543842), test loss: 29.4349541187\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.55327677727,4.00031986901), test loss: 2.97242231369\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (36.763343811,40.5193128593), test loss: 28.0061034203\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (5.30013084412,3.98334890207), test loss: 2.88789259791\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (24.705078125,40.3532290838), test loss: 30.0924184799\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.842856884,3.96657897742), test loss: 3.00974683464\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (28.4379692078,40.1803975292), test loss: 31.5785923719\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.28570127487,3.95022810444), test loss: 3.17960363626\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (26.7087955475,40.0075082359), test loss: 28.1733891726\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.937242567539,3.9342504131), test loss: 3.04793438017\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (61.9237976074,39.8331166599), test loss: 28.0286678553\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.61060261726,3.91863157447), test loss: 2.82030769587\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (21.0753898621,39.6548522717), test loss: 30.5418616295\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.67554759979,3.90305640864), test loss: 3.0758344084\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (18.218870163,39.4753603701), test loss: 31.3070164204\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.24462223053,3.88743411314), test loss: 3.12219246626\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (58.4999237061,39.2991598941), test loss: 26.7153247237\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (6.7691693306,3.87245217097), test loss: 3.04173158258\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (24.6587028503,39.1185498497), test loss: 27.4599183559\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.20454621315,3.85739985606), test loss: 2.80461884141\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (33.5481376648,38.9399550517), test loss: 29.421487236\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.89688324928,3.84244892583), test loss: 3.12411382794\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (9.92317199707,38.7602207202), test loss: 30.1662224293\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (4.67315006256,3.82767295173), test loss: 3.14001812935\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (10.6655302048,38.5816471516), test loss: 25.4072993279\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.32720851898,3.81299784491), test loss: 3.05507962108\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (15.0259218216,38.402262246), test loss: 27.6769000053\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.58711504936,3.79830121029), test loss: 2.76137440652\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.3382911682,38.2246348115), test loss: 28.686286664\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.58130288124,3.78407716918), test loss: 3.05329722315\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (12.8769321442,38.0464817277), test loss: 28.296955514\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.926052987576,3.76973694073), test loss: 3.06808833629\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (21.7139434814,37.8674120522), test loss: 24.654337579\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (4.35293865204,3.75568768815), test loss: 2.93992298543\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (17.4521064758,37.6903266178), test loss: 28.319382\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.69568419456,3.74167415731), test loss: 2.76105623245\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.8221750259,37.5150764977), test loss: 28.01382581\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.95190918446,3.72786535527), test loss: 3.20763528943\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (36.7077674866,37.3387362354), test loss: 27.5837582707\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.56215465069,3.71407754029), test loss: 3.05663985312\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (49.9357566833,37.1633677606), test loss: 25.4172316074\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (4.01598405838,3.70039564861), test loss: 2.84407230318\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (33.7868766785,36.9902064749), test loss: 27.5358773708\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.67211103439,3.6870879461), test loss: 2.8456196785\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (14.0994739532,36.8150429443), test loss: 27.9072826385\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.77626991272,3.67380657571), test loss: 3.18373140842\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (19.5651073456,36.6426064951), test loss: 26.3664964676\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.34044039249,3.66059146137), test loss: 3.01270437688\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.7683458328,36.4699797808), test loss: 25.0498954296\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.31343460083,3.64759338251), test loss: 2.84674831182\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (27.9452819824,36.2991684803), test loss: 26.6883486271\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.88265371323,3.63462484018), test loss: 2.91902530491\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (15.0606470108,36.1284495753), test loss: 28.0226536512\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.09587466717,3.62168877078), test loss: 3.14008500278\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (50.6302833557,35.9601665327), test loss: 24.6347177863\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.51939201355,3.60919835128), test loss: 3.01534251571\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (13.4552555084,35.7914631801), test loss: 25.252528441\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.7037062645,3.59663145635), test loss: 2.76637753099\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (36.8007965088,35.6240151384), test loss: 26.459057188\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.66101908684,3.58429915944), test loss: 3.00269458741\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.7276716232,35.4561065353), test loss: 28.0203049898\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.88154459,3.57195931261), test loss: 3.15593459606\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (14.890709877,35.2912175175), test loss: 24.1611086607\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.81223773956,3.55983719381), test loss: 3.05183932483\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (10.0338201523,35.1256447378), test loss: 25.0679346085\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.61680305004,3.54768440809), test loss: 2.78402453959\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (29.5032157898,34.9618463212), test loss: 26.0984559059\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.4047563076,3.53576473965), test loss: 3.03517105579\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (12.2166576385,34.7989866814), test loss: 27.4946735382\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.768929362297,3.52400557123), test loss: 3.1465659678\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (13.8203687668,34.6356895673), test loss: 23.3377453804\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.51710581779,3.51237131951), test loss: 3.02630974948\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (9.37321662903,34.4739128094), test loss: 26.5232197285\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.93021643162,3.50076330781), test loss: 2.74198499471\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (12.8102817535,34.3124718992), test loss: 26.2901422024\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.71907842159,3.48931865601), test loss: 2.9634776473\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (22.8636569977,34.1525281316), test loss: 26.3405303121\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.23154509068,3.47789561265), test loss: 3.03613483012\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (23.4731407166,33.9926107641), test loss: 23.2521729708\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.26385128498,3.46654601847), test loss: 2.87623434961\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.8508014679,33.8343269398), test loss: 26.91983217\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.44666290283,3.45555385599), test loss: 2.7316173628\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (12.8676633835,33.6761915667), test loss: 26.1238163471\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (5.51668930054,3.44455329146), test loss: 3.10337263346\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (13.0203924179,33.5187137792), test loss: 26.5294433117\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.66409945488,3.43363076142), test loss: 3.09908765554\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (35.4338912964,33.3614907437), test loss: 24.5561927319\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (3.39729595184,3.42280425058), test loss: 2.8561770305\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (13.294178009,33.2053333746), test loss: 27.3987128735\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.01154088974,3.4121103997), test loss: 2.81549772918\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (9.99859809875,33.0491377587), test loss: 27.0020967007\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.52074944973,3.40140609839), test loss: 3.14954697788\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (11.223200798,32.8942986789), test loss: 25.9961168766\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.83675837517,3.39101544353), test loss: 3.07231319845\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (21.6298484802,32.7404578694), test loss: 25.0098192215\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.45994639397,3.38058264952), test loss: 2.78511209488\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (24.0040855408,32.5861404395), test loss: 25.8812883377\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.33099973202,3.37031163007), test loss: 2.8937387526\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (10.7531986237,32.4324252564), test loss: 27.3165468216\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.73172998428,3.3600861685), test loss: 3.12410149872\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (42.8478736877,32.27986166), test loss: 24.187283814\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.25006580353,3.34999764521), test loss: 3.02001830935\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (13.6016788483,32.127161023), test loss: 26.0028547525\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.15737426281,3.33994253773), test loss: 2.71653747261\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (8.56846046448,31.9748505268), test loss: 25.6497843504\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.96922779083,3.32995209219), test loss: 2.96562751532\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (34.7090606689,31.8244769952), test loss: 27.7424365997\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (3.88211512566,3.32021784176), test loss: 3.10914565921\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (21.85779953,31.673384201), test loss: 23.9198211193\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.67252755165,3.31048916751), test loss: 3.04279954135\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (8.89525985718,31.5227118873), test loss: 25.1474198341\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.88707661629,3.30079415407), test loss: 2.78404541612\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (6.07669305801,31.3723622666), test loss: 25.8021317005\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.42612218857,3.29118058327), test loss: 2.97101247311\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (8.06216907501,31.2226413651), test loss: 27.4375383377\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.79517483711,3.28167467349), test loss: 3.11598993093\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (7.41473674774,31.073108309), test loss: 23.2777303696\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.83060526848,3.27215789235), test loss: 2.99843916297\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (6.83672904968,30.9246613519), test loss: 26.1500995159\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.37630772591,3.26290735254), test loss: 2.72655705065\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (11.5776023865,30.7767348326), test loss: 26.0230778217\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.982253670692,3.25359667501), test loss: 2.9005224824\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (8.05398654938,30.6286397782), test loss: 26.9235012054\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.90791225433,3.24443722573), test loss: 3.05021950901\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (13.3515548706,30.4808007349), test loss: 23.5684751034\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.97630310059,3.23525839728), test loss: 2.8515829742\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (16.3415126801,30.3339556549), test loss: 27.0290707827\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.96584606171,3.226213146), test loss: 2.70450376272\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (21.8417034149,30.1872190858), test loss: 26.2869053364\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.60138547421,3.21719016452), test loss: 3.02880289555\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (30.2938594818,30.0407270997), test loss: 27.4133637667\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.94774651527,3.20824187406), test loss: 3.11569222212\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (11.9210882187,29.8954294074), test loss: 25.2062108994\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.25183176994,3.19947884025), test loss: 2.7932766825\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (4.54452943802,29.7497326837), test loss: 28.2504744053\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.16912174225,3.19070823025), test loss: 2.76833100915\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (9.35637283325,29.6048080479), test loss: 27.8911098003\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.25293421745,3.18196645158), test loss: 3.15243887603\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (10.6046676636,29.4600127062), test loss: 26.5420474529\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.3404417038,3.1733125687), test loss: 3.07834466547\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (7.46896696091,29.3159439478), test loss: 26.5187161446\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.14703989029,3.16466370643), test loss: 2.70612885654\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (15.0205545425,29.1722257627), test loss: 26.674541235\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.714408636093,3.15605193686), test loss: 2.86326078475\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (13.744433403,29.0292337517), test loss: 28.3857737064\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.731819391251,3.1476631901), test loss: 3.12968361527\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (9.34025382996,28.886816243), test loss: 25.7609353542\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.817420601845,3.13921493697), test loss: 2.96455758363\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (18.1767120361,28.7448306018), test loss: 26.9150069475\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.56193804741,3.13090706122), test loss: 2.71153125465\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (11.2786417007,28.6028103795), test loss: 27.0858057022\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.623239398,3.12257047802), test loss: 2.92008980513\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (8.37264633179,28.4619489154), test loss: 29.0684570312\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.49435245991,3.11430944164), test loss: 3.0568867445\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (3.36906385422,28.3212447485), test loss: 25.1263813496\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.04088711739,3.10607485457), test loss: 2.99395400882\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (11.413690567,28.1813662847), test loss: 27.3076863766\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.91508293152,3.09792752064), test loss: 2.74314347506\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.8836517334,28.0422325657), test loss: 26.800004673\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.864205956459,3.08990350863), test loss: 2.89162674397\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (3.91021180153,27.9032728765), test loss: 29.4493541718\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.28590750694,3.08189825369), test loss: 3.11365677714\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (2.92045688629,27.765052597), test loss: 25.4502340317\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.27689170837,3.07388318952), test loss: 3.01659604609\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (7.50511598587,27.6273022421), test loss: 27.8867228508\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.11188197136,3.06596537578), test loss: 2.71086699963\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (6.58116722107,27.4905033766), test loss: 27.0769256115\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.532345056534,3.05802514154), test loss: 2.91113539934\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (6.52534580231,27.354143368), test loss: 28.7578554153\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.50951707363,3.05014376278), test loss: 3.056283997\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (6.33970689774,27.2187293084), test loss: 25.3463650703\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.74369943142,3.04243690527), test loss: 2.82812423408\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.03633546829,27.0838971965), test loss: 28.0109984875\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (3.34194993973,3.03472009698), test loss: 2.68583814651\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (3.50591015816,26.9496882184), test loss: 28.2605470181\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.58540797234,3.0270486494), test loss: 3.03286547661\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (7.34439086914,26.8159330091), test loss: 29.9822257519\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.40840911865,3.0193914219), test loss: 3.14103684425\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (6.40634965897,26.6832430346), test loss: 28.4590047359\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.24211549759,3.01179362462), test loss: 2.77166677862\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.95461750031,26.5508733475), test loss: 30.0150462627\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.36910533905,3.00419865368), test loss: 2.76646402478\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (3.07220411301,26.4197072842), test loss: 29.954491806\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (4.64025592804,2.99675891955), test loss: 3.17635686696\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.23314905167,26.2892453343), test loss: 28.2441198349\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.50251889229,2.98931970952), test loss: 3.03992801905\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (11.9245357513,26.1593929369), test loss: 28.3826281071\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.42731380463,2.98193798824), test loss: 2.69414716959\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.59414625168,26.0302126507), test loss: 28.538633728\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.15602517128,2.97456966957), test loss: 2.86757642478\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (9.09715461731,25.9018607788), test loss: 30.1564217567\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.17945384979,2.96725813641), test loss: 3.15667605102\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (4.83400821686,25.7744310473), test loss: 27.3192789555\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.3584227562,2.95996849093), test loss: 2.95527775288\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (2.49783325195,25.647638609), test loss: 28.6851602554\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.08040928841,2.95271480551), test loss: 2.73887213171\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (3.87373924255,25.521916804), test loss: 28.5349040985\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (4.01814031601,2.94562027524), test loss: 2.86906135678\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 6\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (204.10975647,inf), test loss: 172.727449799\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (329.57723999,inf), test loss: 350.386965942\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (33.5125427246,67.1828444138), test loss: 37.9159409285\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.33324813843,64.2253806704), test loss: 3.19633245766\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (16.7385826111,55.7853030849), test loss: 41.0614480257\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.02181768417,33.6412984058), test loss: 3.11805590987\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (59.3137512207,52.0076790263), test loss: 40.6445759773\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.54118585587,23.440925716), test loss: 3.41324510872\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.4231414795,50.1178759248), test loss: 39.0385313511\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.32895326614,18.3357189804), test loss: 3.45516060293\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (47.2856636047,48.9323025774), test loss: 35.767185688\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.470005631447,15.2721566927), test loss: 3.38161166608\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (81.806854248,48.1692333042), test loss: 40.5092391491\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.07493185997,13.2270033159), test loss: 2.97643896639\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (15.3032445908,47.5902271776), test loss: 39.9439759254\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.40109562874,11.7657296687), test loss: 3.27170069218\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (52.977355957,47.1652716737), test loss: 39.5260234833\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.67486000061,10.6702048947), test loss: 3.18555823863\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (40.6472167969,46.8334242436), test loss: 36.0381367207\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.32759666443,9.81958337178), test loss: 3.27415066361\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (100.585273743,46.5529741077), test loss: 39.4451224327\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (7.5538687706,9.1371211329), test loss: 3.05865607858\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (16.3751049042,46.3142787207), test loss: 40.6954312801\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.59406900406,8.57773688753), test loss: 3.14852135479\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (25.8800945282,46.1009852858), test loss: 39.7036443233\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.75374937057,8.11220659017), test loss: 3.3037738502\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (16.4617767334,45.9157518768), test loss: 36.5916591644\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.36605358124,7.71733005644), test loss: 3.16514816284\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (53.0854949951,45.7631196399), test loss: 35.3212054729\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.6713116169,7.378973465), test loss: 3.09696578085\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (52.4145889282,45.6286330875), test loss: 40.9359922886\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.68513560295,7.08549662591), test loss: 2.95877232552\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (30.2608566284,45.4977395358), test loss: 40.5429744244\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.30224704742,6.82880029258), test loss: 3.32066729665\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (30.256652832,45.3874088495), test loss: 39.3822581768\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.11465704441,6.60182235193), test loss: 3.18920732439\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (28.3840522766,45.2887431387), test loss: 34.7623044014\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.87311649323,6.39954808101), test loss: 3.29455221295\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (72.5941162109,45.188479957), test loss: 39.3892721653\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.45924758911,6.21855318425), test loss: 2.90192577243\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (44.9662857056,45.1033418156), test loss: 39.4347870827\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.279674112797,6.05496161159), test loss: 3.23212808967\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (43.2878570557,45.0146480048), test loss: 38.6984550476\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.02447295189,5.90720685483), test loss: 3.23051661849\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (109.972640991,44.9389735085), test loss: 35.5916967869\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.54391956329,5.77321770804), test loss: 3.12283221483\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (31.2190151215,44.861221818), test loss: 36.732172823\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.46228206158,5.65108484843), test loss: 2.95491399765\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (71.3137588501,44.7893299401), test loss: 40.5831186295\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.69969034195,5.53871250029), test loss: 3.00173524916\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (12.352845192,44.7118179396), test loss: 38.9437004805\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.17562389374,5.43466199452), test loss: 3.26977466345\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (45.9738845825,44.6385545893), test loss: 38.4949401855\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.41462182999,5.33911977115), test loss: 3.23191863894\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (35.9881782532,44.5657143912), test loss: 34.6637343645\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.26997089386,5.25001420296), test loss: 3.19476032853\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (25.7841491699,44.4985998501), test loss: 40.1668120384\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.93040847778,5.16753895761), test loss: 2.92301049531\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (34.724319458,44.4346992217), test loss: 39.2530600548\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.01739120483,5.09054162908), test loss: 3.17463389337\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (51.7899093628,44.3675074342), test loss: 38.2622768402\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.73515963554,5.01875254051), test loss: 3.07646530569\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (41.1906661987,44.3056018505), test loss: 34.1456208706\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (0.68678355217,4.95129057942), test loss: 3.12286459208\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (28.3897342682,44.2453801977), test loss: 38.3689107895\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.55730199814,4.88800723314), test loss: 2.87541416883\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (28.1224975586,44.1813970421), test loss: 39.8063607693\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.45049393177,4.82824362926), test loss: 3.09995553493\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (44.296749115,44.1231842303), test loss: 38.542020607\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.47255778313,4.7718725898), test loss: 3.17537043989\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (40.2590026855,44.0598710299), test loss: 35.1756110191\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.93673348427,4.71868672159), test loss: 2.98934744\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (90.765625,44.0013662026), test loss: 35.2373809814\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.48569726944,4.66884564376), test loss: 2.89800821543\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (39.1154632568,43.9383009355), test loss: 38.9499067307\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.532264232635,4.62160619018), test loss: 2.87657714188\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (75.2287445068,43.8784306823), test loss: 39.5154325962\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.60462284088,4.5768058455), test loss: 3.2325247407\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (13.0617179871,43.810346011), test loss: 37.3248719931\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.843754470348,4.53375354326), test loss: 3.16499623656\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (35.9078102112,43.7443768678), test loss: 33.6806018353\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.79286575317,4.4932786104), test loss: 3.18205159009\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (27.8854637146,43.6763145026), test loss: 38.083905077\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.25370979309,4.45431141445), test loss: 2.81980912387\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (55.6144866943,43.6115890702), test loss: 37.6462082863\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.99746656418,4.41748956793), test loss: 3.1345282346\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (30.48695755,43.5437811286), test loss: 37.5426823139\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.62888407707,4.3820671206), test loss: 3.10024593472\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (52.9349822998,43.4742510898), test loss: 34.0138450623\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.50740003586,4.34850519946), test loss: 3.10939139724\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (74.3168258667,43.4065052228), test loss: 36.508131361\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.6383357048,4.31611460512), test loss: 2.8703099668\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (45.2687263489,43.3354872745), test loss: 38.6262783051\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.82211899757,4.28511479057), test loss: 2.95435604453\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (43.4925842285,43.2613901371), test loss: 37.2710306644\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.71408128738,4.25521487944), test loss: 3.17714460492\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.8011207581,43.1874303114), test loss: 34.1440555334\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.73081660271,4.22633109635), test loss: 2.93667699993\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (85.6940994263,43.1112657096), test loss: 32.2245253325\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.89108681679,4.19864560539), test loss: 2.89008134604\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (35.5175743103,43.0312022482), test loss: 37.0701852798\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.96081590652,4.17224504878), test loss: 2.70469489694\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (29.9848079681,42.9465100699), test loss: 35.9127811432\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.03831624985,4.14688338648), test loss: 3.01108038723\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (30.1464424133,42.8621197647), test loss: 36.4449961185\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.22060370445,4.12230369889), test loss: 2.95908290744\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (32.1181564331,42.7698362742), test loss: 31.5973583937\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.83708167076,4.09836202626), test loss: 2.94352290928\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (9.84290790558,42.6729918359), test loss: 34.8448256016\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.10018002987,4.07551281451), test loss: 2.69817755222\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (47.7437705994,42.5686465558), test loss: 35.8279346943\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.14764881134,4.05327697792), test loss: 3.09003172219\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (52.5948410034,42.4618726439), test loss: 35.5935525894\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.53149843216,4.03176886285), test loss: 3.12507612109\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.4811534882,42.3493897592), test loss: 31.6385222912\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.28678226471,4.01084890039), test loss: 2.94866130054\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (51.2554473877,42.2338371355), test loss: 31.7804608345\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.23839783669,3.99083696872), test loss: 2.78593622148\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (106.854919434,42.1161949446), test loss: 34.0854294777\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.61459207535,3.97131883559), test loss: 2.76133111119\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (24.8534164429,41.9910937934), test loss: 33.8576769829\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.58983707428,3.95236662457), test loss: 3.10313392282\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (47.9276123047,41.8631346688), test loss: 33.252907753\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.60728740692,3.93388583326), test loss: 3.03608495891\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (16.6791229248,41.7311863278), test loss: 29.0145191193\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.63871431351,3.91580784208), test loss: 2.92183462977\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (71.5242614746,41.5960690741), test loss: 32.3512875557\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.96157646179,3.89832894859), test loss: 2.68468793631\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (29.1628646851,41.4572798419), test loss: 32.613411665\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (7.58476877213,3.88167263761), test loss: 2.94751880765\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (27.4776496887,41.3127936733), test loss: 32.4427253246\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.46354317665,3.86509175575), test loss: 2.85598215759\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (23.3507022858,41.1660705835), test loss: 29.0397193909\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.36888861656,3.84897009304), test loss: 2.84743137956\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (21.4008827209,41.0125914971), test loss: 30.5423065186\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.97715747356,3.83303273372), test loss: 2.64944683611\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (26.5551071167,40.857107469), test loss: 32.2122507095\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.02814102173,3.81768522128), test loss: 2.90423167646\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (33.4339828491,40.698892357), test loss: 31.9813789368\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (5.50695753098,3.80255128479), test loss: 3.03300364316\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (21.6285076141,40.5389150316), test loss: 29.1441464186\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.93235766888,3.78776033644), test loss: 2.82562844455\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (23.9117107391,40.3737419428), test loss: 27.3068900347\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (4.14250564575,3.77333546687), test loss: 2.71673256457\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (21.4045448303,40.2062315986), test loss: 30.8944139004\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.837048769,3.75938899498), test loss: 2.66204138696\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (28.4748325348,40.0390892656), test loss: 30.9247932911\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.60569620132,3.74555767611), test loss: 3.06402593851\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (6.46363592148,39.8677004325), test loss: 30.9706010818\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.6733648777,3.73194314636), test loss: 2.97108882964\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (26.2485580444,39.6978188723), test loss: 26.6497537613\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.19192814827,3.71833042887), test loss: 2.94872702062\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (30.0276908875,39.5259962491), test loss: 28.3048592567\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.92216563225,3.70483988553), test loss: 2.65023856163\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (21.3328933716,39.3551982822), test loss: 29.3988560915\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.55343937874,3.69156184669), test loss: 2.92346459329\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (24.6811256409,39.1836601507), test loss: 30.8691718578\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (6.97059774399,3.67857584961), test loss: 2.94844771028\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.9918174744,39.0120061978), test loss: 27.5480865598\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.76411128044,3.66531599044), test loss: 2.88795278668\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.2119951248,38.8395923508), test loss: 27.242690134\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.84896421432,3.65188287393), test loss: 2.71867501736\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (18.2064037323,38.6664582505), test loss: 28.8961542606\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.91644120216,3.63837898686), test loss: 2.81347729266\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (21.8790931702,38.4931251292), test loss: 29.5089283943\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.67194747925,3.62510231189), test loss: 3.05601917803\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (44.2316741943,38.3219143082), test loss: 28.4726525784\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.74387907982,3.61188941546), test loss: 2.94207843989\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (19.389957428,38.1497516822), test loss: 25.9025994301\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.45841050148,3.5988109069), test loss: 2.89966924489\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (17.2716064453,37.9779714955), test loss: 29.0993165493\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (5.69028949738,3.58590111516), test loss: 2.66587967277\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (7.62389278412,37.8072904633), test loss: 28.5512704849\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (4.73633670807,3.5731786209), test loss: 2.88765414953\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (23.1964626312,37.6384240272), test loss: 29.8616876602\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.51074838638,3.56048047274), test loss: 2.97162198573\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.2599315643,37.4682848402), test loss: 25.6541208267\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.24705338478,3.54794943005), test loss: 2.89593664706\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (7.41136837006,37.2993989789), test loss: 27.145405364\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.549500763416,3.53545982286), test loss: 2.69512020499\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (18.1928577423,37.1312434825), test loss: 28.9797803879\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.7569770813,3.52313805004), test loss: 2.93942572176\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (10.2376270294,36.9643040702), test loss: 29.1929662228\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (4.60270547867,3.51102710561), test loss: 3.04593734145\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (31.1803588867,36.7987295212), test loss: 27.4147060871\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (5.23864555359,3.4992635556), test loss: 2.93727788031\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (19.4703121185,36.6324371095), test loss: 25.7833664894\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.25942921638,3.4873753527), test loss: 2.766158925\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (18.8889064789,36.4676601702), test loss: 27.8252759933\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.45764386654,3.47560215083), test loss: 2.71864322126\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (16.5310115814,36.3018585677), test loss: 27.6067512512\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (5.36496353149,3.4640392023), test loss: 2.99114696234\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (17.0174922943,36.1377598737), test loss: 28.4502531052\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.53507423401,3.45257869357), test loss: 3.07633615434\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (14.6897964478,35.9741204106), test loss: 24.6115140438\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.61502349377,3.44121384182), test loss: 2.98596770167\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (9.34582901001,35.8112989056), test loss: 28.1864182949\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.33114242554,3.4299716474), test loss: 2.68875807375\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (9.16112232208,35.6480298838), test loss: 26.9111388206\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (4.07642269135,3.41887921635), test loss: 2.87893580794\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (19.7491970062,35.4873511596), test loss: 29.1921521664\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.55203914642,3.40792196461), test loss: 2.98878791928\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (11.8161287308,35.3265964095), test loss: 25.9493654966\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.93733608723,3.39697649678), test loss: 2.98828652799\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (14.5822486877,35.1664298479), test loss: 26.3439002991\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.190166190267,3.38617437376), test loss: 2.74346823096\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (37.1501693726,35.007012268), test loss: 27.3008934498\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.81561124325,3.37544737442), test loss: 2.83511844724\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (8.92873573303,34.8474118421), test loss: 27.9470206738\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.66876769066,3.36481277185), test loss: 3.01299080551\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (17.4238491058,34.6896374706), test loss: 27.2860997438\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.24284744263,3.35437838557), test loss: 2.8843280986\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (24.5533790588,34.5321944375), test loss: 25.0193636179\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (4.24917078018,3.34417785518), test loss: 2.79791951925\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (35.2736816406,34.3751526775), test loss: 27.64309659\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (4.19282150269,3.33391304377), test loss: 2.66793410927\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (13.5628271103,34.2179999459), test loss: 27.2588513374\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.88525056839,3.32373194272), test loss: 2.91219343543\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (17.1709022522,34.0612212807), test loss: 29.1335437298\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (4.12981367111,3.31371120829), test loss: 3.01143525839\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (21.1439399719,33.904638644), test loss: 24.2527550459\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.6446968317,3.3037070519), test loss: 2.96156292856\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (15.4790248871,33.749334798), test loss: 26.3186724186\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.12287282944,3.29384838655), test loss: 2.65719510168\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (8.34184646606,33.5930241172), test loss: 27.7648201942\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.58216238022,3.2840479159), test loss: 2.89574706256\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (13.0709657669,33.4378763681), test loss: 28.2173941612\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.21961545944,3.27437452222), test loss: 3.04438640177\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (14.6528434753,33.2827009072), test loss: 25.6825735092\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.0973546505,3.26479536267), test loss: 2.94467273057\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (7.20796585083,33.1286211145), test loss: 24.6647128344\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.38363313675,3.25523745745), test loss: 2.7107619822\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (23.2148342133,32.9739571624), test loss: 28.7754191399\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.2223815918,3.24578588258), test loss: 2.72484225631\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (12.8042593002,32.82037997), test loss: 26.7454046249\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.940700471401,3.23637608668), test loss: 2.9923811391\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (33.9748916626,32.6661569072), test loss: 28.3053133249\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.58616280556,3.22706216617), test loss: 3.00857394636\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (35.7279243469,32.513610963), test loss: 23.972966671\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.71900892258,3.21792530615), test loss: 2.90541793704\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.79019880295,32.3601894816), test loss: 27.0850557089\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.01499247551,3.20892403682), test loss: 2.71123436242\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (16.666557312,32.2080715269), test loss: 26.2879805565\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.88123512268,3.19992573779), test loss: 2.78671499044\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (14.2824029922,32.0551939442), test loss: 29.207953167\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.53983640671,3.19093193213), test loss: 2.90204816759\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (17.9042243958,31.9029182177), test loss: 25.2315079689\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.770221590996,3.18208435138), test loss: 2.88494042754\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (26.3965911865,31.7507159607), test loss: 26.3723558426\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.988431036472,3.17324897527), test loss: 2.64035229981\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.84377765656,31.5989455188), test loss: 28.5055862427\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.55093586445,3.16452394975), test loss: 2.81163375676\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (4.38118505478,31.4470867753), test loss: 27.6291305065\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.7903380394,3.15585279431), test loss: 2.97025053799\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (15.7046136856,31.2956466263), test loss: 27.9542336941\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.3869035244,3.14725768759), test loss: 2.80904523134\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (10.531162262,31.1448047502), test loss: 24.7653024197\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.49779367447,3.13874806186), test loss: 2.7228724122\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (14.1033382416,30.9943755307), test loss: 29.314859724\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.59919404984,3.13026770253), test loss: 2.66101354063\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (10.7326097488,30.8440834209), test loss: 28.1716674805\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.43709683418,3.12180910713), test loss: 2.8994068712\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (4.31685829163,30.6939492929), test loss: 28.9348127365\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.04802083969,3.11341626742), test loss: 3.04925613701\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (17.5217514038,30.5444731167), test loss: 25.1000173807\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.58662521839,3.10509614655), test loss: 2.98231032789\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (21.202589035,30.3952145393), test loss: 26.6491742134\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (3.12851810455,3.09691783377), test loss: 2.65538467467\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (9.2193107605,30.246538883), test loss: 26.1997870445\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.807609975338,3.08881093433), test loss: 2.79623770714\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (15.634262085,30.0980266787), test loss: 29.0641618252\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (3.11862564087,3.08072896), test loss: 2.93100495487\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (5.80675458908,29.9500392616), test loss: 27.766506815\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.443279206753,3.07262773898), test loss: 2.99376925528\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (9.96574401855,29.8019189887), test loss: 26.7625582218\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.31262278557,3.06465122633), test loss: 2.68141222447\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (9.1442899704,29.6548434341), test loss: 28.5536431313\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.75660157204,3.05667597338), test loss: 2.73188192248\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (8.21706581116,29.5076849208), test loss: 29.292545557\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.0654335022,3.04879653568), test loss: 3.00289482772\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (5.52977991104,29.3611601891), test loss: 28.5148109436\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.92766523361,3.0409170373), test loss: 2.91271633059\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (12.6239643097,29.2150165909), test loss: 25.6408644199\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.77756500244,3.03313724151), test loss: 2.78694358468\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (11.3031578064,29.0696322414), test loss: 29.498467207\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.24824166298,3.02537617571), test loss: 2.63185060024\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (7.44018316269,28.9246775477), test loss: 29.2744810581\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.59965014458,3.01765779328), test loss: 2.74602730125\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (6.46662139893,28.7802392835), test loss: 30.8121190548\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.22658491135,3.0099522136), test loss: 2.94562444985\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (1.74481284618,28.6363210252), test loss: 26.6674536705\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.13586318493,3.00229449377), test loss: 2.91275016069\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (6.82593345642,28.4930751751), test loss: 27.9095990658\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.49474787712,2.99470794293), test loss: 2.65444008112\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (8.02435112,28.3506510275), test loss: 28.3897844791\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.18808221817,2.98723160812), test loss: 2.87597104907\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (12.4480285645,28.2085859478), test loss: 29.2593690395\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.81220924854,2.97981208714), test loss: 3.01461026669\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (5.31822395325,28.0674079374), test loss: 28.2859148979\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.10627782345,2.97240789635), test loss: 2.94675291926\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (4.10150051117,27.9265643211), test loss: 27.116522646\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.665417313576,2.96497674325), test loss: 2.719418329\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (6.4732708931,27.7865913795), test loss: 29.7992387772\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.37721586227,2.95767392631), test loss: 2.66720232368\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (5.54696083069,27.6471928637), test loss: 29.4553150654\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.47551655769,2.95035035174), test loss: 2.9655057475\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.43180561066,27.508640001), test loss: 32.3270085335\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.69216537476,2.94312676828), test loss: 3.11921201646\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (6.09027338028,27.3705096534), test loss: 27.3234882832\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.23446059227,2.9358691115), test loss: 2.91280536205\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (6.5764503479,27.2334571513), test loss: 30.0690446854\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.89526438713,2.92871838508), test loss: 2.69895190448\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (15.8583526611,27.0970259394), test loss: 28.3417554855\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.28416633606,2.92157288625), test loss: 2.80429282784\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (2.491751194,26.9615648515), test loss: 31.9995069504\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.24158811569,2.91446733115), test loss: 2.89137205184\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.41490077972,26.8266162741), test loss: 29.4343931198\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.84725093842,2.90738046099), test loss: 2.9497067526\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (1.74847018719,26.6926837756), test loss: 30.205918169\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.804787397385,2.900324909), test loss: 2.64700770378\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 7\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (310.644287109,inf), test loss: 169.707364655\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (305.537139893,inf), test loss: 348.763275146\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (35.6574630737,66.6196966238), test loss: 34.8593440056\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.45186328888,64.0066725321), test loss: 3.25125348866\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (37.0701293945,55.2853533978), test loss: 38.9578117847\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.63839244843,33.4812413096), test loss: 3.05470965505\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (43.490196228,51.46689293), test loss: 39.068494606\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.84202969074,23.2967242241), test loss: 3.18148066103\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (45.701210022,49.5624826956), test loss: 38.0153600454\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.50478100777,18.2076980968), test loss: 3.24846693277\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (110.688217163,48.3764682829), test loss: 38.0924057961\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.79959726334,15.1533666924), test loss: 3.22175904512\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (41.8033370972,47.5744821466), test loss: 34.4283153057\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.14740538597,13.1132752113), test loss: 3.2844030112\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (44.6680755615,47.0053920458), test loss: 35.3459120274\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.41223049164,11.6570609473), test loss: 3.12035292089\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (19.5166912079,46.5399908608), test loss: 38.4164128304\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.49415874481,10.5637035392), test loss: 2.9060721755\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (28.3492546082,46.1893929082), test loss: 39.3816576958\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.20063400269,9.71349574373), test loss: 3.30900796652\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (39.1266784668,45.9085875265), test loss: 38.6605795145\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.953495204449,9.03344412906), test loss: 3.27088713646\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (28.2091522217,45.6537488126), test loss: 37.3234471321\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.14145457745,8.47627510033), test loss: 3.28800049424\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (22.3523483276,45.4528629127), test loss: 33.6098557949\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.881581187248,8.01217261194), test loss: 3.2543431282\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (113.355804443,45.2851330771), test loss: 36.4911293983\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.0072684288,7.61944816734), test loss: 3.03360981643\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (31.6048679352,45.1196770952), test loss: 39.6519332886\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.94841909409,7.28226033781), test loss: 3.00328209996\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (21.3888168335,44.9796914621), test loss: 38.6823203325\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (6.6822552681,6.98935166891), test loss: 3.29167843461\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (52.4545593262,44.855937084), test loss: 37.3796648026\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.98776292801,6.73270932132), test loss: 3.25864365101\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (55.6524200439,44.7329337717), test loss: 34.2032476425\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.62785387039,6.50645357972), test loss: 3.11635344625\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (49.4514007568,44.6134731994), test loss: 34.2227549076\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.71819019318,6.30507874872), test loss: 3.12564378381\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (42.6949996948,44.5127180185), test loss: 38.1685413361\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.14309668541,6.1247507612), test loss: 2.94322317243\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (36.7361526489,44.4178972014), test loss: 38.2521523476\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (9.16834163666,5.96243830508), test loss: 3.11287617981\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (15.2132425308,44.3289345602), test loss: 37.8989233732\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.51299381256,5.81478515938), test loss: 3.25901693106\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (31.1314716339,44.2454578513), test loss: 37.0663243294\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.18331599236,5.68122013035), test loss: 3.13959318399\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (27.3239784241,44.1535490744), test loss: 33.6811545372\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.85495853424,5.55867648154), test loss: 3.19296728075\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (41.4907226562,44.0749691043), test loss: 35.3438275337\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.17769193649,5.44657692788), test loss: 3.04885523915\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (111.006820679,43.9940442805), test loss: 37.0551965237\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (5.59220838547,5.34292851122), test loss: 2.84733197093\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (91.9298858643,43.9159005935), test loss: 38.2804168224\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.370262146,5.24721124235), test loss: 3.20999931991\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (28.2309951782,43.8341489342), test loss: 37.8597195148\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.91100955009,5.15878471007), test loss: 3.23938580751\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (79.5761871338,43.7599825804), test loss: 36.2604734898\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.85886669159,5.07680466104), test loss: 3.21373399198\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (57.2956809998,43.6830901832), test loss: 32.5351794243\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.644034028053,4.99986439102), test loss: 3.15528870523\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (30.595615387,43.6120238675), test loss: 35.8583256245\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.23345208168,4.92828950659), test loss: 2.92427612841\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (44.3068084717,43.536570121), test loss: 38.5540883064\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.41068673134,4.86116384922), test loss: 2.95726416707\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (52.5051651001,43.4653081678), test loss: 37.1774820328\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.01354074478,4.79800661505), test loss: 3.18491930962\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (24.6228370667,43.3926119814), test loss: 36.2334347248\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.00102877617,4.73820003783), test loss: 3.17230815887\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (46.548866272,43.3200254086), test loss: 32.8643863678\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.23560762405,4.68223495738), test loss: 3.10106029212\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (79.155708313,43.2427530518), test loss: 32.7190210819\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.32471561432,4.62936176313), test loss: 3.02200403214\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (32.1803894043,43.1646018947), test loss: 36.5605741501\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.44425237179,4.57916479525), test loss: 2.89875725806\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (31.122095108,43.0904837514), test loss: 37.7942241192\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.53044891357,4.53183547045), test loss: 3.0824303627\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (36.7098960876,43.0144058196), test loss: 37.0646377563\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.64957559109,4.48645409669), test loss: 3.22932052612\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (44.3355484009,42.9382061018), test loss: 35.4140693903\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.39367222786,4.44384029317), test loss: 3.13682883978\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (106.064109802,42.856525403), test loss: 31.8433357716\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.56196641922,4.40345079989), test loss: 3.11849512458\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (37.0793991089,42.7723216503), test loss: 33.5640178204\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.918110132217,4.36453896144), test loss: 2.97613480091\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (42.9465370178,42.688133324), test loss: 35.2270091057\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.7212395668,4.32766757954), test loss: 2.81363099813\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (14.5453968048,42.5965928871), test loss: 35.7844816685\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.39499902725,4.29230720636), test loss: 3.12784589529\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (40.0388641357,42.5062867241), test loss: 35.8174663305\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.27287185192,4.25863052472), test loss: 3.18254519403\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (36.4682159424,42.4138538344), test loss: 33.9193702936\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.948355257511,4.22652186061), test loss: 3.1255668968\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (35.9152030945,42.3155286132), test loss: 30.5127711773\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.77058172226,4.19558706465), test loss: 3.08224307299\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (16.6709403992,42.2166444024), test loss: 33.8089432716\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.07862448692,4.16605979399), test loss: 2.79949133098\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (80.0602874756,42.1171719309), test loss: 35.328682518\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.53650450706,4.13766050245), test loss: 2.9366314441\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (26.6067733765,42.0092476014), test loss: 34.4682898521\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.28210449219,4.11032291895), test loss: 3.09118443429\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (14.5106868744,41.9000593262), test loss: 33.5190934658\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (6.95620346069,4.08384520457), test loss: 3.10291668177\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (37.0609054565,41.7796267991), test loss: 29.7334250689\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.86256217957,4.05830946248), test loss: 3.02877991199\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (44.6766738892,41.6506905599), test loss: 29.0099894762\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.00884628296,4.03381438501), test loss: 2.94428336024\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (42.6301307678,41.5140746523), test loss: 31.8034924507\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.73734760284,4.01011004863), test loss: 2.82878876925\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (26.2783622742,41.374176347), test loss: 33.2255098343\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.87424707413,3.98721472101), test loss: 3.03878887892\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (24.0914001465,41.2302366199), test loss: 32.4305718422\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (8.8945684433,3.9651904533), test loss: 3.18544402421\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (12.5476570129,41.0813900499), test loss: 31.2963648319\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.22817659378,3.94353943926), test loss: 3.09144534171\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (18.8436698914,40.9278512435), test loss: 27.2861576319\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.45794737339,3.92291523307), test loss: 2.96076652706\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (18.6687355042,40.766597317), test loss: 28.64602561\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.37600564957,3.90273898017), test loss: 2.86701148748\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (24.9829330444,40.6027119217), test loss: 29.8316607952\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.68709135056,3.88330173174), test loss: 2.75198425651\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (81.5404434204,40.4338903456), test loss: 30.1944227219\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.46643924713,3.86418464785), test loss: 3.06670745313\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (67.3038635254,40.2609929925), test loss: 30.9178486824\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.55957555771,3.84563684602), test loss: 3.06125276685\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (19.5460987091,40.0837383936), test loss: 28.8408739805\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.04235577583,3.82759518138), test loss: 3.03996524215\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (33.3852615356,39.9056231749), test loss: 25.4604334116\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.13298726082,3.81003157658), test loss: 3.04055887461\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (36.4698524475,39.723784567), test loss: 27.379420042\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.984458684921,3.79265884995), test loss: 2.6966699928\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (15.4355020523,39.5408680107), test loss: 28.5025889397\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.58140778542,3.77575085742), test loss: 2.87268490791\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (21.6174163818,39.352060394), test loss: 29.0937948704\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.74343848228,3.75911041749), test loss: 3.02834792733\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (45.6011543274,39.1643620447), test loss: 28.7556819916\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (4.05859375,3.74267517439), test loss: 3.02148723602\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (9.90804767609,38.9750242125), test loss: 26.4640179276\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.13538265228,3.72628837554), test loss: 2.97722009122\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (27.7468547821,38.7856462083), test loss: 25.4435268164\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.90400886536,3.71039753718), test loss: 2.89504452646\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (39.3173866272,38.5960094513), test loss: 26.1184371948\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.68998384476,3.69471433834), test loss: 2.78232832551\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (18.4728431702,38.4048651016), test loss: 27.9127954006\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.26401019096,3.6791425122), test loss: 3.01131954491\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (11.8732385635,38.2158841572), test loss: 28.2050508022\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.4439239502,3.66390253327), test loss: 3.15854541361\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (15.6715230942,38.0276324944), test loss: 27.8198503971\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.30929803848,3.64866238478), test loss: 3.08683321923\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.8516082764,37.8387463202), test loss: 24.6449982405\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.22620820999,3.63379968487), test loss: 2.92862187326\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (43.7073059082,37.650562527), test loss: 25.3136376143\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.86034822464,3.61919536639), test loss: 2.86333728731\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (36.56590271,37.4611510204), test loss: 26.1207667828\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.15913796425,3.60460145637), test loss: 2.77357080132\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (31.9873123169,37.2731738944), test loss: 26.5523622513\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (4.11290168762,3.59024348241), test loss: 3.06510709822\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (9.19274425507,37.0856595122), test loss: 28.482760334\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (4.22518730164,3.5759936529), test loss: 3.09999032915\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (35.3905410767,36.8989924479), test loss: 26.0344844341\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.31498146057,3.5619854328), test loss: 3.0054068014\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (17.5758724213,36.7148621484), test loss: 23.5169305801\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.805414021015,3.54819243222), test loss: 3.01235934496\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (22.1788635254,36.5306981468), test loss: 25.0937178731\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.7792185545,3.53451976257), test loss: 2.7144593522\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.3645839691,36.347714703), test loss: 25.4417922974\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.96628594398,3.52108217722), test loss: 2.88974786997\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (46.1722602844,36.1667675949), test loss: 26.152878952\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.1646528244,3.50774870747), test loss: 3.00889018178\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (6.51569032669,35.984538114), test loss: 26.4119059563\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.84061789513,3.49455572455), test loss: 3.06586030722\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (9.5991859436,35.8051928565), test loss: 23.9510732651\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.67074251175,3.4814385517), test loss: 3.09061781764\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (7.55655431747,35.626945533), test loss: 23.6829311371\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.40425205231,3.46850529178), test loss: 2.9146931082\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (19.067024231,35.4501341458), test loss: 24.1447653532\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (4.3991394043,3.45580880572), test loss: 2.81625056267\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (21.3468017578,35.2737110443), test loss: 25.1574187279\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.76963615417,3.44321158507), test loss: 3.00189900547\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (13.2778768539,35.098262836), test loss: 25.5620574951\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.70345044136,3.43077328), test loss: 3.12844201326\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (15.8674030304,34.9251718167), test loss: 25.4356860638\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (7.26784420013,3.41853936143), test loss: 3.13252765834\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (17.1006011963,34.7523164898), test loss: 23.1745520115\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.19882786274,3.40630278385), test loss: 2.98994388282\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (15.1069049835,34.5806438734), test loss: 23.5194693565\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.12336850166,3.39436850547), test loss: 2.81406139582\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (10.3428821564,34.4094538839), test loss: 24.6249222636\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.35754609108,3.38250840405), test loss: 2.78592718691\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (10.3203125,34.2389572125), test loss: 24.8883230209\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.43631398678,3.37085539619), test loss: 3.01899481416\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (36.1187934875,34.0704179137), test loss: 27.0235556602\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.03127861023,3.35919476359), test loss: 3.13645332456\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (44.829914093,33.9022357321), test loss: 24.2907096386\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.66925096512,3.3477653717), test loss: 2.97349890471\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (12.3115282059,33.735340361), test loss: 22.2185196161\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.50897574425,3.33649236485), test loss: 2.9848931551\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (6.53019571304,33.5702055865), test loss: 24.0145750761\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.16078567505,3.3254038971), test loss: 2.74396340549\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (18.4974536896,33.4055295529), test loss: 24.0068236113\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.04980683327,3.31438564716), test loss: 2.92278888077\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (4.89741134644,33.2426075819), test loss: 24.3503890991\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.10874414444,3.30352428859), test loss: 2.94731461704\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (13.6158056259,33.0793125382), test loss: 25.576611805\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.16727399826,3.29277837814), test loss: 3.10387021899\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (34.0221252441,32.9185932562), test loss: 23.0376468658\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (3.5785036087,3.28209809814), test loss: 3.09878424406\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (8.06983757019,32.7577224123), test loss: 22.5833593369\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.10886323452,3.27146758399), test loss: 2.9039206624\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (14.4339427948,32.5982168703), test loss: 23.8365961552\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.46000885963,3.26109442416), test loss: 2.78810724616\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (11.9450111389,32.43984194), test loss: 23.6733103991\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.7960896492,3.25078363223), test loss: 2.98716690987\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (13.8918361664,32.281303241), test loss: 24.6425479412\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.72800552845,3.24056672401), test loss: 3.08149668276\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (4.43877410889,32.1245781599), test loss: 24.5198583126\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.82590866089,3.2304970761), test loss: 3.12838862836\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (15.1365804672,31.9684663032), test loss: 22.4597918034\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.38490700722,3.22044354068), test loss: 2.99203552604\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (19.7665634155,31.8125921647), test loss: 22.6427964687\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.47376060486,3.21058571607), test loss: 2.69644594789\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (17.9600200653,31.6578725079), test loss: 24.2967944622\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.71662950516,3.20086034581), test loss: 2.76526017487\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (42.3343811035,31.5029746234), test loss: 23.6327132463\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.77163624763,3.19115478227), test loss: 2.96041610688\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (19.0277404785,31.3492348236), test loss: 26.3580350876\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (3.46844911575,3.18155622872), test loss: 3.1652882874\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (8.8393201828,31.1959025843), test loss: 24.5370735645\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.73017692566,3.17203547059), test loss: 2.94936971664\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (21.8521118164,31.0433560522), test loss: 21.9245749474\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.25457286835,3.16263504809), test loss: 2.91496593356\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (5.92441654205,30.8923259611), test loss: 23.9351706028\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.513063371181,3.15336530817), test loss: 2.75075894892\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (14.5066127777,30.7413169332), test loss: 24.2088776588\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.38531839848,3.14418583394), test loss: 2.87839221358\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (7.56638908386,30.5914811899), test loss: 23.5844150066\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.31209135056,3.13510869194), test loss: 2.93164375722\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (22.9269218445,30.4420741038), test loss: 25.4334932327\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.24380016327,3.12608744857), test loss: 3.1164521724\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (4.37579250336,30.2927355892), test loss: 22.3220492363\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.50056362152,3.11714766146), test loss: 3.06188339293\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (9.07157707214,30.1446246879), test loss: 22.0497210264\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.75927352905,3.10823067382), test loss: 2.89124422073\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (4.83511447906,29.9968985391), test loss: 23.9182788372\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.96559035778,3.09942387782), test loss: 2.7595831573\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (9.53041648865,29.8502753917), test loss: 23.8139823198\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (3.70726633072,3.09072295678), test loss: 2.99692693353\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (14.584151268,29.7038569976), test loss: 24.7394558907\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.92395877838,3.08208622908), test loss: 3.05826994777\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (6.94948863983,29.5579285957), test loss: 24.3148865223\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.84476697445,3.07351468518), test loss: 3.0888692081\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (10.762263298,29.4130789632), test loss: 22.2020973682\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (4.93178987503,3.06504235149), test loss: 2.99868257344\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (13.6871137619,29.2682226088), test loss: 22.9887844086\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.03154790401,3.05656057048), test loss: 2.70681315511\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (11.9169940948,29.1241745658), test loss: 24.5906520367\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.03540468216,3.04823459911), test loss: 2.74471972585\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.47169494629,28.9803130809), test loss: 23.7640715361\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.39101421833,3.03993998449), test loss: 2.92737737894\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (5.14971065521,28.8371131378), test loss: 26.6688614845\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.28971529007,3.03175507649), test loss: 3.16201049685\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (20.0313148499,28.6947864588), test loss: 24.2764575243\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.0887567997,3.02356261102), test loss: 2.94853586257\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (17.824552536,28.552718856), test loss: 22.4727592945\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.33644080162,3.01547638848), test loss: 2.88494781554\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (5.82678127289,28.4115091924), test loss: 24.441606617\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.854548215866,3.00745318872), test loss: 2.75535203665\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (3.81724357605,28.2710156897), test loss: 24.6730669022\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.783022940159,2.99954653344), test loss: 2.87307866961\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.76024246216,28.1311310052), test loss: 24.4497464418\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.14532542229,2.991663695), test loss: 2.97836423963\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (4.00889778137,27.9919347044), test loss: 25.4690368414\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.00699007511,2.98385108133), test loss: 3.06867445111\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.49355888367,27.8529793985), test loss: 22.95823102\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.93755483627,2.97610672717), test loss: 3.04779102206\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (13.4938869476,27.7150890005), test loss: 22.8414691925\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.99171948433,2.96835401453), test loss: 2.86579688787\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (8.06634712219,27.5775259638), test loss: 25.1707970142\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.09853482246,2.96064241031), test loss: 2.76393238753\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (5.16825914383,27.4407878544), test loss: 25.2425444126\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.0023150444,2.95305306635), test loss: 2.93514859825\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (4.54847335815,27.304784603), test loss: 26.1166033745\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.61933493614,2.9454934167), test loss: 3.08706057221\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (9.81370639801,27.1691251879), test loss: 25.4927760601\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.59729921818,2.9379824968), test loss: 3.12844365835\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.24861907959,27.0342996833), test loss: 23.0747964621\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.69704318047,2.93052889269), test loss: 2.99629080892\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (11.8277244568,26.9001436909), test loss: 23.6562472343\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.31480169296,2.92307738298), test loss: 2.70716154873\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (11.3864574432,26.7664698005), test loss: 25.5782943726\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.42049050331,2.91573453271), test loss: 2.76665011793\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (8.04158496857,26.6336259717), test loss: 24.7813766003\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.62707138062,2.90846105467), test loss: 2.90814869106\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (12.2080068588,26.5011218488), test loss: 26.5424752712\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.98489463329,2.90117825804), test loss: 3.10917831063\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (9.75831794739,26.3695783196), test loss: 24.9982929707\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.78600692749,2.89394293613), test loss: 3.03786635846\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (4.76786422729,26.2385813681), test loss: 23.9251550198\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.95651137829,2.88675807113), test loss: 2.91529506445\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (14.2595443726,26.1084695621), test loss: 25.5510075092\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.19595825672,2.87962543588), test loss: 2.79448794127\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (1.4480240345,25.9792013769), test loss: 25.8428105354\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.518396377563,2.87256112775), test loss: 2.89635767937\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (6.17479515076,25.8505868667), test loss: 26.2651893139\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.839514255524,2.86555912974), test loss: 3.05023221672\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (3.15192914009,25.722752851), test loss: 27.0786538124\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.39127016068,2.85860000027), test loss: 3.10291378498\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (7.63356113434,25.5956346646), test loss: 24.3611436844\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.598146796227,2.85167536494), test loss: 3.08449730277\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (3.20879673958,25.4691407405), test loss: 24.0495975494\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.728946805,2.844797091), test loss: 2.84719886482\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.27517604828,25.3436285889), test loss: 26.9516944885\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.50961363316,2.83792020896), test loss: 2.81011891812\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (3.07137155533,25.218753873), test loss: 25.2196796894\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.73616087437,2.83109630025), test loss: 2.93021028042\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (4.10978507996,25.0947636443), test loss: 27.843599534\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.45407009125,2.82433559424), test loss: 3.12831335813\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.74360847473,24.9715409456), test loss: 27.4085614681\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.57582950592,2.81762081397), test loss: 3.19816558957\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (3.74418497086,24.8489358531), test loss: 24.192699194\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.16513180733,2.81092908145), test loss: 2.9973543793\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 8\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (316.528442383,inf), test loss: 158.281629944\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (291.886962891,inf), test loss: 352.973556519\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (75.9398117065,67.7446251488), test loss: 32.2141263008\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.32482409477,64.4371035641), test loss: 3.22622964233\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (49.6438865662,56.5438309965), test loss: 35.6725967884\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.81486535072,33.7697522943), test loss: 3.00683659613\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (31.0694389343,52.7554606775), test loss: 35.6212047577\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.97391939163,23.542151191), test loss: 3.24841396213\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (49.3527297974,50.8482742076), test loss: 35.5045919895\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.32325029373,18.4286748521), test loss: 3.30939902663\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (55.9980621338,49.7141258322), test loss: 30.99407897\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.28195381165,15.358566603), test loss: 3.30392034054\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (49.5300788879,48.9387936096), test loss: 34.1704485893\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.99408507347,13.3096964683), test loss: 3.0760553509\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (20.5468902588,48.3882393919), test loss: 35.6734675884\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.54926419258,11.8457038028), test loss: 3.08542110622\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (24.1156978607,47.9463115972), test loss: 36.4676717043\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.78325605392,10.7478463537), test loss: 3.347900635\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (58.364982605,47.6022825709), test loss: 34.8336027145\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.67285680771,9.89270525008), test loss: 3.29550526738\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (68.1690368652,47.3235405209), test loss: 31.0256632328\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (7.00963783264,9.20891355374), test loss: 3.26972237229\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (98.2173309326,47.0986354141), test loss: 35.1584898949\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.11708116531,8.64804375776), test loss: 2.98344686031\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (20.2366600037,46.887594388), test loss: 35.748362112\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.42212629318,8.18074359114), test loss: 3.18370253444\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (32.2751617432,46.7076330751), test loss: 35.9356450558\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.94385004044,7.7851499989), test loss: 3.26790385246\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (21.6903305054,46.5529216059), test loss: 31.8181018353\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.30439281464,7.44628507266), test loss: 3.1459228158\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (93.6547088623,46.4206350965), test loss: 31.6163734674\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.6206741333,7.15287958697), test loss: 3.04759316146\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (21.0797119141,46.2913101551), test loss: 34.9026976109\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.3271112442,6.89637046977), test loss: 2.86167264581\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (45.8444747925,46.1722450495), test loss: 35.0338866711\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.73504161835,6.66936115897), test loss: 3.14144372344\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (30.4076690674,46.0581285446), test loss: 34.9418982029\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.01099872589,6.46739114075), test loss: 3.15806543231\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (68.7244644165,45.964977272), test loss: 30.3647976875\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.71861255169,6.28706409343), test loss: 3.17643377781\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (41.5752258301,45.864804528), test loss: 33.3405394554\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.34415745735,6.12395758043), test loss: 2.96467538178\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (29.3212966919,45.7669827632), test loss: 34.9556388617\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.79577600956,5.97643112847), test loss: 3.0141620189\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (53.9069824219,45.6771418715), test loss: 35.7614115238\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (7.82712459564,5.84286493849), test loss: 3.30405933261\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (14.0897378922,45.5888611818), test loss: 34.128945446\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.18930745125,5.72045183554), test loss: 3.22486145794\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (16.7033863068,45.5029425842), test loss: 30.3721910954\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.74033665657,5.60807653624), test loss: 3.19474785477\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (49.8671455383,45.4224556377), test loss: 34.3968755722\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (5.07676792145,5.50469246662), test loss: 2.95283173621\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (95.6704559326,45.3486936468), test loss: 35.0275821209\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (5.01662921906,5.40924127757), test loss: 3.15001024008\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (62.2035865784,45.272943965), test loss: 35.3566174984\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.19212198257,5.32068264265), test loss: 3.25854213834\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (43.8287086487,45.1972876275), test loss: 31.035193634\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.1761701107,5.23827753022), test loss: 3.13045414984\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (32.5537452698,45.119463959), test loss: 30.9439106464\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.41984367371,5.16152302028), test loss: 3.04475040138\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (53.9008712769,45.0442768356), test loss: 34.1413002253\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.64779806137,5.09024735552), test loss: 2.85840755403\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (71.6042251587,44.9709938194), test loss: 34.1770953655\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (4.68651008606,5.02329649808), test loss: 3.11373389959\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (100.411254883,44.8991016303), test loss: 34.0118404865\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.84857773781,4.96035235059), test loss: 3.12114703059\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (20.4343395233,44.8229604788), test loss: 29.3532334089\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.17299699783,4.90118460629), test loss: 3.07349621654\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (6.51678180695,44.744734806), test loss: 32.3122395515\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.60756540298,4.84568645534), test loss: 2.89708889127\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (62.7559547424,44.6687240961), test loss: 34.0271765232\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.20568132401,4.79304759122), test loss: 2.95622828305\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (60.913936615,44.5920267909), test loss: 34.6977780342\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (6.40223407745,4.74353077667), test loss: 3.2287773639\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (42.6646194458,44.5155388626), test loss: 32.9393498421\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.261064052582,4.69627985766), test loss: 3.0988037765\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (23.8581008911,44.4335684391), test loss: 29.1080149889\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.79825234413,4.6516160518), test loss: 3.06219751835\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (35.615070343,44.3508224905), test loss: 32.6684903622\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.93090343475,4.60923178732), test loss: 2.86171175539\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (16.1593723297,44.2680886254), test loss: 33.6261189461\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.43225669861,4.56902077295), test loss: 3.08209040165\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (73.8079528809,44.1871198403), test loss: 33.9549504757\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (4.2171254158,4.53097231898), test loss: 3.11396677494\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (32.8340034485,44.0997574237), test loss: 29.6007539272\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (5.11061096191,4.49488670636), test loss: 2.98520932198\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (40.7071990967,44.0097950657), test loss: 29.2537369728\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.50670993328,4.45999243917), test loss: 2.90778199732\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (24.8416080475,43.9164305597), test loss: 32.1145409346\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (5.83221578598,4.42691781609), test loss: 2.74474320412\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (40.3665237427,43.8262354154), test loss: 32.70831604\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.01884841919,4.39515151401), test loss: 3.05929876268\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (29.3265151978,43.7275492524), test loss: 32.3034150124\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.30660903454,4.36465558048), test loss: 3.0237759769\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (27.3609657288,43.6251036588), test loss: 27.5418325663\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.55122113228,4.33558433982), test loss: 2.95738252401\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (29.7871208191,43.517862469), test loss: 29.7089394569\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (9.43873786926,4.30814565804), test loss: 2.80235161483\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (13.2623653412,43.3974613082), test loss: 31.5876514912\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.94627523422,4.28125676639), test loss: 2.90477336645\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (25.5145740509,43.2716541421), test loss: 32.4899576187\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.41894125938,4.25530282422), test loss: 3.17225476503\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (49.3898277283,43.1418400847), test loss: 29.9018337727\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (5.95400524139,4.23044887588), test loss: 3.02118294537\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (56.4900665283,43.0092719338), test loss: 26.300175333\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.59965109825,4.20647991648), test loss: 2.96482864916\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (36.4014663696,42.8701814643), test loss: 29.1054120064\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.45630931854,4.18336238822), test loss: 2.81486847699\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (29.6339073181,42.7254298572), test loss: 30.6133847237\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.37742090225,4.16101287438), test loss: 3.05701285601\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (25.2634391785,42.5756350778), test loss: 31.0077990055\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (6.34357070923,4.13947209816), test loss: 3.12343376875\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (51.673866272,42.4223316881), test loss: 26.52413764\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.99524211884,4.11865752213), test loss: 2.96772973537\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (39.2422218323,42.2644961607), test loss: 25.9946880102\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.57740116119,4.09850370817), test loss: 2.83215815425\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (102.854232788,42.1043518492), test loss: 28.3464977741\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.32776284218,4.07878669655), test loss: 2.68715782464\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (24.4495868683,41.9373762658), test loss: 29.7417965651\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (4.74469280243,4.05971096851), test loss: 3.03088210523\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (3.67639446259,41.7657966741), test loss: 29.2720306158\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.11218500137,4.04111518673), test loss: 3.00326490402\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (34.4589004517,41.5913376731), test loss: 24.847178793\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.71900653839,4.02281716214), test loss: 2.89556752145\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (42.6637306213,41.413609021), test loss: 26.2078399897\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (5.02591514587,4.00511764103), test loss: 2.73042050004\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (29.7656078339,41.2310833393), test loss: 28.4329030752\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.1484452486,3.98758645365), test loss: 2.87354442775\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (10.5314569473,41.0442394835), test loss: 30.0966173172\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.78070819378,3.97037448475), test loss: 3.1089850843\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (59.5419998169,40.8576370007), test loss: 27.4170913696\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.40005397797,3.9534286002), test loss: 2.9120297879\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.0894641876,40.6688566471), test loss: 24.4421097279\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.76587533951,3.93684447029), test loss: 2.85017095804\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (30.8490028381,40.481364688), test loss: 26.1730617046\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.79188704491,3.92052463115), test loss: 2.71784737706\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (46.7052612305,40.2924499858), test loss: 28.2676249981\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (5.14485692978,3.90462041722), test loss: 2.98532554507\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (48.5033149719,40.1029611654), test loss: 29.537915659\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.45370388031,3.88864480856), test loss: 3.02936527133\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (55.8447418213,39.9134625175), test loss: 25.4583972454\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (5.7142791748,3.87297824443), test loss: 2.93052283525\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.1964559555,39.7238630651), test loss: 24.8096166611\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.923961520195,3.85739905432), test loss: 2.78319159746\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (17.2249298096,39.5337095255), test loss: 26.8179737329\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.73616623878,3.84206504264), test loss: 2.66935084909\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.1251182556,39.3439882915), test loss: 28.0874460697\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.42714619637,3.82680995111), test loss: 3.03252391815\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (22.5837974548,39.1550792808), test loss: 28.1309562683\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (5.28669929504,3.8120487679), test loss: 3.04156178832\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (20.6248874664,38.9669537236), test loss: 24.1839347601\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.07897043228,3.79724988924), test loss: 2.91889187992\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (26.5652484894,38.7795505324), test loss: 24.8574625254\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.22214186192,3.782586753), test loss: 2.71684010923\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (61.8618621826,38.5941726468), test loss: 26.8426046848\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.47157382965,3.76822832879), test loss: 2.89757030606\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (25.0532169342,38.4090010925), test loss: 28.2548681736\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.10434269905,3.75394536272), test loss: 3.1459402144\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (13.4137687683,38.2240174404), test loss: 25.7862126589\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.36729454994,3.73987823728), test loss: 2.99404655397\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (15.7410678864,38.0401252865), test loss: 23.5046797514\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.79339134693,3.72591296229), test loss: 2.88770067692\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (7.01072216034,37.8567525604), test loss: 25.1251249909\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (5.71051311493,3.71218760939), test loss: 2.79059050977\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (49.6512451172,37.6757280611), test loss: 26.1622836828\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (4.07858467102,3.69860168637), test loss: 3.01437323391\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (10.735408783,37.4941273367), test loss: 27.8384019852\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.80134868622,3.68522204154), test loss: 3.10027238131\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (39.2982215881,37.3145989176), test loss: 24.1149355888\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.48615765572,3.67191574083), test loss: 3.02699692249\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (28.7486953735,37.1351067701), test loss: 23.5894899607\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.91809988022,3.65883324916), test loss: 2.82751343548\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (11.6446838379,36.9562287754), test loss: 25.7124573708\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.3535399437,3.64588311593), test loss: 2.74109823704\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (27.9969787598,36.7789343371), test loss: 26.402355814\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.50718212128,3.63302144484), test loss: 3.07389819622\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.207567215,36.6023643943), test loss: 26.3093231678\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.3786740303,3.62037967785), test loss: 3.14013611078\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (7.46843338013,36.4269092706), test loss: 23.3213869095\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.12525582314,3.60781893901), test loss: 3.00425615013\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (6.51797580719,36.2516239573), test loss: 24.0021131158\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.25392985344,3.59542244146), test loss: 2.76369778365\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (54.5676956177,36.078721302), test loss: 25.3409794807\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.40355682373,3.58320630531), test loss: 2.93114184737\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (14.3263339996,35.9056975912), test loss: 26.8163615227\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.89959287643,3.57117067893), test loss: 3.16416676939\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (24.2858695984,35.7346617459), test loss: 25.2559545279\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.11756467819,3.55934965713), test loss: 3.0316911459\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (24.8490161896,35.5638022643), test loss: 23.5207643747\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.25197029114,3.54770611795), test loss: 2.92949084044\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (43.3788833618,35.3943411463), test loss: 24.3491087914\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.76652359962,3.53608244875), test loss: 2.80354978591\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (36.4983062744,35.2251568403), test loss: 25.0576382637\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (3.57816958427,3.5246443007), test loss: 3.00701617002\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (21.187921524,35.0566371687), test loss: 26.97745924\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.02935194969,3.51331722775), test loss: 3.11481858492\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (14.0059185028,34.8887698307), test loss: 23.7590388536\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.16818547249,3.50217635055), test loss: 3.03352344781\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (16.25390625,34.7212776084), test loss: 23.4444861293\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.62987685204,3.49112392924), test loss: 2.84625866711\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (23.4376220703,34.5549310983), test loss: 25.0485132217\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (3.40248131752,3.48034648794), test loss: 2.77633256912\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (12.9898328781,34.388810427), test loss: 25.5823004246\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.38306427002,3.46959820005), test loss: 3.08977505267\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (13.9131803513,34.2237486785), test loss: 25.9502993107\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.7452917099,3.45894590462), test loss: 3.16375432312\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (46.7634620667,34.0598993297), test loss: 22.8097749472\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.11285829544,3.44847291809), test loss: 3.03444726765\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.2730941772,33.8959240725), test loss: 23.6187659264\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.3247590065,3.43808177321), test loss: 2.79987268746\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (14.269867897,33.7326294613), test loss: 25.3558167458\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.30545771122,3.42778683936), test loss: 2.93408297002\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (9.99007797241,33.5697460968), test loss: 26.2084492683\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.20832443237,3.41755333134), test loss: 3.17792961895\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.2733001709,33.407474783), test loss: 24.1273569345\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (4.07900381088,3.40748569099), test loss: 3.0635920167\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (12.0764837265,33.2464238133), test loss: 22.4599470615\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.88707703352,3.39748482145), test loss: 2.94380348921\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (11.7290019989,33.0853405915), test loss: 24.3117386818\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.99866235256,3.38762910999), test loss: 2.81225043833\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (17.3133068085,32.9254140016), test loss: 24.4731496334\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.16803359985,3.37779531549), test loss: 2.9943362534\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (18.80509758,32.7655546269), test loss: 26.2013951302\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.46022129059,3.3680943278), test loss: 3.1217341274\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (12.0891036987,32.6058003367), test loss: 23.098972559\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.70482385159,3.35844968727), test loss: 3.02314715087\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (10.3449726105,32.4469662722), test loss: 22.7181873798\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.839197993279,3.34886210191), test loss: 2.82070812285\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (12.95008564,32.2884972356), test loss: 24.9383119583\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.855343878269,3.33939941786), test loss: 2.7909395963\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (3.25154304504,32.1304374362), test loss: 25.4056452751\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.26695346832,3.32997920525), test loss: 3.10127626061\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (7.73828029633,31.9727380911), test loss: 25.7655465126\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.3387196064,3.32065423883), test loss: 3.18353572786\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (34.4204788208,31.816013573), test loss: 23.2888032913\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.8977098465,3.31142629457), test loss: 3.04568943977\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (17.0982780457,31.6594660247), test loss: 23.7576803684\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.3032977581,3.30231464811), test loss: 2.78029116392\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (15.3992576599,31.503464369), test loss: 24.9029084444\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (6.20508670807,3.29333868152), test loss: 2.93059411049\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.8588809967,31.3478598178), test loss: 26.7249044895\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.26892018318,3.28437801594), test loss: 3.18123809397\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.5659885406,31.1928831648), test loss: 24.2060503006\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.65370178223,3.27544898915), test loss: 3.04214051962\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (26.6674385071,31.0381848322), test loss: 22.6133099079\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.25639939308,3.26663005155), test loss: 2.92976427674\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (8.6673412323,30.883829765), test loss: 24.1921485901\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.67502403259,3.25782962447), test loss: 2.7987280041\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (13.9340181351,30.7298634351), test loss: 24.4755152702\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.78879737854,3.24911293235), test loss: 2.95393110514\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (10.2885684967,30.5763187653), test loss: 27.0841868401\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.64701557159,3.24044755142), test loss: 3.13572028875\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (11.8277521133,30.4233184844), test loss: 23.4287038803\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.41086316109,3.23195610974), test loss: 3.04958225191\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.7496380806,30.2705835224), test loss: 22.7282147884\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.22208368778,3.22343436645), test loss: 2.82208266854\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (9.99968147278,30.1186428934), test loss: 24.6135344028\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.06173944473,3.21498149238), test loss: 2.79837082922\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (16.4729690552,29.967235502), test loss: 25.9028231621\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.64257752895,3.20661449259), test loss: 3.11192030311\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (7.50748062134,29.8161578996), test loss: 26.4925492764\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.29028749466,3.19829871527), test loss: 3.25447725952\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (4.78423500061,29.6655061376), test loss: 23.6724842072\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.898980975151,3.19000751518), test loss: 3.08542311192\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (7.98659276962,29.5154182566), test loss: 24.5495449543\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.02919399738,3.18177388292), test loss: 2.81143041849\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (7.92405557632,29.3660574835), test loss: 27.9822193146\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.81524372101,3.17363536601), test loss: 2.92376255393\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (3.00470471382,29.2172386834), test loss: 27.3903713703\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.13156056404,3.16553756462), test loss: 3.17936048508\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (8.45400810242,29.0689383187), test loss: 25.0511677265\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.1679213047,3.1575037889), test loss: 3.06650155783\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.35513305664,28.9215252911), test loss: 23.4127925396\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.43385207653,3.14947474275), test loss: 2.96759954095\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.56687307358,28.7744745809), test loss: 26.6426657677\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.33068454266,3.14153728104), test loss: 2.7940823257\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (9.18420124054,28.6280400022), test loss: 25.4137165546\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.29682552814,3.13361992618), test loss: 2.93748640418\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (5.65639019012,28.4822354236), test loss: 27.3988892555\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.98866546154,3.12574007937), test loss: 3.12183023989\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (15.3080224991,28.3372764587), test loss: 24.7955786467\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.56988608837,3.11793096327), test loss: 3.13107636273\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (3.57985258102,28.1927912818), test loss: 23.7669096947\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.937213003635,3.11014371536), test loss: 2.86791140288\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (9.44563293457,28.0490030026), test loss: 26.281987524\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.44632816315,3.10241273865), test loss: 2.80435696393\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (7.69508075714,27.9061608454), test loss: 27.4004838943\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.06308746338,3.09473523965), test loss: 3.13966149092\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (20.0761566162,27.7639969458), test loss: 28.3916998386\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.09825706482,3.08714932892), test loss: 3.27643032968\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (10.3522434235,27.6225389305), test loss: 25.2499715328\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (5.90910720825,3.07963339386), test loss: 3.15505030155\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.57606887817,27.4817700538), test loss: 25.7872814178\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.58935379982,3.07212427994), test loss: 2.82954310179\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (5.51654911041,27.3418343814), test loss: 27.3068897247\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.26641654968,3.06462712476), test loss: 2.9399887085\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (10.8611412048,27.2028625302), test loss: 28.9874662876\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.87459850311,3.05721548932), test loss: 3.20195672065\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.93626356125,27.0644085269), test loss: 26.8143153191\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.47668242455,3.04979939302), test loss: 3.08873209357\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.57245922089,26.9268560097), test loss: 25.391105032\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.686484575272,3.04243445854), test loss: 2.99413475394\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.39209318161,26.7901341452), test loss: 26.6270124912\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.66889429092,3.03512131025), test loss: 2.86161264479\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (9.10569667816,26.6542230305), test loss: 26.3630634308\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.75500500202,3.02791392181), test loss: 2.96319859326\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.24541091919,26.5192069025), test loss: 29.1379038811\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.3698720932,3.02069437105), test loss: 3.16210362911\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.00695800781,26.3849779563), test loss: 26.3963933468\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.44388961792,3.01351115328), test loss: 3.17463796139\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (7.82609891891,26.2517703031), test loss: 25.1899525166\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.28583562374,3.00637782877), test loss: 2.91992353946\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (3.83363199234,26.1192702365), test loss: 27.8711134911\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.98370170593,2.99928949349), test loss: 2.85169727951\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.85606837273,25.9876608005), test loss: 30.131966114\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.20828831196,2.9922241786), test loss: 3.1786108911\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.68908119202,25.8570267325), test loss: 29.9272851944\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.053160429,2.98519585553), test loss: 3.34059830159\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (4.16082906723,25.7272595985), test loss: 27.7239808559\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.783435761929,2.97823958663), test loss: 3.1789850533\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 10\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (324.557739258,inf), test loss: 169.223677444\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (289.424072266,inf), test loss: 352.625187683\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (59.6397361755,64.9566374669), test loss: 36.210678196\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.74096918106,64.3244948833), test loss: 3.26269572377\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (28.8063182831,53.4280231068), test loss: 40.9859117508\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.2593588829,33.6865456705), test loss: 2.96867474616\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (24.0292472839,49.5354220478), test loss: 40.1914798737\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.44543743134,23.4741781929), test loss: 3.1904307127\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (45.7483596802,47.6005665375), test loss: 39.1303952217\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.99488258362,18.3628578936), test loss: 3.1838814199\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (84.5512695312,46.4399532254), test loss: 36.2116882801\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.55309391022,15.2935734729), test loss: 3.24148384333\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (26.1173858643,45.6453500679), test loss: 38.959383297\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.85179543495,13.2460006581), test loss: 3.05157847703\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (11.2685041428,45.0506875768), test loss: 41.9156460762\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.68304872513,11.7836417952), test loss: 2.98565925956\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (21.1081161499,44.6077299377), test loss: 39.4709087849\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.962922513485,10.6846406077), test loss: 3.25146311522\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (78.4491271973,44.2764398989), test loss: 37.0072909355\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.35281074047,9.83062268144), test loss: 3.16822588444\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (21.114315033,43.9815005512), test loss: 36.3005885124\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.49666023254,9.14620942314), test loss: 3.28560828865\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (46.8204154968,43.744160993), test loss: 40.1518683434\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.79615664482,8.58635158659), test loss: 2.96940034926\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (18.4370880127,43.5359511399), test loss: 39.8652100563\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.67667198181,8.11973424715), test loss: 3.09311586022\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (50.2486114502,43.3655326317), test loss: 38.1144267082\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (8.64782810211,7.72648561529), test loss: 3.21649398208\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (36.3945846558,43.1987238121), test loss: 36.1485468388\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.55043303967,7.38786983207), test loss: 3.24748672247\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (27.374835968,43.0472024593), test loss: 37.4414944649\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (6.47968101501,7.09460651995), test loss: 3.01944396496\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (21.2562446594,42.9150613611), test loss: 41.154467392\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (0.961105823517,6.8369597862), test loss: 2.90582281053\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (19.0774536133,42.7892482064), test loss: 41.2622247696\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.6228685379,6.60987700198), test loss: 3.21377932429\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (31.3635177612,42.6732045828), test loss: 38.8885621071\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (5.48959732056,6.40789016999), test loss: 3.26332671046\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (22.007938385,42.5655346657), test loss: 35.340212965\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.21762442589,6.22755470402), test loss: 3.32043229938\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (16.5019054413,42.4633172799), test loss: 39.3358251095\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.72357153893,6.06474182343), test loss: 2.8998382926\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (71.253868103,42.3713625804), test loss: 41.2064579964\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.25077676773,5.91742823287), test loss: 3.10337878764\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (66.2721328735,42.2850764982), test loss: 39.2239190102\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.60300731659,5.78346944876), test loss: 3.27802948952\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (32.2363128662,42.1987683226), test loss: 35.9370923042\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.12948393822,5.6606997527), test loss: 3.11125484109\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (21.2975139618,42.1119307265), test loss: 35.6234306812\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (7.28550052643,5.54856921185), test loss: 3.00000162721\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (40.5653762817,42.0306915133), test loss: 40.65533638\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.42863035202,5.44486181912), test loss: 2.82247758806\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (53.4129981995,41.9524503566), test loss: 40.5940082073\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.74955654144,5.34920221238), test loss: 3.18468518853\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (18.151014328,41.8785826088), test loss: 38.1157096386\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.90358269215,5.2603888481), test loss: 3.07955129445\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (7.7246556282,41.7980176606), test loss: 35.2182005405\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.66145968437,5.17811986918), test loss: 3.07592521608\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (35.1985549927,41.7213505421), test loss: 38.3397735596\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.18392300606,5.10103763225), test loss: 2.8757203877\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (37.8786621094,41.646526272), test loss: 39.9466594696\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.733330607414,5.02927770342), test loss: 2.96106124818\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (17.4604454041,41.5723374231), test loss: 38.1711405754\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.31191158295,4.96182139776), test loss: 3.14326805174\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (38.8843231201,41.4964030451), test loss: 35.6128360748\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.02036261559,4.8986271659), test loss: 3.00608617961\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (15.9290571213,41.42036459), test loss: 34.103066206\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.67621898651,4.83921641908), test loss: 3.08458891213\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (47.7642173767,41.3461217457), test loss: 37.9326486588\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.94836521149,4.78362404164), test loss: 2.78344121575\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (22.0644493103,41.2666049967), test loss: 38.2156774998\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.4108902216,4.73116637506), test loss: 2.99763935804\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (31.3458213806,41.1857322575), test loss: 37.2267888546\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.61430311203,4.6812129467), test loss: 3.02344796956\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (20.288444519,41.1051360369), test loss: 34.4166432858\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.904222548008,4.63405299147), test loss: 3.0296852529\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (20.1011943817,41.0219672006), test loss: 35.3838327408\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.06623697281,4.58924932782), test loss: 2.91149152815\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (21.5708789825,40.9362630123), test loss: 39.1366404533\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.80882501602,4.54663531614), test loss: 2.82815108597\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (29.8111381531,40.8510524132), test loss: 37.6851662636\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.94240283966,4.50651194805), test loss: 3.13058232665\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (13.3500270844,40.7628998177), test loss: 35.7121750832\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.8652741909,4.46800954633), test loss: 3.04727321267\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (42.0281219482,40.6729253642), test loss: 33.4259450912\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.97273921967,4.43131153111), test loss: 3.13969768882\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (44.7830581665,40.5845843674), test loss: 36.8931175232\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.43422579765,4.39635901255), test loss: 2.83344956934\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (23.8920478821,40.4915163559), test loss: 37.382474947\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.3624484539,4.36279201012), test loss: 2.98058184683\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (23.1563186646,40.3942158956), test loss: 35.9268931866\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (6.13471031189,4.33078312695), test loss: 3.12511486709\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (44.7882614136,40.2950744043), test loss: 33.4818889141\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.04733800888,4.30003629811), test loss: 3.0378336221\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (37.3279457092,40.1928247485), test loss: 33.7449298859\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.59248614311,4.27058644545), test loss: 2.86236364543\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (13.0213680267,40.0895372873), test loss: 36.7422949791\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.04737138748,4.24219709798), test loss: 2.74617999792\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (30.7194309235,39.9787618326), test loss: 37.598078537\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.61855006218,4.21494951375), test loss: 3.0751303196\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (64.2189712524,39.8642928902), test loss: 35.7240161896\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.23837327957,4.18868014567), test loss: 2.97873517275\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (48.2323570251,39.7391448199), test loss: 31.0314453125\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.08411335945,4.16346944215), test loss: 3.04829081595\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (17.4080638885,39.6066188632), test loss: 33.4615089893\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.9745349884,4.13884838777), test loss: 2.71347400844\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (34.9228782654,39.4684577096), test loss: 36.0183722019\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.39429986477,4.11503552472), test loss: 2.92387872934\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (12.2226715088,39.3260987406), test loss: 35.1003821373\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.61057400703,4.09199928328), test loss: 3.11016073227\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (49.6415405273,39.1796314982), test loss: 31.8661966801\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.51477003098,4.0697454902), test loss: 2.79577496648\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (16.7809658051,39.0281835813), test loss: 29.6847546101\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.63222169876,4.04829976819), test loss: 2.79642349482\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (16.0235691071,38.8718603573), test loss: 33.4382719994\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.29430341721,4.02713263458), test loss: 2.62610948682\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (32.6681137085,38.7102735185), test loss: 34.5175517082\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.57797908783,4.00663859658), test loss: 2.98526148796\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (18.5629463196,38.544248637), test loss: 33.3678294897\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.3942797184,3.98656574901), test loss: 2.86843440533\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.7419700623,38.3753882208), test loss: 29.416886282\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.24268329144,3.9670719843), test loss: 2.86802294254\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (29.9734725952,38.2048237371), test loss: 31.0596914291\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.7761785984,3.94841469134), test loss: 2.72124082148\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.8652687073,38.0315314742), test loss: 33.2350419521\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.62073373795,3.92998147329), test loss: 2.7949821949\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (39.4802246094,37.8571361291), test loss: 33.3469136\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.82057332993,3.9120839881), test loss: 3.01091730893\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (16.8472232819,37.6800085935), test loss: 31.3684553146\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.94560945034,3.89463898231), test loss: 2.82536378503\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (17.5261573792,37.5010677497), test loss: 28.4863149643\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.766823291779,3.87749006969), test loss: 2.96702866256\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (21.0826854706,37.3211932788), test loss: 30.9159033298\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (5.41193580627,3.86073580557), test loss: 2.70463374257\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (32.6897888184,37.1417051052), test loss: 32.4975004673\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.85970425606,3.8442145089), test loss: 2.90552980602\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (7.70231819153,36.9618072272), test loss: 33.8104458332\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.42445206642,3.82790984933), test loss: 2.98141689599\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (30.1887588501,36.7844978506), test loss: 30.8707970858\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.45013237,3.81173008138), test loss: 2.93295138478\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (21.9420433044,36.605273212), test loss: 29.3830762506\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.35016155243,3.79575422879), test loss: 2.78355312645\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (32.0042572021,36.4264718634), test loss: 31.4235514879\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.898067593575,3.77990462156), test loss: 2.70042049587\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (31.6246891022,36.2492946377), test loss: 32.224338007\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (5.3599615097,3.76433700732), test loss: 3.0575458467\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (22.6674766541,36.0726180525), test loss: 32.4015287519\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.55246901512,3.74881280119), test loss: 3.00123061538\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.713514328,35.8955976132), test loss: 28.312788868\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.93188381195,3.73353836669), test loss: 3.0771987319\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (67.7183609009,35.7215958141), test loss: 28.9651484489\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.651237368584,3.71841600681), test loss: 2.74386895746\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (28.5566425323,35.5467234823), test loss: 31.5144325733\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.70473623276,3.7036394918), test loss: 2.972234568\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (43.3002243042,35.3733783108), test loss: 33.6069850445\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (5.22720479965,3.68918089662), test loss: 3.12528536916\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (27.8703155518,35.2005919267), test loss: 30.2372231007\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.05010795593,3.6746346931), test loss: 2.97748022676\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (23.6065368652,35.0283029943), test loss: 28.5416731358\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.69966101646,3.66034907102), test loss: 2.85086782277\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (15.0547189713,34.8566693398), test loss: 30.4164982796\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.2590867281,3.64615629174), test loss: 2.70828777552\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (13.3197031021,34.6856256432), test loss: 31.0243543625\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.27138209343,3.63218034638), test loss: 3.1144364059\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (16.4818992615,34.5160948236), test loss: 32.2662735462\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.92888879776,3.61855882657), test loss: 3.02544066012\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (17.7673740387,34.347132474), test loss: 27.6601686478\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.40094041824,3.6049862435), test loss: 3.02833061218\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (17.1008014679,34.1796794118), test loss: 28.7081487656\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.19510483742,3.59154999773), test loss: 2.75634144843\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (21.7297420502,34.0133484362), test loss: 30.1422443628\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.07998943329,3.57838731188), test loss: 2.93164231479\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (14.4397449493,33.8475101125), test loss: 30.5572636127\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.987673103809,3.56532036682), test loss: 3.17971566021\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (16.1814937592,33.6825265037), test loss: 29.2770941377\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.59650635719,3.55239638213), test loss: 2.91725907624\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (29.0741462708,33.5187247815), test loss: 26.8804436684\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.72858905792,3.53971843646), test loss: 2.94109884799\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (17.4714164734,33.3555234092), test loss: 29.4761138916\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.4109685421,3.52718700332), test loss: 2.75334216058\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (32.9237556458,33.1943456368), test loss: 30.0087091446\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.87907242775,3.51474594293), test loss: 3.07310544252\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (27.5934753418,33.0329100963), test loss: 31.1881689787\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.05893969536,3.50247431599), test loss: 3.03335458338\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (18.1599349976,32.8720288229), test loss: 26.9698830128\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.929673016071,3.49034546261), test loss: 3.01921575069\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (20.03282547,32.7127747254), test loss: 28.1561692238\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.26347637177,3.47839483189), test loss: 2.77810679525\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (14.8339414597,32.5541713386), test loss: 29.5835596561\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.695001363754,3.46651586647), test loss: 2.8794662714\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (12.8602581024,32.3956523441), test loss: 29.9965099812\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.42901611328,3.45482080101), test loss: 3.18990663588\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (48.3853302002,32.2393734815), test loss: 30.2019003153\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.8235270977,3.44325262668), test loss: 2.96616865695\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (25.6198673248,32.0828388957), test loss: 25.9652034402\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.50344848633,3.43188733935), test loss: 3.10328602493\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (28.8929748535,31.9274786057), test loss: 28.4281965256\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (5.16002464294,3.42077305652), test loss: 2.83747445941\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (45.3510246277,31.772746985), test loss: 28.8080243111\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.62812852859,3.40959820659), test loss: 3.003428635\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (10.9675474167,31.6182445895), test loss: 31.8775096416\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.47002720833,3.39857522921), test loss: 3.20758163184\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (6.2497177124,31.4643733693), test loss: 29.1085604191\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.97697639465,3.38760581908), test loss: 3.10354230106\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (6.85517215729,31.3112112362), test loss: 27.3549023151\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.670601725578,3.37678175111), test loss: 2.91824714094\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (15.1521368027,31.1588817203), test loss: 29.4210671902\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (4.11109447479,3.36619949988), test loss: 2.83558254838\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (12.5924892426,31.0070504104), test loss: 30.2799590111\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.51798892021,3.35563344556), test loss: 3.18706749827\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (10.3232336044,30.8560797047), test loss: 31.4336332798\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.44208741188,3.34515936586), test loss: 3.21640242636\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (15.8265390396,30.7057144094), test loss: 26.658142662\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.19737076759,3.33487126038), test loss: 3.21832098961\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (15.4954967499,30.5557458128), test loss: 28.1351312637\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.27380895615,3.32466427953), test loss: 2.8545673728\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (13.7262907028,30.4061985011), test loss: 29.666054678\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.36501693726,3.31450926474), test loss: 3.07196281552\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (24.656042099,30.2575641379), test loss: 31.993786478\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.93385016918,3.30454763853), test loss: 3.32617277801\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.3034000397,30.1093617389), test loss: 29.2957965851\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.93376326561,3.29465394486), test loss: 3.1472168237\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (36.4394683838,29.9619717468), test loss: 27.179127121\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.05779385567,3.28485283584), test loss: 2.97639074624\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (21.3268489838,29.8148924535), test loss: 30.5097053051\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (4.17656326294,3.27516589914), test loss: 2.79632057548\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (13.4827957153,29.6679083608), test loss: 30.3647678852\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.81122589111,3.26554501978), test loss: 3.19097585976\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (10.3757038116,29.5217497867), test loss: 32.1123361588\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.53220367432,3.25600587618), test loss: 3.13890166283\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (13.4514875412,29.3761243873), test loss: 27.1653294563\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.0269010067,3.24658223048), test loss: 3.11525952369\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.60114145279,29.2304596172), test loss: 28.6983437061\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.771560072899,3.2372231182), test loss: 2.83700332642\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (17.7805252075,29.0858578652), test loss: 28.60692904\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.65089130402,3.22798944455), test loss: 2.94411405027\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (14.4573478699,28.9416653558), test loss: 29.7747675419\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.29883718491,3.21885659244), test loss: 3.25414554477\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (11.8334922791,28.7978395306), test loss: 30.3492624998\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.15572571754,3.20988918192), test loss: 3.04367544651\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (25.9277896881,28.654721683), test loss: 26.562029171\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.68596053123,3.20093173159), test loss: 3.00594927967\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (11.615237236,28.511855537), test loss: 30.308252883\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.12800121307,3.19203756014), test loss: 2.81222895682\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (8.57917308807,28.3693699339), test loss: 28.8739080429\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.45658564568,3.18320372918), test loss: 3.03478243053\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (3.30323982239,28.2273628418), test loss: 32.4126734257\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.499220877886,3.17444438339), test loss: 3.16106603593\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (10.3724918365,28.0859791709), test loss: 28.3382694244\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (6.75783491135,3.16584824842), test loss: 3.08946288824\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.95541858673,27.9450563291), test loss: 29.0547768593\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.7065820694,3.15725883878), test loss: 2.84977591932\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (19.6510982513,27.804738219), test loss: 30.0217155933\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.980837464333,3.14871260224), test loss: 2.86415580362\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.76854228973,27.6648981794), test loss: 30.2457873821\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.27597165108,3.14029703575), test loss: 3.26056108773\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (9.63593101501,27.525564999), test loss: 31.2291384697\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.19730472565,3.13193495972), test loss: 3.08267572224\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (8.80383872986,27.3866298275), test loss: 26.7206482887\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.30361962318,3.12359298743), test loss: 3.14071462452\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (7.91446781158,27.2484634212), test loss: 30.8722829819\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.60870027542,3.11537761254), test loss: 2.86880135685\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (2.53139567375,27.1107630812), test loss: 29.1594210625\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.91203832626,3.10719243393), test loss: 3.01788031757\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (25.9484786987,26.9738897521), test loss: 33.0879339218\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.46448206902,3.09907994489), test loss: 3.28513046205\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (9.65511608124,26.8375846905), test loss: 30.8359807014\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.33626317978,3.09100968857), test loss: 3.16924476027\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (7.29370307922,26.7016937139), test loss: 29.392410326\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.91702699661,3.08302530576), test loss: 2.91055354774\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (7.32098484039,26.5665391805), test loss: 31.4555007935\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.949984252453,3.07505021083), test loss: 2.81875020266\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (22.5113239288,26.4321213749), test loss: 30.8886827469\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.92381358147,3.06717412282), test loss: 3.2033888787\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (2.80991506577,26.2979754751), test loss: 33.9181925297\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.33597064018,3.05932014706), test loss: 3.25689539015\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (12.4469327927,26.1647075641), test loss: 28.023852253\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.77009892464,3.05155368511), test loss: 3.22187907398\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (4.27652168274,26.0320456358), test loss: 29.584794879\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.73236227036,3.04386166663), test loss: 2.83649032265\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (9.93707466125,25.9000744705), test loss: 30.1735738754\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (5.78690671921,3.03626060246), test loss: 2.98727324158\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (4.57772159576,25.768670103), test loss: 33.5230545521\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.27153837681,3.02866769678), test loss: 3.33937396407\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (7.59397029877,25.6381629262), test loss: 31.4996759415\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (3.53479504585,3.02113244229), test loss: 3.10047781765\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (6.12866020203,25.508067358), test loss: 28.2217909336\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.694743096828,3.01361090509), test loss: 2.94577530771\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (3.91625928879,25.3787022475), test loss: 31.1146687031\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.33912992477,3.00615683174), test loss: 2.78257215321\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (7.68403720856,25.2500641047), test loss: 31.2821541309\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (4.84080648422,2.99877797617), test loss: 3.19344246835\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.69960546494,25.1221628839), test loss: 35.0109903812\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.68260860443,2.9914349912), test loss: 3.19202819467\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (11.4703712463,24.9950561854), test loss: 29.922836256\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.13875567913,2.98412578762), test loss: 3.11239362806\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (7.82860708237,24.8686661002), test loss: 30.2078814507\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.37017393112,2.97688748128), test loss: 2.86146944761\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.59865093231,24.7429394347), test loss: 30.8824336052\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.66988539696,2.96969299841), test loss: 2.92011051774\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.74929666519,24.6179595843), test loss: 32.2649046421\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.25669193268,2.96251043603), test loss: 3.28726722002\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (4.75912570953,24.4938566164), test loss: 33.1551915169\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.47059464455,2.95541693392), test loss: 3.14001536369\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (2.2927005291,24.3704861908), test loss: 28.609665823\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.0883769989,2.94834532816), test loss: 3.10328459144\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.98331260681,24.2478467132), test loss: 32.0311698914\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.03251886368,2.9413197087), test loss: 2.81721489727\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (5.12629890442,24.1261865223), test loss: 30.1386901379\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.89088177681,2.93431828077), test loss: 2.99802907109\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (1.4440382719,24.0051735472), test loss: 36.2708144188\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.75375449657,2.92739312637), test loss: 3.20036285818\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (4.74678230286,23.8849763703), test loss: 32.1710503578\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.08106529713,2.92048359034), test loss: 3.10281043053\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (11.7589359283,23.7656280484), test loss: 30.6224003315\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.14535331726,2.91363006535), test loss: 2.97610122412\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (2.55997252464,23.646922178), test loss: 33.5887714386\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.48693847656,2.90680746116), test loss: 2.84572610855\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (9.71139431,23.529124249), test loss: 33.0495889664\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.35553717613,2.90003836001), test loss: 3.27671882659\n",
      "run time for single CV loop: 6856.7396431\n",
      "\n",
      "MC # 1, Hype # hyp3, Fold # 3\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (296.567932129,inf), test loss: 188.71985321\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (328.433135986,inf), test loss: 372.532119751\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (29.380191803,125.384867697), test loss: 42.3729343414\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (8.40476799011,203.672040763), test loss: 5.21080691814\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (28.2568645477,84.7333684301), test loss: 40.1273682117\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.36917495728,103.454229728), test loss: 3.45479815006\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (19.3365135193,71.052931709), test loss: 39.1413502216\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.76924979687,70.0169508687), test loss: 3.3513145566\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (44.5444297791,64.213210526), test loss: 35.6285125732\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (8.46408462524,53.3020032406), test loss: 3.52185103893\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (32.512966156,60.0683786579), test loss: 39.0106284618\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.09507584572,43.2700562088), test loss: 3.17307084501\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (11.0606498718,57.2616540648), test loss: 39.905633688\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.97311162949,36.5837306408), test loss: 3.41543132663\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (49.8260345459,55.3071219285), test loss: 37.9804698467\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.54900074005,31.8068718062), test loss: 3.54998080134\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (56.9813728333,53.8098242911), test loss: 34.0638359547\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.15545010567,28.2234859259), test loss: 3.50800641775\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (48.7001113892,52.6304548747), test loss: 39.5378370762\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (6.72530078888,25.4368288505), test loss: 3.03996737599\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (21.7425708771,51.6806560768), test loss: 39.6223049164\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.44714641571,23.2051866125), test loss: 3.45864678621\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (30.3606414795,50.8983345506), test loss: 37.4538885117\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.60086369514,21.3823054969), test loss: 3.45536299348\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (40.3228492737,50.2344001044), test loss: 35.2751483917\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (10.4416294098,19.8633203927), test loss: 3.20123815238\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (32.5116195679,49.6654646587), test loss: 39.993726635\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.01908826828,18.5764229278), test loss: 3.14491687715\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (61.6377182007,49.1760512126), test loss: 39.5583733559\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.61615228653,17.473427809), test loss: 3.54834010899\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (11.6153745651,48.7461934855), test loss: 36.1225138664\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.49524211884,16.5175661571), test loss: 3.39607337713\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (75.5721588135,48.3640141895), test loss: 35.8184715271\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.81713533401,15.6799169729), test loss: 3.16630403399\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (44.0761947632,48.0144683024), test loss: 38.9786764145\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (6.00973701477,14.9425209026), test loss: 3.24186552763\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (20.616979599,47.6928270334), test loss: 38.4014320374\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.48663234711,14.2856890124), test loss: 3.48511781394\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (20.9672622681,47.3894255171), test loss: 34.8118608475\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.47521519661,13.6982098749), test loss: 3.36371918619\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (28.0102767944,47.1293105808), test loss: 37.408119297\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.50567400455,13.1692050807), test loss: 3.16672920883\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (105.026733398,46.8872936469), test loss: 39.4983181953\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.74320578575,12.6898509699), test loss: 3.3576256007\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (37.6300811768,46.6425163794), test loss: 37.5124330521\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.746756672859,12.253691987), test loss: 3.41504417658\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (41.6451873779,46.4149641686), test loss: 33.8666736126\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.12788581848,11.8546685507), test loss: 3.34100216329\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (91.2095336914,46.2058035707), test loss: 36.3756642342\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.58740568161,11.4902396539), test loss: 3.11321979463\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (34.1213798523,45.9953473454), test loss: 38.1085568905\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.19249641895,11.1540804197), test loss: 3.38051653504\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (59.7363471985,45.8009728858), test loss: 37.290557909\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.5470085144,10.8435188023), test loss: 3.35632648766\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (60.5711402893,45.6113031144), test loss: 33.7582706451\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.87113666534,10.555801112), test loss: 3.43824031651\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (25.5525932312,45.4256028752), test loss: 36.4677206039\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.76256036758,10.2882847076), test loss: 3.07637908161\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (37.0581321716,45.2485870198), test loss: 37.4542105198\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.64019882679,10.0387049094), test loss: 3.31933956444\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (28.1844501495,45.0708507674), test loss: 35.8232094765\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.42142701149,9.80659538296), test loss: 3.42741373479\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (24.8088626862,44.8944424231), test loss: 31.5545911312\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.39886200428,9.58864369544), test loss: 3.35237930119\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (56.6322021484,44.7176242117), test loss: 36.5723134041\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.14697360992,9.38461010388), test loss: 3.00653649271\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (34.0545768738,44.5491814428), test loss: 37.809381485\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.20758652687,9.1924465446), test loss: 3.44965116382\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (27.2077903748,44.3827477466), test loss: 34.7411619663\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.23274898529,9.01135236736), test loss: 3.36452932954\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (33.7071914673,44.2101453298), test loss: 32.4176134109\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.554903745651,8.83995336196), test loss: 3.11377792358\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (81.8018417358,44.0375536566), test loss: 36.0347428322\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.72201538086,8.67762833393), test loss: 3.06319543421\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (48.6595153809,43.8621654442), test loss: 35.718391943\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.94487559795,8.52424891577), test loss: 3.42605728209\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (21.9652442932,43.674681255), test loss: 31.8689774036\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.59555339813,8.37842818169), test loss: 3.29760985672\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (44.5908050537,43.4929958103), test loss: 31.5973441124\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.24979555607,8.23945551296), test loss: 3.16406224966\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (19.7989387512,43.3042629573), test loss: 33.5511340141\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.76776194572,8.10729401251), test loss: 3.16693013906\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (53.4823532104,43.1125012853), test loss: 34.1148527622\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.66128563881,7.98116051389), test loss: 3.38297098577\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (20.6797447205,42.9166233954), test loss: 30.4280142307\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.66037988663,7.86064788576), test loss: 3.29608668089\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (34.699005127,42.7169758099), test loss: 31.2529455662\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.04996919632,7.74611825599), test loss: 3.08569957316\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (19.4096450806,42.5105855859), test loss: 33.5646513939\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.33671712875,7.63580059759), test loss: 3.23084477782\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (58.3096160889,42.3013162128), test loss: 33.0961627483\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.44045329094,7.53042738428), test loss: 3.37780872285\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (26.5922203064,42.0880459068), test loss: 28.7115869999\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (4.54992008209,7.42911108507), test loss: 3.36293087006\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (29.0514507294,41.8749223916), test loss: 29.3407186031\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (5.35437774658,7.33191961073), test loss: 3.05424010754\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.1141147614,41.6539471262), test loss: 31.3525776386\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.73637294769,7.23835659138), test loss: 3.24258008301\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (28.5671100616,41.4304289823), test loss: 31.5432435036\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (7.63620376587,7.14888151934), test loss: 3.31727126241\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (17.4209709167,41.2036430283), test loss: 27.0277972698\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.43302059174,7.06247931412), test loss: 3.38230559826\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (11.1810245514,40.9706030129), test loss: 28.5911563635\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.69244229794,6.97947143449), test loss: 3.01459360123\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (22.4590339661,40.742176453), test loss: 30.8485185146\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.70414471626,6.89923297414), test loss: 3.224223423\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (23.3135662079,40.5103528919), test loss: 29.4203486681\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.15166521072,6.82180088199), test loss: 3.40128226578\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (30.1536521912,40.2799977399), test loss: 25.4869220495\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.84509468079,6.74715616991), test loss: 3.30948867798\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (16.7871665955,40.0474918025), test loss: 28.7776429653\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.53442668915,6.67476875537), test loss: 2.97612567544\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (11.222984314,39.8181782214), test loss: 30.0269095421\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.81365644932,6.6052469894), test loss: 3.35954231024\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (25.6411895752,39.5873900982), test loss: 28.375620842\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (9.55135917664,6.53789424412), test loss: 3.37966942191\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (28.2109565735,39.3586345841), test loss: 26.1828967094\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.22983241081,6.4724780215), test loss: 3.13105103672\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (45.3381614685,39.1303418662), test loss: 28.3711161613\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.16869020462,6.40908268063), test loss: 3.05911434889\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.5373916626,38.9050674218), test loss: 28.786101675\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.10348153114,6.34766949955), test loss: 3.34750674367\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (21.0390796661,38.6791828924), test loss: 27.0065607548\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.3682038784,6.2879122868), test loss: 3.28107943237\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (41.0464553833,38.4570528748), test loss: 25.3491089344\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (6.10642957687,6.23050893561), test loss: 3.13280807436\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (23.5011291504,38.2357981851), test loss: 26.928473115\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.51838684082,6.17432279007), test loss: 3.11596847177\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (21.9488601685,38.0138135358), test loss: 28.960104847\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.792319417,6.11992246976), test loss: 3.32853798568\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (19.2477912903,37.7957546426), test loss: 25.8558775425\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.14683127403,6.06690567548), test loss: 3.31268730164\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (48.8338775635,37.5806550883), test loss: 25.4115148902\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.63241708279,6.01536939152), test loss: 3.07293487191\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (12.8264341354,37.3649977444), test loss: 27.679538846\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.570375084877,5.96523227761), test loss: 3.13718017936\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (15.7577123642,37.1508662769), test loss: 28.7788971782\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.3149664402,5.91633981609), test loss: 3.28182962239\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (48.4554901123,36.9419185743), test loss: 24.6572995305\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (4.88384246826,5.8690939517), test loss: 3.32730161846\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (20.9004364014,36.7307751686), test loss: 24.9671602249\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.36443853378,5.82284141217), test loss: 3.00228199065\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (22.8117523193,36.5230269649), test loss: 27.0418246746\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.60315465927,5.77768986248), test loss: 3.18356529474\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (5.94230508804,36.3160296407), test loss: 28.1324581623\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.6209192276,5.73359441567), test loss: 3.39825665355\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (10.3653516769,36.1119636007), test loss: 23.6077871084\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.14015436172,5.69061083116), test loss: 3.31692056358\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (14.9134912491,35.9084838051), test loss: 25.7268329382\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.03793644905,5.6485477976), test loss: 2.96602709293\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (14.4916820526,35.7084976024), test loss: 27.0961180687\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.79569125175,5.60787641184), test loss: 3.12878186703\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.1904411316,35.5097383983), test loss: 26.5943756104\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.14136445522,5.56785073883), test loss: 3.3422558099\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (17.3180198669,35.3111874246), test loss: 23.2944924593\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.97187972069,5.52893599548), test loss: 3.18861568272\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (14.926607132,35.1146379827), test loss: 26.4491492033\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.56308937073,5.49075358671), test loss: 2.9472040996\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.3325881958,34.9211843442), test loss: 26.6779726982\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.9033396244,5.45343426811), test loss: 3.27812601626\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (30.9979095459,34.727587026), test loss: 26.1467684984\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.08951544762,5.41692289375), test loss: 3.33477276564\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (37.6582489014,34.5357550736), test loss: 24.1840842247\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.86825466156,5.38127643667), test loss: 3.0439886421\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (22.8147525787,34.3468246796), test loss: 26.1314441204\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.43512487411,5.34657169272), test loss: 2.98117323518\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (15.0539770126,34.1572142845), test loss: 27.0656905651\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.12427377701,5.31250453168), test loss: 3.27090222239\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (14.9065189362,33.9698322691), test loss: 25.3234143257\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.48499846458,5.27902074167), test loss: 3.29789644182\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.6701698303,33.78323526), test loss: 24.0910983801\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.80607366562,5.24626230379), test loss: 3.0726482898\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (16.6969985962,33.5986088259), test loss: 25.9337661266\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.19998216629,5.21412139416), test loss: 3.03415566087\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (13.2789993286,33.4147094017), test loss: 27.5291311741\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.31214427948,5.18254026041), test loss: 3.26457698941\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (47.2507324219,33.2333519022), test loss: 23.8679599285\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.08574604988,5.15192654589), test loss: 3.26175014973\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (12.3450775146,33.0522133792), test loss: 24.5365286589\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.21629571915,5.12165961657), test loss: 3.00310479701\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (24.7627868652,32.8724828642), test loss: 25.8702030897\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.61326169968,5.09210463768), test loss: 3.11351567507\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (13.1771564484,32.6925663367), test loss: 27.6177384377\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.65785932541,5.06297473229), test loss: 3.30953471065\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (13.4256353378,32.5154528017), test loss: 23.5313492537\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (4.44050884247,5.03442060328), test loss: 3.30238212496\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (9.06239318848,32.3381793755), test loss: 24.6765846252\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.32805633545,5.0062781121), test loss: 2.97724896073\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (24.2314376831,32.1628023753), test loss: 26.1146818161\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.01763296127,4.97884119398), test loss: 3.13286093175\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.1519241333,31.9886402359), test loss: 27.3825630665\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.449091047049,4.95190308384), test loss: 3.39518176913\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (9.36950683594,31.814524553), test loss: 23.1279574871\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.75103259087,4.92546290417), test loss: 3.29632959366\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (6.58998298645,31.6416210701), test loss: 26.110785675\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.89944970608,4.89934666214), test loss: 2.9397120446\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (11.2296857834,31.4695051033), test loss: 26.4500447273\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.74715662003,4.87371303166), test loss: 3.07581240833\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (15.970664978,31.2986917475), test loss: 26.1642468929\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.2780611515,4.84846462893), test loss: 3.28565631509\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (15.3844633102,31.1283844786), test loss: 23.1855713129\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.3664277792,4.82361614636), test loss: 3.08657222688\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.15824985504,30.9597923943), test loss: 26.6918825388\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.85240864754,4.79943052741), test loss: 2.91094723344\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (8.61228752136,30.791567701), test loss: 26.771154356\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (6.25766658783,4.77552614933), test loss: 3.24882885814\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (7.18414163589,30.6242276085), test loss: 26.5733793259\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.73079991341,4.75195394439), test loss: 3.39275137782\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (22.6234970093,30.4573786866), test loss: 24.5841874123\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.41914105415,4.7287396066), test loss: 3.05251469314\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (10.6326522827,30.2917729745), test loss: 27.5810202122\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.80496525764,4.70588844511), test loss: 2.93694493771\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (6.90894365311,30.1264282903), test loss: 27.7844829082\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.17117595673,4.6833015325), test loss: 3.26740634739\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.30055999756,29.9625825894), test loss: 26.1983438015\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.82378387451,4.66134172395), test loss: 3.35996900499\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (15.4723501205,29.7998103495), test loss: 25.0578609943\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.4810693264,4.63952908528), test loss: 2.99985309243\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (19.2741584778,29.6372661331), test loss: 26.162496233\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.41197288036,4.61813717108), test loss: 3.02766052186\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (7.62163448334,29.4752613955), test loss: 28.3156712532\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.55659031868,4.59698074216), test loss: 3.27910490036\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (26.0888175964,29.3146076621), test loss: 24.738594389\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.42998540401,4.57611382469), test loss: 3.28182650208\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (11.387635231,29.1542874609), test loss: 26.2727200747\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.71468091011,4.55557102257), test loss: 2.92649652958\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (6.39562225342,28.9944906642), test loss: 26.4021784067\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.25300884247,4.53531084172), test loss: 3.11316429973\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (23.8131465912,28.8365515365), test loss: 28.505373621\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (4.76817941666,4.51552513832), test loss: 3.3024491787\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (17.7681465149,28.6785180742), test loss: 24.4662473679\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.20628833771,4.49595154218), test loss: 3.32756354213\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.46774816513,28.5211653717), test loss: 25.802288866\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.31859588623,4.47656506534), test loss: 3.01552264988\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (4.34090042114,28.3645042917), test loss: 26.8831832886\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.18764281273,4.45748004511), test loss: 3.12058243752\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (6.51441192627,28.208811925), test loss: 28.3794296265\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (3.54696011543,4.43867019244), test loss: 3.35584936738\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (6.96952867508,28.0537594575), test loss: 24.3813973427\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (4.15236759186,4.42003126449), test loss: 3.29873711467\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (6.11518955231,27.8999929651), test loss: 26.7363838196\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.51717531681,4.4018521003), test loss: 2.95607609749\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (6.93909168243,27.7470304433), test loss: 27.3321638107\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.14587962627,4.38376422463), test loss: 3.07272283733\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.31046438217,27.5946622001), test loss: 27.7910159111\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.26459336281,4.3660027061), test loss: 3.34185032248\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (10.6084690094,27.4428935372), test loss: 24.7247879982\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.77498912811,4.34837043905), test loss: 3.09181552529\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (13.6359968185,27.2923811925), test loss: 27.5589460373\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.76876759529,4.33096005819), test loss: 2.90645778775\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (13.0307970047,27.1425428126), test loss: 28.1655911446\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.85146266222,4.31377727488), test loss: 3.19949391484\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (23.1726131439,26.9935100556), test loss: 28.4684514999\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (3.28026175499,4.29684124658), test loss: 3.39609640837\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (7.24346208572,26.8458130459), test loss: 26.3456867695\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.28965640068,4.28021519441), test loss: 3.0250047341\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (4.66971206665,26.6984840866), test loss: 28.4891690254\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.53311491013,4.26370791747), test loss: 2.92124541551\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (4.29511880875,26.5522264321), test loss: 29.7947528362\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.37004947662,4.24735897861), test loss: 3.30541044772\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.94695234299,26.4066699077), test loss: 27.8610037804\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.94170045853,4.23121900812), test loss: 3.36832026243\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (4.49618721008,26.2622111485), test loss: 27.2677862167\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.76753783226,4.21523201798), test loss: 2.93604107499\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (10.5489368439,26.118649321), test loss: 27.6116294861\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.819510102272,4.19938961414), test loss: 3.02479186356\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (9.65155696869,25.9761757762), test loss: 30.4236346245\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.765546321869,4.18391489684), test loss: 3.33096749485\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.99917507172,25.834635923), test loss: 27.3620356083\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.572914302349,4.16848641895), test loss: 3.26278921366\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (11.9892587662,25.6940505273), test loss: 27.8765918255\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.46867084503,4.15333762691), test loss: 2.95896120071\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (8.51403045654,25.5540834639), test loss: 28.5157183647\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.00542378426,4.13826544788), test loss: 3.10439302325\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (7.35663700104,25.4153973474), test loss: 30.5376573086\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.67317450047,4.12332765705), test loss: 3.24188938141\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (2.54642057419,25.2774708945), test loss: 26.5117321968\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.858503043652,4.10857015586), test loss: 3.28518073559\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (9.99477577209,25.1408137729), test loss: 28.0462569237\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.98022186756,4.09403075263), test loss: 2.97546834648\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (5.898396492,25.0051399523), test loss: 28.5544398785\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.610508739948,4.07969847424), test loss: 3.06220275909\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (2.73474049568,24.8702314473), test loss: 30.8759840488\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.57008481026,4.0654941915), test loss: 3.34475503415\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (2.47820711136,24.7364002957), test loss: 26.8863599777\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.59550392628,4.05136151053), test loss: 3.30010053962\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (3.86883020401,24.6034159911), test loss: 28.5411397457\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.9901907444,4.03740893844), test loss: 2.92520499527\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (4.66805696487,24.4717356674), test loss: 28.8388420105\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.640751481056,4.02355453183), test loss: 3.07948708236\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (4.3854227066,24.3409253701), test loss: 29.7585965633\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.59046387672,4.00983912811), test loss: 3.31853937507\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (5.85762739182,24.2113841876), test loss: 26.6790049076\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.81861400604,3.99640549734), test loss: 3.06536874771\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (5.44230413437,24.0827023174), test loss: 28.615657711\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (3.92598462105,3.98304390236), test loss: 2.90659659505\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (2.87360095978,23.9550516819), test loss: 30.3122208118\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.80270326138,3.96981268028), test loss: 3.2219212532\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.64457941055,23.8282421285), test loss: 30.3923355103\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.76681685448,3.95668005404), test loss: 3.36248369813\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (4.61511421204,23.7026842339), test loss: 29.2588175774\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.48515748978,3.94366864156), test loss: 2.9715811938\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.76304316521,23.5779038377), test loss: 30.128924942\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.35143065453,3.93075747926), test loss: 2.9152048558\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (2.38013935089,23.4545102674), test loss: 31.9476099968\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (5.10740184784,3.91811139815), test loss: 3.34702445567\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.46010303497,23.3320434236), test loss: 29.2952265263\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.40107250214,3.90551047831), test loss: 3.27077228129\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (7.15270185471,23.2105262877), test loss: 29.0853643894\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.80517530441,3.89306951068), test loss: 2.91227689981\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.72998857498,23.0899855547), test loss: 29.1280277729\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.9108953476,3.8806945977), test loss: 3.02110855579\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (6.1861448288,22.9704370457), test loss: 31.9705468655\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.966349959373,3.86842317513), test loss: 3.33356995881\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.64433217049,22.8520610585), test loss: 28.354082489\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.35452866554,3.8562830718), test loss: 3.1874271512\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (2.05645465851,22.7345707068), test loss: 29.3516720772\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.47667241096,3.84424014903), test loss: 2.96707020402\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (3.45595550537,22.6183228265), test loss: 29.6136004925\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (5.00797891617,3.83243574431), test loss: 3.04203070402\n",
      "\n",
      "MC # 1, Hype # hyp3, Fold # 6\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (225.729644775,inf), test loss: 192.483538437\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (350.310791016,inf), test loss: 371.006001282\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (24.2044868469,125.959045657), test loss: 40.852295351\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (8.86080169678,202.931346432), test loss: 5.31291860938\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (16.5242996216,85.1432055168), test loss: 40.8867986441\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.18651485443,103.075139796), test loss: 3.16481337249\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (60.5182876587,71.5077411254), test loss: 40.4213433266\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.527739048,69.7608975062), test loss: 3.48897909522\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.5604362488,64.6837929049), test loss: 38.7652795792\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.24496531487,53.0999863568), test loss: 3.54772597551\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (46.4550170898,60.5322587549), test loss: 35.5200281143\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.574578046799,43.1046686442), test loss: 3.47036174536\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (80.4213562012,57.7880435266), test loss: 40.1284012318\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.03032112122,36.4388430058), test loss: 3.06747207642\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (15.0247364044,55.7913170823), test loss: 39.6487242222\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.68187713623,31.6779780148), test loss: 3.36247249842\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (52.1126785278,54.3004074645), test loss: 39.1832335472\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.98712491989,28.1082919241), test loss: 3.28006972075\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (40.8316879272,53.1362582262), test loss: 35.7439701796\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.47177839279,25.3337121168), test loss: 3.36772763431\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (100.108459473,52.188607341), test loss: 39.0568597317\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (7.55307579041,23.1122619504), test loss: 3.14375045002\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (16.2940826416,51.4021442951), test loss: 40.2612533808\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.73403835297,21.2938329323), test loss: 3.24302966595\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (25.9757423401,50.7304121192), test loss: 39.3139369011\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (5.08414459229,19.7793954408), test loss: 3.41307712495\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (16.2476043701,50.1553177113), test loss: 36.2252405643\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.82926750183,18.4971826819), test loss: 3.28781106472\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (51.9201049805,49.6661626024), test loss: 34.9013623714\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.30476570129,17.3983844093), test loss: 3.19691242874\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (50.8915481567,49.2373593527), test loss: 40.3817808151\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.1099858284,16.4459060702), test loss: 3.07140113413\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (29.7785415649,48.8464343507), test loss: 40.0547641754\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.59990358353,15.6125423666), test loss: 3.43148104548\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (29.5002346039,48.5038894363), test loss: 38.8328931808\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.32805085182,14.8765989012), test loss: 3.28200498819\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (28.1567153931,48.1959607865), test loss: 34.2499643803\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.07123303413,14.2218944792), test loss: 3.41241908669\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (71.1702194214,47.9056201313), test loss: 38.6562334061\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.43068778515,13.6361364769), test loss: 3.02040210068\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (43.6828956604,47.6466151644), test loss: 38.755472517\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.287437677383,13.1081115192), test loss: 3.3513050884\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (43.3348693848,47.3981894144), test loss: 38.0696063995\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.24435997009,12.6306989656), test loss: 3.3404201299\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (106.168083191,47.1741353631), test loss: 34.9291171074\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.98396492004,12.1967840031), test loss: 3.25100750923\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (30.6146697998,46.9579618276), test loss: 35.8836061001\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.58695030212,11.8002053639), test loss: 3.06499119699\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (68.3114318848,46.7560132854), test loss: 39.4948359013\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.72161078453,11.4360924028), test loss: 3.11671798527\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (12.6860694885,46.5556798528), test loss: 38.0473992348\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.41436362267,11.100340772), test loss: 3.40637825131\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (44.801448822,46.365730407), test loss: 37.498742485\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.74141263962,10.7909086459), test loss: 3.39256525934\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (34.5044441223,46.1814517445), test loss: 33.6580778599\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.39882159233,10.5036342487), test loss: 3.34740104973\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (24.2317581177,46.0058417937), test loss: 38.7292892218\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.87888717651,10.2370784715), test loss: 3.05604828\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (32.8896865845,45.8348539746), test loss: 38.0535434246\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.12694501877,9.98859139252), test loss: 3.28818891048\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (50.2244415283,45.6637927077), test loss: 36.9761033058\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.00964713097,9.75663044162), test loss: 3.17863840461\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (38.99401474,45.5002400802), test loss: 32.8059558392\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (0.693405866623,9.53920706626), test loss: 3.27512052059\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (27.3617019653,45.3399234317), test loss: 36.5437315941\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.04809951782,9.33528467115), test loss: 3.01083434224\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (28.5621604919,45.1771509277), test loss: 37.9339495659\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.49726057053,9.14331381477), test loss: 3.21710735559\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (43.5913047791,45.0203884747), test loss: 36.9625981808\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.40384447575,8.96233798476), test loss: 3.30374419391\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (36.0143814087,44.85936918), test loss: 33.4588177443\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.04459691048,8.7914621611), test loss: 3.15608096123\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (82.9287872314,44.7028291549), test loss: 33.0981991291\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.34796476364,8.62993877824), test loss: 3.04808441699\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (38.4302253723,44.5419534287), test loss: 36.3026500225\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.572749972343,8.4768067801), test loss: 3.01421980262\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (66.3902587891,44.3818450655), test loss: 36.972650671\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.66661477089,8.3314767455), test loss: 3.35990669131\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (10.9249811172,44.2117332273), test loss: 34.8901899338\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.894736945629,8.19289030441), test loss: 3.36983051598\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (32.1780052185,44.0390886774), test loss: 31.0745052338\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.01624822617,8.06148170942), test loss: 3.4002746731\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (25.4191246033,43.8622797474), test loss: 34.4400006294\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.48417663574,7.93571709043), test loss: 2.99091666639\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (44.7482032776,43.6861474436), test loss: 34.3450199604\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.51357269287,7.81591871781), test loss: 3.26620259881\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (29.7678527832,43.5047058114), test loss: 34.5429699421\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.71417593956,7.7010992987), test loss: 3.26227924228\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (51.0788612366,43.3195906074), test loss: 30.8201295853\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.05179691315,7.59148052556), test loss: 3.34991013706\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (69.9308853149,43.1335943322), test loss: 32.1678304672\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.53538060188,7.48612074483), test loss: 3.09090716839\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (35.7466049194,42.9419174269), test loss: 34.0321421862\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.47089624405,7.38502769097), test loss: 3.11133121848\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (35.5396575928,42.745717779), test loss: 33.5036131382\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.07345533371,7.28781641038), test loss: 3.35501925945\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.4614334106,42.5473509321), test loss: 30.5884363651\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.79240477085,7.1942985958), test loss: 3.23892252445\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (72.232170105,42.345574605), test loss: 28.1687754869\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.2835495472,7.10459179928), test loss: 3.16219801307\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (32.9414978027,42.138296124), test loss: 31.5821846485\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.0201883316,7.01850802598), test loss: 2.96757415831\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (39.7447357178,41.9277699279), test loss: 31.6778904915\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.27855300903,6.9356774526), test loss: 3.19363320768\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (26.4003562927,41.7156066308), test loss: 32.2727858543\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.11852216721,6.85557621543), test loss: 3.21424466372\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (19.8508987427,41.4977563921), test loss: 27.8136519432\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.24403333664,6.77811740981), test loss: 3.3266861856\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (10.7132110596,41.2797668849), test loss: 29.4974734068\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.779303133488,6.70366997977), test loss: 2.99234417677\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (38.1400642395,41.0616046261), test loss: 31.3435243607\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.42330551147,6.63153424363), test loss: 3.32149189711\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (32.9779205322,40.8439307926), test loss: 32.1112249851\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.10465741158,6.56186575013), test loss: 3.37072791159\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.8257389069,40.6241694239), test loss: 28.4355116367\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.24169313908,6.49429164231), test loss: 3.30562959611\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (44.6477432251,40.4066113957), test loss: 27.8276698112\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.57840871811,6.42902841079), test loss: 3.10737799704\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (88.4855804443,40.1900189681), test loss: 29.324008131\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.71437764168,6.36568933578), test loss: 3.03759242594\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (22.1288032532,39.9722556796), test loss: 30.1671493053\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.20574831963,6.30433174432), test loss: 3.3165381819\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (37.0999031067,39.7558536663), test loss: 29.8726101875\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.57785892487,6.24471967528), test loss: 3.42973727584\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.4371681213,39.5403344291), test loss: 26.1912574768\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.81425368786,6.1867642386), test loss: 3.33498838842\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (69.0773925781,39.3270658116), test loss: 28.6407369137\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.82178521156,6.13072050075), test loss: 3.02079773843\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (21.8063755035,39.1149418294), test loss: 29.5141932487\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (7.02114677429,6.07660621002), test loss: 3.17074884176\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (33.1625289917,38.9036885884), test loss: 29.8769205809\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.55542039871,6.02377633332), test loss: 3.17829146385\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (23.7134933472,38.6938593052), test loss: 27.0784310341\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.06886863708,5.97236385472), test loss: 3.30636960566\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (16.1663093567,38.4829844856), test loss: 27.4505773544\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.03953278065,5.92223839451), test loss: 3.03000804484\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (23.4638175964,38.2743811708), test loss: 28.9481993198\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.47804641724,5.87375453051), test loss: 3.12259255648\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (25.5401058197,38.0674342109), test loss: 29.4652930737\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (4.22283840179,5.82634786381), test loss: 3.32672304511\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (13.968585968,37.862318773), test loss: 27.3003290653\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.57445526123,5.78021587489), test loss: 3.22964720726\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (17.6575183868,37.6579865962), test loss: 25.3222836018\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.75217199326,5.73521031869), test loss: 3.08278362751\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (13.6615333557,37.4559533611), test loss: 28.180363512\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.19668364525,5.69143521192), test loss: 2.96961272955\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (23.8643798828,37.2570194587), test loss: 28.333280468\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.935102939606,5.64863412991), test loss: 3.29084888697\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (4.77889156342,37.0570867224), test loss: 28.809787941\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.32073974609,5.60697911991), test loss: 3.32263149619\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.33294487,36.8602573747), test loss: 24.9460102558\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.980648994446,5.56616660192), test loss: 3.34779132009\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (18.8933925629,36.6637659081), test loss: 26.0540637493\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.958130002022,5.52630047647), test loss: 2.9670329839\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (17.9087677002,36.4705824043), test loss: 27.1982276917\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (4.55947494507,5.4876346122), test loss: 3.13992895484\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (24.3451900482,36.277981836), test loss: 29.1008286476\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (6.31731081009,5.44990699876), test loss: 3.30745405555\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (10.2846059799,36.0873719397), test loss: 25.8094172239\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.70927095413,5.41293391796), test loss: 3.30571882427\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (9.52746772766,35.8971290903), test loss: 25.3251586437\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.58703231812,5.37675240978), test loss: 3.01960944831\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (11.4072303772,35.7080671657), test loss: 26.7770188332\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.45591068268,5.34133587639), test loss: 3.04859075546\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (17.6198921204,35.5197174501), test loss: 27.8156630754\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.49936199188,5.30682292003), test loss: 3.24228059947\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (37.4827728271,35.33430266), test loss: 26.6995284081\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.4540746212,5.27295868398), test loss: 3.27975410521\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (13.8237781525,35.1484594349), test loss: 24.1117020607\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.41278767586,5.23980799265), test loss: 3.22632595301\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (15.7217845917,34.9642881909), test loss: 26.9231294632\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (4.42412281036,5.20736802822), test loss: 2.94362125397\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.02508831024,34.7815336692), test loss: 26.9144404411\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.98557472229,5.17560815182), test loss: 3.06322347224\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (20.6034355164,34.601308504), test loss: 28.1721536636\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.34131121635,5.14440624289), test loss: 3.22830145955\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.1716842651,34.4207003835), test loss: 24.4065124035\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.44009459019,5.11389834294), test loss: 3.21320483685\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (7.00958156586,34.2412790536), test loss: 25.6795093536\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.324036628008,5.08384065385), test loss: 2.94329932034\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (11.9218826294,34.0628685166), test loss: 27.34124856\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.35614323616,5.05442013382), test loss: 3.10607010722\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (10.2843914032,33.8861390846), test loss: 27.8214516401\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (5.09331846237,5.02568937955), test loss: 3.29129577279\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (26.9745788574,33.7108653913), test loss: 25.771004343\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (5.11431217194,4.99765325903), test loss: 3.278337726\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (11.3797645569,33.5353751016), test loss: 24.3446537018\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.38260269165,4.96995854365), test loss: 3.02531058192\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (17.0883636475,33.3615159724), test loss: 26.2484770298\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.72091627121,4.94271451325), test loss: 2.92592044026\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (10.0477399826,33.1872121584), test loss: 26.5164965153\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (6.02917098999,4.91606174676), test loss: 3.21212943494\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (13.9501914978,33.0146710727), test loss: 26.979312706\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.26294136047,4.88986107068), test loss: 3.38640825003\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (11.1165866852,32.8427593188), test loss: 23.3089155674\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.10666847229,4.86409410519), test loss: 3.31791774631\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (9.05937099457,32.6715922033), test loss: 26.3593023777\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.4708712101,4.83878326977), test loss: 2.90821135044\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (9.87853240967,32.5004595402), test loss: 25.6045739174\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (3.8963394165,4.81392556808), test loss: 3.04508424401\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (16.9591217041,32.3315632734), test loss: 27.7053476334\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.1030831337,4.78949177624), test loss: 3.21705724001\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (10.7252025604,32.1628762355), test loss: 24.6418935776\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.9166238308,4.76537179609), test loss: 3.29183364511\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (12.8385810852,31.9951171675), test loss: 25.1811055183\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.336029708385,4.7417440568), test loss: 2.94198215753\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (30.2014274597,31.8279032094), test loss: 25.9474110603\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.06597697735,4.71839125975), test loss: 2.98370687962\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (10.4430294037,31.6609585846), test loss: 27.0314733028\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (4.46683979034,4.69549077385), test loss: 3.18594723344\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (13.2183189392,31.4954170608), test loss: 25.9246959209\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.47705507278,4.67303631955), test loss: 3.18803797662\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (18.9935264587,31.3307191266), test loss: 23.6556236267\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (4.02811098099,4.65104220894), test loss: 3.08136402071\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (22.5393543243,31.1662218156), test loss: 26.193879509\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (4.81414270401,4.62925960001), test loss: 2.91756994426\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (11.6021528244,31.0022079854), test loss: 26.1717387676\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (3.06484413147,4.60778554923), test loss: 3.11305735409\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (13.4399871826,30.8385094017), test loss: 27.89264431\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (5.29475021362,4.58672118193), test loss: 3.23321986198\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (18.4275760651,30.6755055393), test loss: 23.4995572567\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.74706864357,4.56585720907), test loss: 3.25663010478\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (8.66883277893,30.5135047113), test loss: 25.3011122704\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.54718279839,4.545368266), test loss: 2.88791193962\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (6.13415336609,30.3512050281), test loss: 26.9769371033\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.67780065536,4.52514178224), test loss: 3.07269448042\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (9.84740257263,30.1901793662), test loss: 27.2719466209\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.56455993652,4.50526733406), test loss: 3.21662933826\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (11.3219089508,30.029463846), test loss: 25.1441448689\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.50840735435,4.48562849169), test loss: 3.23929264545\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (7.32612514496,29.8700020348), test loss: 24.1505565643\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.78360712528,4.46623800186), test loss: 2.9182721287\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (13.4931793213,29.7105232045), test loss: 27.8904690266\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.911723256111,4.44714883235), test loss: 2.88671948612\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (6.31546878815,29.5522086187), test loss: 26.5880349159\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.928745865822,4.42825277732), test loss: 3.18924542665\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (24.9526424408,29.3937641356), test loss: 27.5328271389\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.86818861961,4.40968033143), test loss: 3.30581222773\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (27.3200397491,29.2367405466), test loss: 23.4890389442\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.31826877594,4.39147765958), test loss: 3.22538515031\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (4.64626216888,29.07960437), test loss: 26.0979245186\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.24534130096,4.37353894348), test loss: 2.96176338792\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.6863765717,28.9237515073), test loss: 25.924529314\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.3864839077,4.35582751027), test loss: 2.98259874582\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (9.63779449463,28.7677911626), test loss: 28.2736116886\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.82127165794,4.33823032671), test loss: 3.13031461537\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (14.4617500305,28.6128230756), test loss: 25.3227759361\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.02788865566,4.32098132467), test loss: 3.19886437953\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (17.9551563263,28.4582911855), test loss: 26.2172068119\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.4356534481,4.30385361652), test loss: 2.89617325664\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.11978244781,28.3046737282), test loss: 28.2645120144\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.39054465294,4.28698982488), test loss: 2.99120471776\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (3.83350467682,28.1513266766), test loss: 27.8834963799\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.16971969604,4.27033199932), test loss: 3.20055958331\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (8.14924621582,27.998946468), test loss: 27.6451037884\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.50353860855,4.2538908978), test loss: 3.08243415356\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (5.39414596558,27.8473042261), test loss: 24.9058709145\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.43345832825,4.23762852059), test loss: 2.97368837595\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (11.0379724503,27.6967312962), test loss: 29.0476394653\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (4.32777404785,4.22155823924), test loss: 2.84670368284\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.82906246185,27.5466792453), test loss: 28.5357721806\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.11574745178,4.20564874632), test loss: 3.11207055449\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (2.87001132965,27.3974459614), test loss: 28.8117821217\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.986044883728,4.18989017652), test loss: 3.33260816336\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (11.9293079376,27.2491608683), test loss: 25.6851373196\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.93212199211,4.17439595422), test loss: 3.29836477041\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (17.5764350891,27.1016841031), test loss: 26.7980589867\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (3.32258677483,4.15914178153), test loss: 2.89994379878\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.08632993698,26.9551483141), test loss: 26.8094721794\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.862228155136,4.14406897047), test loss: 3.03952279091\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (13.173789978,26.8094212484), test loss: 29.3874638081\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (3.64002847672,4.12915809095), test loss: 3.16317820847\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (4.38468265533,26.6645866019), test loss: 27.9558624268\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.464548259974,4.11431602315), test loss: 3.31615405977\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (8.13571357727,26.5203545826), test loss: 27.0128414631\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.62457954884,4.09973136161), test loss: 2.90519538224\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (6.05480861664,26.377415577), test loss: 29.0887198925\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.71711778641,4.08523440286), test loss: 2.92869086266\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (6.16436576843,26.2351141365), test loss: 30.5093902588\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.6697409153,4.07092779506), test loss: 3.20776622891\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (4.26562452316,26.0937644558), test loss: 28.7139363289\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.52919816971,4.05676178685), test loss: 3.2116360575\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (7.5929889679,25.9533226458), test loss: 26.4948218822\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.92437767982,4.04275737427), test loss: 3.0584130466\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (6.81407165527,25.8139404603), test loss: 29.3397753716\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.11691021919,4.028870489), test loss: 2.86370830536\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.81613445282,25.6755762271), test loss: 30.1999275684\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.56895852089,4.01513505358), test loss: 2.93986166269\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.06978034973,25.5381742472), test loss: 31.0559136868\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.839085817337,4.00151588694), test loss: 3.13968833685\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (2.03838348389,25.4016916012), test loss: 27.911945343\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.11984121799,3.98800936411), test loss: 3.18650808632\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.88990592957,25.2663436995), test loss: 28.6472946167\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.98681330681,3.97471903885), test loss: 2.89588255286\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (8.0187702179,25.1321099124), test loss: 29.5308596611\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.13272666931,3.96160675685), test loss: 3.10565638244\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (6.96905517578,24.9987317073), test loss: 30.8505418777\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.72592306137,3.9486282603), test loss: 3.26146993935\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.03827905655,24.8665272089), test loss: 29.1533934116\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.1072306633,3.93577776985), test loss: 3.20195899606\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (4.84936141968,24.7351934917), test loss: 28.213083601\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.611105799675,3.9229516866), test loss: 2.95572118461\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (5.519572258,24.6049289421), test loss: 30.2531209946\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.946557581425,3.91036794565), test loss: 2.86082915664\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (4.542927742,24.4756988677), test loss: 31.1656234741\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.60389733315,3.89782402669), test loss: 3.20178980827\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (3.19405698776,24.3475272697), test loss: 32.3762665749\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (3.07290506363,3.88546018299), test loss: 3.39327871203\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.26876544952,24.2202185168), test loss: 28.3410650253\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.42687869072,3.87314936976), test loss: 3.20559744537\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.1773018837,24.0941147603), test loss: 30.3152732849\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.06951999664,3.86101086297), test loss: 2.92129159868\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (10.3061161041,23.9689788848), test loss: 29.7991102219\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.97191876173,3.84893337815), test loss: 3.01204342246\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.08362030983,23.8450401486), test loss: 32.2983018398\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.63332164288,3.83700087969), test loss: 3.07877394855\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.16628599167,23.7219625204), test loss: 30.5263246536\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.52925252914,3.82514378403), test loss: 3.21101515591\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (2.19676232338,23.600021963), test loss: 31.307489109\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.19639635086,3.81337792583), test loss: 2.86938405782\n",
      "\n",
      "MC # 1, Hype # hyp3, Fold # 7\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (336.65322876,inf), test loss: 189.372687912\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (324.024719238,inf), test loss: 369.204855347\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (33.0212860107,125.735050183), test loss: 37.0967838764\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.5081063509,202.413654011), test loss: 5.40577620268\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (36.9758529663,84.8020628304), test loss: 38.815262413\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.91533732414,102.764911469), test loss: 3.09040045142\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (43.8717460632,71.0844191125), test loss: 38.9444269419\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.90067565441,69.5182077914), test loss: 3.22334983945\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (45.120174408,64.2300373244), test loss: 37.9986352921\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.54772055149,52.9000011027), test loss: 3.29308992922\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (110.73034668,60.0733769276), test loss: 37.952244997\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.84756779671,42.9294589783), test loss: 3.25419402122\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (41.4583282471,57.2900979438), test loss: 34.3464369297\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.2254447937,36.2790151813), test loss: 3.3265832305\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (44.1590805054,55.298722226), test loss: 35.1279132843\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.46001815796,31.530101944), test loss: 3.16855286956\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (19.7506160736,53.7656436892), test loss: 38.1417658806\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.50691747665,27.9675023302), test loss: 2.97231272459\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (28.1647319794,52.5813368668), test loss: 39.194895792\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.44704151154,25.1972600037), test loss: 3.37802975476\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (38.766418457,51.6309966317), test loss: 38.4269873381\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.978862524033,22.9814506168), test loss: 3.34332918227\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (28.3230113983,50.8264682703), test loss: 37.0356428623\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.11263394356,21.1681099263), test loss: 3.37227121592\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (21.4131774902,50.1648410361), test loss: 33.4132970333\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.21710944176,19.6573812838), test loss: 3.31625342965\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (112.217514038,49.6055149499), test loss: 36.1538005352\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.92567777634,18.3791381294), test loss: 3.10319872499\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (31.3428783417,49.1025464213), test loss: 39.2321264267\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (5.02508831024,17.2831669914), test loss: 3.08305038214\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (21.1303291321,48.6682238398), test loss: 38.4191267967\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (6.88128709793,16.3328405326), test loss: 3.35379397869\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (51.0528030396,48.2854438698), test loss: 37.0483850002\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.45417976379,15.5010981008), test loss: 3.31363450587\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (54.579082489,47.9323823292), test loss: 33.8851906776\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.45733308792,14.7675176016), test loss: 3.19572734833\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (49.6197967529,47.6068957784), test loss: 33.8330177784\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.79640293121,14.115325757), test loss: 3.17541874647\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (42.4380493164,47.3196651733), test loss: 37.6472959042\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.5562748909,13.5318059908), test loss: 3.02074562013\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (35.7528457642,47.055077706), test loss: 37.7210839272\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (9.62640571594,13.0065003261), test loss: 3.19275904596\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (15.3577880859,46.810656042), test loss: 37.5189672709\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.64479494095,12.5301964054), test loss: 3.34219323993\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (30.3646678925,46.5836151915), test loss: 36.5572525024\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.16030335426,12.0979244272), test loss: 3.21198700964\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (26.7858276367,46.3587625726), test loss: 33.2315696239\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.02284097672,11.7026515872), test loss: 3.27610451579\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (42.3630447388,46.155621163), test loss: 34.6959220886\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.33004760742,11.3406063389), test loss: 3.12460850477\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (107.37109375,45.9577533829), test loss: 36.2828889847\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (6.24562072754,11.0070410438), test loss: 2.93630062938\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (91.645690918,45.769263986), test loss: 37.6060220718\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.67162919044,10.6990744524), test loss: 3.30158643126\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (27.8181724548,45.5800546758), test loss: 37.1367916346\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.03670048714,10.4141184843), test loss: 3.3303756088\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (75.3667602539,45.401904836), test loss: 35.4021101952\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.9422659874,10.1496759361), test loss: 3.33939124942\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (55.1798553467,45.2252937938), test loss: 31.7761382103\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.797613799572,9.90291475895), test loss: 3.26060227752\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (29.0595169067,45.0578073921), test loss: 34.7229867458\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.24456715584,9.67283035192), test loss: 3.02862136364\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (42.2154998779,44.8885940053), test loss: 37.1900630951\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.4442602396,9.45751811913), test loss: 3.0557751894\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (51.4162864685,44.7256188238), test loss: 36.0937112331\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.73352026939,9.25542828118), test loss: 3.27798486352\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (24.2921905518,44.5628297673), test loss: 35.1091159582\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.03624033928,9.06515395202), test loss: 3.25672130585\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (43.7820701599,44.4013593636), test loss: 31.7086100578\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.3014190197,8.88640293975), test loss: 3.21382051408\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (74.9376525879,44.2362129526), test loss: 31.3088127375\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.87603759766,8.71782857158), test loss: 3.11950631142\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (29.8097991943,44.0702424959), test loss: 34.6561206341\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.52644085884,8.55835524742), test loss: 3.01001731455\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (29.6609783173,43.907996672), test loss: 35.849881506\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.82355833054,8.40765209376), test loss: 3.16844973266\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (34.4129219055,43.7435152676), test loss: 35.3826572418\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.41709971428,8.26427825781), test loss: 3.32638262212\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (38.2586212158,43.5781243121), test loss: 33.6664294243\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.38876104355,8.12872679966), test loss: 3.26613391936\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (96.1555938721,43.4061868638), test loss: 30.0249871492\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.34201383591,7.99971463213), test loss: 3.27351625562\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (34.7148704529,43.22790189), test loss: 31.1277723789\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.91488391161,7.87591393874), test loss: 3.11469677687\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (39.2758598328,43.0432228211), test loss: 32.1472299099\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.52984857559,7.7580663992), test loss: 2.96043116748\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (10.7484493256,42.8504018936), test loss: 32.9413645744\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.80084276199,7.64532159955), test loss: 3.2548247695\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (38.59324646,42.6563788917), test loss: 33.2851806641\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.67234802246,7.53761106395), test loss: 3.31192265749\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (34.6158676147,42.4587717408), test loss: 31.0487976074\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.988784372807,7.43453560107), test loss: 3.32431182265\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (30.2544822693,42.2543899794), test loss: 27.8118716717\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.40262949467,7.33543518476), test loss: 3.28555861115\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (11.3137454987,42.0475939761), test loss: 29.8844511509\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.21401810646,7.24031964884), test loss: 2.96423983574\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (72.9613571167,41.8384532143), test loss: 30.9413143158\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.33963632584,7.14876362691), test loss: 3.08016368747\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (21.3153457642,41.6205821557), test loss: 31.2282835007\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.22107934952,7.06061994458), test loss: 3.22255650163\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (12.9186201096,41.401819323), test loss: 30.6136804819\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (6.39160919189,6.97545284878), test loss: 3.29221661091\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (23.9304542542,41.1805816804), test loss: 27.3294928789\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.61137485504,6.89309807038), test loss: 3.32214651704\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (31.6339092255,40.9538694515), test loss: 26.3148590565\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.54901003838,6.81367840332), test loss: 3.20939933658\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (38.7751235962,40.7236212215), test loss: 27.8255161285\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.99512052536,6.73696751126), test loss: 3.05931796432\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (22.5729961395,40.4928566305), test loss: 29.3343005657\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.47068166733,6.66289938849), test loss: 3.1795462966\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (22.3213500977,40.2623349998), test loss: 29.7513169289\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (10.576757431,6.59138606986), test loss: 3.34036234617\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (15.2760810852,40.0309160827), test loss: 28.9344362497\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.65270256996,6.52188379621), test loss: 3.40807406902\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (18.0690116882,39.7994246467), test loss: 25.4633055687\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.35957288742,6.45499292158), test loss: 3.33412350714\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.6376838684,39.5660708909), test loss: 26.3289454937\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.04931902885,6.39005571816), test loss: 3.164268893\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (27.2539920807,39.3340961877), test loss: 26.6919518471\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.12937116623,6.32722689515), test loss: 3.03387724459\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (61.1848449707,39.1026273354), test loss: 27.6772109032\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.39803409576,6.2661166453), test loss: 3.24193419814\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (58.3307876587,38.8726766746), test loss: 29.2143069744\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.51443052292,6.20689480852), test loss: 3.29933997393\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.2876491547,38.6436089299), test loss: 26.7825082779\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.80871975422,6.14946152122), test loss: 3.38745838106\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (18.4657497406,38.4177319969), test loss: 24.1947113991\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.10473716259,6.09381922565), test loss: 3.40267167985\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (28.4848060608,38.1926281787), test loss: 25.0945832968\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.36251866817,6.03963797908), test loss: 2.99862867594\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (9.01212501526,37.9706097316), test loss: 26.0695333004\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.96268594265,5.98702416077), test loss: 3.09400754869\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (19.6170272827,37.748626419), test loss: 27.2778301239\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.77776885033,5.93589031233), test loss: 3.18938861489\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (39.443611145,37.530519891), test loss: 27.176432848\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.06694626808,5.88605891169), test loss: 3.26761171818\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (9.03567123413,37.3132360431), test loss: 25.1273499489\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.30873095989,5.83744887588), test loss: 3.34439565539\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (23.2957649231,37.0986154802), test loss: 23.9928709984\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.50217890739,5.79028148461), test loss: 3.20217389464\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (28.3620185852,36.8857645142), test loss: 24.1134505272\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.62270545959,5.74431410569), test loss: 3.07102169394\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.1647262573,36.6734233739), test loss: 25.3563554287\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.71037101746,5.69945649709), test loss: 3.16302236915\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (9.69194221497,36.464679485), test loss: 26.4265870571\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.13745331764,5.65578976219), test loss: 3.275476107\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.5119514465,36.2579722717), test loss: 26.0275490284\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.63517284393,5.61297681464), test loss: 3.39228340387\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (18.5077037811,36.0526133516), test loss: 23.2814358473\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.52621364594,5.57146712042), test loss: 3.27813072801\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (40.370513916,35.8492789315), test loss: 23.896050334\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.60651063919,5.53098752218), test loss: 3.13966649175\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (37.9101257324,35.646181445), test loss: 24.0773877144\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.59535360336,5.49128848385), test loss: 2.99008052647\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (31.8864212036,35.445416985), test loss: 24.9323372841\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.37612867355,5.45258582278), test loss: 3.20608760417\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (10.3261566162,35.2462782575), test loss: 26.9671438217\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (5.34195566177,5.41474391613), test loss: 3.29669983387\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (28.170791626,35.0490024829), test loss: 24.4115674019\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.53975057602,5.37776838958), test loss: 3.29508149028\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (18.3836364746,34.8549402504), test loss: 22.2325831413\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.861777424812,5.34166671281), test loss: 3.29409163594\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (20.3301029205,34.6614986458), test loss: 23.6412995577\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.24846458435,5.30636689144), test loss: 2.95915411711\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (9.73792457581,34.4701220647), test loss: 23.8659772396\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.03857183456,5.2718246642), test loss: 3.00731725097\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (38.2649154663,34.2808234696), test loss: 25.1368875504\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.65579283237,5.23794444397), test loss: 3.13917206526\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (7.91230535507,34.0913388964), test loss: 25.2294482708\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (3.07663822174,5.20478262648), test loss: 3.23070854545\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (10.8433179855,33.9047389859), test loss: 22.903789854\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.34705972672,5.17221608306), test loss: 3.38942318559\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.46865367889,33.7198510896), test loss: 22.536892271\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.64204120636,5.14034602193), test loss: 3.1623331964\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (14.0152759552,33.5367389835), test loss: 22.9805777311\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (3.25819253922,5.10915049311), test loss: 3.06615101099\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (17.7702293396,33.3545625301), test loss: 23.7912675619\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.66538357735,5.07857648716), test loss: 3.12706836462\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (10.8721075058,33.1737414138), test loss: 24.8660303116\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.76050937176,5.04859462584), test loss: 3.22481703758\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (14.586101532,32.9950953895), test loss: 24.5102677822\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (9.14392852783,5.01922271395), test loss: 3.39079225361\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (16.1696166992,32.8170919034), test loss: 22.5357025623\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.38687777519,4.99025203603), test loss: 3.30490289032\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (15.320728302,32.6404793541), test loss: 22.8045786381\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.86220741272,4.96201195631), test loss: 3.04384861887\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (10.455704689,32.4645402282), test loss: 23.7858006001\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.8086707592,4.9342140433), test loss: 2.96511574984\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (10.441485405,32.2895790266), test loss: 24.3239435196\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.68022990227,4.90699389907), test loss: 3.18768121004\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (29.1676139832,32.1163483505), test loss: 26.2919304371\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.45237874985,4.88016486518), test loss: 3.30400039554\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (34.0526580811,31.9440867412), test loss: 23.6020895958\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (3.34418511391,4.85386904839), test loss: 3.25273759961\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (8.74124526978,31.7732204457), test loss: 21.652981329\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.21316432953,4.82803728207), test loss: 3.24249643385\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (5.33229732513,31.6039037256), test loss: 23.4694836617\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.549540996552,4.80277239087), test loss: 2.99168800712\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (14.135972023,31.4353964686), test loss: 23.3048864365\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.77497720718,4.77790607879), test loss: 3.04051699042\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (3.5031106472,31.2684235621), test loss: 24.3663321018\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.970657467842,4.75344543851), test loss: 3.06290431023\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (11.9133815765,31.1016433746), test loss: 25.1375184059\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.90892732143,4.72944309302), test loss: 3.25129027963\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (26.8646659851,30.9370404359), test loss: 22.7874213696\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.68608808517,4.70576570535), test loss: 3.39294995964\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (6.94271993637,30.7725299883), test loss: 22.1405723095\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.42290508747,4.68244801524), test loss: 3.1462233603\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (12.7143268585,30.6096291271), test loss: 23.5044767857\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.23592448235,4.65957912257), test loss: 3.00710560083\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (6.45831108093,30.4477655158), test loss: 23.6327493668\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.29034900665,4.63705900113), test loss: 3.1692504704\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (10.5152492523,30.286209296), test loss: 24.7355822563\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.90082478523,4.61488529025), test loss: 3.23380148709\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (5.63747262955,30.1263422502), test loss: 24.2576519489\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.62699055672,4.59306208111), test loss: 3.36566936374\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (14.2131481171,29.9671584719), test loss: 22.5312987566\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.49445438385,4.57146454754), test loss: 3.29362179488\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (15.511677742,29.8085517795), test loss: 22.5467880249\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.29558253288,4.55031945693), test loss: 2.93180308044\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (15.5668821335,29.6510815325), test loss: 24.1214541912\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.21678161621,4.52953418817), test loss: 2.96990550458\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (35.7721786499,29.4938770323), test loss: 24.0225364208\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.21501541138,4.50893242828), test loss: 3.10267327726\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (19.3862533569,29.3377790834), test loss: 26.2880883694\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.86349105835,4.48866694073), test loss: 3.36794800758\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (7.39277458191,29.1823999642), test loss: 24.3528868675\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (4.12121486664,4.46870132608), test loss: 3.27253695726\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (15.6436452866,29.0281296585), test loss: 21.9183831453\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.3353471756,4.44901578246), test loss: 3.13821204156\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (4.61431026459,28.875236315), test loss: 23.9232997417\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.564293980598,4.42963126954), test loss: 2.99120450616\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (12.970199585,28.7227179606), test loss: 24.0020115376\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.889963030815,4.41057454144), test loss: 3.02361370027\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (6.57014799118,28.5713960892), test loss: 24.378795433\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.06710934639,4.39177843371), test loss: 3.05706224442\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (16.1007499695,28.4207848577), test loss: 25.4859310627\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.03958284855,4.37321438693), test loss: 3.27952038646\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.1797337532,28.2707174978), test loss: 22.9502307892\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.29877614975,4.35492422707), test loss: 3.35028637648\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (10.0098686218,28.1217427691), test loss: 22.2624866009\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.15844345093,4.33681299815), test loss: 3.12954125404\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (4.69415187836,27.9735083538), test loss: 24.2511261463\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.37301683426,4.31896377716), test loss: 2.99597952068\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.12766027451,27.8262839898), test loss: 24.7141281128\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.69036459923,4.30134690595), test loss: 3.20214193761\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (12.6072921753,27.6796794378), test loss: 25.5442439556\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.09086537361,4.283993221), test loss: 3.27707929313\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (4.52825927734,27.533673927), test loss: 24.6964508533\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.93244564533,4.2668367024), test loss: 3.36989886463\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.19090843201,27.3887676583), test loss: 22.859001112\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (6.03801488876,4.24990372866), test loss: 3.29891135693\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (10.5836620331,27.2443509008), test loss: 23.3238814354\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.10577273369,4.23309735272), test loss: 3.0021224916\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (10.0856189728,27.1007675377), test loss: 25.0250589371\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.783524930477,4.21660092233), test loss: 2.96228054315\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (9.63988208771,26.9578006237), test loss: 24.8473109245\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.91073083878,4.20025481126), test loss: 3.10797924399\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (4.90509366989,26.8155503193), test loss: 27.4320232391\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.41754102707,4.18415644912), test loss: 3.41476981938\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (17.0267276764,26.6742779223), test loss: 24.8521428585\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.24683141708,4.16819351376), test loss: 3.26693099439\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (13.557970047,26.5337825202), test loss: 22.9450078011\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.82245445251,4.15244963158), test loss: 3.12970873117\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (5.13417291641,26.3940959801), test loss: 25.1132246971\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.99793022871,4.13685880708), test loss: 2.98942028135\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (4.45100307465,26.2554533093), test loss: 25.129883194\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.367476612329,4.1215450189), test loss: 3.05230333209\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (5.09431552887,26.1174610171), test loss: 25.8232842445\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.88887286186,4.10638787323), test loss: 3.1510846585\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (4.05160236359,25.9804106678), test loss: 26.0065071583\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.811958909035,4.09137328061), test loss: 3.27401226163\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (4.62358140945,25.844062035), test loss: 24.1522949696\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.63568234444,4.07658398461), test loss: 3.34669266939\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (9.96553611755,25.7087471627), test loss: 23.9158632278\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.26019144058,4.06187702817), test loss: 3.13554240763\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (5.58849906921,25.5741364357), test loss: 25.8816653728\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.26087832451,4.04733492515), test loss: 3.00608854294\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (4.5950012207,25.4404231959), test loss: 26.5958708286\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.964883446693,4.03298886143), test loss: 3.13688298762\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (2.98158955574,25.3076327976), test loss: 27.5752403498\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.89259076118,4.01879702843), test loss: 3.33503620327\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (7.74905395508,25.1755235249), test loss: 26.3081142426\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.6100256443,4.00476517233), test loss: 3.43610121012\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.65804672241,25.0442891042), test loss: 24.0724029064\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.32142090797,3.9908596268), test loss: 3.28952378035\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (9.1830291748,24.9140515363), test loss: 24.7081645966\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.57380247116,3.97705481301), test loss: 3.00820533335\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (7.624127388,24.7844682649), test loss: 26.4840148926\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.12041270733,3.96346743273), test loss: 2.97565568089\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.91549777985,24.6559157087), test loss: 26.3674173832\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (3.16605210304,3.95005212422), test loss: 3.12084586024\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (9.05905723572,24.5280208763), test loss: 27.96812222\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.96764087677,3.93668963056), test loss: 3.34368712604\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (8.42742156982,24.4011159233), test loss: 26.0400802135\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.45776081085,3.92348326093), test loss: 3.33328004479\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (3.49796795845,24.2750495111), test loss: 24.6660080433\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.24114489555,3.91042442076), test loss: 3.16662787795\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (11.0647535324,24.1499832114), test loss: 26.5404979706\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.54625964165,3.89747966469), test loss: 3.04998133332\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (1.25806570053,24.0259132801), test loss: 26.6436059475\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.696045041084,3.88467501414), test loss: 3.08801186085\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (5.10687637329,23.9027066988), test loss: 27.8975249767\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.642796814442,3.87205282037), test loss: 3.24906557351\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (3.46492028236,23.7803803284), test loss: 27.5173120975\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.35429120064,3.85953316972), test loss: 3.34465900064\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.41360855103,23.6590263365), test loss: 25.5361225128\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.716642141342,3.84712837322), test loss: 3.3650873512\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (2.87533545494,23.5384870633), test loss: 25.4303821564\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.1414513588,3.83485443685), test loss: 3.10783185065\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.69316697121,23.4189895194), test loss: 27.5393323421\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.09859013557,3.82265457135), test loss: 3.05904881954\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (2.61915063858,23.3003391431), test loss: 26.7836039066\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.0850520134,3.81057055728), test loss: 3.15394604802\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.89028835297,23.1826278936), test loss: 29.2400104523\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.9460477829,3.79860975417), test loss: 3.37848376334\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.12388944626,23.0658722513), test loss: 28.2409144878\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.78756308556,3.78679735035), test loss: 3.53086103797\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (2.96068763733,22.9498469943), test loss: 25.514951539\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.26996660233,3.77506040694), test loss: 3.2846496433\n",
      "\n",
      "MC # 1, Hype # hyp3, Fold # 8\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (343.059326172,inf), test loss: 177.430204391\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (310.12008667,inf), test loss: 373.457579041\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (70.8934249878,125.687188957), test loss: 34.1829914093\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.2140610218,203.584526077), test loss: 5.49960616827\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (49.3945808411,85.4771477926), test loss: 35.5828190327\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.01755571365,103.429613546), test loss: 3.0516718477\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (30.858915329,71.9796331518), test loss: 35.5555567741\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.23904371262,70.0109622106), test loss: 3.28530046046\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (48.4780273438,65.2116027132), test loss: 35.3165403843\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.58558797836,53.3027713983), test loss: 3.34098701179\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (56.3146896362,61.158481038), test loss: 30.8454396248\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.18838357925,43.2766987933), test loss: 3.3669220686\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (50.0048599243,58.4338197931), test loss: 33.9188024044\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.9469139576,36.59131494), test loss: 3.14613601267\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (20.5997657776,56.4877475736), test loss: 35.4238580942\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.4861574173,31.8159704748), test loss: 3.14930661023\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (23.6013793945,54.9958068866), test loss: 36.2222266197\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.11165428162,28.235303777), test loss: 3.41489383578\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (57.4146690369,53.8325269374), test loss: 34.5490065098\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.80174529552,25.44936719), test loss: 3.39682993293\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (67.9168701172,52.8959774368), test loss: 30.7285164833\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (7.16522407532,23.2211559798), test loss: 3.36547802985\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (96.2248840332,52.1301970735), test loss: 34.7492706299\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.07522797585,21.3967439981), test loss: 3.06846147478\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (20.3287620544,51.4667573661), test loss: 35.4809927464\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.59592175484,19.8766841765), test loss: 3.25508216023\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (31.672990799,50.9017839129), test loss: 35.6230010509\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.08083319664,18.5904049232), test loss: 3.33879079223\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (21.9086723328,50.4153681351), test loss: 31.5402540207\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.38707208633,17.4881567514), test loss: 3.22268860638\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (91.0691070557,49.9931497266), test loss: 31.2361756325\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.79641485214,16.5332432694), test loss: 3.11999620199\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (21.1763038635,49.6088392619), test loss: 34.463318038\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.34286403656,15.6979144375), test loss: 2.95069839358\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (45.4951782227,49.2631750287), test loss: 34.7375452042\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.84407365322,14.960149852), test loss: 3.20410720706\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (29.7017993927,48.9459012324), test loss: 34.5114311218\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.22096204758,14.304221334), test loss: 3.22974656224\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (66.5070266724,48.6688523621), test loss: 29.9927201748\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.65326285362,13.7177191506), test loss: 3.27261143923\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (40.068775177,48.3989568219), test loss: 32.731931448\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.36656975746,13.1891088863), test loss: 3.06845432818\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (28.2076454163,48.1435531024), test loss: 34.3342373848\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.76153844595,12.7109650105), test loss: 3.09822823703\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (51.3249588013,47.9076691703), test loss: 35.1930476665\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (8.18440151215,12.2769169889), test loss: 3.39188373089\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (14.2273836136,47.6835779645), test loss: 33.4704129696\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.01941585541,11.87990045), test loss: 3.37680040598\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (16.3731040955,47.47031045), test loss: 29.7070036888\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.15377116203,11.5154437923), test loss: 3.33309395313\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (49.2712135315,47.2697000281), test loss: 33.4939170837\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (5.00081539154,11.1800071157), test loss: 3.0704634279\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (93.317199707,47.0818862458), test loss: 34.3249876499\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (5.67492008209,10.8703838559), test loss: 3.25248093009\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (60.5634613037,46.8971472486), test loss: 34.5952038765\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.17103910446,10.5834144121), test loss: 3.36672589779\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (41.0997810364,46.7169371422), test loss: 30.2673920631\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.42195987701,10.3167724188), test loss: 3.24898431897\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (30.3183555603,46.5383891163), test loss: 29.9951053143\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.59266757965,10.0684545828), test loss: 3.14890361428\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (52.2365264893,46.365450957), test loss: 32.99296875\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.41306471825,9.83701291184), test loss: 2.97018151581\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (68.9941558838,46.1969131738), test loss: 33.2686228752\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (4.41135692596,9.62019518751), test loss: 3.2067935884\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (98.6854553223,46.0317281141), test loss: 32.900334692\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.71204447746,9.41674731499), test loss: 3.23589538336\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (20.01146698,45.8637943779), test loss: 28.3055290461\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.20399618149,9.22556813028), test loss: 3.22666795254\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (5.85383892059,45.6948933745), test loss: 30.7645646095\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.98161745071,9.04585548785), test loss: 3.03009414375\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (59.1206970215,45.5288659598), test loss: 32.4484072208\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.42719984055,8.87603027431), test loss: 3.05775605142\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (56.6700363159,45.3619046753), test loss: 33.2466034889\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (6.72875356674,8.71580476089), test loss: 3.34380274415\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (38.2600402832,45.1899585861), test loss: 31.251503706\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.281783819199,8.56386639682), test loss: 3.26895955503\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (21.1965999603,45.0119861494), test loss: 27.3483767271\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.83007574081,8.41991359531), test loss: 3.21426720321\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (30.1504096985,44.832008582), test loss: 30.2731612206\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.04467201233,8.28278606323), test loss: 3.00771124065\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (13.4174995422,44.6512880823), test loss: 31.7611026287\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.854991436,8.15183569987), test loss: 3.21110318005\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (63.7416229248,44.4700836008), test loss: 32.1077656269\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (4.01571464539,8.02709548872), test loss: 3.29825406969\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (34.8723258972,44.2807722562), test loss: 27.7420379162\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (5.590239048,7.90817035443), test loss: 3.22542360425\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (39.4986190796,44.0874089621), test loss: 26.961060667\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.74598181248,7.79408752985), test loss: 3.09656315446\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (23.8854656219,43.8894846791), test loss: 29.3129426003\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (5.74638032913,7.68520904985), test loss: 2.93014514446\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (34.796672821,43.6922657828), test loss: 30.588958168\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.07498455048,7.58077813667), test loss: 3.21138010025\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (21.7119007111,43.4864469457), test loss: 30.0228857517\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.17919802666,7.48056569501), test loss: 3.25805385113\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (20.3723831177,43.2768435104), test loss: 25.5619790554\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.758868694305,7.38450339981), test loss: 3.25091129243\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (25.2115516663,43.0651430163), test loss: 26.8163575172\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (10.5523967743,7.29269156324), test loss: 3.04923774004\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (9.66117858887,42.8496820106), test loss: 28.9968272448\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.25677800179,7.20401949696), test loss: 3.06367767155\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (23.9211769104,42.6321746623), test loss: 30.5976504326\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.65026831627,7.11871134646), test loss: 3.35034004152\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (49.1529998779,42.413262322), test loss: 27.9072194099\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (5.6442193985,7.03663460123), test loss: 3.3044144839\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (45.8266830444,42.1934636899), test loss: 24.6921941757\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.28146743774,6.95745643643), test loss: 3.24902935326\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (31.3979969025,41.9706728778), test loss: 26.5287910938\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.61620593071,6.88103704121), test loss: 3.03846584857\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (20.4375476837,41.7459154665), test loss: 29.1273238897\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.22234487534,6.80719296997), test loss: 3.21008229256\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (19.158662796,41.5199140015), test loss: 29.8109698772\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (5.81753349304,6.73588629771), test loss: 3.36946187615\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (48.3347740173,41.294801041), test loss: 25.6951538801\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.1240530014,6.66694443808), test loss: 3.29073216915\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (29.9655609131,41.0691510063), test loss: 25.0985493183\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.90894842148,6.60025010061), test loss: 3.11433693171\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (90.1836090088,40.8458659091), test loss: 26.9854829073\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.71680307388,6.53557175177), test loss: 2.95125809014\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (26.4817371368,40.6209299027), test loss: 28.9436532974\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (5.51193618774,6.47303059704), test loss: 3.2327260077\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (3.83652186394,40.3967657399), test loss: 28.2800340176\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.08139944077,6.41232363072), test loss: 3.31788370907\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (31.5116710663,40.1749862773), test loss: 24.6669166088\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.39348125458,6.35336119547), test loss: 3.27322463691\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (29.8725700378,39.9539899287), test loss: 25.2017717838\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.95375943184,6.29621589484), test loss: 3.05129415989\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (25.6548843384,39.7346600883), test loss: 27.3656645298\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.826705873013,6.24055708046), test loss: 3.08000115752\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (6.70274734497,39.5152128773), test loss: 29.2196497917\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.36579573154,6.18655177076), test loss: 3.34713783562\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (53.8915252686,39.299012456), test loss: 26.5201955795\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.47515678406,6.13408948379), test loss: 3.3000696063\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.4957675934,39.0836971555), test loss: 24.0341718674\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.84079742432,6.08318891707), test loss: 3.20911600292\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (28.972820282,38.8715001257), test loss: 25.1143204451\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.87425732613,6.03363640611), test loss: 3.02583613396\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (49.3076515198,38.660686966), test loss: 27.4171253204\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (5.59118795395,5.98561824422), test loss: 3.20714227557\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (54.4595756531,38.4513659019), test loss: 28.4995728493\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.52534198761,5.93860036115), test loss: 3.3495087117\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (47.0841064453,38.243387799), test loss: 24.6528458357\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (5.02749013901,5.89286544539), test loss: 3.33569594026\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (15.4688920975,38.0370264194), test loss: 24.1218446016\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.884101092815,5.84820429755), test loss: 3.12244251966\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.4394893646,37.8317559307), test loss: 25.8131149292\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.71388983727,5.80470564958), test loss: 2.97457338572\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (14.871088028,37.6282808189), test loss: 27.2545183659\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.571395397186,5.76218690926), test loss: 3.2612265408\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (21.6360549927,37.4267022584), test loss: 27.0190880299\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (5.70316410065,5.72106836648), test loss: 3.41283979118\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.4581718445,37.2270166359), test loss: 23.6773634911\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.65937590599,5.68070238951), test loss: 3.31540878415\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (24.7123947144,37.0289811858), test loss: 23.9945981741\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.33939528465,5.64124882734), test loss: 3.06298382282\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (55.7937202454,36.8336968656), test loss: 25.8564089775\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.73747372627,5.60281352443), test loss: 3.09827237725\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (18.4837932587,36.6392675359), test loss: 27.568034935\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.28442025185,5.56519541556), test loss: 3.40530629158\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (10.7970275879,36.4459323332), test loss: 25.0533575058\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.50159752369,5.52845803952), test loss: 3.37864083946\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (9.60674858093,36.2543552956), test loss: 23.0483598232\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.24135398865,5.49249261141), test loss: 3.23347253203\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (7.03244113922,36.0641665401), test loss: 24.3315181255\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (5.22179174423,5.45738647803), test loss: 3.08286181688\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (41.9705581665,35.8768050444), test loss: 25.8785450459\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (3.16620898247,5.42300828265), test loss: 3.21449077725\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (14.5067405701,35.689859968), test loss: 27.2313836575\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.08440494537,5.38940771547), test loss: 3.39594710767\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (32.5988960266,35.5053365927), test loss: 23.5503211498\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.06735289097,5.35642852423), test loss: 3.38434842825\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (26.9311256409,35.3215659956), test loss: 23.149600172\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (4.78255462646,5.32424139935), test loss: 3.11368691325\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (10.6987218857,35.1387660996), test loss: 25.2008390665\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.48615193367,5.2926866173), test loss: 2.97354441285\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (22.7046146393,34.9576185004), test loss: 26.1178660393\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.47138702869,5.26170747726), test loss: 3.26290017962\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (11.7509040833,34.7775911274), test loss: 25.5752730131\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.740582406521,5.23140103698), test loss: 3.43767809272\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (6.98345184326,34.5989226581), test loss: 23.0214037657\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.71610057354,5.20161317512), test loss: 3.36190952957\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (7.1653380394,34.4207952315), test loss: 23.4793185234\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.14495515823,5.17248254758), test loss: 3.06538726687\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (48.486541748,34.2450726746), test loss: 24.9351754189\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.76919269562,5.1439321863), test loss: 3.11671513319\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (13.2629470825,34.0695301303), test loss: 26.5712692738\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.22821903229,5.11595994639), test loss: 3.38846776485\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (22.3706111908,33.8959823799), test loss: 24.5669077873\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.67104458809,5.08855869738), test loss: 3.39437329173\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (24.5186786652,33.7230032834), test loss: 22.9910593271\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.50048542023,5.06170243382), test loss: 3.27769261599\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (41.5453033447,33.551443435), test loss: 23.7798918009\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.70916175842,5.0352222336), test loss: 3.08347713351\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (30.9930610657,33.3803025771), test loss: 25.030455637\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (3.72641038895,5.00925148337), test loss: 3.21101671457\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (19.8932628632,33.2101060779), test loss: 26.5876508236\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.720328092575,4.98368559994), test loss: 3.40536415577\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (13.8786449432,33.0407164006), test loss: 23.4405402184\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.35775732994,4.95861696354), test loss: 3.4054846853\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (15.8993730545,32.8719303264), test loss: 23.1129373074\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.6741335392,4.93393847122), test loss: 3.18688542843\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (24.1908321381,32.7044314131), test loss: 24.4410904408\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (4.79529380798,4.90988063636), test loss: 3.03154590428\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (12.4904966354,32.5373143832), test loss: 25.6979802132\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (3.57522511482,4.88609662135), test loss: 3.3057851851\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (9.7091550827,32.3713434537), test loss: 25.5208663464\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.66745817661,4.86267405068), test loss: 3.50777329803\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (39.8169212341,32.2066836897), test loss: 22.852856493\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.28047204018,4.83970365836), test loss: 3.42041516304\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.1346607208,32.0420741696), test loss: 23.3218547821\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.25545454025,4.81707893422), test loss: 3.1378587842\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (11.0104484558,31.8783068189), test loss: 24.9464782476\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.32244575024,4.79481886835), test loss: 3.14367315769\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (7.877805233,31.715149867), test loss: 26.3639560223\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.65810930729,4.7728953573), test loss: 3.43900992274\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.55576133728,31.5528655602), test loss: 23.9704302311\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (4.48357200623,4.75136766195), test loss: 3.39734109044\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (9.51090908051,31.3917282442), test loss: 22.4644766808\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.856876969337,4.7301311772), test loss: 3.26325126886\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.3229665756,31.2308958268), test loss: 23.8137974024\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.0985994339,4.70926794499), test loss: 3.09978007674\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (16.3622360229,31.0712930711), test loss: 24.7069367886\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.04524457455,4.68865305105), test loss: 3.20502861142\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (16.6044387817,30.9120143158), test loss: 26.3298340321\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.6343460083,4.66842364678), test loss: 3.35356625021\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (10.991645813,30.753138416), test loss: 23.4337358236\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.79399740696,4.64845763933), test loss: 3.34095261395\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (9.42121505737,30.595245236), test loss: 22.7711799622\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.635981440544,4.62874486322), test loss: 3.09998520613\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (12.6100263596,30.4379840699), test loss: 24.5672320843\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.388932943344,4.60934081375), test loss: 3.02165002823\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (3.74747228622,30.2813151502), test loss: 26.0967837334\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.39235925674,4.59017501379), test loss: 3.34357013404\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (8.50118923187,30.1252790137), test loss: 25.7987963676\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.37517023087,4.5713198286), test loss: 3.49597922564\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (27.1948127747,29.9703326668), test loss: 23.653869462\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.11741447449,4.55272687263), test loss: 3.42039943933\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (15.7744274139,29.8158091659), test loss: 23.9558701038\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.80448675156,4.53443627647), test loss: 3.08589273095\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (13.4604253769,29.662062181), test loss: 24.9982591152\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (6.31465244293,4.51643042939), test loss: 3.16470760703\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.14669752121,29.508995508), test loss: 27.3524359226\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.48579704762,4.49861107612), test loss: 3.48103676736\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (13.9567451477,29.3566864281), test loss: 24.6884947538\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.87744605541,4.48099111438), test loss: 3.39390105307\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (21.7929267883,29.2049176466), test loss: 23.2760528088\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.36495471001,4.4636438743), test loss: 3.24840269089\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (7.18970775604,29.0537374677), test loss: 24.3435276031\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.06547653675,4.44644503052), test loss: 3.09139488339\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (11.9098148346,28.9031753527), test loss: 25.3749702454\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (3.26167941093,4.42948787595), test loss: 3.20106819272\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (9.55527114868,28.7533251198), test loss: 28.0462785244\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (3.03272294998,4.41272682923), test loss: 3.45337990224\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (10.5444316864,28.6042118421), test loss: 24.4580104828\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.87763762474,4.39631527732), test loss: 3.40287124515\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (5.14525699615,28.4557075299), test loss: 23.4904877186\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.6183975935,4.37998236023), test loss: 3.11912602782\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.6374797821,28.3081303551), test loss: 24.9343885183\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.99321556091,4.3638448172), test loss: 3.04433346391\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (12.7330284119,28.1613437897), test loss: 27.3264530182\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.34156608582,4.34791709574), test loss: 3.37474275231\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.36093759537,28.0151115739), test loss: 27.1916921616\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.98904919624,4.33217225374), test loss: 3.58917678595\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (3.74274277687,27.8695076519), test loss: 24.6606722355\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.08018445969,4.31658881566), test loss: 3.43398453593\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (7.73113250732,27.7247885277), test loss: 25.1284671783\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.50546371937,4.30118882354), test loss: 3.13219167888\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (7.08245325089,27.5809365236), test loss: 27.957055521\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.47026920319,4.28600327627), test loss: 3.13582411408\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (3.01631689072,27.4378125628), test loss: 28.6871191025\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.03561234474,4.27095443321), test loss: 3.4647184968\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (4.88048601151,27.2954517635), test loss: 25.9994794369\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.09434080124,4.25608614134), test loss: 3.40385695696\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (8.28682518005,27.1540871021), test loss: 24.5387274265\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.66032838821,4.24134224727), test loss: 3.25853136778\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.6250333786,27.0134122479), test loss: 26.9066016674\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.57699394226,4.22681129141), test loss: 3.05106197\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (8.08332920074,26.8735077655), test loss: 26.7753896713\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.44193482399,4.2124110771), test loss: 3.1579272449\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (5.09102916718,26.734426207), test loss: 28.6804987907\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.860798835754,4.19814021088), test loss: 3.38059387803\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (12.1743612289,26.5963802167), test loss: 26.1446543694\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.754787921906,4.18403077978), test loss: 3.4520092994\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (3.98303961754,26.4589738675), test loss: 24.9313670158\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.38912272453,4.1700543522), test loss: 3.13709352314\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (8.83029842377,26.322474595), test loss: 26.8761079788\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.09424376488,4.15624050959), test loss: 3.01808684468\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (5.67526817322,26.1870335332), test loss: 29.1806237698\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.04381906986,4.14257293387), test loss: 3.39436835647\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (18.3539733887,26.0524470031), test loss: 29.2492669106\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.4359998703,4.12909244609), test loss: 3.58719126582\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (10.1524515152,25.9187356783), test loss: 26.4355831623\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (5.95517158508,4.11575604735), test loss: 3.48331071734\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.16702699661,25.7858117061), test loss: 26.6481273651\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.52018904686,4.10252153216), test loss: 3.14076233208\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (5.31632423401,25.6538786034), test loss: 27.9575940609\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.54704141617,4.08938066255), test loss: 3.15563768744\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (9.89218330383,25.5228929535), test loss: 30.5325057507\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.21377801895,4.07641116363), test loss: 3.47121548057\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.01101493835,25.3926252918), test loss: 27.6849981546\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.11332035065,4.06349630433), test loss: 3.4027831167\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.11194968224,25.2633555724), test loss: 26.5070547581\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.744573533535,4.05072007023), test loss: 3.28852406144\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (6.63685750961,25.1349842873), test loss: 27.3581268549\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.16807222366,4.03807520095), test loss: 3.1435660392\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (9.8671131134,25.007574899), test loss: 28.0405437946\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.95463132858,4.0256295955), test loss: 3.19070088267\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.78486251831,24.8811195382), test loss: 30.5048667908\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.62506723404,4.01323363754), test loss: 3.40856341422\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.26224994659,24.7555294606), test loss: 27.6796221972\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.43124032021,4.00093907696), test loss: 3.48102414012\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (6.422311306,24.631017083), test loss: 26.3095811844\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.10304379463,3.98876032464), test loss: 3.1949290067\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (3.2630417347,24.5072500565), test loss: 28.0059151649\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.78610086441,3.97669850999), test loss: 3.06561967731\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (4.09546852112,24.3844552672), test loss: 31.6810269356\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.31031608582,3.96473460332), test loss: 3.41679120958\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.53079605103,24.2626512025), test loss: 30.6862880707\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.16065251827,3.95287416394), test loss: 3.63422177434\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (3.58999490738,24.1417597269), test loss: 28.1855966568\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.21009516716,3.94115493109), test loss: 3.46531122327\n",
      "\n",
      "MC # 1, Hype # hyp3, Fold # 10\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (351.796264648,inf), test loss: 188.948426819\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (307.151794434,inf), test loss: 373.300621033\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (60.8245620728,124.204565899), test loss: 38.4056436539\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.47596073151,203.632163305), test loss: 5.37324544191\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (28.7739028931,82.9910383258), test loss: 40.792299366\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.2397942543,103.432351786), test loss: 3.02188971937\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (23.5719833374,69.1609605742), test loss: 40.0326037407\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.76624441147,70.0056038614), test loss: 3.25598538518\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (46.0871810913,62.2528497462), test loss: 38.9231059551\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.97205972672,53.2892736036), test loss: 3.27037220597\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (84.5601425171,58.1040615906), test loss: 36.0405386925\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.50452041626,43.2581035426), test loss: 3.32716907263\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (26.056022644,55.3131705205), test loss: 38.6799681664\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.8715929985,36.5699657729), test loss: 3.12873133123\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (11.4330234528,53.2892802275), test loss: 41.5972802162\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.01560974121,31.7939212798), test loss: 3.08804756403\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (21.5208625793,51.7715522337), test loss: 39.2081158638\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.901918828487,28.2098862844), test loss: 3.35989380777\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (76.1083526611,50.6010336064), test loss: 36.7929659843\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.37272191048,25.4233959432), test loss: 3.30108590722\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (21.3344039917,49.6330488742), test loss: 36.0135042667\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.62900304794,23.1932502286), test loss: 3.39766070545\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (46.3286514282,48.8429215439), test loss: 39.7322363853\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.97652077675,21.3689155445), test loss: 3.08364478052\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (18.2932071686,48.1725546368), test loss: 39.5320685863\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.78188931942,19.848587191), test loss: 3.20375963449\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (49.8733444214,47.6084844392), test loss: 37.8184406757\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (8.88556575775,18.5639949734), test loss: 3.33743858337\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (35.8778457642,47.1032646019), test loss: 35.8575274467\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.63855326176,17.461370355), test loss: 3.37645320892\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (26.7321243286,46.6550976028), test loss: 37.0108762264\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (6.87774419785,16.5061302058), test loss: 3.10691480637\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (21.1863670349,46.2584081034), test loss: 40.594124651\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.00820446014,15.6693182983), test loss: 3.02726246417\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (19.0192375183,45.8971346324), test loss: 40.7539276123\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.72612118721,14.9312620671), test loss: 3.34232956767\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (29.8635463715,45.5695512406), test loss: 38.4079670429\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (5.7648859024,14.2751692584), test loss: 3.42372574806\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (21.9268722534,45.2704117781), test loss: 34.88416605\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.15808820724,13.6886345995), test loss: 3.44556699991\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (16.0933952332,44.9934721698), test loss: 38.6809683323\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.18062615395,13.1603402529), test loss: 3.02802546322\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (69.1435623169,44.740857256), test loss: 40.5649263382\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.24578666687,12.6822976793), test loss: 3.24885913134\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (64.1182403564,44.5058252763), test loss: 38.7135422707\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.61703062057,12.2473079785), test loss: 3.42232734263\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (31.7701702118,44.2811459395), test loss: 35.3921285152\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.34973573685,11.8494882769), test loss: 3.27754626572\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (20.4758720398,44.0648665939), test loss: 34.8812750816\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (7.49378967285,11.4851779688), test loss: 3.11277670562\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (40.5690765381,43.8617624143), test loss: 39.6823065758\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.35886716843,11.1494390801), test loss: 2.96315473318\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (52.4071464539,43.6680068513), test loss: 39.7663205147\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.73013854027,10.8395829812), test loss: 3.33127795458\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (17.2066802979,43.4837947099), test loss: 37.3104238033\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.96707582474,10.5523911715), test loss: 3.22721606791\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (7.26159381866,43.2975071475), test loss: 34.3808773994\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.0576210022,10.2859522551), test loss: 3.23248354197\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (35.1369171143,43.118988929), test loss: 37.1722657204\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.26288199425,10.037327438), test loss: 3.01770815253\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (34.2529029846,42.9450669638), test loss: 38.6951441765\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.776445746422,9.80544381953), test loss: 3.11316393018\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (17.497095108,42.7742177353), test loss: 37.2356962204\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.65189695358,9.588153231), test loss: 3.2954248786\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (38.2071990967,42.6033805908), test loss: 34.6277608871\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.28708767891,9.38446596436), test loss: 3.21190982759\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (14.2254219055,42.4337807391), test loss: 32.8493560791\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.87494373322,9.19300983475), test loss: 3.25162052214\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (44.5631523132,42.2663654254), test loss: 36.1758217812\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.08840465546,9.01310945084), test loss: 2.94070680737\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (21.1108608246,42.0949420166), test loss: 36.8083550453\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.5941400528,8.84337157407), test loss: 3.14493027925\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (29.4248199463,41.9221426427), test loss: 35.84389081\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.7999727726,8.68252091904), test loss: 3.18499437273\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (18.0953216553,41.7489319175), test loss: 32.9616838455\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.977292895317,8.52997816429), test loss: 3.25297320485\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (18.5573425293,41.5727506432), test loss: 33.4394475937\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.20659685135,8.38480164082), test loss: 3.09080021977\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (20.4437332153,41.3931473163), test loss: 36.6398491859\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.63774871826,8.24670245236), test loss: 3.01728985012\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (27.0960159302,41.2088056092), test loss: 35.8562987328\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.13128399849,8.11578412508), test loss: 3.33696188331\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (13.4983673096,41.0185876246), test loss: 33.9030351639\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.07386445999,7.99072070389), test loss: 3.31217182577\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (39.5315742493,40.824194199), test loss: 31.1064730644\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.72238731384,7.87142897216), test loss: 3.38349739313\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (34.2223968506,40.6220676456), test loss: 33.3455622196\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.2509059906,7.75773594336), test loss: 3.03631615043\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (16.1065216064,40.4121285244), test loss: 34.7774588108\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.48628687859,7.64884591757), test loss: 3.1828759253\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (21.3482437134,40.1980258526), test loss: 34.0644730568\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (5.95724964142,7.54464006849), test loss: 3.36058863401\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (40.6185302734,39.9820309433), test loss: 31.3162350178\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.7871491909,7.44461211439), test loss: 3.33268164098\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (27.8967514038,39.7633582547), test loss: 30.8208460808\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.57785701752,7.34867240459), test loss: 3.08088177145\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (10.9748125076,39.5453906527), test loss: 32.8383740425\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.88564109802,7.2563895781), test loss: 2.96648784876\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (26.2887306213,39.3226091882), test loss: 34.5454486847\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.54086089134,7.16770502651), test loss: 3.28866011798\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (55.8779678345,39.0993265757), test loss: 33.6811133862\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.85494351387,7.08220278968), test loss: 3.26845813096\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (27.8189201355,38.8756555536), test loss: 29.297606039\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.22275686264,6.99972991379), test loss: 3.37395376265\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (18.5834159851,38.6524351272), test loss: 30.6560534954\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.42734956741,6.92000019389), test loss: 3.01252485216\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (34.4042053223,38.4287007857), test loss: 33.6680931091\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.28631019592,6.84314941738), test loss: 3.24087123275\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (13.1710605621,38.2062929943), test loss: 34.1145537376\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.63535118103,6.7689422279), test loss: 3.44659275413\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (31.470539093,37.9840024256), test loss: 30.9791401863\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.02219963074,6.69723504141), test loss: 3.21028512716\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (16.9109535217,37.7619030544), test loss: 28.4926926374\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.19964027405,6.62811737612), test loss: 3.16551863849\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.0819911957,37.5406778782), test loss: 31.1697900295\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.02003931999,6.5609250243), test loss: 2.98658822179\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (20.5117034912,37.3203684305), test loss: 33.052347374\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.01640486717,6.49593599377), test loss: 3.29777829647\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (15.7579631805,37.1009106467), test loss: 32.4772894859\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.35267269611,6.43283573105), test loss: 3.23335128129\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.541349411,36.8831819163), test loss: 28.5464157104\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.6756772995,6.37172341703), test loss: 3.31605503261\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (20.4789352417,36.6677965768), test loss: 29.09385252\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.78353834152,6.31286174966), test loss: 3.05829259753\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.3994045258,36.4537951475), test loss: 31.2875546455\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.28267812729,6.25540446367), test loss: 3.11615383029\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (32.4778518677,36.242150193), test loss: 31.8792821407\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.67960786819,6.19966325914), test loss: 3.35196574032\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (14.178814888,36.0322330724), test loss: 30.3536705971\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.99432611465,6.14555575247), test loss: 3.30279666781\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (14.5296154022,35.8238725021), test loss: 27.2751964092\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.958091557026,6.09291743792), test loss: 3.38400443196\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (16.1948432922,35.6169599131), test loss: 29.1558628798\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (4.35053825378,6.04180400777), test loss: 3.0791983664\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (28.7329978943,35.4126348851), test loss: 30.731539917\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.38359224796,5.99207277443), test loss: 3.2096331656\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (9.81295871735,35.2094531546), test loss: 32.2929184437\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.86407613754,5.94368512671), test loss: 3.3561752975\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (27.1995849609,35.0100069874), test loss: 29.6901347637\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.16422367096,5.89652761759), test loss: 3.42175219953\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (22.9239768982,34.8104137105), test loss: 28.0240636349\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.25319314003,5.85065809034), test loss: 3.10483249128\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (24.5833625793,34.6120218211), test loss: 29.5740730286\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.30834174156,5.80589208664), test loss: 3.02006788254\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (24.632358551,34.4161048589), test loss: 30.513132906\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (4.48899459839,5.76232433346), test loss: 3.35778591335\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (18.8459453583,34.2218151356), test loss: 30.6963975906\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.06748747826,5.71969183241), test loss: 3.41274269521\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (5.44102573395,34.0281303732), test loss: 26.8611835957\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.53967452049,5.67823782166), test loss: 3.473950845\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (58.9654197693,33.8381576629), test loss: 27.2774461269\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.831086993217,5.63773756691), test loss: 3.09106245637\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (23.5839576721,33.6482386428), test loss: 29.9839300632\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.80625629425,5.59836838758), test loss: 3.28706883192\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (43.3615913391,33.460696762), test loss: 31.9627378941\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (5.58145236969,5.56004334991), test loss: 3.5139698863\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (29.8604316711,33.2743717905), test loss: 28.9661370993\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.60001087189,5.52235491632), test loss: 3.43175907731\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (18.9956474304,33.0888903423), test loss: 27.2883913517\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.63047933578,5.48559774895), test loss: 3.16623604894\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (15.7359342575,32.9048686949), test loss: 28.891997242\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.55809187889,5.4495392156), test loss: 3.02107238173\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (13.0131435394,32.7219982728), test loss: 29.5068947315\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.942685425282,5.41431181926), test loss: 3.36038585603\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (17.0104694366,32.5410030279), test loss: 30.8206316471\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (3.91046237946,5.38009969041), test loss: 3.34938879609\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (14.5651893616,32.3609910101), test loss: 26.5775332928\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.3721177578,5.3464425974), test loss: 3.38202699721\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (12.8025875092,32.1828122491), test loss: 27.4347061157\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.08626556396,5.31346177255), test loss: 3.08101734221\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (13.4240159988,32.0058697391), test loss: 28.5748826504\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.15914487839,5.28124395304), test loss: 3.23749009371\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (10.7239522934,31.829936166), test loss: 29.5430349827\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.0704395771,5.24965456535), test loss: 3.46070407629\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (14.5181560516,31.6552792363), test loss: 28.6224433661\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.21238350868,5.21869359104), test loss: 3.33586236238\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (20.6060447693,31.4821283996), test loss: 26.0597180367\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.46959292889,5.18841107843), test loss: 3.24078742266\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (17.0685825348,31.310062468), test loss: 28.2400202751\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.25301718712,5.15871357263), test loss: 3.06069943607\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (27.0265426636,31.1400080499), test loss: 29.0317396164\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.4231607914,5.12952813892), test loss: 3.32443791032\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (26.2822647095,30.9703074181), test loss: 30.5268151522\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.77356791496,5.10095758097), test loss: 3.27860734761\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (15.8204174042,30.8013971241), test loss: 26.5749998093\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.1353828907,5.07291642776), test loss: 3.35366016924\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (15.4794979095,30.6340030472), test loss: 27.2894749165\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.32709550858,5.04541743183), test loss: 3.04071136564\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (12.5600833893,30.4676167168), test loss: 28.6935387135\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.591297268867,5.01831188745), test loss: 3.09571734369\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.17127895355,30.3016396632), test loss: 29.5337447643\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.20410108566,4.99179656923), test loss: 3.37448123693\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (36.3917732239,30.1377455138), test loss: 29.6518698931\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.82625424862,4.965743948), test loss: 3.35708219707\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (20.7163009644,29.9740727098), test loss: 25.5350130558\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.2259554863,4.94022982997), test loss: 3.44457474351\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (27.8771934509,29.8118437577), test loss: 27.6866206169\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (5.3464307785,4.91526047483), test loss: 3.1298499316\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (41.3707580566,29.6504109017), test loss: 28.4287306309\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (3.17950868607,4.89055068889), test loss: 3.21465074122\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (7.78469467163,29.4893804805), test loss: 31.7943652153\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.86629056931,4.86631549713), test loss: 3.45020856261\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (4.87321281433,29.3292696932), test loss: 29.2193518162\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.70256209373,4.84237124326), test loss: 3.45913536549\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (6.6637172699,29.1700703745), test loss: 27.1090096474\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.694894790649,4.81885253453), test loss: 3.15609456003\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (14.0721712112,29.0118397068), test loss: 28.7017700195\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (5.16322803497,4.79587268206), test loss: 3.04683054686\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.5579051971,28.854261941), test loss: 30.3927378178\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (3.29834842682,4.77310184904), test loss: 3.3770226717\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (6.55034446716,28.697647112), test loss: 31.3553636551\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.30791401863,4.75065990862), test loss: 3.5541745007\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (8.30326461792,28.5418461747), test loss: 26.6610856533\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.77470874786,4.72862570335), test loss: 3.5435842216\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (11.1681060791,28.3867606417), test loss: 27.9631779194\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.39742875099,4.70692069248), test loss: 3.10923354626\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (11.3285427094,28.2324044255), test loss: 29.4410879135\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.45051789284,4.68551574502), test loss: 3.28784096241\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (17.2067489624,28.0791784711), test loss: 32.4479728222\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.03412699699,4.66449668719), test loss: 3.53090810776\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.3343095779,27.9267075359), test loss: 30.0869011879\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.66493463516,4.64374924476), test loss: 3.50003020465\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (23.1081733704,27.7751745787), test loss: 27.3594089508\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.591406703,4.62330305052), test loss: 3.21122879684\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (18.7598953247,27.6245173943), test loss: 30.1768902779\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (4.7167930603,4.60318887157), test loss: 3.02248535156\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (11.8460178375,27.474371214), test loss: 30.8193974972\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.96589803696,4.58334458379), test loss: 3.37217916846\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.5403842926,27.3251863854), test loss: 32.3098340511\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.31705105305,4.563751925), test loss: 3.35333848298\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (8.11721992493,27.1768982933), test loss: 27.6449121475\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.798767387867,4.54443452259), test loss: 3.3996571973\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (4.97636795044,27.029067438), test loss: 29.2044007778\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.782667934895,4.52537488263), test loss: 3.07704527676\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (13.8346252441,26.8824592575), test loss: 28.5101100922\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.90907263756,4.50665003573), test loss: 3.15625026226\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (11.0870923996,26.7365646406), test loss: 30.5460231781\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.00059556961,4.48816585488), test loss: 3.46725795567\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (10.7649402618,26.5915378242), test loss: 31.1475064754\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.37645864487,4.47001248478), test loss: 3.39038672\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (18.0470485687,26.4474529245), test loss: 27.3440849304\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.01901245117,4.45201874181), test loss: 3.2681134969\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (6.1840262413,26.304053331), test loss: 30.2249504089\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (4.2027759552,4.43426034463), test loss: 3.0490155071\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.88976287842,26.1613389613), test loss: 29.6061363697\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.20289301872,4.41668995651), test loss: 3.22729894817\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (2.08260369301,26.0194292247), test loss: 33.2358059883\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.430425703526,4.39934019127), test loss: 3.38711728454\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (6.80969285965,25.8784682486), test loss: 29.1440383911\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (8.43575286865,4.38233146061), test loss: 3.41414967179\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (9.19969749451,25.7383496735), test loss: 29.5556655407\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (3.17805194855,4.36545005969), test loss: 3.09055118412\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (14.4240217209,25.5990837733), test loss: 30.6578076839\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.01660966873,4.34874058569), test loss: 3.06636297107\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (7.08140516281,25.4606669285), test loss: 31.1299723625\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.30673742294,4.33227954972), test loss: 3.47010008991\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (6.87672615051,25.3230277649), test loss: 32.3383972645\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.87923622131,4.31600973606), test loss: 3.42307061553\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (6.79501152039,25.1861871271), test loss: 27.7178126335\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.72086060047,4.29990219904), test loss: 3.47832797468\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (5.53212690353,25.0503664919), test loss: 31.0421783924\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.04959988594,4.28403404582), test loss: 3.11938274503\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (3.00995016098,24.9153266885), test loss: 30.173754549\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.65616631508,4.26830525712), test loss: 3.23370684385\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (17.2172889709,24.7813425036), test loss: 34.5812089443\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.92922258377,4.25276211044), test loss: 3.54117293954\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (10.7733573914,24.6483486179), test loss: 32.3343013287\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.85127925873,4.23739705623), test loss: 3.51460556686\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (4.69132709503,24.5160414223), test loss: 30.2774374008\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.71015310287,4.22224422731), test loss: 3.13635320067\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (5.89428424835,24.3847315649), test loss: 31.3942685604\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.689700245857,4.20719669375), test loss: 3.02962511182\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (15.2172489166,24.2544032493), test loss: 32.2005079269\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.99329072237,4.19233959021), test loss: 3.40871473551\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (2.73476076126,24.1247434005), test loss: 34.9029485226\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.49180567265,4.17763209929), test loss: 3.56204038709\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (10.1384344101,23.9961543262), test loss: 29.1096709728\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.26655292511,4.16313626656), test loss: 3.52169716954\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (4.16211986542,23.8684544694), test loss: 30.2224480629\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.20766496658,4.14880400785), test loss: 3.10823256373\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (9.39808750153,23.7417658142), test loss: 30.959201622\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (5.88230419159,4.13466259341), test loss: 3.22801354527\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (3.82804584503,23.6158981724), test loss: 35.3176645756\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.17048764229,4.12062313292), test loss: 3.59124036878\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.42083311081,23.4911464502), test loss: 33.07603302\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (4.681432724,4.10675618141), test loss: 3.42122252584\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.87181377411,23.3671155808), test loss: 29.3541763306\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.391319900751,4.09297748878), test loss: 3.17575307786\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (2.90782427788,23.2440488439), test loss: 31.5830960751\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.4980237484,4.07935149016), test loss: 3.03088994026\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (6.96143341064,23.1219551344), test loss: 32.6580271244\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (5.84265184402,4.06591372164), test loss: 3.40985267162\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.69833087921,23.0008136266), test loss: 36.0048182011\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.13853514194,4.05259042368), test loss: 3.41602965295\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (9.28853702545,22.8806844941), test loss: 31.2957019806\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.59188175201,4.03939983672), test loss: 3.38853198886\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (8.0986328125,22.7615012264), test loss: 30.8824063778\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.14032173157,4.02633620595), test loss: 3.13343995214\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (2.97588968277,22.6431598256), test loss: 31.1221485615\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (3.05660820007,4.01341596027), test loss: 3.14105682969\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.14101457596,22.5257867593), test loss: 33.8682260036\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.99499464035,4.00060157416), test loss: 3.51122196913\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.70648813248,22.4094333084), test loss: 34.5702584743\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.00228333473,3.98794411196), test loss: 3.46134723574\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (2.75452136993,22.2940001915), test loss: 29.8713560581\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.75548768044,3.97538011007), test loss: 3.39433677495\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.59413385391,22.1794868909), test loss: 32.0336625099\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.76429629326,3.96293859148), test loss: 3.0656468004\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.92324495316,22.0660734064), test loss: 31.6102358818\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.965949594975,3.95059907915), test loss: 3.20539858639\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (1.75752711296,21.9535042746), test loss: 37.1971196175\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.80642986298,3.93843112747), test loss: 3.40938774943\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (4.26164340973,21.8418684842), test loss: 33.1697425842\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.02048933506,3.9263369747), test loss: 3.38206391335\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.28676128387,21.7311904786), test loss: 31.6050932884\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.497789233923,3.91435272424), test loss: 3.19119231105\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (2.42522144318,21.6213395342), test loss: 34.1735359192\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.86217188835,3.90247518292), test loss: 3.04699890018\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (7.20306301117,21.512500006), test loss: 34.6718842506\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.94649553299,3.8907444175), test loss: 3.47123687118\n",
      "run time for single CV loop: 6865.53636384\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 3\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (273.646148682,inf), test loss: 169.966918182\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (323.444580078,inf), test loss: 367.303872681\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (46.6029701233,96.3628015156), test loss: 37.9113824368\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (17.7635364532,204.33146437), test loss: 11.0689370871\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (29.439535141,70.2570072212), test loss: 40.2880869389\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.87438583374,103.936447208), test loss: 3.26719216108\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (19.0854721069,61.4434157661), test loss: 39.2177373886\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.61419451237,70.2604165726), test loss: 3.182215783\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (45.8064994812,57.0306773493), test loss: 35.7551294804\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (8.28133583069,53.4266193065), test loss: 3.24671243727\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (32.3906173706,54.3308814436), test loss: 39.144713068\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.25030708313,43.324647287), test loss: 3.0172753334\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (10.3746776581,52.4766287567), test loss: 39.9233011723\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.41707491875,36.5923902179), test loss: 3.2233984828\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (51.527015686,51.1953937532), test loss: 37.8933320045\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.64124679565,31.7837929003), test loss: 3.32159335017\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (56.5587997437,50.1954795138), test loss: 34.0634105682\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.32739710808,28.1778105021), test loss: 3.23557422161\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (48.6164550781,49.3998261253), test loss: 39.4800812721\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (6.34749412537,25.3735375764), test loss: 2.9108934015\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (21.0336151123,48.751364694), test loss: 39.5515237808\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.32581758499,23.1285732112), test loss: 3.28169724643\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (30.7175369263,48.2138073521), test loss: 37.2826057434\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.02461194992,21.2947702566), test loss: 3.23767639399\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (39.8392601013,47.7499154065), test loss: 35.1570520401\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (8.70070362091,19.7670276201), test loss: 2.98499185443\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (32.0156936646,47.3470759113), test loss: 39.716706419\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.78173875809,18.4730376169), test loss: 2.98432209194\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (59.326461792,46.9966673968), test loss: 39.3277523518\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.23114681244,17.3646921409), test loss: 3.36405406892\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (12.115436554,46.6838383244), test loss: 35.8209314346\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.72177648544,16.4046495806), test loss: 3.16594789028\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (72.3474273682,46.397894682), test loss: 35.4866368294\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.05299329758,15.564060271), test loss: 2.96536886394\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (44.3844642639,46.127271872), test loss: 38.4746991158\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.43040990829,14.8243179315), test loss: 3.11259523034\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (21.082780838,45.871487414), test loss: 37.8948403835\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.41008281708,14.1661782228), test loss: 3.33568419516\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (20.362361908,45.6200990695), test loss: 34.2231303215\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.54125833511,13.5779958151), test loss: 3.1200042218\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (26.5889129639,45.4016319391), test loss: 36.5941312313\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.73015809059,13.0494984072), test loss: 2.99450599253\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (100.828491211,45.1929281174), test loss: 38.6271205425\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.55038976669,12.5722195275), test loss: 3.25588366985\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (35.1192550659,44.9776253462), test loss: 36.7913408279\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.30327987671,12.1385954327), test loss: 3.28220439553\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (41.9220657349,44.7739488922), test loss: 33.1749036312\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.78857851028,11.7426112985), test loss: 3.10629255772\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (88.808265686,44.5838177318), test loss: 35.4077007771\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (6.6404914856,11.3818643753), test loss: 2.98567673564\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (31.7976951599,44.3872592882), test loss: 37.0651934624\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.18487501144,11.0497165982), test loss: 3.2980573833\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (54.9595527649,44.2018696472), test loss: 36.3162159443\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.87886476517,10.7436681911), test loss: 3.23019938469\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (54.7849693298,44.0174296256), test loss: 32.7934400558\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (6.84189605713,10.461086267), test loss: 3.24262740016\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (24.0057907104,43.8340064883), test loss: 35.1528618336\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.86514806747,10.1988244302), test loss: 3.03043642342\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (35.6418151855,43.6552999097), test loss: 36.2511694908\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.0955760479,9.95484354314), test loss: 3.29933914542\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (27.1154670715,43.4740252959), test loss: 34.6593923569\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.41174805164,9.72841353488), test loss: 3.33456277847\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (23.6266212463,43.2917760532), test loss: 30.4959527493\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.83962368965,9.51653309387), test loss: 3.21414378881\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (58.1411170959,43.1066346323), test loss: 34.8735523701\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.59835529327,9.31848320277), test loss: 2.98633075058\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (30.8286972046,42.9270268836), test loss: 36.2849398136\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.7787232399,9.1327449099), test loss: 3.48213592768\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (25.6051445007,42.748377607), test loss: 33.3750433922\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.23446083069,8.95826481372), test loss: 3.30494820774\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (30.5379180908,42.5624658025), test loss: 31.1443122864\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.545161962509,8.7940450734), test loss: 3.06052842736\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (77.3443603516,42.3757628867), test loss: 34.2004362106\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (4.87123680115,8.63924672078), test loss: 3.12037016451\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (47.4507713318,42.1892661668), test loss: 34.4432024479\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.07789897919,8.49374665169), test loss: 3.48384584188\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (19.4181137085,41.9929879627), test loss: 30.7452507973\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.7640504837,8.35581024815), test loss: 3.1671310544\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (39.7236328125,41.8023827078), test loss: 30.3343791008\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.8996540308,8.22511105584), test loss: 3.14177031517\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (19.4639186859,41.606243245), test loss: 32.0535582066\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.60517406464,8.10146836794), test loss: 3.25995128453\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (50.183883667,41.4089126407), test loss: 33.1027397156\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.13114643097,7.98359218523), test loss: 3.40890806317\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (18.9153842926,41.2085569037), test loss: 29.5639468193\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.84463512897,7.87137366319), test loss: 3.20205637217\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (35.8221702576,41.0067982602), test loss: 30.2016014099\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.06280851364,7.76501192035), test loss: 3.12484737635\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (18.4933242798,40.7998530887), test loss: 32.3207404613\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.07763910294,7.66325012266), test loss: 3.35429371297\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (51.523147583,40.5917606123), test loss: 32.388208437\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.75109100342,7.56598824013), test loss: 3.37699872255\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (30.7019958496,40.3817185043), test loss: 28.2614173412\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (5.95629835129,7.47283286831), test loss: 3.2535482794\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (31.5889320374,40.1738387938), test loss: 28.7520617008\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.4383559227,7.38330225384), test loss: 3.08847071528\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.2527656555,39.9601212957), test loss: 30.7136786461\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.3659183979,7.2970417973), test loss: 3.36468853951\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (27.8302879333,39.7474876533), test loss: 31.223281002\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (9.29502296448,7.21403345512), test loss: 3.27753450871\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (16.6616077423,39.533823939), test loss: 27.0125791073\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.9999332428,7.13333842199), test loss: 3.30758816004\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (10.0853471756,39.3150658667), test loss: 28.2499159336\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.53062462807,7.05509413322), test loss: 3.05711176693\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (23.4960803986,39.1016506366), test loss: 30.4874390841\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.68170070648,6.97906156024), test loss: 3.27630847692\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (22.0803070068,38.8858956885), test loss: 29.3134738445\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.00570631027,6.90515740495), test loss: 3.35715834498\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (28.5736846924,38.6721052686), test loss: 25.8894761562\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (5.87097263336,6.8333720171), test loss: 3.21071615219\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (16.5590362549,38.4562805342), test loss: 28.5583618641\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.68770241737,6.76325225087), test loss: 2.92131001949\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (10.3705711365,38.2439815091), test loss: 30.0980276823\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.92657971382,6.69568741358), test loss: 3.37087852061\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (26.3493041992,38.0300891312), test loss: 28.3760122538\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (8.84440994263,6.6300745491), test loss: 3.28166234195\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (26.1262817383,37.8176773053), test loss: 26.6721334457\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.9328649044,6.56606851443), test loss: 3.03044108748\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (41.7850532532,37.6058206869), test loss: 28.1090386391\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.78558111191,6.5039626141), test loss: 3.03027866781\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.2775325775,37.3965889703), test loss: 29.0206410885\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.83246660233,6.4436577435), test loss: 3.34320776463\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (21.6489181519,37.1866737014), test loss: 27.202639842\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.95154905319,6.38490877184), test loss: 3.13573128283\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (43.1132087708,36.9805265284), test loss: 26.0621289015\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (5.35044765472,6.32829236128), test loss: 3.04206267297\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (23.9012889862,36.774908948), test loss: 26.9075819492\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.60291385651,6.27287361056), test loss: 3.07830295861\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (20.9252471924,36.5682866299), test loss: 29.286452198\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.75677263737,6.21905772608), test loss: 3.2585759908\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (17.1655597687,36.3651878728), test loss: 26.3191028118\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.06825256348,6.16663029724), test loss: 3.17696565986\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (47.5898551941,36.164932536), test loss: 26.1712393522\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.18939161301,6.11563553471), test loss: 3.02041585445\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (12.0278663635,35.9639646836), test loss: 27.6421182275\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.07676935196,6.06595528705), test loss: 3.13077172041\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (16.6349372864,35.7645234484), test loss: 29.3014342189\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.61068534851,6.01739204998), test loss: 3.23925358355\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (44.8351516724,35.5700755664), test loss: 25.4808618188\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (6.37083959579,5.97056635494), test loss: 3.21428073049\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (18.5898666382,35.3734286872), test loss: 25.6443602085\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.19163906574,5.92462987797), test loss: 2.96327957511\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (20.9473342896,35.17969239), test loss: 27.2254336357\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.63031673431,5.87975849278), test loss: 3.14004352987\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (6.10609912872,34.9867740501), test loss: 28.7560044765\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.95651865005,5.83594752019), test loss: 3.28836010695\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (8.53381443024,34.7963155086), test loss: 24.5638330698\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.16454029083,5.79320378158), test loss: 3.20402140021\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.5053138733,34.6065050478), test loss: 26.0039766312\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.89184856415,5.75132997843), test loss: 2.93331499696\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (14.5319919586,34.4201633824), test loss: 27.4114128351\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.54240167141,5.71084384468), test loss: 3.0717485249\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (12.8897628784,34.2347270839), test loss: 27.1635884523\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.09866857529,5.67096026433), test loss: 3.24905480444\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (18.3812408447,34.049464196), test loss: 24.5405859113\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.8121213913,5.63210899171), test loss: 3.08622573912\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (13.4964065552,33.8660594961), test loss: 26.7598446846\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.98256373405,5.59402054287), test loss: 2.88862087131\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.5591011047,33.6858529726), test loss: 27.4864258289\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.82560420036,5.55676312871), test loss: 3.21539530158\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (30.0150241852,33.5055229236), test loss: 27.0083732128\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.07278180122,5.52028772162), test loss: 3.23369256854\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (38.4274749756,33.3272292152), test loss: 25.4990823746\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (4.37146759033,5.48462807668), test loss: 2.97261411846\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (24.8292160034,33.1516542128), test loss: 26.6036338329\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.56487703323,5.44993919672), test loss: 2.94596529305\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (15.5541744232,32.9754274292), test loss: 27.7978115559\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.50998568535,5.41579422534), test loss: 3.21935881972\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (16.046459198,32.8014181847), test loss: 26.3471494198\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.35270690918,5.3822334328), test loss: 3.15744956732\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (10.5318117142,32.6282437755), test loss: 25.4647984266\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.88840150833,5.34940106805), test loss: 2.9920268178\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (17.7934093475,32.4571035392), test loss: 26.4149366379\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.08554410934,5.31714953346), test loss: 2.98896384537\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (12.8280239105,32.2868190182), test loss: 28.5887284279\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.16848015785,5.28544247101), test loss: 3.17893615365\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (47.6027297974,32.1192757923), test loss: 25.1322476506\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.25066709518,5.25474710188), test loss: 3.14228103161\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (11.4350662231,31.9517079406), test loss: 25.9468803406\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.52859830856,5.2243596046), test loss: 2.95444989949\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (22.6514129639,31.7856233213), test loss: 26.5949929714\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.34790754318,5.1946379159), test loss: 3.07831572145\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (15.576467514,31.6193800213), test loss: 28.8513083339\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.62056851387,5.1653506674), test loss: 3.22366900295\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (15.132068634,31.4558445469), test loss: 24.8585780859\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.89310503006,5.13663684638), test loss: 3.2059679389\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (10.1550836563,31.292189295), test loss: 25.9068053246\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.48432934284,5.10833810157), test loss: 2.93234262615\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (23.5703048706,31.1305271012), test loss: 26.9172915459\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.66237068176,5.08071602532), test loss: 3.05451823473\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.2323493958,30.9700781369), test loss: 28.6927271366\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.488957822323,5.05359663094), test loss: 3.27498035431\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (9.33170127869,30.8096664242), test loss: 24.5892279625\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.63602113724,5.02691587778), test loss: 3.17523087263\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (7.05779838562,30.6504970088), test loss: 27.0086762428\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.98526620865,5.00056467398), test loss: 2.88714705706\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (12.0742511749,30.4921061401), test loss: 27.2951667309\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.42540419102,4.97469477496), test loss: 2.99101195931\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (16.478729248,30.3350407487), test loss: 27.3721935272\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.6743042469,4.94920595448), test loss: 3.18739782274\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (16.1027832031,30.1784913304), test loss: 24.7635784388\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.51277077198,4.9240853058), test loss: 2.98453079462\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.21767044067,30.0237149492), test loss: 27.5427663565\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.79110336304,4.89967910715), test loss: 2.84068339169\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (9.8396730423,29.8690594868), test loss: 27.933579278\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (6.19423627853,4.87550244331), test loss: 3.15379146934\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (6.35943317413,29.7152685747), test loss: 28.0223982215\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.08360671997,4.85163350019), test loss: 3.27769245207\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (22.0075302124,29.5619290055), test loss: 26.2128137589\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (3.06888127327,4.82812222649), test loss: 2.9619050771\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (10.3222160339,29.4096632108), test loss: 28.5390012264\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.33350157738,4.8049759294), test loss: 2.89057418108\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (6.73513174057,29.2574754165), test loss: 28.8484949112\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.21745884418,4.78208948131), test loss: 3.19176510572\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (9.48565864563,29.1068659592), test loss: 27.4835273743\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.91952371597,4.75982912125), test loss: 3.21763896942\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (17.9902153015,28.9571676759), test loss: 26.6063088894\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.6460403204,4.73770742993), test loss: 2.93210881948\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (20.1372947693,28.8075404972), test loss: 26.8851285934\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.41842281818,4.71595859072), test loss: 2.97683032155\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (7.96816968918,28.658336658), test loss: 29.5416923523\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.62172937393,4.69446242163), test loss: 3.1931220293\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (27.2061309814,28.5104013638), test loss: 26.0198765755\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.03187203407,4.67326236683), test loss: 3.16993488073\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (10.727273941,28.3627352572), test loss: 27.6317687988\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.02194964886,4.65236385413), test loss: 2.89523053169\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (7.45596504211,28.2156051882), test loss: 27.3016578674\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.71147823334,4.63173185215), test loss: 3.05030440092\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (19.6393508911,28.0701961454), test loss: 29.9720130205\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (5.11807918549,4.61158797955), test loss: 3.22994835377\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (18.145236969,27.9247587578), test loss: 25.592944479\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.16245985031,4.59159560381), test loss: 3.23194749951\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (7.24830627441,27.7797551005), test loss: 26.9781514645\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.46241378784,4.57177053752), test loss: 2.95712545663\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (4.52437829971,27.6354069623), test loss: 27.6060415745\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.3993473053,4.5522366566), test loss: 3.03292151988\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (6.70567798615,27.4918568421), test loss: 29.6535095215\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (3.17054176331,4.5329694654), test loss: 3.28176191747\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (6.9885263443,27.3486797856), test loss: 25.6958078384\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.57801771164,4.51387230457), test loss: 3.18754439205\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (6.54064130783,27.2068247344), test loss: 27.989026165\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.25421440601,4.49524859038), test loss: 2.89585891366\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (9.29587745667,27.0656024343), test loss: 27.9899274349\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.06943821907,4.47669866916), test loss: 2.99962418675\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.8530163765,26.9247154467), test loss: 29.0290405273\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.91684699059,4.45843334148), test loss: 3.26815000474\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (12.3373165131,26.784339928), test loss: 26.1211597443\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.80873560905,4.44031825649), test loss: 2.995617643\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (13.2674045563,26.6450261472), test loss: 28.7666565418\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.98546862602,4.42242672042), test loss: 2.84411680698\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (13.335483551,26.5061953877), test loss: 29.0361601353\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.944931209087,4.40473977606), test loss: 3.13672888875\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (24.1434211731,26.3680133937), test loss: 29.7668613911\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (3.15780425072,4.38729342815), test loss: 3.32258502841\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (10.2421360016,26.2309925324), test loss: 27.8534984589\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.22872281075,4.37017297612), test loss: 2.95231802464\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (3.90022683144,26.0942120985), test loss: 29.0485182762\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.4594014883,4.3531434001), test loss: 2.89620557725\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (7.05625915527,25.9583163825), test loss: 30.486502552\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.434466362,4.33626542603), test loss: 3.27093215287\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.38772106171,25.8229238314), test loss: 28.8946098804\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.68100738525,4.31959697212), test loss: 3.26662219912\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (5.21442222595,25.6885090209), test loss: 28.7780068874\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.37185907364,4.30307668923), test loss: 2.91641200781\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (11.0324325562,25.5547265638), test loss: 28.355123806\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.844524025917,4.28670798114), test loss: 3.00241903663\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (10.5767831802,25.4219243984), test loss: 31.5740852833\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.744216680527,4.27072215217), test loss: 3.29946230054\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (7.01347732544,25.2898840618), test loss: 28.7395408154\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.559385538101,4.25476787982), test loss: 3.20619989932\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (13.3685817719,25.1585978497), test loss: 29.2742442131\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.67249250412,4.23906518699), test loss: 2.97102783918\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.70052146912,25.027729449), test loss: 29.1394234657\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.97014462948,4.22344963203), test loss: 3.05605154037\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (7.87184858322,24.8980034587), test loss: 31.7101882458\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.25289988518,4.20797845379), test loss: 3.23349900842\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (3.11968040466,24.7688112512), test loss: 27.6883511543\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.863922715187,4.19267546948), test loss: 3.22752236724\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (9.27262687683,24.6406849341), test loss: 29.3002083302\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.18473148346,4.17759634261), test loss: 2.96532818377\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.33428764343,24.513448071), test loss: 29.11142869\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.890968799591,4.16272857915), test loss: 3.02438685298\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (2.93844461441,24.3867550328), test loss: 31.8684090614\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.30617165565,4.14797057158), test loss: 3.35947301686\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (3.03316402435,24.2609783234), test loss: 27.9985009193\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.5041911602,4.13328777717), test loss: 3.25485833585\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (9.92132759094,24.135941732), test loss: 29.9330709934\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.95471775532,4.11878909207), test loss: 2.91315370202\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.19398164749,24.0119423302), test loss: 29.4100388527\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.802948534489,4.10438764102), test loss: 3.04891855419\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (4.75909566879,23.8886470539), test loss: 30.9770762444\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.60960149765,4.09013350345), test loss: 3.32737717032\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (6.10586166382,23.766479733), test loss: 28.2574361324\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.01814174652,4.07617889261), test loss: 3.02225925922\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (5.92138767242,23.6450315836), test loss: 29.7142819881\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (3.81782603264,4.0622767726), test loss: 2.90309323668\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (2.40488672256,23.5243924093), test loss: 31.1427666187\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.63213419914,4.04849604379), test loss: 3.22097654343\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.38709783554,23.4044508243), test loss: 31.8121946335\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.99916863441,4.03481778094), test loss: 3.37674683332\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (4.67447280884,23.285614458), test loss: 30.9893450737\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.62613689899,4.02127380861), test loss: 2.95797997564\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.67657995224,23.1673236298), test loss: 30.7399539948\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.47706508636,4.00782912415), test loss: 2.95104321837\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (2.40919589996,23.0502800023), test loss: 32.9368595123\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (5.02104091644,3.99465534462), test loss: 3.37215617299\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.43137931824,22.934067799), test loss: 30.6411466122\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.27466523647,3.9815206469), test loss: 3.27117956579\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (8.28297424316,22.8186082338), test loss: 30.6490249157\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.60839033127,3.96853795949), test loss: 2.96677744687\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.01152563095,22.7039846248), test loss: 29.9669389725\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.7896168232,3.95563292385), test loss: 3.05542461872\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.78376102448,22.5902940922), test loss: 33.1755339622\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.22072505951,3.94283714118), test loss: 3.369293347\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (4.38277816772,22.4775863301), test loss: 29.8156873703\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.6086025238,3.93016929117), test loss: 3.2063449055\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (2.1853094101,22.3656188978), test loss: 31.1336606026\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.72422933578,3.91760704246), test loss: 3.03104716539\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (3.3144197464,22.2547841142), test loss: 30.2713157654\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (4.552339077,3.90528400157), test loss: 3.04858511984\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 6\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (203.651153564,inf), test loss: 173.510970306\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (344.284790039,inf), test loss: 365.838668823\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (39.1111793518,96.8245696163), test loss: 37.2576393604\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (18.4918861389,203.737643653), test loss: 11.3554136515\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (17.3250961304,70.5964142566), test loss: 40.9546985865\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.61395001411,103.621218243), test loss: 2.99232995212\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (58.5009803772,61.8446180941), test loss: 40.5875887871\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.61016941071,70.0393881061), test loss: 3.25883433819\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.4517974854,57.4586023543), test loss: 38.8557778358\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.05648159981,53.2453366051), test loss: 3.31827807426\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (46.4772186279,54.7664661919), test loss: 35.6658710003\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.415425240993,43.1701034915), test loss: 3.19604580402\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (81.7515716553,52.9888456041), test loss: 40.2883383751\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.112388134,36.4523351201), test loss: 2.92146585882\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (14.6411905289,51.6741089438), test loss: 39.7009086609\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.78475403786,31.6550441446), test loss: 3.1436285764\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (54.2858505249,50.6885939269), test loss: 39.1462767601\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.97771596909,28.0588775186), test loss: 3.11779043674\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (40.8876647949,49.9129667187), test loss: 35.6707789898\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.13050174713,25.2645199622), test loss: 3.11249018312\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (100.257820129,49.2726703801), test loss: 39.0019138336\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.97827100754,23.0274633907), test loss: 2.94878529012\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (16.6324825287,48.7343467607), test loss: 40.1606523275\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.5378537178,21.1967763066), test loss: 3.09271330535\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (26.2317790985,48.2659413575), test loss: 39.1997026443\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.0864033699,19.6722684959), test loss: 3.21588541269\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (15.7076101303,47.8598195692), test loss: 35.9907871485\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.479575634,18.3819646544), test loss: 3.0497706145\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (52.157661438,47.5127279123), test loss: 34.7547168255\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.63601827621,17.2764808507), test loss: 2.95195967257\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (52.3526763916,47.2028462095), test loss: 40.027761507\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.44872498512,16.3187396172), test loss: 2.89392649531\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (32.5486030579,46.910491238), test loss: 39.7118149757\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.45183157921,15.4813942122), test loss: 3.22704985738\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (28.9069156647,46.6506456916), test loss: 38.4164780617\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.0761153698,14.7428494069), test loss: 3.12557772696\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (27.4742889404,46.4133619085), test loss: 33.8899319649\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.48373746872,14.0863250194), test loss: 3.13706011176\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (70.3818511963,46.1780603581), test loss: 38.0816525936\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.38991487026,13.4994820804), test loss: 2.87431302369\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (42.8665008545,45.9630222722), test loss: 38.1944951534\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.308984279633,12.9714210183), test loss: 3.18900828958\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (41.7676925659,45.7503459611), test loss: 37.4027735233\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.77558267117,12.4944955235), test loss: 3.17934367061\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (107.903808594,45.554855033), test loss: 34.2067684412\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.82202339172,12.0619780612), test loss: 3.00829971433\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (29.6869907379,45.3607805947), test loss: 35.1527272463\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.23870337009,11.6680470733), test loss: 2.87184091806\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (67.0785751343,45.1753348322), test loss: 38.458662653\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.57307815552,11.3071704407), test loss: 3.00344982147\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (13.4223918915,44.9864597858), test loss: 37.1791213274\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.85225725174,10.9751689966), test loss: 3.27165102661\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (41.8834266663,44.803076907), test loss: 36.4358251095\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.50559997559,10.6698165385), test loss: 3.21320212781\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (33.2875862122,44.6215216514), test loss: 32.6825344086\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.31516289711,10.3872380456), test loss: 3.10736528039\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (22.9986381531,44.4465311682), test loss: 37.3568427563\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.9289867878,10.125657136), test loss: 2.95614060163\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (30.8294868469,44.2738025379), test loss: 36.8443619728\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.34124994278,9.88269797051), test loss: 3.18593127131\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (52.952255249,44.0987447388), test loss: 35.7677370548\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.34921550751,9.6565803996), test loss: 3.0806612879\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (36.3209686279,43.9274189974), test loss: 31.6791245937\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (0.528505802155,9.44541129589), test loss: 3.0464512676\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (28.5512008667,43.7574740786), test loss: 35.118134594\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.38883781433,9.24779669452), test loss: 2.94146659374\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (28.9320335388,43.5828016097), test loss: 36.328856802\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.34588837624,9.06230229118), test loss: 3.19803415537\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (40.984161377,43.4114254051), test loss: 35.7112961054\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.91442978382,8.88804360929), test loss: 3.23078302145\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (31.7452316284,43.2343542449), test loss: 32.0510746956\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.1007194519,8.72403946314), test loss: 3.03626790643\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (78.3693313599,43.0603628727), test loss: 31.762457037\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (4.27527141571,8.5697709163), test loss: 2.95343215764\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (37.4781494141,42.8809810669), test loss: 34.4867441893\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.615137755871,8.42408389607), test loss: 3.01502779424\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (62.696231842,42.701831649), test loss: 35.4397130489\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.3732187748,8.28642463527), test loss: 3.39831300974\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (10.3050775528,42.5129248292), test loss: 33.4428046227\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.20558059216,8.15561041975), test loss: 3.29907571077\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (30.1992874146,42.3234680494), test loss: 29.8851625681\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.99299025536,8.03204823335), test loss: 3.25981713533\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (23.5485439301,42.1307907527), test loss: 32.7234886646\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.54341125488,7.9144434666), test loss: 2.98166009784\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (37.1618690491,41.9391522957), test loss: 32.8678598404\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.24746894836,7.80297132938), test loss: 3.30553198159\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (26.4287319183,41.7430264048), test loss: 33.3175843239\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.8086566925,7.69664939953), test loss: 3.22871051431\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (47.863735199,41.5445838245), test loss: 29.6955774784\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.89954948425,7.59556402569), test loss: 3.19811288118\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (68.1376953125,41.3457667393), test loss: 30.9733791351\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.70128202438,7.49869024839), test loss: 2.99364725947\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (34.0176734924,41.1432778891), test loss: 32.6658733845\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.80196261406,7.40605827389), test loss: 3.15542156696\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (30.9633369446,40.9379942054), test loss: 32.521212101\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.60810542107,7.31709513101), test loss: 3.33276181817\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.7014102936,40.7320688505), test loss: 29.6447004795\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.208725214,7.23148343561), test loss: 3.08875050247\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (64.8230209351,40.5254646963), test loss: 27.8907743454\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.90951681137,7.14894084314), test loss: 3.02283526957\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (30.1207923889,40.3172583998), test loss: 30.7954726219\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.26119399071,7.06920502504), test loss: 2.86951263249\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (42.2772636414,40.1072642934), test loss: 31.0646586895\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.03268074989,6.99164352207), test loss: 3.17718875408\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (26.4010715485,39.8965377534), test loss: 31.6247881889\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.3304271698,6.91619111648), test loss: 3.13988191485\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (19.7902889252,39.6818799679), test loss: 27.6558424473\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.80399668217,6.84263375848), test loss: 3.11370651126\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (9.56144809723,39.4678327018), test loss: 29.1126218557\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.706702828407,6.77161526361), test loss: 2.88972575068\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (35.9770545959,39.2547724463), test loss: 30.9092701912\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.5569961071,6.70261375897), test loss: 3.2556116879\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (26.7498703003,39.0429036057), test loss: 31.695843935\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.90150856972,6.63571570437), test loss: 3.27660675943\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (12.9337444305,38.8298637321), test loss: 28.1631088972\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.53875899315,6.57068045387), test loss: 3.11390230656\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (39.923034668,38.6198325264), test loss: 28.0163643837\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.28466868401,6.50772252483), test loss: 2.96297927499\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (84.2829437256,38.4109398241), test loss: 29.0850450993\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.37497091293,6.44645915381), test loss: 2.95034937263\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.0014019012,38.2016461473), test loss: 30.1058020115\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.01373100281,6.38695001366), test loss: 3.28827391565\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (32.0652999878,37.9938417294), test loss: 29.6123987198\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.8583958149,6.32904615929), test loss: 3.32326447964\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.9637050629,37.7871827691), test loss: 26.4938306808\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.11539554596,6.27261315481), test loss: 3.18612011373\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (64.7643661499,37.5831354667), test loss: 28.5751321077\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.39323329926,6.21785744844), test loss: 2.93322266936\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (20.96509552,37.3805276666), test loss: 29.4784686089\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (8.24212265015,6.16499916099), test loss: 3.12696083188\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (32.656867981,37.1790525428), test loss: 29.9938615799\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.15686368942,6.11319157618), test loss: 3.09365567267\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (23.105386734,36.979034386), test loss: 27.3716931105\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.23398065567,6.06279239493), test loss: 3.14807183146\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (14.5745153427,36.7783605254), test loss: 28.0546033382\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.35782933235,6.01351119124), test loss: 2.93646434247\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (23.1719932556,36.5799417404), test loss: 28.8746912479\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.58285665512,5.96575667481), test loss: 3.07785480022\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (26.2098178864,36.3832906503), test loss: 29.761472702\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.97619581223,5.91902252522), test loss: 3.23981069922\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (14.0321950912,36.1884860924), test loss: 27.4228514194\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.31746912003,5.8734732441), test loss: 3.10869366527\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (15.8788900375,35.9944399582), test loss: 26.1573541164\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.25085926056,5.82900298573), test loss: 2.95809273124\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (12.5177745819,35.8031029402), test loss: 28.5101119041\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.38479161263,5.78568939), test loss: 2.89272411764\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (22.0191135406,35.6143890799), test loss: 28.9315564632\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.07973051071,5.74330009795), test loss: 3.23882780671\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (3.99796485901,35.425221337), test loss: 29.152532959\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.62936639786,5.70197126248), test loss: 3.24904500842\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (16.9704437256,35.2389395158), test loss: 25.72950387\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.44892573357,5.66145997874), test loss: 3.24237003922\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (17.5653953552,35.053249924), test loss: 26.7715884209\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.568759799,5.62182604665), test loss: 2.91997437775\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (17.8503704071,34.8708578979), test loss: 27.7906574488\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (4.3592338562,5.58326738438), test loss: 3.09662688076\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (22.9598655701,34.6891415433), test loss: 29.8508591175\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (7.05961227417,5.54574183159), test loss: 3.22533794343\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.13133144379,34.5094564401), test loss: 26.6073650122\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.80734753609,5.5088606472), test loss: 3.17901158333\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (10.3391056061,34.3302372006), test loss: 26.5139187813\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.71934366226,5.47278123729), test loss: 2.93188327551\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (10.5397491455,34.1522201976), test loss: 27.5438195705\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (4.01861333847,5.43740514315), test loss: 3.00195389092\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (18.4553318024,33.9750003143), test loss: 28.508372879\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (3.09182357788,5.40289603455), test loss: 3.19992543459\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (37.0938873291,33.8006059952), test loss: 27.3622836113\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.84920048714,5.36899682862), test loss: 3.19753903747\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (13.8857660294,33.6258265719), test loss: 25.2551701546\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.44808399677,5.33580498414), test loss: 3.13588337004\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (14.849855423,33.4526142317), test loss: 27.6302966118\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (4.54849863052,5.30329283879), test loss: 2.87830460966\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.02666091919,33.2810864073), test loss: 27.6212189198\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (4.2167134285,5.27142149707), test loss: 3.01864976585\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (19.8948745728,33.1117308403), test loss: 29.0708621979\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.2681157589,5.24008025545), test loss: 3.15227522552\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (11.8698225021,32.9422516649), test loss: 25.4604161978\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.57527542114,5.20940087979), test loss: 3.10670793355\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (5.60883951187,32.7741887802), test loss: 26.9829855204\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.438295423985,5.17918392619), test loss: 2.91361638308\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (11.9367380142,32.6072548101), test loss: 28.1710092545\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.02457427979,5.14955842003), test loss: 3.08863889873\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.2126197815,32.4422739908), test loss: 28.9802118778\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (5.10848522186,5.12055565216), test loss: 3.22183039188\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (28.2442378998,32.2786170338), test loss: 26.7656737804\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (5.22211837769,5.09231288426), test loss: 3.15355969071\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (9.57961845398,32.1148572077), test loss: 25.963397789\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.70668554306,5.06435482367), test loss: 2.94745608568\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (17.7620048523,31.9527261884), test loss: 27.418874836\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.64260029793,5.03682870276), test loss: 2.88588601649\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (10.6181735992,31.7903612059), test loss: 27.7674133301\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (6.05299091339,5.00986830753), test loss: 3.15520383716\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (13.8961582184,31.6297146883), test loss: 28.1661777973\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.65488004684,4.98334977125), test loss: 3.31933439076\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (10.2669744492,31.4698474934), test loss: 24.8268177509\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.90674209595,4.95722626242), test loss: 3.22155019343\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (8.14085769653,31.3106621122), test loss: 27.4524025917\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.53538703918,4.93157968985), test loss: 2.88335209191\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (10.3047637939,31.1516558406), test loss: 26.5701787949\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (4.05700016022,4.90636745004), test loss: 3.00950062871\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (13.6693572998,30.9948757194), test loss: 29.1324604034\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.37961387634,4.88157185384), test loss: 3.1688569814\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (10.3819160461,30.838413435), test loss: 25.8679465294\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.91730821133,4.85707535001), test loss: 3.18655706644\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (12.4171819687,30.6828955497), test loss: 26.7577469826\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.381584048271,4.83305463207), test loss: 2.92466645539\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (27.6869506836,30.5280137187), test loss: 27.3885720015\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.11643862724,4.8093226039), test loss: 2.97618797421\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.5866203308,30.3734796697), test loss: 28.3376660824\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (3.43513512611,4.78599156213), test loss: 3.1503377974\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (13.9631214142,30.2205161714), test loss: 27.1111421585\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (3.11802196503,4.76310502176), test loss: 3.12515472621\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (20.9314346313,30.0683465435), test loss: 25.483820796\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (4.38046646118,4.74070062231), test loss: 3.00133787394\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (19.3130531311,29.9163914996), test loss: 27.7011719704\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (5.3384809494,4.71847976556), test loss: 2.88762081563\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (12.1325464249,29.7650379973), test loss: 27.4957541466\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.77366352081,4.6965437169), test loss: 3.0775454402\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (13.4673442841,29.6139791528), test loss: 29.4110021591\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (4.88260269165,4.67499853846), test loss: 3.19710407555\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (19.5578575134,29.4635027746), test loss: 24.9144757271\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.85287475586,4.65365401116), test loss: 3.17437275946\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (7.9146490097,29.31398585), test loss: 26.8235953808\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.59092235565,4.6326492479), test loss: 2.88147287965\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (5.79655599594,29.1641637513), test loss: 27.9945024014\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.69170820713,4.61191790418), test loss: 3.05735764205\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (11.1862888336,29.0155879081), test loss: 28.6137682438\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.20559740067,4.59150770327), test loss: 3.19182299972\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (12.0281152725,28.8672754178), test loss: 26.2718416691\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.63813149929,4.57134901652), test loss: 3.12778319865\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (6.85194683075,28.7202045507), test loss: 25.9359754562\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.38749933243,4.55140428782), test loss: 2.86213975698\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (14.2738542557,28.573047694), test loss: 29.3509249687\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.10044252872,4.53175517482), test loss: 2.88453864455\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (7.10773944855,28.4269560916), test loss: 28.0260484695\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.913224875927,4.51230202059), test loss: 3.14043777883\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (26.2662277222,28.280577593), test loss: 28.8805077076\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.43987762928,4.49312939325), test loss: 3.26416740417\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (27.9297637939,28.1354582479), test loss: 25.1598941803\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.83345508575,4.4743281944), test loss: 3.14914576262\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (4.34799289703,27.9901370189), test loss: 27.7654684067\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.09743654728,4.45579698078), test loss: 2.93522307724\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (14.1378650665,27.8459526312), test loss: 27.129923296\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.10101270676,4.43745544012), test loss: 2.96134626269\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (10.2369613647,27.7017834821), test loss: 29.974776268\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.51576399803,4.41922749825), test loss: 3.11716369689\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (13.845079422,27.5583528899), test loss: 26.6619806767\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.975020647049,4.40131535098), test loss: 3.13087221384\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (18.6243228912,27.4154318493), test loss: 28.0300959587\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.17014169693,4.38353392071), test loss: 2.91880598664\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (6.9607257843,27.2731478825), test loss: 28.8742199898\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.72752714157,4.36599001448), test loss: 3.01399240494\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (4.56658172607,27.131257246), test loss: 29.3045507908\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.27028131485,4.34866597097), test loss: 3.18674197942\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.49772071838,26.9900516238), test loss: 28.2006990433\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.08568167686,4.33152287856), test loss: 3.05786743164\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (10.3696193695,26.8496754706), test loss: 26.7727036476\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.2922911644,4.31459890837), test loss: 2.92720705718\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (12.5808591843,26.7100686191), test loss: 30.8852680206\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.65587353706,4.29783392011), test loss: 2.8665263474\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (8.31617355347,26.5710183446), test loss: 29.9061859131\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.48873376846,4.28124685073), test loss: 3.08511596918\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (2.80298471451,26.4325495552), test loss: 30.0801687241\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.838793158531,4.26480633852), test loss: 3.33466073126\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (12.9515638351,26.2950162009), test loss: 27.0470042706\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.41130340099,4.24859564963), test loss: 3.24363054931\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (17.5733261108,26.1581046974), test loss: 28.5127738953\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (3.59359550476,4.23264905367), test loss: 2.92043746561\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.65080118179,26.0220594847), test loss: 27.8694843769\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.73842227459,4.21687871175), test loss: 3.03203796446\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (13.8133068085,25.8866040886), test loss: 30.8867864132\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (3.33295369148,4.20125752851), test loss: 3.20346409976\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (6.2035036087,25.7520040548), test loss: 29.2360902786\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.529250502586,4.18571252791), test loss: 3.28647853434\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (8.23251152039,25.6177710215), test loss: 28.5380515099\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.49412405491,4.17040808467), test loss: 2.93069084436\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (5.91321134567,25.484696241), test loss: 30.0219989777\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.83480834961,4.15520246819), test loss: 2.98912226707\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (6.34619903564,25.352121269), test loss: 31.0231271744\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.1551887989,4.14017856876), test loss: 3.25444238782\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (4.56283092499,25.2203341735), test loss: 29.686323595\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.34057831764,4.12528801398), test loss: 3.23767900467\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (7.74042654037,25.0893524275), test loss: 27.9432641983\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.58833146095,4.11056544373), test loss: 3.05865193009\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (6.64598703384,24.9592521012), test loss: 30.9146431923\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.20494842529,4.09596953274), test loss: 2.91870294809\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (5.19157505035,24.8300453054), test loss: 31.1216576576\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.66180992126,4.08152109044), test loss: 2.96514401734\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (8.48222351074,24.7015908473), test loss: 32.1864281654\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.1500493288,4.06718981298), test loss: 3.19845390618\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (2.01221036911,24.5739589977), test loss: 28.8816222191\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.23856508732,4.0529864345), test loss: 3.19243841767\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (5.32736253738,24.4472263706), test loss: 30.4183004856\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.70150566101,4.03895565548), test loss: 2.97604090124\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.5892124176,24.3215658584), test loss: 30.5508932114\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.27353382111,4.02514097987), test loss: 3.18465643823\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (7.84119415283,24.1965276621), test loss: 32.1919318199\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.94539093971,4.01145425953), test loss: 3.32687733769\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (5.22271251678,24.0725760891), test loss: 29.9485451698\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.54201292992,3.9978866402), test loss: 3.22779795229\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (4.94440221786,23.9492989476), test loss: 29.94269104\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.456786781549,3.98435249218), test loss: 3.00062095821\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (6.00405597687,23.8269801039), test loss: 31.0822609901\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.01369047165,3.97105082749), test loss: 2.94248835295\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (4.58928823471,23.7055186367), test loss: 32.2157720566\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.62875843048,3.9578075906), test loss: 3.24983009547\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (3.16393089294,23.584994149), test loss: 33.0678813934\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (3.02333807945,3.94473216134), test loss: 3.46855941117\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (3.5853433609,23.4651972498), test loss: 29.3130197525\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.27075231075,3.93171874875), test loss: 3.24236542583\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.89442586899,23.3464751217), test loss: 31.6747425079\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.82423949242,3.91887507806), test loss: 3.02041154206\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (9.51446723938,23.2286276919), test loss: 30.6344454765\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.15594649315,3.90611556932), test loss: 3.05230057538\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (2.64740276337,23.1117780608), test loss: 33.5707306862\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.00588583946,3.8934910411), test loss: 3.17013995051\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.28975057602,22.995721089), test loss: 31.3490059853\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.87162363529,3.88095555526), test loss: 3.23887048066\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (1.94474601746,22.8805827205), test loss: 32.6931803703\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.15926992893,3.86850971557), test loss: 2.97448189259\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 7\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (311.088195801,inf), test loss: 170.678248215\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (318.571380615,inf), test loss: 363.982901001\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (37.6271057129,96.3898043289), test loss: 34.6166761875\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.66434860229,203.044543196), test loss: 11.41717906\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (36.9694404602,70.1881322951), test loss: 38.9441888809\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.42282581329,103.22052686), test loss: 2.93758229613\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (44.2323150635,61.378016024), test loss: 39.0600198269\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.78965950012,69.7381637366), test loss: 3.08722778857\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (45.4790763855,56.9661481435), test loss: 38.020386672\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.48113620281,53.0014682921), test loss: 3.10917311907\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (111.487640381,54.2661878588), test loss: 37.9192287445\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.81024932861,42.9605022691), test loss: 3.10919618011\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (41.8663101196,52.445533895), test loss: 34.3507439613\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.22019386292,36.2642558793), test loss: 3.09003504813\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (45.1201133728,51.1431651019), test loss: 35.1736978531\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.49024939537,31.482658323), test loss: 2.95557795465\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (19.5471992493,50.1219450409), test loss: 38.1669968605\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.29573249817,27.896129252), test loss: 2.83974216282\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (27.6111907959,49.3327609544), test loss: 39.1514996529\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.56588947773,25.1078401937), test loss: 3.21288221776\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (38.5274276733,48.6957059), test loss: 38.3847574711\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.994550585747,22.8780826295), test loss: 3.1730571568\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (27.6481552124,48.1431528982), test loss: 36.8962182522\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.04966330528,21.0533656925), test loss: 3.18730896711\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (21.2848167419,47.6865680484), test loss: 33.2606369019\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.913076043129,19.5341247011), test loss: 3.07728520334\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (110.573348999,47.2966628162), test loss: 35.9805193901\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.32072138786,18.2492012097), test loss: 2.92117375135\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (30.138507843,46.9357232762), test loss: 38.9073670864\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (5.12777423859,17.147934848), test loss: 2.94598993361\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (21.8879928589,46.6212349828), test loss: 38.0972917318\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (6.306640625,16.1934106121), test loss: 3.17071219087\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (52.2521362305,46.34011785), test loss: 36.6745299339\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.65963935852,15.3583672464), test loss: 3.15949885845\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (52.9100952148,46.0739655998), test loss: 33.4973727703\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.63479804993,14.6222990304), test loss: 2.98289992809\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (50.6741256714,45.822937676), test loss: 33.4803518295\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.96677851677,13.9681385564), test loss: 2.97295563817\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (41.6268615723,45.5979880456), test loss: 37.0726809025\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.41735744476,13.3834483263), test loss: 2.89541391134\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (34.7140159607,45.3859318534), test loss: 37.0899635792\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (8.44224739075,12.857788284), test loss: 3.09185739756\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (15.8742637634,45.1852671904), test loss: 36.9937939644\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.32774734497,12.3820260218), test loss: 3.21637753248\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (30.2766571045,44.9949496167), test loss: 35.9457212448\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.1059217453,11.9506696676), test loss: 3.08691502512\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (26.5921897888,44.8005287125), test loss: 32.6257808447\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.550989151,11.5566921396), test loss: 3.05714461207\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (43.1577110291,44.6218256846), test loss: 34.0431305885\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.69550609589,11.1963375487), test loss: 2.95205008686\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (107.94695282,44.4437138535), test loss: 35.4236310482\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (6.16407346725,10.8647037901), test loss: 2.84185290933\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (86.4172286987,44.2690997976), test loss: 36.6125758648\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.24907016754,10.5589990204), test loss: 3.2170817256\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (26.2040405273,44.0882833165), test loss: 36.219816494\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.19836711884,10.2767448886), test loss: 3.21585321426\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (69.0903625488,43.9148100367), test loss: 34.4007205963\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.82573223114,10.0152492495), test loss: 3.18812406063\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (52.9362945557,43.7396485937), test loss: 30.8819429874\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.917970061302,9.77167712659), test loss: 3.0393633455\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (26.5039405823,43.5696184196), test loss: 33.4908915043\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.41134464741,9.54513499651), test loss: 2.93333708942\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (40.3627662659,43.3948174265), test loss: 35.629928112\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.0418305397,9.33357274277), test loss: 3.00912077725\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (49.3497924805,43.2234569047), test loss: 34.8230062485\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.14319133759,9.1353050596), test loss: 3.20006138086\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (22.7787437439,43.0491544908), test loss: 33.9100011587\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.23678576946,8.94892382089), test loss: 3.16332679093\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (41.0577468872,42.8738852149), test loss: 30.4527097225\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.68967628479,8.77427893714), test loss: 3.02197070718\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (68.0437698364,42.6931560312), test loss: 30.0662182331\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (5.5102853775,8.60992900381), test loss: 2.98911869526\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (28.3510360718,42.5094865983), test loss: 32.8122786522\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.60637068748,8.45489158242), test loss: 2.96436651051\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (27.9324817657,42.3280313247), test loss: 33.8941556931\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.60377120972,8.308804999), test loss: 3.18487509191\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (32.1537437439,42.143032658), test loss: 33.9131797791\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.78719604015,8.17028626673), test loss: 3.32568957806\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (34.1610946655,41.9559892872), test loss: 32.2389274836\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.51120948792,8.0396516916), test loss: 3.20974053741\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (88.7321548462,41.7627303958), test loss: 28.7369639635\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.75011348724,7.91595809063), test loss: 3.11035982072\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (33.2515830994,41.564485219), test loss: 29.8804198742\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.22254014015,7.79797928847), test loss: 3.02065940797\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (37.3482589722,41.3639656098), test loss: 30.5647063255\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.03180217743,7.68606026856), test loss: 2.92604575455\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (9.63905715942,41.1563551808), test loss: 31.4678374767\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.23974752426,7.57908161462), test loss: 3.28194443583\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (38.0980415344,40.9484551168), test loss: 32.1562176228\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.991662144661,7.47725845289), test loss: 3.2718895942\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (31.392288208,40.7380884727), test loss: 29.8533886433\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.26533806324,7.38000887376), test loss: 3.23984882832\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (27.2519016266,40.522824694), test loss: 27.00792346\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.33862376213,7.28675851153), test loss: 3.13973891437\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (9.82729244232,40.3063893953), test loss: 28.6717580795\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.68645739555,7.19762419497), test loss: 2.95754980445\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (69.3774414062,40.0899106627), test loss: 29.6462659836\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.74888443947,7.11204292396), test loss: 3.1016004622\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (18.5971927643,39.8670118419), test loss: 30.3761484146\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (5.02908706665,7.02979669858), test loss: 3.21868371367\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (13.0602664948,39.6444975989), test loss: 30.0893171549\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (7.82705640793,6.9504209097), test loss: 3.17660536766\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (22.0100574493,39.4218201108), test loss: 26.9858418465\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.44902801514,6.8738269536), test loss: 3.04216557741\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (27.2026329041,39.1974278726), test loss: 26.2771200657\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.05458831787,6.80002378867), test loss: 3.00224280953\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (38.9045372009,38.9718601832), test loss: 27.3446374416\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.22657728195,6.72858596362), test loss: 2.97395537198\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (19.2488937378,38.7472356942), test loss: 28.7704585552\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.57834243774,6.65954563422), test loss: 3.18348408937\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (22.4962692261,38.5246739664), test loss: 29.6545840263\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (8.71392154694,6.59269505244), test loss: 3.29590007067\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (16.5719146729,38.3024361238), test loss: 28.7760773182\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.07765746117,6.52753378446), test loss: 3.22743051052\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (17.8534431458,38.0811810223), test loss: 25.5937963009\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.52286183834,6.46462290172), test loss: 3.03756884933\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (12.458735466,37.8589638241), test loss: 26.6975589275\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.45462656021,6.40322402591), test loss: 2.94943566024\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (28.0571556091,37.6387719791), test loss: 26.8271391392\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.5744562149,6.34366670029), test loss: 2.87348404229\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (60.4844589233,37.4195273172), test loss: 27.7145391941\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (5.78858995438,6.28545546801), test loss: 3.16376763582\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (56.5422019958,37.2020702371), test loss: 29.4262067795\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.68762254715,6.22872860144), test loss: 3.151693964\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.067158699,36.9857718533), test loss: 26.8749555111\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.33558940887,6.17344163448), test loss: 3.1773493588\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (16.8978691101,36.7728957736), test loss: 24.7142447233\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.19544172287,6.1193549255), test loss: 3.1689040482\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (26.8001098633,36.5610294861), test loss: 25.4919113874\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.10575842857,6.06621307103), test loss: 2.87515169382\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (7.27849578857,36.3520658139), test loss: 26.3669167995\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.12698054314,6.01433205739), test loss: 3.00771532059\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (16.3934936523,36.1433934127), test loss: 27.6425585747\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.33583259583,5.96357649177), test loss: 3.1057675004\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (37.5079078674,35.9386414769), test loss: 27.7035356522\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.57089710236,5.91384544559), test loss: 3.12563610077\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (8.29274749756,35.7344751359), test loss: 25.6330372334\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.39867913723,5.86505626078), test loss: 3.09076208174\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (23.1883888245,35.5332764935), test loss: 24.8853444099\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.578966856,5.81752772193), test loss: 3.01611843705\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (23.3645133972,35.3336348901), test loss: 24.8196601391\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (4.24865198135,5.77103011759), test loss: 2.9492775321\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.6166362762,35.1347637953), test loss: 25.9325959206\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.74994671345,5.72551220304), test loss: 3.09118798971\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (8.97894477844,34.9392289786), test loss: 27.1268606663\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.03101682663,5.68114466932), test loss: 3.19450534582\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (15.0815105438,34.7456895584), test loss: 26.5875801563\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.81738877296,5.63758301446), test loss: 3.25251388252\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (18.960401535,34.5534624236), test loss: 24.0823920608\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.33474588394,5.59526337272), test loss: 3.08373062015\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (38.1158828735,34.3629222675), test loss: 24.9585636377\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.02702569962,5.55395210374), test loss: 2.98947663903\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (37.5687408447,34.1728026826), test loss: 24.8924728632\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.57491803169,5.51338019579), test loss: 2.88953038454\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (29.9625892639,33.9846676517), test loss: 25.7350693226\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.85462903976,5.47382193019), test loss: 3.10742610693\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (11.9885034561,33.7983137387), test loss: 28.0105033398\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (5.07577514648,5.43508109451), test loss: 3.19782501757\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (30.332151413,33.6136868377), test loss: 25.2616919756\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.973127126694,5.39725223294), test loss: 3.16478608251\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (15.6693334579,33.4320263227), test loss: 23.287987113\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.02071094513,5.36030221635), test loss: 3.14120809138\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (20.2100753784,33.2513873152), test loss: 24.618781352\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.65965247154,5.32414758567), test loss: 2.87989335358\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (9.78903865814,33.0726291335), test loss: 24.7392302036\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.64544391632,5.28882642403), test loss: 2.94221966267\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (36.7845878601,32.8958892214), test loss: 26.0153686523\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.82979428768,5.25420695287), test loss: 3.03854964077\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (7.46213674545,32.719093514), test loss: 26.2642251492\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (3.39812302589,5.22031636679), test loss: 3.1219188869\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.3559446335,32.5448818811), test loss: 23.9574085712\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.33940601349,5.1870261505), test loss: 3.18551265001\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.24546289444,32.3725963169), test loss: 23.9099393368\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.34017014503,5.15444083542), test loss: 3.02002409399\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (14.9957351685,32.2018420473), test loss: 24.1082737207\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (3.36866474152,5.12256091863), test loss: 2.96876383722\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (17.2476463318,32.0321594546), test loss: 24.7712253332\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.00820589066,5.09126944051), test loss: 3.05606906116\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (10.6272277832,31.8636838974), test loss: 25.964454031\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.80555105209,5.06061915738), test loss: 3.14157797098\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (14.3340644836,31.6973276707), test loss: 25.5407812119\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (8.46547698975,5.03059190911), test loss: 3.25294037461\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (16.6146430969,31.531688474), test loss: 23.6640554667\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.36590993404,5.0009670515), test loss: 3.12162723243\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (14.7114601135,31.3671290435), test loss: 24.1720050335\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.966337680817,4.97207342329), test loss: 2.92798029184\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (8.60466861725,31.2033909718), test loss: 24.8400727987\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.50190079212,4.94361681859), test loss: 2.87265742123\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (10.0778408051,31.040518319), test loss: 25.4144031048\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.33342897892,4.91575319706), test loss: 3.06859506965\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (29.5006465912,30.8792059248), test loss: 27.8142457962\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.60321712494,4.8882816191), test loss: 3.19973254204\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (35.4355201721,30.7188325534), test loss: 24.7701272964\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (3.1402463913,4.86135648431), test loss: 3.11858929694\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (9.41006755829,30.5595973795), test loss: 22.9959178448\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.43986845016,4.83494112221), test loss: 3.10001718998\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (5.01956701279,30.4020169104), test loss: 24.7776255608\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.868950247765,4.80905544045), test loss: 2.91409982741\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (13.4362668991,30.2451194807), test loss: 24.4174176455\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.43523538113,4.78355365631), test loss: 2.96527280509\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (2.80105257034,30.0896559962), test loss: 25.3829813004\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.82169687748,4.75850790479), test loss: 2.95560840964\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (11.8961439133,29.9344495533), test loss: 26.4073928833\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.69292628765,4.73390044434), test loss: 3.14687327147\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (25.2212619781,29.7812660021), test loss: 23.9377049446\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (3.29628801346,4.70963286996), test loss: 3.20776844323\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (5.95224380493,29.6282002263), test loss: 23.7031074524\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.55811047554,4.68572962415), test loss: 3.01055964231\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (14.5320158005,29.4766714735), test loss: 24.6508918285\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.35284054279,4.66228337744), test loss: 2.92858961225\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.34636449814,29.3260367714), test loss: 24.8376786709\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.15497088432,4.63918866521), test loss: 3.09562360346\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (10.5188093185,29.1757854282), test loss: 26.171747303\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.05679965019,4.61645215978), test loss: 3.14148066342\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (5.76711988449,29.0269849899), test loss: 25.5205117702\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.54852199554,4.59407819046), test loss: 3.2559838295\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (13.3202571869,28.8789306913), test loss: 23.8635961771\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.49007296562,4.57194699857), test loss: 3.13275276124\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (17.0998744965,28.7312962371), test loss: 24.080515337\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.30377316475,4.55025939654), test loss: 2.85762276053\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (16.7870464325,28.584632604), test loss: 25.3578862667\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.02578139305,4.52893297812), test loss: 2.8917912215\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (36.0723381042,28.4382946444), test loss: 25.1326272011\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.45624732971,4.50779368406), test loss: 3.01094742417\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (18.2672615051,28.2928017412), test loss: 28.2064054966\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (3.68159008026,4.4870029248), test loss: 3.28650351465\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (8.26636886597,28.1480623875), test loss: 25.6550700665\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (4.22887897491,4.46649080258), test loss: 3.14820350409\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (16.6034832001,28.0042455602), test loss: 23.4710626841\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.02933716774,4.44629504586), test loss: 3.03806180954\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (5.03680229187,27.8617667443), test loss: 25.6034256458\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.674274682999,4.42640849526), test loss: 2.94445331246\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (13.1044120789,27.7196076238), test loss: 25.3080202579\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.2874121666,4.40682339937), test loss: 2.97158245742\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (6.53686332703,27.578491774), test loss: 25.506073761\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.65281271935,4.38752106956), test loss: 2.98259355426\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (15.7782115936,27.4380414337), test loss: 27.0529480934\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.15130996704,4.36846569651), test loss: 3.21529359519\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.15067672729,27.2979779022), test loss: 24.1852286816\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.2501885891,4.34967548374), test loss: 3.20107432604\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (9.6258058548,27.1590138668), test loss: 23.9564938307\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.06664896011,4.33106010113), test loss: 3.02021889538\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (5.15288829803,27.0207506219), test loss: 25.7973770618\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.20694375038,4.31271633684), test loss: 2.93960273862\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (7.33226728439,26.8833266828), test loss: 25.9587625504\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.98419523239,4.29461951064), test loss: 3.14270573258\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (11.5019855499,26.7465872671), test loss: 27.2907952785\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.49129033089,4.27676021844), test loss: 3.19553056955\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (4.50670289993,26.6103921067), test loss: 26.2071159363\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.00938749313,4.25913054297), test loss: 3.29339741766\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.35286235809,26.4753038798), test loss: 24.4857666969\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (5.48474216461,4.24172419414), test loss: 3.18293820024\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (11.9226150513,26.3406313339), test loss: 25.2434817314\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.15136194229,4.22445122918), test loss: 2.95930865705\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (12.834394455,26.2066715994), test loss: 26.3827339172\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.01663386822,4.20748103613), test loss: 2.91711641699\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.0653629303,26.0732510003), test loss: 25.9833003998\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.5849353075,4.19065817279), test loss: 3.03742977083\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (5.40950107574,25.9405751375), test loss: 29.3944743156\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.17327427864,4.17409658988), test loss: 3.34470188022\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (17.4550666809,25.8088144815), test loss: 26.3681403637\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.41007804871,4.15766172966), test loss: 3.17689218521\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (14.5595684052,25.6777319936), test loss: 24.666295433\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.51250910759,4.14145080258), test loss: 3.0546339184\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (5.20928955078,25.5474317295), test loss: 26.8493189812\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.07908928394,4.12542421458), test loss: 2.97805365771\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (3.51367974281,25.418093425), test loss: 26.3988060951\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.645171046257,4.10964898674), test loss: 3.0317519933\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (5.1410741806,25.2893143102), test loss: 27.095489502\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.41098284721,4.09401778257), test loss: 3.09259696007\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (3.78482937813,25.1614060568), test loss: 27.4921262741\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.562291920185,4.07856558305), test loss: 3.22217353582\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (8.31790161133,25.0340901783), test loss: 25.365007782\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.54811692238,4.06331870344), test loss: 3.21518834233\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (9.93418693542,24.9076563586), test loss: 25.6710190773\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (3.00949001312,4.04816662626), test loss: 3.04836195409\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (5.81520223618,24.7818418569), test loss: 27.4399620533\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.30898880959,4.03318576842), test loss: 2.97652632147\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (7.63598108292,24.656828154), test loss: 27.4889077663\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.1111035347,4.01841082575), test loss: 3.08237275928\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (2.24221229553,24.5325950978), test loss: 29.1786466122\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.54806733131,4.00378298365), test loss: 3.26484447122\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (7.30126619339,24.4089776023), test loss: 27.7663495541\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.82923030853,3.98932832642), test loss: 3.37096529901\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.62220859528,24.2861828323), test loss: 25.3202775002\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.58026337624,3.97500212751), test loss: 3.20078064799\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (8.46912765503,24.1642627302), test loss: 26.5391815186\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.5552148819,3.96078309259), test loss: 2.9753557235\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (9.39284515381,24.0429369939), test loss: 27.6609588623\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.41671562195,3.94677561694), test loss: 2.97322065234\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (8.83338737488,23.9224758538), test loss: 27.4131037235\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.87725400925,3.93293608705), test loss: 3.06161211282\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (10.0051460266,23.802639864), test loss: 29.5617832661\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.09881663322,3.91916181075), test loss: 3.30293085128\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (8.05158233643,23.6837114421), test loss: 27.3758351326\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (3.3703827858,3.90554875517), test loss: 3.25188992321\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (3.95551753044,23.5655141063), test loss: 26.1462246418\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.34813308716,3.89206580901), test loss: 3.10313373208\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (11.1804561615,23.4482674446), test loss: 28.1776972294\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.25266766548,3.87872248871), test loss: 3.04816733897\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (1.25831413269,23.3319267769), test loss: 27.7711578369\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.669767618179,3.86552868459), test loss: 3.09574098885\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.38281345367,23.216335385), test loss: 29.1359193802\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.95102930069,3.85249735176), test loss: 3.21641571671\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (3.6071395874,23.1014923594), test loss: 28.7284780502\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.65599024296,3.83957744429), test loss: 3.31329267025\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.27499246597,22.9875522934), test loss: 26.5335240364\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.759233832359,3.8267908887), test loss: 3.26749439538\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (3.01522421837,22.8743102165), test loss: 27.0126433849\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.96360301971,3.81412132463), test loss: 3.04734250307\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.80777597427,22.7620668923), test loss: 28.9815925121\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.96591198444,3.80152281664), test loss: 3.04645950049\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (3.19726133347,22.6505722388), test loss: 27.5788835526\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.82462573051,3.78905926458), test loss: 3.12346323282\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.80425333977,22.5398794822), test loss: 30.6349523544\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.37288069725,3.77671920139), test loss: 3.35405960083\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.05050039291,22.4300637851), test loss: 29.5781245232\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.30872309208,3.76450741575), test loss: 3.47325066328\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (2.94431567192,22.3209061935), test loss: 26.5092049599\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.38308739662,3.75239192038), test loss: 3.22301185131\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 8\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (317.123657227,inf), test loss: 159.215674591\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (304.463745117,inf), test loss: 368.19256897\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (78.4831237793,97.0026413536), test loss: 32.0760204792\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (8.7016954422,204.57967012), test loss: 11.7320594072\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (50.9533576965,71.1841534295), test loss: 35.6362582684\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.52506303787,104.082628048), test loss: 2.90973324776\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (30.8004398346,62.481772466), test loss: 35.6010009766\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.027572155,70.3608232181), test loss: 3.11157462597\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (49.5827407837,58.1059180863), test loss: 35.3028387547\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.09650945663,53.5019382715), test loss: 3.17637097538\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (53.8870811462,55.4707141216), test loss: 30.895666647\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.61882400513,43.3883887889), test loss: 3.11281779557\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (49.6203231812,53.683905331), test loss: 33.9161471844\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.06994795799,36.6452827595), test loss: 2.96547543705\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (21.1390762329,52.4017929252), test loss: 35.3800560474\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.28261899948,31.8294710092), test loss: 3.03053699583\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (24.1643371582,51.4031876284), test loss: 36.1624258041\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.64462590218,28.218655434), test loss: 3.2803037703\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (57.0832519531,50.6185752971), test loss: 34.4392168045\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.43909299374,25.4097827379), test loss: 3.20373654962\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (68.1086883545,49.9804524989), test loss: 30.6919523716\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.5531129837,23.1637582622), test loss: 3.11640581489\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (96.7655563354,49.4541518472), test loss: 34.6360720158\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.08867192268,21.3254692445), test loss: 2.93348608911\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (20.2235717773,48.9864500523), test loss: 35.3172632694\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.20361232758,19.7940816664), test loss: 3.13281286955\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (31.6620941162,48.5829645057), test loss: 35.3697372437\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.74638462067,18.4985837348), test loss: 3.20867046714\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (21.3467712402,48.231134253), test loss: 31.2682560921\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.16970252991,17.3888901524), test loss: 2.99632564634\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (90.1313171387,47.9221066704), test loss: 31.0093604088\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (5.2181558609,16.4279038537), test loss: 2.92366183102\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (20.333190918,47.6300711122), test loss: 33.9722826958\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.17509102821,15.5877357215), test loss: 2.82314200997\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (45.1595458984,47.357365635), test loss: 34.2541022778\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.90019822121,14.84625922), test loss: 3.07505449653\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (29.8933563232,47.0999357511), test loss: 33.9348978519\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.76921272278,14.1875592889), test loss: 3.09382830262\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (65.0648651123,46.8709640239), test loss: 29.4764018059\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.57529580593,13.5992005604), test loss: 3.00753331184\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (38.8876571655,46.6420161357), test loss: 32.0363353729\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.290589571,13.0695542464), test loss: 2.91426901817\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (26.6498451233,46.421080116), test loss: 33.5230741501\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.06504178047,12.5910559129), test loss: 3.0083036989\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (48.4452667236,46.212286745), test loss: 34.4932748795\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (7.29211997986,12.1571564434), test loss: 3.28494359553\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (14.1098899841,46.0090820139), test loss: 32.6516510963\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.31040501595,11.7611363273), test loss: 3.16925516427\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (15.6783313751,45.8108843642), test loss: 28.9450755835\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.06909894943,11.3985779366), test loss: 3.07701025307\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (49.6569480896,45.6204399664), test loss: 32.3790678501\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.45180416107,11.0655599121), test loss: 2.94704364538\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (89.1212615967,45.4376906844), test loss: 33.3926632643\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (5.91593027115,10.7588497733), test loss: 3.16665539145\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (56.0561294556,45.2533357384), test loss: 33.6518204212\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.55696463585,10.4752698385), test loss: 3.23705098629\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (40.2256011963,45.0698008835), test loss: 29.2581483126\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.27050709724,10.212379991), test loss: 2.99239678681\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (28.249130249,44.8842627357), test loss: 28.9205061913\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (4.13159275055,9.96808309602), test loss: 2.95489013791\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (49.7183837891,44.70150836), test loss: 31.5414007902\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.11158323288,9.74052398596), test loss: 2.87426422238\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (61.4237823486,44.5193326361), test loss: 32.1089664936\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (5.30691719055,9.52777588075), test loss: 3.13743559718\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (96.2537612915,44.3381629744), test loss: 31.6508631706\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.19482755661,9.32845406499), test loss: 3.13522697687\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (18.9620780945,44.1517630225), test loss: 27.1736431122\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.91860628128,9.14161597507), test loss: 3.01254886985\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (5.82068157196,43.9625386932), test loss: 29.1710309505\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.8470749855,8.96642341975), test loss: 2.96580337882\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (55.679813385,43.773909527), test loss: 30.750411129\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.05084466934,8.80127452317), test loss: 3.06523216367\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (51.7008056641,43.5824747717), test loss: 32.0012628078\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (7.51032781601,8.64598989879), test loss: 3.33081664145\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (33.9591903687,43.3896108086), test loss: 29.879578495\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.359562575817,8.49896698846), test loss: 3.18698949218\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (18.0653076172,43.1907395988), test loss: 26.1705522776\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.25469946861,8.36011680028), test loss: 3.09660501182\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (25.0687999725,42.989381199), test loss: 28.6150606155\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.03171730042,8.22863886734), test loss: 2.99432477057\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (11.8474721909,42.7875002441), test loss: 30.4581032276\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.35904741287,8.10405793839), test loss: 3.22389600873\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (56.1417160034,42.5856856419), test loss: 31.0351561069\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (5.33112430573,7.98607129916), test loss: 3.24439692199\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (36.9415435791,42.3778736669), test loss: 26.7381155252\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.13378858566,7.87391264845), test loss: 3.01099743843\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (39.1777687073,42.1671122525), test loss: 26.0481840611\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.77548575401,7.76657968489), test loss: 2.98528898656\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (23.973613739,41.9531092854), test loss: 28.0280823231\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (7.03322505951,7.66445894483), test loss: 2.90337354243\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (34.1357154846,41.7402504292), test loss: 29.6153302193\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.08157157898,7.56670352785), test loss: 3.20132875443\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (19.5274963379,41.5201791374), test loss: 29.0909542084\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.35440349579,7.47295901735), test loss: 3.169838202\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (17.0133323669,41.298274622), test loss: 25.0264134169\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.51959252357,7.38316102265), test loss: 3.02815568447\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (24.0006008148,41.0763170668), test loss: 25.9693753242\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (8.61347484589,7.29732328074), test loss: 2.98878427148\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (7.70353651047,40.8527965972), test loss: 27.9553215504\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.63990914822,7.21445149243), test loss: 3.08539902568\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (22.641210556,40.6296615103), test loss: 29.9819774628\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.68380606174,7.13472786636), test loss: 3.33489159942\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (50.4595947266,40.4075414133), test loss: 27.2904676676\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (5.09535598755,7.05794505804), test loss: 3.16612804532\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (38.5164260864,40.1860237532), test loss: 24.5997497559\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.90847253799,6.98388900433), test loss: 3.07756609321\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (29.5548801422,39.9637195845), test loss: 26.0435137272\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.70727944374,6.91225857484), test loss: 2.97823010385\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (18.5361671448,39.7413652572), test loss: 28.514548564\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.73509073257,6.84290965506), test loss: 3.19750137329\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.0888671875,39.5199289134), test loss: 29.5590095997\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (6.87413978577,6.77582331946), test loss: 3.24334035516\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (45.6539916992,39.3011121767), test loss: 25.4743095875\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.92037820816,6.71074602629), test loss: 3.01680548191\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (26.8114471436,39.0825392461), test loss: 25.220181942\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.00054526329,6.64765230643), test loss: 2.94359634519\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (86.665145874,38.8672636867), test loss: 26.6209865093\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.55617928505,6.58624922342), test loss: 2.84660627842\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (26.0731296539,38.6512734621), test loss: 28.4868658543\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (4.87643051147,6.52667531136), test loss: 3.15613287091\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (3.4867067337,38.4366581115), test loss: 27.840360117\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.63682889938,6.46873981352), test loss: 3.15713903606\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (30.5602111816,38.2246961705), test loss: 24.5477790833\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.98188066483,6.4122269225), test loss: 2.99761151522\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (26.7349357605,38.0140101413), test loss: 25.203186965\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (5.64157915115,6.35739441733), test loss: 2.91343335509\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (21.8423519135,37.8053356259), test loss: 26.7779551506\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.01018297672,6.30373458926), test loss: 3.00839656889\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (6.89142227173,37.5968936691), test loss: 28.9925857067\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.30788326263,6.25153324437), test loss: 3.25273100734\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (51.7441520691,37.3917634084), test loss: 26.3626054049\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.28776824474,6.20058079763), test loss: 3.08393775523\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (12.0783643723,37.1877884376), test loss: 24.3003690243\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.92132568359,6.15102798358), test loss: 2.97516699731\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (24.8354682922,36.986823481), test loss: 25.0902311802\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.20070075989,6.10272427329), test loss: 2.88650556207\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (50.1228103638,36.7874866623), test loss: 27.1578204632\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (5.09861135483,6.05573746846), test loss: 3.10028665662\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (52.4768218994,36.5897645563), test loss: 28.6619145393\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.11455488205,6.00957486094), test loss: 3.16181547642\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (44.038433075,36.3933210834), test loss: 24.7951329231\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (6.50515985489,5.96462137984), test loss: 3.00035025924\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (15.4878025055,36.1986786676), test loss: 24.6278886318\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.982364118099,5.9205707938), test loss: 2.88720888495\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.703119278,36.0051617845), test loss: 25.8705446243\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.60517764091,5.87755777032), test loss: 2.81134115756\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.1209907532,35.8132403672), test loss: 27.3792532921\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.922823965549,5.83541628864), test loss: 3.11550631225\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (19.760099411,35.6231664258), test loss: 27.2484960794\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (5.74272632599,5.79456233307), test loss: 3.17487675846\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (17.7399673462,35.434945447), test loss: 24.0599114418\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.91941285133,5.75436110199), test loss: 3.00673123896\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (25.1821289062,35.2483905636), test loss: 24.6084126472\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.2020162344,5.7150083728), test loss: 2.88274874389\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (53.5052642822,35.0644232223), test loss: 25.8511643887\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.176394701,5.6765907861), test loss: 2.98228855133\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.2611303329,34.8810003099), test loss: 28.0806922913\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.70935964584,5.63895913429), test loss: 3.23670292199\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (11.6433582306,34.6987929471), test loss: 25.520275116\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.42827939987,5.60209695894), test loss: 3.11593489647\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (7.86359167099,34.5181225374), test loss: 23.8362596989\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.84593069553,5.56593521255), test loss: 2.97807387114\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (5.89901304245,34.338768308), test loss: 24.7079432964\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (5.22003746033,5.53057757366), test loss: 2.90985789895\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (37.9151687622,34.1622204766), test loss: 26.1216268063\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (3.86720895767,5.49587028087), test loss: 3.08746771216\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (10.2926635742,33.9858115279), test loss: 27.9341667175\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.42223310471,5.46187897157), test loss: 3.20678528249\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (30.5541324615,33.8118350576), test loss: 24.1865272999\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.60702264309,5.42845653067), test loss: 3.08316487819\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (29.7949428558,33.6386108998), test loss: 24.0167406082\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (4.1384305954,5.39573481386), test loss: 2.91480491012\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (9.41092586517,33.4661764477), test loss: 25.6794896603\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.48653149605,5.36361880379), test loss: 2.8505091235\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (22.0001583099,33.2953462588), test loss: 26.7156386852\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.58680891991,5.33200055916), test loss: 3.14023267329\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.9710845947,33.1255941708), test loss: 26.3954504967\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.17618393898,5.3010202879), test loss: 3.25305908322\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (6.74663734436,32.9571016795), test loss: 23.918410778\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.81409651041,5.27047571594), test loss: 3.10972135067\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (6.73202991486,32.7891548482), test loss: 24.5119936943\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.8195296526,5.24052303103), test loss: 2.94787468016\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (48.7708625793,32.6236013201), test loss: 25.651018095\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.55848455429,5.21109145248), test loss: 3.03765333891\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (13.181558609,32.4581735348), test loss: 27.5931028366\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.65239524841,5.18224959996), test loss: 3.26423762441\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (20.6238613129,32.2945109869), test loss: 25.5878229141\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.35781955719,5.15398810889), test loss: 3.19427275062\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (24.0414390564,32.1314755733), test loss: 24.3612020969\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.17013883591,5.12622141214), test loss: 3.07692436576\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (38.6545829773,31.9697778427), test loss: 24.8528010845\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.94951248169,5.09882300895), test loss: 2.97021100521\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (26.8911914825,31.8083847399), test loss: 25.9006457329\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (4.29636573792,5.07190880116), test loss: 3.11680809855\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (19.4475784302,31.6480771182), test loss: 27.9341723919\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.806183457375,5.04534868143), test loss: 3.28410837054\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (13.2868585587,31.4885122005), test loss: 24.5693385601\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.82570266724,5.01924995169), test loss: 3.19923833609\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (14.2516365051,31.3295024096), test loss: 24.5999586105\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.72659635544,4.99352583559), test loss: 3.02857500017\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (23.25806427,31.1716801097), test loss: 25.6814531803\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (4.12737178802,4.96837665692), test loss: 2.9584833771\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (13.2691287994,31.0142678548), test loss: 26.8043502808\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.95400619507,4.94347026836), test loss: 3.2081679523\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (10.745344162,30.8579555989), test loss: 26.9826375723\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.17929458618,4.91892711974), test loss: 3.38864895999\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (38.5578727722,30.7027869513), test loss: 24.2089060783\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.38950753212,4.89478504755), test loss: 3.2572853744\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.0500907898,30.5475546788), test loss: 24.8697766304\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.33700370789,4.87100191147), test loss: 3.05807286352\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (13.2571811676,30.3931350695), test loss: 25.9951939106\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.4337053299,4.84755551649), test loss: 3.09947696924\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (7.04701519012,30.2391555658), test loss: 27.7205345631\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.32925450802,4.82442434256), test loss: 3.32542904317\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.22451019287,30.0859519608), test loss: 25.0531598568\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.28715705872,4.80166892277), test loss: 3.26221782416\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (8.20941352844,29.93376351), test loss: 23.8601923943\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.964207053185,4.77921835176), test loss: 3.11340483427\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.6622514725,29.7817856632), test loss: 25.2333296299\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.83895277977,4.75713956838), test loss: 2.98802496344\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (15.0011930466,29.6308909923), test loss: 25.763736105\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.956909418106,4.73530882463), test loss: 3.10098975301\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (17.3137340546,29.4803478743), test loss: 27.6721733093\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.41158771515,4.7138463303), test loss: 3.24786102474\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.91992378235,29.3301327196), test loss: 24.3891418934\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.75559163094,4.69266141126), test loss: 3.1625877291\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (9.16940307617,29.1808336249), test loss: 24.1914650917\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.824046015739,4.67173388744), test loss: 2.94439977854\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (10.6776542664,29.032152947), test loss: 26.0210371494\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.717947602272,4.65112576601), test loss: 2.94063065052\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (3.85741639137,28.8840273449), test loss: 27.4262778282\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.23973178864,4.6307557402), test loss: 3.22137480974\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (7.48530387878,28.7364830672), test loss: 27.1673724174\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.303647995,4.61069490806), test loss: 3.36594134271\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (25.9499988556,28.5900639144), test loss: 24.9647179604\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.81294727325,4.59088881964), test loss: 3.24343125224\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (15.0812225342,28.4440100512), test loss: 25.5549072266\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (3.08483123779,4.57141519018), test loss: 2.99276268184\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (14.5766601562,28.29872075), test loss: 26.3468752861\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (6.82903194427,4.55224016226), test loss: 3.10085519552\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.15768623352,28.1540574053), test loss: 28.9303503036\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.08946883678,4.53322884067), test loss: 3.34326478839\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.9460391998,28.0101261116), test loss: 25.9528639793\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.04804968834,4.51443686689), test loss: 3.24903972149\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (22.2219963074,27.8667329517), test loss: 24.8742381573\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.31182765961,4.4959171609), test loss: 3.09951832891\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (6.8370718956,27.723924744), test loss: 25.8905025244\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.35996675491,4.47755598124), test loss: 2.98951140791\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (11.0336074829,27.5817578614), test loss: 26.5270662308\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.89050722122,4.45943610533), test loss: 3.10602544546\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.81018352509,27.4402194725), test loss: 29.6216720581\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.9613237381,4.44151769823), test loss: 3.34326896667\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (10.0490903854,27.2994240885), test loss: 25.6291733742\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.47631704807,4.4239374672), test loss: 3.22993053496\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (5.38735675812,27.1592410397), test loss: 25.092620039\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.32252597809,4.40645347057), test loss: 2.98081744909\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.32760429382,27.019918294), test loss: 26.4390998363\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.40654635429,4.38919033232), test loss: 2.97602540851\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (12.9538516998,26.8813652257), test loss: 28.7381096363\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.56580877304,4.37212304538), test loss: 3.26910728812\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.50422239304,26.7433220995), test loss: 28.6256361961\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.07020258904,4.35526527361), test loss: 3.45212028623\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (6.7162065506,26.6059554801), test loss: 25.7647892952\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.26074802876,4.33857263357), test loss: 3.25774287581\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (7.10596609116,26.4693902652), test loss: 26.8795493603\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.2134449482,4.32206920597), test loss: 3.04722925723\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (7.31836509705,26.3336208618), test loss: 28.8279596329\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.26440095901,4.30576740669), test loss: 3.09639704227\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (3.07014226913,26.198560435), test loss: 30.1510136604\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.13583898544,4.28964767274), test loss: 3.3458245188\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (9.49475765228,26.0642610214), test loss: 27.0099443436\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.90185201168,4.27370346429), test loss: 3.26176975518\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.71494293213,25.9308446672), test loss: 26.0893266201\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.64788413048,4.25790214778), test loss: 3.1229360044\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (5.9981842041,25.7981332952), test loss: 28.3058870316\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.68297731876,4.24230893799), test loss: 2.98387757838\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (7.70620441437,25.6661206693), test loss: 27.7109606743\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.3935508728,4.22685558739), test loss: 3.09138924479\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (6.86640071869,25.5349266825), test loss: 30.171608305\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.01802825928,4.21155048597), test loss: 3.30218651891\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (10.2962398529,25.4046696954), test loss: 27.2201375484\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.05663502216,4.19642132316), test loss: 3.29646524489\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (2.91186237335,25.275020982), test loss: 26.5308412075\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.26189351082,4.18142645822), test loss: 3.02457465082\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (8.85286140442,25.1462674871), test loss: 28.1411497116\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.5294598341,4.16660238184), test loss: 2.98301482052\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.94117879868,25.0184911422), test loss: 30.6401323318\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.4374423027,4.15193283088), test loss: 3.31885001659\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (17.1454849243,24.8915479487), test loss: 30.6637537956\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.45187234879,4.13746867239), test loss: 3.47975406945\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (8.49425315857,24.7653985128), test loss: 27.8910054207\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (6.21554946899,4.12316438549), test loss: 3.33643805385\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.15636205673,24.6400061664), test loss: 28.6521770239\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.24641156197,4.10895079242), test loss: 3.08030563742\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (4.41817760468,24.5155519348), test loss: 29.0819887161\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.25109767914,4.09485462251), test loss: 3.13797307611\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (11.7419071198,24.3920510425), test loss: 32.0220272064\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.86392390728,4.08094219051), test loss: 3.38418237865\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.05063009262,24.2692046612), test loss: 28.815258956\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.23901033401,4.06710553468), test loss: 3.29147003591\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.38170266151,24.1473083151), test loss: 28.4167774677\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.643867373466,4.05341504943), test loss: 3.17901468277\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (6.42477703094,24.0262202292), test loss: 29.1220608234\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.21890497208,4.03986016471), test loss: 3.0772020787\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (9.68227386475,23.9060497438), test loss: 29.0123533249\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.75577378273,4.02649414069), test loss: 3.13433043361\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.69601154327,23.7867564863), test loss: 32.0242513657\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.63279747963,4.01320075244), test loss: 3.34437037706\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.85205936432,23.6682663556), test loss: 28.8577876091\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.60351991653,4.00001888756), test loss: 3.32998645902\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (6.58501338959,23.5507641593), test loss: 28.1296220303\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.35973834991,3.98695595819), test loss: 3.07119251788\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (3.29818153381,23.4339494762), test loss: 29.3111421585\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.77687835693,3.97402438208), test loss: 3.02538345456\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.72564029694,23.3180673595), test loss: 33.0027011871\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.23289787769,3.96119207683), test loss: 3.35952788293\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.11485004425,23.2030513297), test loss: 31.9727606773\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.12000799179,3.94847138342), test loss: 3.52202718109\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (3.52424478531,23.0888814463), test loss: 29.083513546\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.621983706951,3.93587849446), test loss: 3.31737903953\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 10\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (325.14074707,inf), test loss: 170.268418503\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (301.53717041,inf), test loss: 368.015760803\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (59.3292694092,94.7773460073), test loss: 36.0884545803\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (10.4865655899,204.13947926), test loss: 11.3668813229\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (28.0845661163,68.3344890466), test loss: 40.9730288506\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.18646001816,103.830496224), test loss: 2.86625158489\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (25.0139160156,59.42866692), test loss: 40.1366875648\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.93251848221,70.1809313067), test loss: 3.05046622157\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (45.359916687,54.9749845021), test loss: 39.0700937748\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (5.13926267624,53.3544578048), test loss: 3.06462631524\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (84.643081665,52.2921918304), test loss: 36.1731773853\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.61910247803,43.2579636426), test loss: 3.03993498087\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (25.3633365631,50.4711117835), test loss: 38.7981295586\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.60498690605,36.5275071515), test loss: 2.91133577824\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (12.4737472534,49.1335351142), test loss: 41.6420802116\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.04302406311,31.7222480519), test loss: 2.93271541595\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (20.6794223785,48.124029307), test loss: 39.1712562084\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (0.936810910702,28.1174771414), test loss: 3.19267394841\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (76.9813919067,47.3444295583), test loss: 36.7843663692\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.60831332207,25.3154639239), test loss: 3.0387711674\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (21.2939529419,46.6857709788), test loss: 35.9960290909\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.12389421463,23.0731793127), test loss: 3.12994097471\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (45.8102111816,46.1448100348), test loss: 39.6486701012\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.69091629982,21.2392158444), test loss: 2.91614208221\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (18.2257843018,45.6789866493), test loss: 39.4254674911\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.53472876549,19.7113845582), test loss: 3.03824236393\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (49.8806495667,45.285447051), test loss: 37.7284738541\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (8.9244594574,18.4207474305), test loss: 3.14488342404\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (35.6675796509,44.9221347104), test loss: 35.7639886856\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.57180261612,17.3133180264), test loss: 3.06596529484\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (26.7521362305,44.5947001497), test loss: 36.8787220478\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (5.80024290085,16.3541654676), test loss: 2.8937867105\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (21.2516460419,44.3019908755), test loss: 40.2959822178\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (0.980380535126,15.514601417), test loss: 2.87079349458\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (19.2616119385,44.0290858125), test loss: 40.4866483688\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.57758450508,14.7746060177), test loss: 3.17175374627\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (29.2001686096,43.7768157905), test loss: 38.1492667198\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.89973735809,14.1171418183), test loss: 3.18837445378\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (21.9271469116,43.5417594862), test loss: 34.55252285\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.30234694481,13.5296751924), test loss: 3.16673101783\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (15.5457439423,43.3144050285), test loss: 38.0803822517\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.00394463539,13.0011206874), test loss: 2.86351985335\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (66.6845397949,43.1013094767), test loss: 39.8904840469\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.45251941681,12.523540566), test loss: 3.07055429518\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (60.8453445435,42.8979139788), test loss: 38.1627282619\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.7737364769,12.0900163587), test loss: 3.22369114161\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (30.7936706543,42.6983118018), test loss: 34.8289299488\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.03839278221,11.6944045436), test loss: 2.98198331594\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (20.8499107361,42.5012845126), test loss: 34.1937503338\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (7.50529146194,11.3328333022), test loss: 2.88596069217\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (38.5081481934,42.3122231969), test loss: 38.6746843338\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.53270530701,11.0001569212), test loss: 2.80844188035\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (51.5394897461,42.1273963389), test loss: 38.8524312496\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.51691436768,10.6937707186), test loss: 3.1996235013\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (15.3278102875,41.9482391437), test loss: 36.5267426491\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.76226532459,10.4103896888), test loss: 3.07756450772\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (7.41913509369,41.763601718), test loss: 33.544425869\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.67670559883,10.1480163438), test loss: 2.97580313683\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (34.3857879639,41.5831030883), test loss: 36.0370404243\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.77290809155,9.90374752397), test loss: 2.89132847488\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (30.7268657684,41.4037588638), test loss: 37.3540858746\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.42803645134,9.67659235355), test loss: 3.01158585846\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (17.3876495361,41.2257118111), test loss: 36.2255095482\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.15606880188,9.46405173091), test loss: 3.1830583185\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (36.073513031,41.0452471242), test loss: 33.6632012844\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.82735168934,9.26511550514), test loss: 2.99334163666\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (12.7706918716,40.8640999507), test loss: 31.8358181\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.47900104523,9.07850435188), test loss: 3.06267087162\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (42.8715362549,40.6837485152), test loss: 34.7228147507\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.51925373077,8.90370152893), test loss: 2.86629180014\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (20.1605262756,40.4984335566), test loss: 35.542411375\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.25558042526,8.73912691315), test loss: 3.0594735533\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (27.4174060822,40.3107574607), test loss: 34.7892017841\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.7651655674,8.58366983902), test loss: 3.10496003628\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (16.9154052734,40.121365793), test loss: 31.8386642933\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.03683638573,8.43707153896), test loss: 3.01835128665\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (17.8126888275,39.9282916277), test loss: 32.1887803078\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.30175018311,8.29832415813), test loss: 2.95409782827\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (19.6542663574,39.7322094737), test loss: 34.8922325134\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.94155311584,8.16675876637), test loss: 2.9539921999\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (26.1978492737,39.5354330984), test loss: 34.647213316\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.93814277649,8.0421948084), test loss: 3.24792202413\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (12.4741668701,39.3352797384), test loss: 32.9385742187\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.30558300018,7.92357964397), test loss: 3.13963243067\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (40.0424423218,39.1333335228), test loss: 30.2036600113\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.42803192139,7.8106297579), test loss: 3.17628139853\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (31.0012512207,38.9314168226), test loss: 32.2773269653\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.69042348862,7.70309978768), test loss: 2.97670729756\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (14.7464256287,38.7259105388), test loss: 33.8768576622\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.21698093414,7.60026316564), test loss: 3.09567592442\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (20.2753639221,38.5174189652), test loss: 33.539074707\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (6.88432741165,7.50205666338), test loss: 3.21569746733\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (36.0718307495,38.3078654169), test loss: 30.8909155369\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.75141167641,7.40786968333), test loss: 3.08120764494\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (26.8795318604,38.0965013001), test loss: 30.4669861317\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.48281407356,7.3176056256), test loss: 2.94217707813\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (11.4386005402,37.8872395318), test loss: 32.1189182281\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.73434948921,7.2307545354), test loss: 2.91244743764\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (24.7476177216,37.6744552617), test loss: 33.8715909958\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.4783744812,7.14727278095), test loss: 3.23803073168\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (53.9765739441,37.462096692), test loss: 33.1906663418\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.29752278328,7.06675170917), test loss: 3.14417876303\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (24.599565506,37.2501955256), test loss: 29.0464859962\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.65055656433,6.98920766241), test loss: 3.13165650666\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (19.4750556946,37.0398138535), test loss: 30.3005800962\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.44496774673,6.91407603942), test loss: 2.89556995034\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (33.8525047302,36.8297777103), test loss: 32.9828785896\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.01406812668,6.84151709475), test loss: 3.10157440305\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (13.1698141098,36.6215725055), test loss: 33.8081377029\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.81877279282,6.77127383067), test loss: 3.26304971874\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (30.4550457001,36.4144584227), test loss: 30.6722049952\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.59933519363,6.70338651318), test loss: 2.94654769003\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (17.1022949219,36.2092147903), test loss: 28.6517825127\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.49645388126,6.6376772982), test loss: 2.90351029932\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (12.8490085602,36.0052154819), test loss: 30.9274086475\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.08557772636,6.5735781531), test loss: 2.78985993862\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (22.9554710388,35.8025676496), test loss: 32.717377758\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.712767362595,6.51142316182), test loss: 3.13333058953\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (16.1639461517,35.6008512852), test loss: 32.360236454\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.34919190407,6.45084659458), test loss: 3.04674915671\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (16.1953620911,35.4009937299), test loss: 28.6346445084\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.20157754421,6.39197384394), test loss: 2.97835818827\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.5212230682,35.2034866657), test loss: 29.1206244946\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.0357632637,6.33500718251), test loss: 2.88744218349\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (11.7186517715,35.0074810846), test loss: 30.9985916138\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.9130821228,6.27925893733), test loss: 2.95507901013\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (32.8524856567,34.8139775354), test loss: 31.9657666683\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.05610752106,6.22502418451), test loss: 3.16287024617\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (14.5776367188,34.621872153), test loss: 30.3323397636\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.08087873459,6.17219583841), test loss: 2.98193883896\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (13.6033325195,34.4313039773), test loss: 27.7976300716\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.1642639637,6.12060343745), test loss: 3.0702150017\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.2819194794,34.2418878942), test loss: 29.2652769327\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (5.2384853363,6.07036270949), test loss: 2.88234987557\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (26.3589515686,34.0551322432), test loss: 30.7836165428\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.0530359745,6.02130538233), test loss: 3.00911217332\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (8.55909252167,33.8690885013), test loss: 32.6753097534\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.6360809803,5.97346608965), test loss: 3.13556760252\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (27.2791786194,33.6868020998), test loss: 30.0533606052\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.07342743874,5.92669559185), test loss: 3.04851682186\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (22.6804084778,33.5042265548), test loss: 28.6750854015\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.85431575775,5.88107595596), test loss: 2.87498870492\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (23.9733638763,33.3226320578), test loss: 29.6876067162\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.05435395241,5.83645933943), test loss: 2.84788318872\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (23.5541572571,33.1434301917), test loss: 30.9118596077\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (5.57056617737,5.79298426241), test loss: 3.17352332473\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (18.3592777252,32.9656509843), test loss: 31.030762434\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.67443239689,5.75034681957), test loss: 3.17191915214\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (5.29246711731,32.7884661577), test loss: 27.5145191908\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.4180932045,5.70876002235), test loss: 3.16349687278\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (57.2258110046,32.6148299228), test loss: 27.9722396851\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.892399728298,5.66804295189), test loss: 2.90476426184\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (23.62241745,32.4412286683), test loss: 30.4237665176\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (4.5735874176,5.62844236902), test loss: 3.08297396004\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (46.3556518555,32.2698595794), test loss: 32.584236002\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (5.35891246796,5.58983111958), test loss: 3.24965040386\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (29.3793544769,32.0995468952), test loss: 29.5521794796\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.5251326561,5.55177701724), test loss: 3.06239231229\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (21.0513839722,31.9300678872), test loss: 28.2294950485\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.13761353493,5.51461498599), test loss: 2.92307302356\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (16.5390510559,31.7618591726), test loss: 29.2670051575\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.22728872299,5.4781197314), test loss: 2.83838114738\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (12.629732132,31.5947232803), test loss: 30.200052762\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.905489087105,5.44241147989), test loss: 3.17437159717\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (16.619972229,31.4293278054), test loss: 31.391218853\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (3.25349378586,5.40765151182), test loss: 3.12586653233\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (15.8307590485,31.2649104401), test loss: 27.2170483112\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.68101549149,5.37344378841), test loss: 3.07350605428\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (13.3474979401,31.1022290813), test loss: 28.3465975761\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.29901194572,5.33989997339), test loss: 2.89344180524\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (12.7023563385,30.9406140866), test loss: 29.0850625515\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.40989232063,5.30710472069), test loss: 3.05419181287\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (12.1042108536,30.7800896105), test loss: 30.2668935776\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.10158121586,5.27488982107), test loss: 3.2508392185\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (13.6277074814,30.6205908516), test loss: 28.96565485\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.58085393906,5.2433006698), test loss: 3.02350636721\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (23.4292373657,30.4627330815), test loss: 27.0131164789\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.72162377834,5.21236340937), test loss: 2.9843659699\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (14.280632019,30.3056402205), test loss: 28.9891584873\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.52712869644,5.18202106142), test loss: 2.87940158248\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (27.4912929535,30.1506575849), test loss: 29.6255277157\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.77795124054,5.15217517233), test loss: 3.10688773692\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (30.0525417328,29.9959618045), test loss: 31.1540536642\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.41363811493,5.12289556047), test loss: 3.09657077193\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (14.8455247879,29.8418859055), test loss: 27.243096447\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.16153228283,5.09416019792), test loss: 3.06227010489\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (15.7438058853,29.6893436364), test loss: 28.0125552177\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.77523899078,5.0659646429), test loss: 2.88116282225\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.6725206375,29.53766173), test loss: 29.2005712509\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.686511397362,5.03816661059), test loss: 2.95995985866\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (10.2494840622,29.386384296), test loss: 30.2049265862\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.91052865982,5.01093028247), test loss: 3.20692824125\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (38.8628234863,29.2371634404), test loss: 29.9604957581\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.82833337784,4.9841195325), test loss: 3.08308589756\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (21.4189929962,29.0880316049), test loss: 26.508958602\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.31083774567,4.95788036161), test loss: 3.17597340345\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (30.6804504395,28.940276317), test loss: 28.4772956848\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (5.17803192139,4.93220887526), test loss: 2.9662735343\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (42.5497398376,28.7931889425), test loss: 28.8799605846\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (3.49365258217,4.90676920407), test loss: 3.03911786377\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (9.8284740448,28.6463823816), test loss: 32.4986493111\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.58785772324,4.88179026049), test loss: 3.25485802293\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (5.61544942856,28.5004124061), test loss: 29.5833146572\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.86432385445,4.85712902848), test loss: 3.14457335472\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (7.02478599548,28.3552420271), test loss: 28.1492609501\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.597295999527,4.83288979035), test loss: 2.96168961525\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (14.3242893219,28.2108907795), test loss: 29.2048675537\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (4.6299829483,4.8091888283), test loss: 2.91628832817\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (12.906920433,28.0671510608), test loss: 30.716855669\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.66880202293,4.78573108759), test loss: 3.20323561728\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (7.25137710571,27.9242592677), test loss: 31.7498750448\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.52248954773,4.76262907583), test loss: 3.31431179643\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (8.20456504822,27.7819503284), test loss: 27.2995383263\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.62730693817,4.73993212776), test loss: 3.27220095694\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (14.4111776352,27.6403224693), test loss: 28.6575017929\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.49444127083,4.71756406287), test loss: 2.98238258958\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (11.1614141464,27.4991896643), test loss: 29.955281496\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.5663279295,4.69549427605), test loss: 3.15230454803\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (21.70429039,27.3590832913), test loss: 32.7872718811\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.37554144859,4.67379778081), test loss: 3.35418502092\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (9.49156188965,27.2194318801), test loss: 30.1218313217\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.1079416275,4.65241078517), test loss: 3.20916850269\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (25.9595603943,27.0806650339), test loss: 28.2737315655\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.11648249626,4.63130590889), test loss: 3.02636036873\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (21.4636230469,26.9425893287), test loss: 30.8971051693\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (4.01907539368,4.61051547316), test loss: 2.91134426892\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (11.2335166931,26.8047872412), test loss: 31.28768363\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.85911822319,4.59000531968), test loss: 3.2284203738\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (8.45773506165,26.6678669233), test loss: 32.8002893448\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.81296539307,4.56975776685), test loss: 3.19639680982\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (9.41625213623,26.5316410862), test loss: 28.1438714981\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.92375934124,4.54979267041), test loss: 3.16788791418\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.6548833847,26.3957119574), test loss: 29.8495361328\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.996146142483,4.53008384812), test loss: 2.97733998895\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (15.8070888519,26.2608848865), test loss: 28.9384255409\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.45698142052,4.51067335577), test loss: 3.05742740631\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (12.013507843,26.1265994599), test loss: 31.1303227425\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.13838863373,4.4915571063), test loss: 3.32637379766\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (10.7764072418,25.992952308), test loss: 31.2047747135\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.21788048744,4.47277495703), test loss: 3.16974774003\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (20.4027862549,25.8600900217), test loss: 28.0710130692\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.19912528992,4.45415024669), test loss: 3.08673240542\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (7.67197751999,25.7275332158), test loss: 30.6376051426\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (4.18515348434,4.4357661064), test loss: 2.9595903039\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.93313312531,25.5954262845), test loss: 29.8393750191\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.36598336697,4.41758432397), test loss: 3.09757394791\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (4.55185508728,25.4639325639), test loss: 33.670604229\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.598994493484,4.39963225127), test loss: 3.25773651302\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (8.83489608765,25.3331513809), test loss: 29.5484840393\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (7.43721866608,4.38200436884), test loss: 3.19545567632\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (10.190451622,25.2030313233), test loss: 29.9961363316\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.61964917183,4.36451246199), test loss: 2.96988614649\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (17.1480484009,25.0735991537), test loss: 30.7613501072\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.957666397095,4.34720406131), test loss: 3.00209032595\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.27947044373,24.9447716298), test loss: 31.6871129036\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.75219631195,4.33014894888), test loss: 3.34334177971\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (7.82029056549,24.816572768), test loss: 32.2628264904\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.11809492111,4.31328159767), test loss: 3.25102787018\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (7.10076284409,24.6890243984), test loss: 28.2809146404\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.4621515274,4.29658073238), test loss: 3.28786432445\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.80315160751,24.5622827596), test loss: 31.499804306\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.2156317234,4.28010203731), test loss: 3.05399188101\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (2.82760167122,24.4361032771), test loss: 30.523392725\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.64846277237,4.26379479507), test loss: 3.14622938037\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (17.6131057739,24.3108011913), test loss: 34.8635659218\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.49156570435,4.24767670884), test loss: 3.42372060418\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (11.1794490814,24.1863276091), test loss: 32.3016734123\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.10899615288,4.23171959765), test loss: 3.31135853976\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.09184026718,24.0623752709), test loss: 31.1960263252\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.85624456406,4.21597562618), test loss: 3.04159862399\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (8.6123752594,23.9392534152), test loss: 32.0700452328\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.998968303204,4.20035398745), test loss: 2.9922811389\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (15.968878746,23.8168951353), test loss: 32.6497635365\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.27197134495,4.18493376378), test loss: 3.31315160096\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (2.50026893616,23.6950189937), test loss: 34.9830044746\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.56967759132,4.16965664784), test loss: 3.43596772552\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (11.5087223053,23.5740639708), test loss: 29.5392094135\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.70853567123,4.15457418509), test loss: 3.36694838107\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (4.7173705101,23.4538207548), test loss: 31.1123650074\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.23767900467,4.1396953613), test loss: 3.07870380282\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (9.26874732971,23.3343731699), test loss: 31.1570720196\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (6.43255996704,4.12501938695), test loss: 3.19191774726\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (4.28415393829,23.2155776146), test loss: 35.5578028202\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.01471138,4.11043984365), test loss: 3.49966790974\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (6.97004795074,23.0977109504), test loss: 33.017396307\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (4.26090431213,4.09604457061), test loss: 3.27609173357\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.68360614777,22.9803986425), test loss: 30.1864687443\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.589869499207,4.08174522259), test loss: 3.09511755109\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (3.59203982353,22.8638697567), test loss: 32.0805803299\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.42242527008,4.06760748676), test loss: 3.01938502789\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (7.64469766617,22.7481244133), test loss: 33.0466549397\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (5.02906894684,4.05364593988), test loss: 3.3605106011\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (5.67586660385,22.6331604662), test loss: 36.1973645687\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.41701924801,4.03980858962), test loss: 3.35401135683\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (10.7886486053,22.5190336508), test loss: 31.6087852001\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.21634411812,4.02610173996), test loss: 3.2785767436\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (7.89508008957,22.4056609811), test loss: 31.7785562038\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.19043707848,4.01253901512), test loss: 3.10785717368\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.12690019608,22.2929655608), test loss: 31.4394979\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.67890071869,3.99911925258), test loss: 3.14002363682\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.77501249313,22.18109895), test loss: 34.2534561157\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.62068343163,3.98580814723), test loss: 3.45459997058\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.78152036667,22.0700612784), test loss: 34.4030659199\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.57802653313,3.97264599524), test loss: 3.3386558637\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (2.55410385132,21.9597608745), test loss: 30.6781249046\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.93138647079,3.95960072729), test loss: 3.30288216472\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.99491596222,21.8502229796), test loss: 32.9696177959\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.01814293861,3.94668121917), test loss: 3.08965814859\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (5.46285581589,21.7416544254), test loss: 31.9114046097\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.60658800602,3.93386702044), test loss: 3.17655955851\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (1.66531443596,21.6337744077), test loss: 37.5380235195\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.73148846626,3.92121092072), test loss: 3.37981366813\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.68877220154,21.5266880627), test loss: 33.1598696709\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.43613219261,3.90864966862), test loss: 3.28090183735\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.33171272278,21.4204076815), test loss: 32.5534172535\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.767075777054,3.89620914532), test loss: 3.1469093293\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (2.30477428436,21.3147999746), test loss: 34.0242138386\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.65170133114,3.88387231098), test loss: 3.07823776901\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (7.96691274643,21.2100775759), test loss: 34.9703061104\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.41695952415,3.87166699965), test loss: 3.44704131782\n",
      "run time for single CV loop: 7112.18155789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:124: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:126: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Deign Net Architecutre and Run Caffe\n",
    "exp_name = 'Exp6_MC'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'HC_CT'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "multi_label = False\n",
    "start_MC=1\n",
    "n_MC=1\n",
    "MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "start_fold = 1\n",
    "n_folds = 5\n",
    "fid_list = [3,6,7,8,10]#np.arange(start_fold,n_folds+1,1)\n",
    "niter = 80000\n",
    "batch_size = 256\n",
    "\n",
    "pretrain = False\n",
    "multimodal_autoencode = False\n",
    "load_pretrained_weights = False\n",
    "HC_snap = 4000\n",
    "CT_snap = 6000\n",
    "\n",
    "if Clinical_Scale in ['BOTH','ADAS13_DX'] :\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "# dr: Dropout rate, lr: learning rate of each modality, tr: loss-weight for each task\n",
    "hype_configs = {\n",
    "                'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':150,'HC_CT_ff':10,'COMB_ff':50,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},\n",
    "                      'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}},\n",
    "                \n",
    "                'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':100,'HC_CT_ff':25,'COMB_ff':50,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},\n",
    "                      'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}},\n",
    "                \n",
    "                'hyp3':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':100,'HC_CT_ff':25,'COMB_ff':100,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},\n",
    "                      'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}},\n",
    "                \n",
    "                'hyp4':{'node_sizes':{'HC_L_ff':100,'HC_R_ff':100,'CT_ff':150,'HC_CT_ff':25,'COMB_ff':100,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':286},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},\n",
    "                      'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}},\n",
    "                \n",
    "#                 'hyp5':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':150,'HC_CT_ff':25,'COMB_ff':25,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':10,'HC_CT':2},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}}\n",
    "\n",
    "                }\n",
    "\n",
    "CV_perf_MC = {}\n",
    "for mc in MC_list:\n",
    "    CV_perf_hype = {}\n",
    "    for hype in hype_configs.keys():    \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "        tr = hype_configs[hype]['tr']\n",
    "        solver_configs = hype_configs[hype]['solver_conf']\n",
    "        \n",
    "        CV_perf = {}\n",
    "        start_time = time.time()\n",
    "        for fid in fid_list:            \n",
    "            print ''\n",
    "            print 'MC # {}, Hype # {}, Fold # {}'.format(mc, hype, fid)\n",
    "            snap_prefix = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}'.format(mc,fid,exp_name,hype,modality)\n",
    "\n",
    "            train_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/train_C688.txt'.format(mc,fid)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "\n",
    "            train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "            with open(train_filename_txt, 'w') as f:\n",
    "                    f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "            if pretrain:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_train.prototxt'.format(mc,fid)\n",
    "                with open(train_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(train_filename_txt, batch_size, node_sizes, modality)))            \n",
    "            else:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(train_net_path, 'w') as f:            \n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            if pretrain:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_test.prototxt'.format(mc,fid)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(test_filename_txt, batch_size, node_sizes,modality)))\n",
    "            else:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            # Define Solver\n",
    "            solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "            with open(solver_path, 'w') as f:\n",
    "                f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, snap_prefix)))\n",
    "\n",
    "            ### load the solver and create train and test nets\n",
    "            #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "            #solver = caffe.get_solver(solver_path)\n",
    "            #solver = caffe.NesterovSolver(solver_path)\n",
    "            solver = caffe.NesterovSolver(solver_path)\n",
    "\n",
    "            if load_pretrained_weights:    \n",
    "                if hype in {'hyp1','hyp3','hyp5'}:\n",
    "                    pre_hype = 'hyp1'\n",
    "                else:\n",
    "                    pre_hype = 'hyp2'\n",
    "                #snap_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_HC_CT_iter_10000_concat50.caffemodel'.format(fid)\n",
    "                #snap_path = baseline_dir + 'API/data/fold{}/train_snaps/Exp11_{}_HC_CT_iter_10000.caffemodel'.format(fid,hype)\n",
    "                snap_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_{}_HC_CT_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'.format(mc,fid,pre_hype,HC_snap,CT_snap)\n",
    "                print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "                solver.net.copy_from(snap_path)\n",
    "\n",
    "            #run caffe\n",
    "            results = run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode)\n",
    "            CV_perf[fid] = results\n",
    "            \n",
    "        print('run time for single CV loop: {}'.format(time.time() - start_time))\n",
    "\n",
    "        CV_perf_hype[hype] = CV_perf\n",
    "        \n",
    "CV_perf_MC[mc] = CV_perf_hype\n",
    "\n",
    "# pickleIt(CV_perf_MC, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.1111111111\n"
     ]
    }
   ],
   "source": [
    "#Time for training \n",
    "#HC_CT fold 9,10, number of iterations: 40k, two hyper-params\n",
    "#start time: 14:47\n",
    "#end time: 15:31\n",
    "#runtime_mean = (13+31)/4\n",
    "#print runtime_mean\n",
    "\n",
    "tx=70 #time for 10k iters\n",
    "itx=4 # num of 10k iters\n",
    "hx=4 #hyp choices\n",
    "fx=10 #k-folds\n",
    "mx=10 #mc-folds\n",
    "\n",
    "num_hrs = (tx*itx*hx*fx*mx)/(3600.0)\n",
    "print num_hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbcAAAJoCAYAAABVztwOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5fb66d4410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "#niter = 30000\n",
    "test_interval = 500\n",
    "n_hype_configs = len(hype_configs.keys())\n",
    "pid = 1\n",
    "for hype in CV_perf_hype.keys(): \n",
    "    CV_perf = CV_perf_hype[hype]\n",
    "    n_CV_configs = len(CV_perf)\n",
    "    for fid in fid_list:        \n",
    "        train_loss_list = CV_perf[fid]['train_loss']\n",
    "        test_loss_list = CV_perf[fid]['test_loss']\n",
    "        \n",
    "        for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "            #plt.figure()\n",
    "            ax1 = plt.subplot(n_hype_configs,n_CV_configs,pid)            \n",
    "            ax1.plot(arange(niter), train_loss, label='train')\n",
    "            ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test', linewidth='3')                            \n",
    "            ax1.set_xlabel('iteration')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_title('fid: {}, hyp: {}, Test loss: {:.2f}'.format(fid, hype, test_loss[-1]))\n",
    "            ax1.legend(loc=1)\n",
    "            #ax1.set_ylim(0,10)\n",
    "        pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp11'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'R_HC'\n",
    "batch_size = 256\n",
    "encoding_layer = 'code'\n",
    "weight_layers = 'code'\n",
    "multi_task = False\n",
    "\n",
    "if modality in ['L_HC', 'R_HC']:\n",
    "    snap_iter = 10000\n",
    "    \n",
    "    if modality == 'R_HC':\n",
    "        output_node_size = 16471\n",
    "    else: \n",
    "        output_node_size = 16086\n",
    "        \n",
    "    hype_configs = {\n",
    "                   'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':8000,'out':output_node_size},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "\n",
    "elif modality in ['CT']:  \n",
    "    snap_iter = 100000\n",
    "    \n",
    "    hype_configs = {\n",
    "                    'hyp1':{'node_sizes':{'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-4, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "elif modality in ['HC_CT']:  \n",
    "    snap_iter = 20000\n",
    "    hype_configs = {\n",
    "                 'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':500,'out_HC':16086,'out_CT':686},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "hype = 'hyp1'\n",
    "pretrain_preproc = 'ae_preproc_sparse_HC_10k_CT_100k_{}'.format(hype)\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "\n",
    "# Save either L or R HC encodings. Turn on CT + CS save only for one of these. \n",
    "# (Also decide if you want to copy CT raw scores or encodings)\n",
    "save_encodings = True\n",
    "save_scores = False\n",
    "CT_encodings = True\n",
    "\n",
    "for cohort in ['inner_train', 'inner_test','outer_test']:\n",
    "    data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "    test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)       \n",
    "    input_nodes = ['X_{}'.format(modality)]\n",
    "    #input_nodes = ['X_L_HC','X_CT']\n",
    "    net_file = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "    with open(net_file, 'w') as f:\n",
    "        f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "\n",
    "    test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "    with open(test_filename_txt, 'w') as f:\n",
    "        f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "    sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "    if modality == 'CT':\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "    else:\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "        \n",
    "    print 'Check Sparsity for model: {}'.format(model_file)\n",
    "    \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    \n",
    "    encodings_hdf_file = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,pretrain_preproc)\n",
    "    \n",
    "    if save_encodings:    \n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        input_data.create_dataset(input_nodes[0],data=encodings)           \n",
    "        input_data.close()\n",
    "        \n",
    "    if save_scores:\n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        if not CT_encodings: #copy raw scores for CT instead of encodings\n",
    "            CT_data = load_data(data_path, 'X_CT','no_preproc')                    \n",
    "            input_data.create_dataset('X_CT', data=CT_data)    \n",
    "            \n",
    "        adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "        mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "        input_data.create_dataset('y', data=adas_scores)           \n",
    "        input_data.create_dataset('y3', data=mmse_scores)    \n",
    "        input_data.create_dataset('dx_cat3', data=dx_labels)\n",
    "        input_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting TSNE embeddings \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "exp_name = 'Exp11'\n",
    "#preproc = 'no_preproc'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "Clinical_Scale = 'ADAS13_DX' \n",
    "batch_size = 256\n",
    "snap_interval = 2000\n",
    "snap_start = 2000\n",
    "encoding_layer = 'ff3'\n",
    "weight_layers = 'ff3'\n",
    "cohort = 'outer_test'\n",
    "multi_task = False\n",
    "\n",
    "modality = 'HC_CT'\n",
    "snap_end = 21000\n",
    "\n",
    "\n",
    "hype_configs = {\n",
    "            'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-2}},\n",
    "}\n",
    "\n",
    "hype = 'hyp1'\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "tr = hype_configs[hype]['tr']\n",
    "\n",
    "data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "test_net_path = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)       \n",
    "#input_node = 'X_{}'.format(modality)\n",
    "\n",
    "net_file = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)\n",
    "with open(net_file, 'w') as f:\n",
    "    #f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "\n",
    "test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "with open(test_filename_txt, 'w') as f:\n",
    "    f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "n_snaps = len(np.arange(snap_start,snap_end,snap_interval))\n",
    "tsne_encodings = {}\n",
    "net_weights = {}\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    print sn, snap_iter\n",
    "    model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    net_weights[snap_iter] = results['wt_dict']\n",
    "    print encodings.shape\n",
    "\n",
    "    adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "    mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "    dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "    tsne_model = TSNE(n_components=2, random_state=0, init='pca')\n",
    "    tsne_encodings[snap_iter] = tsne_model.fit_transform(encodings) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dx_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-5ee105f36158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_wts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcolor_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdx_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RdYlBu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmarker_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dx_labels' is not defined"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "plot_encodings = True\n",
    "plot_wts = False\n",
    "\n",
    "color_scores = dx_labels\n",
    "cm = plt.get_cmap('RdYlBu') \n",
    "marker_size = 100\n",
    "marker_size = (np.mod(color_scores+1,2)+1)*50\n",
    "\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    plt.subplot(np.ceil(n_snaps/2.0),2,sn+1)\n",
    "    if plot_encodings:\n",
    "        plt.scatter(tsne_encodings[snap_iter][:,0],tsne_encodings[snap_iter][:,1],c=color_scores,s=marker_size,cmap=cm)\n",
    "    if plot_wts:\n",
    "        plt.imshow(net_weights[snap_iter][weight_layers])\n",
    "        \n",
    "    plt.title(snap_iter)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp6_MC 1 HC_CT\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.55303686211799585, 0.49693910946598835, 0.60993869812030554, 0.69393038277481611, 0.42177414969242183]\n",
      "ADAS mse: [49.700769704482376, 90.572473360139185, 48.73837119923143, 37.988021638007147, 72.478438097492798]\n",
      "ADAS means: 0.555123840434, 59.8956147999\n",
      "\n",
      "MMSE corr: [0.58931598218829084, 0.47174535490151237, 0.47015150652751014, 0.49813515814628423, 0.37599681158327103]\n",
      "MMSE mse: [3.8974553866244221, 5.9494114676830714, 4.6762546646183072, 5.6847041479726501, 7.4960633482840588]\n",
      "MMSE means: 0.481068962669, 5.54077780304\n",
      "\n",
      "opt_hyp: {8: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 50, 'HC_L_ff': 25, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 150, 'HC_CT_ff': 10, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 25}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'HC_CT': 2, 'CT': 2}}, 10: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 50, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 100, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'HC_CT': 2, 'CT': 2}}, 3: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 50, 'HC_L_ff': 25, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 150, 'HC_CT_ff': 10, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 25}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'HC_CT': 2, 'CT': 2}}, 6: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 50, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 100, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'HC_CT': 2, 'CT': 2}}, 7: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 1}, 'node_sizes': {'COMB_ff': 50, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 100, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 2, 'HC_CT': 2, 'CT': 2}}}\n",
      "\n",
      "opt_snap: {8: 9, 10: 9, 3: 10, 6: 7, 7: 7}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-ab4bb3215497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfid_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcs_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mr_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{}_r'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mMSE_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{}_mse'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mRMSE_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{}_rmse'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "#exp_name = 'Exp11'\n",
    "#niter = 20000\n",
    "#modality = 'CT'\n",
    "#start_fold = 1\n",
    "#n_folds = 10\n",
    "#fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "#preproc = 'no_preproc'\n",
    "batch_size = 256\n",
    "snap_interval = 5000\n",
    "snap_start = 5000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "#Clinical_Scale = 'ADAS13'\n",
    "\n",
    "#MC_list = np.arange(1,2,1)\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "idx = 0  \n",
    "df_perf_dict_adas = {}\n",
    "df_perf_dict_mmse = {}\n",
    "df_perf_dict_opt = {}\n",
    "\n",
    "for mc in MC_list:\n",
    "    print exp_name, mc, modality\n",
    "\n",
    "    for hype in hype_configs.keys():      \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "\n",
    "        for fid in fid_list:\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "            #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "            #with open(test_filename_txt, 'w') as f:\n",
    "            #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                    f.write(str(adninet_ff_HC(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC']\n",
    "                elif modality == 'CT':\n",
    "                    f.write(str(adninet_ff_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_CT']\n",
    "                elif modality == 'HC_CT':\n",
    "                    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "                elif modality == 'HC_CT_unified_hyp1':\n",
    "                    f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_HC_CT']\n",
    "                else:\n",
    "                    print 'Wrong modality'\n",
    "\n",
    "            #print 'Hype # {}, MC # {}, Fold # {}, Clinical_Scale {}'.format(hype, mc, fid, Clinical_Scale)\n",
    "            data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            if Clinical_Scale == 'ADAS13':\n",
    "                act_scores = load_data(data_path, 'adas','no_preproc')\n",
    "            elif Clinical_Scale == 'MMSE': \n",
    "                act_scores = load_data(data_path, 'mmse','no_preproc')\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                act_scores_adas13 = load_data(data_path, 'adas','no_preproc')\n",
    "                act_scores_mmse = load_data(data_path, 'mmse','no_preproc')\n",
    "            else:\n",
    "                print 'unknown clinical scale'\n",
    "\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort)\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            sub.call([\"cp\", baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort), baseline_dir + \n",
    "                      'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)])\n",
    "            #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "            #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "\n",
    "            if Clinical_Scale in ['ADAS13', 'MMSE']:                \n",
    "                multi_task = False\n",
    "                cs_list = ['opt']\n",
    "                iter_euLoss = []\n",
    "                iter_r = []        \n",
    "                iter_pred_scores = []\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = np.squeeze(results['X_out'])            \n",
    "                    iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                    iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "                fold_r[config_idx] = np.array(iter_r)\n",
    "                fold_act_scores[fid] = act_scores\n",
    "                fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                multi_task = True\n",
    "                cs_list = ['adas','mmse']\n",
    "                iter_euLoss_adas13 = []\n",
    "                iter_r_adas13 = []        \n",
    "                iter_pred_scores_adas13 = []\n",
    "                iter_euLoss_mmse = []\n",
    "                iter_r_mmse = []        \n",
    "                iter_pred_scores_mmse = []\n",
    "\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = results['X_out']   \n",
    "                    encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                    encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                    iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                    iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                    iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                    iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "                fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "                fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "                fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "                fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "                fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "                fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "                fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "\n",
    "    \n",
    "    if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "        fold_perf_dict = {'fold_r':fold_r,'fold_euLoss':fold_euLoss,'fold_act_scores':fold_act_scores,'fold_pred_scores':fold_pred_scores}\n",
    "    else: \n",
    "        fold_perf_dict = {'fold_r_adas13':fold_r_adas13,'fold_r_mmse':fold_r_mmse,'fold_euLoss_adas13':fold_euLoss_adas13,'fold_euLoss_mmse':fold_euLoss_mmse,\n",
    "                         'fold_act_scores_adas13':fold_act_scores_adas13,'fold_act_scores_mmse':fold_act_scores_mmse,\n",
    "                          'fold_pred_scores_adas13':fold_pred_scores_adas13,'fold_pred_scores_mmse':fold_pred_scores_mmse}\n",
    "        \n",
    "    opt_metric = 'euLoss' #euLoss or corr\n",
    "    task_weights = {'ADAS':1,'MMSE':0}\n",
    "    save_multitask_results = False\n",
    "    CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/'  \n",
    "    save_path = '{}{}_{}_NN_{}'.format(CV_model_dir,exp_name,mc,modality)\n",
    "\n",
    "    results = compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path)  \n",
    "    \n",
    "    # populate the perf dictionary for 10 MC x 10 folds\n",
    "    model_choice = 'APANN'    \n",
    "    for fid in fid_list: \n",
    "        for cs in cs_list:            \n",
    "            r_valid = results['{}_r'.format(cs)][fid-1]\n",
    "            MSE_valid = results['{}_mse'.format(cs)][fid-1]\n",
    "            RMSE_valid = results['{}_rmse'.format(cs)][fid-1]\n",
    "        \n",
    "            if cs == 'adas':\n",
    "                df_perf_dict_adas[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                     'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                \n",
    "            if cs == 'mmse':\n",
    "                df_perf_dict_mmse[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                     'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                \n",
    "            if cs == 'opt':\n",
    "                df_perf_dict_opt[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                     'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "            idx+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results at: /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/\n"
     ]
    }
   ],
   "source": [
    "#Save df style dictionaly for seaborn plots\n",
    "cohort = 'ADNI1'\n",
    "# df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_tuned.pkl'.format(exp_name, cohort,modality)                \n",
    "# pickleIt(df_perf_dict_adas,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE_tuned.pkl'.format(exp_name, cohort,modality)  \n",
    "pickleIt(df_perf_dict_mmse,df_perf_dict_path)\n",
    "\n",
    "print 'saving results at: {}'.format(CV_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_preproc\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 25)\n",
    "n_rows = n_folds\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "plot_perf = False\n",
    "\n",
    "if multi_task:\n",
    "    euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "    corrs = [fold_r_adas13,fold_r_mmse]\n",
    "else:\n",
    "    euLosses = [fold_euLoss]\n",
    "    corrs = [fold_r]\n",
    "    \n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        \n",
    "        if plot_perf:\n",
    "            plt.figure(fid)\n",
    "            plt.subplot(n_rows,n_cols,1)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "            plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "            plt.legend()\n",
    "            plt.subplot(n_rows,n_cols,2)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "            plt.title('correlation, fid: {}'.format(fid))\n",
    "            plt.legend(loc=2)\n",
    "\n",
    "print preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path):\n",
    "    fold_r_dict = {}\n",
    "    fold_euLoss_dict = {}\n",
    "    fold_act_scores_dict = {}\n",
    "    fold_pred_scores_dict = {}\n",
    "\n",
    "    if multi_task:\n",
    "        fold_r_dict['ADAS'] = fold_perf_dict['fold_r_adas13']\n",
    "        fold_r_dict['MMSE'] = fold_perf_dict['fold_r_mmse']\n",
    "        fold_euLoss_dict['ADAS'] = fold_perf_dict['fold_euLoss_adas13']\n",
    "        fold_euLoss_dict['MMSE'] = fold_perf_dict['fold_euLoss_mmse']\n",
    "        fold_act_scores_dict['ADAS'] = fold_perf_dict['fold_act_scores_adas13']\n",
    "        fold_act_scores_dict['MMSE'] = fold_perf_dict['fold_act_scores_mmse']\n",
    "        fold_pred_scores_dict['ADAS'] = fold_perf_dict['fold_pred_scores_adas13']\n",
    "        fold_pred_scores_dict['MMSE'] = fold_perf_dict['fold_pred_scores_mmse']\n",
    "        \n",
    "        NN_results = computePerfMetrics(fold_r_dict, fold_euLoss_dict, fold_act_scores_dict, fold_pred_scores_dict, opt_metric, hype_configs, \n",
    "                                        Clinical_Scale,task_weights)\n",
    "        adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "        mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "        adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "        mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "        adas_rmse = NN_results['opt_ADAS']['CV_RMSE'].values()\n",
    "        mmse_rmse = NN_results['opt_MMSE']['CV_RMSE'].values()\n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['opt_ADAS']['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['opt_ADAS']['actual_CV_scores']\n",
    "        \n",
    "        print 'ADAS corr: {}'.format(adas_r)\n",
    "        print 'ADAS mse: {}'.format(adas_mse)\n",
    "        print 'ADAS means: {}, {}'.format(np.mean(adas_r),np.mean(adas_mse))\n",
    "        print ''\n",
    "        print 'MMSE corr: {}'.format(mmse_r)\n",
    "        print 'MMSE mse: {}'.format(mmse_mse)\n",
    "        print 'MMSE means: {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse))\n",
    "        print ''\n",
    "        print 'opt_hyp: {}'.format(opt_hyp)\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        \n",
    "        return {'adas_r':adas_r, 'adas_mse':adas_mse, 'adas_rmse':adas_rmse, 'mmse_r':mmse_r, 'mmse_mse':mmse_mse, 'mmse_rmse':mmse_rmse,\n",
    "               'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        NN_results = computeSingleTaskPerfMetrics(fold_perf_dict, opt_metric, hype_configs)\n",
    "        opt_r = NN_results['opt_r'].values()        \n",
    "        opt_mse = NN_results['opt_mse'].values()        \n",
    "        opt_rmse = NN_results['opt_rmse'].values()        \n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['actual_CV_scores']\n",
    "        print 'opt corr: {}'.format(opt_r)\n",
    "        print 'opt mse: {}'.format(opt_mse)\n",
    "        print 'means: {}, {}'.format(np.mean(opt_r),np.mean(opt_mse))\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        print ''\n",
    "    \n",
    "        return {'opt_r':opt_r, 'opt_mse':opt_mse, 'opt_rmse':opt_rmse,'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    \n",
    "\n",
    "\n",
    "    if save_multitask_results:\n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')        \n",
    "        save_path = save_path + '_' + st + '.pkl' \n",
    "        print 'saving results at: {}'.format(save_path)\n",
    "        output = open(save_path, 'wb')\n",
    "        pickle.dump(NN_results, output)\n",
    "        output.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mmse_rmse',\n",
       " 'actual_CV_scores',\n",
       " 'adas_r',\n",
       " 'adas_mse',\n",
       " 'adas_rmse',\n",
       " 'predicted_CV_scores',\n",
       " 'mmse_mse',\n",
       " 'mmse_r']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    \n",
    "     # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "\n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    opt_rmse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "\n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    opt_rmse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "\n",
    "    print 'Clinical_Scale: {}'.format(Clinical_Scale)\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        print 'This function does not work for single clinical scale models. Use the code from the next cell'\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "\n",
    "        fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "        fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "        fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "        fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "\n",
    "        for fid in fid_hype_map.keys():\n",
    "            r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "            r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "            r_perf_array = np.array(fid_r_perf[fid])\n",
    "            euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "            euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "            euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "            if opt_metric == 'euLoss':\n",
    "                h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "            else:\n",
    "                h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "\n",
    "            opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "            opt_snap[fid] = snp\n",
    "\n",
    "            opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "            opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "            opt_rmse_adas[fid] = np.sqrt(2*euLoss_perf_array_adas[h,snp]) #RMSE\n",
    "            opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "            opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "            opt_rmse_mmse[fid] = np.sqrt(2*euLoss_perf_array_mmse[h,snp]) #RMSE\n",
    "\n",
    "            actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "            opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "            actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "            opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "        opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'CV_RMSE':opt_rmse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "        opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'CV_RMSE':opt_rmse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_snap':opt_snap, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "def computeSingleTaskPerfMetrics(fold_perf_dict,opt_metric,hype_configs):\n",
    "    corrs = fold_perf_dict['fold_r']\n",
    "    euLosses = fold_perf_dict['fold_euLoss']\n",
    "    act_scores = fold_perf_dict['fold_act_scores']\n",
    "    pred_scores = fold_perf_dict['fold_pred_scores']\n",
    "\n",
    "    \n",
    "    NN_multitask_results = {}\n",
    "   \n",
    "    snap_array = np.arange(snap_start,niter+1,snap_start)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = {}\n",
    "    opt_mse = {}\n",
    "    opt_rmse = {}\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "    actual_scores = {}\n",
    "    opt_pred_scores = {}\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "        # if want to find best hyp from mse values\n",
    "        if opt_metric == 'euLoss':\n",
    "            h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        else:\n",
    "            h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "            \n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        r = r_perf_array[h,snp]        \n",
    "        opt_r[fid] = r\n",
    "        opt_mse[fid] = 2*eu_loss\n",
    "        opt_rmse[fid] = np.sqrt(2*eu_loss)\n",
    "        #print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)        \n",
    "        opt_snap[fid] = snp\n",
    "        opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "        actual_scores[fid] = fold_act_scores[fid]\n",
    "        opt_pred_scores[fid] = fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp]\n",
    "\n",
    "    #print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r.values()), np.mean(opt_mse.values()))\n",
    "    #print opt_r\n",
    "    #print opt_mse\n",
    "    return {'opt_r':opt_r,'opt_mse':opt_mse,'opt_rmse':opt_rmse,'opt_hype':opt_hype,'opt_snap':opt_snap, 'actual_scores':actual_scores,'opt_pred_scores':opt_pred_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "\n",
    "model_file = 'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-05-06-17-14-49.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {4:0,5:1}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_ADAS13_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(5.64658243068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update fold performance for multitask\n",
    "print NN_results['opt_ADAS'].keys()\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "#model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl'\n",
    "model_file_soa = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-23-52.pkl'\n",
    "print 'current state of art: ' + model_file_soa\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file_soa,'rb'))\n",
    "\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "#load old file\n",
    "# model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-02-48.pkl'\n",
    "# print 'updated fold result file: ' + model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "idx_pairs = {1:1,3:3}\n",
    "CV_data = update_multifold_perf(boxplots_dir + model_file_soa, NN_results, idx_pairs)\n",
    "\n",
    "print \"\"\n",
    "print 'updated CV_data'\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "#print 'saving results at: {}'.format(save_name)\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(CV_data, output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():        \n",
    "        for idx in idx_pairs.keys():            \n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "\n",
    "    return CV_data\n",
    "\n",
    "def update_multifold_perf(saved_perf_file, new_perf_dict, idx_pairs):    \n",
    "    perf_metrics = ['CV_r', 'actual_CV_scores', 'CV_MSE', 'predicted_CV_scores']    \n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        if key == 'opt_hype':\n",
    "            for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                CV_data[key][idx_key] = new_perf_dict[key][idx_val]\n",
    "                \n",
    "        else:\n",
    "            for pm in perf_metrics:\n",
    "                for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                    CV_data[key][pm][idx_key] = new_perf_dict[key][pm][idx_val]\n",
    "    \n",
    "    return CV_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "model_files = ['Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl',             \n",
    "             'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-05-23-07-31-29.pkl']\n",
    "\n",
    "#'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-03-19-29-16_Fold9_10.pkl'\n",
    "\n",
    "Clinical_Scale = 'ADAS'\n",
    "perf_array = []\n",
    "for mf in model_files:\n",
    "    CV_data = pickle.load(open(boxplots_dir + mf,'rb'))['opt_{}'.format(Clinical_Scale)]        \n",
    "    perf_array.append(CV_data['CV_r'].values())\n",
    "\n",
    "print (np.max(np.array(perf_array),axis=0))\n",
    "print np.mean(np.max(np.array(perf_array),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "\n",
    "\n",
    "save_detailed_results = True\n",
    "multi_task = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results    \n",
    "    if not multi_task:\n",
    "        NN_results = {}\n",
    "        NN_results['CV_MSE'] = opt_mse\n",
    "        NN_results['CV_r'] = opt_r\n",
    "        NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "        NN_results['actual_CV_scores'] = actual_scores\n",
    "        NN_results['tid_snap_config_dict'] = opt_hype\n",
    "        print opt_r\n",
    "        print opt_mse\n",
    "        \n",
    "    else:\n",
    "        NN_results = NN_multitask_results\n",
    "        print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "        print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "        print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "        print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])\n",
    "        \n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_BOTH_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    for modality in modalities:\n",
    "        in_data = load_data(in_data_path, 'Fold_{}_{}'.format(fid,modality), preproc)         \n",
    "\n",
    "        # HDF5 is pretty efficient, but can be further compressed.\n",
    "        comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "        with h5py.File(out_data_path, 'a') as f:      \n",
    "            if modality == 'X_R_CT': #Fix the typo            \n",
    "                modality = 'X_CT'            \n",
    "\n",
    "            f.create_dataset('{}'.format(modality), data=in_data, **comp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp6'\n",
    "exp_name_out = 'Exp6_MC'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "modalities = ['X_L_HC','X_R_HC','X_R_CT','adas','mmse','dx']\n",
    "dataset = 'ADNI1'\n",
    "preproc = 'no_preproc' #binary labels\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "\n",
    "for mc in np.arange(1,3,1):\n",
    "    for fid in np.arange(1,11,1):\n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_train_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_valid_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_ADAS13_NN_valid_MC_{}.h5'.format(exp_name,dataset,mc)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name_out)\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC: fid[1:4] 8000; fid[5,6] 6000, fid[7,8,9]: 8000 fid 10: 6000\n",
    "CT: fid[1:5] 12000 fid[6:10] 6000\n",
    "\n",
    "#spawn net\n",
    "ADNI_FF_train_hyp1_CT.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "modality = 'HC_CT'\n",
    "pretrain_snap_HC = 4000\n",
    "pretrain_snap_CT = 6000\n",
    "n_folds = 10\n",
    "\n",
    "#Custom snaps per fold\n",
    "#HC_iter = [8000,8000,8000,8000,6000,6000,8000,8000,8000,6000]\n",
    "#CT_iter = [12000,12000,12000,12000,12000,6000,6000,6000,6000,6000]\n",
    "\n",
    "hyp = 'hyp2'\n",
    "for fid in np.arange(1,n_folds+1,1):\n",
    "    print 'fid: {}'.format(fid)\n",
    "    #for AE_branch in ['CT','R_HC','L_HC']:\n",
    "    for AE_branch in ['CT','HC']:\n",
    "        print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "        if AE_branch == 'L_HC':\n",
    "            params_FF = ['L_ff1', 'L_ff2']            \n",
    "            AE_iter = 12000\n",
    "        elif AE_branch == 'R_HC':\n",
    "            params_FF = ['R_ff1', 'R_ff2']            \n",
    "            AE_iter = 12000\n",
    "        elif AE_branch == 'HC':\n",
    "            params_FF = ['L_ff1','R_ff1']\n",
    "            AE_iter = pretrain_snap_HC\n",
    "        elif AE_branch == 'CT':\n",
    "            #params_FF = ['ff1', 'ff2']\n",
    "            params_FF = ['ff1']\n",
    "            AE_iter = pretrain_snap_HC\n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            #Only use this during 1 of the modalities to avoid overwritting\n",
    "            print 'Spawning new net'\n",
    "            pretrain_net = caffe.Net(baseline_dir + 'API/data/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(1,hyp,modality), caffe.TRAIN)\n",
    "        else:\n",
    "            print 'Wrong AE branch'\n",
    "\n",
    "        # conv_params = {name: (weights, biases)}\n",
    "        conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "        for conv in params_FF:\n",
    "            print '{} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "        # Review AE net params \n",
    "        #fid for pretain is 1 because it's same definition for all the folds.\n",
    "        net_file = baseline_dir + 'API/data/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(1,hyp,AE_branch)\n",
    "        #model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/Exp11_{}_{}_iter_{}.caffemodel'.format(fid,hyp,AE_branch,AE_iter) \n",
    "\n",
    "        AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "        #params_AE = ['encoder1', 'code']\n",
    "        params_AE = params_FF #if you are using pretrained NN\n",
    "        \n",
    "        # fc_params = {name: (weights, biases)}\n",
    "        fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "        for fc in params_AE:\n",
    "            print '{} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "        #transplant net parameters\n",
    "        for pr, pr_conv in zip(params_AE, params_FF):\n",
    "            conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "            conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "        save_net = True\n",
    "        if save_net:\n",
    "            save_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_{}_{}_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'.format(fid,hyp,modality,pretrain_snap_HC,pretrain_snap_CT)\n",
    "            print \"Saving net to \" + save_path\n",
    "            pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_data['opt_ADAS']['CV_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print n_snaps/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
