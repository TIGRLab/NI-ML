{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import caffe\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.y  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "#    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "#    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=0.5))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=0.5))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,concat_param=dict(axis=1))\n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='gaussian',std=0.177))    \n",
    "    n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))    \n",
    "    #n.accuracy = L.Accuracy(n.ip1, n.label)\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y)    \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y3)              \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT,n.y  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "    n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff3 = L.InnerProduct(n.ff2, num_output=node_sizes['ff2'], param=dict(lr_mult=2), weight_filler=dict(type='xavier'))\n",
    "    #n.NLinEnL3 = L.ReLU(n.ff3, in_place=True)\n",
    "    #n.dropL = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff2'], param=dict(lr_mult=2), weight_filler=dict(type='xavier'))\n",
    "    #n.NLinEnL4 = L.ReLU(n.ff4, in_place=True)\n",
    "    #n.dropL4 = L.Dropout(n.ff4, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "\n",
    "    #--------Output--------------#\n",
    "    n.output = L.InnerProduct(n.ff2, num_output=1, param=dict(lr_mult=1), weight_filler=dict(type='xavier'))    \n",
    "    #n.accuracy = L.Accuracy(n.ip1, n.label)\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y)    \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y3)    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.y  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.y,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "#     n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,n.ff1, concat_param=dict(axis=1))\n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinMMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "\n",
    "\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.y,loss_weight=1)          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.y3,loss_weight=1)  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder1 = L.InnerProduct(n.X_CT, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers  \n",
    "    n.NLinEn1 = L.ReLU(n.encoder1, in_place=True)\n",
    "    #n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "    #n.NLinEn2 = L.Sigmoid(n.encoder2, in_place=True)\n",
    "    #code layer\n",
    "    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=0.177))  \n",
    "    #Decoder layers\n",
    "    n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "    #n.decoder2 = L.InnerProduct(n.decoder1, num_output=5000, weight_filler=dict(type='xavier'))\n",
    "    #n.NlinDe2 = L.Sigmoid(n.decoder2, in_place=True)\n",
    "    \n",
    "    n.output = L.InnerProduct(n.decoder1, num_output=node_sizes['out'], weight_filler=dict(type='gaussian',std=0.177))    \n",
    "    #n.accuracy = L.Accuracy(n.ip1, n.label)\n",
    "    if modality == 'CT':\n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)        \n",
    "    elif modality =='R_HC':\n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    return n.to_proto()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 2 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,multi_task):\n",
    "\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 100\n",
    "    #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "    # losses will also be stored in the log\n",
    "    if not multi_task:\n",
    "        train_loss = zeros(niter)\n",
    "        test_loss = zeros(int(np.ceil(niter / test_interval)))        \n",
    "    else: \n",
    "        train_ADAS13_loss = zeros(niter)\n",
    "        test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_MMSE_loss = zeros(niter)\n",
    "        test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "    #output = zeros((niter, batch_size))\n",
    "    #solver.restore()\n",
    "    #the main solver loop\n",
    "    for it in range(niter):\n",
    "        #solver.net.forward()\n",
    "        solver.step(1)  # SGD by Caffe    \n",
    "        # store the train loss\n",
    "        if not multi_task:\n",
    "            train_loss[it] = solver.net.blobs['loss'].data        \n",
    "        else: \n",
    "            train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "            train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "        # store the output on the first test batch\n",
    "        # (start the forward pass at conv1 to avoid loading new data)\n",
    "        #solver.test_nets[0].forward()\n",
    "        #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "        # run a full test every so often\n",
    "        # (Caffe can also do this for us and write to a log, but we show here\n",
    "        #  how to do it directly in Python, where more complicated things are easier.)\n",
    "        if it % test_interval == 0:        \n",
    "            t_loss = 0\n",
    "            t_ADAS13_loss = 0\n",
    "            t_MMSE_loss = 0\n",
    "            for test_it in range(test_iter):\n",
    "                solver.test_nets[0].forward()\n",
    "                if not multi_task:\n",
    "                    t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                else: \n",
    "                    t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                    t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "            if not multi_task:\n",
    "                test_loss[it // test_interval] = t_loss/test_iter            \n",
    "                print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                    it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval])\n",
    "            else:\n",
    "                test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/test_iter\n",
    "                test_MMSE_loss[it // test_interval] = t_MMSE_loss/test_iter            \n",
    "                print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                    it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                    it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "    if not multi_task:\n",
    "        perf = {'train_loss':[train_loss],'test_loss':[test_loss]}\n",
    "    else:\n",
    "        perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,fid,exp_name,modality):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "\n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    s.type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 200000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 5000\n",
    "    s.snapshot_prefix = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}'.format(fid,exp_name,hype,modality)\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hype # hyp1, Fold # 1\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (146.757263184,inf), test loss: 165.149679031\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (361.337341309,inf), test loss: 350.273678284\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (113.262268066,57.4790588379), test loss: 51.8605964661\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.36686229706,15.5323260837), test loss: 4.76897688389\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (22.6414413452,55.1424179983), test loss: 50.5715242004\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.21889019012,9.55094330747), test loss: 4.38546725333\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (22.7381896973,54.020217673), test loss: 51.1365011406\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.4671831131,7.53379957057), test loss: 4.42742974728\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (34.1791992188,53.4945012312), test loss: 51.3004525089\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.54277539253,6.51521596127), test loss: 4.40385705441\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (98.752456665,53.1392021282), test loss: 50.8678245544\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (7.39963150024,5.89923553756), test loss: 4.49789042711\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (43.2679176331,52.7779949395), test loss: 50.2536284971\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.92540454865,5.48279892919), test loss: 4.35231294364\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (25.6048812866,52.4406030619), test loss: 51.0551949596\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.17705416679,5.18060088847), test loss: 4.2597299403\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (47.3331947327,52.1269969267), test loss: 49.8956858063\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.84693622589,4.94935199029), test loss: 4.30763310313\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (24.7187213898,51.8993972069), test loss: 48.6955950069\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.97763562202,4.76955622148), test loss: 4.16253878474\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (46.9851264954,51.582635632), test loss: 49.2302309227\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.25636291504,4.62071134752), test loss: 4.15515961051\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (33.7912864685,51.2860221086), test loss: 48.9172433281\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (0.552779018879,4.49792614074), test loss: 4.09613437653\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (25.8533725739,50.9675303514), test loss: 47.914057951\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.174930810928,4.39285252504), test loss: 4.21747339368\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (87.7883148193,50.6147435839), test loss: 47.6589376736\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (5.3342256546,4.30511687076), test loss: 4.29021064311\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (24.2697544098,50.2152419749), test loss: 47.798257761\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (0.804914832115,4.22829590054), test loss: 3.99134726882\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (27.4809017181,49.7572055191), test loss: 44.7636246872\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.64411151409,4.16074950762), test loss: 4.03655522823\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (75.353729248,49.3159006346), test loss: 46.1731189632\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.58262753487,4.10166619834), test loss: 3.9924483186\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (23.6872081757,48.780088284), test loss: 44.4043108559\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.89203667641,4.04773666554), test loss: 3.96850076199\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.534614563,48.2349661454), test loss: 42.9595758438\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.5259386301,3.99975928077), test loss: 3.9435939765\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (127.660354614,47.6581059912), test loss: 45.8163057041\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.57908153534,3.9549895492), test loss: 4.26074755967\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (115.985527039,47.0644175708), test loss: 44.9442547512\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.45641469955,3.9146606537), test loss: 4.05297022849\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (55.0527038574,46.4741252022), test loss: 40.6286384106\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.74023580551,3.87623182644), test loss: 3.93590371788\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (28.4764041901,45.8577846471), test loss: 40.5232339478\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.38702344894,3.83942922195), test loss: 3.90350547612\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (73.7792816162,45.2907007226), test loss: 44.7372045851\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (5.78836727142,3.80475335489), test loss: 3.84668914914\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (30.4421081543,44.6961873779), test loss: 40.007369523\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (8.04806137085,3.77047903734), test loss: 3.87432883978\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (13.0449371338,44.1212812744), test loss: 41.6180564594\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.1963994503,3.73758270594), test loss: 3.90037374496\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (13.3344993591,43.545999025), test loss: 39.2689262152\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (5.27776861191,3.7046639204), test loss: 4.15299681038\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (49.8518829346,42.9910113577), test loss: 38.2744344521\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (6.80572032928,3.67421412281), test loss: 3.8027596131\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (28.0972423553,42.444865777), test loss: 38.101865108\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.59452366829,3.64342501111), test loss: 3.88851521045\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (18.5210418701,41.8852935011), test loss: 41.4637692595\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (4.354845047,3.61313131489), test loss: 3.88187800348\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (10.2560882568,41.354914221), test loss: 38.014832859\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.44253182411,3.58345562586), test loss: 3.86579149425\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (17.2759723663,40.82304348), test loss: 39.9055057859\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.80356144905,3.55366486182), test loss: 3.93517644674\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (7.81908750534,40.3013185143), test loss: 41.2336180925\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.80591320992,3.52512819371), test loss: 3.87776305795\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (9.66521549225,39.7802791695), test loss: 38.2562100792\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.47304797173,3.49572892861), test loss: 4.13684669763\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (7.61551570892,39.2632328748), test loss: 36.102999177\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.529848456383,3.4677237203), test loss: 3.97388289511\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (6.28482818604,38.7671302896), test loss: 39.0961996365\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.8914899826,3.4399875073), test loss: 3.94426460683\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (13.5607681274,38.2672665978), test loss: 37.0664324999\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.86785101891,3.41220125085), test loss: 3.95205521196\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (54.9771575928,37.7796965251), test loss: 36.6119968343\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.5061416626,3.3847870759), test loss: 3.81646639466\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (3.80695652962,37.2936799202), test loss: 37.475442524\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.28127098083,3.35741077212), test loss: 3.93725612283\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (30.9115581512,36.8181572428), test loss: 36.5166312051\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.00518226624,3.33137868293), test loss: 3.84587428212\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (6.22096252441,36.3546500068), test loss: 37.7733758926\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.03838574886,3.30473312816), test loss: 3.9944734396\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (32.8363189697,35.8885627578), test loss: 38.5137528229\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.94325113297,3.27917889222), test loss: 4.01865082428\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (6.89110183716,35.4367781435), test loss: 42.6652730799\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.91195750237,3.25385198023), test loss: 3.93577906698\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (21.6516971588,34.9828737411), test loss: 39.7249619389\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.80655670166,3.22874469866), test loss: 3.94134563923\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (28.3582210541,34.5409248942), test loss: 37.8036524677\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.520084261894,3.20405717865), test loss: 3.82583283961\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (14.8312282562,34.1056306221), test loss: 36.6574553967\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.19314944744,3.17957638342), test loss: 4.09975879818\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (17.7650184631,33.6692616894), test loss: 36.276766777\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.35047388077,3.15612205579), test loss: 3.8884682402\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (15.7861185074,33.2441095384), test loss: 38.3569592571\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.12369740009,3.13211918881), test loss: 4.01880324125\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (14.1832513809,32.8221861159), test loss: 36.8432048678\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.552575409412,3.10903803474), test loss: 3.90348686755\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (9.23534584045,32.4113728384), test loss: 36.4888525438\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.79291141033,3.08608818563), test loss: 3.85043178499\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (6.09390592575,31.9981912571), test loss: 37.4400127697\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.46835231781,3.06303191766), test loss: 3.95687312275\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (9.45851516724,31.5919041533), test loss: 36.7789494753\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.35659766197,3.04026770123), test loss: 3.85029630601\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (16.6466217041,31.1972130044), test loss: 36.562669425\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.82196354866,3.01763510789), test loss: 3.7860251376\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (5.80928897858,30.8046750933), test loss: 40.9725682831\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.17950022221,2.9960758378), test loss: 3.85334817648\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (6.45718097687,30.4207147575), test loss: 40.3083312941\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.485685735941,2.97437534155), test loss: 4.02976367801\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (16.8468818665,30.0386838812), test loss: 37.9274896002\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.68901407719,2.95338484394), test loss: 3.82818720102\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (15.7548160553,29.6654709238), test loss: 37.945074873\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.754614531994,2.93270719557), test loss: 3.86393636167\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (5.39196920395,29.297654628), test loss: 39.0615151358\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.37204456329,2.9123547733), test loss: 3.81223397166\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (5.76452112198,28.9342759229), test loss: 38.3095568609\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.07771170139,2.89258620758), test loss: 3.81359323442\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (4.9395031929,28.5793328016), test loss: 37.2247047234\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.42742669582,2.87275073382), test loss: 3.75739809871\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (2.31281614304,28.2267528837), test loss: 37.927586937\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.31023919582,2.85386785838), test loss: 3.79837234616\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (8.95828437805,27.8849751185), test loss: 41.2243460798\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.742327928543,2.83498971753), test loss: 4.11040134355\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (8.44263076782,27.5458835115), test loss: 38.0885269737\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.34411740303,2.81670230902), test loss: 3.7435499531\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (20.3487491608,27.2132411647), test loss: 47.1825969219\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.20388829708,2.79856717259), test loss: 3.9580687201\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (4.48262119293,26.8851121239), test loss: 38.2917795324\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.11892986298,2.78049322661), test loss: 3.83246990606\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (5.78371524811,26.5635717259), test loss: 38.6467739534\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.793863296509,2.76330829893), test loss: 3.79137741029\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (7.20618915558,26.2496355368), test loss: 39.4519879055\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.362507522106,2.74583894712), test loss: 3.81040694445\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (1.78806388378,25.9391883858), test loss: 38.4254024601\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.6454012394,2.72928254142), test loss: 3.8570542866\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (3.2100007534,25.6351171161), test loss: 39.434070406\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.78986787796,2.71255265917), test loss: 4.06513856798\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (3.76619839668,25.3355049276), test loss: 38.2130205488\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.13181352615,2.69628087569), test loss: 3.74916497886\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (9.57615184784,25.0433499167), test loss: 39.2258568954\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.95712447166,2.68041972878), test loss: 3.79387417257\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (4.0766916275,24.7557177315), test loss: 40.5200044346\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.28655743599,2.66442093429), test loss: 3.85786617473\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (5.76611614227,24.4730700194), test loss: 39.2553384209\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.069886446,2.64912399099), test loss: 3.81554248035\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (3.42946624756,24.1958144198), test loss: 38.7996330452\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.967387974262,2.63358605398), test loss: 3.81353561491\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (4.01428699493,23.9243702746), test loss: 40.1285033035\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.38336491585,2.61891915172), test loss: 4.17784975722\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (2.9492585659,23.6581947741), test loss: 38.4571517611\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.438860654831,2.60410129513), test loss: 3.82099629134\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (5.75992488861,23.3965191009), test loss: 39.9997181511\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.719084024429,2.58960140195), test loss: 3.87126701117\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (2.6110291481,23.139288218), test loss: 38.8190991688\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.261546731,2.57533519163), test loss: 3.58581638634\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (6.72231292725,22.8875213381), test loss: 38.4347476339\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.984019398689,2.561071225), test loss: 3.84244231045\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (4.06265830994,22.6410902339), test loss: 39.8068556261\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.12781429291,2.54758349739), test loss: 3.79076744914\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (2.43827652931,22.3990442204), test loss: 39.9279042292\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.26321053505,2.53368640913), test loss: 3.80574469119\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (1.99089050293,22.1613127286), test loss: 39.4216638327\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.65002655983,2.5204286503), test loss: 3.81764365226\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (7.27547502518,21.9280970299), test loss: 39.0473550749\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.73413920403,2.50721967527), test loss: 3.79010060072\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (3.90708303452,21.6994300099), test loss: 41.713506813\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.2092499733,2.49422922531), test loss: 3.92585594058\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (3.49162530899,21.4752057776), test loss: 38.1804245949\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.63255500793,2.48142652283), test loss: 3.56647173196\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (2.3100130558,21.2551659697), test loss: 40.2768726492\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.727140188217,2.46853761459), test loss: 3.77159522653\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (0.858871340752,21.039085022), test loss: 39.2574665213\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.25553607941,2.45629320458), test loss: 3.96957121432\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (0.703497886658,20.8273981475), test loss: 39.3090292883\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.501553356647,2.44386624353), test loss: 3.8013073653\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (2.40164136887,20.6199371251), test loss: 38.9831906796\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.89256715775,2.43187241707), test loss: 3.79448675454\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (2.3354036808,20.4160675168), test loss: 44.5350766754\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.130118221045,2.4198896548), test loss: 3.91509805471\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (0.687183856964,20.2156569572), test loss: 38.8636037779\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.36702537537,2.40806732996), test loss: 3.76656507239\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (3.08085489273,20.0191556492), test loss: 39.3573805904\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.8774061203,2.39649640379), test loss: 3.7582548207\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (2.04838013649,19.8265138504), test loss: 39.8461193037\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.23647177219,2.38484560625), test loss: 3.9193924123\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (2.13580703735,19.6375731924), test loss: 38.6255479527\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.682263851166,2.37364633017), test loss: 3.87588261515\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (1.32107698917,19.4518226694), test loss: 39.4823321676\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.06684720516,2.36235707958), test loss: 3.85824220583\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (2.54005050659,19.2694574288), test loss: 39.7739133978\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.03400301933,2.35138941043), test loss: 3.727876634\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (4.34632778168,19.0903842782), test loss: 45.1090870571\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.80417704582,2.34052142831), test loss: 3.93847450227\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (3.7768175602,18.9147212525), test loss: 38.7325672817\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (4.64725399017,2.32965154465), test loss: 3.76981081933\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (1.75248932838,18.7420368931), test loss: 39.0419388723\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.51616764069,2.3190826653), test loss: 3.67941325426\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (1.6134083271,18.5722596805), test loss: 39.2412135029\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (3.27768659592,2.30841051817), test loss: 3.83062327012\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (1.55629611015,18.4057978348), test loss: 38.5198247671\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.85958957672,2.29816334128), test loss: 3.77400830045\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (2.18569636345,18.242249839), test loss: 40.8995681667\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.692748010159,2.28779491299), test loss: 3.84024018079\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (1.72304272652,18.0815535705), test loss: 38.3610520267\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.79386591911,2.27768988429), test loss: 3.58068872154\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (2.12771129608,17.9234034433), test loss: 38.9289249277\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.22546339035,2.26768370861), test loss: 3.70087949544\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (1.3012367487,17.7680688655), test loss: 40.250540123\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.49218678474,2.25761016233), test loss: 3.77858876437\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (1.77851986885,17.6154580627), test loss: 41.4463273907\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.00220823288,2.24795747669), test loss: 3.67250377804\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (0.759113430977,17.4655641022), test loss: 38.9787431049\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.976467192173,2.2380637898), test loss: 3.79540609181\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (2.01339125633,17.3180550703), test loss: 39.1207760572\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.735427975655,2.22858511928), test loss: 3.99568741798\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (1.10584831238,17.1730037568), test loss: 39.3320686531\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.21567821503,2.21906847192), test loss: 3.89405510753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:59: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:61: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-f114f30510a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;31m#run caffe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_caffe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mniter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmulti_task\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[0mCV_perf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mrun_caffe\u001b[1;34m(solver, niter, multi_task)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Generate API style txt files\n",
    "exp_name = 'Exp11'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'HC_CT'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "start_fold = 1\n",
    "n_folds = 1\n",
    "niter = 50000\n",
    "#node_sizes = {'L_ff1':5000,'R_ff1':5000,'L_ff2':500,'R_ff2':500,'ff1':500,'ff2':100,'ff3':50,'ff4':50}\n",
    "#node_sizes = {'En1':500,'code':100,'out':686}\n",
    "#L,R: 16086,16471\n",
    "pretrain = False\n",
    "load_pretrained_weights = False\n",
    "\n",
    "if Clinical_Scale == 'BOTH':\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "hype_configs = {\n",
    "                'hyp1':{'node_sizes':{'L_ff1':25,'R_ff1':25,'L_ff2':25,'R_ff2':25,'ff1':250,'ff2':250,'ff3':50,'ff4':50},\n",
    "                       'dr':{'HC':0,'CT':0},'lr':{'HC':1,'CT':5},'solver_conf':{'base_lr':7e-6, 'wt_decay':1e-4}},\n",
    "#                 'hyp2':{'node_sizes':{'L_ff1':25,'R_ff1':25,'L_ff2':25,'R_ff2':25,'ff1':150,'ff2':150,'ff3':25,'ff4':25},\n",
    "#                        'dr':{'HC':0,'CT':0},'lr':{'HC':1,'CT':5},'solver_conf':{'base_lr':7e-6, 'wt_decay':1e-4}},\n",
    "#                 'hyp3':{'node_sizes':{'L_ff1':25,'R_ff1':25,'L_ff2':25,'R_ff2':25,'ff1':100,'ff2':100,'ff3':25,'ff4':25},\n",
    "#                        'dr':{'HC':0,'CT':0},'lr':{'HC':2,'CT':5},'solver_conf':{'base_lr':7e-6, 'wt_decay':1e-4}},\n",
    "#                 'hyp4':{'node_sizes':{'L_ff1':25,'R_ff1':25,'L_ff2':25,'R_ff2':25,'ff1':50,'ff2':50,'ff3':25,'ff4':25},\n",
    "#                        'dr':{'HC':0,'CT':0},'lr':{'HC':2,'CT':5},'solver_conf':{'base_lr':7e-6, 'wt_decay':1e-4}}\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "CV_perf_hype = {}\n",
    "for hype in hype_configs.keys():    \n",
    "    node_sizes = hype_configs[hype]['node_sizes']\n",
    "    dr = hype_configs[hype]['dr']\n",
    "    lr = hype_configs[hype]['lr']\n",
    "    solver_configs = hype_configs[hype]['solver_conf']\n",
    "    \n",
    "    CV_perf = {}\n",
    "    for fid in np.arange(start_fold,n_folds+1,1):\n",
    "        print 'Hype # {}, Fold # {}'.format(hype, fid)\n",
    "        train_filename_txt = baseline_dir + 'API/data/fold{}/train_C688.txt'.format(fid)\n",
    "        test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "\n",
    "        train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "        test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "        with open(train_filename_txt, 'w') as f:\n",
    "                f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "        with open(test_filename_txt, 'w') as f:\n",
    "                f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "        # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "        if pretrain:\n",
    "            train_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_train.prototxt'.format(fid)\n",
    "            with open(train_net_path, 'w') as f:\n",
    "                f.write(str(adninet_ae(train_filename_txt, 256, node_sizes, modality)))            \n",
    "        else:\n",
    "            train_net_path = baseline_dir + 'API/data/fold{}/ADNI_FF_train.prototxt'.format(fid)\n",
    "            with open(train_net_path, 'w') as f:            \n",
    "                if modality == 'HC':\n",
    "                      f.write(str(adninet_ff_HC(train_filename_txt, 256, node_sizes,dr, lr, Clinical_Scale)))\n",
    "                elif modality == 'CT':\n",
    "                      f.write(str(adninet_ff_CT(train_filename_txt, 256, node_sizes,dr, lr, Clinical_Scale)))\n",
    "                elif modality == 'HC_CT':\n",
    "                      f.write(str(adninet_ff_HC_CT(train_filename_txt, 256, node_sizes,dr, lr, Clinical_Scale )))\n",
    "                else:\n",
    "                      print 'Wrong modality'\n",
    "\n",
    "        if pretrain:\n",
    "            test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "        else:\n",
    "            test_net_path = baseline_dir + 'API/data/fold{}/ADNI_FF_test.prototxt'.format(fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                      f.write(str(adninet_ff_HC(test_filename_txt, 256, node_sizes,dr, lr, Clinical_Scale)))\n",
    "                elif modality == 'CT':\n",
    "                      f.write(str(adninet_ff_CT(test_filename_txt, 256, node_sizes,dr, lr, Clinical_Scale)))\n",
    "                elif modality == 'HC_CT':\n",
    "                      f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr, lr, Clinical_Scale)))\n",
    "                else:\n",
    "                      print 'Wrong modality'\n",
    "\n",
    "        # Define Solver\n",
    "        solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "        with open(solver_path, 'w') as f:\n",
    "            f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, fid, exp_name, modality)))\n",
    "\n",
    "        ### load the solver and create train and test nets\n",
    "        #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "        solver = caffe.get_solver(solver_path)\n",
    "\n",
    "        if load_pretrained_weights:    \n",
    "            #snap_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_HC_CT_iter_10000_concat50.caffemodel'.format(fid)\n",
    "            snap_path = baseline_dir + 'API/data/fold{}/train_snaps/Exp11_{}_HC_CT_iter_30000.caffemodel'.format(fid,hype)\n",
    "            print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "            solver.net.copy_from(snap_path)\n",
    "\n",
    "        #run caffe\n",
    "        results = run_caffe(solver,niter,multi_task)\n",
    "        CV_perf[fid] = results\n",
    "\n",
    "    CV_perf_hype[hype] = CV_perf\n",
    "# in case the kernel dies... \n",
    "pickleIt(CV_perf_hype, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modality = 'HC_CT'\n",
    "# pkl_file = open(baseline_dir + 'API/CV_perf/{}'.format(modality), 'rb')\n",
    "# CV_perf = pickle.load(pkl_file) \n",
    "# pkl_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 30)\n",
    "niter = 50000\n",
    "test_interval = 500\n",
    "n_folds = 1\n",
    "n_hype_configs = len(hype_configs.keys())\n",
    "pid = 1\n",
    "for hype in CV_perf_hype.keys(): \n",
    "    CV_perf = CV_perf_hype[hype]\n",
    "    n_CV_configs = len(CV_perf)\n",
    "    for fid in [1]: #np.arange(start_fold,n_folds+1,1):        \n",
    "        train_loss_list = CV_perf[fid]['train_loss']\n",
    "        test_loss_list = CV_perf[fid]['test_loss']\n",
    "        \n",
    "        for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "            ax1 = plt.subplot(n_hype_configs,n_CV_configs,pid)\n",
    "            #ax2 = ax1.twinx()\n",
    "            ax1.plot(arange(niter), train_loss, label='train')\n",
    "            ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test', linewidth='3')\n",
    "            ax1.set_xlabel('iteration')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_title('fid: {} Test loss: {:.2f}'.format(fid, test_loss[-1]))\n",
    "            ax1.legend(loc=1)\n",
    "            ax1.set_ylim(0,50)\n",
    "            pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp11 HC_CT\n",
      "Hype # hyp1, Fold # 1\n",
      "('X_L_HC', <caffe._caffe.Blob object at 0x7f33fce8fc08>)\n",
      "('X_R_HC', <caffe._caffe.Blob object at 0x7f33fce8fde8>)\n",
      "X_out.shape: (71, 1),(71, 1)\n",
      "Extracting features from data...\n",
      ". ('X_L_HC', <caffe._caffe.Blob object at 0x7f33fce8fde8>)\n",
      "('X_R_HC', <caffe._caffe.Blob object at 0x7f33fce8fc08>)\n",
      "X_out.shape: (71, 1),(71, 1)\n",
      "Extracting features from data...\n",
      ". ('X_L_HC', <caffe._caffe.Blob object at 0x7f33fce8fc08>)\n",
      "('X_R_HC', <caffe._caffe.Blob object at 0x7f33fce8fde8>)\n",
      "X_out.shape: (71, 1),(71, 1)\n",
      "Extracting features from data...\n",
      ". ('X_L_HC', <caffe._caffe.Blob object at 0x7f33fce8fde8>)\n",
      "('X_R_HC', <caffe._caffe.Blob object at 0x7f33fce8fc08>)\n",
      "X_out.shape: (71, 1),(71, 1)\n",
      "Extracting features from data...\n",
      ". ('X_L_HC', <caffe._caffe.Blob object at 0x7f33fce8fc08>)\n",
      "('X_R_HC', <caffe._caffe.Blob object at 0x7f33fce8fde8>)\n",
      "X_out.shape: (71, 1),(71, 1)\n",
      "Extracting features from data...\n",
      ". ('X_L_HC', <caffe._caffe.Blob object at 0x7f33fce8fde8>)\n",
      "('X_R_HC', <caffe._caffe.Blob object at 0x7f33fce8fc08>)\n",
      "X_out.shape: (71, 1),(71, 1)\n",
      "Extracting features from data...\n",
      ". ('X_L_HC', <caffe._caffe.Blob object at 0x7f33fce8fc08>)\n",
      "('X_R_HC', <caffe._caffe.Blob object at 0x7f33fce8fde8>)\n",
      "X_out.shape: (71, 1),(71, 1)\n",
      "Extracting features from data...\n",
      ". ('X_L_HC', <caffe._caffe.Blob object at 0x7f33fce8fde8>)\n",
      "('X_R_HC', <caffe._caffe.Blob object at 0x7f33fce8fc08>)\n",
      "X_out.shape: (71, 1),(71, 1)\n",
      "Extracting features from data...\n",
      ". ('X_L_HC', <caffe._caffe.Blob object at 0x7f33fce8fc08>)\n",
      "('X_R_HC', <caffe._caffe.Blob object at 0x7f33fce8fde8>)\n",
      "X_out.shape: (71, 1),(71, 1)\n",
      "Extracting features from data...\n",
      ". ('X_L_HC', <caffe._caffe.Blob object at 0x7f33fce8fde8>)\n",
      "('X_R_HC', <caffe._caffe.Blob object at 0x7f33fce8fc08>)\n",
      "X_out.shape: (71, 1),(71, 1)\n",
      "Extracting features from data...\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "exp_name = 'Exp11'\n",
    "niter = 50000\n",
    "modality = 'HC_CT'\n",
    "start_fold = 1\n",
    "n_folds = 1\n",
    "preproc = 'no_preproc'\n",
    "batch_size = 256\n",
    "snap_interval = 5000\n",
    "snap_start = 5000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "Clinical_Scale = 'BOTH'\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "print exp_name, modality\n",
    "\n",
    "for hype in hype_configs.keys():      \n",
    "    node_sizes = hype_configs[hype]['node_sizes']\n",
    "    dr = hype_configs[hype]['dr']\n",
    "    lr = hype_configs[hype]['lr']\n",
    "    \n",
    "    for fid in np.arange(start_fold,n_folds+1,1):\n",
    "        test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "        #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "        #with open(test_filename_txt, 'w') as f:\n",
    "        #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "        test_net_path = baseline_dir + 'API/data/fold{}/ADNI_FF_test.prototxt'.format(fid)\n",
    "        with open(test_net_path, 'w') as f:\n",
    "            if modality == 'HC':\n",
    "                f.write(str(adninet_ff_HC(test_filename_txt, 256, node_sizes,dr,lr,Clinical_Scale)))\n",
    "                input_nodes = ['X_L_HC','X_R_HC']\n",
    "            elif modality == 'CT':\n",
    "                f.write(str(adninet_ff_CT(test_filename_txt, 256, node_sizes,dr,lr,Clinical_Scale)))\n",
    "                input_nodes = ['X_CT']\n",
    "            elif modality == 'HC_CT':\n",
    "                f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,Clinical_Scale)))\n",
    "                input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "            else:\n",
    "                print 'Wrong modality'\n",
    "    \n",
    "        print 'Hype # {}, Fold # {}'.format(hype, fid)\n",
    "        data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "        if Clinical_Scale == 'ADAS13':\n",
    "            act_scores = load_data(data_path, 'y','no_preproc')\n",
    "        elif Clinical_Scale == 'MMSE': \n",
    "            act_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        elif Clinical_Scale == 'BOTH':\n",
    "            act_scores_adas13 = load_data(data_path, 'y','no_preproc')\n",
    "            act_scores_mmse = load_data(data_path, 'y3','no_preproc')\n",
    "        else:\n",
    "            print 'unknown clinical scale'\n",
    "        \n",
    "        net_file = baseline_dir + 'API/data/fold{}/ADNI_FF_test.prototxt'.format(fid)\n",
    "        test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "        test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "        with open(test_filename_txt, 'w') as f:\n",
    "                f.write(test_filename_hdf + '\\n')  \n",
    "        \n",
    "        sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "        #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "        #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "        \n",
    "        if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "            multi_task = False\n",
    "            iter_euLoss = []\n",
    "            iter_r = []        \n",
    "            iter_pred_scores = []\n",
    "            for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "                results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers)\n",
    "                encodings = np.squeeze(results['X_out'])            \n",
    "                iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "            config_idx = '{}_{}'.format(hype,fid)\n",
    "            fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "            fold_r[config_idx] = np.array(iter_r)\n",
    "            fold_act_scores[fid] = act_scores\n",
    "            fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "        \n",
    "        elif Clinical_Scale == 'BOTH':\n",
    "            multi_task = True\n",
    "            iter_euLoss_adas13 = []\n",
    "            iter_r_adas13 = []        \n",
    "            iter_pred_scores_adas13 = []\n",
    "            iter_euLoss_mmse = []\n",
    "            iter_r_mmse = []        \n",
    "            iter_pred_scores_mmse = []\n",
    "            \n",
    "            for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "                results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                encodings = results['X_out']   \n",
    "                encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "            config_idx = '{}_{}'.format(hype,fid)\n",
    "            fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "            fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "            fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "            fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "            fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "            fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI0AAAFHCAYAAAAsg7xBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f33ef7c1850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 5)\n",
    "n_rows = 1\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "corrs = [fold_r_adas13,fold_r_mmse]\n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        plt.figure(fid)\n",
    "        plt.subplot(n_rows,n_cols,1)\n",
    "        plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "        plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "        plt.legend()\n",
    "        plt.subplot(n_rows,n_cols,2)\n",
    "        plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "        plt.title('correlation, fid: {}'.format(fid))\n",
    "        plt.legend(loc=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid:1, best hype:3, snap: 95000, euLoss:2.74450251145\n",
      "fid:1, best hype:3, snap: 100000, r:0.533498301\n",
      "fid:2, best hype:2, snap: 100000, euLoss:2.62579928662\n",
      "fid:2, best hype:3, snap: 90000, r:0.566744788766\n",
      "fid:3, best hype:1, snap: 100000, euLoss:3.10801707296\n",
      "fid:3, best hype:1, snap: 100000, r:0.557948689595\n",
      "fid:4, best hype:3, snap: 80000, euLoss:2.09578689676\n",
      "fid:4, best hype:3, snap: 65000, r:0.586182234853\n",
      "fid:5, best hype:3, snap: 100000, euLoss:3.01594207838\n",
      "fid:5, best hype:3, snap: 100000, r:0.47279473223\n",
      "fid:6, best hype:3, snap: 70000, euLoss:2.19099077647\n",
      "fid:6, best hype:3, snap: 85000, r:0.539159569911\n",
      "fid:7, best hype:4, snap: 75000, euLoss:2.03020024736\n",
      "fid:7, best hype:4, snap: 70000, r:0.64227141139\n",
      "fid:8, best hype:4, snap: 70000, euLoss:2.52263564647\n",
      "fid:8, best hype:4, snap: 100000, r:0.4777182734\n",
      "fid:9, best hype:4, snap: 100000, euLoss:2.63600623212\n",
      "fid:9, best hype:4, snap: 100000, r:0.501349739541\n",
      "fid:10, best hype:4, snap: 80000, euLoss:3.14356719446\n",
      "fid:10, best hype:4, snap: 100000, r:0.392442306576\n",
      "CV Perf: r:0.527011004726, mse:5.22268958861\n",
      "[0.53349830100027984, 0.56674478876630852, 0.55794868959549382, 0.58618223485250021, 0.47279473222962209, 0.53915956991057779, 0.64227141138992661, 0.47771827339987488, 0.50134973954107631, 0.39244230657622881]\n"
     ]
    }
   ],
   "source": [
    "# Find optimal config based on inner_test\n",
    "snap_array = np.arange(snap_start,niter+1,snap_interval)\n",
    "                    \n",
    "fid_hype_map = defaultdict(list)\n",
    "fid_euLoss_perf= defaultdict(list)\n",
    "fid_r_perf= defaultdict(list)\n",
    "for hype_fid in fold_euLoss.keys():\n",
    "    hype = int(hype_fid.split('_')[0][3])\n",
    "    fid = int(hype_fid.split('_')[1])\n",
    "    fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "    fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "    fid_hype_map[fid].append(hype)\n",
    "\n",
    "opt_r = []\n",
    "opt_mse = []\n",
    "opt_hype = []\n",
    "actual_scores = []\n",
    "opt_pred_scores = []\n",
    "\n",
    "for fid in fid_hype_map.keys():\n",
    "    r_perf_array = np.array(fid_r_perf[fid])\n",
    "    euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "    \n",
    "    h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "    eu_loss = euLoss_perf_array[h,snp]\n",
    "    opt_mse.append(2*eu_loss)\n",
    "    print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)\n",
    "    h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "    r = r_perf_array[h,snp]\n",
    "    opt_r.append(r)\n",
    "    opt_hype.append(hype_configs['hyp{}'.format(fid_hype_map[fid][h])])\n",
    "    print 'fid:{}, best hype:{}, snap: {}, r:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],r)\n",
    "    actual_scores.append(fold_act_scores[fid])  \n",
    "    opt_pred_scores.append(fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r), np.mean(opt_mse))\n",
    "print opt_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-10-27-02.pkl\n",
      "old CV_r: [0.58713291550558833, 0.74574527911517141, 0.71271529005993939, 0.71772651594819259, 0.68488215732574409, 0.64485628063795464, 0.59854968077229798, 0.6848414923548054, 0.60031497549309831, 0.57364714888913393]\n",
      "\n",
      "old mean(CV_r): 0.65504117361, mean(MSE): 55.6504641076\n",
      "\n",
      "old CV_mse: [59.060642573000067, 49.994822378856952, 42.505167081404124, 41.230911409916544, 62.596431523635779, 45.418731041811768, 75.699131356850913, 45.237745781162964, 64.553970362370492, 70.207087566978927]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'opt_mse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5113630b0573>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mNN_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mNN_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CV_MSE'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt_mse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mNN_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CV_r'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt_r\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mNN_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'predicted_CV_scores'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt_pred_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'opt_mse' is not defined"
     ]
    }
   ],
   "source": [
    "# Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-10-27-02.pkl\n",
    "# old CV_r: [0.58713291550558833, 0.74574527911517141, 0.71271529005993939, 0.71772651594819259, 0.68488215732574409, \n",
    "#            0.64485628063795464, 0.59854968077229798, 0.6848414923548054, 0.60031497549309831, 0.57364714888913393]\n",
    "# Exp11_ADNI2_MMSE_NN_HC_CT_2016-05-15-19-59-42.pkl\n",
    "# old CV_r: [0.48038349554607179, 0.60131716074726682, 0.53198020733649787, 0.64770981175015141, 0.56547528703482142,\n",
    "#            0.52085739989088009, 0.65811053610310188, 0.57630405337929402, 0.51059593904907996, 0.48178577037836778]\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "# model_files = ['Exp7_ADNI2_ADAS13_NN_HC_CT_2016-05-01-13-46-16.pkl',\n",
    "#                'Exp7_ADNI2_ADAS13_NN_HC_CT_2016-05-02-10-29-55.pkl', 'Exp7_ADNI2_ADAS13_NN_HC_CT_2016-05-03-19-02-13.pkl']\n",
    "\n",
    "model_file = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-10-27-02.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {0:0,2:2}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp13_ADNI1and2_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        for idx in idx_pairs.keys():\n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "        \n",
    "    return CV_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HC (ADNI1)\n",
    "fid:1, best hype:1, snap: 10000, euLoss:24.9804792275\n",
    "fid:1, best hype:2, snap: 8000, r:0.618163002854\n",
    "fid:2, best hype:1, snap: 12000, euLoss:25.967983131\n",
    "fid:2, best hype:2, snap: 10000, r:0.590332098517\n",
    "fid:3, best hype:1, snap: 14000, euLoss:34.8323371747\n",
    "fid:3, best hype:1, snap: 14000, r:0.513585520325\n",
    "fid:4, best hype:2, snap: 18000, euLoss:30.0616446921\n",
    "fid:4, best hype:2, snap: 10000, r:0.562053923318\n",
    "fid:5, best hype:1, snap: 8000, euLoss:45.2696952675\n",
    "fid:5, best hype:2, snap: 6000, r:0.484386218568\n",
    "fid:6, best hype:1, snap: 8000, euLoss:36.2613456917\n",
    "fid:6, best hype:1, snap: 8000, r:0.448447097693\n",
    "fid:7, best hype:1, snap: 6000, euLoss:25.3728563975\n",
    "fid:7, best hype:2, snap: 4000, r:0.569872808577\n",
    "fid:8, best hype:2, snap: 10000, euLoss:24.2016694373\n",
    "fid:8, best hype:2, snap: 8000, r:0.529221803897\n",
    "fid:9, best hype:1, snap: 8000, euLoss:27.0590434962\n",
    "fid:9, best hype:2, snap: 14000, r:0.476044796782\n",
    "fid:10, best hype:1, snap: 12000, euLoss:33.7379212677\n",
    "fid:10, best hype:1, snap: 20000, r:0.511206989432\n",
    "CV Perf: r:0.530331425996, mse:61.5489951566\n",
    "            \n",
    "CT (ADNI1)\n",
    "fid:1, best hype:4, snap: 26000, euLoss:25.8350339172\n",
    "fid:1, best hype:4, snap: 28000, r:0.580744990132\n",
    "fid:2, best hype:1, snap: 20000, euLoss:27.4633322097\n",
    "fid:2, best hype:1, snap: 30000, r:0.539312798756\n",
    "fid:3, best hype:1, snap: 24000, euLoss:32.985009536\n",
    "fid:3, best hype:4, snap: 18000, r:0.550124971628\n",
    "fid:4, best hype:4, snap: 8000, euLoss:39.0047762008\n",
    "fid:4, best hype:3, snap: 40000, r:0.385853287133\n",
    "fid:5, best hype:2, snap: 12000, euLoss:36.2558308892\n",
    "fid:5, best hype:3, snap: 16000, r:0.518366572313\n",
    "fid:6, best hype:4, snap: 26000, euLoss:33.4368286296\n",
    "fid:6, best hype:4, snap: 32000, r:0.519355577479\n",
    "fid:7, best hype:4, snap: 8000, euLoss:26.6360230396\n",
    "fid:7, best hype:4, snap: 10000, r:0.480488994696\n",
    "fid:8, best hype:2, snap: 8000, euLoss:25.9672358442\n",
    "fid:8, best hype:4, snap: 16000, r:0.47992385987\n",
    "fid:9, best hype:3, snap: 8000, euLoss:23.7660096974\n",
    "fid:9, best hype:2, snap: 12000, r:0.515238620828\n",
    "fid:10, best hype:3, snap: 40000, euLoss:34.3815763364\n",
    "fid:10, best hype:2, snap: 38000, r:0.527298506806\n",
    "CV Perf: r:0.509670817964, mse:61.14633126\n",
    "            \n",
    "HC_CT (ADNI1) \n",
    "fid:1, best hype:6, snap: 14000, euLoss:21.8368207263\n",
    "fid:1, best hype:6, snap: 14000, r:0.665541584179\n",
    "fid:2, best hype:8, snap: 20000, euLoss:24.492447482\n",
    "fid:2, best hype:8, snap: 20000, r:0.592235431033\n",
    "fid:3, best hype:8, snap: 20000, euLoss:29.2468359\n",
    "fid:3, best hype:8, snap: 20000, r:0.614328598919\n",
    "fid:4, best hype:6, snap: 12000, euLoss:30.1642826759\n",
    "fid:4, best hype:6, snap: 10000, r:0.536940060297\n",
    "fid:5, best hype:2, snap: 16000, euLoss:33.9339476376\n",
    "fid:5, best hype:2, snap: 16000, r:0.561498850486\n",
    "fid:6, best hype:8, snap: 14000, euLoss:35.6290443771\n",
    "fid:6, best hype:8, snap: 16000, r:0.461981628273\n",
    "fid:7, best hype:6, snap: 12000, euLoss:21.0467020887\n",
    "fid:7, best hype:6, snap: 12000, r:0.624393932281\n",
    "fid:8, best hype:2, snap: 18000, euLoss:21.1833302525\n",
    "fid:8, best hype:2, snap: 18000, r:0.596613031213\n",
    "fid:9, best hype:7, snap: 16000, euLoss:23.4504655627\n",
    "fid:9, best hype:8, snap: 20000, r:0.549850038264\n",
    "fid:10, best hype:6, snap: 20000, euLoss:29.4776804418\n",
    "fid:10, best hype:6, snap: 20000, r:0.579462867606\n",
    "CV Perf: r:0.578284602255, mse:54.0923114289\n",
    "\n",
    "fid:1, best hype:4, snap: 12000, euLoss:19.385883703\n",
    "fid:1, best hype:4, snap: 12000, r:0.717012338726\n",
    "fid:2, best hype:4, snap: 12000, euLoss:23.2118685134\n",
    "fid:2, best hype:4, snap: 12000, r:0.618635434743\n",
    "fid:3, best hype:2, snap: 16000, euLoss:28.9700777636\n",
    "fid:3, best hype:3, snap: 14000, r:0.614679267678\n",
    "fid:4, best hype:3, snap: 12000, euLoss:27.6156180904\n",
    "fid:4, best hype:3, snap: 12000, r:0.580956772823\n",
    "fid:5, best hype:2, snap: 12000, euLoss:34.3337375218\n",
    "fid:5, best hype:4, snap: 10000, r:0.558080830398\n",
    "fid:6, best hype:1, snap: 14000, euLoss:35.2833445481\n",
    "fid:6, best hype:1, snap: 14000, r:0.493586554303\n",
    "fid:7, best hype:3, snap: 8000, euLoss:21.561520043\n",
    "fid:7, best hype:2, snap: 10000, r:0.604751678545\n",
    "fid:8, best hype:4, snap: 8000, euLoss:22.6269883545\n",
    "fid:8, best hype:2, snap: 16000, r:0.578988034023\n",
    "fid:9, best hype:1, snap: 12000, euLoss:22.5312161778\n",
    "fid:9, best hype:1, snap: 16000, r:0.568848116216\n",
    "fid:10, best hype:4, snap: 14000, euLoss:29.1751097441\n",
    "fid:10, best hype:4, snap: 14000, r:0.589610760215\n",
    "CV Perf: r:0.592514978767, mse:52.9390728919\n",
    "\n",
    "            \n",
    "HC (ADNI2)\n",
    "fid:1, best hype:3, snap: 18000, euLoss:31.1006562908\n",
    "fid:1, best hype:1, snap: 30000, r:0.586353688344\n",
    "fid:2, best hype:3, snap: 26000, euLoss:32.9662736235\n",
    "fid:2, best hype:3, snap: 24000, r:0.669333897132\n",
    "fid:3, best hype:2, snap: 36000, euLoss:30.384235121\n",
    "fid:3, best hype:2, snap: 40000, r:0.544646225582\n",
    "fid:4, best hype:2, snap: 32000, euLoss:26.8515617888\n",
    "fid:4, best hype:2, snap: 26000, r:0.606256451398\n",
    "fid:5, best hype:3, snap: 36000, euLoss:45.8194681009\n",
    "fid:5, best hype:1, snap: 18000, r:0.517164527931\n",
    "fid:6, best hype:4, snap: 40000, euLoss:24.0025679138\n",
    "fid:6, best hype:4, snap: 40000, r:0.62570107699\n",
    "fid:7, best hype:1, snap: 16000, euLoss:50.5119887715\n",
    "fid:7, best hype:3, snap: 24000, r:0.408465338901\n",
    "fid:8, best hype:4, snap: 30000, euLoss:32.3904189083\n",
    "fid:8, best hype:4, snap: 40000, r:0.537722161255\n",
    "fid:9, best hype:2, snap: 38000, euLoss:39.3835394087\n",
    "fid:9, best hype:2, snap: 40000, r:0.49499395939\n",
    "fid:10, best hype:2, snap: 12000, euLoss:37.8077445797\n",
    "fid:10, best hype:2, snap: 10000, r:0.57777398392\n",
    "CV Perf: r:0.556841131084, mse:70.2436909014\n",
    "            \n",
    "CT (ADNI2)\n",
    "fid:1, best hype:4, snap: 40000, euLoss:31.564576893\n",
    "fid:1, best hype:4, snap: 40000, r:0.535450818923\n",
    "fid:2, best hype:4, snap: 32000, euLoss:26.624324672\n",
    "fid:2, best hype:4, snap: 34000, r:0.733523463006\n",
    "fid:3, best hype:4, snap: 32000, euLoss:21.5213626512\n",
    "fid:3, best hype:4, snap: 20000, r:0.711414187155\n",
    "fid:4, best hype:2, snap: 30000, euLoss:26.0918774873\n",
    "fid:4, best hype:2, snap: 22000, r:0.628084544407\n",
    "fid:5, best hype:4, snap: 40000, euLoss:37.9048385987\n",
    "fid:5, best hype:4, snap: 40000, r:0.591108478187\n",
    "fid:6, best hype:4, snap: 26000, euLoss:27.6405805728\n",
    "fid:6, best hype:4, snap: 40000, r:0.567989262491\n",
    "fid:7, best hype:3, snap: 22000, euLoss:37.3339119496\n",
    "fid:7, best hype:1, snap: 40000, r:0.606678494326\n",
    "fid:8, best hype:2, snap: 38000, euLoss:24.0600832735\n",
    "fid:8, best hype:2, snap: 38000, r:0.651449306233\n",
    "fid:9, best hype:4, snap: 38000, euLoss:29.1995566658\n",
    "fid:9, best hype:4, snap: 38000, r:0.642322450054\n",
    "fid:10, best hype:3, snap: 22000, euLoss:34.1581885294\n",
    "fid:10, best hype:4, snap: 32000, r:0.598128556457\n",
    "CV Perf: r:0.626614956124, mse:59.2198602587\n",
    "\n",
    "HC (ADNI1+ADNI2)\n",
    "fid:1, best hype:4, snap: 16000, euLoss:25.5427257732\n",
    "fid:1, best hype:1, snap: 20000, r:0.635262286181\n",
    "fid:2, best hype:4, snap: 36000, euLoss:28.007589395\n",
    "fid:2, best hype:2, snap: 32000, r:0.640702711425\n",
    "fid:3, best hype:4, snap: 36000, euLoss:31.3078344445\n",
    "fid:3, best hype:2, snap: 28000, r:0.562932234264\n",
    "fid:4, best hype:4, snap: 34000, euLoss:28.8538741144\n",
    "fid:4, best hype:4, snap: 22000, r:0.581952986768\n",
    "fid:5, best hype:4, snap: 28000, euLoss:40.9219323271\n",
    "fid:5, best hype:4, snap: 28000, r:0.513852217676\n",
    "fid:6, best hype:4, snap: 28000, euLoss:31.9896590757\n",
    "fid:6, best hype:2, snap: 40000, r:0.514971207915\n",
    "fid:7, best hype:4, snap: 18000, euLoss:38.4145699362\n",
    "fid:7, best hype:2, snap: 14000, r:0.46256342695\n",
    "fid:8, best hype:2, snap: 32000, euLoss:28.4115783735\n",
    "fid:8, best hype:1, snap: 40000, r:0.518761400372\n",
    "fid:9, best hype:4, snap: 28000, euLoss:33.7696027174\n",
    "fid:9, best hype:2, snap: 36000, r:0.482283921963\n",
    "fid:10, best hype:4, snap: 16000, euLoss:37.4498623203\n",
    "fid:10, best hype:2, snap: 14000, r:0.515666128844\n",
    "CV Perf: r:0.542894852236, mse:64.9338456955\n",
    "            \n",
    "CT (ADNI1+ADNI2)\n",
    "fid:1, best hype:1, snap: 32000, euLoss:29.830174769\n",
    "fid:1, best hype:1, snap: 40000, r:0.532392927922\n",
    "fid:2, best hype:2, snap: 40000, euLoss:28.0781115603\n",
    "fid:2, best hype:2, snap: 40000, r:0.636745029524\n",
    "fid:3, best hype:1, snap: 40000, euLoss:26.6943931513\n",
    "fid:3, best hype:2, snap: 26000, r:0.655258153866\n",
    "fid:4, best hype:4, snap: 26000, euLoss:33.4955111359\n",
    "fid:4, best hype:4, snap: 30000, r:0.492795606363\n",
    "fid:5, best hype:2, snap: 38000, euLoss:37.4164549165\n",
    "fid:5, best hype:2, snap: 34000, r:0.558587798746\n",
    "fid:6, best hype:2, snap: 34000, euLoss:29.5190398753\n",
    "fid:6, best hype:2, snap: 40000, r:0.561838293275\n",
    "fid:7, best hype:2, snap: 10000, euLoss:34.8524304954\n",
    "fid:7, best hype:1, snap: 12000, r:0.488013116528\n",
    "fid:8, best hype:4, snap: 32000, euLoss:24.5997470284\n",
    "fid:8, best hype:4, snap: 40000, r:0.594486998119\n",
    "fid:9, best hype:2, snap: 38000, euLoss:28.5725313744\n",
    "fid:9, best hype:2, snap: 40000, r:0.570555072946\n",
    "fid:10, best hype:2, snap: 26000, euLoss:35.9999022312\n",
    "fid:10, best hype:2, snap: 36000, r:0.52921470762\n",
    "CV Perf: r:0.561988770491, mse:61.8116593076\n",
    "            \n",
    "HC_CT (ADNI1+ADNI2)\n",
    "fid:1, best hype:4, snap: 18000, euLoss:26.5206985927\n",
    "fid:1, best hype:4, snap: 18000, r:0.599511137562\n",
    "fid:2, best hype:2, snap: 28000, euLoss:30.5457669599\n",
    "fid:2, best hype:2, snap: 32000, r:0.592150226726\n",
    "fid:3, best hype:1, snap: 38000, euLoss:30.3515014599\n",
    "fid:3, best hype:1, snap: 40000, r:0.579741031528\n",
    "fid:4, best hype:3, snap: 32000, euLoss:30.0210360115\n",
    "fid:4, best hype:3, snap: 34000, r:0.552430441356\n",
    "fid:5, best hype:3, snap: 28000, euLoss:39.182075602\n",
    "fid:5, best hype:3, snap: 28000, r:0.523933996482\n",
    "fid:6, best hype:4, snap: 40000, euLoss:33.234443951\n",
    "fid:6, best hype:4, snap: 40000, r:0.488648456284\n",
    "fid:7, best hype:2, snap: 18000, euLoss:34.6361386606\n",
    "fid:7, best hype:2, snap: 24000, r:0.491865758471\n",
    "fid:8, best hype:2, snap: 24000, euLoss:26.6209951932\n",
    "fid:8, best hype:2, snap: 26000, r:0.548111760686\n",
    "fid:9, best hype:3, snap: 28000, euLoss:32.9634885617\n",
    "fid:9, best hype:4, snap: 40000, r:0.465846376987\n",
    "fid:10, best hype:2, snap: 16000, euLoss:35.1673696246\n",
    "fid:10, best hype:2, snap: 16000, r:0.524438537544\n",
    "CV Perf: r:0.536667772362, mse:63.8487029234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51050829586604762, 0.55489790509990333, 0.51051434233193749, 0.57461758968662113, 0.46522819835558527, 0.52223579546293253, 0.63373413755162267, 0.45191005777711357, 0.48101748155492663, 0.36983149498103951]\n",
      "[5.8069002314135911, 5.4253284440655989, 6.6457785843506993, 4.2981556260252862, 6.1525092301934867, 4.4842359537216696, 4.1313801974327413, 5.240332442527837, 5.4384563253360989, 6.2830388918832485]\n",
      "saving results at: /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp13_ADNI1and2_MMSE_NN_HC_CT_2016-05-13-00-48-41.pkl\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "print opt_r\n",
    "print opt_mse\n",
    "\n",
    "save_detailed_results = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results\n",
    "    NN_results = {}\n",
    "\n",
    "    NN_results['CV_MSE'] = opt_mse\n",
    "    NN_results['CV_r'] = opt_r\n",
    "    NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "    NN_results['actual_CV_scores'] = actual_scores\n",
    "    NN_results['tid_snap_config_dict'] = opt_hype\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp13_ADNI1and2_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "model_file = 'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-04-25-13-26-25.pkl'\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "opt_mse = CV_data['CV_MSE']\n",
    "opt_r = CV_data['CV_r']\n",
    "opt_hype = CV_data['tid_snap_config_dict']\n",
    "#actual_scores = []\n",
    "#opt_pred_scores = []\n",
    "print np.mean(np.array(opt_mse))\n",
    "print np.mean(np.array(opt_r))\n",
    "print opt_r[4], opt_hype[4]\n",
    "\n",
    "a = [0.71701233872649583, 0.61863543474266547, 0.61467926767752157, 0.58095677282250102, 0.55808083039804779, \n",
    "     0.49358655430330067, 0.60475167854479739, 0.57898803402319077, 0.56884811621626763, 0.58961076021473602]\n",
    "\n",
    "b = [0.71671762632760383, 0.61621460856727428, 0.61134923509603034, 0.58863609091191105, 0.55675880434099656, \n",
    "     0.47100346323317155, 0.58819413911512908, 0.56460945813269181, 0.57653317702204721, 0.59351872839470088]\n",
    "\n",
    "perf_concat = np.vstack((np.array(opt_r),np.array(a),np.array(b)))\n",
    "print np.max(perf_concat,axis=0)\n",
    "print np.mean(np.max(perf_concat,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some defs to load data and extract encodings from trained net\n",
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    print net.blobs.items()[0]\n",
    "    print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        data_layers.append(net.blobs.items()[i][1])    \n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "    \n",
    "    net.reshape()            \n",
    "    print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}\n",
    "\n",
    "def generate_APIdata(in_data_path,out_data_path,fid,modality,preproc, CS_only):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    in_data = load_data(in_data_path, 'Fold_{}_X_{}'.format(fid,modality), preproc)\n",
    "    # get labels (no_preproc)        \n",
    "\n",
    "    # HDF5 is pretty efficient, but can be further compressed.\n",
    "    comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "    with h5py.File(out_data_path, 'a') as f:\n",
    "        if not CS_only:\n",
    "            if modality == 'R_CT': #Fix the typo\n",
    "                #Because we have separate ADNI1+2 cohort for ADAS13 no need to create 'y'\n",
    "                in_label = load_data(in_data_path, 'Fold_{}_y3'.format(fid), 'no_preproc') \n",
    "                modality = 'CT'\n",
    "                f.create_dataset('y3', data=in_label, **comp_kwargs)\n",
    "\n",
    "            f.create_dataset('X_{}'.format(modality), data=in_data, **comp_kwargs)\n",
    "        else:\n",
    "            if modality == 'R_CT': #Fix the typo\n",
    "                in_label = load_data(in_data_path, 'Fold_{}_y3'.format(fid), 'no_preproc')\n",
    "                f.create_dataset('y3', data=in_label, **comp_kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp13'\n",
    "exp_name_out = 'Exp13'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "preproc = 'no_preproc'\n",
    "modalities = ['L_HC','R_HC','R_CT']\n",
    "dataset = 'ADNI1and2'\n",
    "Clinical_Scale = 'ADAS13_MMSE'\n",
    "CS_only = False\n",
    "\n",
    "for fid in np.arange(1,11,1):\n",
    "    for modality in modalities:\n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_{}_NN_OuterFold_{}_train_InnerFold_1.h5'.format(exp_name,dataset,Clinical_Scale,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_{}_NN_OuterFold_{}_valid_InnerFold_1.h5'.format(exp_name,dataset,Clinical_Scale,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_{}_NN_valid.h5'.format(exp_name,dataset,Clinical_Scale)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name_out,preproc)\n",
    "\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modality,preproc, CS_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "modality = 'HC_CT'\n",
    "n_folds = 8\n",
    "for fid in np.arange(1,n_folds+1,1):\n",
    "    print 'fid: {}'.format(fid)\n",
    "    for AE_branch in ['CT','R_HC','L_HC']:\n",
    "        print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "        if AE_branch == 'L_HC':\n",
    "            params_FF = ['L_ff1', 'L_ff2']\n",
    "            AE_iter = 10000\n",
    "        elif AE_branch == 'R_HC':\n",
    "            params_FF = ['R_ff1', 'R_ff2']\n",
    "            AE_iter = 10000\n",
    "        elif AE_branch == 'CT':\n",
    "            params_FF = ['ff1', 'ff2']\n",
    "            AE_iter = 50000\n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            #Only use this during 1 of the modalities to avoid overwritting\n",
    "            print 'Spawning new net'\n",
    "            pretrain_net = caffe.Net(baseline_dir + 'API/data/fold{}/ADNI_FF_pretrain.prototxt'.format(1), caffe.TRAIN)\n",
    "        else:\n",
    "            print 'Wrong AE branch'\n",
    "\n",
    "        # conv_params = {name: (weights, biases)}\n",
    "        conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "        for conv in params_FF:\n",
    "            print '{} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "        # Review AE net params \n",
    "        #fid for pretain is 1 because it's same definition for all the folds.\n",
    "        net_file = baseline_dir + 'API/data/fold{}/ADNI_AE_{}_test.prototxt'.format(1,AE_branch)\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "\n",
    "        AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "        params_AE = ['encoder1', 'code']\n",
    "        # fc_params = {name: (weights, biases)}\n",
    "        fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "        for fc in params_AE:\n",
    "            print '{} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "        #transplant net parameters\n",
    "        for pr, pr_conv in zip(params_AE, params_FF):\n",
    "            conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "            conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "        save_net = True\n",
    "        if save_net:\n",
    "            save_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_{}_iter_{}_concat50.caffemodel'.format(fid,modality,AE_iter)\n",
    "            print \"Saving net to \" + save_path\n",
    "            pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "16086,16471"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
