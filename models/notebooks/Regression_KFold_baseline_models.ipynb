{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "#from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import KFold\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasets\n",
    "\n",
    "#input data\n",
    "train_val_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_train_plus_val.pkl'\n",
    "test_file = '/projects/francisco/data/ADNI/cli_ct_seg_fused_test.pkl'\n",
    "\n",
    "#k-fold indices (from a saved file)\n",
    "kf_file = \"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_train_valid_KFold_idx.pkl\"\n",
    "\n",
    "#save dir for trained model \n",
    "CV_model_dir = '/projects/nikhil/ADNI_prediction/models/CV_pkls/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grab CV data with specific feature columes (independent vars) and specific clinical scale (dependent var)\n",
    "def load_CV_data(in_file, kf_file, feature_cols, clinical_scale):\n",
    "\n",
    "    data = pd.read_pickle(in_file)\n",
    "    data_trunc = data[clinical_scale + feature_cols]\n",
    "    # remove nans\n",
    "    data_trunc = data_trunc[np.isfinite(data_trunc[clinical_scale[0]])]\n",
    "    X = np.asarray(data_trunc[feature_cols],dtype=float)\n",
    "    y = np.asarray(data_trunc[clinical_scale[0]],dtype=float)\n",
    "    \n",
    "    kf = pickle.load( open(kf_file, \"rb\" ) )\n",
    "    X_train = []\n",
    "    X_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    for train, valid in kf:        \n",
    "        X_train.append(X[train])\n",
    "        X_valid.append(X[valid])\n",
    "        y_train.append(y[train])\n",
    "        y_valid.append(y[valid])\n",
    "    \n",
    "    print valid\n",
    "    # Return train and validation lists comprising all folds as well as unsplit data\n",
    "    return {'X_train':X_train,'X_valid':X_valid,'y_train':y_train,'y_valid':y_valid,'X':X,'y':y}\n",
    "\n",
    "#Load test data\n",
    "def load_test_data(in_file, feature_cols, clinical_scale):\n",
    "\n",
    "    data = pd.read_pickle(in_file)\n",
    "    data_trunc = data[clinical_scale + feature_cols]\n",
    "    # remove nans\n",
    "    data_trunc = data_trunc[np.isfinite(data_trunc[clinical_scale[0]])]\n",
    "    X = np.asarray(data_trunc[feature_cols],dtype=float)\n",
    "    y = np.asarray(data_trunc[clinical_scale[0]],dtype=float)\n",
    "    return {'X':X, 'y':y}\n",
    "\n",
    "def innerCVLoop(model_clf,hyper_params,fold_X, fold_y,save_model,save_model_path):\n",
    "    clf = grid_search.GridSearchCV(model_clf, hyper_params,cv=3,verbose=0)\n",
    "    clf.fit(fold_X, fold_y)\n",
    "    #Save classifier\n",
    "    if save_model:\n",
    "        save_model(clf,save_model_path)\n",
    "        \n",
    "    return clf\n",
    "\n",
    "def save_classifier(clf,save_model_path):\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    save_model_filename = save_model_path + '_' + st + '.pkl'\n",
    "        \n",
    "    f = open(save_model_filename, 'wb')\n",
    "    pickle.dump(clf, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8   9  11  14  26  27  48  52  53  55  73  75  83  88  91  93 100 101\n",
      " 105 108 109 122 148 149 162 167 169 173 176 177 189 190 200 211 226 241\n",
      " 349 353 366 373 392 400 420 433 442 451 492 496 499 506 541 543 556 558\n",
      " 566 575 578 579]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22029, 74, 21946, 21946)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab specific columns as features from the original table\n",
    "data = pd.read_pickle(train_val_file)\n",
    "data_cols = data.columns\n",
    "regex=re.compile(\".*(CT_).*\")\n",
    "CT_cols = [m.group(0) for l in data_cols for m in [regex.search(l)] if m] \n",
    "\n",
    "regex=re.compile(\".*(_HC_)(\\d+)\")\n",
    "HC_cols = [m.group(0) for l in data_cols for m in [regex.search(l)] if m] \n",
    "\n",
    "#feature_cols = ['L_HC_VOL','R_HC_VOL'] + CT_cols\n",
    "feature_cols = HC_cols + CT_cols\n",
    "clinical_scale = ['ADAS13']\n",
    "\n",
    "cv_data = load_CV_data(train_val_file,kf_file, feature_cols, clinical_scale)\n",
    "test_data = load_test_data(test_file, feature_cols, clinical_scale)\n",
    "\n",
    "len(data_cols), len(CT_cols), len(HC_cols), 11427+10519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this data for computing subject wise performance during outerloop cross-validation + held-out testset\n",
    "fused_data_dir = '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "CV_fused_file = 'HC_CT_inflated_CV_OuterFolds_valid_partition_fused.h5'\n",
    "test_fused_file = 'HC_CT_inflated_CV_OuterFolds_test_partition_fused.h5'\n",
    "\n",
    "CV = True\n",
    "L_HC_offset=11427\n",
    "R_HC_offset=10519\n",
    "\n",
    "if CV:\n",
    "    cv_data_valid_X = cv_data['X_valid']\n",
    "    cv_data_valid_y = cv_data['y_valid']\n",
    "\n",
    "    CV_fused_data = h5.File(fused_data_dir + CV_fused_file,'a')\n",
    "    for f in np.arange(10):\n",
    "        all_data = cv_data_valid_X[f]\n",
    "        CV_fused_data.create_dataset('Fold_{}_X_L_HC'.format(f+1),data=all_data[:,:L_HC_offset])\n",
    "        CV_fused_data.create_dataset('Fold_{}_X_R_HC'.format(f+1),data=all_data[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "        CV_fused_data.create_dataset('Fold_{}_X_CT'.format(f+1),data=all_data[:,L_HC_offset+R_HC_offset:])\n",
    "        CV_fused_data.create_dataset('Fold_{}_y'.format(f+1),data=cv_data_valid_y[f])\n",
    "    \n",
    "    CV_fused_data.close()\n",
    "\n",
    "else:\n",
    "    test_fused_data = h5.File(fused_data_dir + test_fused_file,'a')\n",
    "    all_data = test_data['X']\n",
    "    test_fused_data.create_dataset('test_X_L_HC'.format(f+1),data=all_data[:,:L_HC_offset])\n",
    "    test_fused_data.create_dataset('test_X_R_HC'.format(f+1),data=all_data[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "    test_fused_data.create_dataset('test_X_CT'.format(f+1),data=all_data[:,L_HC_offset+R_HC_offset:])    \n",
    "    test_fused_data.create_dataset('test_y',data=test_data['y'])\n",
    "    test_fused_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# K-fold validations (nested)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import grid_search\n",
    "import datetime\n",
    "import time\n",
    "import collections\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "model_list = ['LR', 'LR_L1', 'SVR', 'RFR']\n",
    "model_choice_id = 3\n",
    "model_choice = model_list[model_choice_id]\n",
    "\n",
    "if model_choice == 'LR':\n",
    "    model_clf = LinearRegression()   \n",
    "    inner_loop = False #only needed to optimize hyper-params\n",
    "    feat_imp = False\n",
    "    saved_model_name = 'LR_2015-10-13-16-38-28.pkl'\n",
    "    \n",
    "elif model_choice == 'LR_L1':\n",
    "    model_clf = Lasso()\n",
    "    hyper_params = {'alpha':[0.2, 0.1, 0.05, 0.01]} \n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = False\n",
    "    if clinical_scale[0] == 'ADAS13':\n",
    "        saved_model_name = 'LR_L1_2015-10-13-16-47-35.pkl'\n",
    "    else:\n",
    "        saved_model_name = 'LR_L1_MMSE_2015-10-16-16-27-29.pkl'\n",
    "    \n",
    "elif model_choice == 'SVR':\n",
    "    model_clf = SVR()\n",
    "    hyper_params = {'kernel':('linear', 'rbf'), 'C':[1,2.5,5,7.5,10]}\n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = False\n",
    "    if clinical_scale[0] == 'ADAS13':\n",
    "        saved_model_name = 'SVR_2015-10-14-13-40-45.pkl'\n",
    "    else:\n",
    "        saved_model_name = 'SVR_MMSE_2015-10-16-18-02-11.pkl'\n",
    "    \n",
    "elif model_choice == 'RFR':\n",
    "    model_clf = RandomForestRegressor()\n",
    "    hyper_params = {'n_estimators':[50,100,200,500],'min_samples_split':[1,2,4,8]}\n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = True\n",
    "    if clinical_scale[0] == 'ADAS13':\n",
    "        saved_model_name = 'RFR_2015-10-13-15-15-11.pkl'\n",
    "    else:\n",
    "        saved_model_name = 'RFR_MMSE_2015-10-16-16-22-42.pkl'\n",
    "else:\n",
    "    print \"Unknown model choice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously saved model: \n",
      "CV R2 (mean, std_err): 0.23, 0.02\n",
      "CV MSE (mean, std_err): 59.45, 3.04\n"
     ]
    }
   ],
   "source": [
    "load_model_path = CV_model_dir + saved_model_name\n",
    "\n",
    "# train a new classifer? (if false then load a single pretrained classifier based on most frequent hyperparams found previously\n",
    "# This will NOT load N different classifiers each for outer-CV fold  \n",
    "train_clf = False\n",
    "\n",
    "cv_X_train = cv_data['X_train']\n",
    "cv_y_train = cv_data['y_train']\n",
    "cv_X_valid = cv_data['X_valid']\n",
    "cv_y_valid = cv_data['y_valid']\n",
    "\n",
    "no_of_folds = len(cv_X_train)\n",
    "\n",
    "CV_R2_train=[] #R2 score for each outer fold on train set\n",
    "CV_R2_valid=[] #R2 score for each outer fold on validation set\n",
    "\n",
    "CV_MSE_train=[] #MSE for each outer fold on train set\n",
    "CV_MSE_valid=[] #MSE for each outer fold on validation set\n",
    "\n",
    "save_model = False #do you want to save all classifiers per each fold? \n",
    "if train_clf:\n",
    "    # outer CV loop\n",
    "    hp_dict = collections.defaultdict(list) #store best hyper-parameters for each fold\n",
    "    for i in np.arange(no_of_folds):\n",
    "        fold_X = cv_X_train[i]\n",
    "        fold_y = cv_y_train[i]\n",
    "        \n",
    "        if inner_loop: \n",
    "            save_model_path_fold = save_model_path + '_fold_' + str(i)\n",
    "            clf = innerCVLoop(model_clf,hyper_params,fold_X, fold_y,save_model,save_model_path_fold)\n",
    "            for hp in hyper_params:\n",
    "                hp_dict[hp].append(clf.best_estimator_.get_params()[hp])\n",
    "\n",
    "        else:\n",
    "            clf = model_clf\n",
    "            clf.fit(fold_X,fold_y)\n",
    "        \n",
    "        #CV_score\n",
    "        CV_R2_train.append(clf.score(cv_X_train[i],cv_y_train[i]))\n",
    "        CV_R2_valid.append(clf.score(cv_X_valid[i],cv_y_valid[i]))\n",
    "        \n",
    "        CV_MSE_train.append(mse(clf.predict(cv_X_train[i]),cv_y_train[i]))\n",
    "        CV_MSE_valid.append(mse(clf.predict(cv_X_valid[i]),cv_y_valid[i]))\n",
    "           \n",
    "    #retrain model on the entire train + valid set with most frequent hyper-params during cross-val\n",
    "    if inner_loop:\n",
    "        hp_mode = {}\n",
    "        for hp in hyper_params:\n",
    "            hp_mode[hp] = mode(hp_dict[hp])[0][0]\n",
    "            \n",
    "        print 'most frequent hp:' + str(hp_mode)\n",
    "    \n",
    "else: \n",
    "    #Grabs the best classifer as a result of N-fold nested CV along with the MSE and R2 stats of the outerloop\n",
    "    print \"Loading previously saved model: \"\n",
    "    f = open(load_model_path)\n",
    "    result = pickle.load(f)\n",
    "    test_clf = result['best_clf']\n",
    "    CV_R2_valid = result['CV_R2']\n",
    "    CV_MSE_valid = result['CV_MSE']\n",
    "    f.close()\n",
    "    \n",
    "print 'CV R2 (mean, std_err): ' + '{:04.2f}, {:04.2f}'.format(np.mean(CV_R2_valid),stats.sem(CV_R2_valid))\n",
    "print 'CV MSE (mean, std_err): ' + '{:04.2f}, {:04.2f}'.format(np.mean(CV_MSE_valid),stats.sem(CV_MSE_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must  match the input. Model n_features is 76 and  input n_features is 22020 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-555a7d1ad402>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0msave_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier_model_and_stats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_model_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mpearson_r_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mR2_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mMSE_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    629\u001b[0m                              \u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"threading\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_parallel_helper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m             for e in self.estimators_)\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;31m# Reduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \"\"\"\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_helper\u001b[1;34m(obj, methodname, *args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_parallel_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethodname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;34m\"\"\"Private helper to workaround Python 2 pickle limitations\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethodname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    349\u001b[0m                              \u001b[1;34m\" match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                              \u001b[1;34m\" input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must  match the input. Model n_features is 76 and  input n_features is 22020 "
     ]
    }
   ],
   "source": [
    "# Evaluate the test set based on most frequnt hyper-params\n",
    "if train_clf:\n",
    "    #test_clf = LinearRegression()  \n",
    "    #test_clf = Lasso(alpha=0.05)    \n",
    "    test_clf = SVR(kernel='linear',C=1)\n",
    "    #test_clf = RandomForestRegressor(n_estimators=500,min_samples_split=1)\n",
    "    test_clf.fit(cv_data['X'],cv_data['y'])\n",
    "\n",
    "    save_model = True\n",
    "    save_model_path = CV_model_dir + model_choice + '_' + clinical_scale[0]\n",
    "    if save_model:\n",
    "        classifier_model_and_stats = {'best_clf':test_clf, 'CV_R2':CV_R2_valid, 'CV_MSE':CV_MSE_valid}\n",
    "        save_classifier(classifier_model_and_stats,save_model_path)\n",
    "\n",
    "pearson_r_train = stats.pearsonr(test_clf.predict(cv_data['X']),cv_data['y'])\n",
    "R2_train = test_clf.score(cv_data['X'],cv_data['y'])\n",
    "MSE_train = mse(test_clf.predict(cv_data['X']),cv_data['y'])\n",
    "\n",
    "pearson_r_test = stats.pearsonr(test_clf.predict(test_data['X']),test_data['y'])\n",
    "R2_test = test_clf.score(test_data['X'],test_data['y'])\n",
    "MSE_test = mse(test_clf.predict(test_data['X']),test_data['y'])\n",
    "\n",
    "print \"train r: \" + str(pearson_r_train), \"test r: \" + str(pearson_r_test)\n",
    "print \"train R2 score: \" + str(R2_train), \"train MSE: \" + str(MSE_train)\n",
    "print \"test R2 score: \" + str(R2_test), \"test MSE: \" + str(MSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#R2_list = []\n",
    "#MSE_list = []\n",
    "#R2_list.append(CV_R2_valid)\n",
    "#MSE_list.append(CV_MSE_valid)\n",
    "boxplot_config_R2 = collections.defaultdict(list)\n",
    "boxplot_config_MSE = collections.defaultdict(list)\n",
    "method_labels = ['LR_L1', 'SVM', 'RFR']\n",
    "\n",
    "for i in np.arange(3):\n",
    "    boxplot_config_R2[method_labels[i]].append(R2_list[i])\n",
    "    boxplot_config_MSE[method_labels[i]].append(MSE_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/ipykernel/__main__.py:10: FutureWarning: \n",
      "The default value for 'return_type' will change to 'axes' in a future release.\n",
      " To use the future behavior now, set return_type='axes'.\n",
      " To keep the previous behavior and silence this warning, set return_type='dict'.\n",
      "/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/ipykernel/__main__.py:22: FutureWarning: \n",
      "The default value for 'return_type' will change to 'axes' in a future release.\n",
      " To use the future behavior now, set return_type='axes'.\n",
      " To keep the previous behavior and silence this warning, set return_type='dict'.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.artist import setp\n",
    "plt.figure()\n",
    "font_small = 12\n",
    "font_med = 16\n",
    "font_large = 24\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "df_array = pd.DataFrame(dict([ (k,pd.Series(v[0])) for k,v in boxplot_config_R2.iteritems() ]))\n",
    "bplot = df_array.boxplot(column=method_labels, fontsize=font_large)\n",
    "#plt.xlabel('Method',fontsize=font_large)\n",
    "plt.ylabel('R2',fontsize=font_large)\n",
    "plt.xticks(fontsize=font_med)\n",
    "plt.yticks(fontsize=font_small)\n",
    "setp(bplot['boxes'], linewidth=2)\n",
    "setp(bplot['whiskers'], linewidth=2)\n",
    "setp(bplot['fliers'], linewidth=2)\n",
    "setp(bplot['medians'], linewidth=2)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "df_array = pd.DataFrame(dict([ (k,pd.Series(v[0])) for k,v in boxplot_config_MSE.iteritems() ]))\n",
    "bplot = df_array.boxplot(column=method_labels, fontsize=font_large)\n",
    "#plt.xlabel('Method',fontsize=font_large)\n",
    "plt.ylabel('MSE',fontsize=font_large)\n",
    "\n",
    "#plt.title('Number of Atlases',fontsize=font_large)\n",
    "#plt.ylim([.5,1.0])\n",
    "plt.xticks(fontsize=font_med)\n",
    "plt.yticks(fontsize=font_small)\n",
    "setp(bplot['boxes'], linewidth=2)\n",
    "setp(bplot['whiskers'], linewidth=2)\n",
    "setp(bplot['fliers'], linewidth=2)\n",
    "setp(bplot['medians'], linewidth=2)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This Lasso instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-19f43cebbb68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Compute scores and MSEs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my_cv_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \"\"\"\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[0m_center_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m         \"\"\"\n\u001b[1;32m--> 704\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'n_iter_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m             return np.ravel(safe_sparse_dot(self.coef_, X.T, dense_output=True)\n",
      "\u001b[1;32m/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m: This Lasso instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "# Compute scores and MSEs\n",
    "\n",
    "y_cv_pred = test_clf.predict(cv_data['X'])\n",
    "y_test_pred = test_clf.predict(test_data['X'])\n",
    "\n",
    "x_data_array = [cv_data['y'],test_data['y']]\n",
    "y_data_array = [y_cv_pred,y_test_pred]\n",
    "lable_array = ['CV train performance','test performance']\n",
    "\n",
    "# only test perf\n",
    "#x_data_array = [test_data['y']]\n",
    "#y_data_array = [y_test_pred]\n",
    "#lable_array = ['test performance']\n",
    "\n",
    "plt.figure()\n",
    "font_small = 8\n",
    "font_med = 16\n",
    "font_large = 24\n",
    "no_of_plots = len(lable_array)\n",
    "plt.style.use('ggplot')\n",
    "plt_col = no_of_plots\n",
    "plt_row = 1\n",
    "\n",
    "for i in np.arange(no_of_plots):\n",
    "    x = x_data_array[i]\n",
    "    y = y_data_array[i]\n",
    "\n",
    "    plt.subplot(plt_row,plt_col,i+1)\n",
    "    plt.scatter(x, y, c='crimson', label=lable_array[i],s=40)\n",
    "    fit = np.polyfit(x,y,1)\n",
    "    fit_fn = np.poly1d(fit) \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    if p_value < 0.0001:\n",
    "        p_value_sig = '<0.0001'\n",
    "    else:\n",
    "        p_value_sig = str(p_value)\n",
    "        \n",
    "    label_str = 'r-value: {:04.2f}'.format(r_value) + '\\n' + 'p-value: ' + p_value_sig + '\\n' + 'std_err: {:04.2f}'.format(std_err) \n",
    "    # fit_fn is now a function which takes in x and returns an estimate for y\n",
    "    plt.plot(x, fit_fn(x),linewidth=3, label=label_str)\n",
    "    plt.title(model_choice,fontsize=font_large)\n",
    "    plt.xlabel('Actual Score',fontsize=font_large)\n",
    "    plt.ylabel('Predicted Score',fontsize=font_large)            \n",
    "    plt.legend(fontsize=font_med,loc=2)\n",
    "\n",
    "if feat_imp:\n",
    "    plt.figure()\n",
    "    #plt.subplot(plt_row,plt_col,4)\n",
    "    x_pos = np.arange(len(feature_cols))\n",
    "    \n",
    "    if model_choice == 'RFR':\n",
    "        feature_wts = test_clf.feature_importances_\n",
    "    elif model_choice == 'LR_L1':\n",
    "        feature_wts = np.squeeze(test_clf.coef_)\n",
    "    else: \n",
    "        print 'no feature_wt vector found'\n",
    "        \n",
    "    sorted_feat_idx = np.argsort(np.abs(feature_wts))[::-1]        \n",
    "    plt.bar(x_pos,feature_wts[sorted_feat_idx],color='crimson')\n",
    "    plt.ylabel('Feature Importance',fontsize=font_large)\n",
    "    #Sort the feature name list as well \n",
    "    sorted_feature_cols = []\n",
    "    for i,sort_idx in enumerate(sorted_feat_idx):\n",
    "        sorted_feature_cols.append(feature_cols[sort_idx])\n",
    "\n",
    "    plt.xticks(x_pos,sorted_feature_cols,rotation=70,fontsize=font_small)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Additional Scripts \n",
    "# Concat of Train + Valid (to generate multi-folds)\n",
    "t_data = pd.read_pickle(train_file)\n",
    "v_data = pd.read_pickle(valid_file)\n",
    "frames = [t_data, v_data]\n",
    "result = pd.concat(frames)\n",
    "result.to_pickle(\"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_train_plus_val.pkl\")\n",
    "\n",
    "# Generatng K-Folds\n",
    "sampx = 100 #Train + Valid samples\n",
    "foldx = 10   \n",
    "kf = KFold(sampx, n_folds=foldx,shuffle=True)\n",
    "\n",
    "#for train, test in kf:\n",
    "#    print(\"%s %s\" % (train, test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
