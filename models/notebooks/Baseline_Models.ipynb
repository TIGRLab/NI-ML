{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import re\n",
    "import collections\n",
    "import tables as tb\n",
    "from math import isnan\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data imports\n",
    "my_name = 'Exp6'\n",
    "cohort = 'ADNI2'\n",
    "clinical_scale = 'ADAS13'\n",
    "exp_name = '{}_{}_{}'.format(my_name,cohort,clinical_scale)\n",
    "atlas = 'AAL'\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "HC_L_data_path = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_left_{}.pkl'.format(cohort,my_name)\n",
    "HC_R_data_path = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_right_{}.pkl'.format(cohort,my_name)\n",
    "CT_data_path = baseline_dir + 'CT/civet_out/{}_subject_ROI_CT_dict_{}.pkl'.format(cohort,atlas)\n",
    "CT_unique_ROIs_path = baseline_dir + 'CT/civet_out/ADNI_unique_ROIs_{}.pkl'.format(atlas)\n",
    "sub_CS_data_path = baseline_dir + 'CS/{}_BL_PTID_{}_dict.pkl'.format(cohort,clinical_scale)\n",
    "sub_DX_data_path = baseline_dir + 'CS/{}_BL_PTID_DX_bl_dict.pkl'.format(cohort)\n",
    "sub_age_data_path = baseline_dir + 'CS/{}_age.pkl'.format(cohort)\n",
    "sub_sex_data_path = baseline_dir + 'CS/{}_sex.pkl'.format(cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fused labels and volumes\n",
    "sub_HC_L_dict = pickle.load( open(HC_L_data_path, \"rb\" ) )\n",
    "sub_HC_R_dict = pickle.load( open(HC_R_data_path, \"rb\" ) )\n",
    "\n",
    "HC_L_total_vol_dict = {}\n",
    "HC_R_total_vol_dict = {}\n",
    "\n",
    "HC_common_keys = set(sub_HC_L_dict.keys()) & set(sub_HC_R_dict.keys())\n",
    "print 'Common HC keys: {}'.format(len(HC_common_keys))\n",
    "for key in HC_common_keys:\n",
    "    fused_vol = stats.mode(sub_HC_L_dict[key],axis=0)[0] #Left HC\n",
    "    HC_L_total_vol_dict[key] = np.sum(fused_vol)\n",
    "    fused_vol = stats.mode(sub_HC_R_dict[key],axis=0)[0] #Right HC\n",
    "    HC_R_total_vol_dict[key] = np.sum(fused_vol)\n",
    "    \n",
    "#Mean CT values for AAL\n",
    "sub_CT_dict = pickle.load( open(CT_data_path, \"rb\" ) )\n",
    "unique_ROIs = pickle.load( open(CT_unique_ROIs_path, \"rb\" ) )\n",
    "#unique_ROIs = sub_CT_dict['137_S_0459'][0].keys()\n",
    "CT_mean_dict = {}\n",
    "for key in sub_CT_dict.keys():\n",
    "    CT_roi_dict = sub_CT_dict[key][0]\n",
    "    mean_CT_list = []\n",
    "    for roi in unique_ROIs:\n",
    "        #ignore roi list (mainly the background ones...)\n",
    "        if not roi in [0]:\n",
    "            mean_CT_list.append(np.mean(CT_roi_dict[roi]))\n",
    "                \n",
    "    CT_mean_dict[key] = mean_CT_list\n",
    "    \n",
    "print 'HC_L_keys: {}, HC_R_keys: {}, CT_keys: {}'.format(len(HC_L_total_vol_dict), len(HC_R_total_vol_dict), len(CT_mean_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print stats.mode(sub_HC_L_dict[key],axis=0)[0].shape\n",
    "#print stats.mode(sub_HC_R_dict[key],axis=0)[0].shape\n",
    "#sub_age_dict = pickle.load(open(sub_age_data_path, \"rb\"))\n",
    "#sub_sex_dict = pickle.load(open(sub_sex_data_path, \"rb\"))\n",
    "\n",
    "sub_CS_dict = pickle.load( open(sub_CS_data_path, \"rb\" ) )\n",
    "sub_CS_dict_clean = filter(lambda k: not isnan(sub_CS_dict[k]), sub_CS_dict) #remove NaNs\n",
    "sub_CS_dict_clean = {k: sub_CS_dict[k] for k in sub_CS_dict if not isnan(sub_CS_dict[k])}\n",
    "\n",
    "print cohort, clinical_scale\n",
    "scores = sub_CS_dict_clean.values()\n",
    "print np.mean(scores), np.std(scores),  np.min(scores), np.max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dump the unique ordered list of CT ROIs that is consistent across ADNI1 and 2. \n",
    "sub_CT_dict = pickle.load( open(CT_data_path, \"rb\" ) )\n",
    "unique_ROIs = sub_CT_dict['137_S_0459'][0].keys()\n",
    "print len(unique_ROIs)\n",
    "pickleIt(unique_ROIs,baseline_dir + 'CT/civet_out/ADNI_unique_ROIs_{}.pkl'.format(atlas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_demographics = False\n",
    "# ADAS score dict\n",
    "sub_CS_dict = pickle.load( open(sub_CS_data_path, \"rb\" ) )\n",
    "sub_CS_dict_clean = filter(lambda k: not isnan(sub_CS_dict[k]), sub_CS_dict) #remove NaNs\n",
    "sub_CS_dict_clean = {k: sub_CS_dict[k] for k in sub_CS_dict if not isnan(sub_CS_dict[k])}\n",
    "\n",
    "#Dx score dict (for stratified k-fold)\n",
    "sub_DX_dict = pickle.load( open(sub_DX_data_path, \"rb\" ) )\n",
    "#sub_DX_dict_clean = filter(lambda k: not isnan(sub_DX_dict[k]), sub_DX_dict) #remove NaNs\n",
    "#sub_DX_dict_clean = {k: sub_DX_dict[k] for k in sub_DX_dict if not isnan(sub_DX_dict[k])}\n",
    "\n",
    "#Demographics\n",
    "sub_age_dict = pickle.load(open(sub_age_data_path, \"rb\"))\n",
    "sub_sex_dict = pickle.load(open(sub_sex_data_path, \"rb\"))\n",
    "    \n",
    "common_keys = list(set(HC_L_total_vol_dict.keys()) & set(HC_R_total_vol_dict.keys()) \n",
    "                   & set(CT_mean_dict.keys()) & set(sub_CS_dict_clean.keys()))\n",
    "print '# of common keys across modalities: {}'.format(len(common_keys))\n",
    "X = []\n",
    "y = []\n",
    "y_dx = [] #Used for creating balanced k-folds\n",
    "for key in common_keys:\n",
    "    if use_demographics: #Age does come up as important feature from RF runs :-)\n",
    "        #print \"using age and sex variables\"\n",
    "        X.append(np.array([HC_L_total_vol_dict[key]] + [HC_R_total_vol_dict[key]] + CT_mean_dict[key] + \n",
    "                         [sub_age_dict[key]] + [sub_sex_dict[key]]))\n",
    "    else:\n",
    "        X.append(np.array([HC_L_total_vol_dict[key]] + [HC_R_total_vol_dict[key]] + CT_mean_dict[key]))\n",
    "        \n",
    "    y.append(sub_CS_dict_clean[key])\n",
    "    if key in sub_DX_dict:\n",
    "        if sub_DX_dict[key] in ['EMCI','LMCI']:\n",
    "            y_dx.append('MCI')\n",
    "        elif sub_DX_dict[key] in ['CN','SMC']:   \n",
    "            y_dx.append('CN')\n",
    "        elif sub_DX_dict[key] in ['AD']:  \n",
    "            y_dx.append('AD')\n",
    "        else:\n",
    "            print \"Unknown Dx\"\n",
    "    else:\n",
    "        print \"No Dx found for {}\".format(key)\n",
    "        y_dx.append('CN') #Two subjects without Dx \n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X shape: {}, y shape: {}'.format(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine ADNI1 and ADNI2 cohorts \n",
    "def concatCohorts(exp_setup_path_1,exp_setup_path_2):\n",
    "    exp_setup = pickle.load( open(exp_setup_path_1, \"rb\" ) )\n",
    "    X1 = exp_setup['X']\n",
    "    y1 = exp_setup['y']\n",
    "    y1_dx = exp_setup['y_dx']\n",
    "    exp2_common_subs = exp_setup['common_subs']\n",
    "    exp_setup = pickle.load( open(exp_setup_path_2, \"rb\" ) )\n",
    "    X2 = exp_setup['X']\n",
    "    y2 = exp_setup['y']\n",
    "    y2_dx = exp_setup['y_dx']\n",
    "    exp1_common_subs = exp_setup['common_subs']\n",
    "    \n",
    "    #concat\n",
    "    X = np.vstack((X1,X2))\n",
    "    y = np.concatenate((y1,y2))\n",
    "    y_dx = np.concatenate((y1_dx,y2_dx))\n",
    "    common_subs = exp1_common_subs + exp2_common_subs\n",
    "\n",
    "    return {'X':X, 'y':y, 'y_dx':y_dx, 'common_subs':common_subs}\n",
    "\n",
    "def concatDictionaries(modality_path_dict_1,modality_path_dict_2):    #Not tested yet.. \n",
    "    #input: {'L_HC':'path/to/dict',...,'CS':'path/to/dict' }\n",
    "    comb_data_dict = {}\n",
    "    for modality in modality_path_dict_1.keys():\n",
    "        adni1_data = pickle.load( open(modality_path_dict_1[modality], \"rb\" ) )\n",
    "        adni2_data = pickle.load( open(modality_path_dict_2[modality], \"rb\" ) )\n",
    "        #check for common entries --> which shouldn't exist\n",
    "        common_keys = list(set(adni1_data.keys()) & set(adni2_data.keys()))\n",
    "        if len(common_keys) > 0:\n",
    "            print '{} common_keys found across cohorts for modality: {}'.format(len(common_keys),modality)\n",
    "        else:\n",
    "            adni_1_2_data = adni1_data.copy()\n",
    "            adni_1_2_data.update(adni2_data)            \n",
    "            comb_data_dict[modality] = adni_1_2_data\n",
    "            print '{} data combined with {} total subjects'.format(modality, len(adni_1_2_data))\n",
    "        \n",
    "    return comb_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# either 'exp': combines experimental data for adni1 and 2 --> used for baseline models\n",
    "# or 'dict' for NN models. Combine HC,CT,CS\n",
    "# kf from exp-concatination is still used for NN model CV\n",
    "concat_type = 'exp' \n",
    "exp_name = 'Exp6'\n",
    "Clinical_Scale = 'MMSE'\n",
    "\n",
    "if concat_type == 'exp':\n",
    "    print 'Combining exp-setups'\n",
    "    exp_setup_path_adni1 = baseline_dir + 'exp_data/CV_{}_ADNI1_{}.pkl'.format(exp_name, Clinical_Scale)\n",
    "    exp_setup_path_adni2 = baseline_dir + 'exp_data/CV_{}_ADNI2_{}.pkl'.format(exp_name, Clinical_Scale)\n",
    "\n",
    "    comb_data = concatCohorts(exp_setup_path_adni1,exp_setup_path_adni2)\n",
    "    X = comb_data['X']\n",
    "    y = comb_data['y']\n",
    "    y_dx = comb_data['y_dx']\n",
    "    common_keys = comb_data['common_subs']\n",
    "    print X.shape, y.shape, len(common_keys)\n",
    "    exp_name = exp_name + '_ADNI1and2_' + Clinical_Scale\n",
    "    print exp_name\n",
    "    \n",
    "elif concat_type == 'dict':\n",
    "    print 'Combining dictionaries' \n",
    "    cohort = 'ADNI1'\n",
    "    modality_path_dict_1 = {}\n",
    "    modality_path_dict_2 = {}\n",
    "    modality_path_dict_1['HC_L'] = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_left_{}.pkl'.format(cohort,exp_name)\n",
    "    modality_path_dict_1['HC_R'] = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_right_{}.pkl'.format(cohort,exp_name)\n",
    "    modality_path_dict_1['CT'] = baseline_dir + 'CT/civet_out/{}_subject_ROI_CT_dict_{}.pkl'.format(cohort,atlas)    \n",
    "    modality_path_dict_1['CS'] = baseline_dir + 'CS/{}_BL_PTID_ADAS13_dict.pkl'.format(cohort)\n",
    "    modality_path_dict_1['DX'] = baseline_dir + 'CS/{}_BL_PTID_DX_bl_dict.pkl'.format(cohort)\n",
    "    cohort = 'ADNI2' \n",
    "    modality_path_dict_2['HC_L'] = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_left_{}.pkl'.format(cohort,exp_name)\n",
    "    modality_path_dict_2['HC_R'] = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_right_{}.pkl'.format(cohort,exp_name)\n",
    "    modality_path_dict_2['CT'] = baseline_dir + 'CT/civet_out/{}_subject_ROI_CT_dict_{}.pkl'.format(cohort,atlas)    \n",
    "    modality_path_dict_2['CS'] = baseline_dir + 'CS/{}_BL_PTID_ADAS13_dict.pkl'.format(cohort)\n",
    "    modality_path_dict_2['DX'] = baseline_dir + 'CS/{}_BL_PTID_DX_bl_dict.pkl'.format(cohort)\n",
    "    \n",
    "    comb_data = concatDictionaries(modality_path_dict_1,modality_path_dict_2)\n",
    "    \n",
    "    for key in comb_data.keys():\n",
    "        print '{} : {}'.format(key, len(comb_data[key]))\n",
    "else:\n",
    "    print 'Wrong concat_type'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save concatinated dictionaries\n",
    "cohort = 'ADNI1and2'\n",
    "modality_path_dict_comb = {}\n",
    "modality_path_dict_comb['HC_L'] = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_left_{}.pkl'.format(cohort,exp_name)\n",
    "modality_path_dict_comb['HC_R'] = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_right_{}.pkl'.format(cohort,exp_name)\n",
    "modality_path_dict_comb['CT'] = baseline_dir + 'CT/civet_out/{}_subject_ROI_CT_dict_{}.pkl'.format(cohort,atlas)    \n",
    "modality_path_dict_comb['CS'] = baseline_dir + 'CS/{}_BL_PTID_ADAS13_dict.pkl'.format(cohort)\n",
    "modality_path_dict_comb['DX'] = baseline_dir + 'CS/{}_BL_PTID_DX_bl_dict.pkl'.format(cohort)\n",
    "\n",
    "for key in comb_data.keys():\n",
    "    pickleIt(comb_data[key],modality_path_dict_comb[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create folds for CV (default Stratified based on DX)\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "#Encode Dx labels to integers map: {AD:0,CN:1,LMCI:2}\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_dx)\n",
    "y_dx_int = le.transform(y_dx) \n",
    "\n",
    "KF_type = baseline_dir + 'exp_data/CV_Exp6_ADNI1and2_ADAS13.pkl'  #'path/to/previous/setup' or 'stratified' or 'none'\n",
    "if KF_type == 'none':\n",
    "    print 'KFold'\n",
    "    kf = KFold(len(y), n_folds=10)\n",
    "elif KF_type == 'stratified': \n",
    "    print 'Stratified KFold'\n",
    "    kf = StratifiedKFold(y_dx_int, n_folds=10)\n",
    "else:\n",
    "    print 'Loading previously saved KF from: {}'.format(KF_type)\n",
    "    old_exp_setup = pickle.load( open(KF_type, \"rb\" ) )\n",
    "    kf = old_exp_setup['kf']\n",
    "    \n",
    "for train_index, test_index in kf:    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print 'DX distribution (AD,CN,LMCI): {}'.format((np.sum(y_dx_int[test_index]==0),np.sum(y_dx_int[test_index]==1),\n",
    "                                              np.sum(y_dx_int[test_index]==2)))\n",
    "    print 'y_train_mean: {}, y_test_mean: {}'.format(np.mean(y_train),np.mean(y_test))\n",
    "    \n",
    "save_experimental_setup = True #Saves X, y, and KF\n",
    "save_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/CV_{}.pkl'.format(exp_name)    \n",
    "print 'Saving exp_setup to: {}'.format(save_path)\n",
    "if save_experimental_setup:    \n",
    "    exp_setup = {'X': X, 'y': y, 'y_dx': y_dx, 'kf':kf,'common_subs':common_keys,'exp_name':exp_name}    \n",
    "    pickleIt(exp_setup, save_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# QC plots\n",
    "n_feat = X.shape[1]\n",
    "print X.shape\n",
    "\n",
    "feat_corr  = []\n",
    "for col in np.arange(n_feat):\n",
    "    feat_corr.append(stats.pearsonr(X[:,col],y)[0])\n",
    "\n",
    "print 'L_HC corr: {}, R_HC_corr: {}'.format(feat_corr[0],feat_corr[1]) \n",
    "print 'HC Correlation in each fold'\n",
    "fid = 1\n",
    "for train_index, test_index in kf:    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_L_HC_vol_mean = np.mean(X_train[:,0])\n",
    "    train_L_HC_vol_std = np.std(X_train[:,0])\n",
    "    train_R_HC_vol_mean = np.mean(X_train[:,1])\n",
    "    train_R_HC_vol_std = np.std(X_train[:,1])\n",
    "    \n",
    "    test_L_HC_vol_mean = np.mean(X_test[:,0])\n",
    "    test_L_HC_vol_std = np.std(X_test[:,0])\n",
    "    test_R_HC_vol_mean = np.mean(X_test[:,1])\n",
    "    test_R_HC_vol_std = np.std(X_test[:,1])\n",
    "    \n",
    "    train_L_HC_corr = stats.pearsonr(X_train[:,0],y_train)[0]\n",
    "    train_R_HC_corr = stats.pearsonr(X_train[:,1],y_train)[0]\n",
    "    test_L_HC_corr = stats.pearsonr(X_test[:,0],y_test)[0]\n",
    "    test_R_HC_corr = stats.pearsonr(X_test[:,1],y_test)[0]\n",
    "    print fid\n",
    "    fid+=1\n",
    "    print 'test L_HC vol, std, corr: {}, {}, {}'.format(test_L_HC_vol_mean,test_L_HC_vol_std,test_L_HC_corr)\n",
    "    print 'test R_HC vol, std, corr: {}, {}, {}'.format(test_R_HC_vol_mean,test_R_HC_vol_std,test_R_HC_corr)\n",
    "    print 'train L_HC vol, std, corr: {}, {}, {}'.format(train_L_HC_vol_mean,train_L_HC_vol_std,train_L_HC_corr)\n",
    "    print 'train R_HC vol, std, corr: {}, {}, {}'.format(train_R_HC_vol_mean,train_R_HC_vol_std,train_R_HC_corr)\n",
    "    \n",
    "\n",
    "alpha = 0.5\n",
    "plt.figure()\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('HC_L total volume')\n",
    "plt.hist(X[y_dx_int==0,0],bins=100,alpha=alpha,label='AD: {}'.format(np.mean(X[y_dx_int==0,0])))\n",
    "plt.hist(X[y_dx_int==1,0],bins=100,alpha=alpha,label='CN: {}'.format(np.mean(X[y_dx_int==1,0])))\n",
    "plt.hist(X[y_dx_int==2,0],bins=100,alpha=alpha,label='MCI: {}'.format(np.mean(X[y_dx_int==2,0])))\n",
    "plt.legend()\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('HC_R total volume')\n",
    "plt.hist(X[y_dx_int==0,1],bins=100,alpha=alpha,label='AD: {}'.format(np.mean(X[y_dx_int==0,1])))\n",
    "plt.hist(X[y_dx_int==1,1],bins=100,alpha=alpha,label='CN: {}'.format(np.mean(X[y_dx_int==1,1])))\n",
    "plt.hist(X[y_dx_int==2,1],bins=100,alpha=alpha,label='MCI: {}'.format(np.mean(X[y_dx_int==2,1])))\n",
    "plt.legend()\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('CT mean per ROI')\n",
    "plt.bar(np.arange(n_feat-2),np.mean(X[:,2:],axis=0))\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('Correlation with CS for each feature')\n",
    "plt.bar(np.arange(n_feat),feat_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adni1_covar_df =  pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/CS/adni1_covar.pkl', \"rb\" ) )\n",
    "adni2_covar_df =  pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/CS/adni2_covar.pkl', \"rb\" ) )\n",
    "\n",
    "print 'ADNI1'\n",
    "print adni1_covar_df.describe()\n",
    "print 'ADNI2'\n",
    "print adni2_covar_df.describe()\n",
    "                                                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Volume significance testing across ADNI1 and 2\n",
    "import statsmodels.formula.api as smf\n",
    "#'AGE','PTGENDER','WholeBrain'\n",
    "cols=('ID', 'cohort', 'Age', 'Sex', 'TBV', 'HC_L', 'HC_R')\n",
    "\n",
    "meth_df = pd.DataFrame(columns=cols)\n",
    "adni1_covar_df =  pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/CS/adni1_covar.pkl', \"rb\" ) )\n",
    "adni2_covar_df =  pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/CS/adni2_covar.pkl', \"rb\" ) )\n",
    "\n",
    "cohort = 'adni1'\n",
    "for img_idx in adni1_HC_L_total_vol_dict.keys():\n",
    "    row = pd.Series([img_idx, cohort, np.array(adni1_covar_df[(adni1_covar_df.PTID ==img_idx)]['AGE'])[0], \n",
    "                 np.array(adni1_covar_df[(adni1_covar_df.PTID ==img_idx)]['PTGENDER'])[0], \n",
    "                 np.array(adni1_covar_df[(adni1_covar_df.PTID ==img_idx)]['WholeBrain'])[0], \n",
    "                 adni1_HC_L_total_vol_dict[img_idx], adni1_HC_R_total_vol_dict[img_idx]], index=cols)\n",
    "        \n",
    "    #add row to the table\n",
    "    meth_df.loc[len(meth_df)] = row\n",
    "    \n",
    "cohort = 'adni2'\n",
    "for img_idx in adni2_HC_L_total_vol_dict.keys():\n",
    "    row = pd.Series([img_idx, cohort, np.array(adni2_covar_df[(adni2_covar_df.PTID ==img_idx)]['AGE'])[0], \n",
    "                 np.array(adni2_covar_df[(adni2_covar_df.PTID ==img_idx)]['PTGENDER'])[0], \n",
    "                 np.array(adni2_covar_df[(adni2_covar_df.PTID ==img_idx)]['WholeBrain'])[0], \n",
    "                 adni2_HC_L_total_vol_dict[img_idx], adni2_HC_R_total_vol_dict[img_idx]], index=cols)\n",
    "        \n",
    "    #add row to the table\n",
    "    meth_df.loc[len(meth_df)] = row\n",
    "        \n",
    "#meth_df\n",
    "est = smf.ols(formula='HC_R ~ Sex + Age + TBV + C(cohort, Treatment(reference=\"adni1\"))', data=meth_df).fit()\n",
    "print 't_vals:'\n",
    "print est.tvalues\n",
    "print 'p_vals:'\n",
    "print est.pvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load save experimental setup\n",
    "cohort = 'ADNI2'\n",
    "exp_name = 'Exp6'\n",
    "exp_setup_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/CV_{}_{}_ADAS13.pkl'.format(exp_name,cohort)\n",
    "exp_setup = pickle.load( open(exp_setup_path, \"rb\" ) )\n",
    "\n",
    "X = exp_setup['X']\n",
    "y = exp_setup['y']\n",
    "# y_dx = exp_setup['y_dx']\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit(y_dx)\n",
    "# y_dx_int = le.transform(y_dx) \n",
    "\n",
    "print cohort\n",
    "# print 'Left HC'\n",
    "# print 'AD: {}'.format(int(np.mean(X[y_dx_int==0,0])))\n",
    "# print 'MCI: {}'.format(int(np.mean(X[y_dx_int==2,0])))\n",
    "# print 'CN: {}'.format(int(np.mean(X[y_dx_int==1,0])))\n",
    "# print 'Right HC'\n",
    "# print 'AD: {}'.format(int(np.mean(X[y_dx_int==0,1])))\n",
    "# print 'MCI: {}'.format(int(np.mean(X[y_dx_int==2,1])))\n",
    "# print 'CN: {}'.format(int(np.mean(X[y_dx_int==1,1])))\n",
    "# print  'CT'\n",
    "# print 'AD: {}'.format((np.mean(X[y_dx_int==0,2:])))\n",
    "# print 'MCI: {}'.format((np.mean(X[y_dx_int==2,2:])))\n",
    "# print 'CN: {}'.format((np.mean(X[y_dx_int==1,2:])))\n",
    "\n",
    "#ADNI1: CT\n",
    "#AD: 2.89622991643\n",
    "#MCI: 2.98721598902\n",
    "#CN: 3.06806548109\n",
    "#ADNI2: CT\n",
    "#AD: 2.79175707773\n",
    "#MCI: 2.92246056401\n",
    "#CN: 2.9658935139\n",
    "\n",
    "kf = exp_setup['kf']\n",
    "for train_index, test_index in kf:    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print y_test\n",
    "    \n",
    "#print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configs for K-fold validations (nested)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import grid_search\n",
    "import datetime\n",
    "import time\n",
    "import collections\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import ipyparallel as ipp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import KFold\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pick model with its configs/hyper-params\n",
    "model_list = ['LR_L1', 'SVR', 'RFR']\n",
    "model_choice_id = 1\n",
    "model_choice = model_list[model_choice_id]\n",
    "modality = 'HC_CT'\n",
    "\n",
    "if model_choice == 'LR_L1':\n",
    "    model_clf = Lasso()\n",
    "    hyper_params = {'alpha':[0.1, 0.05, 0.01]} \n",
    "    scale_data = True #Scales HC and CT features\n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = True    \n",
    "        \n",
    "elif model_choice == 'SVR':\n",
    "    model_clf = SVR()\n",
    "    hyper_params = {'kernel':['linear','rbf'], 'C':[1,10,25]}\n",
    "    scale_data = True #Scales HC and CT features\n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = True\n",
    "           \n",
    "elif model_choice == 'RFR':\n",
    "    model_clf = RandomForestRegressor(n_jobs=6)\n",
    "    hyper_params = {'n_estimators':[10,50,100,200],'min_samples_split':[2,4,8]}\n",
    "    #hyper_params = {'min_samples_split':[2,4,8,16]}\n",
    "    scale_data = False\n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = True   \n",
    "    \n",
    "else:\n",
    "    print \"Unknown model choice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and Test models\n",
    "from functools import partial #Parallelize!!! \n",
    "\n",
    "# Load save experimental setup\n",
    "cohort = 'ADNI2'\n",
    "exp_name = 'Exp6'\n",
    "Clinical_Scale = 'MMSE'\n",
    "exp_setup_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/CV_{}_{}_{}.pkl'.format(exp_name,cohort,Clinical_Scale)\n",
    "exp_setup = pickle.load( open(exp_setup_path, \"rb\" ) )\n",
    "\n",
    "X_raw = exp_setup['X']\n",
    "\n",
    "if modality == 'HC_CT':\n",
    "    X_modality = X_raw\n",
    "elif modality == 'HC':\n",
    "    X_modality = X_raw[:,:2]\n",
    "elif modality == 'CT':\n",
    "    X_modality = X_raw[:,2:]\n",
    "else:\n",
    "    print \"Wrong modality selected...\"\n",
    "    \n",
    "if scale_data:\n",
    "    X = preprocessing.scale(X_modality)\n",
    "else:\n",
    "    X = X_modality\n",
    "\n",
    "print 'X shape {}'.format(X.shape)\n",
    "\n",
    "y = exp_setup['y']\n",
    "kf = exp_setup['kf']\n",
    "exp_name = exp_setup['exp_name']\n",
    "\n",
    "#Some paths to store models and performance stats\n",
    "CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/output/'\n",
    "saved_model_name =''\n",
    "save_model_path = CV_model_dir + exp_name + '_' + model_choice\n",
    "load_model_path = CV_model_dir + saved_model_name\n",
    "\n",
    "# train a new classifer? Or load a single pretrained classifier based on most frequent hyperparams found previously?\n",
    "# This will NOT load K different classifiers each for outer-CV fold  \n",
    "train_clf = True\n",
    "save_model = False #do you really want to save all classifiers per each fold? (default false) \n",
    "\n",
    "if train_clf:\n",
    "    # Create list of all the fold-subsets (needed for parallelization)\n",
    "    X_train = []\n",
    "    X_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []    \n",
    "    for train, valid in kf:        \n",
    "        X_train.append(X[train])\n",
    "        X_valid.append(X[valid])\n",
    "        y_train.append(y[train])\n",
    "        y_valid.append(y[valid])\n",
    "\n",
    "    CV_r_train=[] #pearson r score for each outer fold on train set\n",
    "    CV_r_valid=[] #pearson r score for each outer fold on validation set\n",
    "\n",
    "    CV_R2_train=[] #R2 score for each outer fold on train set\n",
    "    CV_R2_valid=[] #R2 score for each outer fold on validation set\n",
    "\n",
    "    CV_MSE_train=[] #MSE for each outer fold on train set\n",
    "    CV_MSE_valid=[] #MSE for each outer fold on validation set\n",
    "    \n",
    "    predicted_CV_scores = []\n",
    "    actual_CV_scores = []\n",
    "    \n",
    "    # Parallization configs for ipython notebook cluster    \n",
    "    rc = ipp.Client()\n",
    "    dview = rc[:]\n",
    "    dview.push(dict(computeOuterFold = computeOuterFold))\n",
    "    dview.push(dict(innerCVLoop = innerCVLoop))\n",
    "    mapfunc = partial(computeOuterFold, model_clf=model_clf, hyper_params=hyper_params, inner_loop=inner_loop, \n",
    "                  save_model=save_model, save_model_path=save_model_path)\n",
    "    parallel_result = dview.map_sync(mapfunc, X_train, y_train, X_valid, y_valid)    \n",
    "    \n",
    "    hp_dict = collections.defaultdict(list)\n",
    "    for pr in parallel_result:\n",
    "        CV_r_train.append(pr['r_train'])\n",
    "        CV_r_valid.append(pr['r_valid'])\n",
    "        CV_R2_train.append(pr['R2_train'])\n",
    "        CV_R2_valid.append(pr['R2_valid'])\n",
    "        CV_MSE_train.append(pr['MSE_train'])\n",
    "        CV_MSE_valid.append(pr['MSE_valid'])\n",
    "        predicted_CV_scores.append(pr['predicted_fold_score'])\n",
    "        actual_CV_scores.append(pr['actual_fold_scores'])\n",
    "        \n",
    "        for hp in hyper_params:\n",
    "            hp_dict[hp].append(pr['hp_dict'][hp])\n",
    "            \n",
    "    #Find out most frequent hyper-params during cross-val    \n",
    "    hp_mode = {}\n",
    "    for hp in hyper_params:\n",
    "        hp_mode[hp] = mode(hp_dict[hp])[0][0]\n",
    "\n",
    "    print 'most frequent hp:' + str(hp_mode)\n",
    "    \n",
    "else: \n",
    "    #Grabs the best classifer as a result of N-fold nested CV along with the MSE and R2 stats of the outerloop\n",
    "    print \"Loading previously saved model: \"\n",
    "    f = open(load_model_path)\n",
    "    result = pickle.load(f)\n",
    "    test_clf = result['best_clf']\n",
    "    CV_r_valid = result['CV_r']\n",
    "    CV_R2_valid = result['CV_R2']\n",
    "    CV_MSE_valid = result['CV_MSE']\n",
    "    f.close()\n",
    "\n",
    "print 'CV r (mean, median, std_err): ' + '{:04.2f},{:04.2f},{:04.2f}'.format(np.mean(zip(*CV_r_valid)[0]),np.median(zip(*CV_r_valid)[0]),stats.sem(zip(*CV_r_valid)[0]))\n",
    "print 'CV R2 (mean, median, std_err): ' + '{:04.2f},{:04.2f}, {:04.2f}'.format(np.mean(CV_R2_valid),np.median(CV_R2_valid),stats.sem(CV_R2_valid))\n",
    "print 'CV MSE (mean, median, std_err): ' + '{:04.2f},{:04.2f}, {:04.2f}'.format(np.mean(CV_MSE_valid),np.median(CV_MSE_valid),stats.sem(CV_MSE_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #check feature importance (QC for HC importance)\n",
    "# for fid in np.arange(10):\n",
    "#     model_clf.fit(X_train[fid],y_train[fid])\n",
    "#     feat_imp = model_clf.feature_importances_\n",
    "#     print \n",
    "#     print 'fid: {} r: {}'.format(fid, zip(*CV_r_valid)[0][fid])\n",
    "#     print feat_imp[70:], np.argsort(feat_imp)[70:]\n",
    "\n",
    "# #print zip(*CV_r_valid)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute single model from the most frequent hyper-params (for across dataset testing)\n",
    "\n",
    "#test_clf = Lasso(alpha=0.05)    \n",
    "#test_clf = SVR(kernel='rbf',C=1)\n",
    "test_clf = RandomForestRegressor(n_estimators=200,min_samples_split=2,n_jobs=4)\n",
    "\n",
    "save_CV_perf = False    \n",
    "if save_CV_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    save_model_filename = save_model_path + '_' + modality + '_' + st + '.pkl'\n",
    "    classifier_model_and_stats = {'best_clf':test_clf, 'CV_R2':CV_R2_valid, 'CV_MSE':CV_MSE_valid, 'CV_r': CV_r_valid,\n",
    "                                 'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    pickleIt(classifier_model_and_stats,save_model_filename)\n",
    "    print 'saving results at: {}'.format(save_model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting stuff...\n",
    "boxplot_config_r = {}\n",
    "boxplot_config_R2 = {}\n",
    "boxplot_config_MSE = {}\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/output/'\n",
    "Clinical_Scale = 'ADAS'\n",
    "cohort = 'ADNI1'\n",
    "multi_task = False #For ANN models\n",
    "if Clinical_Scale == 'ADAS':\n",
    "    if cohort == 'ADNI1':      \n",
    "        baseline_models = {'LR_HC*':'Exp6_ADNI1_ADAS13_LR_L1_HC_2016-04-21-15-09-33.pkl',\n",
    "                           'LR_CT*':'Exp6_ADNI1_ADAS13_LR_L1_CT_2016-04-21-15-13-04.pkl',\n",
    "                           'LR_HC_CT*':'Exp6_ADNI1_ADAS13_LR_L1_HC_CT_2016-04-21-15-04-00.pkl',\n",
    "                           'SVR_HC*':'Exp6_ADNI1_ADAS13_SVR_HC_2016-04-21-15-14-29.pkl',\n",
    "                           'SVR_CT*':'Exp6_ADNI1_ADAS13_SVR_CT_2016-04-21-15-17-53.pkl',\n",
    "                           'SVR_HC_CT*':'Exp6_ADNI1_ADAS13_SVR_HC_CT_2016-04-21-15-28-22.pkl', \n",
    "                           'RFR_HC':'Exp6_ADNI1_ADAS13_RFR_HC_2016-04-21-15-40-32.pkl',\n",
    "                           'RFR_CT':'Exp6_ADNI1_ADAS13_RFR_CT_2016-04-21-15-38-44.pkl',\n",
    "                           'RFR_HC_CT':'Exp6_ADNI1_ADAS13_RFR_HC_CT_2016-04-21-15-31-53.pkl'}\n",
    "        \n",
    "    if cohort == 'ADNI2':\n",
    "        baseline_models = {'LR_HC*':'Exp6_ADNI2_ADAS13_LR_L1_HC_2016-04-21-15-07-08.pkl',\n",
    "                           'LR_CT*':'Exp6_ADNI2_ADAS13_LR_L1_CT_2016-04-21-15-13-32.pkl',\n",
    "                           'LR_HC_CT*':'Exp6_ADNI2_ADAS13_LR_L1_HC_CT_2016-04-21-15-04-35.pkl',\n",
    "                           'SVR_HC*':'Exp6_ADNI2_ADAS13_SVR_HC_2016-04-21-15-14-13.pkl',\n",
    "                           'SVR_CT*':'Exp6_ADNI2_ADAS13_SVR_CT_2016-04-21-15-20-53.pkl',\n",
    "                           'SVR_HC_CT*':'Exp6_ADNI2_ADAS13_SVR_HC_CT_2016-04-21-15-26-27.pkl', \n",
    "                           'RFR_HC':'Exp6_ADNI2_ADAS13_RFR_HC_2016-04-21-15-42-00.pkl',\n",
    "                           'RFR_CT':'Exp6_ADNI2_ADAS13_RFR_CT_2016-04-21-15-36-31.pkl',\n",
    "                           'RFR_HC_CT':'Exp6_ADNI2_ADAS13_RFR_HC_CT_2016-04-21-15-34-16.pkl'}\n",
    "                \n",
    "    if cohort == 'ADNI1and2':\n",
    "        baseline_models = {'LR_HC*':'Exp6_concat_LR_L1_HC_2016-04-26-13-36-43.pkl',\n",
    "                           'LR_CT*':'Exp6_concat_LR_L1_CT_2016-04-26-13-41-43.pkl',\n",
    "                           'LR_HC_CT*':'Exp6_concat_LR_L1_HC_CT_2016-04-26-13-42-08.pkl',\n",
    "                           'SVR_HC*':'Exp6_concat_SVR_HC_2016-04-26-13-54-38.pkl',\n",
    "                           'SVR_CT*':'Exp6_concat_SVR_CT_2016-04-26-13-54-11.pkl',\n",
    "                           'SVR_HC_CT*':'Exp6_concat_SVR_HC_CT_2016-04-26-13-48-03.pkl', \n",
    "                           'RFR_HC':'Exp6_concat_RFR_HC_2016-04-26-13-56-18.pkl',\n",
    "                           'RFR_CT':'Exp6_concat_RFR_CT_2016-04-26-13-59-21.pkl',\n",
    "                           'RFR_HC_CT':'Exp6_concat_RFR_HC_CT_2016-04-26-14-04-03.pkl'}\n",
    "elif Clinical_Scale == 'MMSE':\n",
    "    if cohort == 'ADNI1':    \n",
    "        baseline_models = {'LR_HC*':'Exp6_ADNI1_MMSE_LR_L1_HC_2016-05-11-15-16-33.pkl',\n",
    "                           'LR_CT*':'Exp6_ADNI1_MMSE_LR_L1_CT_2016-05-11-15-17-04.pkl',\n",
    "                           'LR_HC_CT*':'Exp6_ADNI1_MMSE_LR_L1_HC_CT_2016-05-11-15-17-21.pkl',\n",
    "                           'SVR_HC*':'Exp6_ADNI1_MMSE_SVR_HC_2016-05-11-15-37-34.pkl',\n",
    "                           'SVR_CT*':'Exp6_ADNI1_MMSE_SVR_CT_2016-05-11-15-37-08.pkl',\n",
    "                           'SVR_HC_CT*':'Exp6_ADNI1_MMSE_SVR_HC_CT_2016-05-11-15-34-50.pkl', \n",
    "                           'RFR_HC':'Exp6_ADNI1_MMSE_RFR_HC_2016-05-11-15-38-55.pkl',\n",
    "                           'RFR_CT':'Exp6_ADNI1_MMSE_RFR_CT_2016-05-11-15-42-19.pkl',\n",
    "                           'RFR_HC_CT':'Exp6_ADNI1_MMSE_RFR_HC_CT_2016-05-11-15-44-15.pkl'}\n",
    "        \n",
    "    if cohort == 'ADNI2':\n",
    "        baseline_models = {'LR_HC*':'Exp6_ADNI2_MMSE_LR_L1_HC_2016-05-11-16-29-36.pkl',\n",
    "                           'LR_CT*':'Exp6_ADNI2_MMSE_LR_L1_CT_2016-05-11-16-30-11.pkl',\n",
    "                           'LR_HC_CT*':'Exp6_ADNI2_MMSE_LR_L1_HC_CT_2016-05-11-16-30-25.pkl',\n",
    "                           'SVR_HC*':'Exp6_ADNI2_MMSE_SVR_HC_2016-05-11-16-30-48.pkl',\n",
    "                           'SVR_CT*':'Exp6_ADNI2_MMSE_SVR_CT_2016-05-11-16-33-41.pkl',\n",
    "                           'SVR_HC_CT*':'Exp6_ADNI2_MMSE_SVR_HC_CT_2016-05-11-16-52-01.pkl', \n",
    "                           'RFR_HC':'Exp6_ADNI2_MMSE_RFR_HC_2016-05-11-16-54-51.pkl',\n",
    "                           'RFR_CT':'Exp6_ADNI2_MMSE_RFR_CT_2016-05-11-16-59-24.pkl',\n",
    "                           'RFR_HC_CT':'Exp6_ADNI2_MMSE_RFR_HC_CT_2016-05-11-18-29-58.pkl'}\n",
    "                \n",
    "    if cohort == 'ADNI1and2':\n",
    "        baseline_models = {'LR_HC*':'Exp6_ADNI1and2_MMSE_LR_L1_HC_2016-05-13-13-17-45.pkl',\n",
    "                           'LR_CT*':'Exp6_ADNI1and2_MMSE_LR_L1_CT_2016-05-13-13-18-17.pkl',\n",
    "                           'LR_HC_CT*':'Exp6_ADNI1and2_MMSE_LR_L1_HC_CT_2016-05-13-13-18-35.pkl',\n",
    "                           'SVR_HC*':'Exp6_ADNI1and2_MMSE_SVR_HC_2016-05-13-13-19-11.pkl',\n",
    "                           'SVR_CT*':'Exp6_ADNI1and2_MMSE_SVR_CT_2016-05-13-13-28-56.pkl',\n",
    "                           'SVR_HC_CT*':'Exp6_ADNI1and2_MMSE_SVR_HC_CT_2016-05-13-13-45-58.pkl', \n",
    "                           'RFR_HC':'Exp6_ADNI1and2_MMSE_RFR_HC_2016-05-13-13-47-54.pkl',\n",
    "                           'RFR_CT':'Exp6_ADNI1and2_MMSE_RFR_CT_2016-05-13-13-50-40.pkl',\n",
    "                           'RFR_HC_CT':'Exp6_ADNI1and2_MMSE_RFR_HC_CT_2016-05-13-13-56-10.pkl'}\n",
    "        \n",
    "        \n",
    "# Not plotted:\n",
    "# 'LR_L1': 'LR_L1_ADAS13_2015-11-06-12-25-21.pkl'\n",
    "# 'LR_L1_infl':'LR_L1_ADAS13_inflated_train_parallel_2015-11-27-15-05-17.pkl'\n",
    "r_means =  {}  \n",
    "r_medians =  {}\n",
    "r_sems =  {}\n",
    "rmse_means =  {}\n",
    "rmse_sems =  {}\n",
    "predicted_CV_scores = {}\n",
    "actual_CV_scores = {}\n",
    "keys_used = []\n",
    "for key,val in baseline_models.iteritems():\n",
    "    pkl_file = open(boxplots_dir + val, 'rb')\n",
    "    saved_data = pickle.load(pkl_file)\n",
    "    pkl_file.close()            \n",
    "    boxplot_config_r[key] = (zip(*saved_data['CV_r'])[0])\n",
    "    boxplot_config_MSE[key] = (saved_data['CV_MSE'])\n",
    "    print 'key: {}, mean: {}, median: {}, std: {}'.format(key,np.mean(zip(*saved_data['CV_r'])[0]),\n",
    "                                              np.median(zip(*saved_data['CV_r'])[0]),\n",
    "                                              np.std(zip(*saved_data['CV_r'])[0]))\n",
    "    r_means[key] = (np.mean(zip(*saved_data['CV_r'])[0]))\n",
    "    r_medians[key] = (np.median(zip(*saved_data['CV_r'])[0]))\n",
    "    r_sems[key] = (stats.sem(zip(*saved_data['CV_r'])[0]))        \n",
    "\n",
    "    rmse_means[key] = (np.mean(np.sqrt(saved_data['CV_MSE'])))\n",
    "    rmse_sems[key] = (stats.sem(np.sqrt(saved_data['CV_MSE'])))  \n",
    "    if key in ['LR_HC_CT*','SVR_HC_CT*','RFR_HC_CT']:\n",
    "        predicted_CV_scores[key] = (saved_data['predicted_CV_scores'])\n",
    "        actual_CV_scores[key] = (saved_data['actual_CV_scores'])\n",
    "        \n",
    "# Since ANN model perfs are saved differently :-/\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "if Clinical_Scale == 'ADAS':\n",
    "    if cohort == 'ADNI1':\n",
    "        if not multi_task:\n",
    "            ANN_models ={'CT':'Exp6_ADNI1_ADAS13_NN_CT_2016-07-26-19-27-19.pkl',\n",
    "                         'HC':'Exp6_ADNI1_ADAS13_NN_HC_2016-07-28-16-18-29.pkl',\n",
    "                         'HC_CT':'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-07-30-14-15-34.pkl'}\n",
    "        else:\n",
    "            ANN_models ={'HC':'Exp6_ADNI1_ADAS13_MMSE_NN_HC_2016-06-02-10-26-05.pkl', \n",
    "                         'CT':'Exp6_ADNI1_ADAS13_MMSE_NN_CT_2016-06-01-23-42-45.pkl',\n",
    "                         'HC_CT':'Exp6_ADNI1_ADAS13_MMSE_NN_HC_CT_2016-05-24-11-35-17.pkl'}\n",
    "\n",
    "    elif cohort == 'ADNI2':\n",
    "        if not multi_task:\n",
    "            ANN_models ={'CT':'Exp7_ADNI2_ADAS13_NN_CT_2016-04-25-15-40-42.pkl',\n",
    "                         'HC':'Exp7_ADNI2_ADAS13_NN_HC_2016-04-25-22-08-00.pkl',\n",
    "                         'HC_CT':'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-08-03-20-27-06.pkl'}\n",
    "        else:\n",
    "            ANN_models ={'HC':'Exp11_ADNI2_ADAS13_MMSE_NN_HC_2016-06-02-15-02-47.pkl',\n",
    "                         'CT':'Exp11_ADNI2_ADAS13_MMSE_NN_CT_2016-06-02-18-01-52.pkl',\n",
    "                         'HC_CT':'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-18-19-36.pkl'}\n",
    "\n",
    "    elif cohort == 'ADNI1and2':\n",
    "        if not multi_task:\n",
    "            ANN_models ={'CT':'Exp8_ADNI1and2_ADAS13_NN_CT_2016-04-26-13-22-48.pkl',\n",
    "                         'HC':'Exp8_ADNI1and2_ADAS13_NN_HC_2016-04-26-16-53-38.pkl',\n",
    "                         'HC_CT':'Exp8_ADNI1and2_ADAS13_NN_HC_CT_2016-05-08-12-06-36.pkl'}\n",
    "        else:\n",
    "            ANN_models ={'HC':'Exp14_ADNI1and2_ADAS13_MMSE_NN_HC_2016-06-03-14-04-38.pkl',\n",
    "                         'CT':'Exp14_ADNI1and2_ADAS13_MMSE_NN_CT_2016-06-02-23-24-00.pkl',\n",
    "                         'HC_CT':'Exp14_ADNI1and2_ADAS13_MMSE_NN_HC_CT_2016-05-21-11-24-21.pkl'}\n",
    "        \n",
    "    else:\n",
    "        print \"Wrong cohort\"\n",
    "        \n",
    "elif Clinical_Scale == 'MMSE':    \n",
    "    if cohort == 'ADNI1':    \n",
    "        if not multi_task:\n",
    "            ANN_models ={'HC':'Exp6_ADNI1_MMSE_NN_HC_2016-05-10-23-14-55.pkl',\n",
    "                         'CT':'Exp6_ADNI1_MMSE_NN_CT_2016-05-10-21-28-16.pkl',\n",
    "                         'HC_CT':'Exp6_ADNI1_MMSE_NN_HC_CT_2016-05-11-10-21-35.pkl'}\n",
    "        else:\n",
    "            ANN_models ={'HC':'Exp6_ADNI1_ADAS13_MMSE_NN_HC_2016-06-02-10-26-05.pkl', \n",
    "                         'CT':'Exp6_ADNI1_ADAS13_MMSE_NN_CT_2016-06-01-23-42-45.pkl',\n",
    "                         'HC_CT':'Exp6_ADNI1_ADAS13_MMSE_NN_HC_CT_2016-05-24-11-35-17.pkl'}\n",
    "\n",
    "    elif cohort == 'ADNI2':\n",
    "        if not multi_task:\n",
    "            ANN_models ={'CT':'Exp11_ADNI2_MMSE_NN_CT_2016-05-09-16-07-42.pkl',\n",
    "                         'HC':'Exp11_ADNI2_MMSE_NN_HC_2016-05-09-18-20-08.pkl',\n",
    "                         'HC_CT':'Exp11_ADNI2_MMSE_NN_HC_CT_2016-05-14-23-15-06.pkl'}\n",
    "        else:\n",
    "            ANN_models ={'HC':'Exp11_ADNI2_ADAS13_MMSE_NN_HC_2016-06-02-15-02-47.pkl',\n",
    "                         'CT':'Exp11_ADNI2_ADAS13_MMSE_NN_CT_2016-06-02-18-01-52.pkl',\n",
    "                         'HC_CT':'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-18-19-36.pkl'}\n",
    "\n",
    "    elif cohort == 'ADNI1and2':\n",
    "        if not multi_task:\n",
    "            ANN_models ={'CT':'Exp13_ADNI1and2_MMSE_NN_CT_2016-05-13-19-59-43.pkl',\n",
    "                         'HC':'Exp13_ADNI1and2_MMSE_NN_HC_2016-05-12-22-59-44.pkl',\n",
    "                         'HC_CT':'Exp13_ADNI1and2_MMSE_NN_HC_CT_2016-05-16-10-49-37.pkl'}        \n",
    "        else:\n",
    "            ANN_models ={'HC':'Exp14_ADNI1and2_ADAS13_MMSE_NN_HC_2016-06-03-14-04-38.pkl',\n",
    "                         'CT':'Exp14_ADNI1and2_ADAS13_MMSE_NN_CT_2016-06-02-23-24-00.pkl',\n",
    "                         'HC_CT':'Exp14_ADNI1and2_ADAS13_MMSE_NN_HC_CT_2016-05-21-11-24-21.pkl'}\n",
    "    else:\n",
    "        print \"Wrong cohort\"\n",
    "\n",
    "        \n",
    "for key,val in ANN_models.iteritems():\n",
    "    pkl_file = open(boxplots_dir + val, 'rb')\n",
    "    saved_data_raw = pickle.load(pkl_file)\n",
    "    if multi_task:\n",
    "        saved_data = saved_data_raw['opt_{}'.format(Clinical_Scale)]        \n",
    "        boxplot_config_r['ANN_'+key] = tuple(saved_data['CV_r'].values())\n",
    "        boxplot_config_MSE['ANN_'+key] = tuple(saved_data['CV_MSE'].values())\n",
    "        pkl_file.close()\n",
    "        print 'key: {}, mean: {}, median: {}, std: {}'.format(key,np.mean(saved_data['CV_r'].values()),\n",
    "                                                  np.median(saved_data['CV_r'].values()),\n",
    "                                                  np.std(saved_data['CV_r'].values()))\n",
    "        r_means['ANN_'+key] = (np.mean(saved_data['CV_r'].values()))\n",
    "        r_sems['ANN_'+key] = (stats.sem(saved_data['CV_r'].values()))\n",
    "\n",
    "        rmse_means['ANN_'+key] = (np.mean(np.sqrt(saved_data['CV_MSE'].values())))\n",
    "        rmse_sems['ANN_'+key] = (stats.sem(np.sqrt(saved_data['CV_MSE'].values())))\n",
    "        if key == 'HC_CT':\n",
    "            predicted_CV_scores['ANN_' + key] = (saved_data['predicted_CV_scores'].values())\n",
    "            actual_CV_scores['ANN_' + key] = (saved_data['actual_CV_scores'].values())\n",
    "            \n",
    "    else:\n",
    "        saved_data = saved_data_raw\n",
    "        \n",
    "        boxplot_config_r['ANN_'+key] = (saved_data['CV_r'])\n",
    "        boxplot_config_MSE['ANN_'+key] = (saved_data['CV_MSE'])\n",
    "        pkl_file.close()\n",
    "        print 'key: {}, file: {}, mean: {}, median: {}, std: {}'.format(key,pkl_file,np.mean(saved_data['CV_r']),\n",
    "                                                  np.median(saved_data['CV_r']),\n",
    "                                                  np.std(saved_data['CV_r']))\n",
    "        r_means['ANN_'+key] = (np.mean(saved_data['CV_r']))\n",
    "        r_sems['ANN_'+key] = (stats.sem(saved_data['CV_r']))\n",
    "\n",
    "        rmse_means['ANN_'+key] = (np.mean(np.sqrt(saved_data['CV_MSE'])))\n",
    "        rmse_sems['ANN_'+key] = (stats.sem(np.sqrt(saved_data['CV_MSE'])))\n",
    "        if key == 'HC_CT':\n",
    "            predicted_CV_scores['ANN_' + key] = (saved_data['predicted_CV_scores'])\n",
    "            actual_CV_scores['ANN_' + key] = (saved_data['actual_CV_scores'])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot foldwise results\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "fids = np.arange(1,11,1)\n",
    "for key in ['ANN_CT','ANN_HC','ANN_HC_CT','LR_CT*','SVR_HC_CT*','RFR_HC_CT']: #boxplot_config_r.keys():\n",
    "    if key in ['ANN_CT','ANN_HC','ANN_HC_CT']:\n",
    "        custom_marker = 'd'    \n",
    "        print 'modality: {}, perf: {}'.format(key, boxplot_config_r[key])\n",
    "    else:\n",
    "        custom_marker = 'o'        \n",
    "    \n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(fids,boxplot_config_r[key],linestyle='None',marker=custom_marker,markersize=15,label=key)\n",
    "    plt.legend(bbox_to_anchor=[0.5, -0.1], loc='center', ncol=6)    \n",
    "    plt.xlim(0,11)\n",
    "    plt.title('{}_{}'.format(cohort, Clinical_Scale))\n",
    "    \n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(fids,boxplot_config_MSE[key],linestyle='None',marker=custom_marker,markersize=15,label=key)\n",
    "    plt.legend(bbox_to_anchor=[0.5, -0.1], loc='center', ncol=6)\n",
    "    plt.xlim(0,11)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Boxplots for CV statistics (r, mse, R2)\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "stat_measure_list = [boxplot_config_r,boxplot_config_MSE] #boxplot_config_MSE\n",
    "stat_measure_names = ['Pearson r', 'RMSE'] #'MSE'\n",
    "from matplotlib.artist import setp\n",
    "font_tiny = 20\n",
    "font_small = 25\n",
    "font_med = 30\n",
    "font_large = 40\n",
    "plt.rcParams['figure.figsize'] = (30, 40)\n",
    "\n",
    "plot_score_diff = False #see trend of error w.r.t. actual scores\n",
    "#plt.xkcd()\n",
    "\n",
    "# custom_cols = ['LR_HC*','LR_CT*','LR_HC_CT*','SVR_HC*','SVR_CT*','SVR_HC_CT*','RFR_HC','RFR_CT','RFR_HC_CT',\n",
    "#                'ANN_HC','ANN_CT','ANN_HC_CT']\n",
    "custom_cols = ['LR_HC*','LR_CT*','LR_HC_CT*','SVR_HC*','SVR_CT*','SVR_HC_CT*','RFR_HC','RFR_CT','RFR_HC_CT'\n",
    "               ,'ANN_HC','ANN_CT','ANN_HC_CT']\n",
    "\n",
    "grp_idx = 3\n",
    "boxplot = False\n",
    "errorbarplot = True\n",
    "scatterplot = True\n",
    "\n",
    "my_colors = ['steelblue', 'olivedrab', 'cadetblue', 'salmon']\n",
    "#plt.figure()\n",
    "ax1 = plt.subplot2grid((6,2), (0,0), colspan=2,rowspan=2)\n",
    "ax2 = plt.subplot2grid((6,2), (2,0), colspan=2,rowspan=2)\n",
    "ax3 = plt.subplot2grid((6,2), (4,0))\n",
    "ax4 = plt.subplot2grid((6,2), (4,1))\n",
    "ax5 = plt.subplot2grid((6,2), (5,0))\n",
    "ax6 = plt.subplot2grid((6,2), (5,1))\n",
    "\n",
    "ax_list = [ax1,ax2,ax3,ax4,ax5,ax6]\n",
    "\n",
    "if errorbarplot:\n",
    "    custom_r_means = []    \n",
    "    custom_r_sems = []    \n",
    "    custom_rmse_means = []\n",
    "    custom_rmse_sems = []\n",
    "    for c, col in enumerate(custom_cols):\n",
    "        custom_r_means.append(r_means[col])        \n",
    "        custom_r_sems.append(r_sems[col])\n",
    "        custom_rmse_means.append(rmse_means[col])        \n",
    "        custom_rmse_sems.append(rmse_sems[col])\n",
    "        \n",
    "    if grp_idx == 3:\n",
    "        x_r = np.array([0,1,2,4,5,6,8,9,10,12,13,14])  # padding to better appearance \n",
    "    else:\n",
    "        x_r = np.array([0,1,2,3])  # padding to better appearance \n",
    "    \n",
    "    x_mse = x_r \n",
    "    #plt.bar(x, np.array(custom_r_means), yerr=np.array(custom_r_sems))\n",
    "    width = 0.25                    # bar width\n",
    "\n",
    "    #fig, ax = plt.subplots()\n",
    "    plot_corr = True\n",
    "    if plot_corr: \n",
    "\n",
    "        rects1 = ax1.bar(x_r[:grp_idx], custom_r_means[:grp_idx],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[0],        # bar colour\n",
    "                        yerr=custom_r_sems[:grp_idx],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4},       # error-bar width\n",
    "                        label = 'Linear Regression')\n",
    "\n",
    "        rects2 = ax1.bar(x_r[grp_idx:2*grp_idx], custom_r_means[grp_idx:2*grp_idx],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[1],        # bar colour\n",
    "                        yerr=custom_r_sems[grp_idx:2*grp_idx],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4},\n",
    "                        label = 'Support Vector Regression')\n",
    "\n",
    "        rects3 = ax1.bar(x_r[2*grp_idx:3*grp_idx], custom_r_means[2*grp_idx:3*grp_idx],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[2],        # bar colour\n",
    "                        yerr=custom_r_sems[2*grp_idx:3*grp_idx],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4},\n",
    "                         label = 'Random Forest')\n",
    "\n",
    "        rects4 = ax1.bar(x_r[3*grp_idx:], custom_r_means[3*grp_idx:],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[3],        # bar colour\n",
    "                        yerr=custom_r_sems[3*grp_idx],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4},\n",
    "                        label = 'Artificial Neural Network')\n",
    "        autolabel(rects1,ax1)\n",
    "        autolabel(rects2,ax1)\n",
    "        autolabel(rects3,ax1)\n",
    "        autolabel(rects4,ax1)\n",
    "        #Plot annotations \n",
    "        ax1.set_ylabel('Correlation',fontsize=font_large)\n",
    "        ax1.set_xlim(-0.5,max(x_r)+1)\n",
    "        ax1.set_ylim(0,1)\n",
    "        ax1.set_xticks(x_r + .1)\n",
    "        ax1.set_xticklabels(custom_cols,rotation=10)\n",
    "        ax1.tick_params(labelsize=font_small)\n",
    "        #ax1.set_yticks(fontsize=font_med)\n",
    "        ax1.set_title('{}_{}'.format(cohort, Clinical_Scale),fontsize=font_med)\n",
    "        ax1.legend(fontsize=font_med, loc=2)\n",
    "    \n",
    "    plot_mse = True\n",
    "    if plot_mse: \n",
    "        rects5 = ax2.bar(x_mse[:grp_idx], custom_rmse_means[:grp_idx],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[0],        # bar colour\n",
    "                        yerr=custom_rmse_sems[:grp_idx],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4})\n",
    "\n",
    "        rects6 = ax2.bar(x_mse[grp_idx:2*grp_idx], custom_rmse_means[grp_idx:2*grp_idx],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[1],        # bar colour\n",
    "                        yerr=custom_rmse_sems[grp_idx:2*grp_idx],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4})\n",
    "\n",
    "        rects7 = ax2.bar(x_mse[2*grp_idx:3*grp_idx], custom_rmse_means[2*grp_idx:3*grp_idx],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[2],        # bar colour\n",
    "                        yerr=custom_rmse_sems[2*grp_idx:3*grp_idx],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4})\n",
    "\n",
    "        rects8 = ax2.bar(x_mse[3*grp_idx:], custom_rmse_means[3*grp_idx:],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[3],        # bar colour\n",
    "                        yerr=custom_rmse_sems[3*grp_idx:],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4})\n",
    "        autolabel(rects5,ax2)\n",
    "        autolabel(rects6,ax2)\n",
    "        autolabel(rects7,ax2)\n",
    "        autolabel(rects8,ax2)\n",
    "\n",
    "        #Plot annotations \n",
    "        ax2.set_ylabel('RMSE',fontsize=font_large)\n",
    "        ax2.set_xlim(-0.5,max(x_r)+1)\n",
    "        if Clinical_Scale == 'ADAS':\n",
    "            ax2.set_ylim(0,12)\n",
    "        elif Clinical_Scale == 'MMSE':\n",
    "            ax2.set_ylim(0,4)\n",
    "        else:\n",
    "            print 'unknown clinical scale'\n",
    "        \n",
    "        ax2.set_xticks(x_r + 0.1)\n",
    "        ax2.set_xticklabels(custom_cols,rotation=10)\n",
    "        ax2.tick_params(labelsize=font_small)\n",
    "        #ax1.set_yticks(fontsize=font_med)\n",
    "        ax2.legend(fontsize=font_med, loc=2)\n",
    "    \n",
    "if scatterplot:\n",
    "    #plt.figure()\n",
    "    #fid = 4\n",
    "    for k, key in enumerate(['LR_HC_CT*','SVR_HC_CT*', 'RFR_HC_CT', 'ANN_HC_CT']):\n",
    "        x = np.squeeze(np.hstack(actual_CV_scores[key]))\n",
    "        y = np.squeeze(np.hstack(predicted_CV_scores[key]))\n",
    "        \n",
    "        if plot_score_diff:\n",
    "            y = (y-x)\n",
    "            ax_list[k+ax_offset].set_ylabel('(Predicted - Actual) Score',fontsize=font_small)            \n",
    "            label_str = 'Clinical Scale: {}'.format(Clinical_Scale) + '\\n' + 'cohort: {}'.format(cohort)                   \n",
    "            \n",
    "        fit = np.polyfit(x,y,1)\n",
    "        fit_fn = np.poly1d(fit) \n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "        rmse_value = np.sqrt(mse(x,y))\n",
    "        if p_value < 0.0001:\n",
    "            p_value_sig = '<0.0001'\n",
    "        else:\n",
    "            p_value_sig = str(p_value)\n",
    "            \n",
    "        #Plot annotations  \n",
    "        if not plot_score_diff:\n",
    "            label_str = 'model: {}'.format(key) + '\\n' + 'r: {:04.2f}'.format(r_value) + '\\n' + 'rmse: {:04.2f}'.format(rmse_value) \n",
    "            if Clinical_Scale == 'ADAS':                \n",
    "                ax_list[k+ax_offset].set_ylim(0,40)\n",
    "            elif Clinical_Scale == 'MMSE':                \n",
    "                ax_list[k+ax_offset].set_ylim(20,35)\n",
    "            else:\n",
    "                print 'unknown clinical scale'\n",
    "            ax_list[k+ax_offset].set_ylabel('Predicted Score',fontsize=font_small)            \n",
    "            \n",
    "        # fit_fn is now a function which takes in x and returns an estimate for y                \n",
    "        ax_list[k+ax_offset].plot(x, fit_fn(x),linewidth=3, c=my_colors[k], label=label_str)\n",
    "        #plt.title(model_choice,fontsize=font_large)                        \n",
    "        ax_offset = 2\n",
    "        #plt.subplot(2,2,k+1)        \n",
    "        ax_list[k+ax_offset].scatter(x, y, c=my_colors[k], s=40)                \n",
    "        ax_list[k+ax_offset].set_xlabel('Actual Score',fontsize=font_small)        \n",
    "        ax_list[k+ax_offset].legend(fontsize=font_tiny,loc=2)\n",
    "        ax_list[k+ax_offset].tick_params(labelsize=font_med)\n",
    "\n",
    "#Save figure\n",
    "save_fig = True\n",
    "if save_fig:\n",
    "    exp_name = 'Exp6'\n",
    "    save_name = '{}{}_{}_{}_PerfPlots.tif'.format(boxplots_dir, exp_name, cohort, Clinical_Scale)\n",
    "    print 'Save image path: ' + save_name\n",
    "    box_fig = plt.gcf()\n",
    "    box_fig.savefig(save_name,format='TIFF', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some Defs\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()\n",
    "\n",
    "#Outer Fold Computation (need the imports inside def if you want to parallelize!)\n",
    "def computeOuterFold(train_X, train_y, valid_X, valid_y, model_clf, hyper_params, inner_loop, save_model, save_model_path):\n",
    "    import collections\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn import grid_search\n",
    "    import datetime\n",
    "    import time\n",
    "    import collections\n",
    "    from scipy.stats import mode\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from scipy import stats\n",
    "    \n",
    "    print 'Starting Outerfold computation'\n",
    "    \n",
    "    hp_dict = collections.defaultdict(list) #store best hyper-parameters for each fold\n",
    "    \n",
    "    if inner_loop:     \n",
    "        print 'Starting InnerFold computation'\n",
    "        save_model_path_fold = save_model_path + '_fold_' \n",
    "        clf = innerCVLoop(model_clf,hyper_params,train_X, train_y,save_model,save_model_path_fold)\n",
    "        for hp in hyper_params:\n",
    "            hp_dict[hp].append(clf.best_estimator_.get_params()[hp])\n",
    "            \n",
    "        print 'Ending InnerFold computation'\n",
    "\n",
    "    else:\n",
    "        clf = model_clf\n",
    "        clf.fit(fold_X,fold_y)\n",
    "        \n",
    "    #CV_scores    \n",
    "    r_train = stats.pearsonr(clf.predict(train_X),train_y)\n",
    "    r_valid = stats.pearsonr(clf.predict(valid_X),valid_y)\n",
    "        \n",
    "    R2_train = clf.score(train_X,train_y) \n",
    "    R2_valid = clf.score(valid_X,valid_y)\n",
    "        \n",
    "    MSE_train = mse(clf.predict(train_X),train_y)\n",
    "    MSE_valid = mse(clf.predict(valid_X),valid_y)\n",
    "    \n",
    "    print 'Ending OuterFold computation'\n",
    "    \n",
    "    return {'r_train':r_train, 'r_valid':r_valid, 'R2_train':R2_train, 'R2_valid':R2_valid,\n",
    "            'MSE_train':MSE_train, 'MSE_valid':MSE_valid, 'hp_dict':hp_dict, \n",
    "            'predicted_fold_score': clf.predict(valid_X), 'actual_fold_scores':valid_y}\n",
    "\n",
    "#Inner Fold Computation (need the imports inside def if you want to parallelize!)\n",
    "def innerCVLoop(model_clf,hyper_params,fold_X, fold_y,save_model,save_model_path):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn import grid_search\n",
    "    clf = grid_search.GridSearchCV(model_clf, hyper_params,cv=3,verbose=0)\n",
    "    clf.fit(fold_X, fold_y)\n",
    "    #Save classifier\n",
    "    if save_model:\n",
    "        save_model(clf,save_model_path)\n",
    "        \n",
    "    return clf\n",
    "\n",
    "def autolabel(rects,_ax):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        _ax.text(rect.get_x() + rect.get_width()/2., 1.1*height,\n",
    "                '{:03.2f}'.format(height),\n",
    "                ha='center',            # vertical alignment\n",
    "                va='bottom',             # horizontal alignment\n",
    "                fontsize = 35)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "matches = [x for x in y_dx if x == 'No_DX']\n",
    "print matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CS_vals = sub_CS_dict_clean.values()\n",
    "plt.hist(CS_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohort = 'ADNI2'\n",
    "exp_setup_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/CV_Exp5_{}_ADAS13.pkl'.format(cohort)\n",
    "exp_setup = pickle.load( open(exp_setup_path, \"rb\" ) )\n",
    "kf = exp_setup['kf']\n",
    "adni2_sub_list = exp_setup['common_subs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_keys = list(set(adni1_sub_list) & set(adni2_sub_list))\n",
    "print len(adni1_sub_list), len(adni2_sub_list)\n",
    "print common_keys\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/output/'\n",
    "adni1_RFR_file = 'Exp4_ADNI1_ADAS13_RFR_HC_CT_2016-02-29-15-24-23.pkl'\n",
    "adni2_RFR_file = 'Exp5_ADNI2_ADAS13_RFR_HC_CT_2016-04-04-16-18-06.pkl'\n",
    "pkl_file = open(boxplots_dir + adni1_RFR_file, 'rb')\n",
    "saved_data = pickle.load(pkl_file)\n",
    "pkl_file.close()            \n",
    "\n",
    "adni1_act_scores = np.hstack(saved_data['actual_CV_scores'])\n",
    "adni1_prd_scores = np.hstack(saved_data['predicted_CV_scores'])\n",
    "\n",
    "pkl_file = open(boxplots_dir + adni2_RFR_file, 'rb')\n",
    "saved_data = pickle.load(pkl_file)\n",
    "pkl_file.close()            \n",
    "\n",
    "adni2_act_scores = np.hstack(saved_data['actual_CV_scores'])\n",
    "adni2_prd_scores = np.hstack(saved_data['predicted_CV_scores'])\n",
    "\n",
    "# print np.hstack(adni1_act_scores)\n",
    "plt.subplot(2,1,1)\n",
    "plt.scatter(adni1_act_scores,adni1_prd_scores,c='olivedrab')\n",
    "plt.title('ADNI1 Perf')\n",
    "plt.xlabel('Actual ADAS scores')\n",
    "plt.ylabel('Predicted ADAS scores')\n",
    "plt.subplot(2,1,2)\n",
    "plt.scatter(adni2_act_scores,adni2_prd_scores,c='olivedrab')\n",
    "plt.xlabel('Actual ADAS scores')\n",
    "plt.ylabel('Predicted ADAS scores')\n",
    "plt.title('ADNI2 Perf')\n",
    "\n",
    "box_fig = plt.gcf()\n",
    "box_fig.savefig('/projects/nikhil/ADNI_prediction/input_datasets/CS/adas_correlations.png', bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "node_sizes1 = {'L_ff1':50,'R_ff1':50,'L_ff2':50,'R_ff2':50,'ff1':100,'ff2':100,'ff3':20}\n",
    "node_sizes2 = {'L_ff1':50,'R_ff1':50,'L_ff2':50,'R_ff2':50,'ff1':100,'ff2':100,'ff3':40}\n",
    "dr1 = {'HC':0.25,'CT':0.25}\n",
    "dr2 = {'HC':0.5,'CT':0.5}\n",
    "\n",
    "list1 = ['node_sizes1','node_sizes2']\n",
    "list2 = ['dr1','dr2']\n",
    "\n",
    "s=[list1,list2 ]\n",
    "hype_combs = list(itertools.product(*s))\n",
    "hype_configs = {}\n",
    "for c, comb in enumerate(hype_combs):\n",
    "    hype_configs['hyp{}'.format(c)] = {'node_sizes':comb}\n",
    "    \n",
    "    hype_configs = {'hyp1':{'node_sizes':{'L_ff1':50,'R_ff1':50,'L_ff2':50,'R_ff2':50,'ff1':100,'ff2':100,'ff3':20},\n",
    "                       'dr':{'HC':0.25,'CT':0.25}},\n",
    "                'hyp2':{'node_sizes':{'L_ff1':50,'R_ff1':50,'L_ff2':50,'R_ff2':50,'ff1':100,'ff2':100,'ff3':40},\n",
    "                       'dr':{'HC':0.5,'CT':0.5}}\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.title('HC_L total volume')\n",
    "plt.hist(X[y_dx_int==0,0],bins=100,alpha=alpha,label='AD: {}'.format(np.mean(X[y_dx_int==0,0])))\n",
    "plt.hist(X[y_dx_int==1,0],bins=100,alpha=alpha,label='CN: {}'.format(np.mean(X[y_dx_int==1,0])))\n",
    "plt.hist(X[y_dx_int==2,0],bins=100,alpha=alpha,label='MCI: {}'.format(np.mean(X[y_dx_int==2,0])))\n",
    "plt.legend()\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('HC_R total volume')\n",
    "plt.hist(X[y_dx_int==0,1],bins=100,alpha=alpha,label='AD: {}'.format(np.mean(X[y_dx_int==0,1])))\n",
    "plt.hist(X[y_dx_int==1,1],bins=100,alpha=alpha,label='CN: {}'.format(np.mean(X[y_dx_int==1,1])))\n",
    "plt.hist(X[y_dx_int==2,1],bins=100,alpha=alpha,label='MCI: {}'.format(np.mean(X[y_dx_int==2,1])))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load one of the data sets that come with seaborn\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "sns.jointplot(\"total_bill\", \"tip\", tips, kind='reg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.ones(10)\n",
    "y = 2*x\n",
    "print x\n",
    "sns.jointplot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        ADAS        MMSE\n",
    "ADNI1   15793       15793  (Exp6)\n",
    "ADNI2   17358/17240 17240  (Exp7/Exp11)  \n",
    "ADNI1_2 33152       33035  (Exp8/Exp13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HC_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_2016-06-02-15-02-47.pkl'\n",
    "CT_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_CT_2016-06-02-18-01-52.pkl'\n",
    "pkl_file = open(boxplots_dir + CT_file, 'rb')\n",
    "saved_data_raw = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saved_data_raw['opt_hype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CT_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_CT_2016-06-02-18-01-52.pkl'\n",
    "pkl_file = open(boxplots_dir + CT_file, 'rb')\n",
    "saved_data = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saved_data['opt_hype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py as h5\n",
    "import tables as tb\n",
    "import numpy as np\n",
    "fid = 4\n",
    "subset = 'valid'\n",
    "hdf_file = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp11_ADNI2_ADAS13_NN_{}.h5'.format(subset)\n",
    "dataset = 'Fold_{}_{}_dx'.format(fid,subset)\n",
    "\n",
    "save_to_file = False\n",
    "dummy_vars = np.array(['AD', 'LMCI', 'EMCI', 'SMC', 'CN'])\n",
    "dummy_var_array = create_dummy_vars(hdf_file, dataset, dummy_vars, save_to_file)\n",
    "\n",
    "print 'output_vector'\n",
    "print dummy_var_array\n",
    "print np.sum(dummy_var_array==0)/float(len(dummy_var_array)), np.sum(dummy_var_array==1)/float(len(dummy_var_array)), np.sum(dummy_var_array==2)/float(len(dummy_var_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dummy_vars(input_file, dataset, dummy_vars,save_to_file):        \n",
    "    #takes a categorical output variable and changes to binary vectorized form\n",
    "    #dummy_vars lists all the categories \n",
    "    dummy_var_match_list = []                    \n",
    "    categorical_data = load_data(input_file, dataset, 'no_preproc')         \n",
    "    for cat_data_item in categorical_data:        \n",
    "        match_list = dummy_vars == cat_data_item                \n",
    "        if np.sum(match_list) == 1:\n",
    "            match_list = np.argmax(match_list)\n",
    "            dummy_var_match_list.append(match_list)        \n",
    "        else:\n",
    "            print 'no match found for {}'.format(cat_data_item)\n",
    "            \n",
    "    \n",
    "    output_vector = np.array(dummy_var_match_list)  \n",
    "    #print output_vector\n",
    "    #collapse LMCI-->EMCI and smcs-->CN\n",
    "    output_vector[output_vector==2] = 1\n",
    "    output_vector[output_vector==3] = 2\n",
    "    output_vector[output_vector==4] = 2\n",
    "    \n",
    "    if save_to_file:\n",
    "        print 'Adding vectorized dataset to: {}'.format(input_file)\n",
    "        \n",
    "#         hdf_data = h5.File(input_file, mode='a')            \n",
    "#         hdf_data.create_dataset(dataset + '_vectorized', data=output_vector)            \n",
    "#         hdf_data.close()   \n",
    "        \n",
    "        # use table to save data\n",
    "        FILTERS = tb.Filters(complevel=5,complib='zlib')    \n",
    "        fileh = tb.open_file(input_file, mode='a', filters=FILTERS)        \n",
    "        data  = fileh.create_array(fileh.root, dataset + '_cat3', output_vector)\n",
    "        fileh.close()\n",
    "        \n",
    "    return output_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf_data = h5.File(hdf_file) \n",
    "hdf_data.close() \n",
    "FILTERS = tb.Filters(complevel=5,complib='zlib')    \n",
    "fileh = tb.open_file(hdf_file, mode='r', filters=FILTERS)\n",
    "fileh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "\n",
    "fid = 4\n",
    "cm = plt.get_cmap('viridis') \n",
    "adas = load_data(hdf_file, 'Fold_{}_{}_y'.format(fid,subset), 'no_preproc')\n",
    "mmse = load_data(hdf_file, 'Fold_{}_y3'.format(fid), 'no_preproc')\n",
    "\n",
    "plt.scatter(adas,mmse,c=dummy_var_array,s=50,cmap=cm)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "256*20/4311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array(range(10))\n",
    "a[a==2]=1\n",
    "a[a==4]=2\n",
    "\n",
    "print a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
