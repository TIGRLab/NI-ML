{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "import tables as tb\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "naming = 'Exp5'\n",
    "side = 'l'\n",
    "#HDF file generate from Jon's scripts comprising matched candidate labels and filenames.\n",
    "if naming == 'Exp4':\n",
    "    #PTID_idx = 0 #Filename has only one PTID\n",
    "    input_file = baseline_dir + 'HC_preproc/combined.h5'\n",
    "\n",
    "    # Run this script for each split, side combination:\n",
    "    split = 'test'\n",
    "\n",
    "    input_HC = load_data(input_file,'{}_{}_data'.format(side,split))\n",
    "    #input_classes = input_data['{}_{}_classes'.format(side, split)][:]\n",
    "    input_files = load_data(input_file,'{}_{}_files'.format(side,split))    \n",
    "    \n",
    "if naming == 'Exp5':\n",
    "    PTID_idx = 1 #Filename has two PTIDs\n",
    "    if side == 'l':\n",
    "        #input_file = '/projects/jp/NI-ML/preprocessing/adni2/04_mask/candidate_left.h5'\n",
    "        input_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC_preproc/adni1_and_2/candidate_l_factor=1.0_thresh=300_data.h5'\n",
    "    else:\n",
    "        #input_file = '/projects/jp/NI-ML/preprocessing/adni2/04_mask/candidate_right.h5'\n",
    "        input_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC_preproc/adni1_and_2/candidate_r_factor=1.0_thresh=300_data.h5'\n",
    "    \n",
    "    input_HC = load_data(input_file,'data')\n",
    "    #input_classes = input_data['{}_{}_classes'.format(side, split)][:]\n",
    "    input_files = load_data(input_file,'files')    \n",
    "    \n",
    "id_participant = re.compile(r\"\"\"\n",
    " (?<=ADNI_)      # Match the first string after ADNI_\n",
    " (.*?)          # Lazy quantifier so it only grabs the first immediate match.\n",
    " (?=_MR)        # End at the _MR\n",
    "\"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# T100_L = 74865./12756\n",
    "# T100_R = 71631./13610\n",
    "# T300_L = 74681./11076\n",
    "# T300_R = 79392./10161\n",
    "# T500_L = 62600./9732\n",
    "# T500_R = 66583./8953\n",
    "# T750_L = 50276./8713\n",
    "# T750_R = 55167./8039\n",
    "\n",
    "# print ' T100_L:{} T100_R:{} \\n T300_L:{} T300_R:{} \\n T500_L:{} T500_R:{} \\n T750_L:{} T750_R:{}'.format(T100_L,T100_R,T300_L,T300_R,T500_L,T500_R,T750_L,T750_R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#change HC labels to (-1,1) from (0,1)\n",
    "input_HC = 2*input_HC - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print input_HC.shape\n",
    "print input_files[:4]\n",
    "f= input_files[0]\n",
    "subject_id = re.findall(id_participant, f)\n",
    "print subject_id\n",
    "print input_HC[:3,5000:5100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary length: 681\n",
      "739\n"
     ]
    }
   ],
   "source": [
    "# Generate dictionary {subject_id:[candidate_labels]}\n",
    "import collections\n",
    "import scipy.stats as stats\n",
    "subject_idx=[]\n",
    "unique_subject_idx = []\n",
    "subject_vol_dict = collections.OrderedDict()\n",
    "subject_class_dict = collections.OrderedDict()\n",
    "ADNI1_PTID = 0\n",
    "ADNI2_PTID = 1\n",
    "#find volume indices for each unique subject\n",
    "seen = set([])\n",
    "ADNI1 = False\n",
    "ADNI2 = True\n",
    "for j, f in enumerate(input_files):\n",
    "    #subject_id = re.search(id_participant, f).group(0) \n",
    "    \n",
    "    subject_id_list = re.findall(id_participant, f) #ADNI 2 has two sub_id matches - pick the second one [0,1]. \n",
    "    subject_id = ''\n",
    "    if ADNI1 and len(subject_id_list) == 1:\n",
    "        subject_id = subject_id_list[ADNI1_PTID] \n",
    "    elif ADNI2 and len(subject_id_list) == 2: \n",
    "        subject_id = subject_id_list[ADNI2_PTID]\n",
    "    else:\n",
    "        subject_id = ''\n",
    "        \n",
    "    if not subject_id == '':\n",
    "        subject_idx.append(subject_id)\n",
    "        if (subject_id not in subject_vol_dict) and not (np.sum(input_HC[j]) == 0):\n",
    "            subject_vol_dict[subject_id]=[]\n",
    "            #subject_class_dict[subject_id]=[]\n",
    "\n",
    "        # check if the candidate label is not empty\n",
    "        if not np.sum(input_HC[j]) == 0:\n",
    "        \n",
    "            subject_vol_dict[subject_id].append(input_HC[j])\n",
    "            #subject_class_dict[subject_id].append(input_classes[j])\n",
    "\n",
    "        if subject_id not in seen:\n",
    "            unique_subject_idx.append(subject_id)\n",
    "            seen.add(subject_id)\n",
    "\n",
    "print \"Dictionary length: \" + str(len(subject_vol_dict))\n",
    "    \n",
    "print len(unique_subject_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save Dictionaries\n",
    "exp_name = 'Exp5'\n",
    "cohort = 'ADNI2'\n",
    "side = 'left'\n",
    "\n",
    "sub_HC_vol_file = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_{}_{}.pkl'.format(cohort,side,exp_name)\n",
    "\n",
    "import pickle\n",
    "f = open(sub_HC_vol_file, 'wb')\n",
    "pickle.dump(subject_vol_dict, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tmp = subject_vol_dict.values()\n",
    "matches = [x for x in tmp if x == []]\n",
    "print len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize HC\n",
    "import random\n",
    "plt.rcParams['figure.figsize'] = (20, 10)\n",
    "input_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC_preproc/adni1_and_2/candidate_l_factor=1.0_thresh=300_data.h5'\n",
    "datamask =  load_data(input_file.format(side),'datamask')\n",
    "volmask = load_data(input_file.format(side),'volmask')\n",
    "baseline_shape = volmask.shape\n",
    "features = load_data(input_file.format(side),'data')\n",
    "\n",
    "sampx = 20\n",
    "X = np.array(random.sample(features,sampx))\n",
    "print X.shape\n",
    "max_per_feature = np.max(X,axis=0)\n",
    "print '{} HC shape {}, np.sum(np.max(X,axis=0)): {}, diff={}'.format(side, X.shape,np.sum(max_per_feature),X.shape[1]-np.sum(max_per_feature))\n",
    "\n",
    "plot_list = []\n",
    "for i in np.arange(sampx):    \n",
    "    plot_list.append((X[i], 'X {}'.format(i)))    \n",
    "    \n",
    "plot_slices(plot_list, baseline_shape, datamask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats.mstats_basic import mquantiles\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "def get3DVol(HC_input, HC_shape, input_mask):\n",
    "    flatvol = np.zeros(np.prod(HC_shape))\n",
    "    flatvol[input_mask] = HC_input\n",
    "    vol = flatvol.reshape(-1, HC_shape[2]).T\n",
    "    return vol\n",
    "\n",
    "def plot_slices(slice_list, baseline_shape, baseline_mask, llimit=0.01, ulimit=0.99, xmin=1000, xmax=2400):\n",
    "    \"\"\"\n",
    "    Plot dem slices.\n",
    "    :param slice_list:\n",
    "    :param llimit:\n",
    "    :param ulimit:\n",
    "    :param num_slices:\n",
    "    :param xmin:\n",
    "    :param xmax:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_slices = len(slice_list)\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    cols = 1\n",
    "    rows = num_slices / cols\n",
    "    plt.cla()\n",
    "    for j, input in enumerate(slice_list):\n",
    "        quantiles = mquantiles(input[0], [llimit, ulimit])\n",
    "        wt_vol = get3DVol(input[0], baseline_shape, baseline_mask)\n",
    "        plt.subplot(rows, cols, j + 1)\n",
    "        #im = plt.imshow(wt_vol[:, xmin:xmax], cmap=plt.cm.Reds, aspect='auto', interpolation='none', vmin=-.06, vmax=0.06)\n",
    "        im = plt.imshow(wt_vol, cmap=plt.cm.Reds, aspect='auto', interpolation='none')\n",
    "        plt.grid()\n",
    "        plt.title(input[1])\n",
    "        plt.colorbar()\n",
    "        im.set_clim(quantiles[0], quantiles[1])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = input_files[-4]\n",
    "subject_id = re.findall(id_participant, f)\n",
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(X,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "sub_DX_data_path = baseline_dir + 'CS/ADNI1_BL_PTID_DX_bl_dict.pkl'\n",
    "sub_DX_dict = pickle.load( open(sub_DX_data_path, \"rb\" ) )\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "y_dx = [] #Used for creating balanced k-folds\n",
    "for key in subject_vol_dict.keys():\n",
    "    X.append(np.sum(np.squeeze(np.array(stats.mode(subject_vol_dict[key])[0]))))\n",
    "    y.append(sub_DX_dict[key])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X shape: {}, y shape: {}'.format(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=0.7\n",
    "plt.hist(X[y=='AD'],alpha=a,bins=100,label='AD')\n",
    "plt.hist(X[y=='CN'],alpha=a,bins=100,label='CN')\n",
    "plt.hist(X[y=='LMCI'],alpha=a,bins=100,label='MCI')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.hist(R_HC_vols)\n",
    "plt.title('Right HC')\n",
    "plt.subplot(2,1,2)\n",
    "plt.hist(L_HC_vols)\n",
    "plt.title('Left HC')\n",
    "#R:12386 #L=10262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
