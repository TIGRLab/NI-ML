{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import caffe\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(1)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "# Some defs to load data and extract encodings from trained net\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    #print net.blobs.items()[0]\n",
    "    #print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        #print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        #print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        #print net.blobs[input_node].shape\n",
    "        \n",
    "        data_layers.append(net.blobs[input_node])    \n",
    "        #print 'X_list shape: {}'.format(X_list[i].shape)\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "    \n",
    "     \n",
    "    net.reshape()            \n",
    "    #print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        #print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1, concat_param=dict(axis=1))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_CT_SpecCluster_dyn,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT_SpecCluster_dyn, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT_unified(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_HC_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_HC_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_HC_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_HC_CT,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_HC_CT, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=4, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas, n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT_SpecCluster_dyn, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "#     n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,n.ff1, concat_param=dict(axis=1))\n",
    "    #n.concat = L.Concat(n.X_L_HC,n.X_R_HC,n.X_CT, concat_param=dict(axis=1))\n",
    "    \n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=lr['COMB']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.dropC1 = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2) #This is done automatically by caffe! \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        #n.ff2_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2ADAS13 = L.ReLU(n.ff2_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        #n.ff2_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2MMSE = L.ReLU(n.ff2_MMSE, in_place=True)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        n.ff1_DX = L.InnerProduct(n.ff3, num_output=node_sizes['DX_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1DX = L.ReLU(n.ff1_DX, in_place=True)\n",
    "        \n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_DX = L.InnerProduct(n.ff1_DX, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.DX_loss = L.SoftmaxWithLoss(n.output_DX, n.dx_cat3,loss_weight=tr['DX'])  \n",
    "    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_CT, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.dropC1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "        \n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_R_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_L_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))       \n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='HC_CT': #multimodal AE - experimental stage\n",
    "        HC_node_split = 0.8\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_L_HC = L.InnerProduct(n.X_L_HC, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.9*node_sizes['En1'])))        \n",
    "        n.NLinEn_L_HC = L.ReLU(n.encoder_L_HC, in_place=True)\n",
    "        \n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_CT = L.InnerProduct(n.X_CT, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.1*node_sizes['En1'])))        \n",
    "        n.NLinEn_CT = L.ReLU(n.encoder_CT, in_place=True)\n",
    "        \n",
    "        #Concat         \n",
    "        n.encoder1 = L.Concat(n.encoder_L_HC,n.encoder_CT, concat_param=dict(axis=1))\n",
    "        \n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers (or common encoder layers for multimodal) \n",
    "    \n",
    "#     n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.Sigmoid(n.encoder2, in_place=True)\n",
    "    #code layer\n",
    "#    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='xavier'))  \n",
    "    \n",
    "#     n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "    \n",
    "    #Decoder layers\n",
    "    if modality == 'CT':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)                    \n",
    "    \n",
    "    elif modality =='R_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    elif modality =='HC_CT':        \n",
    "        n.decoder_L_HC = L.InnerProduct(n.code, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_L_CT = L.ReLU(n.decoder_L_HC, in_place=True)\n",
    "        n.decoder_CT = L.InnerProduct(n.code, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_CT = L.ReLU(n.decoder_CT, in_place=True)\n",
    "        \n",
    "        n.output_L_HC = L.InnerProduct(n.decoder_L_HC, num_output=node_sizes['out_HC'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_HC = L.SigmoidCrossEntropyLoss(n.output_L_HC, n.X_L_HC)\n",
    "        \n",
    "        n.output_CT = L.InnerProduct(n.decoder_CT, num_output=node_sizes['out_CT'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_CT = L.EuclideanLoss(n.output_CT, n.X_CT)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    return n.to_proto()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode):\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 20\n",
    "    \n",
    "    if multimodal_autoencode:\n",
    "        train_loss_HC = zeros(niter)\n",
    "        test_loss_HC = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_loss_CT = zeros(niter)\n",
    "        test_loss_CT = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "        for it in range(niter):            \n",
    "            solver.step(1)  # SGD by Caffe \n",
    "            train_loss_HC[it] = solver.net.blobs['loss_HC'].data\n",
    "            train_loss_CT[it] = solver.net.blobs['loss_CT'].data\n",
    "            \n",
    "            if it % test_interval == 0:                        \n",
    "                t_loss_HC = 0\n",
    "                t_loss_CT = 0                                \n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()   \n",
    "                    t_loss_HC += solver.test_nets[0].blobs['loss_HC'].data\n",
    "                    t_loss_CT += solver.test_nets[0].blobs['loss_CT'].data\n",
    "                \n",
    "                test_loss_HC[it // test_interval] = t_loss_HC/(test_iter)\n",
    "                test_loss_CT[it // test_interval] = t_loss_CT/(test_iter)\n",
    "                print 'HC Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_HC[it], np.sum(train_loss_HC)/it, test_loss_HC[it // test_interval])\n",
    "                print 'CT Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_CT[it], np.sum(train_loss_CT)/it, test_loss_CT[it // test_interval])\n",
    "                \n",
    "        perf = {'train_loss':[train_loss_HC, train_loss_CT],'test_loss':[test_loss_HC,test_loss_CT]}\n",
    "    else: \n",
    "        \n",
    "        #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "        # losses will also be stored in the log\n",
    "        test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "        if not multi_task:\n",
    "            train_loss = zeros(niter)\n",
    "            test_loss = zeros(int(np.ceil(niter / test_interval)))  \n",
    "\n",
    "        else: \n",
    "            if multi_label:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_DX_loss = zeros(niter)\n",
    "                test_DX_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "            else:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_MMSE_loss = zeros(niter)\n",
    "                test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "\n",
    "        #output = zeros((niter, batch_size))\n",
    "        #solver.restore()\n",
    "        #the main solver loop\n",
    "        for it in range(niter):\n",
    "            #solver.net.forward()\n",
    "            solver.step(1)  # SGD by Caffe    \n",
    "            # store the train loss\n",
    "            if not multi_task:\n",
    "                train_loss[it] = solver.net.blobs['loss'].data        \n",
    "            else: \n",
    "                if multi_label:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_DX_loss[it] = solver.net.blobs['DX_loss'].data\n",
    "                else:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "            # store the output on the first test batch\n",
    "            # (start the forward pass at conv1 to avoid loading new data)\n",
    "            #solver.test_nets[0].forward()\n",
    "            #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "            # run a full test every so often\n",
    "            # (Caffe can also do this for us and write to a log, but we show here\n",
    "            #  how to do it directly in Python, where more complicated things are easier.)\n",
    "            if it % test_interval == 0:        \n",
    "                t_loss = 0\n",
    "                t_ADAS13_loss = 0\n",
    "                t_MMSE_loss = 0\n",
    "                t_DX_loss = 0\n",
    "                correct = 0\n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()                \n",
    "                    if not multi_task:\n",
    "                        t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                        if multi_label:\n",
    "                            correct += sum(solver.test_nets[0].blobs['output'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "                    else: \n",
    "                        if multi_label:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_DX_loss += solver.test_nets[0].blobs['DX_loss'].data\n",
    "                            correct += sum(solver.test_nets[0].blobs['output_DX'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "\n",
    "                        else:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "                if not multi_task:\n",
    "                    test_loss[it // test_interval] = t_loss/(test_iter)\n",
    "                    if multi_label:\n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                    print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                        it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval], test_acc[it // test_interval])\n",
    "                else:\n",
    "                    if multi_label:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_DX_loss[it // test_interval] = t_DX_loss/(test_iter) \n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'DX Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                            it, train_DX_loss[it], np.sum(train_DX_loss)/it, test_DX_loss[it // test_interval],test_acc[it // test_interval])\n",
    "                    else:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_MMSE_loss[it // test_interval] = t_MMSE_loss/(test_iter)             \n",
    "\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "        if not multi_task:\n",
    "            perf = {'train_loss':[train_loss],'test_loss':[test_loss],'test_acc':[test_acc]}\n",
    "        else:\n",
    "            if multi_label:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_DX_loss],'test_loss':[test_ADAS13_loss,test_DX_loss],'test_acc':[test_acc]}\n",
    "            else:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "                \n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,snap_prefix):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    \n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    #s.solver_type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 100000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 2000\n",
    "    s.snapshot_prefix = snap_prefix\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MC # 1, Hype # hyp4, Fold # 1\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold1/pretrained_models/ADNI2_ff_hyp4_HC_CT_HC_snap_40000_CT_snap_40000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (138.114898682,inf), test loss: 170.100344849\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (415.866210938,inf), test loss: 378.905175781\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (55.3248405457,88.2384722118), test loss: 48.9131054878\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.53912425041,81.2418770908), test loss: 2.98468012512\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (45.9494895935,69.0860486298), test loss: 48.3424469948\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.4172770977,42.1443818904), test loss: 2.90561853051\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (38.1010284424,62.3202206141), test loss: 45.1582288265\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.04659748077,29.0929064792), test loss: 2.75328783989\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (41.8289031982,58.8241104565), test loss: 43.9381557465\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.13978385925,22.5573006768), test loss: 2.67716296911\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (191.881469727,56.7251362473), test loss: 44.0286202431\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.52801132202,18.6346858422), test loss: 2.74562773705\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (46.9871025085,55.2408128629), test loss: 43.8744120598\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.47219347954,16.0231325404), test loss: 2.76660538018\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (60.5200576782,54.1342062357), test loss: 43.8843099594\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.85503220558,14.1513914051), test loss: 2.77152101994\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (105.476821899,53.2502530689), test loss: 45.115243721\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.51114559174,12.7517118482), test loss: 2.95753777325\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (45.1693954468,52.5873758846), test loss: 44.9354148388\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.01820158958,11.6606236566), test loss: 2.95850434899\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (55.8394393921,51.9907943726), test loss: 44.6664954185\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.65351712704,10.7891205512), test loss: 2.934676072\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (38.6444549561,51.5058104487), test loss: 44.8982415676\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.36984980106,10.0748119494), test loss: 2.92984293997\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (25.6562290192,51.0537147444), test loss: 43.9960694313\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.31523132324,9.47776739407), test loss: 2.85908461511\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (39.4323387146,50.642657419), test loss: 44.0711494923\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.69215643406,8.97444071642), test loss: 2.83757831156\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (20.0318336487,50.2522785677), test loss: 44.6128330231\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.36426925659,8.53960329704), test loss: 2.88233414292\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (152.539825439,49.9192265786), test loss: 44.1926793098\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (5.57103204727,8.16627525584), test loss: 2.83658839464\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (14.8502416611,49.5756460661), test loss: 41.7884748459\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.94817852974,7.83574506972), test loss: 2.83845545948\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (36.7141799927,49.256643787), test loss: 41.3251708031\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.36092877388,7.54581616924), test loss: 2.84392362833\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (16.750541687,48.967529879), test loss: 40.9382164001\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.8326472044,7.28659928953), test loss: 2.95719987154\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (38.1773490906,48.6628579104), test loss: 45.6688755035\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.32581067085,7.05431690538), test loss: 3.28911664188\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (36.4501304626,48.3706904945), test loss: 44.9924732208\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.95446157455,6.84429634959), test loss: 3.27788909376\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (23.7052268982,48.0883456116), test loss: 44.1809756279\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.834635734558,6.65314086299), test loss: 3.24289355278\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (14.9031381607,47.7955019327), test loss: 42.968100214\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.016751647,6.48008396807), test loss: 3.20750891566\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (19.4399223328,47.5039880876), test loss: 42.7944250584\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.64886689186,6.3197058788), test loss: 3.10654842854\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (36.8929672241,47.2050859951), test loss: 42.7638787746\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.81518542767,6.17343337468), test loss: 3.04316428304\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (23.9416790009,46.9224267376), test loss: 43.9852506638\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.465288341045,6.03734730519), test loss: 3.04598616958\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (23.2535266876,46.6208255401), test loss: 43.3649145126\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (5.95176362991,5.9120617241), test loss: 2.960396415\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (69.1539154053,46.3310800029), test loss: 37.3158972502\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.94872665405,5.79424422748), test loss: 2.66383590698\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (11.7378063202,46.0196541332), test loss: 36.9483484268\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.98296165466,5.68381655612), test loss: 2.69230356216\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (21.6499786377,45.7063179926), test loss: 36.7392930508\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.37817764282,5.58099167288), test loss: 2.67189111114\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (37.8315162659,45.3848595961), test loss: 36.5572943449\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (5.39802360535,5.48339799682), test loss: 2.63026385307\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (29.2205810547,45.0641202668), test loss: 35.8464515686\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.92985987663,5.39218508557), test loss: 2.64405716062\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (34.9505119324,44.7292449388), test loss: 34.9792831898\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.31489872932,5.30473259062), test loss: 2.63626803458\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (11.10115242,44.3799938274), test loss: 33.2246594429\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.8925755024,5.22278587327), test loss: 2.57754864693\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (17.0933303833,44.03836014), test loss: 33.8367907763\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.49954199791,5.14426240331), test loss: 2.48681094944\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (23.8541965485,43.6776333775), test loss: 33.4752842188\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.16362094879,5.06975499019), test loss: 2.55102581978\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (53.894859314,43.3149298475), test loss: 34.6522089481\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.51648712158,4.99822073591), test loss: 2.61360883713\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (35.399307251,42.9428628259), test loss: 35.3414063692\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.36822414398,4.92941917242), test loss: 2.72395689487\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (8.567527771,42.5559123411), test loss: 34.7845378876\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.91666460037,4.86416440723), test loss: 2.66496558785\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (21.5405921936,42.1623702456), test loss: 33.8415956497\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.28473138809,4.80068139784), test loss: 2.68713774681\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (57.4765892029,41.7546349357), test loss: 32.9662941456\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.52896881104,4.74067886064), test loss: 2.62835144401\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (23.3942184448,41.3432329378), test loss: 32.4220792294\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.06062889099,4.68197156062), test loss: 2.62069685161\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (11.8667593002,40.9186539851), test loss: 32.0085488141\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.21330857277,4.62613731817), test loss: 2.59009051323\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (17.0783805847,40.5005960009), test loss: 31.5669950962\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.92216193676,4.57180680452), test loss: 2.54478984177\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (20.7623920441,40.0735319577), test loss: 30.2903982639\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.15347266197,4.51942807955), test loss: 2.41758719683\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (19.2613868713,39.6480698375), test loss: 29.4763549805\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.508867919445,4.4685437279), test loss: 2.39741603732\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (44.3602600098,39.2273456888), test loss: 28.3462471247\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.93155789375,4.41938089706), test loss: 2.42835344076\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (30.604473114,38.8083057505), test loss: 28.1848643541\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.75873923302,4.37230255885), test loss: 2.3986438334\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (13.3587903976,38.3947762972), test loss: 28.0226201534\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.95144045353,4.32592368964), test loss: 2.4915048331\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (23.943397522,37.9825666317), test loss: 29.5566091776\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.91928088665,4.28176812744), test loss: 2.60370765328\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (25.3318958282,37.5843736242), test loss: 28.940570569\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.07132077217,4.23851180848), test loss: 2.57695060968\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (15.0721235275,37.1881309221), test loss: 27.6084120989\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.51899123192,4.19700087212), test loss: 2.56321262717\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (40.7982788086,36.8049710339), test loss: 28.2195212841\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.36979055405,4.15647780614), test loss: 2.5402105391\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (2.4459207058,36.4262394784), test loss: 28.3296100259\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.72348070145,4.11698298527), test loss: 2.5604737699\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (8.50747776031,36.0565577642), test loss: 29.4157057047\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.21455395222,4.07906946392), test loss: 2.55068814456\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (7.6121840477,35.6982604239), test loss: 29.5254311323\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.498251616955,4.04177142497), test loss: 2.62497121692\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (44.8187675476,35.3470729706), test loss: 29.3851674795\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.77917432785,4.00633368696), test loss: 2.63822830915\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (6.32827663422,35.0055517119), test loss: 27.2939192772\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.729002714157,3.97123576022), test loss: 2.64221429229\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (23.0690784454,34.6702397619), test loss: 27.2307048321\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.55544710159,3.93772091676), test loss: 2.63954292536\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (5.8051199913,34.3482422343), test loss: 27.553173089\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.880959630013,3.90483572698), test loss: 2.7777446121\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (3.54629278183,34.0297394508), test loss: 30.3078310013\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.10319018364,3.87315073374), test loss: 2.97698517889\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (40.9960632324,33.7215927397), test loss: 30.251107192\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.82534790039,3.84200765847), test loss: 2.94745330364\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (16.3279533386,33.4214577915), test loss: 28.8591259241\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.25227403641,3.81173304601), test loss: 2.86842262447\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (6.84072971344,33.1273181189), test loss: 29.0138989568\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.272771060467,3.7826460107), test loss: 2.90994841456\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (12.0454216003,32.8431387439), test loss: 28.800936389\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.887832999229,3.75385803511), test loss: 2.70200839639\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (3.85524702072,32.5627814631), test loss: 29.7870405912\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.492883652449,3.72639404047), test loss: 2.71743036807\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (10.4401388168,32.2936222252), test loss: 30.2087795973\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.10841321945,3.69922039497), test loss: 2.55366177857\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (5.89906311035,32.0272704256), test loss: 28.0128286123\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.28877711296,3.67313499159), test loss: 2.48192587197\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (13.1399440765,31.7714991763), test loss: 26.8664298058\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.736238718033,3.64746232121), test loss: 2.36452968717\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (10.4771909714,31.5188504465), test loss: 27.2109531879\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.39896631241,3.62244045823), test loss: 2.39497743845\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (5.02739620209,31.2731021205), test loss: 27.3596433401\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.58390831947,3.59821451775), test loss: 2.37705871761\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.887090683,31.0347479283), test loss: 27.6334611416\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (5.31901264191,3.57437290093), test loss: 2.39733985066\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (8.93307209015,30.8007995473), test loss: 27.5729266644\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.54402399063,3.55141051667), test loss: 2.38798562884\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (20.1633644104,30.5737828875), test loss: 26.7597806215\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.04623925686,3.52861590645), test loss: 2.41071234643\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (15.9325141907,30.3497845179), test loss: 26.8529751062\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.544803678989,3.50675176762), test loss: 2.39113359153\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (10.0484113693,30.1344457053), test loss: 27.5326335907\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.40706896782,3.48524991993), test loss: 2.36024790108\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (17.418800354,29.9210657909), test loss: 28.018859005\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.11894059181,3.46441529006), test loss: 2.41362424493\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (7.64430570602,29.7140409345), test loss: 29.1793259859\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.95247793198,3.44383482696), test loss: 2.52686745226\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.5515880585,29.5120281365), test loss: 29.4141038656\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.13417363167,3.42363576233), test loss: 2.57634283006\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (25.1624965668,29.3134722583), test loss: 30.4635773897\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.21741163731,3.40422076462), test loss: 2.53193130344\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 2\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold2/pretrained_models/ADNI2_ff_hyp4_HC_CT_HC_snap_40000_CT_snap_40000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (138.376617432,inf), test loss: 161.566664886\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (427.696380615,inf), test loss: 397.002104187\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (77.8589096069,114.368538157), test loss: 62.4909864902\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.85158967972,125.247620123), test loss: 3.24943217337\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (41.372795105,82.8108965025), test loss: 48.6447469711\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.09086894989,64.1014743971), test loss: 3.14397247434\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (91.9109115601,71.3779552606), test loss: 47.7502964973\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.24103546143,43.715088798), test loss: 3.19972624183\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (34.8203163147,65.5684184253), test loss: 51.2626265526\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.05102276802,33.5246018302), test loss: 3.39863260984\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (38.9388084412,61.9826127838), test loss: 51.9374715805\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.87904977798,27.4159134332), test loss: 3.49444423318\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (56.7161178589,59.4855070327), test loss: 51.9327773094\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (4.1739692688,23.3644774136), test loss: 3.50042105317\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (36.614616394,57.6852407244), test loss: 52.4011990547\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.83275461197,20.4696713129), test loss: 3.51745690107\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (144.625427246,56.320014039), test loss: 50.0015334129\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (6.16282081604,18.2989241223), test loss: 3.49487884641\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (36.0579299927,55.2149173053), test loss: 47.7653195381\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.53517341614,16.6117788158), test loss: 3.40460636616\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (50.8063850403,54.3063111549), test loss: 47.7311944008\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.19991016388,15.2599099585), test loss: 3.58386397362\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (50.0344314575,53.5436156782), test loss: 45.8718199253\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.11116719246,14.157002671), test loss: 3.62925232351\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (69.6473999023,52.9063799751), test loss: 51.8323938847\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (7.80042409897,13.2364052397), test loss: 3.98419712782\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (191.358123779,52.3532575707), test loss: 50.5689448833\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (6.9978351593,12.4572284338), test loss: 3.9514703393\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (151.748504639,51.8535135501), test loss: 49.7385820389\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (5.46838855743,11.7900814233), test loss: 3.7777744323\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (51.2952651978,51.4009437254), test loss: 50.7834895134\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.12024736404,11.2100346037), test loss: 3.7227715373\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (50.3644371033,50.9875728076), test loss: 51.6469967365\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.90582561493,10.7043103769), test loss: 3.72308988273\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (73.7997589111,50.6050105855), test loss: 45.4119081974\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.68630027771,10.2567470766), test loss: 3.30404409766\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (177.334518433,50.2349812598), test loss: 45.2642165184\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (6.5812921524,9.85908414069), test loss: 3.34461888671\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (160.473251343,49.8792334134), test loss: 45.421939373\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (5.44592189789,9.50355286609), test loss: 3.36421507001\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (39.951007843,49.5368327101), test loss: 43.0902857304\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.72121345997,9.18195738339), test loss: 3.31350607276\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (48.640335083,49.2075353048), test loss: 46.7550191402\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.97772336006,8.89243294139), test loss: 3.3375459075\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (71.4183578491,48.8998990069), test loss: 48.2641193867\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.54326868057,8.62785146803), test loss: 3.37580331266\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (146.735153198,48.5985896324), test loss: 48.0156774521\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (4.96345710754,8.38457431256), test loss: 3.2904843092\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (157.130111694,48.3029063232), test loss: 47.9369529486\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.26793003082,8.15696904098), test loss: 3.22259834409\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (41.4863624573,48.006703934), test loss: 47.4115241051\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.614749193192,7.9441882326), test loss: 3.22009121776\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (36.3589553833,47.7121177559), test loss: 43.0550907135\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.34786343575,7.74838638154), test loss: 3.00403501093\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (61.2580795288,47.4283970237), test loss: 43.2117678642\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.0694475174,7.56561397277), test loss: 3.14342854619\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (110.502098083,47.1401414312), test loss: 45.5504441261\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.21719956398,7.39526068455), test loss: 3.17292597294\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (148.148254395,46.8576489244), test loss: 46.9866728783\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (4.9680891037,7.23660442894), test loss: 3.32984374762\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (29.272693634,46.5667674671), test loss: 46.2921636105\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.599704742432,7.08697056093), test loss: 3.315722543\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (30.5250759125,46.2728538342), test loss: 47.1504503965\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.22662997246,6.94758861229), test loss: 3.38649417162\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (27.2973365784,45.9817581929), test loss: 47.2142172337\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.67830824852,6.81548785931), test loss: 3.3462197274\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (47.4107513428,45.6830064139), test loss: 42.5284411907\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.05409121513,6.6910700935), test loss: 3.20826992989\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (27.4254798889,45.3841138172), test loss: 41.9428184748\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.62112212181,6.57364307875), test loss: 3.36685180068\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (28.3532238007,45.0850008445), test loss: 40.2643102169\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.534704208374,6.46180411244), test loss: 3.46810740232\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (36.2197341919,44.7734547063), test loss: 41.9221772671\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.17324519157,6.35638232156), test loss: 3.50013368726\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (13.9621047974,44.4609389133), test loss: 43.9358995914\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.27704036236,6.25532059016), test loss: 3.77360976934\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (44.1285552979,44.1412534401), test loss: 42.6248882055\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.5602722168,6.15923664401), test loss: 3.5626870513\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (14.843421936,43.8175438676), test loss: 42.8843810081\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.30611884594,6.06763166853), test loss: 3.53539570272\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (22.5860996246,43.4940888641), test loss: 43.9849913597\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.615524291992,5.97959973318), test loss: 3.46437537074\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (17.8503952026,43.1569652351), test loss: 37.6465843439\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.29592955112,5.89586817076), test loss: 3.00293083191\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (14.9336118698,42.8201832014), test loss: 37.1073442936\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.03578305244,5.81498545813), test loss: 3.02973446846\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (35.0228424072,42.4768833551), test loss: 37.2639513969\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.96111917496,5.73751860859), test loss: 3.03024065495\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (10.8153057098,42.129677812), test loss: 36.3940927982\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.657863497734,5.66305449322), test loss: 3.03774293065\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (18.1867389679,41.7838573828), test loss: 36.6910620928\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.85113132,5.59108354772), test loss: 2.97537797093\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (19.8033905029,41.4275282837), test loss: 38.8596984863\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.738034784794,5.52217969455), test loss: 3.06630981565\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (19.8636360168,41.0730265667), test loss: 38.665719986\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.810287952423,5.45524750522), test loss: 2.98475990891\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (16.9158000946,40.7151400887), test loss: 38.1667982757\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (4.57263660431,5.39084113325), test loss: 2.96686322093\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (8.09567737579,40.3564670124), test loss: 38.081795311\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.740446925163,5.32857597698), test loss: 2.96372752488\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (22.2846069336,40.0018219316), test loss: 37.2353654861\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.18238413334,5.26815683578), test loss: 2.92042861581\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (21.8056869507,39.6411571443), test loss: 35.704779923\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.640288174152,5.21009619472), test loss: 2.90166171789\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (19.1462097168,39.2854072751), test loss: 36.1493020296\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.700038671494,5.15346284898), test loss: 2.91628539264\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.1777534485,38.9306126695), test loss: 38.79005723\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (6.01112651825,5.09880511184), test loss: 3.09747760296\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (7.07864665985,38.5784080154), test loss: 37.8276552916\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.20500361919,5.04581055769), test loss: 3.07633965015\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.0475120544,38.2331505747), test loss: 37.2783291817\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.967579722404,4.99424974234), test loss: 3.05180741847\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (19.2506561279,37.8864788571), test loss: 38.0087379456\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.63502764702,4.94459533553), test loss: 3.16848305762\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.2779808044,37.5473908333), test loss: 36.7865142822\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.597692072392,4.89603866207), test loss: 3.11243883967\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (22.3659934998,37.2128557491), test loss: 34.8234185815\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (6.68435144424,4.84906501057), test loss: 3.25326479673\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.3511343002,36.8829136365), test loss: 33.9707329512\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.1479139328,4.80348754541), test loss: 3.24197510779\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (28.2473659515,36.5618931169), test loss: 32.8010667801\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.17100918293,4.75903663176), test loss: 3.18393018842\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.3466644287,36.2421518624), test loss: 36.8698312461\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.80304956436,4.71616605695), test loss: 3.67078905106\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.5494060516,35.9308166907), test loss: 36.5299087286\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.3577221632,4.67414911573), test loss: 3.61965635419\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (20.1114349365,35.6259567071), test loss: 36.5576669693\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (6.39894676208,4.63345664304), test loss: 3.45038669109\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.9426059723,35.3263513496), test loss: 37.0568411589\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.51907038689,4.59390371321), test loss: 3.33211878538\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (36.8356399536,35.0356737883), test loss: 32.9955762386\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.976707696915,4.5552633705), test loss: 3.04875721335\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.3794202805,34.7472009584), test loss: 33.0139308453\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.74173927307,4.51795912383), test loss: 2.88435663581\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (14.7288360596,34.4668905085), test loss: 33.3426562548\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.509111642838,4.48131826513), test loss: 2.92442394495\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (16.3679656982,34.1933343574), test loss: 33.3490564108\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (5.43524169922,4.44578160492), test loss: 2.94958391786\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (38.5871582031,33.9246050275), test loss: 32.9859844208\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (4.11467981339,4.41119882465), test loss: 2.90353920311\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (32.2064285278,33.6638885318), test loss: 34.5070389271\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.920187473297,4.3773498765), test loss: 2.89459986687\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (9.47828197479,33.4054544149), test loss: 35.7059968233\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.84083247185,4.34464426045), test loss: 2.93378096223\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (14.8210515976,33.1541835545), test loss: 35.2351660728\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.397856354713,4.31243721386), test loss: 2.87606154084\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (22.6092395782,32.9087581753), test loss: 35.5787789345\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (6.04192495346,4.28109411142), test loss: 2.89359574914\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (39.110710144,32.6671789216), test loss: 35.721254015\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.42319202423,4.25061484386), test loss: 2.91989260316\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (28.1638393402,32.4323379551), test loss: 34.9300645828\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.40328574181,4.22070486322), test loss: 2.82149932683\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (6.88835811615,32.1993700355), test loss: 34.2602501869\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.820824801922,4.19167918787), test loss: 2.82447382808\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.7434272766,31.9727948855), test loss: 37.100972867\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.85706019402,4.16313745906), test loss: 3.00227499902\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (19.2356262207,31.751518367), test loss: 36.7759287834\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (4.67848396301,4.13523429899), test loss: 3.0097561419\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (35.1985702515,31.5338396429), test loss: 35.8311939716\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.62160181999,4.10815083295), test loss: 3.00835643113\n",
      "run time for single CV loop: 1326.3590498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:124: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:126: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Deign Net Architecutre and Run Caffe\n",
    "exp_name = 'Exp11_MC'\n",
    "cohort = 'ADNI2'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'HC_CT'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "multi_label = False\n",
    "start_MC=1\n",
    "n_MC=1\n",
    "MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "start_fold = 1\n",
    "n_folds = 2\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "niter = 40000\n",
    "batch_size = 256\n",
    "\n",
    "pretrain = False\n",
    "multimodal_autoencode = False\n",
    "load_pretrained_weights = True\n",
    "\n",
    "if Clinical_Scale in ['BOTH','ADAS13_DX'] :\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "# dr: Dropout rate, lr: learning rate of each modality, tr: loss-weight for each task\n",
    "hype_configs = {\n",
    "#                 'hyp1':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':100,'HC_CT_ff':25,'COMB_ff':50,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':.1,'CT':.1,'COMB':1},\n",
    "#                       'tr':{'ADAS':1,'MMSE':8,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},\n",
    "    \n",
    "#                 'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':50,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':.2,'CT':.2,'COMB':1},\n",
    "#                       'tr':{'ADAS':1,'MMSE':8,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},     \n",
    "                \n",
    "#                 'hyp3':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':200,'HC_CT_ff':25,'COMB_ff':50,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':0.1,'CT':0.1,'COMB':1},\n",
    "#                       'tr':{'ADAS':1,'MMSE':4,'DX':1},'solver_conf':{'base_lr':3e-6, 'wt_decay':1e-2}},   \n",
    "    \n",
    "                'hyp4':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':25,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':.1,'CT':.1,'HC_CT':1,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':2,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}}\n",
    "                    \n",
    "                }\n",
    "\n",
    "CV_perf_MC = {}\n",
    "for mc in MC_list:\n",
    "    CV_perf_hype = {}\n",
    "    for hype in hype_configs.keys():    \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "        tr = hype_configs[hype]['tr']\n",
    "        solver_configs = hype_configs[hype]['solver_conf']\n",
    "        \n",
    "        if hype in ['hyp1']:\n",
    "            HC_snap = 20000 #5000 for ADNI1\n",
    "            CT_snap = 20000 #5000 for ADNI1\n",
    "            pre_hype = 'hyp1'\n",
    "        elif hype in ['hyp2']:\n",
    "            HC_snap = 20000 #5000 for ADNI1\n",
    "            CT_snap = 20000 #5000 for ADNI1\n",
    "            pre_hype = 'hyp2'\n",
    "        elif hype in ['hyp3']:\n",
    "            HC_snap = 20000 #5000 for ADNI1\n",
    "            CT_snap = 25000 #5000 for ADNI1\n",
    "            pre_hype = 'hyp3'\n",
    "        elif hype in ['hyp4']:\n",
    "            HC_snap = 40000 #5000 for ADNI1\n",
    "            CT_snap = 40000 #5000 for ADNI1\n",
    "            pre_hype = 'hyp4'\n",
    "        else:\n",
    "            print 'unknown hyp config'\n",
    "            \n",
    "            print hype, pre_hype,HC_snap,CT_snap\n",
    "            \n",
    "        CV_perf = {}\n",
    "        start_time = time.time()\n",
    "        for fid in fid_list:            \n",
    "            print ''\n",
    "            print 'MC # {}, Hype # {}, Fold # {}'.format(mc, hype, fid)\n",
    "            snap_prefix = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}'.format(mc,fid,exp_name,hype,modality)\n",
    "\n",
    "            train_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/train_C688.txt'.format(mc,fid)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "\n",
    "            train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "            with open(train_filename_txt, 'w') as f:\n",
    "                    f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "            if pretrain:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_train.prototxt'.format(mc,fid)\n",
    "                with open(train_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(train_filename_txt, batch_size, node_sizes, modality)))            \n",
    "            else:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(train_net_path, 'w') as f:            \n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            if pretrain:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_test.prototxt'.format(mc,fid)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(test_filename_txt, batch_size, node_sizes,modality)))\n",
    "            else:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            # Define Solver\n",
    "            solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "            with open(solver_path, 'w') as f:\n",
    "                f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, snap_prefix)))\n",
    "\n",
    "            ### load the solver and create train and test nets\n",
    "            #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "            #solver = caffe.get_solver(solver_path)\n",
    "            #solver = caffe.NesterovSolver(solver_path)\n",
    "            solver = caffe.NesterovSolver(solver_path)\n",
    "\n",
    "            if load_pretrained_weights:                    \n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_HC_CT_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                snap_path = baseline_dir + net_name.format(mc,fid,cohort,pre_hype,HC_snap,CT_snap)\n",
    "                print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "                solver.net.copy_from(snap_path)\n",
    "\n",
    "            #run caffe\n",
    "            results = run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode)\n",
    "            CV_perf[fid] = results\n",
    "            \n",
    "        print('run time for single CV loop: {}'.format(time.time() - start_time))\n",
    "\n",
    "        CV_perf_hype[hype] = CV_perf\n",
    "        \n",
    "CV_perf_MC[mc] = CV_perf_hype\n",
    "\n",
    "# pickleIt(CV_perf_MC, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.4444444444\n"
     ]
    }
   ],
   "source": [
    "#Time for training \n",
    "#HC_CT fold 9,10, number of iterations: 40k, two hyper-params\n",
    "#start time: 14:47\n",
    "#end time: 15:31\n",
    "#runtime_mean = (13+31)/4\n",
    "#print runtime_mean\n",
    "\n",
    "tx=140 #time for 10k iters\n",
    "itx=5 # num of 10k iters\n",
    "hx=2 #hyp choices\n",
    "fx=10 #k-folds\n",
    "mx=5 #mc-folds\n",
    "\n",
    "num_hrs = (tx*itx*hx*fx*mx)/(3600.0)\n",
    "print num_hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbcAAAJoCAYAAABVztwOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUOO2oiiKoiiKoiiKoiiKoiiKsuP4/33PUCIWuhmsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1fbca9d350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "#niter = 30000\n",
    "test_interval = 500\n",
    "n_mc = len(CV_perf_MC.keys())\n",
    "\n",
    "for m, mc in enumerate(CV_perf_MC.keys()): \n",
    "    CV_perf_hype = CV_perf_MC[mc]\n",
    "    \n",
    "    for hype in CV_perf_hype.keys(): \n",
    "        CV_perf = CV_perf_hype[hype]\n",
    "        n_CV_configs = len(CV_perf)\n",
    "        pid = m*n_CV_configs + 1\n",
    "        \n",
    "        for f, fid in enumerate(fid_list):  \n",
    "            train_loss_list = CV_perf[fid]['train_loss']\n",
    "            test_loss_list = CV_perf[fid]['test_loss']\n",
    "            \n",
    "            for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "                #plt.figure()\n",
    "                ax1 = plt.subplot(n_mc,n_CV_configs,pid)            \n",
    "                ax1.plot(arange(niter), train_loss, label='train',linewidth='.5',alpha=0.25)\n",
    "                ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test_{}'.format(hype), linewidth='3')                            \n",
    "                ax1.set_xlabel('iteration')\n",
    "                ax1.set_ylabel('loss')\n",
    "                ax1.set_title('fid: {}, hyp: {}, Test loss: {:.2f}'.format(fid, hype, test_loss[-1]))\n",
    "                ax1.legend(loc=1)\n",
    "                ax1.set_ylim(0,100)\n",
    "            pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp11'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'R_HC'\n",
    "batch_size = 256\n",
    "encoding_layer = 'code'\n",
    "weight_layers = 'code'\n",
    "multi_task = False\n",
    "\n",
    "if modality in ['L_HC', 'R_HC']:\n",
    "    snap_iter = 10000\n",
    "    \n",
    "    if modality == 'R_HC':\n",
    "        output_node_size = 16471\n",
    "    else: \n",
    "        output_node_size = 16086\n",
    "        \n",
    "    hype_configs = {\n",
    "                   'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':8000,'out':output_node_size},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "\n",
    "elif modality in ['CT']:  \n",
    "    snap_iter = 100000\n",
    "    \n",
    "    hype_configs = {\n",
    "                    'hyp1':{'node_sizes':{'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-4, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "elif modality in ['HC_CT']:  \n",
    "    snap_iter = 20000\n",
    "    hype_configs = {\n",
    "                 'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':500,'out_HC':16086,'out_CT':686},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "hype = 'hyp1'\n",
    "pretrain_preproc = 'ae_preproc_sparse_HC_10k_CT_100k_{}'.format(hype)\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "\n",
    "# Save either L or R HC encodings. Turn on CT + CS save only for one of these. \n",
    "# (Also decide if you want to copy CT raw scores or encodings)\n",
    "save_encodings = True\n",
    "save_scores = False\n",
    "CT_encodings = True\n",
    "\n",
    "for cohort in ['inner_train', 'inner_test','outer_test']:\n",
    "    data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "    test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)       \n",
    "    input_nodes = ['X_{}'.format(modality)]\n",
    "    #input_nodes = ['X_L_HC','X_CT']\n",
    "    net_file = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "    with open(net_file, 'w') as f:\n",
    "        f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "\n",
    "    test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "    with open(test_filename_txt, 'w') as f:\n",
    "        f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "    sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "    if modality == 'CT':\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "    else:\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "        \n",
    "    print 'Check Sparsity for model: {}'.format(model_file)\n",
    "    \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    \n",
    "    encodings_hdf_file = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,pretrain_preproc)\n",
    "    \n",
    "    if save_encodings:    \n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        input_data.create_dataset(input_nodes[0],data=encodings)           \n",
    "        input_data.close()\n",
    "        \n",
    "    if save_scores:\n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        if not CT_encodings: #copy raw scores for CT instead of encodings\n",
    "            CT_data = load_data(data_path, 'X_CT','no_preproc')                    \n",
    "            input_data.create_dataset('X_CT', data=CT_data)    \n",
    "            \n",
    "        adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "        mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "        input_data.create_dataset('y', data=adas_scores)           \n",
    "        input_data.create_dataset('y3', data=mmse_scores)    \n",
    "        input_data.create_dataset('dx_cat3', data=dx_labels)\n",
    "        input_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting TSNE embeddings \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "exp_name = 'Exp11'\n",
    "#preproc = 'no_preproc'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "Clinical_Scale = 'ADAS13_DX' \n",
    "batch_size = 256\n",
    "snap_interval = 2000\n",
    "snap_start = 2000\n",
    "encoding_layer = 'ff3'\n",
    "weight_layers = 'ff3'\n",
    "cohort = 'outer_test'\n",
    "multi_task = False\n",
    "\n",
    "modality = 'HC_CT'\n",
    "snap_end = 21000\n",
    "\n",
    "\n",
    "hype_configs = {\n",
    "            'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-2}},\n",
    "}\n",
    "\n",
    "hype = 'hyp1'\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "tr = hype_configs[hype]['tr']\n",
    "\n",
    "data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "test_net_path = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)       \n",
    "#input_node = 'X_{}'.format(modality)\n",
    "\n",
    "net_file = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)\n",
    "with open(net_file, 'w') as f:\n",
    "    #f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "\n",
    "test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "with open(test_filename_txt, 'w') as f:\n",
    "    f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "n_snaps = len(np.arange(snap_start,snap_end,snap_interval))\n",
    "tsne_encodings = {}\n",
    "net_weights = {}\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    print sn, snap_iter\n",
    "    model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    net_weights[snap_iter] = results['wt_dict']\n",
    "    print encodings.shape\n",
    "\n",
    "    adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "    mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "    dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "    tsne_model = TSNE(n_components=2, random_state=0, init='pca')\n",
    "    tsne_encodings[snap_iter] = tsne_model.fit_transform(encodings) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "plot_encodings = True\n",
    "plot_wts = False\n",
    "\n",
    "color_scores = dx_labels\n",
    "cm = plt.get_cmap('RdYlBu') \n",
    "marker_size = 100\n",
    "marker_size = (np.mod(color_scores+1,2)+1)*50\n",
    "\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    plt.subplot(np.ceil(n_snaps/2.0),2,sn+1)\n",
    "    if plot_encodings:\n",
    "        plt.scatter(tsne_encodings[snap_iter][:,0],tsne_encodings[snap_iter][:,1],c=color_scores,s=marker_size,cmap=cm)\n",
    "    if plot_wts:\n",
    "        plt.imshow(net_weights[snap_iter][weight_layers])\n",
    "        \n",
    "    plt.title(snap_iter)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp11_MC 1 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.62297587170804425, 0.67764933681857142]\n",
      "ADAS mse: [64.357039055245551, 53.769512368572308]\n",
      "ADAS means: 0.650312604263, 59.0632757119\n",
      "\n",
      "MMSE corr: [0.46756760623908078, 0.45964478759353355]\n",
      "MMSE mse: [6.3986880314121048, 5.9819195866372734]\n",
      "MMSE means: 0.463606196916, 6.19030380902\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'HC_CT': 1, 'COMB': 1, 'CT': 0.1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'HC_CT': 1, 'COMB': 1, 'CT': 0.1}}}\n",
      "\n",
      "opt_snap: {1: 6, 2: 9}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.62297587170804425, 0.67764933681857142]\n",
      "ADAS mse: [64.357039055245551, 53.769512368572308]\n",
      "ADAS means: 0.650312604263, 59.0632757119\n",
      "\n",
      "MMSE corr: [0.46756760623908078, 0.45964478759353355]\n",
      "MMSE mse: [6.3986880314121048, 5.9819195866372734]\n",
      "MMSE means: 0.463606196916, 6.19030380902\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'HC_CT': 1, 'COMB': 1, 'CT': 0.1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'HC_CT': 1, 'COMB': 1, 'CT': 0.1}}}\n",
      "\n",
      "opt_snap: {1: 6, 2: 9}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.62297587170804425, 0.67764933681857142]\n",
      "ADAS mse: [64.357039055245551, 53.769512368572308]\n",
      "ADAS means: 0.650312604263, 59.0632757119\n",
      "\n",
      "MMSE corr: [0.46756760623908078, 0.45964478759353355]\n",
      "MMSE mse: [6.3986880314121048, 5.9819195866372734]\n",
      "MMSE means: 0.463606196916, 6.19030380902\n",
      "\n",
      "opt_hyp: {1: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'HC_CT': 1, 'COMB': 1, 'CT': 0.1}}, 2: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 2e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'HC_CT': 1, 'COMB': 1, 'CT': 0.1}}}\n",
      "\n",
      "opt_snap: {1: 6, 2: 9}\n"
     ]
    }
   ],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "#exp_name = 'Exp11'\n",
    "#niter = 60000\n",
    "#modality = 'CT'\n",
    "# start_fold = 1\n",
    "# n_folds = 10\n",
    "# fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "#preproc = 'no_preproc'\n",
    "#batch_size = 256\n",
    "snap_interval = 4000\n",
    "snap_start = 4000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "#Clinical_Scale = 'ADAS13'\n",
    "\n",
    "#MC_list = np.arange(1,11,1)\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "idx = 0  \n",
    "df_perf_dict_adas = {}\n",
    "df_perf_dict_mmse = {}\n",
    "df_perf_dict_adas_tuned = {}\n",
    "df_perf_dict_mmse_tuned = {}\n",
    "df_perf_dict_opt = {}\n",
    "\n",
    "for mc in MC_list:\n",
    "    print exp_name, mc, modality\n",
    "\n",
    "    for hype in hype_configs.keys():      \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "\n",
    "        for fid in fid_list:\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "            #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "            #with open(test_filename_txt, 'w') as f:\n",
    "            #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                    f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC']\n",
    "                elif modality == 'CT':\n",
    "                    f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_CT_SpecCluster_dyn']\n",
    "                elif modality == 'HC_CT':\n",
    "                    f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC','X_CT_SpecCluster_dyn']\n",
    "                elif modality == 'HC_CT_unified_hyp1':\n",
    "                    f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_HC_CT']\n",
    "                else:\n",
    "                    print 'Wrong modality'\n",
    "\n",
    "            #print 'Hype # {}, MC # {}, Fold # {}, Clinical_Scale {}'.format(hype, mc, fid, Clinical_Scale)\n",
    "            data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            if Clinical_Scale == 'ADAS13':\n",
    "                act_scores = load_data(data_path, 'adas','no_preproc')\n",
    "            elif Clinical_Scale == 'MMSE': \n",
    "                act_scores = load_data(data_path, 'mmse','no_preproc')\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                act_scores_adas13 = load_data(data_path, 'adas','no_preproc')\n",
    "                act_scores_mmse = load_data(data_path, 'mmse','no_preproc')\n",
    "            else:\n",
    "                print 'unknown clinical scale'\n",
    "\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort)\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            sub.call([\"cp\", baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort), baseline_dir + \n",
    "                      'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)])\n",
    "            #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "            #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "\n",
    "            if Clinical_Scale in ['ADAS13', 'MMSE']:                \n",
    "                multi_task = False\n",
    "                cs_list = ['opt']\n",
    "                iter_euLoss = []\n",
    "                iter_r = []        \n",
    "                iter_pred_scores = []\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = np.squeeze(results['X_out'])            \n",
    "                    iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                    iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "                fold_r[config_idx] = np.array(iter_r)\n",
    "                fold_act_scores[fid] = act_scores\n",
    "                fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                multi_task = True\n",
    "                cs_list = ['adas','mmse']\n",
    "                iter_euLoss_adas13 = []\n",
    "                iter_r_adas13 = []        \n",
    "                iter_pred_scores_adas13 = []\n",
    "                iter_euLoss_mmse = []\n",
    "                iter_r_mmse = []        \n",
    "                iter_pred_scores_mmse = []\n",
    "\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = results['X_out']   \n",
    "                    encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                    encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                    iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                    iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                    iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                    iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "                fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "                fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "                fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "                fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "                fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "                fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "                fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "\n",
    "    \n",
    "    if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "        fold_perf_dict = {'fold_r':fold_r,'fold_euLoss':fold_euLoss,'fold_act_scores':fold_act_scores,'fold_pred_scores':fold_pred_scores}\n",
    "    else: \n",
    "        fold_perf_dict = {'fold_r_adas13':fold_r_adas13,'fold_r_mmse':fold_r_mmse,'fold_euLoss_adas13':fold_euLoss_adas13,'fold_euLoss_mmse':fold_euLoss_mmse,\n",
    "                         'fold_act_scores_adas13':fold_act_scores_adas13,'fold_act_scores_mmse':fold_act_scores_mmse,\n",
    "                          'fold_pred_scores_adas13':fold_pred_scores_adas13,'fold_pred_scores_mmse':fold_pred_scores_mmse}\n",
    "        \n",
    "    opt_metric = 'euLoss' #euLoss or corr or both\n",
    "    #task_weights = {'ADAS':1,'MMSE':0} \n",
    "    save_multitask_results = False\n",
    "    CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/'  \n",
    "    save_path = '{}{}_{}_NN_{}'.format(CV_model_dir,exp_name,mc,modality)\n",
    "    \n",
    "    # Create dictionaries of perfromance based on differnt task weights \n",
    "    for key, task_weights in {'adas':{'ADAS':1,'MMSE':0},'mmse':{'ADAS':0,'MMSE':1},'both':{'ADAS':1,'MMSE':1}}.items():\n",
    "        print 'Weights Tuned for: {}'.format(key)                              \n",
    "        results = compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path)  \n",
    "\n",
    "        # populate the perf dictionary for 10 MC x 10 folds\n",
    "        model_choice = 'APANN'    \n",
    "        for f, fid in enumerate(fid_list): \n",
    "            if key in ['adas','mmse']:\n",
    "                r_valid = results['{}_r'.format(key)][f]\n",
    "                MSE_valid = results['{}_mse'.format(key)][f]\n",
    "                RMSE_valid = results['{}_rmse'.format(key)][f]\n",
    "\n",
    "                if key == 'adas':\n",
    "                    df_perf_dict_adas_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                elif key == 'mmse':\n",
    "                    df_perf_dict_mmse_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                else: \n",
    "                    print 'unknown key'\n",
    "\n",
    "            elif key == 'both':                    \n",
    "                for cs in cs_list:            \n",
    "                    r_valid = results['{}_r'.format(cs)][f]\n",
    "                    MSE_valid = results['{}_mse'.format(cs)][f]\n",
    "                    RMSE_valid = results['{}_rmse'.format(cs)][f]\n",
    "\n",
    "                    if cs == 'adas':\n",
    "                        df_perf_dict_adas[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                    elif cs == 'mmse':\n",
    "                        df_perf_dict_mmse[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}                    \n",
    "                    else: \n",
    "                        print 'unknown key'\n",
    "\n",
    "\n",
    "            else: #single task dictionary\n",
    "                df_perf_dict_opt[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "\n",
    "            idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results at: /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/\n"
     ]
    }
   ],
   "source": [
    "#Save df style dictionaly for seaborn plots\n",
    "cohort = 'ADNI2'\n",
    "update = 1\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_up_{}.pkl'.format(exp_name, cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse,df_perf_dict_path)\n",
    "\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_tuned_up_{}.pkl'.format(exp_name,cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas_tuned,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE_tuned_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse_tuned,df_perf_dict_path)\n",
    "\n",
    "\n",
    "print 'saving results at: {}'.format(CV_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 25)\n",
    "n_rows = n_folds\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "plot_perf = False\n",
    "\n",
    "if multi_task:\n",
    "    euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "    corrs = [fold_r_adas13,fold_r_mmse]\n",
    "else:\n",
    "    euLosses = [fold_euLoss]\n",
    "    corrs = [fold_r]\n",
    "    \n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        \n",
    "        if plot_perf:\n",
    "            plt.figure(fid)\n",
    "            plt.subplot(n_rows,n_cols,1)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "            plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "            plt.legend()\n",
    "            plt.subplot(n_rows,n_cols,2)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "            plt.title('correlation, fid: {}'.format(fid))\n",
    "            plt.legend(loc=2)\n",
    "\n",
    "print preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path):\n",
    "    fold_r_dict = {}\n",
    "    fold_euLoss_dict = {}\n",
    "    fold_act_scores_dict = {}\n",
    "    fold_pred_scores_dict = {}\n",
    "\n",
    "    if multi_task:\n",
    "        fold_r_dict['ADAS'] = fold_perf_dict['fold_r_adas13']\n",
    "        fold_r_dict['MMSE'] = fold_perf_dict['fold_r_mmse']\n",
    "        fold_euLoss_dict['ADAS'] = fold_perf_dict['fold_euLoss_adas13']\n",
    "        fold_euLoss_dict['MMSE'] = fold_perf_dict['fold_euLoss_mmse']\n",
    "        fold_act_scores_dict['ADAS'] = fold_perf_dict['fold_act_scores_adas13']\n",
    "        fold_act_scores_dict['MMSE'] = fold_perf_dict['fold_act_scores_mmse']\n",
    "        fold_pred_scores_dict['ADAS'] = fold_perf_dict['fold_pred_scores_adas13']\n",
    "        fold_pred_scores_dict['MMSE'] = fold_perf_dict['fold_pred_scores_mmse']\n",
    "        \n",
    "        NN_results = computePerfMetrics(fold_r_dict, fold_euLoss_dict, fold_act_scores_dict, fold_pred_scores_dict, opt_metric, hype_configs, \n",
    "                                        Clinical_Scale,task_weights)\n",
    "        adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "        mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "        adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "        mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "        adas_rmse = NN_results['opt_ADAS']['CV_RMSE'].values()\n",
    "        mmse_rmse = NN_results['opt_MMSE']['CV_RMSE'].values()\n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['opt_ADAS']['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['opt_ADAS']['actual_CV_scores']\n",
    "        \n",
    "        print 'ADAS corr: {}'.format(adas_r)\n",
    "        print 'ADAS mse: {}'.format(adas_mse)\n",
    "        print 'ADAS means: {}, {}'.format(np.mean(adas_r),np.mean(adas_mse))\n",
    "        print ''\n",
    "        print 'MMSE corr: {}'.format(mmse_r)\n",
    "        print 'MMSE mse: {}'.format(mmse_mse)\n",
    "        print 'MMSE means: {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse))\n",
    "        print ''\n",
    "        print 'opt_hyp: {}'.format(opt_hyp)\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        \n",
    "        return {'adas_r':adas_r, 'adas_mse':adas_mse, 'adas_rmse':adas_rmse, 'mmse_r':mmse_r, 'mmse_mse':mmse_mse, 'mmse_rmse':mmse_rmse,\n",
    "               'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        NN_results = computeSingleTaskPerfMetrics(fold_perf_dict, opt_metric, hype_configs)\n",
    "        opt_r = NN_results['opt_r'].values()        \n",
    "        opt_mse = NN_results['opt_mse'].values()        \n",
    "        opt_rmse = NN_results['opt_rmse'].values()        \n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['actual_CV_scores']\n",
    "        print 'opt corr: {}'.format(opt_r)\n",
    "        print 'opt mse: {}'.format(opt_mse)\n",
    "        print 'means: {}, {}'.format(np.mean(opt_r),np.mean(opt_mse))\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        print ''\n",
    "    \n",
    "        return {'opt_r':opt_r, 'opt_mse':opt_mse, 'opt_rmse':opt_rmse,'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    \n",
    "\n",
    "\n",
    "    if save_multitask_results:\n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')        \n",
    "        save_path = save_path + '_' + st + '.pkl' \n",
    "        print 'saving results at: {}'.format(save_path)\n",
    "        output = open(save_path, 'wb')\n",
    "        pickle.dump(NN_results, output)\n",
    "        output.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predicted_CV_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6f668dc9c41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted_CV_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual_CV_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predicted_CV_scores'"
     ]
    }
   ],
   "source": [
    "a = np.squeeze(results['predicted_CV_scores'][7])\n",
    "b = np.squeeze(results['actual_CV_scores'][7])\n",
    "\n",
    "print a , b                               \n",
    "print stats.pearsonr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    \n",
    "     # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "\n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    opt_rmse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "\n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    opt_rmse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "\n",
    "    print 'Clinical_Scale: {}'.format(Clinical_Scale)\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        print 'This function does not work for single clinical scale models. Use the code from the next cell'\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "\n",
    "        fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "        fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "        fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "        fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "\n",
    "        for fid in fid_hype_map.keys():\n",
    "            r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "            r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "            r_perf_array = np.array(fid_r_perf[fid])\n",
    "            euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "            euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "            euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "            if opt_metric == 'euLoss':\n",
    "                h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "            else:\n",
    "                h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "\n",
    "            opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "            opt_snap[fid] = snp\n",
    "\n",
    "            opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "            opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "            opt_rmse_adas[fid] = np.sqrt(2*euLoss_perf_array_adas[h,snp]) #RMSE\n",
    "            opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "            opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "            opt_rmse_mmse[fid] = np.sqrt(2*euLoss_perf_array_mmse[h,snp]) #RMSE\n",
    "\n",
    "            actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "            opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "            actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "            opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "        opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'CV_RMSE':opt_rmse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "        opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'CV_RMSE':opt_rmse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_snap':opt_snap, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "def computeSingleTaskPerfMetrics(fold_perf_dict,opt_metric,hype_configs):\n",
    "    corrs = fold_perf_dict['fold_r']\n",
    "    euLosses = fold_perf_dict['fold_euLoss']\n",
    "    act_scores = fold_perf_dict['fold_act_scores']\n",
    "    pred_scores = fold_perf_dict['fold_pred_scores']\n",
    "\n",
    "    \n",
    "    NN_multitask_results = {}\n",
    "   \n",
    "    snap_array = np.arange(snap_start,niter+1,snap_start)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = {}\n",
    "    opt_mse = {}\n",
    "    opt_rmse = {}\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "    actual_scores = {}\n",
    "    opt_pred_scores = {}\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "        # if want to find best hyp from mse values\n",
    "        if opt_metric == 'euLoss':\n",
    "            h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        else:\n",
    "            h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "            \n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        r = r_perf_array[h,snp]        \n",
    "        opt_r[fid] = r\n",
    "        opt_mse[fid] = 2*eu_loss\n",
    "        opt_rmse[fid] = np.sqrt(2*eu_loss)\n",
    "        #print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)        \n",
    "        opt_snap[fid] = snp\n",
    "        opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "        actual_scores[fid] = fold_act_scores[fid]\n",
    "        opt_pred_scores[fid] = fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp]\n",
    "\n",
    "    #print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r.values()), np.mean(opt_mse.values()))\n",
    "    #print opt_r\n",
    "    #print opt_mse\n",
    "    return {'opt_r':opt_r,'opt_mse':opt_mse,'opt_rmse':opt_rmse,'opt_hype':opt_hype,'opt_snap':opt_snap, 'actual_scores':actual_scores,'opt_pred_scores':opt_pred_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "\n",
    "model_file = 'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-05-06-17-14-49.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {4:0,5:1}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_ADAS13_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(5.64658243068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update fold performance for multitask\n",
    "print NN_results['opt_ADAS'].keys()\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "#model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl'\n",
    "model_file_soa = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-23-52.pkl'\n",
    "print 'current state of art: ' + model_file_soa\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file_soa,'rb'))\n",
    "\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "#load old file\n",
    "# model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-02-48.pkl'\n",
    "# print 'updated fold result file: ' + model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "idx_pairs = {1:1,3:3}\n",
    "CV_data = update_multifold_perf(boxplots_dir + model_file_soa, NN_results, idx_pairs)\n",
    "\n",
    "print \"\"\n",
    "print 'updated CV_data'\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "#print 'saving results at: {}'.format(save_name)\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(CV_data, output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():        \n",
    "        for idx in idx_pairs.keys():            \n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "\n",
    "    return CV_data\n",
    "\n",
    "def update_multifold_perf(saved_perf_file, new_perf_dict, idx_pairs):    \n",
    "    perf_metrics = ['CV_r', 'actual_CV_scores', 'CV_MSE', 'predicted_CV_scores']    \n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        if key == 'opt_hype':\n",
    "            for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                CV_data[key][idx_key] = new_perf_dict[key][idx_val]\n",
    "                \n",
    "        else:\n",
    "            for pm in perf_metrics:\n",
    "                for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                    CV_data[key][pm][idx_key] = new_perf_dict[key][pm][idx_val]\n",
    "    \n",
    "    return CV_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "model_files = ['Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl',             \n",
    "             'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-05-23-07-31-29.pkl']\n",
    "\n",
    "#'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-03-19-29-16_Fold9_10.pkl'\n",
    "\n",
    "Clinical_Scale = 'ADAS'\n",
    "perf_array = []\n",
    "for mf in model_files:\n",
    "    CV_data = pickle.load(open(boxplots_dir + mf,'rb'))['opt_{}'.format(Clinical_Scale)]        \n",
    "    perf_array.append(CV_data['CV_r'].values())\n",
    "\n",
    "print (np.max(np.array(perf_array),axis=0))\n",
    "print np.mean(np.max(np.array(perf_array),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "\n",
    "\n",
    "save_detailed_results = True\n",
    "multi_task = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results    \n",
    "    if not multi_task:\n",
    "        NN_results = {}\n",
    "        NN_results['CV_MSE'] = opt_mse\n",
    "        NN_results['CV_r'] = opt_r\n",
    "        NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "        NN_results['actual_CV_scores'] = actual_scores\n",
    "        NN_results['tid_snap_config_dict'] = opt_hype\n",
    "        print opt_r\n",
    "        print opt_mse\n",
    "        \n",
    "    else:\n",
    "        NN_results = NN_multitask_results\n",
    "        print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "        print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "        print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "        print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])\n",
    "        \n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_BOTH_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    for modality in modalities:\n",
    "        in_data = load_data(in_data_path, 'Fold_{}_{}'.format(fid,modality), preproc)         \n",
    "\n",
    "        # HDF5 is pretty efficient, but can be further compressed.\n",
    "        comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "        with h5py.File(out_data_path, 'a') as f:      \n",
    "            if modality == 'X_R_CT': #Fix the typo            \n",
    "                modality = 'X_CT'            \n",
    "\n",
    "            f.create_dataset('{}'.format(modality), data=in_data, **comp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp6'\n",
    "exp_name_out = 'Exp6_MC'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "modalities = ['X_L_HC','X_R_HC','X_R_CT','adas','mmse','dx']\n",
    "dataset = 'ADNI1'\n",
    "preproc = 'no_preproc' #binary labels\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "\n",
    "for mc in np.arange(1,3,1):\n",
    "    for fid in np.arange(1,11,1):\n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_train_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_valid_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_ADAS13_NN_valid_MC_{}.h5'.format(exp_name,dataset,mc)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name_out)\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC: fid[1:4] 8000; fid[5,6] 6000, fid[7,8,9]: 8000 fid 10: 6000\n",
    "CT: fid[1:5] 12000 fid[6:10] 6000\n",
    "\n",
    "#spawn net\n",
    "ADNI_FF_train_hyp1_CT.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n"
     ]
    }
   ],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "cohort = 'ADNI2'\n",
    "modality = 'HC_CT'\n",
    "pretrain_snap_HC = 20000 #ADNI1: 5000\n",
    "pretrain_snap_CT = 20000 #ADNI1: 5000\n",
    "n_folds = 10\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/' \n",
    "#Custom snaps per fold\n",
    "#HC_iter = [8000,8000,8000,8000,6000,6000,8000,8000,8000,6000]\n",
    "#CT_iter = [12000,12000,12000,12000,12000,6000,6000,6000,6000,6000]\n",
    "#mc=1\n",
    "hc_hyp = 'hyp1'\n",
    "ct_hyp = 'hyp1'\n",
    "pretrain_hyp = 'hyp2' #This is hyp generated using optimal combination of hc and ct hyps. Prototxt file is saved with this hyp extension\n",
    "\n",
    "exp_name = 'Exp11_MC'\n",
    "\n",
    "for mc in np.arange(6,11,1):\n",
    "    for fid in np.arange(1,n_folds+1,1):\n",
    "        print 'fid: {}'.format(fid)\n",
    "        #for AE_branch in ['CT','R_HC','L_HC']:\n",
    "        for AE_branch in ['CT','HC']:\n",
    "            print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "            if AE_branch == 'L_HC':\n",
    "                params_FF = ['L_ff1', 'L_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'R_HC':\n",
    "                params_FF = ['R_ff1', 'R_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'HC':\n",
    "                params_FF = ['L_ff1','R_ff1']\n",
    "                AE_iter = pretrain_snap_HC\n",
    "                hyp = hc_hyp\n",
    "            elif AE_branch == 'CT':\n",
    "                #params_FF = ['ff1', 'ff2']\n",
    "                params_FF = ['ff1']\n",
    "                AE_iter = pretrain_snap_CT\n",
    "                hyp = ct_hyp\n",
    "                #fid for pretain is 1 because it's same definition for all the folds.\n",
    "                #Only use this during 1 of the modalities to avoid overwritting\n",
    "                print 'Spawning new net'\n",
    "                pretrain_net = caffe.Net(baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,pretrain_hyp,modality), caffe.TRAIN)\n",
    "            else:\n",
    "                print 'Wrong AE branch'\n",
    "\n",
    "            # conv_params = {name: (weights, biases)}\n",
    "            conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "            for conv in params_FF:\n",
    "                print 'target {} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "            # Review AE net params \n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hyp,AE_branch)\n",
    "            #model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "            model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hyp,AE_branch,AE_iter) \n",
    "\n",
    "            AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "            #params_AE = ['encoder1', 'code']\n",
    "            params_AE = params_FF #if you are using pretrained NN\n",
    "\n",
    "            # fc_params = {name: (weights, biases)}\n",
    "            fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "            for fc in params_AE:\n",
    "                print 'pretrained {} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "            #transplant net parameters\n",
    "            for pr, pr_conv in zip(params_AE, params_FF):\n",
    "                conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "                conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "            save_net = True\n",
    "            if save_net:\n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_{}_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                save_path = baseline_dir + net_name.format(mc,fid,cohort,pretrain_hyp,modality,pretrain_snap_HC,pretrain_snap_CT)\n",
    "                print \"Saving net to \" + save_path\n",
    "                pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_data['opt_ADAS']['CV_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print n_snaps/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
