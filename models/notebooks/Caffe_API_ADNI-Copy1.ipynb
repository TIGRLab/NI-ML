{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import caffe\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(1)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.y  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "#    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "#    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=0.5))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=0.5))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,concat_param=dict(axis=1))\n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='gaussian',std=0.177))    \n",
    "    n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))    \n",
    "    #n.accuracy = L.Accuracy(n.ip1, n.label)\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y)    \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y3)              \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT,n.y  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "    n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff3 = L.InnerProduct(n.ff2, num_output=node_sizes['ff2'], param=dict(lr_mult=2), weight_filler=dict(type='xavier'))\n",
    "    #n.NLinEnL3 = L.ReLU(n.ff3, in_place=True)\n",
    "    #n.dropL = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff2'], param=dict(lr_mult=2), weight_filler=dict(type='xavier'))\n",
    "    #n.NLinEnL4 = L.ReLU(n.ff4, in_place=True)\n",
    "    #n.dropL4 = L.Dropout(n.ff4, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "\n",
    "    #--------Output--------------#\n",
    "    n.output = L.InnerProduct(n.ff2, num_output=1, param=dict(lr_mult=1), weight_filler=dict(type='xavier'))    \n",
    "    #n.accuracy = L.Accuracy(n.ip1, n.label)\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y)    \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y3)    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.y  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.y,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "#     n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,n.ff1, concat_param=dict(axis=1))\n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    #n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "  \n",
    "    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2) #This is done automatically by caffe! \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        #n.ff2_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2ADAS13 = L.ReLU(n.ff2_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        #n.ff2_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2MMSE = L.ReLU(n.ff2_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff4, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "\n",
    "\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.y,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.y3,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder1 = L.InnerProduct(n.X_CT, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers  \n",
    "    n.NLinEn1 = L.ReLU(n.encoder1, in_place=True)\n",
    "    #n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "    #n.NLinEn2 = L.Sigmoid(n.encoder2, in_place=True)\n",
    "    #code layer\n",
    "    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=0.177))  \n",
    "    #Decoder layers\n",
    "    n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "    #n.decoder2 = L.InnerProduct(n.decoder1, num_output=5000, weight_filler=dict(type='xavier'))\n",
    "    #n.NlinDe2 = L.Sigmoid(n.decoder2, in_place=True)\n",
    "    \n",
    "    n.output = L.InnerProduct(n.decoder1, num_output=node_sizes['out'], weight_filler=dict(type='gaussian',std=0.177))    \n",
    "    #n.accuracy = L.Accuracy(n.ip1, n.label)\n",
    "    if modality == 'CT':\n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)        \n",
    "    elif modality =='R_HC':\n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    return n.to_proto()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,multi_task):\n",
    "\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 100\n",
    "    #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "    # losses will also be stored in the log\n",
    "    if not multi_task:\n",
    "        train_loss = zeros(niter)\n",
    "        test_loss = zeros(int(np.ceil(niter / test_interval)))        \n",
    "    else: \n",
    "        train_ADAS13_loss = zeros(niter)\n",
    "        test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_MMSE_loss = zeros(niter)\n",
    "        test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "    #output = zeros((niter, batch_size))\n",
    "    #solver.restore()\n",
    "    #the main solver loop\n",
    "    for it in range(niter):\n",
    "        #solver.net.forward()\n",
    "        solver.step(1)  # SGD by Caffe    \n",
    "        # store the train loss\n",
    "        if not multi_task:\n",
    "            train_loss[it] = solver.net.blobs['loss'].data        \n",
    "        else: \n",
    "            train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "            train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "        # store the output on the first test batch\n",
    "        # (start the forward pass at conv1 to avoid loading new data)\n",
    "        #solver.test_nets[0].forward()\n",
    "        #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "        # run a full test every so often\n",
    "        # (Caffe can also do this for us and write to a log, but we show here\n",
    "        #  how to do it directly in Python, where more complicated things are easier.)\n",
    "        if it % test_interval == 0:        \n",
    "            t_loss = 0\n",
    "            t_ADAS13_loss = 0\n",
    "            t_MMSE_loss = 0\n",
    "            for test_it in range(test_iter):\n",
    "                solver.test_nets[0].forward()\n",
    "                if not multi_task:\n",
    "                    t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                else: \n",
    "                    t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                    t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "            if not multi_task:\n",
    "                test_loss[it // test_interval] = t_loss/test_iter            \n",
    "                print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                    it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval])\n",
    "            else:\n",
    "                test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/test_iter\n",
    "                test_MMSE_loss[it // test_interval] = t_MMSE_loss/test_iter            \n",
    "                print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                    it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                    it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "    if not multi_task:\n",
    "        perf = {'train_loss':[train_loss],'test_loss':[test_loss]}\n",
    "    else:\n",
    "        perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,fid,exp_name,modality):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "\n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    s.type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 200000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 5000\n",
    "    s.snapshot_prefix = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}'.format(fid,exp_name,hype,modality)\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate API style txt files\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'HC_CT'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "start_fold = 4\n",
    "n_folds = 5\n",
    "niter = 60000\n",
    "#node_sizes = {'L_ff1':5000,'R_ff1':5000,'L_ff2':500,'R_ff2':500,'ff1':500,'ff2':100,'ff3':50,'ff4':50}\n",
    "#node_sizes = {'En1':500,'code':100,'out':686}\n",
    "#L,R: 16086,16471\n",
    "pretrain = False\n",
    "load_pretrained_weights = False\n",
    "\n",
    "if Clinical_Scale == 'BOTH':\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "# dr: Dropout rate, lr: learning rate of each modality, tr: loss-weight for each task\n",
    "hype_configs = {\n",
    "                'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':50,'COMB_ff':50,'ADAS_ff':25,'MMSE_ff':25},\n",
    "                       'dr':{'HC':0,'CT':0},'lr':{'HC':1,'CT':5},'tr':{'ADAS':1,'MMSE':5},\n",
    "                        'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-4}},\n",
    "                'hyp2':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':100,'COMB_ff':100,'ADAS_ff':25,'MMSE_ff':25},\n",
    "                       'dr':{'HC':0,'CT':0},'lr':{'HC':1,'CT':5},'tr':{'ADAS':1,'MMSE':5},\n",
    "                        'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-4}},\n",
    "                'hyp3':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'COMB_ff':50,'ADAS_ff':25,'MMSE_ff':25},\n",
    "                       'dr':{'HC':0,'CT':0},'lr':{'HC':1,'CT':5},'tr':{'ADAS':1,'MMSE':5},\n",
    "                        'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-4}},\n",
    "                'hyp4':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':100,'COMB_ff':100,'ADAS_ff':25,'MMSE_ff':25},\n",
    "                       'dr':{'HC':0,'CT':0},'lr':{'HC':1,'CT':5},'tr':{'ADAS':1,'MMSE':5},\n",
    "                        'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-4}},\n",
    "                'hyp5':{'node_sizes':{'HC_L_ff':100,'HC_R_ff':100,'CT_ff':50,'COMB_ff':50,'ADAS_ff':25,'MMSE_ff':25},\n",
    "                       'dr':{'HC':0,'CT':0},'lr':{'HC':1,'CT':2},'tr':{'ADAS':1,'MMSE':5},\n",
    "                        'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-4}},\n",
    "                'hyp6':{'node_sizes':{'HC_L_ff':100,'HC_R_ff':100,'CT_ff':50,'COMB_ff':100,'ADAS_ff':25,'MMSE_ff':25},\n",
    "                       'dr':{'HC':0,'CT':0},'lr':{'HC':1,'CT':2},'tr':{'ADAS':1,'MMSE':5},\n",
    "                        'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-4}},\n",
    "                'hyp7':{'node_sizes':{'HC_L_ff':200,'HC_R_ff':200,'CT_ff':50,'COMB_ff':50,'ADAS_ff':25,'MMSE_ff':25},\n",
    "                       'dr':{'HC':0,'CT':0},'lr':{'HC':1,'CT':2},'tr':{'ADAS':1,'MMSE':5},\n",
    "                        'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-4}},\n",
    "                'hyp8':{'node_sizes':{'HC_L_ff':200,'HC_R_ff':200,'CT_ff':100,'COMB_ff':100,'ADAS_ff':25,'MMSE_ff':25},\n",
    "                       'dr':{'HC':0,'CT':0},'lr':{'HC':1,'CT':2},'tr':{'ADAS':1,'MMSE':5},\n",
    "                        'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-4}}\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "CV_perf_hype = {}\n",
    "for hype in hype_configs.keys():    \n",
    "    node_sizes = hype_configs[hype]['node_sizes']\n",
    "    dr = hype_configs[hype]['dr']\n",
    "    lr = hype_configs[hype]['lr']\n",
    "    tr = hype_configs[hype]['tr']\n",
    "    solver_configs = hype_configs[hype]['solver_conf']\n",
    "    \n",
    "    CV_perf = {}\n",
    "    for fid in np.arange(start_fold,n_folds+1,1):\n",
    "        print 'Hype # {}, Fold # {}'.format(hype, fid)\n",
    "        train_filename_txt = baseline_dir + 'API/data/fold{}/train_C688.txt'.format(fid)\n",
    "        test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "\n",
    "        train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "        test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "        with open(train_filename_txt, 'w') as f:\n",
    "                f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "        with open(test_filename_txt, 'w') as f:\n",
    "                f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "        # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "        if pretrain:\n",
    "            train_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_train.prototxt'.format(fid)\n",
    "            with open(train_net_path, 'w') as f:\n",
    "                f.write(str(adninet_ae(train_filename_txt, 256, node_sizes, modality)))            \n",
    "        else:\n",
    "            train_net_path = baseline_dir + 'API/data/fold{}/ADNI_FF_train.prototxt'.format(fid)\n",
    "            with open(train_net_path, 'w') as f:            \n",
    "                if modality == 'HC':\n",
    "                      f.write(str(adninet_ff_HC(train_filename_txt, 256, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                elif modality == 'CT':\n",
    "                      f.write(str(adninet_ff_CT(train_filename_txt, 256, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                elif modality == 'HC_CT':\n",
    "                      f.write(str(adninet_ff_HC_CT(train_filename_txt, 256, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                else:\n",
    "                      print 'Wrong modality'\n",
    "\n",
    "        if pretrain:\n",
    "            test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "        else:\n",
    "            test_net_path = baseline_dir + 'API/data/fold{}/ADNI_FF_test.prototxt'.format(fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                      f.write(str(adninet_ff_HC(test_filename_txt, 256, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                elif modality == 'CT':\n",
    "                      f.write(str(adninet_ff_CT(test_filename_txt, 256, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                elif modality == 'HC_CT':\n",
    "                      f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                else:\n",
    "                      print 'Wrong modality'\n",
    "\n",
    "        # Define Solver\n",
    "        solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "        with open(solver_path, 'w') as f:\n",
    "            f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, fid, exp_name, modality)))\n",
    "\n",
    "        ### load the solver and create train and test nets\n",
    "        #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "        solver = caffe.get_solver(solver_path)\n",
    "\n",
    "        if load_pretrained_weights:    \n",
    "            #snap_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_HC_CT_iter_10000_concat50.caffemodel'.format(fid)\n",
    "            snap_path = baseline_dir + 'API/data/fold{}/train_snaps/Exp11_{}_HC_CT_iter_30000.caffemodel'.format(fid,hype)\n",
    "            print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "            solver.net.copy_from(snap_path)\n",
    "\n",
    "        #run caffe\n",
    "        results = run_caffe(solver,niter,multi_task)\n",
    "        CV_perf[fid] = results\n",
    "\n",
    "    CV_perf_hype[hype] = CV_perf\n",
    "# in case the kernel dies... \n",
    "pickleIt(CV_perf_hype, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modality = 'HC_CT'\n",
    "# pkl_file = open(baseline_dir + 'API/CV_perf/{}'.format(modality), 'rb')\n",
    "# CV_perf = pickle.load(pkl_file) \n",
    "# pkl_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 30)\n",
    "niter = 30000\n",
    "test_interval = 500\n",
    "n_folds = 1\n",
    "n_hype_configs = len(hype_configs.keys())\n",
    "pid = 1\n",
    "for hype in CV_perf_hype.keys(): \n",
    "    CV_perf = CV_perf_hype[hype]\n",
    "    n_CV_configs = len(CV_perf)\n",
    "    for fid in [1]: #np.arange(start_fold,n_folds+1,1):        \n",
    "        train_loss_list = CV_perf[fid]['train_loss']\n",
    "        test_loss_list = CV_perf[fid]['test_loss']\n",
    "        \n",
    "        for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "            ax1 = plt.subplot(n_hype_configs,n_CV_configs,pid)\n",
    "            #ax2 = ax1.twinx()\n",
    "            ax1.plot(arange(niter), train_loss, label='train')\n",
    "            ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test', linewidth='3')\n",
    "            ax1.set_xlabel('iteration')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_title('fid: {} Test loss: {:.2f}'.format(fid, test_loss[-1]))\n",
    "            ax1.legend(loc=1)\n",
    "            #ax1.set_ylim(0,50)\n",
    "            pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "#exp_name = 'Exp11'\n",
    "#niter = 50000\n",
    "modality = 'HC_CT'\n",
    "start_fold = 1\n",
    "n_folds = 10\n",
    "preproc = 'no_preproc'\n",
    "batch_size = 256\n",
    "snap_interval = 5000\n",
    "snap_start = 5000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "Clinical_Scale = 'BOTH'\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "print exp_name, modality\n",
    "\n",
    "for hype in hype_configs.keys():      \n",
    "    node_sizes = hype_configs[hype]['node_sizes']\n",
    "    dr = hype_configs[hype]['dr']\n",
    "    lr = hype_configs[hype]['lr']\n",
    "    \n",
    "    for fid in [9,10]: #np.arange(start_fold,n_folds+1,1):\n",
    "        test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "        #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "        #with open(test_filename_txt, 'w') as f:\n",
    "        #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "        test_net_path = baseline_dir + 'API/data/fold{}/ADNI_FF_test.prototxt'.format(fid)\n",
    "        with open(test_net_path, 'w') as f:\n",
    "            if modality == 'HC':\n",
    "                f.write(str(adninet_ff_HC(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                input_nodes = ['X_L_HC','X_R_HC']\n",
    "            elif modality == 'CT':\n",
    "                f.write(str(adninet_ff_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                input_nodes = ['X_CT']\n",
    "            elif modality == 'HC_CT':\n",
    "                f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "            else:\n",
    "                print 'Wrong modality'\n",
    "    \n",
    "        print 'Hype # {}, Fold # {}'.format(hype, fid)\n",
    "        data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "        if Clinical_Scale == 'ADAS13':\n",
    "            act_scores = load_data(data_path, 'y','no_preproc')\n",
    "        elif Clinical_Scale == 'MMSE': \n",
    "            act_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        elif Clinical_Scale == 'BOTH':\n",
    "            act_scores_adas13 = load_data(data_path, 'y','no_preproc')\n",
    "            act_scores_mmse = load_data(data_path, 'y3','no_preproc')\n",
    "        else:\n",
    "            print 'unknown clinical scale'\n",
    "        \n",
    "        net_file = baseline_dir + 'API/data/fold{}/ADNI_FF_test.prototxt'.format(fid)\n",
    "        test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "        test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "        with open(test_filename_txt, 'w') as f:\n",
    "                f.write(test_filename_hdf + '\\n')  \n",
    "        \n",
    "        sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "        #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "        #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "        \n",
    "        if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "            multi_task = False\n",
    "            iter_euLoss = []\n",
    "            iter_r = []        \n",
    "            iter_pred_scores = []\n",
    "            for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "                results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers)\n",
    "                encodings = np.squeeze(results['X_out'])            \n",
    "                iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "            config_idx = '{}_{}'.format(hype,fid)\n",
    "            fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "            fold_r[config_idx] = np.array(iter_r)\n",
    "            fold_act_scores[fid] = act_scores\n",
    "            fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "        \n",
    "        elif Clinical_Scale == 'BOTH':\n",
    "            multi_task = True\n",
    "            iter_euLoss_adas13 = []\n",
    "            iter_r_adas13 = []        \n",
    "            iter_pred_scores_adas13 = []\n",
    "            iter_euLoss_mmse = []\n",
    "            iter_r_mmse = []        \n",
    "            iter_pred_scores_mmse = []\n",
    "            \n",
    "            for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "                results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                encodings = results['X_out']   \n",
    "                encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "            config_idx = '{}_{}'.format(hype,fid)\n",
    "            fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "            fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "            fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "            fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "            fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "            fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "            fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "            fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 5)\n",
    "n_rows = 1\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "corrs = [fold_r_adas13,fold_r_mmse]\n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        plt.figure(fid)\n",
    "        plt.subplot(n_rows,n_cols,1)\n",
    "        plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "        plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "        plt.legend()\n",
    "        plt.subplot(n_rows,n_cols,2)\n",
    "        plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "        plt.title('correlation, fid: {}'.format(fid))\n",
    "        plt.legend(loc=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_r = {}\n",
    "fold_euLoss = {}\n",
    "fold_r['ADAS'] = fold_r_adas13\n",
    "fold_r['MMSE'] = fold_r_mmse\n",
    "fold_euLoss['ADAS'] = fold_euLoss_adas13\n",
    "fold_euLoss['MMSE'] = fold_euLoss_mmse\n",
    "fold_act_scores['ADAS'] = fold_act_scores_adas13\n",
    "fold_act_scores['MMSE'] = fold_act_scores_mmse\n",
    "fold_pred_scores['ADAS'] = fold_pred_scores_adas13\n",
    "fold_pred_scores['MMSE'] = fold_pred_scores_mmse\n",
    "opt_metric = 'corr'\n",
    "\n",
    "Clinical_Scale = 'BOTH'\n",
    "task_weights = {'ADAS':1,'MMSE':1}\n",
    "NN_results = computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights)\n",
    "\n",
    "adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "\n",
    "\n",
    "print 'ADAS corr: {}'.format(adas_r)\n",
    "print 'ADAS mse: {}'.format(adas_mse)\n",
    "print 'ADAS means: {}, {}'.format(np.mean(adas_r),np.mean(adas_mse))\n",
    "print ''\n",
    "print 'MMSE corr: {}'.format(mmse_r)\n",
    "print 'MMSE mse: {}'.format(mmse_mse)\n",
    "print 'MMSE means: {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse))\n",
    "print ''\n",
    "#print opt_configs['opt_hype']\n",
    "\n",
    "save_multitask_results = False\n",
    "if save_multitask_results:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp14_ADNI1and2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " ADAS corr: [0.62653468489339437, 0.57398790326499494, 0.56841732449558113, 0.6740394351486253]\n",
    "ADAS mse: [47.755783137161401, 56.548712674040083, 65.702452953670502, 48.480179709367462]\n",
    "ADAS means: 0.610744836951, 54.6217821186\n",
    "\n",
    "MMSE corr: [0.46910818651842767, 0.48950483315512056, 0.35777695700423851, 0.51244971266314343]\n",
    "MMSE mse: [5.4762197482116912, 5.7244238242647798, 6.8451252678110848, 6.3989595163666895]\n",
    "MMSE means: 0.457209922335, 6.11118208916\n",
    "\n",
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        for hype_fid in fold_euLoss.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "            \n",
    "    \n",
    "    # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "    \n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "    \n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "    \n",
    "    fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "    fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "    fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "    fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "    \n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "        r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "        euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "        \n",
    "        if opt_metric == 'euLoss':\n",
    "            h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        else:\n",
    "            h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "        \n",
    "        opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "        \n",
    "        opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "        opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "        opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "        opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "        \n",
    "        actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "        opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "        actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "        opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "    opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "    opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "corrs = [fold_r_adas13,fold_r_mmse]\n",
    "act_scores = [fold_act_scores_adas13, fold_act_scores_mmse]\n",
    "pred_scores = [fold_pred_scores_adas13,fold_pred_scores_mmse]\n",
    "Clinical_Scales = ['ADAS13', 'MMSE']\n",
    "NN_multitask_results = {}\n",
    "for Clinical_Scale, fold_euLoss, fold_r, fold_act_scores, fold_pred_scores in zip(Clinical_Scales, euLosses, corrs, act_scores, pred_scores):\n",
    "    snap_array = np.arange(snap_start,niter+1,snap_interval)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = []\n",
    "    opt_mse = []\n",
    "    opt_hype = []\n",
    "    actual_scores = []\n",
    "    opt_pred_scores = []\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "        h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        opt_mse.append(2*eu_loss)\n",
    "        print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)\n",
    "        h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "        r = r_perf_array[h,snp]\n",
    "        opt_r.append(r)\n",
    "        opt_hype.append(hype_configs['hyp{}'.format(fid_hype_map[fid][h])])\n",
    "        print 'fid:{}, best hype:{}, snap: {}, r:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],r)\n",
    "        actual_scores.append(fold_act_scores[fid])  \n",
    "        opt_pred_scores.append(fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "    print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r), np.mean(opt_mse))\n",
    "    print opt_r\n",
    "    print opt_mse\n",
    "    NN_multitask_results[Clinical_Scale] = {'opt_r':opt_r,'opt_mse':opt_mse,'opt_hype':opt_hype,\n",
    "                                            'actual_scores':actual_scores,'opt_pred_scores':opt_pred_scores}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-10-27-02.pkl\n",
    "# old CV_r: [0.58713291550558833, 0.74574527911517141, 0.71271529005993939, 0.71772651594819259, 0.68488215732574409, \n",
    "#            0.64485628063795464, 0.59854968077229798, 0.6848414923548054, 0.60031497549309831, 0.57364714888913393]\n",
    "# Exp11_ADNI2_MMSE_NN_HC_CT_2016-05-15-19-59-42.pkl\n",
    "# old CV_r: [0.48038349554607179, 0.60131716074726682, 0.53198020733649787, 0.64770981175015141, 0.56547528703482142,\n",
    "#            0.52085739989088009, 0.65811053610310188, 0.57630405337929402, 0.51059593904907996, 0.48178577037836778]\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "# model_files = ['Exp7_ADNI2_ADAS13_NN_HC_CT_2016-05-01-13-46-16.pkl',\n",
    "#                'Exp7_ADNI2_ADAS13_NN_HC_CT_2016-05-02-10-29-55.pkl', 'Exp7_ADNI2_ADAS13_NN_HC_CT_2016-05-03-19-02-13.pkl']\n",
    "\n",
    "model_file = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-10-27-02.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {0:0,2:2}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp13_ADNI1and2_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        for idx in idx_pairs.keys():\n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "        \n",
    "    return CV_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "\n",
    "\n",
    "save_detailed_results = True\n",
    "multi_task = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results    \n",
    "    if not multi_task:\n",
    "        NN_results = {}\n",
    "        NN_results['CV_MSE'] = opt_mse\n",
    "        NN_results['CV_r'] = opt_r\n",
    "        NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "        NN_results['actual_CV_scores'] = actual_scores\n",
    "        NN_results['tid_snap_config_dict'] = opt_hype\n",
    "        print opt_r\n",
    "        print opt_mse\n",
    "        \n",
    "    else:\n",
    "        NN_results = NN_multitask_results\n",
    "        print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "        print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "        print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "        print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])\n",
    "        \n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_BOTH_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some defs to load data and extract encodings from trained net\n",
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    print net.blobs.items()[0]\n",
    "    print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        data_layers.append(net.blobs.items()[i][1])    \n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "    \n",
    "    net.reshape()            \n",
    "    print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}\n",
    "\n",
    "def generate_APIdata(in_data_path,out_data_path,fid,modality,preproc, CS_only):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    in_data = load_data(in_data_path, 'Fold_{}_X_{}'.format(fid,modality), preproc)\n",
    "    # get labels (no_preproc)        \n",
    "\n",
    "    # HDF5 is pretty efficient, but can be further compressed.\n",
    "    comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "    with h5py.File(out_data_path, 'a') as f:\n",
    "        if not CS_only:\n",
    "            if modality == 'R_CT': #Fix the typo\n",
    "                #Because we have separate ADNI1+2 cohort for ADAS13 no need to create 'y'\n",
    "                in_label = load_data(in_data_path, 'Fold_{}_y3'.format(fid), 'no_preproc') \n",
    "                modality = 'CT'\n",
    "                f.create_dataset('y3', data=in_label, **comp_kwargs)\n",
    "\n",
    "            f.create_dataset('X_{}'.format(modality), data=in_data, **comp_kwargs)\n",
    "        else:\n",
    "            if modality == 'R_CT': #Fix the typo\n",
    "                in_label = load_data(in_data_path, 'Fold_{}_y3'.format(fid), 'no_preproc')\n",
    "                f.create_dataset('y3', data=in_label, **comp_kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp13'\n",
    "exp_name_out = 'Exp13'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "preproc = 'no_preproc'\n",
    "modalities = ['L_HC','R_HC','R_CT']\n",
    "dataset = 'ADNI1and2'\n",
    "Clinical_Scale = 'ADAS13_MMSE'\n",
    "CS_only = False\n",
    "\n",
    "for fid in np.arange(1,11,1):\n",
    "    for modality in modalities:\n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_{}_NN_OuterFold_{}_train_InnerFold_1.h5'.format(exp_name,dataset,Clinical_Scale,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_{}_NN_OuterFold_{}_valid_InnerFold_1.h5'.format(exp_name,dataset,Clinical_Scale,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_{}_NN_valid.h5'.format(exp_name,dataset,Clinical_Scale)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name_out,preproc)\n",
    "\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modality,preproc, CS_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "modality = 'HC_CT'\n",
    "n_folds = 8\n",
    "for fid in np.arange(1,n_folds+1,1):\n",
    "    print 'fid: {}'.format(fid)\n",
    "    for AE_branch in ['CT','R_HC','L_HC']:\n",
    "        print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "        if AE_branch == 'L_HC':\n",
    "            params_FF = ['L_ff1', 'L_ff2']\n",
    "            AE_iter = 10000\n",
    "        elif AE_branch == 'R_HC':\n",
    "            params_FF = ['R_ff1', 'R_ff2']\n",
    "            AE_iter = 10000\n",
    "        elif AE_branch == 'CT':\n",
    "            params_FF = ['ff1', 'ff2']\n",
    "            AE_iter = 50000\n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            #Only use this during 1 of the modalities to avoid overwritting\n",
    "            print 'Spawning new net'\n",
    "            pretrain_net = caffe.Net(baseline_dir + 'API/data/fold{}/ADNI_FF_pretrain.prototxt'.format(1), caffe.TRAIN)\n",
    "        else:\n",
    "            print 'Wrong AE branch'\n",
    "\n",
    "        # conv_params = {name: (weights, biases)}\n",
    "        conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "        for conv in params_FF:\n",
    "            print '{} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "        # Review AE net params \n",
    "        #fid for pretain is 1 because it's same definition for all the folds.\n",
    "        net_file = baseline_dir + 'API/data/fold{}/ADNI_AE_{}_test.prototxt'.format(1,AE_branch)\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "\n",
    "        AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "        params_AE = ['encoder1', 'code']\n",
    "        # fc_params = {name: (weights, biases)}\n",
    "        fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "        for fc in params_AE:\n",
    "            print '{} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "        #transplant net parameters\n",
    "        for pr, pr_conv in zip(params_AE, params_FF):\n",
    "            conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "            conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "        save_net = True\n",
    "        if save_net:\n",
    "            save_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_{}_iter_{}_concat50.caffemodel'.format(fid,modality,AE_iter)\n",
    "            print \"Saving net to \" + save_path\n",
    "            pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "16086,16471"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
