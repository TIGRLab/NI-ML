{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import caffe\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(2)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "# Some defs to load data and extract encodings from trained net\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    #print net.blobs.items()[0]\n",
    "    #print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        #print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        #print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        #print net.blobs[input_node].shape\n",
    "        \n",
    "        data_layers.append(net.blobs[input_node])    \n",
    "        #print 'X_list shape: {}'.format(X_list[i].shape)\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "    \n",
    "     \n",
    "    net.reshape()            \n",
    "    #print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        #print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1, concat_param=dict(axis=1))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT_AAL_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT_ALL_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_CT_AAL_dyn,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT_AAL_dyn, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT_unified(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_HC_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_HC_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_HC_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_HC_CT,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_HC_CT, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=4, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.adas, n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "    n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "    n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "    n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff2,n.R_ff2,n.ff2, concat_param=dict(axis=1))\n",
    "    #n.concat = L.Concat(n.X_L_HC,n.X_R_HC,n.X_CT, concat_param=dict(axis=1))\n",
    "    \n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.dropC1 = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2) #This is done automatically by caffe! \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        #n.ff2_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2ADAS13 = L.ReLU(n.ff2_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        #n.ff2_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2MMSE = L.ReLU(n.ff2_MMSE, in_place=True)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        n.ff1_DX = L.InnerProduct(n.ff3, num_output=node_sizes['DX_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1DX = L.ReLU(n.ff1_DX, in_place=True)\n",
    "        \n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_DX = L.InnerProduct(n.ff1_DX, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.DX_loss = L.SoftmaxWithLoss(n.output_DX, n.dx_cat3,loss_weight=tr['DX'])  \n",
    "    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_CT, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.dropC1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "        \n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_R_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_L_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))       \n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='HC_CT': #multimodal AE - experimental stage\n",
    "        HC_node_split = 0.8\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_L_HC = L.InnerProduct(n.X_L_HC, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.9*node_sizes['En1'])))        \n",
    "        n.NLinEn_L_HC = L.ReLU(n.encoder_L_HC, in_place=True)\n",
    "        \n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_CT = L.InnerProduct(n.X_CT, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.1*node_sizes['En1'])))        \n",
    "        n.NLinEn_CT = L.ReLU(n.encoder_CT, in_place=True)\n",
    "        \n",
    "        #Concat         \n",
    "        n.encoder1 = L.Concat(n.encoder_L_HC,n.encoder_CT, concat_param=dict(axis=1))\n",
    "        \n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers (or common encoder layers for multimodal) \n",
    "    \n",
    "#     n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.Sigmoid(n.encoder2, in_place=True)\n",
    "    #code layer\n",
    "#    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='xavier'))  \n",
    "    \n",
    "#     n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "    \n",
    "    #Decoder layers\n",
    "    if modality == 'CT':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)                    \n",
    "    \n",
    "    elif modality =='R_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    elif modality =='HC_CT':        \n",
    "        n.decoder_L_HC = L.InnerProduct(n.code, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_L_CT = L.ReLU(n.decoder_L_HC, in_place=True)\n",
    "        n.decoder_CT = L.InnerProduct(n.code, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_CT = L.ReLU(n.decoder_CT, in_place=True)\n",
    "        \n",
    "        n.output_L_HC = L.InnerProduct(n.decoder_L_HC, num_output=node_sizes['out_HC'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_HC = L.SigmoidCrossEntropyLoss(n.output_L_HC, n.X_L_HC)\n",
    "        \n",
    "        n.output_CT = L.InnerProduct(n.decoder_CT, num_output=node_sizes['out_CT'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_CT = L.EuclideanLoss(n.output_CT, n.X_CT)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    return n.to_proto()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode):\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 20\n",
    "    \n",
    "    if multimodal_autoencode:\n",
    "        train_loss_HC = zeros(niter)\n",
    "        test_loss_HC = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_loss_CT = zeros(niter)\n",
    "        test_loss_CT = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "        for it in range(niter):            \n",
    "            solver.step(1)  # SGD by Caffe \n",
    "            train_loss_HC[it] = solver.net.blobs['loss_HC'].data\n",
    "            train_loss_CT[it] = solver.net.blobs['loss_CT'].data\n",
    "            \n",
    "            if it % test_interval == 0:                        \n",
    "                t_loss_HC = 0\n",
    "                t_loss_CT = 0                                \n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()   \n",
    "                    t_loss_HC += solver.test_nets[0].blobs['loss_HC'].data\n",
    "                    t_loss_CT += solver.test_nets[0].blobs['loss_CT'].data\n",
    "                \n",
    "                test_loss_HC[it // test_interval] = t_loss_HC/(test_iter)\n",
    "                test_loss_CT[it // test_interval] = t_loss_CT/(test_iter)\n",
    "                print 'HC Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_HC[it], np.sum(train_loss_HC)/it, test_loss_HC[it // test_interval])\n",
    "                print 'CT Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_CT[it], np.sum(train_loss_CT)/it, test_loss_CT[it // test_interval])\n",
    "                \n",
    "        perf = {'train_loss':[train_loss_HC, train_loss_CT],'test_loss':[test_loss_HC,test_loss_CT]}\n",
    "    else: \n",
    "        \n",
    "        #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "        # losses will also be stored in the log\n",
    "        test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "        if not multi_task:\n",
    "            train_loss = zeros(niter)\n",
    "            test_loss = zeros(int(np.ceil(niter / test_interval)))  \n",
    "\n",
    "        else: \n",
    "            if multi_label:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_DX_loss = zeros(niter)\n",
    "                test_DX_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "            else:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_MMSE_loss = zeros(niter)\n",
    "                test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "\n",
    "        #output = zeros((niter, batch_size))\n",
    "        #solver.restore()\n",
    "        #the main solver loop\n",
    "        for it in range(niter):\n",
    "            #solver.net.forward()\n",
    "            solver.step(1)  # SGD by Caffe    \n",
    "            # store the train loss\n",
    "            if not multi_task:\n",
    "                train_loss[it] = solver.net.blobs['loss'].data        \n",
    "            else: \n",
    "                if multi_label:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_DX_loss[it] = solver.net.blobs['DX_loss'].data\n",
    "                else:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "            # store the output on the first test batch\n",
    "            # (start the forward pass at conv1 to avoid loading new data)\n",
    "            #solver.test_nets[0].forward()\n",
    "            #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "            # run a full test every so often\n",
    "            # (Caffe can also do this for us and write to a log, but we show here\n",
    "            #  how to do it directly in Python, where more complicated things are easier.)\n",
    "            if it % test_interval == 0:        \n",
    "                t_loss = 0\n",
    "                t_ADAS13_loss = 0\n",
    "                t_MMSE_loss = 0\n",
    "                t_DX_loss = 0\n",
    "                correct = 0\n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()                \n",
    "                    if not multi_task:\n",
    "                        t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                        if multi_label:\n",
    "                            correct += sum(solver.test_nets[0].blobs['output'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "                    else: \n",
    "                        if multi_label:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_DX_loss += solver.test_nets[0].blobs['DX_loss'].data\n",
    "                            correct += sum(solver.test_nets[0].blobs['output_DX'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "\n",
    "                        else:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "                if not multi_task:\n",
    "                    test_loss[it // test_interval] = t_loss/(test_iter)\n",
    "                    if multi_label:\n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                    print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                        it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval], test_acc[it // test_interval])\n",
    "                else:\n",
    "                    if multi_label:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_DX_loss[it // test_interval] = t_DX_loss/(test_iter) \n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'DX Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                            it, train_DX_loss[it], np.sum(train_DX_loss)/it, test_DX_loss[it // test_interval],test_acc[it // test_interval])\n",
    "                    else:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_MMSE_loss[it // test_interval] = t_MMSE_loss/(test_iter)             \n",
    "\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "        if not multi_task:\n",
    "            perf = {'train_loss':[train_loss],'test_loss':[test_loss],'test_acc':[test_acc]}\n",
    "        else:\n",
    "            if multi_label:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_DX_loss],'test_loss':[test_ADAS13_loss,test_DX_loss],'test_acc':[test_acc]}\n",
    "            else:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "                \n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,snap_prefix):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    \n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    #s.solver_type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 100000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 5000\n",
    "    s.snapshot_prefix = snap_prefix\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp6_MC'\n",
    "preproc = 'no_preproc'\n",
    "fid = 1\n",
    "mc=1\n",
    "train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "\n",
    "train_data_CT = load_data(train_filename_hdf, 'X_CT',preproc)\n",
    "train_data_y = load_data(train_filename_hdf, 'y','no_preproc')\n",
    "test_data_CT = load_data(test_filename_hdf, 'X_CT',preproc)\n",
    "test_data_y = load_data(test_filename_hdf, 'y','no_preproc')\n",
    "\n",
    "print train_data_CT.shape, train_data_y.shape, test_data_CT.shape, test_data_y.shape\n",
    "np.mean(train_data_y), np.std(train_data_y), np.mean(test_data_y), np.std(test_data_y)\n",
    "\n",
    "ct_corr_train = []\n",
    "ct_corr_test = []\n",
    "for c in np.arange(0,train_data_CT.shape[1],1):\n",
    "    ct_corr_train.append(stats.pearsonr(train_data_CT[:,c],train_data_y)[0])\n",
    "    ct_corr_test.append(stats.pearsonr(test_data_CT[:,c],test_data_y)[0])\n",
    "    \n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(np.mean(train_data_CT, axis=0)-np.mean(test_data_CT, axis=0))\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.array(ct_corr_train) - np.array(ct_corr_test),label='train-test')\n",
    "#plt.plot(ct_corr_test,label='test')\n",
    "plt.legend()\n",
    "\n",
    "mean_diff = np.mean(train_data_CT, axis=0)-np.mean(test_data_CT, axis=0)\n",
    "\n",
    "print mean_diff[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "150*4*100*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MC # 1, Hype # hyp1, Fold # 7\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (245.569168091,inf), test loss: 149.946160889\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (273.23034668,inf), test loss: 287.392970276\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (33.5504379272,46.0645080433), test loss: 39.5632929325\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.62677764893,7.27504865611), test loss: 3.28546296954\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (36.3595657349,45.1461138067), test loss: 38.4122056007\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.51575636864,5.22975985545), test loss: 3.29705156088\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (31.0241966248,44.6741072286), test loss: 36.6350155354\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.35830664635,4.54652290634), test loss: 3.44718600512\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (28.0774745941,44.3281909709), test loss: 38.2791481495\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.24961614609,4.20078084117), test loss: 3.2192740798\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (31.3135166168,44.0304680988), test loss: 37.2937931538\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.6055161953,3.99263791623), test loss: 3.4612293303\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (19.9206314087,43.7231275053), test loss: 34.0892370701\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.99739980698,3.85314119824), test loss: 3.08557161689\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (23.6582336426,43.4069519757), test loss: 36.8754042625\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.85975790024,3.75258056634), test loss: 3.06552900672\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (43.2850532532,43.0723250704), test loss: 34.422743082\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.49775385857,3.67607031882), test loss: 3.15254638195\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (53.993106842,42.6788259402), test loss: 33.4362308979\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.73105764389,3.61670271733), test loss: 3.13247630596\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (30.9196395874,42.279156686), test loss: 43.2901231766\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.34806632996,3.56877767348), test loss: 3.14838496447\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (30.6749629974,41.8930564454), test loss: 33.7507497787\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.62306857109,3.52756497685), test loss: 3.0519975841\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (25.4649467468,41.5088558448), test loss: 32.5564352989\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.29799985886,3.49169772082), test loss: 3.07337793708\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (61.2587432861,41.1490941332), test loss: 42.5410032272\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.90050041676,3.45845013873), test loss: 3.01117155552\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (52.5866661072,40.8001239503), test loss: 35.6714390755\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.12157225609,3.42848279455), test loss: 3.10333757401\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (22.6267471313,40.4738703139), test loss: 33.3631400108\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.10502052307,3.39987970894), test loss: 2.85890239477\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (37.6172180176,40.1655372902), test loss: 40.0618100166\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.6499838829,3.37278503539), test loss: 3.02206490636\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (25.2060012817,39.8777528319), test loss: 40.2745411873\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.23726558685,3.34649646572), test loss: 2.99308358133\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.0298995972,39.5989031236), test loss: 35.5182254314\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.87472462654,3.32165696795), test loss: 3.09377547503\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (24.217956543,39.3271031231), test loss: 33.4633039474\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.34312796593,3.29782327063), test loss: 3.0298866868\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (69.2705612183,39.0728445652), test loss: 31.8422746181\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.44881439209,3.27558276791), test loss: 2.85109151006\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (29.1517524719,38.8306336613), test loss: 33.7169392586\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.92344617844,3.25408077505), test loss: 3.10230805278\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (30.5928993225,38.5916332673), test loss: 33.3010255814\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.13420915604,3.23319422601), test loss: 3.00298587084\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (42.9848556519,38.3695886679), test loss: 35.9949954033\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.81956386566,3.21387628276), test loss: 2.90915808976\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (40.5198783875,38.1481377881), test loss: 34.7114737511\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.03137660027,3.19505901939), test loss: 2.86755905747\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (11.2955493927,37.9374723428), test loss: 32.14909482\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.48380374908,3.17718567975), test loss: 3.0853423357\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (32.7621231079,37.7438734), test loss: 36.0085954666\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.29661512375,3.16002370087), test loss: 3.09452857375\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (27.1153774261,37.5488500905), test loss: 33.3613459587\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.82565021515,3.14345484427), test loss: 2.8661273241\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (22.9043273926,37.3658261762), test loss: 31.3184019089\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.87568259239,3.12748690724), test loss: 2.93509430289\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (48.7480735779,37.1864921414), test loss: 32.3988394737\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.45138907433,3.11264247172), test loss: 3.03340065181\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (51.5523300171,37.0102316), test loss: 35.0959690571\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.79587960243,3.09825612664), test loss: 3.16103953123\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (30.4353046417,36.8440433913), test loss: 30.4941766739\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.82732439041,3.08456890134), test loss: 2.8626014173\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (35.2688827515,36.6846272699), test loss: 35.5941427231\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.73442816734,3.07141794071), test loss: 2.70133594275\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (36.9664039612,36.5278716388), test loss: 31.3266138077\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.03133106232,3.05864512193), test loss: 2.86124245524\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (16.1696052551,36.37993915), test loss: 30.6912567139\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.5177488327,3.04647215735), test loss: 3.53461940289\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (38.1166915894,36.2306388951), test loss: 32.2499184608\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.09283828735,3.0347253762), test loss: 3.33528862596\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (22.7362098694,36.0895392455), test loss: 35.8205910683\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.78981947899,3.02353840987), test loss: 2.81813559532\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (26.9257526398,35.9551951102), test loss: 32.4574460983\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.46433496475,3.01257795125), test loss: 2.68596064448\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (26.5622577667,35.8198403224), test loss: 34.6570288658\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.68880224228,3.00211872759), test loss: 2.77683125734\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (49.4350814819,35.6896622636), test loss: 33.6335095406\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.91376686096,2.99198233294), test loss: 2.89828698039\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (22.4008331299,35.5602321127), test loss: 35.1023579597\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.10342025757,2.98227321067), test loss: 2.82711863518\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (17.0516452789,35.4368272587), test loss: 31.9632616043\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.90446019173,2.97296502597), test loss: 2.73517224193\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (17.4778060913,35.3174805091), test loss: 30.6272912025\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.67895507812,2.96385372689), test loss: 2.77731567323\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (32.5716018677,35.2025072996), test loss: 32.5988595009\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.07996153831,2.95489568635), test loss: 2.82913368344\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (38.8119468689,35.0884785578), test loss: 35.6912549019\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.28362441063,2.94633323004), test loss: 3.83559703827\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (36.9429626465,34.9743467702), test loss: 51.9164708138\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (5.4107966423,2.93807428481), test loss: 2.79472857118\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (25.8148765564,34.8663425725), test loss: 34.7555793762\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.1723626852,2.93000430675), test loss: 2.67103524804\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (15.7164764404,34.7606783881), test loss: 32.1972231865\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.32238006592,2.92229314791), test loss: 2.80083043277\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (32.4111709595,34.6547471415), test loss: 44.2183817863\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.05955791473,2.91465294326), test loss: 2.79991678596\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (36.058506012,34.5537436846), test loss: 32.2880311966\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.75510299206,2.90743522005), test loss: 2.96708898544\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (26.0566558838,34.4509271655), test loss: 31.4403161049\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.20238137245,2.90025070644), test loss: 2.75556334257\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (30.0412940979,34.3529009742), test loss: 42.5203813553\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.17487668991,2.89339787475), test loss: 2.8982173562\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (24.4395008087,34.2593685754), test loss: 40.3931623459\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.43261623383,2.88662593349), test loss: 2.68150659204\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (29.4644832611,34.164222614), test loss: 32.7313682556\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.58083486557,2.87999999161), test loss: 3.39354283214\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (28.2673511505,34.0714581718), test loss: 30.7747691631\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.14810562134,2.87353660339), test loss: 3.19239221215\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (41.5158119202,33.980269181), test loss: 35.6403512478\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.36182641983,2.86734275507), test loss: 2.7520041585\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (28.9293479919,33.8894423845), test loss: 31.3879647255\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.51257944107,2.86131198254), test loss: 3.15297313929\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (26.4685115814,33.8025673654), test loss: 33.3024734497\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.85267472267,2.85538368971), test loss: 2.81509233713\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (43.2054138184,33.7167494192), test loss: 39.8121588707\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.19017124176,2.84967034073), test loss: 2.87933545709\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (35.8810310364,33.6305993662), test loss: 39.1907689095\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.79972088337,2.84396334164), test loss: 3.35475656986\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (16.1403484344,33.5471995948), test loss: 31.7292626381\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.36615967751,2.83850721787), test loss: 2.85762599707\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (26.0013504028,33.4644599198), test loss: 33.9509168148\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.2043364048,2.8330638816), test loss: 2.98920427561\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (24.4757041931,33.3837421144), test loss: 30.3025888443\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.77313113213,2.82781632437), test loss: 3.06212820411\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (19.5205039978,33.3047127122), test loss: 32.8015693188\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.956002414227,2.82251590412), test loss: 2.78385536671\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (36.7119750977,33.225446634), test loss: 31.9567188263\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.24106287956,2.81755898845), test loss: 2.98195432425\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (43.8610153198,33.1478027951), test loss: 38.7851676941\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.58685827255,2.81258583297), test loss: 3.3626132369\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (28.914484024,33.0700864997), test loss: 32.4454046249\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.07115364075,2.80776847415), test loss: 2.84141841531\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (29.4093399048,32.9950562047), test loss: 33.218423748\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.86178708076,2.80309430629), test loss: 2.7702660203\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (49.6789703369,32.9214262461), test loss: 35.2081113815\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.33447289467,2.79845813641), test loss: 3.2230036974\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (14.8355617523,32.8472744115), test loss: 31.890890789\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.2085981369,2.79384642053), test loss: 3.69581546783\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (26.8024082184,32.775120202), test loss: 30.7391418457\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.26491761208,2.78938514107), test loss: 3.33661025167\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (34.5740394592,32.7024761198), test loss: 33.4862261772\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.59081411362,2.78496427514), test loss: 2.72132584453\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (28.4218215942,32.6324516987), test loss: 32.0489940643\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.73624420166,2.78063761073), test loss: 2.7644012183\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (25.4594707489,32.5624403971), test loss: 30.4559558868\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.99585556984,2.77643583855), test loss: 2.89865732193\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (41.2105751038,32.4924496762), test loss: 33.0244091988\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.88481330872,2.77225068034), test loss: 3.14206036329\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (12.3424043655,32.4235598748), test loss: 34.8973104477\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.08446216583,2.76822849693), test loss: 2.82502446175\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.5383653641,32.3547440167), test loss: 30.5582647324\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.86667084694,2.76421687777), test loss: 2.92286188006\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (21.9985313416,32.2878277413), test loss: 33.3795845032\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.83959937096,2.76032434417), test loss: 2.82070202827\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (27.7658901215,32.222915632), test loss: 33.0182474613\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.03097844124,2.75642574109), test loss: 2.90104177892\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (37.8302993774,32.1563853929), test loss: 38.4034850121\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.40011620522,2.75256342384), test loss: 4.3802834034\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (30.8903961182,32.090741607), test loss: 44.2426876068\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (4.5425658226,2.74882563133), test loss: 2.77882782817\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (30.4764575958,32.0256528074), test loss: 32.0376608849\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.46300089359,2.74508905168), test loss: 2.96450543404\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (17.9438705444,31.9604556739), test loss: 32.3490515232\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.46035575867,2.74148394905), test loss: 2.94348755479\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (29.1167945862,31.8973698086), test loss: 44.4617247581\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.31938362122,2.73789578682), test loss: 3.04046081305\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (33.8325195312,31.8344208483), test loss: 32.2279816628\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.84064674377,2.73437577418), test loss: 3.23742081523\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (22.3260192871,31.7707864414), test loss: 32.3062134743\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.08523130417,2.73087128887), test loss: 2.81581902504\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (22.9069519043,31.7085819475), test loss: 35.3316389084\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.06041574478,2.72750079654), test loss: 2.77870412469\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (22.2513237,31.6466083037), test loss: 36.7363350868\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.94509625435,2.72402718868), test loss: 2.83222185969\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (25.7843475342,31.5855126362), test loss: 33.7987363815\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.9645512104,2.7207322912), test loss: 3.49775403738\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (32.3918228149,31.5245679533), test loss: 33.3561775684\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (4.35201501846,2.71732242907), test loss: 3.28167526722\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (41.5170173645,31.4634252896), test loss: 31.3683340073\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.92321300507,2.71403028628), test loss: 2.92599861026\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (29.6449451447,31.4026288183), test loss: 34.2580054283\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.21282529831,2.71075911842), test loss: 3.25664509535\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (20.8144721985,31.3422274704), test loss: 34.6473179817\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.04036903381,2.70754869174), test loss: 2.99813833833\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (41.3523788452,31.2836469513), test loss: 35.1353170395\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.00413990021,2.70441627999), test loss: 3.14895831347\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (19.0281677246,31.2238441953), test loss: 33.1367446899\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.64170134068,2.7012173385), test loss: 3.24318912625\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (21.3020515442,31.1640570067), test loss: 32.1925170898\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.71164059639,2.69807502868), test loss: 3.13251839876\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (21.3366298676,31.1057739196), test loss: 31.253837204\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.21179318428,2.6949657126), test loss: 3.24069043398\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (17.3797836304,31.0467225979), test loss: 33.6029494286\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.47446846962,2.69187415658), test loss: 2.96516461968\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (18.4749221802,30.9891943568), test loss: 31.6494871616\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.704672396183,2.68876967034), test loss: 2.97580880523\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (23.641374588,30.9315539755), test loss: 31.7299157619\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (3.07008004189,2.68580920707), test loss: 3.2005267024\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (34.5498199463,30.8730600702), test loss: 43.4158833504\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.24394869804,2.68275121516), test loss: 3.87043915987\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (23.9932937622,30.8154388163), test loss: 34.0099952698\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.03983592987,2.67980980486), test loss: 3.10353496671\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (27.953704834,30.7580131173), test loss: 33.2461378098\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.45545208454,2.67686954603), test loss: 2.81910299659\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (40.0046768188,30.7020631563), test loss: 34.7989368439\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.18172454834,2.67402379169), test loss: 3.32515586615\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (17.6905136108,30.6458676229), test loss: 30.765545845\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (3.25877118111,2.67111793562), test loss: 4.12217543125\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (35.6110610962,30.5889506661), test loss: 32.9839930058\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.98323559761,2.66819474241), test loss: 3.47238733768\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (17.6612167358,30.5323147694), test loss: 34.9240358829\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.33299541473,2.66534842794), test loss: 2.77210649848\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (22.2975006104,30.476180283), test loss: 31.3769807816\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.07750844955,2.66252747731), test loss: 2.95779761076\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (23.8885993958,30.420304898), test loss: 33.8712203026\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.15938019753,2.65973051897), test loss: 3.03230925202\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (43.4821815491,30.3655622005), test loss: 35.4686984062\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.92951071262,2.656953575), test loss: 3.04872814417\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (9.28040313721,30.3095849597), test loss: 32.2954191208\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.28453087807,2.65419870151), test loss: 3.06770654917\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (18.0566291809,30.2543423945), test loss: 33.0562791824\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.72500967979,2.6514772575), test loss: 2.86641865373\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (19.2513904572,30.1995075452), test loss: 32.1132875443\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.72650182247,2.64881234541), test loss: 2.89898198843\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (26.0688934326,30.1454325412), test loss: 34.7752875328\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.1930835247,2.64610449396), test loss: 3.15640345216\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (30.5290927887,30.091897009), test loss: 41.0959992409\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.90560793877,2.64350166116), test loss: 4.32922922373\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (23.4612674713,30.0376537656), test loss: 40.5753510475\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (3.52872872353,2.64081352356), test loss: 2.88753047585\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (25.9803180695,29.9837727845), test loss: 31.6131829262\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.14741253853,2.63815114766), test loss: 3.0477285862\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (15.4931459427,29.929808342), test loss: 34.7703402519\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.17715060711,2.63556057829), test loss: 3.0585342288\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (24.6528987885,29.8765613025), test loss: 48.868706131\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.22687625885,2.63302861955), test loss: 3.62922278047\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (15.2425785065,29.8248212704), test loss: 33.9833992004\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.36524915695,2.63050736186), test loss: 3.6224244833\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (14.4179382324,29.7718956027), test loss: 34.4724758148\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.95490288734,2.62796523434), test loss: 2.85028614998\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (10.5527706146,29.7190856086), test loss: 34.1347689629\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.7422760725,2.62545896016), test loss: 2.86865105033\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (23.4010677338,29.6674753722), test loss: 38.2945072651\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.15664362907,2.62297823367), test loss: 3.05836372972\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (25.6076183319,29.6151249742), test loss: 35.0241667747\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.76685142517,2.62053190992), test loss: 4.07948334217\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (35.3532600403,29.5643877849), test loss: 33.3438827991\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (4.7537651062,2.61809184009), test loss: 3.42868329883\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (35.7161827087,29.513607543), test loss: 33.206551075\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.77141928673,2.6156576799), test loss: 3.08520388007\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (19.4736785889,29.4614758063), test loss: 35.8794882774\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.83574485779,2.61319191302), test loss: 3.39088499546\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (18.0346908569,29.4102650681), test loss: 47.0487934113\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.76671624184,2.6108146512), test loss: 3.39620095491\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (32.0477600098,29.3596591267), test loss: 36.4228672981\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.67358350754,2.60847159232), test loss: 3.24550092816\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (17.625831604,29.3098993405), test loss: 34.6913066864\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.71432948112,2.60614635644), test loss: 3.16911813021\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (21.1593704224,29.260542726), test loss: 33.5354296684\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.36989212036,2.60380660244), test loss: 3.06536871791\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (25.3055992126,29.2101737233), test loss: 34.7079525948\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.42687511444,2.60140640115), test loss: 2.94738745093\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (20.1756305695,29.1600295835), test loss: 32.1519591331\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (3.00520610809,2.59911040373), test loss: 3.30187164545\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (23.3788871765,29.1103776457), test loss: 34.0141657829\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.672993123531,2.59678481042), test loss: 3.0272339642\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (20.9506397247,29.0611626615), test loss: 35.5075931549\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.05059289932,2.5945482539), test loss: 3.34804397821\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (25.6142959595,29.0128597662), test loss: 53.8313241959\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.50963807106,2.59227095555), test loss: 3.78220493793\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (18.9268016815,28.9633148463), test loss: 40.6336239815\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.99680435658,2.58998395813), test loss: 3.27829262614\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (27.2166595459,28.9149017147), test loss: 35.0808367729\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.32768309116,2.587751427), test loss: 2.91687461138\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (29.35820961,28.8660810947), test loss: 37.3436168671\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.51524031162,2.58557436955), test loss: 3.45155023932\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (15.8644933701,28.8181238742), test loss: 33.0808354378\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.53547668457,2.58337896607), test loss: 4.59708901644\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (32.9186630249,28.7713515865), test loss: 35.8234038353\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.32663249969,2.58118031108), test loss: 4.29386364222\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (17.9748687744,28.7230984357), test loss: 32.0209165573\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.1158362627,2.57892929917), test loss: 3.09834725857\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (16.6099395752,28.67522647), test loss: 32.5238119125\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.888692379,2.57676611542), test loss: 3.1337919116\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (22.5656833649,28.627265792), test loss: 33.9069177628\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.30302119255,2.57460714194), test loss: 2.98021579981\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (39.3768348694,28.5804470822), test loss: 35.3096276283\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.87845480442,2.57247486756), test loss: 3.6076177001\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (11.8899040222,28.5341547134), test loss: 36.3023468971\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.33304929733,2.57036420906), test loss: 3.11478135586\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (20.7572097778,28.4872687158), test loss: 34.8791822433\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.61295080185,2.56822704874), test loss: 2.92085711956\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (16.4599399567,28.4403994285), test loss: 32.8925662041\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.56877994537,2.5661145764), test loss: 2.99481385946\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (23.8056755066,28.3941488447), test loss: 34.555807209\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.28858697414,2.56399566058), test loss: 2.96066110134\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (26.5459575653,28.3477429154), test loss: 47.8839944839\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.00364160538,2.56194970938), test loss: 4.56203441024\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (17.8835411072,28.3022591126), test loss: 40.1118953705\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.66324234009,2.55986342272), test loss: 2.86617273092\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (17.3780784607,28.2567473195), test loss: 37.6920324326\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.28795313835,2.55776208655), test loss: 3.06516240239\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (14.8907699585,28.210684241), test loss: 35.6174149513\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.11388516426,2.55566087857), test loss: 3.17572845817\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (18.9457874298,28.1647712877), test loss: 42.3207672119\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.30351209641,2.5536454581), test loss: 3.89742054939\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (13.1368179321,28.1195792062), test loss: 35.466486454\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.27358090878,2.55161359384), test loss: 3.21745282412\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (10.2299766541,28.0752874384), test loss: 35.0495703697\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.52926802635,2.54963031902), test loss: 3.03032850027\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (9.4244966507,28.0310644549), test loss: 34.5441960335\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.71206140518,2.54761002026), test loss: 2.99685713053\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (13.9623622894,27.9862225167), test loss: 36.7962748528\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.98998498917,2.54555755894), test loss: 3.17887026072\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (19.9107875824,27.9415517352), test loss: 36.1083407402\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.3417596817,2.5435820629), test loss: 4.47134568691\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (34.2391242981,27.8970556434), test loss: 37.8179932594\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (5.32488632202,2.54160872423), test loss: 3.27571191788\n",
      "run time for single CV loop: 877.352685928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:124: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:126: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Deign Net Architecutre and Run Caffe\n",
    "exp_name = 'Exp6_MC'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'CT'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "multi_label = False\n",
    "start_MC=1\n",
    "n_MC=1\n",
    "MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "start_fold = 7\n",
    "n_folds = 7\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "niter = 80000\n",
    "batch_size = 512\n",
    "\n",
    "pretrain = False\n",
    "multimodal_autoencode = False\n",
    "load_pretrained_weights = False\n",
    "HC_snap = 4000\n",
    "CT_snap = 6000\n",
    "\n",
    "if Clinical_Scale in ['BOTH','ADAS13_DX'] :\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "# dr: Dropout rate, lr: learning rate of each modality, tr: loss-weight for each task\n",
    "hype_configs = {\n",
    "                'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':25,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},\n",
    "                      'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':1e-4, 'wt_decay':1e-3}},\n",
    "                \n",
    "#                 'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':100,'HC_CT_ff':25,'COMB_ff':25,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},\n",
    "                \n",
    "#                 'hyp3':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':50,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},\n",
    "                \n",
    "#                 'hyp4':{'node_sizes':{'HC_L_ff':100,'HC_R_ff':100,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':25,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},\n",
    "                \n",
    "#                 'hyp5':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':150,'HC_CT_ff':25,'COMB_ff':25,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':10,'HC_CT':2},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}}\n",
    "\n",
    "                }\n",
    "\n",
    "CV_perf_MC = {}\n",
    "for mc in MC_list:\n",
    "    CV_perf_hype = {}\n",
    "    for hype in hype_configs.keys():    \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "        tr = hype_configs[hype]['tr']\n",
    "        solver_configs = hype_configs[hype]['solver_conf']\n",
    "        \n",
    "        CV_perf = {}\n",
    "        start_time = time.time()\n",
    "        for fid in fid_list:            \n",
    "            print ''\n",
    "            print 'MC # {}, Hype # {}, Fold # {}'.format(mc, hype, fid)\n",
    "            snap_prefix = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}'.format(mc,fid,exp_name,hype,modality)\n",
    "\n",
    "            train_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/train_C688.txt'.format(mc,fid)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "\n",
    "            train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "            with open(train_filename_txt, 'w') as f:\n",
    "                    f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "            if pretrain:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_train.prototxt'.format(mc,fid)\n",
    "                with open(train_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(train_filename_txt, batch_size, node_sizes, modality)))            \n",
    "            else:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(train_net_path, 'w') as f:            \n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            if pretrain:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_test.prototxt'.format(mc,fid)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(test_filename_txt, batch_size, node_sizes,modality)))\n",
    "            else:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            # Define Solver\n",
    "            solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "            with open(solver_path, 'w') as f:\n",
    "                f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, snap_prefix)))\n",
    "\n",
    "            ### load the solver and create train and test nets\n",
    "            #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "            #solver = caffe.get_solver(solver_path)\n",
    "            #solver = caffe.NesterovSolver(solver_path)\n",
    "            solver = caffe.NesterovSolver(solver_path)\n",
    "\n",
    "            if load_pretrained_weights:    \n",
    "                if hype in {'hyp1','hyp3','hyp5'}:\n",
    "                    pre_hype = 'hyp1'\n",
    "                else:\n",
    "                    pre_hype = 'hyp2'\n",
    "                #snap_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_HC_CT_iter_10000_concat50.caffemodel'.format(fid)\n",
    "                #snap_path = baseline_dir + 'API/data/fold{}/train_snaps/Exp11_{}_HC_CT_iter_10000.caffemodel'.format(fid,hype)\n",
    "                snap_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_{}_HC_CT_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'.format(mc,fid,pre_hype,HC_snap,CT_snap)\n",
    "                print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "                solver.net.copy_from(snap_path)\n",
    "\n",
    "            #run caffe\n",
    "            results = run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode)\n",
    "            CV_perf[fid] = results\n",
    "            \n",
    "        print('run time for single CV loop: {}'.format(time.time() - start_time))\n",
    "\n",
    "        CV_perf_hype[hype] = CV_perf\n",
    "        \n",
    "CV_perf_MC[mc] = CV_perf_hype\n",
    "\n",
    "# pickleIt(CV_perf_MC, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Time for training \n",
    "#HC_CT fold 9,10, number of iterations: 40k, two hyper-params\n",
    "#start time: 14:47\n",
    "#end time: 15:31\n",
    "#runtime_mean = (13+31)/4\n",
    "#print runtime_mean\n",
    "\n",
    "tx=70 #time for 10k iters\n",
    "itx=4 # num of 10k iters\n",
    "hx=4 #hyp choices\n",
    "fx=10 #k-folds\n",
    "mx=10 #mc-folds\n",
    "\n",
    "num_hrs = (tx*itx*hx*fx*mx)/(3600.0)\n",
    "print num_hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbcAAAJoCAYAAABVztwOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3X+4XWV5J/zvnQRSGIgmokkaIKbg26ZaROvYqdQaoIio\n1P6YxmmhGfTtay+0M07bt22oFaGMWitlpjNVLlugBMUfqbRFRlqopgmV19IpVYcWoVYKCBHQohCg\ngRzO8/6xd8I2noQTkp3DOuvzua5zXeustfZ67r3Zf4Rv7txPtdYCAAAAAABdMmemCwAAAAAAgD0l\n3AYAAAAAoHOE2wAAAAAAdI5wGwAAAACAzhFuAwAAAADQOcJtAAAAAAA6R7gNAMAOVfV/VdXnquqB\nqvqFqrqwqt62m/snq+q79kNd76iqD457nSnWfUVVfWV/r/t0VVVfraqXzXQdAACQCLcBAPhWv5pk\nQ2vtGa2132utndlae+du7m/TfXBV/X1VPTjys62qrtyD2qa91j42lnWr6i1V9b+ramtVXbIHr7u6\nqrYMP8PHqurRkc/0/XtRz7ur6vef6uvHrap+tapuG/7Fy1eq6reqqnZx7w9V1aer6l+q6t6quryq\nnj1y/Tuq6uLhta9X1R9X1XP237sBAGBfEG4DADBqeZJ/2IP7pwwXp9Jae0FrbcH2nyRfSbJ+Twuc\nRe5Ocl6Si/fkRa21V7fWDh1+hpcnec/I5/rmcRT6NPHxJC9qrT0jyQuTHJfk53dx7zOT/M8kRyZ5\nbgZ/QTEa3P9qku9LsjLJsiSPJblgLFUDADA2wm0AAJIkVfXpJMcned+wC/joqvrDqvrNkXt+pao2\nV9VdVfWGPMWu5qp6RZJnJfnjPXjZ/KpaN6ztpqp68fBZ/29VfXyn5/+Pqvpvw+O/rKp3VdUNw67f\nP6mqZ+5ZufVLwy7fu6vqjOHJl1TVPaPdw1X1E1X1ueHxO6rqj6rqo8Oa/7aqjtl+b2vtT1trn0hy\n/x7UMt2Cf7yqvlBV36iqTVW1cuTa24f/DR+oqn+oquOq6nVJfinJfxzW+tfTWOM7qup9w2fdWVW/\nXVVzh9cWV9WfDdf/WlV9anfrT+c9tdZua609MPx1bpLJJEfv4t7/Nfx8H26t/WuS9yUZHafy3CRX\nt9bub609msFfsjx/OnUAAPD0IdwGACBJ0lo7MclfJXnLsAv4n0avV9WrMghAT0zyvCQ/stP1n66q\nz09zuTVJrhgGj9N1apIPJ3lGkqsyCCyT5ENJTq6qBcM65iZ5fZJ1I6/92SRnJFmS5PEMunq31/2F\nqvoPu1l3SZJDk3xnkp/LIPx/Rmvtb5N8PckrR+49fad1fzTJx5IsTPKRJH+6PQAel6r6d0l+L8l/\nTLIoyQeH684ZhutnJDlm2AH9miR3tdauzKBzed3wv/2/m8ZSv5nkBRmEwt+fZFUGHdFJ8mtJbhmu\nvzTJOcPaplx/eO2Eqtr8JO/tjKp6MMm9GXwHL5pGnUnyinzrv0j4gyTHD0P4Q5L8dJKrp/ksAACe\nJoTbAABM108l+cPW2heHofQ5oxdbax9prR37ZA+pqoOS/Pskf7iH63+mtXZNa61lENgeM1z3niTX\nDetLklOSfK21Nhq0f3Ck7rcn+antHdettRe21j66m3UfS3Jea+3x1tqfJXkoyXcPr12WQXCeqlqU\n5OQMAvjtbmyt/Ulr7fEMwuPvSDKd4HhvvCnJ77XWPt8GLkoyP4MAemJYwwuqam5r7fbW2h1PcZ2f\nSXJ2a+0brbWvJfmvGX4WSbZl8JcBz22tTbTWPjM8v8v1W2sbWmvfubsFW2uXDsexfE8GAfXXnqzI\nqvr+JL+SJ4L3JPni8LVfTfKNDMaXvHs6bxoAgKcP4TYAANP1nRnMyd7ujuzBzO0RP5nkX1prf7WH\nr7tn5PiRJN9RVdv/PHtZBl3TSXJaBuH3qJ3rPjDJYdNc919aa5M7rX3I8PhDSV47DOxXJ7mutXbf\nVOsOQ/m7Mvgcx2l5kl+vqvuHP9/I4L0ua63dnGRtkncmubeqPji60eIeWpLkzpHf78hgfnWGz/9q\nkr+sqn+sql9Mkl2sv8cbObbW/jHJbUn+x+7uG45juSrJm4ad9ttdlEHQ/swk/ybJtUk+uad1AAAw\ns4TbAABM11eTHDHy+/I8tZnbazIIo/elP01yTFU9P8lrM9hocdTOdT+WwUiRvdJa25zksxkE9qfn\n20P1HesOO8UPT7Lb0Rv7wFcy6KheNPxZ2Fo7pLX2p8OaP9haOy7JdyU5KIOO62TP/1vek8Fnud3y\nDDbJTGvtwdbaf2mtPTeDz+Y3quoHd7H+eU/pXSbzhs+YUlUdleSaJGtba1fsdPmFSS4e1vlYBmNc\nXl5VBz/FWgAAmAHCbQAApmt9kjOqauUwBDx7Tx9QVYdnsGnluimu/XNVrdmTx20/GG4KeEUGI0Fu\naK3dtdO9p1fV9wzrPjfJHw07qfeFD2Yw8uIF+fYNMr+/qn5sOGf7F5NsTfLXyWA2eFV9RwabI86r\nqvmj87irarKqfvgp1PP7Sf7TcBxHquqQqjp1uAHkyqr64ao6MMmjSf41g40Zk8Ec6xV7sM5Hkryj\nqhYNu69/PcNwf7je9mdtyaBLevJJ1t+tqvq5qjpsePx9GXzmn9rFvcuTfDrJe1prU/1Fyt9ksHnm\nIcNa3pzkttbaI9N65wAAPC0ItwEAGLVz4Lvj99banyf570k2JPnHDMLDHarqZ6rqpid5/ulJrm+t\n/fNOrz0gg80H/3oval2X5PsydVf4B4fXN2cwkuStI2v/fVX99F6s+ycZdC3/cWtt607Xrsxgc8tv\nZDAu5ceH87eT5DcyGHHya8NrjyR527CmI5I8mOTJPs9vC+hba/9fkv+c5APDkSS3ZLBhYsugU/p3\nMpg3fXcGIznePnzpR5P8m+Eok8/s/Nwp1js7yc0ZbNT4dxlsRvre4bWVGYwkeTDJxiTvba3dsLv1\nq+rEqhod6bKz45PcXFVbMvjM/yiDv6jI8PX/VFU/Pvz15zPomn93VT1YVVt2evZ/yaDz+7YMOtB/\nOIMOcwAAOqT2XcPKFA+vmp/B5j4HZvCHx4+31s6tqnck+X+SbP8D5q8P/2cpVXVWkjdm0N3x1tba\ntWMrEACAp4WqOi7Jm1trp+3FM47IYKPAJa21h0bO/2UGG0pesveV7nLtf8pgrvOGkXPvSHJUa21P\nutG3v/a0JN/bWnvbPiwTAABmlXnjfHhr7dGqOr619sjwn1heX1V/Nrx8QWvtgtH7hxu+rM6g0+Pw\nJJ+qquftw38yCgDA01Br7fok1z/V1w83lvzlJB8dDbb3h6r6ySSTo8H23mqt7TwzHAAA2MlYw+0k\nGZlbN3+43vaguqa4/XUZ/A/JRJLbq+pLSV6a5IZx1wkAQDcN52jfm+Sfk5wyxS1ja5QYdoWvzGDc\nCgAAsB+NPdwedtHcmOSoJO9rrf3vqnp1kl+oqp9N8rdJfrm19kCSZRnsNr/d3cNzAAAwpWEzxaG7\nuX7CGNc+fjfXzt3VNQAAYO+NfUPJ1tpka+1FGYwZeWlVfW+S9yf5rtbasRls4PI7464DAAAAAIDZ\nY+yd29u11h6sqo1JXrXTrO0/SHLV8PjuDHY13+7w4blvUVVmcAMAAAAAdEBrbaoR1XttrOF2VR2W\nZFtr7YGqOijJSUl+q6qWtNbuGd72E0n+fnj8iSSXV9V/y2AcydFJ/maqZ9tjki4655xzcs4558x0\nGbBHfG/pIt9busp3ly7yvaWrfHfpIt9buqhqLLl2kvF3bi9Nsm44d3tOko+11q6uqsuq6tgkk0lu\nT/LzSdJau7mq1ie5Ocm2JG9uUmwAAAAAAHYy1nC7tXZTkhdPcX7Nbl7z7iTvHmddAAAAAAB029g3\nlASesGrVqpkuAfaY7y1d5HtLV/nu0kW+t3SV7y5d5HsL36q6OPWjqkwrAQAAAAB4mquqbm4oCQAA\nAAAwWzz3uc/NHXfcMdNlPC0tX748t99++35dU+c2AAAAAMA0DLuQZ7qMp6VdfTbj7Nw2cxsAAAAA\ngM4RbgMAAAAA0DnCbQAAAAAAOke4DQAAAABAzjzzzLzzne+c6TKmzYaSAAAAAADT8HTfUHLFihW5\n+OKLc8IJJ+z3tW0oCQAAAADAPvf444/PdAn7nHAbAAAAAKDj1qxZkzvvvDOvfe1rs2DBgrz3ve/N\nnDlzcskll2T58uU58cQTkySrV6/O0qVLs3DhwqxatSo333zzjme84Q1vyNlnn50k2bRpU4444ohc\ncMEFWbx4cZYtW5ZLL710Jt7aLgm3AQAAAAA67rLLLsuRRx6ZT37yk3nwwQezevXqJMl1112XW265\nJddcc02S5NWvfnW+/OUv57777suLX/zinHbaabt85j333JMtW7Zk8+bNueiii/KWt7wlDzzwwH55\nP9Mh3AYAAAAA2Aeq9s3P3hide11VOffcc3PQQQdl/vz5SZIzzjgjBx98cA444ICcffbZ+cIXvpAt\nW7ZM+awDDzwwb3/72zN37tyccsopOeSQQ3LrrbfuXYH7kHAbAAAAAGAfaG3f/OxLhx9++I7jycnJ\nrF27NkcffXSe+cxnZsWKFamqfP3rX5/ytc961rMyZ84TEfLBBx+chx56aN8WuBeE2wAAAAAAs0BN\n0fY9eu7DH/5wrrrqqmzYsCHf/OY3c/vtt6e19i3d3l0i3AYAAAAAmAWWLFmS2267LUmmDK23bNmS\n+fPnZ+HChXn44Ydz1llnTRmId4VwGwAAAABgFli7dm3OO++8LFq0KFdcccW3Bddr1qzJkUcemWXL\nluUFL3hBXvayl+3R859uQXh1seW8qloX6wYAAAAAuquqOjvCY9x29dkMz48lFde5DQAAAABA5wi3\nAQAAAADoHOE2AAAAAACdI9wGAAAAAKBzhNsAAAAAAHSOcBsAAAAAgM4RbgMAAAAA0DnCbQAAAAAA\nOke4DQAAAABA5wi3AQAAAABmgRUrVmTDhg179Yx169bl5S9/+T6qaLyE2wAAAAAAJElaa6mqmS5j\nWoTbAAAAAAAdt2bNmtx555059dRTs2DBgpx//vm54YYbctxxx2XhwoV50YtelE2bNu24/9JLL81R\nRx2VBQsW5KijjspHPvKR3HLLLTnzzDPz2c9+NoceemgWLVo0g+/oyVVrbaZr2GNV1bpYNwAAAADQ\nXVWVXeWSde6+7XZu79jz/HPFihW55JJLcvzxx2fz5s055phjcvnll+fkk0/Opz/96bz+9a/Prbfe\nmoMOOihLly7NjTfemKOPPjr33ntv7r///qxcuTLr1q3LxRdfnOuuu26P1t7VZzM8P5ZWcJ3bAAAA\nAACzxPaA+UMf+lBe85rX5OSTT06SnHjiiXnJS16Sq6++Okkyd+7c3HTTTdm6dWsWL16clStXzljN\nT5VwGwAAAABglrnjjjuyfv36LFq0KIsWLcrChQtz/fXX56tf/WoOPvjgfOxjH8uFF16YpUuX5tRT\nT82tt9460yXvsXkzXQAAAAAAQNc9lTEi+9roRpBHHHFE1qxZkw984ANT3nvSSSflpJNOyqOPPpq3\nve1tedOb3pRNmzZ1ZjPJROc2AAAAAMCssGTJktx2221JktNPPz1XXXVVrr322kxOTmbr1q3ZtGlT\nNm/enPvuuy+f+MQn8sgjj+SAAw7IIYcckjlzBlHx4sWLc9ddd2Xbtm0z+VamRbgNAAAAADALrF27\nNuedd14WLVqU9evX58orr8y73vWuPPvZz87y5ctz/vnnZ3JyMpOTk7nggguybNmyHHbYYbnuuuty\n4YUXJklOOOGEPP/5z8+SJUvynOc8Z4bf0e7Vrnb3fDqrqtbFugEAAACA7qqqyCWntqvPZnh+LLNO\ndG4DAAAAANA5wm0AAAAAADpHuA0AAAAAQOcItwEAAAAA6BzhNgAAAAAAnSPcBgAAAACgc4TbAAAA\nAAB0jnAbAAAAAIDOEW4DAAAAAJAzzzwz73znO2e6jGmr1tpM17DHqqp1sW4AAAAAoLuqKk/nXHLF\nihW5+OKLc8IJJ+z3tXf12QzP1zjW1LkNAAAAADDLPf744zNdwj4n3AYAAAAA6Lg1a9bkzjvvzGtf\n+9osWLAg733vezNnzpxccsklWb58eU488cQkyerVq7N06dIsXLgwq1atys0337zjGW94wxty9tln\nJ0k2bdqUI444IhdccEEWL16cZcuW5dJLL52Jt7ZLwm0AAAAAgI677LLLcuSRR+aTn/xkHnzwwaxe\nvTpJct111+WWW27JNddckyR59atfnS9/+cu577778uIXvzinnXbaLp95zz33ZMuWLdm8eXMuuuii\nvOUtb8kDDzywX97PdAi3AQAAAAD2hap987MXRudeV1XOPffcHHTQQZk/f36S5IwzzsjBBx+cAw44\nIGeffXa+8IUvZMuWLVM+68ADD8zb3/72zJ07N6ecckoOOeSQ3HrrrXtV374k3AYAAAAA2Bda2zc/\n+9Dhhx++43hycjJr167N0UcfnWc+85lZsWJFqipf//rXp3zts571rMyZ80SEfPDBB+ehhx7ap/Xt\nDeE2AAAAAMAsUFN0fY+e+/CHP5yrrroqGzZsyDe/+c3cfvvtaa19S7d3lwi3AQAAAABmgSVLluS2\n225LkilD6y1btmT+/PlZuHBhHn744Zx11llTBuJdIdwGAAAAAJgF1q5dm/POOy+LFi3KFVdc8W3B\n9Zo1a3LkkUdm2bJlecELXpCXvexle/T8p1sQXl1sOa+q1sW6AQAAAIDuqqrOjvAYt119NsPzY0nF\ndW4DAAAAANA5wm0AAAAAADpHuA0AAAAAQOcItwEAAAAA6BzhNgAAAAAAnSPcBgAAAACgc4TbAAAA\nAAB0jnAbAAAAAIDOEW4DAAAAANA5wm0AAAAAgFlgxYoV2bBhw149Y926dXn5y1++jyoaL+E2AAAA\nAABJktZaqmqmy5gW4TYAAAAAQMetWbMmd955Z0499dQsWLAg559/fm644YYcd9xxWbhwYV70ohdl\n06ZNO+6/9NJLc9RRR2XBggU56qij8pGPfCS33HJLzjzzzHz2s5/NoYcemkWLFs3gO3py1Vqb6Rr2\nWFW1LtYNAAAAAHRXVWVXuWRt3LhP12qrVu3xa1asWJFLLrkkxx9/fDZv3pxjjjkml19+eU4++eR8\n+tOfzutf//rceuutOeigg7J06dLceOONOfroo3Pvvffm/vvvz8qVK7Nu3bpcfPHFue666/Zo7V19\nNsPzY2kF17kNAAAAADBLbA+YP/ShD+U1r3lNTj755CTJiSeemJe85CW5+uqrkyRz587NTTfdlK1b\nt2bx4sVZuXLljNX8VAm3AQAAAABmmTvuuCPr16/PokWLsmjRoixcuDDXX399vvrVr+bggw/Oxz72\nsVx44YVZunRpTj311Nx6660zXfIemzfOh1fV/CTXJTlwuNbHW2vnVtXCJB9LsjzJ7UlWt9YeGL7m\nrCRvTDKR5K2ttWvHWSMAAAAAwN56KmNE9rXRjSCPOOKIrFmzJh/4wAemvPekk07KSSedlEcffTRv\ne9vb8qY3vSmbNm3qzGaSyZg7t1trjyY5vrX2oiTHJjmlql6aZG2ST7XWvjvJhiRnJUlVfW+S1UlW\nJjklyfurS58mAAAAAMAMWbJkSW677bYkyemnn56rrroq1157bSYnJ7N169Zs2rQpmzdvzn333ZdP\nfOITeeSRR3LAAQfkkEMOyZw5g6h48eLFueuuu7Jt27aZfCvTMvaxJK21R4aH8zPo3m5JXpdk3fD8\nuiQ/Njz+0SQfba1NtNZuT/KlJC8dd40AAAAAAF23du3anHfeeVm0aFHWr1+fK6+8Mu9617vy7Gc/\nO8uXL8/555+fycnJTE5O5oILLsiyZcty2GGH5brrrsuFF16YJDnhhBPy/Oc/P0uWLMlznvOcGX5H\nu1e72t1zny1QNSfJjUmOSvK+1tpZVfWN1trCkXvub60tqqr/meSzrbUPD89flOTq1tof7/TMNu66\nAQAAAABGVVXkklPb1WczPD+W6Rz7o3N7cjiW5PAkL62q52fQvf0tt427DgAAAAAAZo+xbig5qrX2\nYFVtTPKqJPdW1eLW2r1VtSTJfcPb7k5yxMjLDh+e+zbnnHPOjuNVq1Zl1dNgYDsAAAAAQJ9t3Lgx\nGzdu3C9rjXUsSVUdlmRba+2BqjooyTVJfivJK5Lc31p7T1X9WpKFrbW1ww0lL0/yA0mWJfmLJM/b\neQaJsSQAAAAAwP5mLMmuzcRYknF3bi9Nsm44d3tOko+11q6uqr9Osr6q3pjkjiSrk6S1dnNVrU9y\nc5JtSd4sxQYAAAAAYGdj31ByHHRuAwAAAAD7m87tXZuVG0oCAAAAAMC+tt82lAQAAAAA6LLly5en\naixNyJ23fPny/b6msSQAAAAAAIyFsSQAAAAAADBCuA0AAAAAQOcItwEAAAAA6BzhNgAAAAAAnSPc\nBgAAAACgc4TbAAAAAAB0jnAbAAAAAIDOEW4DAAAAANA5wm0AAAAAADpHuA0AAAAAQOcItwEAAAAA\n6JzOhts/8vnPz3QJAAAAAADMkM6G2/du2zbTJQAAAAAAMEM6G25vm5yc6RIAAAAAAJgh3Q23W5vp\nEgAAAAAAmCHCbQAAAAAAOke4DQAAAABA53Q33DZzGwAAAACgtzobbk/o3AYAAAAA6K3OhtvGkgAA\nAAAA9JdwGwAAAACAzul0uN0E3AAAAAAAvdTZcDtJHhduAwAAAAD0UqfDbaNJAAAAAAD6SbgNAAAA\nAEDnCLcBAAAAAOicbofbk5MzXQIAAAAAADOg2+G2zm0AAAAAgF4SbgMAAAAA0DnCbQAAAAAAOqfb\n4baZ2wAAAAAAvdTtcFvnNgAAAABAL3U63J4QbgMAAAAA9FKnw22d2wAAAAAA/STcBgAAAACgc7od\nbttQEgAAAACgl7odbuvcBgAAAADoJeE2AAAAAACdI9wGAAAAAKBzuh1um7kNAAAAANBL3Q63dW4D\nAAAAAPSScBsAAAAAgM4RbgMAAAAA0DndDrfN3AYAAAAA6KVuh9s6twEAAAAAekm4DQAAAABA53Q6\n3J4QbgMAAAAA9FKnw22d2wAAAAAA/dTtcNuGkgAAAAAAvdTtcFvnNgAAAABALwm3AQAAAADoHOE2\nAAAAAACd0+1w28xtAAAAAIBe6na4rXMbAAAAAKCXhNsAAAAAAHSOcBsAAAAAgM7pdrht5jYAAAAA\nQC91O9zWuQ0AAAAA0EvCbQAAAAAAOke4DQAAAABA53Q63J4QbgMAAAAA9FKnw20bSgIAAAAA9FO3\nw22d2wAAAAAAvSTcBgAAAACgc4TbAAAAAAB0TrfDbTO3AQAAAAB6qdvhts5tAAAAAIBeEm4DAAAA\nANA5Yw23q+rwqtpQVf9QVTdV1X8ann9HVd1VVX83/HnVyGvOqqovVdUXq+qVu3u+cBsAAAAAoJ/m\njfn5E0l+qbX2+ao6JMmNVfUXw2sXtNYuGL25qlYmWZ1kZZLDk3yqqp7X2tQptpnbAAAAAAD9NNbO\n7dbaPa21zw+PH0ryxSTLhpdripe8LslHW2sTrbXbk3wpyUt39Xyd2wAAAAAA/bTfZm5X1XOTHJvk\nhuGpX6iqz1fVRVX1jOG5ZUm+MvKyu/NEGP5thNsAAAAAAP007rEkSZLhSJKPJ3lra+2hqnp/kt9s\nrbWq+q9JfifJz+3RQy+9NFuTnLNhQ1atWpVVq1bt67IBAAAAANgDGzduzMaNG/fLWrWLcdb7boGq\neUn+V5I/a6397hTXlye5qrV2TFWtTdJaa+8ZXvvzJO9ord2w02ta/vIvkySTr3hFqqaacAIAAAAA\nwEyqqrTWxhLg7o+xJJckuXk02K6qJSPXfyLJ3w+PP5HkP1TVgVW1IsnRSf5mdw9/3GgSAAAAAIDe\nGetYkqo6LslpSW6qqs8laUl+PcnPVNWxSSaT3J7k55OktXZzVa1PcnOSbUne3J6ktXxba/tntgoA\nAAAAAE8bYx9LMg6jY0ke+KEfyoJ54m0AAAAAgKebro8lGattHQznAQAAAADYO90PtycnZ7oEAAAA\nAAD2s+6H2zq3AQAAAAB6R7gNAAAAAEDnCLcBAAAAAOic7ofbZm4DAAAAAPRO98NtndsAAAAAAL0j\n3AYAAAAAoHOE2wAAAAAAdE73w20ztwEAAAAAeqfz4faEzm0AAAAAgN7pfLhtLAkAAAAAQP8ItwEA\nAAAA6BzhNgAAAAAAndP9cNuGkgAAAAAAvdP9cFvnNgAAAABA7wi3AQAAAADoHOE2AAAAAACd0/1w\n28xtAAAAAIDe6X64rXMbAAAAAKB3hNsAAAAAAHSOcBsAAAAAgM7pfrht5jYAAAAAQO90P9zWuQ0A\nAAAA0DudD7cnhNsAAAAAAL3T+XBb5zYAAAAAQP8ItwEAAAAA6Jzuh9s2lAQAAAAA6J3uh9s6twEA\nAAAAeke4DQAAAABA5wi3AQAAAADonO6H22ZuAwAAAAD0TvfDbZ3bAAAAAAC9I9wGAAAAAKBzhNsA\nAAAAAHRO98NtM7cBAAAAAHqn++G2zm0AAAAAgN4RbgMAAAAA0DmdD7cnhNsAAAAAAL3T+XBb5zYA\nAAAAQP90P9y2oSQAAAAAQO90P9zWuQ0AAAAA0DvCbQAAAAAAOke4DQAAAABA53Q/3DZzGwAAAACg\nd7ofbuvcBgAAAADoHeE2AAAAAACdI9wGAAAAAKBzOh9uT7SWJuAGAAAAAOiVzobbo4VPCLcBAAAA\nAHqls+H2AVU7jo0mAQAAAADol+6G23OeKF24DQAAAADQL90Nt0c6t40lAQAAAADol1kRbm+bnJzB\nSgAAAAAA2N9mR7itcxsAAAAAoFe6G26buQ0AAAAA0FvdDbd1bgMAAAAA9NbsCLfN3AYAAAAA6JXZ\nEW7r3AYAAAAA6JXuhttmbgMAAAAA9FZ3w22d2wAAAAAAvdXZcHuemdsAAAAAAL3V2XBb5zYAAAAA\nQH8JtwEAAAAA6Jzuhts2lAQAAAAA6K3uhtsjndsTwm0AAAAAgF6ZFeG2DSUBAAAAAPpldoTbOrcB\nAAAAAHpcEKfCAAAgAElEQVSlu+G2mdsAAAAAAL3V3XBb5zYAAAAAQG+NNdyuqsOrakNV/UNV3VRV\n/3l4fmFVXVtVt1bVNVX1jJHXnFVVX6qqL1bVK3f1bDO3AQAAAAD6a9yd2xNJfqm19vwkP5jkLVX1\nPUnWJvlUa+27k2xIclaSVNX3JlmdZGWSU5K8v2okxR6hcxsAAAAAoL/GGm631u5prX1+ePxQki8m\nOTzJ65KsG962LsmPDY9/NMlHW2sTrbXbk3wpyUuneraZ2wAAAAAA/bXfZm5X1XOTHJvkr5Msbq3d\nmwwC8CTPGd62LMlXRl529/Dct9G5DQAAAADQX/sl3K6qQ5J8PMlbhx3cO6fRe5xOzzNzGwAAAACg\nt+aNe4GqmpdBsP3B1tqVw9P3VtXi1tq9VbUkyX3D83cnOWLk5YcPz32bv/rd302++c0kyT+98pXJ\nihVjqR8AAAAAgOnZuHFjNm7cuF/WqjbmkR5VdVmSr7fWfmnk3HuS3N9ae09V/VqSha21tcMNJS9P\n8gMZjCP5iyTPazsVWVXt3bffnrP++Z+TJL9yxBH57aOOGuv7AAAAAABgz1RVWmv15HfuubF2blfV\ncUlOS3JTVX0ug/Ejv57kPUnWV9Ubk9yRZHWStNZurqr1SW5Osi3Jm3cOtrezoSQAAAAAQH+NNdxu\nrV2fZO4uLv/ILl7z7iTvfrJnH2DmNgAAAABAb+2XDSXHYTTcntC5DQAAAADQK7Mi3DaWBAAAAACg\nX7obbpu5DQAAAADQW90Nt3VuAwAAAAD01uwIt20oCQAAAADQK7Mj3Na5DQAAAADQK90Nt83cBgAA\nAADore6G2zq3AQAAAAB6a3aE22ZuAwAAAAD0SmfD7Xk6twEAAAAAequz4baxJAAAAAAA/dXdcNuG\nkgAAAAAAvdXdcNvMbQAAAACA3pod4bbObQAAAACAXpkV4faEcBsAAAAAoFe6G26buQ0AAAAA0Fvd\nDbeNJQEAAAAA6K3ZEW7bUBIAAAAAoFdmR7itcxsAAAAAoFe6G26buQ0AAAAA0FvdDbd1bgMAAAAA\n9NasCLcnWksTcAMAAAAA9EZnw+2qytyR3yeE2wAAAAAAvdHZcDtJ5hlNAgAAAADQS50Ot20qCQAA\nAADQT90Ot0c7tycnZ7ASAAAAAAD2p9kTbuvcBgAAAADoDeE2AAAAAACd0+1we2Tm9oRwGwAAAACg\nN6YVblfVW6tqQQ1cXFV/V1WvHHdxT0bnNgAAAABAP023c/uNrbUHk7wyycIkP5vkt8ZW1TTZUBIA\nAAAAoJ+mG25vT5FfneSDrbV/GDk3Y3RuAwAAAAD003TD7Rur6toMwu1rqurQJDPeKj06c1u4DQAA\nAADQH/Omed//neTYJLe11h6pqkVJ3jC+sqZH5zYAAAAAQD9Nt3P7B5Pc2lr7ZlWdnuQ3kjwwvrKm\nx8xtAAAAAIB+mm64fWGSR6rqhUl+OcmXk1w2tqqmSec2AAAAAEA/TTfcnmittSSvS/J7rbX3JTl0\nfGVNj5nbAAAAAAD9NN2Z21uq6qwkP5vk5VU1J8kB4ytreubp3AYAAAAA6KXpdm6/PsmjSd7YWrsn\nyeFJ3ju2qqbJzG0AAAAAgH6aVrg9DLQvT/KMqnptkq2tNTO3AQAAAACYEdMKt6tqdZK/SfJTSVYn\nuaGq/v04C5sO4TYAAAAAQD9Nd+b225L829bafUlSVc9O8qkkHx9XYdNhQ0kAAAAAgH6a7sztOduD\n7aF/2YPXjs1o5/aEcBsAAAAAoDem27n951V1TZKPDH9/fZKrx1PS9NlQEgAAAACgn6YVbrfWfqWq\nfjLJccNTv99a+5PxlTU9Zm4DAAAAAPTTdDu301q7IskVY6xlj5m5DQAAAADQT7sNt6tqS5KpUuNK\n0lprC8ZS1TTp3AYAAAAA6KfdhtuttUP3VyFPhZnbAAAAAAD9NOfJb3n60rkNAAAAANBP3Q63zdwG\nAAAAAOilTofb83RuAwAAAAD0UqfDbTO3AQAAAAD6afaE2zq3AQAAAAB6Q7gNAAAAAEDndDvctqEk\nAAAAAEAvdTvcHuncnhBuAwAAAAD0xqwJt20oCQAAAADQH7Mn3Na5DQAAAADQG90Ot83cBgAAAADo\npW6H2zq3AQAAAAB6afaE22ZuAwAAAAD0xuwJt3VuAwAAAAD0RrfDbTO3AQAAAAB6qdvhts5tAAAA\nAIBe6nS4Pc/MbQAAAACAXup0uK1zGwAAAACgn4TbAAAAAAB0TrfDbRtKAgAAAAD0UrfDbTO3AQAA\nAAB6adaE2xM6twEAAAAAemPWhNvGkgAAAAAA9MdYw+2quriq7q2q/zNy7h1VdVdV/d3w51Uj186q\nqi9V1Rer6pVP9nwztwEAAAAA+mncndt/mOTkKc5f0Fp78fDnz5OkqlYmWZ1kZZJTkry/aqQ1ewo6\ntwEAAAAA+mms4XZr7TNJvjHFpalC69cl+WhrbaK1dnuSLyV56e6ev/PM7SbgBgAAAADohZmauf0L\nVfX5qrqoqp4xPLcsyVdG7rl7eG5KZ/zpGamqzB05Z1NJAAAAAIB+mIlw+/1Jvqu1dmySe5L8zlN5\nyOfu+VwSc7cBAAAAAPpo3v5esLX2tZFf/yDJVcPju5McMXLt8OG5Kd155Z05555zMnnHHckLX5gc\ne6xwGwAAAABgBm3cuDEbN27cL2vVuOdUV9Vzk1zVWvu+4e9LWmv3DI9/Mcm/ba39TFV9b5LLk/xA\nBuNI/iLJ89oUBVZVe/ZvPzv3/cp9edZnPpP7JyaSJF972cty2IEHjvX9AAAAAAAwPVWV1tpUezDu\ntbF2blfVh5OsSvKsqrozyTuSHF9VxyaZTHJ7kp9PktbazVW1PsnNSbYlefNUwfZ2Wx7bkiSZN7Kp\npM5tAAAAAIB+GGu43Vr7mSlO/+Fu7n93kndP59lbJ7ZmYnIiBwi3AQAAAAB6ZyY2lNxnHnrsIRtK\nAgAAAAD0UKfD7S2PbvnWzu3JyRmsBgAAAACA/aXT4fZDjz1kLAkAAAAAQA91Otze8ti3dm5PCLcB\nAAAAAHqh2+H2o1vM3AYAAAAA6KFOh9vGkgAAAAAA9FOnw+2dx5LYUBIAAAAAoB+6HW4/ukXnNgAA\nAABAD3U73H7MzG0AAAAAgD7qdLht5jYAAAAAQD91Otz+trEkZm4DAAAAAPRCt8Ptx7Zkns5tAAAA\nAIDe6XS4bSwJAAAAAEA/dTrctqEkAAAAAEA/dTvcNnMbAAAAAKCXOh1uG0sCAAAAANBPnQ63tzy2\nRbgNAAAAANBD3Q63H/3WmdsTwm0AAAAAgF7odLhtLAkAAAAAQD91Otx+eNvDmfdEtm1DSQAAAACA\nnuh0uJ0kbXLbjmOd2wAAAAAA/dD5cHtSuA0AAAAA0DudD7cff/zRHcfCbQAAAACAfphd4baZ2wAA\nAAAAvTALwu3Hdhzr3AYAAAAA6IfOh9vbHt/6xLFwGwAAAACgF7ofbk/86xPHwm0AAAAAgF7ofLg9\nMdq5beY2AAAAAEAvdD7cfmzikR3HOrcBAAAAAPphFoTbxpIAAAAAAPRN58PtR7c9vONYuA0AAAAA\n0A+dD7cf2/bEWJIJ4TYAAAAAQC90Ptzeuu2hHcc2lAQAAAAA6IdZEG4bSwIAAAAA0DezINwe6dwW\nbgMAAAAA9ELnw+1/fWzLjmPhNgAAAABAP3Q+3H7ksQd3HJu5DQAAAADQD7Mg3Na5DQAAAADQN90P\ntx8d6dwWbgMAAAAA9ELnw+2HdW4DAAAAAPROZ8Ptg+YdNDho23acM3MbAAAAAKAfOhtuH3LgIYOD\nNrHjnM5tAAAAAIB+6Gy4fej8QwcHk8JtAAAAAIC+6W64feAw3G6P7zgn3AYAAAAA6IfOhttTjSWZ\nEG4DAAAAAPRCZ8PtHWNJdgq3m4AbAAAAAGDW6264vX0sSZI5eSLQ1r0NAAAAADD7dTbc3jGWJMnc\nkXDb3G0AAAAAgNmvs+H2rjq3hdsAAAAAALNfd8Pt+aPh9uSO422Tk1PdDgAAAADALNLZcHt0LEm1\nkXBb5zYAAAAAwKzX2XB7dCxJtcd3HAu3AQAAAABmv+6G2/NHw+2JHcfCbQAAAACA2a+74fZI53Yb\n7dw2cxsAAAAAYNbrbLg9OnM7kzq3AQAAAAD6pLPh9uhYksm2bcexcBsAAAAAYPbrbrg9OpZkUrgN\nAAAAANAnnQ23R8eSTD7+2I5jM7cBAAAAAGa/zobbo2NJHp98dMfxhM5tAAAAAIBZr7Ph9mjn9uOj\nndvCbQAAAACAWa+z4faBcw/MgXMPHPxiQ0kAAAAAgF7pbLidjGwqOTmx45xwGwAAAABg9ut2uL19\n7nZ7fMc5G0oCAAAAAMx+nQ63d8zdbjq3AQAAAAD6pNPh9o6xJMJtAAAAAIBe6Xa4vX0syeTIWBLh\nNgAAAADArNfpcPuJsSRmbgMAAAAA9Emnw21jSQAAAAAA+km4DQAAAABA53Q63H5iLIlwGwAAAACg\nTzodbu/YUNLMbQAAAACAXhlruF1VF1fVvVX1f0bOLayqa6vq1qq6pqqeMXLtrKr6UlV9sape+WTP\n///Zu+/wKKqvD+DfSaOFpnRFQOlNQVAQRVBEQAQ7FkBsKMgriIr1J2BXsGChqXSlWUCqCEiTDqH3\nEpESegghpM/7x93JlJ1yJ2wICd/P8/Bkd/bunck2sueeOSerLEkmM7eJiIiIiIiIiIiILic5nbk9\nGsDdlm1vAJivqmoNAAsBvAkAiqLUBvAIgFoA2gIYqiiK4ja5nrmtB7fTGdwmIiIiIiIiIiIiyvdy\nNLitquoyAKctmzsCGBu4PBbAfYHLHQBMUlU1XVXVWAC7AdzkNj9rbhMRERERERERERFdnnKj5nYZ\nVVWPAoCqqnEAygS2XwXgP8O4Q4FtjrLKkhhrbjO4TURERERERERERJTvReT2AQDIVjS6W7cBCC/5\nHxADoNER4DqxnQ0liYiIiIiIiIiIiHLHokWLsGjRoouyr9wIbh9VFKWsqqpHFUUpB+BYYPshABUN\n464ObLM1duwAbIzbhFHDRwHlS2Ztt8vcXrUKqFkTKF486CYiIiIiIiIiIiIiCpEWLVqgRYsWWdcH\nDhyYY/u6GGVJlMA/zR8AugUuPwlgumH7o4qiRCmKUgVAVQCr3SaWLUvSpAnw7rvZOXQiIiIiIiIi\nIiIiuhTlaHBbUZSfASwHUF1RlAOKojwF4BMAdymKshPAnYHrUFV1G4ApALYBmA2gp6q6F9AuWkAL\nbjs3lIyNFT/T0+Fpzx7giy+8xxERERERERERERFR7srRsiSqqj7ucFMrh/EfA/hYdv7oqGhxIdMQ\n3LbU3K5SRXY24JtvgK+/Bvr2lb8PEREREREREREREV18F6MsSY4pEF4AEWERnmVJcsJff8llgxMR\nERERERERERFR6OXp4LaiKKLutktZEo1XzHvVKmDUKPl9t24NzJsnP56IiIiIiIiIiIiIQidPB7eB\nQGkSh8ztmjXl53n1VSAx0d++ZZPEd+4Exo71NzcREREREREREREROcvzwe2iBSyZ24aa2zt35sYR\nBRswAOjWLbePgoiIiIiIiIiIiCj/yPvBbcmyJG5SU4GUFPnx990nP/bgQWDbNt+HRERERERERERE\nREQu8nxwOzoqGsgMDm6vXCk/R6tWwJo18uOnTxc/ZeLobdoAmzbJz01ERERERERERERE3vJ0cPvo\nUa0siV5zO11VkZkJNG1qHusWiF63LocOEP4ywgGgYUPg9OmcORYiIiIiIiIiIiKi/CJPB7fLlbMv\nS+KnMkmjRkBSkvz42Fj5sfHxQEKC/HgAiIkB9u3zdx8iIiIiIiIiIiKiy02eDm4DgbIkloaSjz0m\nf38/WdvHjwNVqsiPr1ULOHZMfjwRERERERERERERycnzwe2iUUVNNbcTk1VMnRr6/aSmApMmmbdt\n3Sq2O4mL87cPRfF/XERERERERERERESXo7wf3LbU3E7zU5PEhz//BF56ybztjTeAb77Jkd0RERER\nERERERERkYu8H9y21NxOh31wO4di3o71uhs2zP6cMscaGwvs2pX9fRARERERERERERHlZXk+uG2t\nuZ2OTOn7tm6dE0ckxMTIjz140H9JkptuAmrU8HcfIiIiIiIiIiIiovwizwe3ixYw19xO95Gi/ddf\nOXFEwOTJ8mMzMoAFC/zvIyXF/32IiIiIiIiIiIiI8ou8H9yOMtfcPns+h+qP+PDoo/JjJ04EunXz\nN//GjUBCgr/7EBEREREREREREeUneT64LcqS6MFtNezi1ty+UOfPB2/zOtYbbsiZYyEiIiIiIiIi\nIiLKK/J8cLtoAXNDSUTI19x2Ys2KTksDOnS44GlzVc2a9oF0IiIiIiIiIiIiorwozwe3C4Vbgtvh\nABCc+jxypPn69OnOc/76q/n6N99k+/BC7uuvs3e/nTuBU6dCeyxEREREREREREREuSXPB7e/HhQt\nLhhKkyDCuwbJzJny+zh71udBSVq4EOjeXX78rFlA797mbWPHut8nLQ14/HH/x0ZERERERERERER0\nKcvzwe3DsUXFhUxj9vYlWmDbYupU++1ONbfbtw/e5tWM8tQp0bSSiIiIiIiIiIiIKD/J88HtcLUQ\nwpQwS91t7+C2ojjftnx59o9n3Ljs3zcnnDypX75Um2oSERERERERERER+ZXng9sKFERHRVvqbns3\nlfz+e+fbfvgh+8fz5JPZv2+orVoF1KmT20dBREREREREREREFHp5PrgNAEWjivquuX05yE4DyWXL\ngJUrQ38sRERERERERERERKEUkdsHEApFCxQ119wOcXB71qyQTndRbN+evSzy224DChYEzp8P/TER\nERERERERERERhUqez9xOSwMKKtG+a257SU3VL69Z4zzu3Xfl57TWvF6xQm4c4F4j3Or0aWDgQOD4\nce95jTIy3G8nIiIiIiIiIiIiulTk+eD29OnAhtVFpWpupweGOAWVjT76SPw8cuQCD9Bg4UL9cmYm\nsHGj/bgLDTL37AlMnuz/fldeeWH7JSIiIiIiIiIiIrpY8nxwGwCQIldzOzISeO014JZbvKc8eRKY\nOROoUCFExwhzNnivXs7jmjUD/vor+/tJTs7e/c6cET+9MryJiIiIiIiIiIiIclu+qLmN1GjphpKD\nB8tNOXy4/8PYskV+7Lp17rfHxvrf/4XQMtX9SEkBChQI/bEQERERERERERERecknwe3QN5RMTwe+\n/VZ+/IkTQL16F7zbHOWUkX3ddcC+ff7m+vdfoHJlZnkTERERERERERFR7shHZUm8a27nlOHDgdKl\n5cdPmwasXp1zx+OX38A2AMTHy49VFGD9ev/7ICIiIiIiIiIiInKSP4LbqXI1t3NKjx7+xj/8sL/x\nfrKj33tPBM8vhNf+MjKATZv8zfnnn8ChQ9k/JiIiIiIiIiIiIiKjfBLcjjZnbl/k4LasTZuA+fNF\nyRMZJ04AS5YAYR7P0vTpQKNG4vKPP/o7JkUJ3pbpkfg+dSrQtau//bz1FtCqlfuYkycvfq1xIiIi\nIiIiIiIiypvyR83tlKKAekq/fokGt994w9942VIns2d7N6gE5DPAvYLv58/rlxVFZHJ7BeABIDnZ\n/fYHHhDBfNbxJiIiIiIiIiIiIi/5JHPb0lAyPO9HR2Wzu40SEkJ/HEavviqC2U8/bd4eqmC0nzre\nREREREREREREdHnLJ8HtaHPN7YvcUDInbN7s/z7FiwMHDoT+WDQrVuTc3EZe5UuIiIiIiIiIiIiI\n8kdwO6Vonqi57cewYfJj/WZOL11qLi2Sk0aP9n+fBQtCfxxERERERERERESUv+SP4HZq/gtu5wQt\nCN68OfDdd/bNJEPNWsLETU4dT2qqd71vIiIiIiIiIiIiylvySXA7Gsg0lCVhcNtTaqr77Zs3A4mJ\nQGysuP7AA8Dy5fLz33Yb0KCBv2PauNHf+MGDgbg473EdOgC1agEnT/qbn4iIiIiIiIiIiC5d+SO4\nbS1Lkg9qbvuxbZv/+7z9tvvt9esDvXoBVaqI67NmOY+1K4uybBmwYYP/4/LjtdeAjh29x8XEiCB9\nqVI5ezxERERERERERER08eSP4PZlXpbkn39yZt7Tp8XP4cO9xyoKcOyY+xin2uAffABUq+bv2DTJ\nyf5rjhMREREREREREVHelz+C22mFLcHtDOexl7HsluXo0UNu3PHj7rc7lUIZPRrYsyd7x7RpE/Dy\ny+5jvYLumuRkEaRft441uomIiIiIiIiIiC51+SO4rYYB6eH69QIpuXcsl7BGjfw1bTxwQL/sVaPb\nKC3NfvuRI8Hbtm8H9u3zf0zGbPJVq+Tv70b7HRs1AoYMCc2cRERERERERERElDPyR3AbANIY3A41\n2ZrZa9aInydPAqdOAVFR8vuoXVt+bGYmUKmS3uRSRnq69xhNmOHd4CeYL4OZ4ERERERERERERKGV\nf4Lb6YZfpQAjiRfTLbeIn7ffDvzyS87tR6utrTW5lNGrl/zYMJ/vhgUL5Eq2HDkCFCoEJCX5m5+I\niIiIiIiIiIic5Z/gdlqEfjkqxGm3JG3mTPfbz5yRm2f79gs7jgYNgIgIYMQI+fv4KdkCACNHivIo\nXr9zYqL4WaQIMHu2v30QERERERERERGRPQa3KaRmzHC/vUQJ4Pnngb173cfVrg188415m7EGuNHm\nzcHbNmwAMiT7ip47JwLbb70lN14zZYr4ee+97uOMGeGHDvnbBxEREREREREREdnLP8Ht1Ej9MoPb\nl7SRI4GqVYGmTd3HvfSS3pxy2zbg2mvtx507B6xYkf3jiY8XP7/6Sv4+AwbIj/VT7mTVKqBfP6Bx\nY/n7EBERERERERERXY4ivIfkEcbM7ci03DsOkrZypfeYqCggIUEEsN0sX+4dLAdE3e6UFKBgQXE9\nLQ2YNcv7fpqHHhINLWXLqwD+gttNmsiPJSIiIiIiIiIiupzlo8ztKP1yZHruHQeF3ObNejNJJzNm\nAHv2eM8VFiaaO2r+/FOUSbF6911gyBDg7Fnz9l9/Bdatk9uXcZ9e1q0DCheWn1PToYO437597uMW\nLAAWLwaWLPG/DyIiIiIiIiIioktR/sncTjEEt6MY3M5PmjUDWrVyH7N4MVCtmgiCV64sP7db0LxP\nH+CKK4AuXbzn2bsXuO468zYtMH7NNd73X78eOH/ee5yVVuN861bnsi2A+fHzWiggIiIiIiIiIiLK\nC/Jn5nYEg9v5zfz58mP//TfnjsNJ1apAUpJ+fdYsoFgx8c9q1argAHNCgvy+li8HkpPlx8uWOomJ\n0ZtwfvIJg+BERERERERERHRpy0fB7YL65ciM3DsOylWKIjfugw9E8FZrJulEVfUg7623uo8tUkS/\nfPiw/Zju3UWwedEi8/ZXXw0ee/fdwGuvBW9v1gz45hsgOtr9eDSrVsmNa9hQlF0BgDffBDIz5e4X\nEwP884/z7fv3A1Onys1FREREREREREQkKw8Hty1ppSkF9MsMbpOH2bOBUaOArl3dxz35JPDjj+Ky\nWwDXr3TDyQVOQeR584DBg83btLFffWVusmlXSzsjw7sRp9Xp08CHH7qPycgQWeoZGUBqKtC8uXvg\n/513gEceEcf+3XfO486eBU6c8He8RERERERERER0+crDwW2LFEPmdoRkyildtlasAA4elBu7ZUvO\nHkt4uPxYLdBuzQwfPBgYMABISdG3vfWWfXZ3v37A++/bz//55yIYDYgFgIULg8e8+KLIUo+IAAoU\nABIT5Y79iy+AXr2cb7/nHqBsWbm5rDI81rN27MjevEREREREREREdOnKR8HtQvrlcBYLJm8DBsiN\nGzLEX9b2rl3A11+7j4mNlZ+vRg39clyc87iBA8W+NU4B3UGDgHfftb9t9279cocOwJ13Bo/ZuNH+\nvvHx9k0xly4VP+1KrBj995+ema4oQM+ezmO7dRPNPocMAQ4dEoF2N7VqAUeOuI/JzAQmTHAfQ0RE\nREREREREl478E9xONgS3IxjcptDyqrdt9OWX3tne3buLxpAyjAFrP/74Q36slq0tw6muecmSwcFw\nRRFBa6PZs+3nsG4bNgyYMyc4KJ2eDowdK0qo9OnjXVNcC5inpwNjxjhn7B86BHTp4j6XJikJuOkm\n4PhxufGyMjPZyJOIiIiIiIiISFb+CW6nMLhNue+OO4Dhw+XGahnNsnbudM64vlBudbYVxVy7261p\n59at3vu65x7n/QDAp5/q29q1C262aQ3+Pvig+Hn6dHCGfUqKudHnU08B7dt7H+PixcAvvzjf/sUX\nwJo1QJkyou54TIyoGe7k9dflXhfh4ebfn4iIiIiIiIiInOWf4HZyYf2yjxrGRKH099/+xrsFiq3e\neMPf3F6M5U68JCXpl2Uzzv3Yv1+vI279PffsAY4e1a87PWZXXBGcYX/llUBysrj8xBPi58aNQKVK\nwMyZ5rHavBkZQIsWwMMPi+zxzZuD9/W//+mX16wBGjYE3n7b/rhiY4HPPgM+/lhkjR87Zj9Os2GD\n++2AyEJv2xa46ir5xRSNUwNTQCwcaM1OU1KYRU5EREREREREl7b8E9w+b0jPZHCb8oCcCBz+/LN8\ng0et3MmMGfLzy84t46OP9MvXXqsHoa1WrxaBXI2fBQFjxrkxU/7AAWDBAvNYbd6nntK39ewJ1K/v\nvg8toH7smChJYyyjkpoKVKmiX69Y0btppqqKEiluv+eIEcDcuWJBYPRooHNnoGNH5/G//QZs2was\nXevewHToUCAyUlwuWBB47z3zY+jk6FGgf3/vcQCwbJnzc2109iywd6/cnERERERERER0eco/wW1j\n5naYj+gXUS558035sU2aANOmeY/75BPg11/9BYCNQWYvt9zifntCgp6Z7FUr3CnT2Y5MgPVCpKXp\nzSTHjw++/bHHvOeYPBno2xcYN07fZmxQeeCAfrlSJaBqVft50tP1gLqiAPPnBy+EnDqlX169Gvjp\nJ4izBvUAACAASURBVFFjff9++8z6Bx8E6tTRb1MUsahhnXfbNvP1AQOA6GgRHLdz8qSos16unAiE\nO9m5Uyy8AMBtt4nsfOPZAEZ16gBnzgC9eonHSKZ8T2amCO5b67sbnTkjfu7eLX+GRUqK3DgiIiIi\nIiIiyh35J7htytwOAxSXc++J8hivpolG+/bl3HHs2OE9RstMNmZbh1JCQmjm+eorvbzInDnuZV8m\nTcrePrQSH1YHDjhnJZ85Yw5e33WXCGAbOS1eXHst0KyZeZuxTnvv3vrlDh2CG2I6zfvMM8HbFAUo\nVQpYuFDfVqgQ8NxzwWPfekuUhenQQVwfMgS4916RoW61bRvw779AfLy43rw5MHGi/XFpwsNFcH/J\nEucxJUqIuuydO4va+GvX2o9bsEBfBChYUCwqZWS47x8Qi0TdujnfPnOmKGEDyNWmJyIiIiIiIiJv\n+Se4nWY4114JAyId0gKJ8jm3DFqrTZuAlSu9x5UpA2zfLjKcZeVEkH3nTlFb242frHW3YGgoeB3L\nlCn6Za28ycmTweOaNAH++kt+XqP335c/Pu364MHm7fHxQNeu3vtKTgZ++MF5P8YSOAsXOi+AWDPK\nH388eMxHH4ns/0WL9G2dOwOdOulBZKuxY/WFgsaNgfPng8e0agW0aaNfv/9+YMwY9wD3Bx+IYxk7\n1nnMvfeKAH9qKlC3rgii25UmUlVRpx0Qv/cLL4j3npfERPfXxZIl+u2qynrqRERERERElD/k2eC2\nAss38wzDr6JEAFFnL+4BEeVB118vP9ZYVsPLwYPyY2WCbLt2iTrTWmmJUPMTLI6Jcb/9n3/00ixe\n83bqZL4f4NxQUmu4KTOvrDJlRCkT67yvvRY81q5ci4zvvxelcuwcPmzfYDMjIziYbP2d339fBLhb\ntjRvnzIFePVV+/29/LL5euHC9uOsr8lnnxW/h1Fqqr4IYGww2rKl88JOUhLwzjvicqtW9hncS5fq\nddonThSv+9q17efTnDkD9OnjPmb3bv1yiRLA6687Nxc9dkzctnevKPeycaP73JqvvnIfW7y4XMNU\nIiIiIiIiIll5NrgdJN0Q+VAigKgQdr4joqygnAyvMhKaDh3ky4y88IK/oO6JE/JjZQLsmzaJnw0b\nuo+bMQP4v/8Tl0MVhLbymvfoUfm5tNrov/wCfPNN9o/JiVtN95MngerVg7ffeCMwa5b7vH4e2507\n5ccC9q+HHj3MJVg2bxaLANYzJRYtAq67zn7eQ4eAQYP06/XqBZecOeuwLmu34HD0qCh9s3Ah8OOP\nYlvBgvYNO7XHKyFB/Bs0CLjvPvt9lS0rstDnzxflXm64wX6c0e7dYvHgs8+cxyQkAOvXA7//Lo4n\nNdV73iFDRNkgmc+J/fv1x8HOyy+LMjdERERERESUf+TP4HZYBFCAmdtEoeRUo9hOv35y42bMAKZO\nlZ/XreyDVenS3mPi4kQQ/IMPvMf6yXLX+HnMZP38s6hh7aZcOf/zfv559o7HjqLo9bS9gtDZzcZ3\nm/fsWb0JaVwcULOmv3nOnbPfPmJE8P369/c+VjfWkiNhDv8rDx4cHLgtV05sNy7kpKSI7dazJ7Tj\nLV5c3zZjBjBqlP3+rAskxgx/O9oixYoV7s1kVVVfKCpQwH7Mhx/qi0h9+gDt2nm/R9PTgYEDRZa9\nkz/+0BuUFiwodzbKoUOi1I7Xe06zapXzYllKCtC0qdw8REREREREJCd/BreVcJYlIcoj7BoQOvnu\nO7lxsgGkXbtEPWunGs0XIi0NGD48dPN16waMHm2u0x1KMpnQWlBShhZwlc2wzsjwl43tNjYmBmjR\nQlx2aup5obyO1VhW5ZFHnMd16yaCrtZ57Rqc2gVu33wT6N7dvO3MmeCSJ36ahQLBJUuuvTa45v6w\nYSJYPmaMvm3/ftGw02nR4rffgAED9OvaIoTR7NnB5X/cap6fPg08/7y++OXUR8D4GKSkAJUqiaxw\nK1UFbrpJXG7VSnxGffyx8/6NmjQRx28X4D51Sj+2KVNEs9cFC7znTEoSx25XI94qPt45+x8Qj5Ox\nfj8REREREVFel3+C29aa28zcJrpsyTTJ1OzdG/r979sXXN/ZiZ+A7oYNOVfqxClj2EjLXnfLjs3O\nvIBcaZgbbxRBScD7cdixQ26ckZ/Xgte8ERH6Za+zE4wBTm3eTz+VPxYv8fEio9qJ3RkRSUnBGdhR\nUcC//+rX588X5UKeeso87tAh5zrhs2ebr0dHB4+xe2y/+MI+wFysmGgya8xAd1rc0uY1Zss7Ldis\nWSNek0kSvakffVQEwYsUEdfbtxelV5z2DwBduoi68a1a2c85fjywbh1w/Lg+b6JDtbXz5/VSSNWr\ni4Udp/fTyJGifnxysiipI1uyZ8AAcdaIk4kT5XotxMTIfzb6oar+Gh4TEREREVH+kX+C26y5TUQ5\nTDZQunatfJa5X9OmyY+11nO206aNyOTUGlrKcKtrbLVnj/cYVZV7bNevF9mvgPxz4Se4XbWq++1T\npgDffut/Xj9CNe977+llN/r3F0FNJ926BW/78EMRULaKj9cvuy1cGBugAnJBYuu81gzkQYP0RQuN\nU5by8uXB27TH1hh4P3DAOSPaGiAuWDB4zNatwOTJYoHC+DvaHZef57ZrV6BvX/OcI0bYH+uOHeJ1\nefCgCIavXw/UqeM8t6qKmvH9+zuX7Dl5EvjpJ3F53TpR8uX9953nfPxx4JNPgCNHgNtvd84OHzVK\nNB7169Qp4MsvnW9//32x+CLjmmv0z5FQKllSLrueiIiILi0xR2Lw5YovceycTad7IsoT8mdwOyyC\nZUmIKM+QDXp9/bX8nD16ADffLDc2J2qD25WbcFKqlL+gUK9e7qUXjLxqRQN6fXAZQ4aInzkR3N68\nGejc2X2MW4DPaPlyPTjpVtLjQvh5DLTsYzvW14o2b7Fi5u2nTwONGsntr1mz4G12xzt0KPDKK/Zz\nWIPb2lkDRnXryh2Pcf9ffmlupmk3r53//Q9YssS8LTNTz7CvWFHfbq3lbiRzlsTw4fprUXvMd+zQ\na5Y7zbt2rTjG1q299/Hee0DRos5nKaxapZfH+fVXEex3snmzfjk21v01/99/5jMQQiU+XrxGvUye\nbC5FFCrbt8tlzxMREZHuZNJJtBjbAn3n9cUjU11qCRLRJS3/BLczLDW3WZaEiC5joaz3beRW3sKo\nWTNg+nS5sadOAb/8In8MMlnxqakiYG5XHsLq6quDa0y7SU8P7YJAXJwI8P7+u8i8deMW4HMiWxoG\n8M72P3RID456BbfLlpXbZ3S0OSPcbd7sLip07erc6HLkSPszF558MrjpZK1acvubNQs4Zkn+0Y7d\n+hzaZYTLmjpVlEXxQ1XNJU7q1TM//m6aNw/epi0gHTggsutl9e8vjsNaqkbTpInIiM/IEOVv3Bhf\n41WqACVKAIsWeR/DmTPiubLz88+imaimdu3gMwesZBYOHn0UeOIJ73F+1a4N3HWX97iMDLnHxq/d\nu4Fbbgn9vERERDlp5q6ZSEgRf2gs/ncx1h1el8tHRETZkWvBbUVRYhVF2agoSoyiKKsD20oqijJP\nUZSdiqL8qShKcR8zmqMThXykDRIRXcbeekt+rJ/gxX33+T8WL64ZlwXjgQajgNLbkJoqMllzonzJ\nTz85N2I08grIaaZMsS8NEiqyWe6Ad7b/PffoixZej5k1uOvGmL0cqqz4G2/Ug43jxzuPy8iwf/y1\nzHcjr+CmZupUUdpFs3MnUKaM3H01S5YAlSu7j/FzhoRm+nTR9FOzZYvz+0omA/vaa8XPmTPdM7sB\n++DvkiXAZ5/Zj4+LEwHzV18V1+1eG8ePBy9CJCaKhqduFi8WQfD27e1vf+45c8Pj7dtFNrkbVRV/\nioaikW1amvfvYJWc7D1mwQKgZcvsHZObZcvkFz+Tk+UWAvyKjc2ZeYmIKP+auXum6fqwtT7/8yWi\nS0JuZm5nAmihqmoDVVVvCmx7A8B8VVVrAFgI4E1fMxpPQy3I4DYRUV5hbUroxKlZIACg49NAx2eA\np5sBhU/g5Elg8OCQHF6W06fd61cbFfexPBsXJzJZZckEgFNTRaBtzJjQzKfRSi/4yQiX1bGje1Zp\nYiLw2GNyc61fn71j+O+/7N3PTaga186ZY140cXve2rY1X9fGypSp0QKEdrWzjxzxvj8gstyNixZp\naXoNcWvw16med9euohyLmzZt9CbCEybIHRtgrv1tLGti1bKle7A6M1P/TFBV4KWXgMhI7+PW3HBD\ncI16QCw69OwpfgJivvvvd59LJrDrp0zRc895B/Q1fj5DChUSTUhDrUoVf2cBERHR5S01IxV/7vnT\ntO3nzT/j9HmJOmNEdEnJzeC2YrP/jgDGBi6PBeAv7y/D8Fc9M7eJiC4fRY4CNQIpxYXigZo+Om9C\nNMSTcfKkfbNCJ3//7eswpCxeLDduwADRCFBWWpr82PXr5YNTJ096j1m+XAQJnRoRGk2aJLff7Ni2\nDahUyXuMjHnz9FIoXoE/4+0bNjiPGzLEXJbHbYFh7lzvY8yOChXkxu3YYS530rMnMHq0uBwZKb8/\nryxo4z66dHEfqwXmrc9H/frO91m0SD/e3buDs6OPHgWef15cVlXRgBMAPvjA/Vi049i40T24Xq+e\neG9MmODcUHjePH3/sg16ZfzwgygPY9dc1srvPmX6IcTEiH3L1DLXnDnjPebzz4Fbb5WbTyYbXtO7\nN9C4sfx4IiLKXUv+XYKzqeZTHM+nn8fYjWMd7kFEl6rcDG6rAP5SFGWNoijPBraVVVX1KACoqhoH\nwPEkXgU26SnGbJQCbFlPRHTZqP0rEGYoTVXrN193z6mAqbH8gxuvWtdGLVrIj7U2IHQjW3c5IUG+\nWemkSSIr3csDD4jAtp/Gok6Bvgshs/86deTm2rFDL1/jJ8u9QQP5saEKYu7dq2f9xsX5e93Icmty\nmRPWWUpmqqp8YF4bb/Xhh+5Ba6/M6d27xU/r89amjfuimde8d9+tj9Mq9CmKfbNc61xHjng3ovTK\nhj51Sn7RzY+PPhINX6+4Qv4+Mv0Tpk2zr7Nvp1Ahkb0ukxU/d658PwZFkZtz61b5s0kSE+U/E7p2\n1RebQmn1au/eEYB4nmTPAskpss18iSj/mrlLL0lSoaj+R8LQNUORqfpoyENEuS43g9vNVFVtCKAd\ngBcVRbkNCIpY+6ucl274i65g0gUeHhER5Rl1ppivV1kAFJBI4btEpKbmzLx+Gsf9/LPcuD595GsK\ny5YQAfzVKT5yxLtEAyBff3fPHtFA0s8x+Kkpbg20Xohdu/Q66l6BLNmFkAcfBH79VVx+8UW5DHq/\n/AT4tRrbdmJizNednuO9e52zgydOzP7iiFsWcdeueokUO9WrO9/mFhzevFnUk/ZifSysQWtrM9EP\nPhAB/6pVved1C5wOGOA/UCrz3vSzeBMdLT+vX02aeJ+Fc+BAzgRr69YVjUJlysn4qcE/fry5YaqT\nHTvE2QOybr5ZnKXhZdQo+cWmBQvkA9Hjx8u/bgoWlFuMOHFC7v2nMb7H3IwbB3z7rfy8ss6elTuG\ns2dzpv6+8QwWL48/7t2rITs2bszdxYuUFPmEAco9qqpixq4ZWde/bfstihUoBgDYfWo3Fu5fmFuH\nRkTZkGvBbVVVjwR+HgcwDcBNAI4qilIWABRFKQfA8atjJgYCGBD4t0hszGBwm4joshN9BKhkSTWN\nSAWqzc6d48mj+vTJmXnr1pUbJ1OmQPP003LjwsPlgxLjxvmr0V62rPzYIUO8x8hmvo4ZI+o6JyV5\nB6f8ZNNqAfNQ1lI3lqRxCzglJpqDh59/7jy2YUP5/Ts1xHRqYKn54gvnLP6hQ83XjcFUa8b7WIez\nms+eBV54wf0YjIzZ/Dfd5DzOGtj99FPz9QkTgE6d9OtaXfCUFPdmvZmZcuU+jObOBV5+OXi73fvR\nKSjr57WozWF8DGSyuP3O7+Taa/017/Vj50735x0QWfq1a4d+3ytWiLrvMp8l2vtWZpFQJrtb06qV\neM96LQLHx/uvuS5zZlHbtqKeu6yiRcWCqZf/+z/xz4u2sCRzrIAos9Ssmfe4ffvkF8BnzJD/P3rR\nIqBRI7mxEyfKnTk3a1bwWVNjNozBe4vfQ2JqcCT/hhuAr7/2njczUz4IfuyYeMxkPld69ABKlpSb\nV1Fy7rOD3O04sQP7Tu8DAERHRaNdtXbodn23rNuHrhnqcE8ikrVo0SIMGDAg619OypXgtqIohRVF\niQ5cLgKgNYDNAP4A0C0w7EkA053mCEN/6MHtFmJjuuHXKeCjSB4REeVdtX8BlEBEQzVE0HyWJqHc\nJfMlX+OnnrSfoMSpU3LjZOuunz0rgmIymYR+ys0kJop66jlRPkQmoKjViH7iCfdxderoj5XXvLIl\nfHKKseb8K6+4j7Vmjzvp1s35thEjnG/bsME5iL9mjfP9zp4179PaqNJY3sL6mqxc2Xw9yZAjYiyb\n9OijwWUyrM9t8+YiIGhs2qnR3o+ffKJnuUdH25dQyU7ZHWNwOzzc/HsA4r2jBZL69RM/t24VQSvZ\nee3YZVYnJ9v3MtD276eWuFeD3E2b5D+/NH6y52U+n7QzLnIie/7dd0VZIDdduwJ//OFvXplj1TJw\nw8O9x2oLmaHM2tUWVsqXl1sUiI0VCyKh1KGDKJ/ktgim8dO/Q9a8eaLfhfZ6nLlrJp6a/hT6L+qP\ntxe8bXsfmTJjH34oMvhllC0LXHcdMGWK91g/i/WAuVG0k/vuE/ufOlVuTkWRC8S/8IL7grJGVcWZ\nHLJ27RJnPVzKjCVJWl/XGgUiCqBH4x5Z26bvnI6DCR51u4jIVYsWLfJ3cBtAWQDLFEWJAbASwAxV\nVecB+BTAXYqi7ARwJ4BPfM3K4DYR0eXHWJJkdS/9crU5QAT7L5C8hZJnoMpkxQHi1OjoaNF0MNT8\n1CffutV7zPjxInAhE9zWGjDKlLLRAnh+SibIeust9yCCTPakxhoEdfPSS/rlUGUHv/WWftkrkOkk\nIUFkZ2tWrQJmG05g8ZMJXaSI+frVV4ufkycHZ8dag9AyZQbOnROlhbTyCXZlFIzznj4NtGsntj38\nsPO81mClNdDWvLl4XwLAoEHiZ9263u9pVRWvkf795d5PAFCunCh3ZLRvH1BMnPWOK68UAaCEBP1Y\nvKiqCBhZG13aPbfnz4uSHlZPPRW8LTnZPtCbnTM5jPOMHx/83s/I8Pf5pfEKlGUnoOwnEC/zXtfO\ngMqJAD/gnWU8fnzO7X/XLuDOO93HaO+TnGbMqB23aRyS04O/98s8BrINoo1kAtGh6odhNH26+Px4\n5BH5+8i8ZkeMkCuPs3QpUKuWfJZ5jRpiMdTLwYNA585yc6anh/a1bSxJcm/1ewEANUvVxB1VxGp7\nppqJketGhm6HlKtUVUXPWT1R9OOi+Owfj1P4KE/KleC2qqr7VVW9QVXVBqqq1lNV9ZPA9lOqqrZS\nVbWGqqqtVVX192dKGoPbRESXlaKHgErLxOXMMGDJO8CJQGHbqHPAdTlQPJgolx06JB/I6dxZrjTM\n33+LU/9zIhC/f797xnF2ffyx95jJk+XmWrXK//5jYoCKFf3fz4ufgKJXwM+YZR2qkjPWEiXGeW+5\nRX6euDhRwgEAatY0B/W//lqULNB07AjMmSMuu5WeUFVRDscpuLRrl/m69rskJQUHbYzZ2GvWiOf7\nvffk3k8HD4q5tbMcNNbA2Jkzosa9lkXuJSwMKF1alD0wsvt9x4wRJT3stgMiC1NrclqoEPD778Fj\n7eZVVfcAnzH4ZJdNPWAA8P775m3Hj194trGf17e2OBDK0jVGuTVv167iZ04F172ysn/+Wa4fxoX4\n78x/mLtHP30rPjnelIGrvWZz6jHwmvf8ef+LNzmR7Q7YH+uWY1vw7B/PYtauWb7m0t4z2uKcDJna\n7wsWAD/9JDdfZKQ4O8K6uGcnJcX9745T50/hn/8CnYVVBaVOt826rWcjvXHA9+u/R2qGqIkUHy/6\necjatSvnXofk38qDKzFs7TAkpibi9fmvY87uObl9SBRiudlQMvTSDeeLReVQdy4iIrp01DGcn7n/\nDuBcGWD7A/q2mjbf1okuUbLZXitW6Bl6XmS/NAKipq1X47zskK2/6tZI0khR5Jv3Pfqo+L2++859\n3N13y81nZC37ESpeQTrj41S6tPtYY7mfUGUTGrNHU1LMQeEVK8xjy5eXn9cY3LTWy5VtOrd7N1Cm\njH7dWv/Z+tiWKCF+JiSYS7OcOQNEROjXP/zQHGz34rToYd3//PlA377isp+6u9Zmodbn9swZ7wWl\n06dFbWKNXckJ67xffCF+h+LFnef1CuYYn2ftLJAHHxQLHLI2bw7ej93re/NmYNmy4O2FCgUfq0xG\nriyvx2DmTH1/fjKdZQNlfoLrfsrjeO3fWrIoJ4zZMAYqzAcybuO4oHF+gop+mk96PbZ33OHeWNhO\nlSpyzU1lnDjhHOBPTk9Gmwlt8GPMj+g4qSN2nJCvM3KhZaJC5eWX7fs5WI0Y4d6rYO6euchUA0/m\noZtwZI/eSKVDjQ6oUFR0vI1LjMO0HaIDdbdu3g2YjWrUkDsjMC4u9KWEKNiwtcNM15/+42kcP+ej\nAQRd8vJXcDvN8FdogRQAXCojIsrXjCVJtgbO1TQGt2v8AYRJdLciIs8GlUayX3Tbt5evBfz55/IZ\n3n4Cy+vX+6vp7kULlsnU4AVEBrDM/rVAgFdwW6Y+qsaYERjKZqGa558Hvv/e+XbZJnhWfo7VGBwc\nMsQcfCpVSjRElJnX+Dq1ywz0WiDRGN8bW7aYa49b9//WW8CBA+KyNSNy4EDz9V9/td+HdV5VBV5/\nHRg92vtYk5Lc679b92NX5sRKex1rmfZux6pltstkeBqPqX794N4Hdp9J7doBt93mPNcDD4gzAVJT\n3QP2VitWuC+4GIN6diWZ7r1Xv2wtAeRGNljoNW7tWn1h44or7GvGZ2fe7ARAhw4F3rYvm22zg0yM\n2jAqaPOcPXNw7Nwx0zY/gVXZutsy88qWLbKSXbD1YuxfYD3WUTGjcOisGJChZmDAogEARJ12r+fg\nUgluA3KPldeChbEkCXbea7otMjwS3Rt2z7qulcHRFqQKF5Y6TAByJdnuucff4p6s06f9Ne/Nz04m\nncSUreaC+XGJceg+sztUptfnG/kruG2suR0WBkSy1ioRUb5V/ABQMZAmmBkO7AicC3u4EXAmUCC2\n8CmgUg503SOiHOGWaWUkmw0OAE2byo1TlOCMWCeTJskFtxVFZADL1DR97jnxhTwngtArVng35JP1\nySfAunXislcjRj+MwROvx1Y77T8z0zs4aPxyL/vY+nkOvEr5GLMM/QSIrH2XHnrIeaxx3quvdg9Y\nW73wgvjZty8wzpAA++ef5lq406aZa7g70RYX2rWzv9362M6apTdp9ZO9mmo5Sdc6b2KiXHmI7duD\nM/zdJCWJ8jvNmzuP0WIlx46Jvgsy7GqpO80bG+v+WjLGapo3Dy7lY1zw8cM4b8WKwIwZ5tv9vG9K\nltQvy/a7QOW/ERsfK+5fsCQaVRD/EaRnpmPSlkmOxxpKORHgDyXrQpcmNSMVnywztzObvHUyUHYT\nAOCjj9zn9fN7aQtKfp4DP33mvOZNT3c/EyMtI81U2ga72geNee7G5xCuiP+IFv+7GFuObcl6DPyU\nnZF5DPycvdGli1hUlgmaN28OXHut/Nz52egNo5GSIVY8ykfrp5RN2zENP8b8mFuHRSGWv4LbGYZP\n3bAIIMpHGgAREeUttQ3f1vbdCSSVClxR9EA3ANT6LfT7LnQKiLwIXZOI6KKSraH92GNA69ah3feP\nPwIPPP0vktUz3oN9uPtucbx6+QEVaPg9cMNoGM9ylM3eBIBXXhE/ZbPXAWDuXPfbjcETryCZn8w5\np3248dr/gAF6cLVcOfn9h7LuubGMifH3sp7V0Lu3/Lx/GdpUzLKU5JWtpXz6tPl4jh41B4+tz0F7\nQ1ypcWPzbcaxXmVIrNerVRP1172oqt5kVFG8FyuMZW/c5gT8ZaSXLi3Kpbg9X9rCgVfmqvGxWrpU\nNCM0sr4OVVWc5eG1WGWc9+BBYIkld8Du/ZWUFFzrHsheA1A01E8v6lK/C55t8GzWdWtpEq+gYnx8\n9kpLec1rfGytCzAXg9N7ZuyGsfgvwaZuTMt3fc/rRTu7KS5OLPDIsJ6p4sarNMw77wAffOB8+z//\n/YP4ZPECrFisInC0Prp3B4YZqlZUKFoB99fSP/SGrRmWrYULPyWCZN4TEyYAI0fKnSFw8KDcZ1By\ncu4vyuSkTDUTw9cOz7r+4R0f4sXGL2Zd7z23N3af3J0bh0YhlmeD24pdyZF0w7tSiQCifBSvIyKi\nvKWOoVPc1k7m27YbvoXX/B1QQtjdqc4U4LXSQM86QNEcKrpLRJefFgMwu1oVPLOhujgzxYWiyH8Z\nnTfPUk/5lsFAh+7AfU8DjfVv8xER8nPGxIhguExwe/Fi4NNPgbZt3cetWqV/Ec+J7HU/83qNGzjQ\nX2MxTagCCKtXAy1bisuHDgFffuk81lq/XJafhQsja73hPn3Mx5fd53b3bvfHzzhvp07yJXH69zc3\nQjWWp3nxRWDPHv16fLxcxmRsrAgsygb2jOMmTHAep6piocDaqNSOW5NCu+fg66+B664TpXyceAXq\njM+P1hvgf/8TtYdluDZWLHQKqKX3UWmoPIMDcx5BVHgUAGDdkXXYekyP+A0Z4n4mQPv2wcF5GX4y\ntwsU8D//hTLuv2xZ8dpKy0jDx8v0Dsxd6nfRB9WcDlTwrgeWnc+ugweBhg3938+L13MQG+t+u7EB\nafvq7QGIX05b5NIYG0uO2zQOGRH+Y0t+steNZzOEYl7Zz1ptoVQ7g8ZLly7iTDMv58+LsTJWrsyZ\nhuYAMH/ffOw9Lf7DLlGwBDrV7YTP7voMNUuJWjBJaUl4ZHJnjB4X+s6uqppzDYYpWJ4NbtsyCfow\n8QAAIABJREFUliVRIoACDG77cs0y4L4ngcqLcvtIiIjclYgFrl4tLmdEADvuM99+4DYg6Upxudhh\nqT/cpYSnAnf3BcIygZKxQLPPQjMvEV3ebhkEtBgIKCoQfUxczwkRycAthqLdzT4L6ksgE8RISBDB\ncJkazC1aAG+84T3uyy+BokVF0NhPFrmMmBiRwX7ihPs4rTZ6dgO7XkIZtN+/X/wcNy64PER2+cme\nfyTQ5iI1IxXRnZ8CnmsMlF9vO9YYvM3uYzB/vvm6W+b2FHNpVdx4Y/b2OXSo+bps8KlzZ6B7d/mS\nSGXL2m8/dw74zPBnhqqKZsLPP+89Z+XKerDKK8tdUfQzO6zZ7sYgmnZZKwvlVk9d6w1wxseJKFFR\nLiVT6v0ERIiyAo0qNMJvw+rjo3dLokONDllDxm/SOy0nJuqlmBQluPawXS3i9HSPADvMj8eUKcFB\nRtnXd2am+fUVqjIqxv2fPSs+J37a/BP2x4sPjCsLXYmh9wzFI3Ue0QdKZG9nd2EuJ2o++1lgsDY5\nBuyC24I1CNmicousAGhiaiLiyrisPFlUqyZ3rNklM6/fM5UaNvRuCvvFF2IBbuxY73n37XNfrDNq\n2hTo0UNubGqquYSWm379gI/n64v43a7vhsKRhVE4sjB+euAnRIZFAgA2HF+Np0eHqHabweDB8n9P\n+CmNR/bybHDbO3M7nGVJ/Ig+AnRuA9wwDniirWfG0AUpGA/cPMTxD2AiIk+1p+qX990FnL/CfHtm\nBLBT/8ITstIkdScBxQzdehr+ILKJckr1GWKfBVyKBxJR3tbwe6B1P/O2BqNy5rOl3s9AtCE9qsS/\n4uyWS0jVqqIOshdFkfvS2K6d+NI+b5732G+/FQF7maCA36DFyZPAII81i/r1/ZcyiIjwN96Nn7rn\nUwP/DY9cNxLnqo0BrloLPPwIEB7cyc04b6iy17du1cudpKe7L4is9/GVI1TH56dBr9P+ly4VwSTN\n6dPiTAgZhw/rWbNuQWhAlCVwCkLa1XDWau57Bc1lGd9L//0X/N5SoZpKknSr/0zW+6Rr/a5Z2yds\nmgAo9i8E68KWXRD6jjvMWfxex9qpU3ApCbvH4Pvv9feL8XhefDF47IWy7j9DzcBHS/WC2i83eRnR\nUdEYcPsAIDPwIFSbK5LcXGR3USongrt+SsNYn8/dJ3dj58mdAIDCkYVxR5U7HOdVFMWUvX2w3FDA\nLgZlQzvjo2tXYNMmqbv44vUYvP66XFkmwPya8eo/oJUkC2VwXfP7794LtRkZIrD95JNycw4acRCL\nj/yRdf2FRi9kXW5YviHeb/m+Prj5B1jxn81qiI2BA/UFZjcbNjjflpKeEtQIV7ZUUnq63FlEgDgr\nRyZhYOVKcVZYXpZng9u2jDW3WZbEn5bvAlGBd0hkMtBKIsUmW1Tg8fZA2z7As008/yMlIrJVx5CW\ntfUR+zHbH9Av1/oNsn+QOlPF6fxGUeeARsPsh1+oRsOBxzsADz0GvFIe6PgUUPEfXPjvQUSXjDqT\ngXtt0jCjkoAbR4Z4ZyrQ9PPgzU2/CN6Wj3g16bNq1QooVsx7XJ068l/eGzcW5R68Ap6bN4tMN5kG\nh5mZ4gurn+C2scSGnXHj9Nq3MsGstIw0DF5u+H/xir1AU5caKZLzynjxRRE0BETJG2tmd3b5CcjI\nlrVYtSp7+7cuMNx8s1zGpBfrc/DSS6KOr5djx/TAth3rYzd4sOgl4Ee7dsCvv+rXY2KACQvWAeVE\nhDBKKYRZnzyWVcO/TdU2KFVY1FI5dPYQUOVvqf3YvQ6XL/duarpnj/hMccrwtnv9dO+uN2512398\nPHDXXe77B/RGr0eOBDdOtO5//pHJ2H1K1BMuUbAEet3UCwBQq3QtYJOhY2zL/7nu81KqyXwhTT2N\nWdutrm2FghEFXeften1XFI4UTR4Si2zxHbtISgpu6BoKXqUujGd8eNXnduttkN39W+eV5bXgM2qU\nXEmULA2/hxooTXlHlTtQo5S5RtKrt7yK5pUC3YHDMnDnd11wNsU9hli1qui7IfNZ7PQYHE08ioYj\nG6Ls4LLo/3f/rO2yj3/v3vLNinv39s7IB0T2/K23ys3px/Tpeh3+nJa/gtvGzO0wliWRVmazyBAy\nqjcRqLg89Puq/QtwzT/icnga8PDDrFlLRP6U3CsyxAAgIzK4JIlmXysgJfA//5V7gDIS3VfcXDcP\nKGtT6PLmr8Wp/qEUcR643dBdJyoJaDAGeOZW4MXaIsheRLKYJxFdmqrNBh7oLEqRAMDhG4HZhgLJ\nN30jSiGFStW5QJlt4nJKNJAuatWi4sqc+ZuPsngFzIyeeQZ4V6LHW3w8EBkZ3CzQzssvi4CAdqq8\nm08+EfWtZbLiJ2+djH/P/Gve2PyDoL/t/ZQ7+eYb7/1qkgJ9nf08vqF0++3iZ2ame8PUJk2y10DR\n6zmQLbcybpy5sZz1ORhl+RroKOosGt2zESh2EAgLju5a5504UW5aRQFmz9avG3sE9OkDnK6sR8hT\nYx7GhlXFs65Hhkfi8bqP63e43r5egVf2ulGbNs73/fZbEYCLivI/r9u4w4dFSZb5870X5H76Sfys\nUAGoV8+clWmaV8nE6L16Z8XeN/dG8YL6Y4fF74rSfgBQZREW7FuIc+fsA2zG37NYMflmqW4BzoSE\n4Br9MubOdV+McXsOZuzSU4PvrX6v6Ta7gG3xgsXRuZ5hEaDx0OBBHnIje92obl3327MT3Pazfz/9\nKbzm9VPmKC0jDbjx+6zrPRoF1z0JDwvHuPvGIUoVK9rnC+1Fn7l9XOfVfp/s1j1PzUjFg1MexLbj\n4u+x95a8h583/wxA/F+xxqOS5pdfBpfM8iL7fKWluWebGymKXEb4ffcBjz/uPS4U8mxw274sibHm\ndjgzt2Xd1U/UjwX005MAoE3v0DZhC08FWr1p3lY0DnjkIdtTGImIbNUxnNu5524guYT9uPSCwO52\n+vULLU3SzHA++eqewJmK4nL0MccvU9l24/fi8xEwfy4DQOkdQOvXgL5XAY88CFSd43gaLhFdoiot\nEe/f8ECK7vFawIS5wLruwNlyYluxw6IsUagYzzxZ/xyw+Qn9ej7P3s5rPv7YewwgvrDKlKn46iv5\nMhnJyUD16jJNC1V0Gf6pfjUzEImNOgfcaf57f/x4YNYs4LvvvDN5X3pJ/nRrTSgb991wg3eGu9W2\nbd4NU/1kRGoBA6/gtp9yK8aFjWxl4ZbcB/SqBfS4AehbEXg3CkOiSqPOt/Vw1/i70PX3rpie1E98\nllRaEnR8bvtcuRK45x6HsZFJopySJuaZoMZzXa/XS5Og1q9ZpUnT0vS5vILQFSvqj/uffzofK2Cu\ni+tVC16rN++1/x49xPsDAAoVMt92yqZClVau5sABcwapaT+1fkXsuUCNp5Si6H1zb/Mkp68DNjyV\ndfXuT99BdLSKF14IboZqnPfsWVHyYs0asf3IkeDj02iv+9WrgRTL1/333gOGDzfvQ7bfglZPff9+\nu3Ii9veJT47H0gNLs663q9bOlCns9B7t0dgQFK39KxAdh6efljtOIGcaCoYyYG58Lcoeq9f+Fy8G\natcWl6tWDX7unfhpWutl+s7pQFHx4iwfXR4da3S0HVepRCW0TtOjxaM2jMJv272/M2anNIuqqug1\nuxf++e8f0/Zn/3gWKCeiyl7B7ffe896vlZ/XS4MG7rcnJOhjvHoUZGf/FyLPBrdtBZUlYc1tT9fN\nE3W2ABFAmTQdSA/8hXjVWqD+eOf7+nXjCHG6IgCkFNX/CK64Amjb2/l+2VUgAbj9PeDe7iIzqtMD\nop54t9tF05uedYDeVYCXqopxDA4R5Q0yJUk0xtIkF1JXtlwMcG2gc1pmGLD8NWClYWW/6eehWwyM\nSAaaGQIGc78Cvl8FrO2uZ6IDIihW+zegczvg6dvyz/95BeOBEvtDm7FKdCkpv06UaIsMpOedrgyM\n+wtIKgVkFABW/58+tukXCEkponIxwLULxeXMcGBlb2BFX/32mr+L4BVRgGeTymqzgbKB7n+pRYBf\nDGm6N4wDrtJrcRw5ArRvD/TqJbfv6Gi52uOffy6ykq3BTjvTpomAi1ed1ORkvYyIbBZuKIPrp04B\nNUUPO18lZypWdL89Lk6/7Ls0TKFTwBPtzD1HAKREnMC2k1swf998jN80HguSBwF3vwI8dTtQVzJt\n28PJsr8ABQO1N05WA/69zRQAS04WtXNrlw5E0qKSspIZjEHX114zz2tdOPDTyM0YqHn4YfNt1sdW\naxYqk+G9aJH9/q68MnibVvsYEMFmTdZ+lExxFoVmdS+ULGTTEXXJO1ln8WRUWAFUm4ORI0WtaKPg\nIJ1eoqZCBfvjNrr5ZmDECPM2u0DmwIFiX7fd5j6f9hxce23wZ5XdY3vgADD0z3lIzxQLyjeWvxEV\nilYwZcCOGQMsXCgC+61b63Xobyh3A26pGCjeHZ4GtH0Jo8eKeSZN8u6loB2rXbb7nj1ytZutXnsN\nOBR4O/pdDLTKTuY24H6GgfHsCyB0QXPjc+t1hkPnr/SykVf++ywiwyMdx9bJfBzY/GjW9edmPIf1\nR9xXD/2UZtHe20PXDMX36/Vs8uIFxJkU59PPA53uBwqd9PUYyFq+PHSNuv/9V8/uzolM/wuRZ4Pb\nUg0lq/4JVJsFFM6BNr35gZIB3GX4n37DU8Cu9sDyV/Vtrd4MTcCkwBkRQNYs6g/8ZSgG1WiEaKgU\nKmFpwKMdgZb9RQZk/Z+AWr+LQH7lJSJwX2YbUDJWBNxb9hcZ5JFJoTsGIgq9K3YD5WPE5fQCwE77\nVfgse9rqp96X3yCCptlhzHjc9jAQXxlY9xyQHDi9s9QuoMYftnf1rcEokbEJAGfLiwzLQzcBM0cA\nnx8Bpo0CDjQz36fiCnG2TV537Xyg79VAn2uB/xUQtcafu0lkuLbpIxYR6kwBrl4Z+lIwRBdDqR2i\ngbdWOu9sOWD8X8DZq/Qxa18AUkWNT5TbCFRZeOH7vcVQa3vrw8CZSsCxusCe1mJbWKZo9k0k69ZP\n9Mtrnxf/N26/X9/W7v8uaNFXJmB85Ih8Y7H77wdatBDBMC+dOwPly8sFL957zzvTFxClLJYtA/r1\n8x67Z48IBrhlxFrJBGe1Wu5ewRFTIDM8RQRdSokmfMiIBBLLAqpHCmXb3kDB094H5bH/w+UMqf7r\nnwFg3m+hQqLx35PXG14IgbPpjJnjM2aYT7e/kNrvf/3lfJtTZumJE+aAn9044wKEH8bAUdbvVeOP\nrDrlBcKKmBczjc5cA6wz9H1o+T8AqmdG5qJFItvcD+ucTnXPAX81ejtavgrYPbYPPwy8Pda5JImm\nXz+xIPHXX+ZGkH1uNiS01JkK3PckMjIz8Prr3u/pzEzxuxctGnxbtWrmrOZnnxU/Z84EfnNJHl61\nSpSkOnlSvvayndhYc48JP0FIt0UN63Mbqlrexuf2wQedx+08sRMpV2kL+mEIi3Ev1B2mKMCsoVln\n5Z46fwo3jrwRLce2xB87/0CmGnxgO3e6H6vxeFu2BBbFLkLvufp3tSfqPYGVz65E0ajAC6NkLPDQ\no0jLcG+6YXwMZI4BALp0EWdPhUJOZPqHSp4NbttKt2RuV5sDPNEe6FdGZOg+1El8Mb5mKRB5gUtc\n+cH147L+00NqYeDvQPB52RsioAKIUzlulTw30s2tnwJFAm2q4ysBa14EVrwMbH5MH9OulwhYhEKb\nPkCVRf7uU2sa0K0FUEQi/YOIcocxa3tPGyDFo+tXSjFgn6E7T61sZG8XPwDUnaxf1xYAU4sCawx/\n2Tf7DBcsPMX8mftPP1FeRZMaLRYiRy0Dvt0myqNoGo4SfQ3yqlI7xCJjlOH/56JxwFVrRIZ6kyHA\n3a8CD3cCnm0KvFgrn3xeqyLDrcGPQJhEFznKu0rEAl1b6X8PnS8JjJ8HnKpqHnf+CtOp4qbAdHYU\n+89c3mTFK/aXG/6Y7WAUXWauWQZUCkSfMiKBlS+Ly39+bjgDdE3oS3ZdRLKBxv79gf/7P+9xEyeK\nbFSvLE9NWFhwVrAdRQFq1PAeB4iGYdu2iUC/jEc6qUDHp0VikObXn4DBccD7qcDgwyj923r89sBs\nFPv7R2DBh0BCYKGuyHHgzrdN88nWV+7bFxg2DNh1chdOFwvsOzMc2Oi8kvFEvSf0gHuVheJzz8LY\nWC9UjU0BcwNYt7IJlSvLjQOAyZPdb3ci5lWB5u9nbUtZ2gNIKoV77eO5wNI3gbRALZQK64Ga07Bw\noZ6ZvWVL8GtRNrBtLA1jbUZqV3bHTwkPp8fQ7rnNVDPE2SYB7au3t72v0/4fqv0QKh81vNHr/4yn\npj+FTIh02BMnnI9TVfWsWbsSM0Y//iiC5ffe6x681Y7VT9Z2+/bB5al27TJfr1kT2LFDfP55vUZP\nB/5cWL5cz3LXWJ+DUAVBjcek1elPTg6u6z18raHeza57EXne/dQWRQGQXBL4fRyiwqOyti+KXYSO\nkzqi5rc18d3q73AuVX/AvRqFvvWWOBsAAFBiPx6a8hAyVPFCaFShEb6/93vULFUT4+83VEq4bj5m\nJL3lfawBy320S5EtIeLFuP9nnpG7z+HDwCMeJ1uHQj4Lbht+nTDLeVwlY4G6U8QX46ebA28WA55p\nCtwyyP9pmBHJosZpu14im+yOt0W3+1Lb884X08hzwB3v6NeXvwacDSy/pUYD8w3ZGLd8Lr6QZVex\ng0ATQ+f0BR8GgjUK8McPQNz1YntEqng8o7O5bK25cSRwk+EcozUvAL+NAyb/AkyYDYxeBIxcDQzd\nDAzZC/xjyFS/ao0ImpTacWHHQEQ5w09JEo0xkyw7dbebfAWEBf4q3d8CONxIv23VS4ambCuAiv8E\n3d2XG8YAxQPpV4llRf1dJydqAbO/NS8S3vuc7Re6S17hE6JMQ8FAp5j0KO+ssJKxwP1PhrY3RG5o\n+S7w0ONAx2eBDs8iJCUo6NJT9DDQ5S79tP7UIsCEOcCxevbjV/bR3wPV5gClt2V/3zd/rX+Gxd5u\n/gzbexdwNNBtKuqcqfkSkSNj6axNnYGEq8Xl+CqWM0DfEGUCKUdZA1RO1q4F6tSRG1u0KDD1eH+g\nvqHe9V+figx9AMiMABLL4/imBnigflskLH4aWPoWMMfQFLfRcKCCXkC2aVMRGJGpm9u3L/DAB4ZO\nl7vuARLL2Y6dNQtYNf8q0UgcEE166/8UNO7dd0X998OH5bLXV61yHwMAKHIUkff1QPVXuqPFA3s8\nM+hnzJD7/R99FLjqKvfGiZrUVGDCBMNzW22OCFIDQFrBrEXMmTMDh1xEZOxmSSwPrDYUn77jf4CS\nga1bRQO/iRPFY2ZkLQmRlCSC127ByYkTRWmOhATRYNXuOfj7b/3yrl3id9u9O7isjLWGc2amKE90\n4kTw46sowNqjK4DCIrJcoWgFNCzf0PYYjUFY4zyKoqDuwSGmpJbxm8bjYIPnACXTtQb32rV68PfK\nK73LkMgugPmt5T1rVnCTV7sFhm3bRBkXWc2aiabFRtbnIDNTPOfaa9Doiiv0y8bXT1JS8OvJ7jUz\ncKCo6511v7QkjNk4Rt+wpofnY5U1b2wLrH52NR6t+yjCFf3B2X1qN3rN6YWrv7waaPW6iG8BGD3a\nec5p0wIXohKBxzri5PmTAICyRcri906/o1CkWFDqWLMj3m2ud5FemDIIk7c4r24ZHwO3BsypqUBD\nw8s8VLXfjc+tdRHuTPIZbIiz70g5dart5pDKs8Ft27IkxprbWzuLeoL/NdUzCIzCMkV3+Nb9gN7X\nAc83AG770DmoGR0nsqoevQ/od6WocXrTdyKbrPlHwMOPAr1qA28WBbo3EqvcTb4SK8eX4h91Tb8w\nnPZezvyHKCD+UD3UWFyOSBFNJ7Or5bt6XcnDDYEthkBMWmFg0u9AUuBTrdhhsTSc3Vqr1ywF2hn+\nc97yiDjFZFMXYPuDokTBv7cDhxuL03FPXwv8NQiY9Z3etK3kfrHwUUmiQw8RXTylduhnm6QVBHY6\npaBY7Oygv78rLve3gFYw3lwyabnlr+vE8uLzUtNM8i9SO+GpwG0f6df/eU18RroKnEYXX0lcLRQP\n3N81b/UQCA8sbGo9GVILAz+sBN5PAb6MBUYtBX75WXypXtXL/LxX/TNvN8Kr9Rtwu6Em5g1jzTUy\nKX8odlCcGXZloEtdegFg4h/AoZud73OqKrDDcK61MUnAjwIJYtFfY/17D4r5dPWbv2a9e3JXZgtQ\nIxChUBXxf5XR0jf17N3oo+L7FeU5iVVHA7fr2b9Y+3zwc21n+/3A7kB3TUUF2vfI1t8kyanp2Bpu\n6JS4/lnHse3bB7JcNxoKRV8/DtbF4n37RC3pq66SC1w3aeIRiK60BHihAdB4OHYX+x6L69QRZT1c\nylxqWfNupU00hw8Dzz/vPe7110XZgW3bgLlzVXMp0HXPmxYFjh8XQcOff7ZM8k8/va9Lma1AnSl4\n8kmgRAngo4/gqUgRoFEj72D86NHArbcCtWp5LzDUqCHKE1WvDgwebL7N2vDyiiuAcuXEAortvNX1\nqOpdle5Bkyb2T+zGjfplrbHf7beLrPuZMxSRVLLWkHjSYDTQ/nkkp2QiI8O+bvrChUCVKoEr4SlY\nf2hzVu1vWXbNg0+dAt54Q1x++WXzGQRW11yjH8ubhn6/do/VvHmiRrrf4zOyznvgADBkiMhI/+QT\n822nDSeMHT8ONG8uLhcpAlNNdMD+/Rgfb74+ectkxCcHNp66Dth3V1Zgt3Fj+x4NxnmvL3c9Jj44\nET/U24+6Z17LqosNiKakuPUzoE9l4KFOGP7XPGRkZqBIEfPvkfUYKJnAfU8CZUWH5qjwKPze6Xdc\nXexq09j+LfoDO/WzCZ7+42lsjNuIfTY5uMZjjYkJvl2TkGC+PVSlQazP7cqthzBszTDcPeFulB5U\nGg9NeQhqYGcyvTNCKc8Gt20Zy5Icry+acP24HPjoLDBiHTBzGBDzlMhQsWaEld8A3PmO6ALds44I\nyFZZIL5kPnsz8Gp5kVVVc7poVOEkMhmosE580LV5GXjyTuCVCqIuXbhkm1iristFHcSSe73HyoiO\nE2VCNH+/L7K1jdQw8fhp6kzN6nztS5nNIhNR89cgMbdRfBXg14l68KnSMqD1K/Ct+L9ApwdFkzUA\nOHIDMH0UrLXZbK3pKZppajUuC8UDXVqbu3MTUe4yZm3vbifKgshIKg0cCHSmUVSgxnT5fd44AigQ\n6DtwrLYohWJlDBbVnJ79Mz+uHweUCKRKnCsl6u7KSC4B/DZB/wytsujCguwXlQq0f8F8yvNvE4C4\nBkBmpKgLfOBWsSj6Tz9gzjciKLjsdX38nW8CV62++Id+oUpvE3/wWt3xLlAvOOOM8qjiB0Qj6yt3\ni+sZEcDUycD+O7zvaywZcv347JXhafiD3ozteE3x2Wm1+XFxpgggMstrX4T0Gk1YGlB2E/vj5CXG\nElw77hNnERmlFRGLkZqmX4p+GZR3VFkA3GsI4O1uI4J6Mt+poACzvxFJCID4XtxouPtd7FSbLUqT\nAaJc5p623vfZfr84KwYASm8X+84JSqb4bv9kS1HCUxORKhasX6wdaGIeHE3SylM88EDQTSHRY9B8\n4OpA5D49KmhBokwZ8fPtty13TCptbpTeYkC2zkh/weNP15deAjZvFmUvZILmss4ETvzbs0eU1Qhi\nCG7/+nF7rF5tXwPb6I03gHbtgCVLDPXS1TBg1jBgvSFV+8Yf8FfkiyhQUEXLlvZzZRTdL85k6Xs1\nHlpQH1e93wAT5nifcpGeLuoqV68uegYYDRyoZ2J/9ZUIro4YYR+8/c9wUudww9vRLri9fbt+ecIE\n8TM+HnjnneCx2pkKO3boDS7t5q1XT695bwyu21m6VF/MsAZ3Zc56GLZWbySJtc8DalhWcHvtWtEs\n1DjviBHA+4Z1PG0f08dVxJYvP8PBvgfxTdtvUPUKQ3p4WAZQdwpW17gblYdURlKTd7By1x7TcYSF\nQZQHqq2fNfzFHcNRM7ppcKBZDRPff05UByCyz++ffD+uq3sqqJSNzGOQtX8Dp8xtVVURcyQGSu3f\nxPepIsewerVzJFxRICpW3Pox8OzNaPrL1eg5uyfm7Z2HtMw07D29F1uPixpQoWy2LCPPBrc9G0pG\nGG7PjASONBSBgumjgGGbgc+Oi6Zcu+7RTynXlNkmVqqfbCVOy7na5kvzyWrA8r7A72OApW+IFeqE\nq4LHAeI0z1ZvAj3rBj5YJZdNrl4hTmF9phnQtg/wUjWR3VbRR3EdOy366zVNj9YVAX87/91iPt29\nTR//K+939RPBJEA8Rk5f5va2BhYY/oe7+VtzUNxL5DmRVV8k8OXoXGkRrE4rIj/HrvbA6CUikx0Q\nf6A8+EQg4yQvnSauXpzyOOEp4ksp0cWSnZIkmu2GbxGypUnCU0WdZ83yV4MX5wDxxd6YTdw0G/Vx\nw9LMWdvLX/X3+XXgVmCp4ZtKy/8BFdb6P46L7ZbBYjFYM/9jYIdEMdCF7wMHA2kl4enAQ4+KxsV5\nRcF48X+WtnByuoooeaPp+LQ4E4nythKxIrB9ReBbVEYkMOUX70a4mgPNzGfRNR7mPt4qLE2cRahZ\n0df+MyyjALC6l379ls+RY3/3KBlA+XWiLOATbYE3SgI9rhdJJPf0AKJ9dNCji6/4v+bED+NCo9Hm\nx4EDt4jL4WnA3Q7N7OjSU3or0OkBPVkorj7wy2RRhkTW6etEiRLNnW/7LzvZ0FAceMOTcvtPKwJs\ne0i/nhM13wudBB7rIL7bhwWiRedKAYcM5Z5K/As8+gDQuS1wpWTNmJCwZG3HPGNuVuxlxSvA+RLi\ncqld4n2bB0t1LrHm45XYL7LRASCtIBI3ifI1iYnec82ZY7NRDQNmfC9el5rGw5HR+v8AqHrwUQnU\n+X68vagUYOhBdgxb0GVJ48AiiLPISFEHe88e12EARI3nF14Avv7afVx8vF4eaJzNW0Tya3r2AAAg\nAElEQVTLWAfEGQFnzgAlSwIf2pyEUzFQynr9erEQ8OOP4iT8sDCIM8cq/gM0Hgq0eh1bo0aL/0Mg\n6rgDwNmz9seolaH54gsRVN+2TWQhW7O0je64A1h3eB3WHA78AukFsvqXbN6sn62waZMokZGaKhYP\nJk0KnmvzZhFkB4BD+6PRs1EvPJ+6A9M6TRPl3QwOJhwEmn+IdnOroenI5hgdMxqJqYk4e9XvQMsB\n+mNZ/SV88/RTWWcZGIWHA0gpDkz+PesMiv3x+4EHH0Nqmjn+ZtcPYtkyEbg3sga34+L0xTUAOJd6\nDj+s/wGNvm+EhiMbQn3kQeC5m4HXyuLm6dGoM7QOmg+/B10nv4hB/wzCpC2T8Mb8N9B2Vk1RsaLV\nW7Zx0hv/n73zDo+iWv/4dze9JxAgoTfpTVFABQTsIIh6AXu56rX3XhC59p+99ysqqIgFC0oH6V16\nJ/SSCumbbDm/P747zMzutISE5vk8z3m25M2Z2Zkzp7ztZHbHwfJjs3+LSxytrStrEJfLJWJRBg/i\n9H+4dgdw8w6+H9cU+MzBdtgAF8RtJgHtf2SOqqjycJlABJUHmwYDmwcD+W2M64rLZ9hBg9Uszf5S\nw1AVtlwMTH4TyDfZ/aPhUnqOnzLZ/Jx39+IiZeNlVZts1FsH3NFFHYzH/mnsiaiQvBu4p616TX75\njAOlE1pOB64PbuQWcAMfrTTPLQkAEMCw4UDHYHZ+Xwzw7S9UfFt6CghuMNYx6GnkjwK+nKF6alaV\nlJ3ANQNp5FBYcTO9EHxx5v93rEndQS8CZSK3pxfb7K7efG+38Z5jBA0PFz5EBffy26iIU3K2n6ik\nbaO1csugGrxW1SA+j94SaVn0oNh2wbE7l+OJ+muAO7vwvTcOeDUnPOLEiuTdwIPBmDx/JPBqLj2e\nreg2BhgaNP4VZwJvbacSyIimc7mfA0CD6Vs7TXND2h6rrC7w1o6q/T6ARq2b+jDlFkAj7Mcrql6P\nEZHlVGgUNwyOGQ7dBqxo+ysVvIoBdOX1wMQxzutO3Q7c3k31Sl07Avjh25o5t9rE5efCuE1wJ5zK\neODzhUBhU+DfZ6tjT1kdfm8235Ac36Rl0atPicbwRQPf/0hDelXoOJ6p7wAqUN7c5Xwu0ulb5nMH\ngJL67Je0G9Rqic8DHmiqzvfGzAJ29KvauRoiqFRoMZOl2V+MjjOjMp6bE85/hAs9yfHFxfcCPYNJ\nPrf3A76cZS6buRz4zxlqH2+35vgnElHJ+U1+G+fRaLVJ4gFGLCv9VlFD4LPFak71qhDp4ZpTiVpZ\ndS3w89fW/3P4PPYDDzZR9wp4ZzNQcIqz/20xk5HTAPvMN/YC/mjr/3FKoyXU3CnXB6ARcsJ4pqnr\n9gW9c5VNgwH2/QsfAuY8VTWnhaoSUUEl4kVBQ5I/CnhnK+cWVaHv83Tu05LbjjqHDZcF92w4zudZ\nofR8B7j4Pr7fPBD4ZlLN1OvyA0NvBLqOVb9bdB/v9alfMGIhzSbBNkAj4cznHep0BNByBtDjPUYp\nzHiRUY7HEleA6+gGq4GMVUCD1ajTfhUKxA5j+YJWQNa5dHrcPoBRAzVE6wdvwdbkz/mhKn1OCHFx\nQLlGJfjdd8yDv317MMVMg9VAty8QdfpYeKPCdxONdSfAUxFQ51RZ5wJjJ+vu8dVXc0PQsrKQNEXt\nfqZxLEiL7Pvxzf134cbh6XjwzhTcdpv++RNC9eZetIjGkNmzgf79gZSQadQ77wDjpmzA+oSP4D71\nSxRWHLljUIQrAv2a98PZ6UNx89lD0DSlKTweIDY23Mt89Ghg1CgXhLDbWKl6nFzK7St3AbcFvWO+\nawJ83KrqlUeVUsHd/kfmlMvpRGX21ou4e31VcfvYsfUfqZ/M+6OouJrztKpIy/gb6D8KaPubvo5A\nBLD/VKCRgRfeweas5+9/O5sUXT1IXVBvOx/4egpsB6h+o4B+QStwSX3g3S32yj9XgLnHM4OJflb8\nG/j1c/vziy4BbumlWlcBphdZcjfD0o3yz/Z5gSllFH772HoTNifEHqLXQgvNpN2TQk/2lTcCe3vg\nuBnY07Lo8dn1S9XLIpSAG8juqiq7d/WunjI6Lh8YfBvQ4Uf9975otsH5jwGHmle93iMhfSPz4Zdk\ncHO/QFTV62j3Mz0/IyvpQTn+R6ZFOKoIeiYPvEeNQACosJvyxtE3HkQXA83/AprP5uTp75urPkGu\nCSIqOYHr+5zah64bBkz43vr/jLj1DLUf/elrfa7sMARwZ2e1L5r+EjDvcWv5m89SFctzn9BHo1jh\n9gF3t1NzTs94Qe/xVBXSsoDbu6oewStu5sa9R0J8Lr1OFOv86mvYzx7JIi1jJfDv3moU0c7ewFfT\nzY0HZnSYoN9+uyoG2GPFgKeBvhr3lwnj1UiE1B0cAxODMaUFrYDPFh75xD+ikt5Dnb/h/Vz4UNWV\nrCcq9dYDCTnAzj6AMNg5qTaosxW4YQCQEowD9sUA438yTglih9sH3NuanoBAFeY4gvMwZVOxWaOB\nv56x/pdBdwBnBOOVNw1mCqBqExzTBjwd7uQRSlldID4//Ls5TzFtXFX7haNBVCmf04QcpotJzNa/\n+mOYsmPzJeYGhRON+FzggWbqYv3rycC2C63/Z8gtwGnB+X9eW+DD1UemaIws5xyptucjKTsZ/bSz\nb40qXgBwc9nWf9KxquU0jtdldYBZ/6XDSFWclmqSqFLgpnPUVB4VicAXc4ED3apfZ6upwHWaNuLU\naNb7ZXpGA/SSHDPb+TFdAebDVfrfb3/h3itHhOBc9MKHGImgMP9hzvW0a4+4Aq77T/9IdSYDgMIm\n7IM3DgXK0o/wfDREeujlfvYr6obkALD8FnoXV5XoYraDTJNkvoWN+Rs2Xsbn41i1VyvcXqZmaTWV\npeFS9V78/qHztH9OcPmBy68DOmt2agxEqIYZLVsv5JhWnEmHvrQd6t+yBjBNa2l9kwMFldr9ngWa\najav9yQzZd+q63DUdBMuP+fxLWYBzWcBzeYCMSYu2E440IVK7qzzWKo75sceYjpgZYz6fD4zEdQm\nEZXAKZNozDjlD+P7XtAS+HQJUF7XcbWtbx2JrY0M9uDxR1InWZbO+srScekFdfHLuAZML1eSwVLa\nAK89m4GH70kC4OIz0W4i0gd+gLzE2eH1emOpH0rIoUHG7n5WxjNV1Mah+OW1gdifVQe3306P/GbN\nmPKnb1+DKAoAgFRu63C5XCIOpShHiKJz2G7gzqBy4IdGwPsOLbxHg/hcWkG7f6J6MABMgTH3KXYM\nHUJC5YWLSoS/nqG1uv5abpzVeRyVcFo8KQyN2dMLyO5CD4BQJZ/Wk1q46Emd3cX+3KNK6b2dHEyk\nNO8xYPrL1v/TeRxwRVBx5I0D3t3s3OpfZytwaw8gLiScoTyNippld3AjSICef1dpwnuX3An88b6z\n49gRUQkMvhXoZhCvk9ueSu7V1x47j+U6W4JK7a+NO1I7dvamMnrzIDgaDFtOo3Va2YjUCH8kr8m8\nJ2rf27DeOnoXdBqvPlNZAxjy7UlzXk+3McCQm/UTUG8s8PtHwCqDfLi1QdI+YNCdzNdsREUSFZ5L\n73SgnBH0LOn+CSd0njQuTA6XrsaDq9vHyV+rabzXjRfpjSXeOHrSzX+0dj1PtL+j3S/A+Y/oFSMB\nN/DVjOp5FPZ+iSFUAJ/hKW9womnU/lv/yY2DASr339ht367a/8S8/wDDOt/c5czo2OVr4PLgJkjl\nafTaPpLoga5fAZdp2u74H7ihbnVI2wZce1G4ciq7E71Qq/OcJx5gH68sPA+2AD5dXH0FwuD/AN2D\nCzhvHPDJMiC3Q/Xqqm20bQQwHk8bLuXCUpmc7zqLbb7KCjLB57jr1/T+jQ9J2rdxCDD57aNvkDwa\n1N3M39zpO9UTPutc4IfvalapYHbsGwaocyZvLPDdRHsloBW93lS98XLbAR+sM04voqX5LODGAeo5\nvLnb/rfX3QTc0079/O5G8yhDK5rOAy54WM37GkpxZtBjqz9fDzXn5rDnPU6vLy2HmlHpt/qa2jdO\nuL006ibvpoJKKfH5ms/5/KwY5uwoT6XxatX1wUX2ceIYUR20zi77uzEyyO73JGQD97RRI2wmv0HP\nfMcIev62nsxxufls7m+0cQhTTVZBWWBLXD6jQDuP4/4/AO/flDeD6Qeqee9cfqDRUipA2kwyVxoC\nHFsnv+0sJ39N4QoA7SYC54xWN+0ORADf/OYsz7Ud/xoBdAo6JOS25/rTysDRYgaVhUou65++AlZf\nV7Vjnvsk0Oclvl9/BR0aQg1QinEqLp/OUxUpjOjTlRTO4/q8qEYIA2wXE8dYp5jK+BsYdBfQZKH+\n+4Ab2H02Fe4bL3XukR5KVBn3hTn7VX3eb4DOb/+bV7WUJKF1t/+Jzj+tJ5vvN+ZJBvadQacvpVit\niyM9VIo2Wsy1SuPFXAPtOIdr0h3noHrPmaD+QFFmt5hlrJzzRwJvb69eJIIVbh9wxdX6NqJQnkYH\nsGW3c6NohbgC4PJr6VSpUNSI69g9vTQVCEYj9HtW7ZeMWH8F1661McdxBaiHaj4rGIE1xzoCS8Ef\nxWc+uwu9yxsuY6Sr1f515WnBMfO6KoyZgv1qz3e4MTtAhflHKx3+fw2ReADoMpYRHMrcsyIR+GwR\nkNuxanWFRnlWF28cld7RpXrnOYX8U9g2V96oceQVNBSk7qCiO3UHS8ouGl82DeGcOhhF+NhjwCuv\nhFdtjlRu6zBVbl+2B7g3uACf2BB4+zgM5c1cwZA+rcXNiLUjgNmjwjdpAfjgnPE+Q49CF6sKvmgu\n7rO7skPJ7kIlUWYwk79TT2oFrbLaF82Od2df45DRSA+9EBUvozlPAjOruFN62jZa7buMC08TI1xU\nyG64nNdS8VDc3g/4emr1PHdNEUCP97mwVHJmagm4uWBdeSMf9Op657gCzLGeGUwhoLHGoawulWrK\nQjZ9I73VO3+jV8gCnBT89Qy9Y5rO5wKz6VyGzoTKKmR34oRi7Qjjaxfp4YZtZ76l/37p7fztvV8O\nX8AG3ByY5j5pk4qmGjRYTaV2hx/0hiKFvDbAN787myz2fJv57M1Ydhvw59u16DUmaOm98EEgVhMW\nVNgY2Nsz3EN+X3dOXPadjjBiivicnv5xuHIglMImVHIf6MZ21nw2JyzK4tOKokbcKGrN1ai1CUPG\n37wmLWbrv89vzY1utwyqXr2hihuA12He48zRqPU+uf5coOVMvl94Pxe3drj87PsURbCTRbzLz42H\n0oM5GWf+F5gz0vp/bBGcZHcOJpErT6O3XFUn8w2XMUWT0WQI4KJm4hhnObIVIsuBG/upXuCeZKbe\nOBJldFQZvfKViWR2J3pImKVucAVo7D31cy4u5j5VQ+kXbKi3jrnsFKXY1guBcZOMFXbtJjJ6SOnj\n1o4AfvzGXqEJ0Hu/y1gaTew8Zr1xjCBb8FDN93Mu/9HzlAY48e74PRXaZsqjQ83oQb3/tNo5h/SN\nVGwrigZvHL2fs847snpjioAHmqh99Ljf7fvBqy+hIg3geD3JYb7uq4aoEYRV+T+Aiv3zHgfah+QQ\n9aQwWlAJQc5vA8PxwxVgKpUBT+s92gAguzO9ycvqcV+VsnTN+3pH5h3tCjAKZMBINYVCbVDQkkq6\nVdcxJ/HRIGUX0xUUtMYRjdnRJUxbozieTPgOWDfC2f+e+TpwYXDjZW8c56oHW/GcClozQuVQC/Ue\nRpVSOaUotI3m3wDnIz98W/00hADHjza/cb3RerLeK1fLtvMZNXGohfO6M4LKlja/69NUhOKNC1/r\nbLgMmPJ61Y5XVVx+KuP6vAA0WKv/W016tybt5dxIWa+ZRcKlbaNRrP1E9TtPCvD6PuPIXSvSNwJ3\nG6yha4J93YHvJzi7N64AHQ7Of9R8LpXbjkryTUO4l4jduBldTB3AWa+H11lSn6kil91RMynpAD4j\nLadR2d32t3Dns1CKGqmK7n3daURotJhrxYyV5s8YwN8/7zFeD9v5TlCh2XE814Rm/QTAdem+M4CF\nDzjvt6qK2wv86yp17banB+/D2hHW89G+zwH9RqvzPX8U1zpL72A/2O9ZekZr8UfR4NZ8tn6eV5wB\n/PJFDaR/EnyGWswMemfPDo+uCqWkPvVOBxTdU1cgr124ISuigm2hxUwaskKdqbQcbEGnuVXXha/r\nIyp4bm1/ZR+ujVoAat5Dv0oIGjSbzGfEXnWcBACmTr7wIaDBKqaOi89T+9EjIRDB/mbpHcD2c52t\nLWoUqdzW4XK5RDxKUIYQ78HBe4EHg5PS3zOB16vZkGodwdCV8x8J94Jdfzkw+1lnCsGoMqaiOPPN\nqk3GK+PpSV0Va64rwJB7rRJTuGiN29NLLbkdeD4XBHcBKE1nvq/q5k2MK6D1q8f71jmrDjYHPl1a\nix5ZgoriU7/g4tnIY8eTTGXv5kH0dDANK9LUmbmCC/FO41UvRiMCbiq4y+vQMh2q1M0aQKX2znOM\n/z+mkINH03nBMj98cnGoGSdEf/9bnUTWX8NNNRusUeVK69Fb5nBIe9Ci3Pf5cGUkwBzzf9/E9D5H\nsvjMWMlNUkIXzQAnQ9q2WZ7G1CI7TLashuBkQfFAAuiF9Oc7wCW36/Ot7z2DxpyaDoFN3UGP01bT\n9N8vvR2Y/go9d1vMZJi4ovgE+NwtvYMGI08qFZDdP2af4tSTzCkHutIy23y2GtqusLsXvYv29qi5\n4yXu56ZD3cbo23h5KvDXqGB4+hHmTRzwFDd0CY12KGgJLHiEhqp664HbuvP7QATw9jbn+ey6fwwM\nDk6mCpvwf60Mbp2/4TMGcBH31g77XOBOiD3EfNSKkXF7f+Crac6Vja3/BIYPU9uUN5bK1dhDwCV3\ncHM7hXmPsj1ahaa6/PSIPPsVoPVUfhdwM+9hTeRgrb+WCu4oDz8vvQOY9IFeJj6XfXj3j8MXQYvv\nBma8VHOLwVBiD9JbXVmEFLTkmGWV7kzrrQuEp7pxBag0qLNVLU3nmxvPDzXlIiEhB+gekqomrw2j\nno5UCesKcJHZ5wWOG/ltgmGm59KAUJWoGifE59Go1+k7NSVQKKGKI28s8PvH9KR1QkxhMGruG3pn\nedI4xoS+ViYy3YySUqYynoZW03GoilzwcHCjR/B5/nKmuWz6Bm72A3DMeG+j8yiL5rOBG4Pn7NTj\nOz6X4/PpH+kXqb4Y5h+d90TV+rWIChpq+z5nrRTUUplAA96mIVwIOzKsCyo0z33K2pvWCH8UvaFK\nG4S/ltanor/rV+Zz111n0TCY3YUpEIszUWMGY5efC/6e76jzsry29H5fc3X1FOva/qigFduU03QE\nEZXMvZy+yVxGuDhmljbgQj40QtWMgJvrprlPOh/fXH6G93ceR4WdkbIgEMH+Was8rIznOLHkLotj\nCUYhnPUaj2GEP5IOQlsGUfFxsAXXTn2f18/jfDE0PM57wnxsiilkSrM6W/m7cjsEFUoWxkq3l/1Z\nnxf180uA/eWs/3I9UJP0egu46AH1GO9tUOdV0cXsO3u9qb/vlQnAT2OZAqM63NLTcLOzI2LpHYz6\nq+p6JvYQU4e0mwg0WWDsnAPQoam0Hu+fP5ptQPs+EMm1QahzW1FDRlauuLXqhoCq4PbSa7f9z0Db\nX8IVijVFbjv+njXXhM/7668NRmaNt9Z/HGpGJ4JtF9CoWtPzDyNcAToLlaVTueuU1n9yLaA1HBS0\nUtMVKvijGMU+7wmuS6NKOTdQ0okpLLkTmPZ/VYi0FTxWi5k8/+azgSSbDWCLMzkX2dGfrwdbolpj\nWHQJdRMtpzMtcKhhW2FPT47tlYkc31pNNVf0FrSi13ZtzemPJREVwWiy/KDCO59zsIQczj8TD7Ak\nZNPRQjv/Lc4Elt/KfqKmoxeqhFRu6zBVbg/cDzwSnDj9mQH8X7vwfz6eiC7hxKLLWFo2/3qmenl+\nXQGg1RR2DMEk/paK0tnPALNHV/04jRZTwW3mAQww9MLtVx+kP94BltxT9WOF4vJzAdLjvfCNNisT\ngM8XOEuxUhNEl3Ay3G2MPie3FuFiJ7z5EpbsLjjc4ddfy4V4x/H2XnV2bL2AXp67elft/5L2ciLd\n/ePwgaE0HVh8HyduA57STzQ3D6Riu7SBcb1N5nOCqg2xUihPY87yVTdQYexkAIwppOWzx7tAO4Pc\nn5sG87nZdzoVKpddr7Y9fyQ9zlbcov8fVwC46H51QySAm8F88zsX39ElzBHZabz+mvz47ZErfpTj\nn/E+cwlqFzEFrZgbOdSDNKKCIYd9n9crFEsa0DsiVOkMcOGwdgRzDwsXDQNKabBGX4+WwsZA1vmq\nh51yn11+hnmd+6SquFFYdR29cKob+ghQCdBtDBfi2msSiKBCe/aomg09TtlJZVX3T8O9pZR8ZYr3\n+5qrqNR1SmQ5c5IqC2KrcFqXn3m9623g5+r2zWY0nQfceI7aZ6+5Elh6F8P8rKz03b4AhtyqGgDK\n6gDf/qbmrctcAQy/Qj8B3d6fHnS6vkHQ+NL5Gz5PoWGzNTU+KGgNC0AwHcvl9Hg5/SNOmq0UJgdb\nsH9z6sWdsZIKjFP+oDEqry2VCvnB17y2DM91BYCrB6v9YmUC82jbKt8Ec/D30KTaWnUtvXjrbOVC\nxOxZVvAkA+uH8Tnd1Ue9740XMhWSEs2lsHYEvQWr+jy7Ary+/Ubr98zQEnAD+7sHNxE6l/1udTdp\nTtnFZ/i0T43DW30xNKyuvZJjcIuZDP/VRqcsvptKCjPjU3QJx56zX7X3VAulMgEY9wcVWDVFyi7g\nvpbqc/nRCnXO6PIzBDumkFFAvV9SIzc2Xsq0KI4RNO4pyt45T9L7zOWnct/t4zko71tO41w2NPJn\n9TVMp3UkG13FFNHr96zXq268PdCV7X7N1dzoLZQmCxiV1jwkIWR5KttMWTqVm4dLXf1nTwrs5zGC\nxqYuX7MP1EZohVKeRiV3aKnKfj9xBYxI6fG+atg0YncvGrrWDXeWDiqikm1PSbVTHY+4pnMZTeAk\nQiyUygT2G1svpjG0/lqmydN6EmYNoCLU6F4rJB4ATv0f07aZXZ89Pdl21w1nOor+I6mY1a59dp/J\n+Zo24iiiknP7s17TO4QolDSgInvzIM6zjFKPJe1jmwxNh1jUkAp8X5xqyEwLKrSNjD+BCIaaH25H\nnfla2IRpqnq/HG50qUjk/GDhgw6cc6qB2wf8p7ua9mTjpYyi6foVf3OoIm3lDTTmHknqx5bTOPZG\nVmiMUfXDDVLldRmlGnuIJaZQfR97iM9tRRL3uKoJr9+EHHrzKwq60HloVTjUlF7wK286Brn9Bcel\nRkvU0nCZdcoJgG1zbw8+a3t7UgHZ822uA0LnaEWN2CazzuX10qYaC6UikYrWbRew5J+CEyoNVOp2\npq0zMrT6o+h8NvcJ4zH1lEnApTfr12h5bbiZouKEFFGppuNRlJ+JB2h0bD7L3lBRWo/zY0WhndcW\nNX59XQHqErp+TWdCqzEzlPJUGgw3DWE/e1RSaB7vCM5lE7PpFJvbvoYzHFQXqdzW4XK5RAKKUYoQ\na8wFB4AnNvL91AbAS7UUjnQiEHuQk6sGq9VSdzMHku9/qP4D33w2O5tGi6n8scr1nN+aeSFraodq\nhTpbGI516hcABCezmwfX7DGckrqDE7OuX4VbV7UUNqaysOFy84V/eZqa/1pnjcsL79w3D6RSW5eT\nqxrEFfBa9nzbJmQyFpj6Oj0WnAxkmcup5DbysgaMc5bHFVBplrmC/5+5wlz5v+Ey4K+R4caghssY\nUq1Voi14kNZrEUGPg0tv5qCpsOUi5g7WeToIKloveFj1Qgu4gVnPcRJZ1fCd2EO0SLf+k0YabcRG\nwM0J86znrL0t0rYBA+8ON+5oyenIVCqrrzP3knP7mJ5DUXYnZtPYkHW+/UQlpohKjDDvmnhugpR1\nHhVWTiI14nM5Se0y1tizZtMlwLRXqSSsLeLzqLzq+a658urj5VVPYdD3OWCAZuO2ooYhSs+g4rPJ\nQoYwAlw4vbmz5j1L+j8DnPOc/ruDzanwWX1tSOorQSOK9twPNePu3qH3Ia4AuOw6fS64ooZqzvvO\n3zC9gNkzvPABKlFrdGIs6G2uhIR6Uti/KMYDLeWpNLSlZYVv4rz4bubBNhwnBaMtznoVaDXd/pQq\nEqls0XoXff89Fc5OcPuAK4eq6SWc4I+kAmj1dTQAmimQ3T7g9A+ZAkKrcKpI5IaT2wew7Vt5vrgC\nzMnZb7SxQscKXwwNs1sGsjhZKNVbD5z9f/S4DA1j9UeyH1t7JRUnof1Q3c3AiMv0C+OdvYEJE2jQ\nUoj08Lr0eck8jNyKikQ+M7vPrvr/2nHF1eqmVaXpnF/FFFmHqf5vbtUN4NpUdFVlez/23Ubps6pL\nQjYVG0n7gx5KucE5kua9Wah7wM2xafV1nDvU2UbDfdvf9XLeOHqZz3+0djz8Ij1UaHX9inMBszDs\nUIozwxXeuR30z2X9tRzLun4drigLRFDpZWQcCETQs3HNNXTCOGy88OmNGc3m8hkHqAx8a0f1FGmR\nnnDlrFJSdukVyNmdVGX2rt7ha4nkPcDl1+iNE6X1gJ+/CokGEnREOf0j9lVG1z2vDa/Bmqv1OXEV\nGi3m3FE7f/dFcx6+7HYahHu9rSr/FZQUfYvvpRLP6dyx8SLgovtq3uvYCE8K2/3ie2vWicCIJguA\nmzX9Yl7bcG/+mo4KjPTQ6cCTiuNS0RlVxvWBklohMcfZ/xW0pPfuqutrfp19JLh9jBxSlN0Zq2ig\nUxTZe3uYG+wS9/M5Ov1D50awikSmMFk7ggrt43Hz4aoQWc613mn/42d/ZFCp/aS9oTg+Fxh8m37t\nHYhg1FZCtnkqWzPK05hCakdwb4zcDkc3fYUyZnb5mk4kRn13QSsqszcN4frzuFDcSuyRym0dpsrt\nAdnAyOAidmY94LkqJm6XVI2oMiohGy9iabJQVSoKF3enrk2lsytAZeVxMZAJKvhtqO8AACAASURB\nVFDa/M7SZL61h7tCRRLD7dZeGdwd2GSC4vYFNzXKo7dHTYeSRJXRm+Ws18K9WfadRgOCUf53O9Ky\ngsr/L43DjAJuYM+Z9CQ3C0PSsu5fzA9rFeqVvAe4arDeI3HTJfTIHHKL3gN87XBatc2ue9O53M1a\n61GSdS4H+8KmLEVNeD90Cz1BxfEpf3IR22ShsSEopwPPa29P+9+u1NvhRy56FAW5N5aLp+X/OXqb\nVaVtY+ohI+NFwM1c3jvPodfizj7qgimqjBP4LmOp5De6Jtmd6U1ZE17yTokuYbjoma/rPReyBnAj\nv6oSl8/cpHbeK1rmPAXMNNgV+0hxe5m7OVSRo7D/1KAH3zAqwZWNGQGm6hn3h7knnCtAZXi/Z9UQ\nW+EyD7ctqU+vp9XXVKHNV5HYg8Dtp5p75e3uRWXEuuFBpa9ge7z4Xv3mOAUt+WwqqZ7cXnpenvWa\nfU57K+Y+zrD2qhBdAtzUJ9zLGqCCs+AUNXdt/in0iq6K513iAaZJ6zo2/G8BNzfB2avZMCqnE42F\noRuQKVQkUlGy7I5gzsYZDM9vuMy8bQC85lsG0vNmRz99n9p4Eb0NjTbd3d+Nx1p/hb1yJroYGHqT\nfj+DooY0cO4/jeNg3+fDlVQFreg5uacnjZVxB9nWQl/dfmDxPdWLwnNCw2XAf85wLr+nJ6MEqjou\nuL3A/S3Cr4MVue1oSN58SdWPd8QIKvkVL+l2E9UURVoq46n81bZDfyRDdP8aae31W5PE5zICMHMF\nFdP111bdo/lgCz6L0cXGKeHK6nJesPQOPhdtf6XRovVk54p1I8zyJR8pEZV0GEnew36sqIn9/7h9\nNCaf85z+ns57lPtddB5HL+3Q1BsAI5JWXc/xb/9psG2zEZWMiOj7gt6QYjTmVSYwdcCiB6q/Ya8r\nwLZ83uPW6QF8MUwHUNCa51J/HefeVn0twPax8EGmWKlu6sjqMOQW4DSDPZ+KGrL/WHvVMcj/epzg\n8nPuElnO9hZZwdeISkZxKt+V16Gy0WlaoBONmEIao858MzxaFKAhctNgzie3XFz9CLDjmTa/cVxY\ne1UV+xDBdffAe4w31LSiIonrNsUz+0DXo7tvihXxeYx4b/8z+8atF1KhndcOx6XRSmKDVG7rMFVu\n980BRgc9cuakA6M6Hf2T+0cjOCnNXMGQ5pr02jnRiCvgAqLN71RuapUmyqC89kp6pRz1MDILtAqc\nOluooJg9+si9AlwB5mjr+iU3r3EaXuyPZDjlnp4Ml8xx+ExHl9CrVLspjTdWv9hdfitDa+0G7sT9\nwLAR4Zt5hFLSQM0XmbncejFSnkYFyNwnq2eciSli6LEvlpO7qoQt1yTNZzHFS6hyK5TsTvQcaDXN\nZPfyKEYtrL6Wxp5jNZmKqKTHcY93eZ7f/1D9dEdKiob0jdab5wBUBr61o/a8plwBekx1GcvIGycp\nFradR4WfUeh0KK0n04POyCukIolKxzVXc7J8NBZjjRcC/+6jGk8qE9i2lt1mrnRM2gdcclu4EWDx\n3dw0qtdb4em+Am56YC98kL8zfRPvd/pGRkekb9T3/ZsHcnPB6rTvuAKm5RJuKn4KWjNvbk3kZ1do\nPhsYeJd5yK+CN5bK89Rd+u8rEzhmLHzQOEdz7EEeo+UMeqpZ5d/1xql5utv+xnztoew4hyG62y5A\n1RY3gnn3BzylGqH9UfSQDf1Nh5oy9dWq648fj6B/XalPmwVQqVWRRCVVRTK9MQubMneuk82VjWg1\nBbj4PirzA5HBEqG+F8H3FUlcgK+45fhRtsQUsQ/u+pWx4hfgNVtzFa/R0drc0ZTg/FlRdCul3npj\nJb0VB7rSE3ftlcZKn/g8bp7ZZSzQdEHV6vYkA2/uOrrKUCc0n8V8taGpr4zY2ZsGzg1XVG/+XX8t\nMORmY6/qkgZBw97tNTcviy5maqRmf7HfPbwRZ/C1uFG4MjiqlG0ntD0l7+OGcwsf4jkei3y08XnA\n3W3V+YIvBpj/CDe2Pxnz40qqT6SHfXjPtzlH29FPTTUmU05Yk7oDGHqDPrIl4GYfUpIRTMmTwT6r\nuCEdzfafdvyM4ZKTHKnc1uFyuUQiilCCJP0fzs4Dng/u9jy/LvC0k81kJJJaxu2jYqnRYi42tww6\nMSZwrkDteE+Y5Sz3RVORuP805mXdfxq9eKvrme8KMEd071fC/zbvUaYdcKoQcXuB8x+jF0F12Xs6\njRlbLqb34/FiDT9SXH56qLeYycVXxkpnUQsAQ8hWXUcF4bFS0Nc2bh/z6BkpPpVUQH+8Cyy5++ic\nT0QF71fncVQaGuVsXnUt8OvnVTNqpe4Ahg0DGi3jYnXzJVQcbRl4bLxqWv/J9A27z2S4uRMlPQTD\n+i+6T6+UDqUynvnsFz5AxbdVfQm5vNdRpUyZcbwvHNxeXrdmf3G/g3rr7J/nygS23wUPV21T5+Q9\nNIycMomGL6dGz42XAvMe42LsSGg1FbjiKmOjTHEGo4RW3HKcRIdpcAXYpiDYritSOKf4p3o72pGy\ni/1d16/VFEWbLuEmuEdrr5bq4vLTCzdUSZm+SR/5FHBzf4HF9wZT0Dic26Rl0aDb5jc+f2YGjEAk\n29jie2pug9SaJiGHyhyj1G2eZBqolt/m3EnCCpefqevOfYqRALntqDBefe3x5bASSlRpMP3dMfZ0\nbDYHOP9Rel3Ofrb63u0SicQCERwrvFRml9U9edaekhMcqdzWYarc7pkPvBzM+bi4DvD4cT5plUj+\n6aTs5MYZh5ozl1dt5I3rNgYY/B/Ve/ZIQmozV9ArNGVXsOzma9LecAVQWR1g24VUZm+7sHY26Dke\niSlkaHizv+ht2XCZfhGe14a5T9dcHdxZ+x9MXD4VVU429KoNYg/R0NRlLD1qXSKYNuNFVGvx6/Iz\nZUdBq+PPs68qJO1jnxGa57qkPjfAVML8/wlEl7Df024YpaR8qYzXKLWPsA1HVDA65pQ/qOwOTSPg\nj2SfMf8x/SZuR0rqduDKy9RUM6XpHB+W3WG9B4LkBEQwZYNwMdXOiUxEBXPI11/LKKOtF9F54p+O\nK8CNZs99gqlX9p5BD+W1I2rH0zM+l3PAA6dK45JEIpFIJI6Qym0dLpdLJKEQxQjxxOpeALwWDI9f\nngo83O3on5xEIjn+aLSESu4tF9dOHni3jwqx5N18LWzKtDzSQk7lWOOF3DBqf3cuNo+115AknIRs\neqBJD6ogQS/u/iOpqF9yN6MMTsbcjlUlIZsen3ntamfjPYD9xSl/MNd2YRNg6Z32mylVl6gyoPvH\nNK6uuh6oTLL/H4lEcvySuJ+e6EabQ0okEolEIjmGSOW2DlPldteDwFtB75tVKcD9tbShj0QikUgk\nEolEIpFIJBKJRCKRSBxQe8rtkyuGyq/5OREnntJeIpFIJBKJRCKRSCQSiUQikUgkzjhhldsuGCiv\n/RoDQKRUbkskEolEIpFIJBKJRCKRSCQSycnKCavcNsSnVW4HzOUkEolEIpFIJBKJRCKRSCQSiURy\nQnPCKrcNPbd90nNbIpFIJBKJRCKRSCQSiUQikUj+CZywym1DtMptmXNbIpFIJBKJRCKRSCQSiUQi\nkUhOWk4u5bZ2Q0npuS2RSCQSiUQikUgkEolEIpFIJCctJ6xy2z4ticy5LZFIJBKJRCKRSCQSiUQi\nkUgkJysnrHLbEJmWRCKRSCQSiUQikUgkEolEIpFI/hGcsMptW89tqdyWSCQSiUQikUgkEolEIpFI\nJJKTlhNWuW2IzLktkUgkEolEIpFIJBKJRCKRSCT/CE4u5bYu57ZUbkskEolEIpFIJBKJRCKRSCQS\nycnKCavclhtKSiQSiUQikUgkEolEIpFIJBLJP5cTVrltiF+bcxuAkQJcIpFIJBKJRCKRSCQSiUQi\nkUgkJzwnrHLb0HMbLpmaRCKRSCQSiUQikUgkEolEIpFI/gGcsMptU/xSuS2RSCQSiUQikUgkEolE\nIpFIJCc7J59yW+u5HSHzbkskEolEIpFIJBKJRCKRSCQSycnICavcNkpLsns3ZFoSiUQikUgkEolE\nIpFIJBKJRCL5B3DCKrcB4Lzz9J8bNYJpWpJ27dSv33wTeOcd83qfeqr65xQVVf3/lUgkEolEIpFI\nJBKJRCKRSCQSiTNOWOW2CwJ9+qife/QAXC4APs1P0ii3J00CzjiD7++/H6hb17jet94Cnn+e73v2\nBMaMsT6PK69U3w8fDrz8srHct99a16OleXNnch07Oq9TIpFIJBKJRCKRSCQSiUQikUhOJk5Y5fZ9\n9wLPPGPwB5Oc2y1bAvfco/5JOMhYcvHFwA03mP99+XL952++MZd1h1zpOXPMZbdvB/r1sz09LFoE\nVFbay3k8wOTJ9nISiUQikUgkEolEIpFIJBKJRHKicMIqtxXuugt45BHgww+DXxjk3D7lFOf1uVzh\n35WUGMuedpr+c0SEudK8VSvgs8/4/qOPoPM6t+Oii4CHH1Y/jxkDXHghEBdnnwbl9tuBmBjKSyQS\niUQikUgkEolEIpFIJBLJyULksT6B6tLjDGqR33sv5A+anNs//iKQUQacdRY/d+sGxMfzfagSuqgI\nSE4GGjRQv1MU3QkJ6nd16wLXXQd8+ik/16+vr0db76hRQGIi06BERgLduwO33GL8e669FujfH1iy\nJPxv336rpkoB6E1u5VGu8PbbwL332stpiYgA/P6q/Y9EIpFIJBKJRCKRSCQSiUQikRxtTljl9vnn\nm/xBk3O7eSuB05LUP3XuDJSW8r1WCe3x0Ls5N1fNxT1rFnD66eHVP/QQ8PjjwBtv8PMrrwC9ewP3\n3Rde77PPOvstV18NfP45EB0N/Pvf+r8p9Smv8+db1yUElfJjxwJXXeXs+Fry8oBVq6joD/VMl0gk\nEolEIpFIJBKJRCKRSCSS44UTNy2JWf4PTVoSr5PE2qBiGwDS01Vv7X796HWtZcgQ4IknKKPIxcYC\nw4YB+/bpZa0U240a6T+3bk3FthVJQSW94oVuxMSJfF24kIrt0Dzft92m//zII1TOKwgBpKYC55wD\nnHqq9fkAQIsW9jISiUQikUgkEolEIpFIJBKJRFIbnLjKbRO6dNQotwMBU7mhQ4FPPqHXthOefVaf\n99qMQYOAyy5jShIjSkqASy7h+4EDgXnzgKefDpfLyNB/fvxxYMMGZ+faq1e4Yhtgru8OHfi+uBh4\n6SXg0UeB8nJn9YZyzTXV+z+JRCKRSCQSiUQikUgkEolEIjlSTlzltolXdlqSM8/tpCTg1ltVr207\nRo1ytglkhw7ATz+Z/12bv3vSJODss403hfz8c2D/fvVzbCzQrp1xnUuX2p9XKImJzK+t1G12qaZP\nB777Lvzvb70FPPMMkJOj/z7UOxwApk4N/87jAerUsT/PhASmh0lPD/+bNj+6wrx59nWa/a8ZqanA\ngAHO5Z1y663OZZOTjb9X7qEWI8OGRCKRSCQSiUQikUgkEolEcrJx0qnBojSaPadpSY5H4uPDvbfN\nOP10KkrPPrvmz6NvX2DECL5fs0Y93n33USlfrx6wbp0q/957wF136es4/3zgq6/oJb5qFb+LiQHy\n88OP9/bbQLNm6ufhw6m8D924EwAOHFDfK+lVjK7BmWeGf/faa8CLL4Z/H0r//sDBg8CECeF/e+IJ\n/We3G9i0ybieTz7Rf/7lF+CBB4xlQ40dQgCFhcay332nvp86ldf05ZfD5Vq21H9+803WO3JkuOza\ntfqNWoWgMcIucmH4cMqaPXbvv6++X7gQqKjgtbVj+XJg5kzg77/tZWfNokHGieFk5kxu1vrOO9Zy\niYl8vpKSrOUUmjcH/vtfZ7Lx8UyB5JRhw5zLSiQSiUQikUgkEolEIpGc9AghTrgCQIh9+4QRF69a\nJTBrlsCsWeL3vDxDmX8yW7YIsXChM9kmTYTw+/XfLV0qxM6d4bJffinEm2/y/fffKypOllBmzFDf\na+UuvpjfbdwoxA8/CFFWJoTXy+86dNDLJiWp/z99Os9TkdXKAULcdJMQzzzD91FR+nOpW1cve+iQ\nEIEA3z/0kBAlJZTLzw+v9/XX1fft2hn/JqVs26a+37hRlf37b73cnDn6Ol591bzeK64QYurU8Ov8\nf/8XLrtlixAzZ+qvsxDqb9UWr1eIr77ie+1j9vDD4bKvvsp7M2qUELt2WV+DkhIhJk4MbxOnnKKX\nc7mE2L5diOHDhdizRy9rVG9ZGe+PlmbNwuU6d2Y7mTMnvF0b1btqlRCbNtnLpaSwvS5dqsrt3m0s\ne8YZQnTtKsQjjwjh8VjXCwgRHy/EY4+pctOnm8sCQjz+uBC5uUKUlgrRqJG17Dnn8ByKioT45htr\n2Tfe4HOblWUtBwjx8suUmzfPXva114T47Tch1q2zl33qKSFefFGI/v3tZW+4QYjly4X497/tZV94\nwbhtm5XvvnMmN3iwEAcPOq/30kudyzot2j6qJsuZZzqXXbKkds7BaenTp3bqjY4+tr9LFllkkUUW\nWWSRRRZZZJHlxCoQQtSSnri2Kq7NAkCIvXuFEYNXrz6s3P45J8dQRlL7HDxIpZEd2oY+cKC53Nln\n62Wt+PNPvezdd/P7ykpVWa1Qp45xvaNG6RWgeXl6uTffpBJ4xgwhFi3Sy7ZrF/4QC0GZPn30xw9V\n6s2dy++ffZZGArNrpdQZCAixY4de7rffwmUVmZUrhcjOtq/X7xdi61a9XKgC0IrWrfWyw4eby153\nnV72vffMZUPPNS7OWG7PnnDZvn2d16vcBzs5QIgLLjCW/esv42vrtN6HHw6XM1NuO633u++M5cyU\n207P9bff2BZDeeONcNndu/ksOql3zpzwes87z1h2504hNm+msUPB5wuX++UXIQoKwo/fqVO4rMvF\ntnTgAOuyOtfx4/nMTJ1q/7vee4995M6dat9x6JCx7JQpfGa1xpMmTcLlWremseyyy3gdlOv24ovG\n9WZl8betXGl9rsOG0WByxRU8j9JS4+cLoLHiySdpkFHqNVPwv/kmDSuXXipETg7vyaOPGsv+/rsQ\nl18uxB130Oi0f78Qn31m/IwvXSpE06ZCnHaaavQyqlNpX717C/HAA/y/zZuN5V58UYgJE/j+5puF\n+OIL83rfeUd9f8st7Hs//NBYduFC9vUAjbA33GB9rtp7PXEi+55Qubg4IYYOVT8PGSLEpEnm9Y4c\nqf9sNH4pJSZG/9nq2tZGCTUo1Jbh5Pzzncs+/XTtnENEhDO5b789uvdAFllkkUUWWWSRRRZZql8g\nhJDKbfWkAVPl9uVr1hxWbn8fqsWTHHf4fCyff07FjBl5eXoFgR2K3P33C1FcbC5nptw2Or5WbsIE\nZ8cH9J63oaxdq5c1U6qG1ml3DRQvbYDHcHquVvVWRbmtrbdfP2u5UOW203O9/nprWa2CYvv2cO9u\ns3pjY53JAVS4WUVCOL22oXKPPab37FYwU+Y4rXf8eGM5I+X2++87qxNQIw5CMVJuOz1XwDhAx0yZ\n47ReM4yU2+npxrKTJzuvNz3dWbs1U247/V2PPmost3FjuOyhQ87qBHgPjZg9u/rneuedQhQWhst9\n8cWRXQOz33X99eGyoYZGhbZtjY8fCISfc2qqXq5TJ+fnqq1XO0bNnx8ud955/NvSpXojy+rVxnX6\n/TRCaJ8fo+Nfey3/tmiR3oj06qvG9U6fzqge7XUOlfvsM9a1eTPHbCWiatw482vw889CfP01jSj5\n+UJUVITLff89DSAADVRKYF58fLjsihWMHHn8cbap/HwakoyOP3UqDRCAELNm8bNRRBOgGo7dbv62\nXbuEWLw4XC4xkQYZxci7cKEQH31kfF0Btf/NzKQB57ff9FFZ2qKN8FAMoEZy2lKvnhB33cX+v0UL\nY5nTTuNrbCzlndTbuDHbpPK/diUtLTxSy6z89JMzuXPOcRZRpJSoKOeyzz/vTM6oj62J0q2bM7l6\n9YTo0qV2zsFpSUo6tseXRRZZZJGl9sq//nXsz0GW479cc41TWQghpHJbPWkgPF9BkBFr1x5Wbo87\ncMBQRnLiEggYe12GsnAh0xIYeYhqeeghLgpefpnpPKyO+8kn6kO5YIF1vYpcTIy1XKhy28RmI4Tg\nglqRq1PHul6tctsO7fGnTDGX+/VXvazTerVpUIy49lpV9uWXrWW1Cutx46xltR6+Ts/VTnbWrOpd\ng+XLjZXVRscfNIiKKTO03v7Tp9Mz1km9kyebPw+hyu3bbzevM9QbeurU8DQvClrldmIi0wM5OVfA\n+tkN9fD96CPn9ZoxbJherl49eglb1atVhJoRqty+6y5juYqKcMWLVbRBqPf2228byxkpXsz4/nt6\n+obeXyPMFKtGADRyKXLa1ExaAgEqDKtSrzaqx0y5HWqYMzNaKNx6q7Pjf/mlXsFtp9y++WZV1mQK\nI4QIN3Qoym0jzjyTfYbduT76qN6rPCWFxjIzLr/c2TWYNUs/jn3zjbGcx0OFZWgqMDNCPf5nzjSW\nq6igct5JnYcOhctaGX9DjSJCMGqgokIvt2uX6tlf1fFGGcuzs/WGCyHCFdxC0BCyerVezudjf6nI\n9emjKv+3bAmvN/QZV35X6LwmK0uInj1Vuauv5vGfeILGAsUgUlHBMStUwer1MoLj99/19YYacAYN\n4vHr1dMboefNE2LAAL3snXcy2uD66/V90+LFjKTTyv773zRcREXRcKLw+efhfUy/fkxddtlljMAp\nLuY5hcoBTC0GUL68nHV+/z2jUYxkMzL47O/bx2u1cqVxvUrkxtdfq8+mkZy2vPIKI2DmzBHi9NON\nZa66iq8zZrDvnTLFus5Bg/i85eXRQcTuHBTHFDu5p56iA8dbb9nL3ngjI4FC58hG5ZNP2J86UbDf\ncw/br50cwHnFH3/YyyUk8Br897/2ssOHGxv9jcpllxlHKBmVZcv0836z0qED59kpKc7qdRodc/vt\nXE85kR03jtfWiezUqUI0b+5Mdu5cZ3L/+Y9xZKVRueQSY2cGo7Jnj3Pj2e+/mxsaQ4uTZwtgW1Ei\nwezKmDGManZar3aOYVbcbo4DTup8+WUhxo51JtuwIftZJ7L5+c4MfR9+SCO4k/4NCHdwMyvffefs\nOUxMZJR5aKS5UbnoIqYYNBpftCUqipGTTtvLjBnstwYPtpabMIHX1Em9r7/OuZtdX9ipE2WnTTOP\nBFXKf//LCMtly+wN6Tt28DeVlFi3gyFDeD0feohzGSuj+0svsR968UUa883kUlM5D5k0iRHEL7xg\nLjtgAGU/+oiRvFZpPF98kfO77t15D6xSjq5bx7nAwoU8j0suMZabNInr9oUL6YRhdW9TUvj30aOp\nB3rwQXPZQIB9wMSJnNOapfyk4weEEFK5rZ40QLegrl3Z427adHiWfe369YeV22P27+cdeewxzige\neYR3xgxlZf3ss1y12jFlivpkGsXkK5SUsAX9/bdxwupQ9u7lbFVxeTLD79fH4EtqnUDAXmEuBDvh\nhQutvcaFUAdMbR5yM3w+TvY//dRaoSmEEOvXO68XoBLeiaySI1xZ0NnVO3BgeNqUUBTl9iWX2Nep\n1Gum6NBy5ZXOZQEqzp0Ee7z+OhWhTgDsPeeFUCfEVsYFhcpKDqJOj9+vn7nST0FZwHz4oaoUsWLk\nSCq17FDSITjhlVe4OCkttfayF4KpTT79lLJ2z2O9elTwCBGu5NFSUUHP3MJCIdas0StDjJgzh130\nunXWcj/8wMV3eTmHrdDUSEbs2SPExx9byxQUsCxcyHM1MzD4fBzOios5mf/5Z/vjC0Glsd01KCqi\nImjevPA0Rlr+/pvXdckSPpdOhq1336XBsajIWq6ykpOu8ePNh2GPh8Pq339zYvbpp/bHz8riYs3O\nkCmEEM89x77OTLGrnMOhQ0J88IG9cVII3rczz+R0xK4PVRYoisetFYqR0moqpKBES116qb2sori3\nij4Sgm1byYOuTNGsUCbDa9ZYy738siprRSCgLj4bNrQfS5U6zVJgKWza5Oz4QqjjMyBEmzbWstWJ\nWFM88s3IyXFmEAmt1yhVlpabblJl7frlDz5QZa0cC4Sgh7iTc/V4VLmOHfX7gISi7CvipF6tZ7jd\n+Bi6iDNj+3b9XjI5OezDN2ywrjcxkedjhrK/jFIWLGC9oVka/X4hGjTQy06dSmWL1njj9XKc1Rr8\nOnak4dfrDR93/P7wa/Diixx7tP1zXp4Q7duHyz70UHj/PHWqsZF25Eh9dMqhQ+aKkuXL9UZVv19v\nbLS6b6++aq643b2bacuE4Nj+wQfGkUJXXqkfHwoK6Phhdvxly1TZUIOctnz2md7YdfvtQowYYSzb\nrZvqZJGba62kuOEG/b01kwM4v1DmVh9/HB7RFHq9FF56ybre7dsp5/FYX6sfflD7ca2Rz6jcead6\nfLv9U1asoFwgYK3Uat9e37bs9oVR2LVLiF69zOWefFKdK9kZecaOVeudNs1aVmvUVtZJZkWZOxcX\nC3HvveZy48fbp+5TyrvvqnJ2abW0fYaVEnTwYPU5FIKGITPZ//1PlfN4wp0ZlHLGGfp5x/795ga0\n9u2Fjr17jeXS0zlH0vaxRtFfABWA2rSBfr+5MvyFF8Ln1UZp60LvvxDs3598Mlxu+HDjtb5RnW53\n+Lpt2TLOSc3alJbc3HC5tDT1GdRiZhgzQtnnS1vuvdd4rdC3ryrTsCFff/wxXM7vVw3G2vLii+Gy\nOTnOz1VrQFGO/9RTxmvGBQuc17t8ufr3pk35ajbv1dbVuzcdGoyOX1pqrLw3W4NpZR55hGO8VG6H\nnrTRHZ05U4hAQNy0YcNh5fane/fy6QqVNVutjh+vl6tb19xNePRovayZi10gwATOWlmrXODa2V63\nbvpEq1rKyvS90ZAh5nUKocq1aMHe0YyCArq19OtHk6adlumSSzjrmzfPWuO5aRNX0xMm2K9+haC7\nxNy55hobhT17uBqwcnnWsn+/vcZGCM6QKivtDQzHMVb2Fi3r1ztTuFWVfDap2AAAIABJREFUrCxn\nl2/nTucKt6pQXOzMliQEPWmsFJ/Vxco7U0turnGahiMlJ6d27q1EIjlxsRtWq4vT4VJR9DvFiUFZ\nCC4OnEwFAgEqN52wbRuNB1aKUoWKCnvFssKtt9KT3m5rmLIyKmGsvPe1pKVRwWaHEonWs6e9rDLd\ntYuW8npVJaATlGnp119by40ZY71407J9O+WcGJUjI53Vq42ksFNuK17ZLpd9W9QqLe1QDOAdOljL\nHTigV+zYocjZpY977jnndd5zjyp75ZXmchUV+mg8p+f6zjvW87UHHtAvuazmVvv2qXL/+59136it\n86+/zOsNTYH07bfm9YZ6hq9bZx65p1XUPPwwU4OZ1Ru67A3dw0cI9oNr1zICQZEbMYLKkND1g3Ic\nbZ2nnEJFqhHaNgCY77uTmxseSTByZPjz6/WyvZxxhl521CjjekOV3CkpVCKGUlDAY8XGqrL/+hfv\nbygeT3i9/fsbrzPy8sK9QY0cFgIBqiS0kXBXXsnPoW28qMh4H5l584zr/eUXvdzEicbtZfHi8Ii5\nlSvDlaU+H88hNL2W2TpnxQq9QrxjR2OVSn4+lci33abKzp5tXKeyVtIef9gw4/XugQNU5Go3FTdj\n+nT+XkXu5ZeN5zOFhfwNS5ZwrLWr948/+KrITZ5sLJeVxbmA1uBkRmUl1+1ab3atElzL8uW8DtqI\nAiN8Pt6DJUtUg4MVmzfr96jJyDCWKyig6ufXX9WIJzP27KGKRnkWHn/cXLaigv2G1uPZjG+/5e8a\nP57naZZWtqKC62XFU9tuzvXbb5xv/PIL+yWzSMhdu3gfevSgYt+oL9by/vvsV6dMsdbj7NnD+ZbH\nQ6OhmbpQCB7zjDMY4WvlXBQIcL576qnW56jIdu3KcdNuHrthgxoloSCV26EnHdqzK+XUU8V/xo07\nrNz+wCoR34ABnCGtWsXe2iz2rEULmqWXLOEsyONh6zSS7dePbnTr1qm9otluQ99+yxavHWnMzMib\nNoW7AF9xRbjcyJHGrVY7YijlueeMZUNnDoDxaOjzhcd83HSTsZudUTLZDz8MlzOTNesxtm7Vy1nl\nAAm9Z/Pnm/cac+aoq57GjdkrmaGdkXTuHB6vrOWll7hbV3y8dX4GIbiiPvdc3g87jcE777Cdffed\ntTa1spJ/nzyZvbEdGzeyLdu5WVZW8r5buY9q8futr5NEIpFIJJJaw++391qvDhUVeu9MK/76i4oN\nJ4b4gwfto2QU3nnHeYTdt98687cQgl6WTuq9/Xb7KZ4QnO4qHqlOSEqi4s0OJcLOCRdeKA4r9Kzw\netV0Gk4AeL3MPNEVXn9dnULboXirOTFM9ejhvF5FziwFl4J28107tErQ0DRCZse3q1e7jLEjP1+V\nTU62ltV6QtpRvz7lRoywfm4rKvQKbjsUufvvt5Z7911V1u651SqC7VIeavPbW+H3q0vf4cPtjV3a\nTZCt0Hpt2imKtF6bZnuiKGjTytihpDCwiyby+ZhKAXAWxaoc3yrNnhC8BsoeTE4A6IlqF12n9Vu0\no6SEck6iaD/6iKki7KIshVBTcZml5NPyySf0brUjEODzYGa00OL1cunvJGIvEAhPJWZGbi4V7U7G\nfEVx6gQnyQtC65Yc3wQC+vSsUrkdetJmCmtA3HXvvYeV22+HJo08nkvLls5lY2LsZRo0EOK++/SJ\nII1KdDRHnM8/54hmJdumDV0iFi403/kLYCzNNdcw7mDVKus6zz+fsRzTplFZPXSosdx11zHx0pdf\nckVy4IB5naNHc/Y7bx4NCOvWGSdCu+gijgrLlnHWmJtrnmRx+nTOUFetUuMgjRKxDR7M+vbvZ++s\nrIS0cSFKUZIwlpfrTfTvvx8uaxabH2qaz8w0z60Rmlxs0iRjOSH0CbPcbnP3DCHURJFKsaJ7d1XO\nKo9AWZmarCkjw3pE9PtpQne5mATZKOZJYds2trdzzqHJ04rKSl6zyy+3d3+eNYvxoe++a26aV/D5\nOHNwMsvYtIntzm626/Pxmdixw5m7+sGDdEGwW6kro5GTPDQSiUQikUhqhdLS2vEL2L3bPoWeEJwu\nWHmHadm40blyYtcuptxzwrXXOouI+/VXevU6SYG1fj19TuzSPwnBTUbtNjMXQlWsPvSQ/fRp1izm\noB040L5eZVNdJyiK6KFDreXy87kXiNN6ATW3rxXavVzsUPLl2ylrhWAKu6qcK2CfaTQnp+qKVScK\nvddf59KkZUt7WcVz2Em05dNP04PaDp+Py1wnKcuE4PLIbON5LR6PMzkhuHz97Td7Q5dS7/TpzurN\nyXGu1PR6rZeGWpz0hRKJ5MioTeW2i/WfWLhcLtOzfuDOO/HWsGEAgNc/+AAPTphw9E5MIjlaJCcD\nZ54JTJliLhMZCdStCzRsCPz9t7lchw6sLyMDOPVUoHFj4Oabw+Uuvhho2RJwu4H69YG2bYE33wQW\nLgyXffRRoGlTHj89neewdi0wYoRe7rzzgHvvBdLSWG9cHFCnDtC/P7B9u1520iSgSRPA5wNiY1l3\nfDwwejTw2muq3OmnA2+9BbRowWsQEQHExLDuyEh9ncOGAW+/zd/v9wNRUZTbvh3o2BEoL1dlp00D\nzj1X/exy8XXrVuCUU/T1btoEtGkTfl2mTwfOP1/9/OmnwI03hp8XAPz6K3Dppernu+8G3n03XA4A\nPvwQuPNO9bPHw99sxIUXAlOn8v0llwCff877GUpZGfDii8ALL/Dz0KHAzz8b1wmo1wMA3ngDeOAB\nY7m1a4H//heYMIH3PDubbcSu3ocfBu67j+3TiNde430vLGQ7njPHvM6VK9lOMjKAefOA5s3NZUeO\nBH75BejRg8/FmWcay5WVAU8+yXvctCnw/fdAYqKxbGEh8P77wJo1fK6uv978+Lm5vPeRkbzHZ59t\nLvvXX3zWy8uBPn2A3r2t6/3wQ57jgw+aywF8RuLigPbt2V/UrWssV1wMbNwIbN4MpKYCgwZZ1zt+\nPOV69uSrGdu3Axs2UCYzk8+2GdnZwKpVbFOdOgHR0eayu3cD+/YBKSlAu3bW5zpjBuUaNuQ5aNu7\nFr8fyMkBSkqApCS2MSuWLmX/17q1tVxWFtfp8fFAQgL7LCu2buX1snq2AGDnTt7b5GT2q2b4fLxW\nsbH8XXFx1vXm5bFtRUfzOTfD6wUKCniuZn2WQmEh64uNNb/+Cn4/UFHB62XHwYO8B05QZp92xwd4\nzYz6diM5l4tjlUQikRwD8vLshwuAw7zRdDoUIdgNFxaaTxm0FBZyaLEaLgAgEABKSynrhIICdu92\nXXYgAFRWWg+DEolEIjlyXC4XhBAOJtLVoLa05rVZYOEJ/Mhttx323H7ZbqcEWWSRRZbjqbjdQrRq\npU9EGVratWMsXuPGjPvVJpTTloYN6aF+7rmMKHjsMfOdU4YOFeLGGxmbfOWV3CnCaKeb9HTGmt53\nH12SXnvNfPeajh3pOvXWW5T78EPuepOQEC77/PP0fP/gA75OmUK3mFA5l4uRDZMm0RXkjz+YRujt\nt8NlW7bk8aZMoaf8jBmM4jCKJnnkEcY2LljAiI8lS5hQ7qKLwmU/+4wyCxYwLnb1aro+aZOSAoyi\n+P57/n3tWia0UyJEQuusV49/37CB57dyJd3TVq/WJ/YDGAO5di2TvW3bRre3ggJ9iiSlvPYaf8fu\n3Tz2tm10C/r443DZrVsZSbJ3L0t+Pl3Onn02/FxXrWKkQG4uZQsK6FYYGi111110h8vPp5tNQQFd\nDwOB8OOPHs04/eJivubnq8kFQ3fxWbCA0QclJYxA8HjU3X61cj16MGqmpIQypaVqQtPQ7dxbtOCx\nvV7KKi5/SlJMrez//R9lfT4WrYtikyZ62R9+UGPxFHkh+Pu06cViY3l/jAhN4up2myf0VeJflfLN\nN+YulOPG6WVHjjSW83rDd/+y2g1S2Q0HYOLCzZuN5fbvZ2y3Itutm3mdoTvVjRqlj3HU8vXXelkr\nVzDtc3Pqqfqd3EK59FJVNiPD3MWrslKIW25RZb/+2tyFNC9P3VmwTh32hWZoEwRHRVknAp8yRW2L\n119v7rYWCOj3pnn0UWuX20GDuLtm06aMTTdznSsqUvuZtm3tk2BHR9PV9O67re/BggXcY0aJELRi\n40b2w/37c8ywcon88EPKjhhhnxTz6ae52+t991lHShUX0xW1Tx/mRdDufhhKIMDrOmIEXSKtXI6X\nLWN/ed999inmCgs5Z3jiCf2Oa0Y8+CDbygcfWKfjUxKe3nAD+xc798mHH+Y4smSJeUJpIdjXDxtG\n91G7CLiVKzlPGTvW3pV7xQqmZ5wwwX6H4ttu45xl1izrvD2FhZR55x32uXaMGMF7deiQ9fWaN495\nCZy4uubm8rrOm2efl2D8eLbxdevM+00hOD79/DPl7XIoVFRwv6tp0+zvQVER++y5c+1d+pct47xu\n3Tr7zRmKi9m/2uV+EYK/f9Ys9rlWSduLihgtOnu2s80WfviBx7dz0d+wgbLr1tmHYGzezOMfOGB/\nb7Oy2C86OdcxY/hs24U0HDrEyNGNG+3vQSDAUAmrnW4Vli7l8ffssXeRLirixgRO9slasoTnavd8\nl5XxeVm/3n6jI5+P9yo72/5+FRez3Tpx51++nPPt4mLLexsIBISvuJjzQic5xBYt4hhj1x8XFVFu\n/37rvkDBKDWuEfv3sw062UCqoIBzWCd5vrKz2QbszjUQUO+XHR4P61Tm8Xb1btni7HcpazwnUdE+\nnxq9b8fmzTy+XV9QUsJny2nOuT17eB5O2syBA87CSrZutd+vTwhBFbT03D6Mlef2UzffjBevvRYA\n8Nznn+PpsWOP3olJJBKJRCKR/BOJj2cEQ01z0UXA5Mk1W2fTpoyi+fxz47+npNCr3O1mxNLq1eZ1\nud1UDaekMIqnsJDRCxqK4uMx64ILMLNNG1RERaH/rl24MCICqWPGGNeZlkaP7ogIoGtXRhZ8/LGx\nbNu2PIfISP6uQAD4889wubPPZkRBWRm92vv2pZviZ5+Fy3booLpxJiUBZ5zBSK3CwnDZXr14vkVF\n/Ny9O7BgAbBsmV7uootUt0ghGAXRpw9w9dXGx2/Xjt78Hg9Qrx6jC7RRWgrduwOdOzNiJTaWERut\nWwOXXRYue889fPX7eX3btQNmz2aklJaGDXm9mjTh9crMBE47DVi8GHj+eb1sx46MiIqNpetnSgqj\nXO64gxEyWkaN4m8vLeW97diRbWbQIEYQaLn7bra9QIBup02b8vrfcotezu3mOcXH83hxcbwm8fHA\nOefoZf/zH94vIfj7k5IYCfPaa8CPP+plb7iB0T+JicD+/UCDBjyfiROBV17Ry77+Ov/m8fAa1KnD\n6LW2bcPvwfff89w8Ht6zjAxei+HD6T6sMHAg20ZGBusEGGVWWQmcdZa+zqFDGQUoBA55PNji86Gy\nQQM0W7AAmQ8/jIhAQH/8pCTV9TgxkdFbzzzDaDot777LSDC3m+frctEN+ZZbgCVL9LLz5/Ne+f2U\nT09n+wl1Mb7uOkZhxcRQPhDg9di7l+1IyyOPAJdfzraluExnZvL5bdVKL/vll4yC8vlY3G4+l6NG\nAf/7n1523jz+bpeLbbFOHaBZM0aqaZ+xESPYDjMyUFhZiWWVlVgM4GBkJHredRf6r1yJuspzP3Qo\no7w8HvadPh/v18GDbEfa9j1xIp+R6Gg1wiUlhdFsoW3222/ZZl0uXoOICLbFn34Cbr1VL7t7N6+/\n8nzHxbHeO+8Exo1T5Xr2BD76iBFDfj/rTUnhvRo+HJg1S5V99FHgttt4/IoKnmtaGmVDo43eeAO4\n5hr+/spKyqenA3//jYpBg7C1USMklJejUV4eoqZP5zMD8Ph16lB29279vT3nHEZRtmzJc/V6eQ3q\n1mU0o7b/7tCB/ZnSBrxeyin9/N13q7KjRzNqMCGBsko7FILPnNeryr7+OtutEHxmIyN5rtnZPFef\nT5VdtEiNXPX7ea7x8cDcufrIVQBYvpzPSGSk6o6flMT+WNt/33EHn8+EBF5XIfib4uOBK65ge1K4\n/npGcUZEqP1Gair7znvuYXtSmDaNfWVUFO+VEhkWEaGPpIqJAb76itGuAM9VaQczZvB3aTVTeXms\ny++nXFwc61uyhG1P4fLLgVdfBRo0QL7fj6WHDmGJ14ulZWVYsncvcmJjEVtRgZTSUiRHRiI5MxPJ\nERFIdrmQEhWF5OhoNImJQc/PPsPpr72GBI+H9X7xBdtxZKR6XjExwIEDHNeystRzmD+ffVxUFO+5\nEpVXVsbrrTBkCJ/vZs1U1wG3m8/Fn3+yv1bo3JnPUHIyz0G5DkJwbqCNKh07Vu3jhGB9SpjFoEHA\nH3+osh98wLYQSlERx8BXX1W/y8rSRXkKIbCqpAST9uzBpBkzsK1hQ3TfvBmXZmVhyAMPILN9+/B6\n587lPElh2DDgu++Mw0tCr8Ftt7EvDY3g9fmAl15ie1aYM4dzISPOOkuNkM/MZD8f+hwBwK5dwL/+\nxWhQhZwczp1C8fl4vxUef5zR0UYhNl98gbz778f8zp0xt3NnLOnfH7Ft26JncjJ6JSejZ1IS0qOj\nee+WLtW3759+AgYPNgzzqU3P7ZNOuf3sDTdg9I03AgCe+fJLjDZbOAQJuFyoiIpiiY5W30dFwRcR\ngdjKSiR4PIivqEBCeTlivF6Y3QkBoDIqCqWxsSiNjUVJXBxKY2MR5fcjrbgYdYqKkODxmP6/RCI5\nMREADiYlYUdGBnbXrw8BIK6yEnEVFYj3eMLex3s8iPL7j/VpH/eUxcRgbYsWWNWqFVa1aoW1LVog\n0u9H123bcOqWLei2dSva7dqFSO3CVRKGNyICuampiPF6EVdRgdjKSrhPwLFfS0VUFKJ8vhP+dyh4\noqLgAhCjXVDWIgJAcXw8clJTkZOWhtzUVL5PTUVuMEVMWnEx0kpKkFZcjNTgq/I5rbgYccrCURKG\nz+3G0nbtMPX00zHt9NOxqEMH+EPSjkT4/Th77VoMWrQIgxYtQocdO+T80ICymBjsbNAA+9LTEeXz\nIT44J1deEzwexFVUnDR9wfHOoYQE7MjIwM6MDOxs0IDvGzTAzgYNUBYTg5b796P13r260iw7u0bn\nPCWxsdjaqBE2N2mCLY0aYUvjxtjcuDG2NG6MvJAUV5E+H5pmZ6N5djaaZWej+YEDaH7gwOH3jXJz\n5RwiBJ/bjTUtW2Jx+/aHy8amTSFClDquQACnbt2Kc1eswHnLl6P3mjWIr6iotfPyRkRgf9262Jue\njn3p6dibno4ig9RToVqC9KKiw22xSU5OrfcVBUlJWNWqFVa2bn24rG/WDL6gksft96Nhfj6aZWej\naU4OmgbbZtOcHDTJyUHdoiKklpQgvoo6AwHOjQoTE+ENGW+EQV6WKJ8PMV7v4RJxEj8HAkB5TAzy\nk5ORm5qKvJQU5KWk8H1y8uH3pbGxqFdYiIyCAjQoKEDGwYN8LShAg4MHUbeo6IjaT2VkJLLT0rAj\nIwNL27XD0rZtsaRdO2Q1anREvy/C70eXbdvQa8MG9NywAb3Wr0eb3btrbU4hAOSlpGBXgwbYXa8e\n9tarh0i///B1a3DwIBocPFir/YEdJbGxmNG9Oyb16oU/evbEXiNFb5Ce69fj0vnzMXTePLTbtatq\n1y0iggp8C8piYrAjIwNZmZnIatiQr5mZ2J6ZiUOJiWicm6sbl7TjVI3Otbt3p1HJBAFgR0YG5nXu\njLldumBep07YYJW6E0DrvXvRa/9+9Jo/H73Wr0eXbdvMx/tGjYC9e+ECpHJbi5Vy+4VrrsHTQa+G\n6MpKxHi9hzt04XJB+T/hcsEbGXl4oHGK2+8/PJlOKC9HRCBAZXZQkW1XX5TXS0V3sCiLxCi/Hy66\n0h8elF2A7rPf7YYvIoLnHRERVgIu1+H/cQcC9u81xzsW7/1ut+636N5HRCDgdh++zonl5UjweMJe\nI/1+VERFwRMdDU/QOKG8Vz4H3G7b66EcUzkHb2QkS/B8/BERiPD7EREIqEXz2R0IINrnO6w4iqus\nRGywKN/FVlaiIioKJXFxhw0foe/LY2LgFgKRfj8i/X5EKK+BgPqdzXvlf0z/Hjx3T3Q0ymJjURYT\ng7KgQUZ5XxYTA08wV632+dG+AkCk34+E8nLdM3H4vcfDiZkQhu3VqChtPLR4IyNRGRWFSqPX4Pto\nrxfJZWUspaVhr0nl5Yj3eBDj9SI22DfEVlYiJnhvYrzew+2pQtOWQl9zU1N1C7odGRnYkZGBEie5\nXTWkFRUhIzhhyjh4EJn5+ernggKkFxYixutFtNeLaJ/v8KvyXWRw4DBqr4c/B7+z+ux3uw3bS2hb\nMpUJfgZoLPQH+6KA23341a95r/su5Pvc1NTDiuxVrVphc+PGCNjkoY2tqEDnrCycunUrum3dilO3\nbEF6YaHumimvUT6fbsLid7uN21RUFPw2iR+FywVvRITa12j6HaW9VEZGIsbrPayIiauo0CllFCNH\nZWQk5TXGVe37iEAg7P+0r1E+H7LT0rA9OFFSJkzbMzKwPTMTu+vVC7uOsRUVh88nLliSysuRXliI\neocOod6hQ3xfWKj7zhsZiYLkZORrivZzYUICEjwepJSWIrWkRFeU7xLLy+EPtgF/8LkP/awoXrPT\n0pCTlqa+DypjS+LjEeH3o35wAp2hmUwr79MLCxFwucKeCW1/b/bcaGWifT4klZWxlJer74OfE8rL\n4Y2MRHlMDMpiYlCuKcrnkrg4HExMREFyMgqSknAwKQkFSUmHP5cHPVpTi4vVvkDTLyiv8RUVYf29\ntm8XLhfyk5MPK6nNlNc5qamotMpH7oCUkhI0CS7Gm+TmoklODhrn5h5+n1FQAJcQYc+5cLkOf/Zq\n+nDtc1AZGYmK6GhURkaiTOMsoLxq33uioxEf9G5KCbYzXSkpQXxFhe6+au+38t4lBKJ8PkT6/Yjy\n+RD1/+3de3Bc1Z0n8O+v1d1qPSxZkiVhydjYWAaMg19EdngUJAzEhEkISW0GajeZnQqb2uzskN2k\npjbMH4P+Sc3sbG0lqd1salJhM7PUzLDJzLCwNexAgBAIicFgGz+wDbZs69l6t7qtR7909o/fuVdX\nrXtl2Zajbuv7qerqe6+ubt/H75577jn3nOt8e6Y5aVfQtatvzRr8fPdu/GLnTowH9bkfYEM8jof2\n78dn3n4bN/b2Yspeh914smnLVHk50pEIjIjmqzCbV/TmHwHM294532VlSEejmLLLLIxd5/eqbbrQ\nkExijU0P1iSTaLDDdRcuIJzPIzQz4+bvnGEnjwXY64NN973fzmegvt4tJPVeYwsLK4PEbN5jtbdC\nxqdyRoxBoroa41VVvt/JykpE8nlU27yn97NqchLVU1OoyGTcfei9bnjjed7ffMYBIJLPI5rNzok3\nb/zlQyF3fr9ruRFx121eGmWH59wL2f1VeG/k5AknYjE3H+gMT8RiGK+qQldTE5KXGNeAFrxsiMex\nua8PLcPD7no5ebPC9HRs1SrE6+sxUFeHeH39vOHEYjtcXuS6+RUsbBgYwLqhIcyIzDsXpz3nTSYS\nWTCv4/32y/d4vwvvU90P9JyO2Ycj5ly7PdOiudxsGhsKwQDucp3pzvUoZe8/Cj9Dq1fj0ObN7jXp\nUkQzGdxx/DjuO3gQt3R1uXkYv7x0Ohp118lRGKOZSAR9nsLswdWr5xWwX6ryTAab+vq0sNt+b+rr\ncyuWvfvce47kQyHftNgZnyovR+fatTi8eTO6m5uvaB0d4VxuTh6qLpXC6okJRHI5JCsrkayqQrKy\nEuNVVe5w1vtE5iUqy+dRbu+HnE/Ymy7ZfH/hddF3uh2/6HRb/pGsrMSYzRslqqsxVl09ZzgTicym\nc5702JvO5crKMF5VNfux6brzuZJ9491Ha8bH3fIbb4W/M1w7MYELFRXor69Hf0ODfuzwYq9nS6Eu\nmUT7yZOoT6XctKAwD+bEeNRzzL33x874VHk5upqa0N3UhO7GRnQ3NWH6Yu9JAVA9OenmzZvGxtzr\nZ4XPPUhFJoNoNuvmp/IF+Svn24ho3sKWKTn5DCffMRWN4rVdu/D69u2Xlc9t6+7G5996C3cdPap5\neyePFIvNyy851/TC67IzbcpWzscX8+KBAE32nsa73yrSacTsA0vOJ5zPu/vBb98AcO9RC69lzrTT\nra0LVgIsRiydxu2nTuG7P/gBbi9oxehg4XaBhQq3/9sjj+CJJ574ra4PERGVhrDNVGfD4XlPUhIR\nXS07P/wQ97/3HirSaby4Zw8O+DWDJaIrUp7JYHNvLyrSaZxvbsbQYl8WSwsqy+ex7exZ7DlxAvXJ\nJF7fsQMHbr6Z+ahLsCEeRyYcRv9i3txJK0o0k8GOM2fQfuIE2k+exMdPnkRbby+mo9HAioxEdTWO\nbdyI/Vu34oMNG6644udatzqVwr533sFD+/dj+5kzeG3XLjx/xx14Y/t2pmMLCOdyuP3UKdx99Cju\nPHYM09Eo9t9yC/Zv3YqDbW2BFQgnvvIV3Nzd7fu3FVe4LSL7AHwPQAjA08aY/1zw98C1jtfV4ZPf\n/S5Obtiw6N8rL6id8tZWTkci7lMLE7HYRWuAwrmc+1Rxlf3OhsPuk1qXUxNORMWvamoKGwYGsCEe\nRySfx5R9CmrOE3Gep+Uv9kQyaZPXLT092H7mjPvJhMM41NaGwzfeiENtbehpalru1SwJjWNjyJWV\nYTIWQ/oKn9gtBmX5/DWVGQ3ncvp02G9xm6qmptCYSKApkUDT2Bia7NP5jYkEAOgTUzbvMlYwPFpT\nc8kt31aa6wcGcP977+H+d9/FfQcPorGgv+qBujr8v/Z2/NPevXj59tsv64nYlcDpVmLd8DAM4NvS\nbNLpJ5SuuorpafcJ58LvWCaDzpYWnG5pwenWVvez1NfpcC6HTX19aOvtxZaeHrTZz5aeHqwbGprT\nbcBELDavNYDT4u5Kn6i7ll0/MIA9touDPSdOYNdHH8326WuNV1Uns8sPAAAY4klEQVThl9u349Vd\nu/DK7t344CLN16+UzMygeWwMrcPDaB0eRsvwMOpTqTndkBR2vzEjgr41azQWW1owWF9/VdcR0HKF\nbWfPYodtUbjj9Gnc1tmJGvtOinQkgp7GRnQ1NeF8czO6bHx2NTWhp7ERiepqJKqrF/VkbKGIbcUa\n8+nKoHA/Oa13nCfsr/XC0Wgm47Y8WuNplei2SBofR2U6jeHaWt9WIwN1dVfcaiSUz6MpkUDLyAhu\nO3MGHz91Cu0nTuC2zk5EvX2XX6JkZSUO3HyzW+i4f+vWq/6UuNN6b71ttZcrK8NAXZ37idfXL8nT\n8ldiW2en2/XbJ44f9+2CanTVKvzT3r14/s478c/t7Zi4CvmJcC6HDQMD2Njfj039/djU14dN/f3Y\n2N+PugsX0NPYqNelgutUV3Pzb/1ep3pyEnccP467jxzBXUePov3kycDuZdKRCA5v3qwxZ2Pv3Nq1\nWJ1KYeThhwO78FlRhdsiEgLwIYD7APQBOADgUWPMSc88C661gV5wnaaa8PsG3GYyl7Jnc6HQnGZ7\n2bKy2cLs6emLJkzTkYg2R66pcW8QE9XVbjMLpxmUX7Mob1cATpMe70fsthtPk5NiHp6zLbZZtbep\nEjB7E1PYJNn5dvpF93YzMafLiWwWIdtU27sOheuyUJMqp19Xv6bz3o/T/MTb5GM6GsV0JOIOT5w4\ngfWtrW4TV6eLFWe4Ip2e1+x5UcMBXXssNOw0ZSzsu9IZdvZd4XnjHc+Gw27Fz2R5uds9j7c5qxgz\nrysLv8/F5vHrZsLb3UQ2HJ5Tuz1n2H77NY30dimRKytzK7oK48gZXn3hwvwbu3gcDcnkotOSfCiE\nkZoaN+PkfPo9w6M1NZrx9DTTd7qqyEQi7sUuVBCrhc3pF/xbLoeymZl5TcAKm9snTp9GbOvWBbuP\n8TaBChmDsnx+znjI6cKnYFrIxkfIdr2x7exZtyB729mz826mCg3X1ODw5s041NaGQ21tOHbDDZiI\nxXy7sinMZIntTsjt8iWTcccX0/eg002Mk+4Upj/RbBZpW0HqLYhxKjwmYzFkbLcXTtw5y3TWpTyb\nxYzIgl0IZSMRrE6l3IzSxv5+bIzHsamvDxttfMY8fTnPiGgT9ILuM5JVVW5fhENOX4R22OmPMJzP\noyGZdD/1nuGGZBK1Fy5gqrzcvTlzmvp7PxOxmG/3Tt7xqulptxljky2AbR4ddQtiaycmkI5EMOi5\n6fBmqAfq63FmYABrNm0KTNcX6n7C23Q2Ew4jVVnpfpLOcEUFUpWVmIjF3G6pCpuLV3jS1/pk0u2O\nrN6+h8MZr56aghHBcG3tbHrQ0IB4XZ1+22nT0eic65BzPnqbazYkk76F1s54YyKBxvHxi55bC3G6\nEeppbHSbqDrNVXvs+FBt7bzz3dt01EkTnK6Wyj3purdLJuedJ968lvf6GctkcKGiQp9oqqyc1xQ5\nWVWFqWjUPf6FeQ1nmsH8bp4Kuyor87lWeccr02l84vhx3P/uu7jpEvq7zJaV4Vcf+xhe3LMHr+ze\njZHOTtTfeONsV2c2npzh8mwWYsycvCIwN/8IYM52FuYdne4wvM2CC5sIxzIZpCoqMFxbixGbDgzX\n1MwZHlu1ak4XCPO6WrDr5aTzZQXpvvNdn0q511X3E49j7ejoRdNjp1uNlH2azamE8VbQOM3bjcic\nbpIKv2smJpArK/PtssH5TJaXz8uXRHK5wHG/vzn5XCe25jRt9sSfXzrlHQeg3UzYNMkd9qRRadun\n/0L3RE6XF1WefKGTJ3TOu3VDQ2hMJALj+nUA9/pMn4pGtdC7tRXDtbXuehWmp8mqKkzEYlh94YLb\nxdR1nv5uneEr7ffWazoSQVdBgbcz3F9fj0gu56bl3q4GnWnRXG5Ofscvn+PEeFCeyNtlo3NcvF0O\nAXoOOQ9ceR+a8F7HM+HwbFrrWaa3a8iKTMa3qx3v+K3nzmHt6Ogl78u+hga8tnMnXt+xAyM1Nb73\nZG6eOpNxz2vvvYV3H5TNzOC60VG0jIygdWgI142OXnH/6MnKyjkVL6dbW3G+uRkjnZ2osy9yLOzu\nCQDKZmbmHfuKgmt9UyKB7adP46bu7iXpY37a9p9dmIfKhMNul4u1ExNuF4y1ExOX/d4O5/pX+A6y\neV14+VwXL2W637R8KOSmv94upbzj5dmsm/560w9vOlc2M+Om5fO6KLtwYU4++EqOyUht7ZwKf+81\nxxmvmp7G2pER/diu5daOjKApkVjSvs1fh3+aawB0trTg0ObNyEQic9ODgnTHeWec9x100wVxEMnl\ncP3QENYPDLhdzzmVNUEM9CGJgbo6DNTXY6i2FhMVFW7XHt4ufbxpWNhzX1DYRWuZTduCyphmbAXN\nLefP46H9+7F+cPCS9ud0JIJXd+/GC3fcgfPNzfPyRYV5pHLn+h7QtVh5JoN1Q0NYd5nvdsiFQuht\nbMRYdfXcrpA8+86Z7nSLVdgtlXc/FV7DYgXXtoZkElvPnbuidDZeV4fOlhbccfx44DwrrXB7L4Cn\njDEP2vFvAzDep7cvVrhNVKw67IfoSuVDITdjcrV14NqIW2/hlVOAfS28wC0XCvGFWD46cG3ELa08\nHWDsUunpAOOWSlMHGLtUejrAuKXSczULt4uxTWkrAG8HLT0A2ufN9ad/Cpw8CXz4IbB3L9DZCbzy\nCuDc4G/cCDz0kH5PTACHDgHnzgF1dUBFBfDaa8DUlM4bDgPf/Cawaxfw5pvA0aPA9DRw1136Gy++\nqPNFo8C+fcDv/i5w9iyQyQAffQSk08BNNwHnzwOvvqrLbWzUN5Ledx9QU6NvJv3gA6C5GbjtNuDY\nMeCnP9X1raoCvvpV3Y79+wERIJEAKit1OS+9BHR16XasWwd89rNAbS1gjP7+uXPA3Xfr+IEDwK9/\nret7++36+7EYcOaMbnc4DIRCus4vvQQMDuq0LVuA++/XZQwO6u8fOaLbOjwMPP88MDCg837hC8CO\nHbo9AJBKAatXA/X1ul2//KXum5YW3QeNjUB1NdDdDfT36zJuvhl4913g8OHZY/rFLwIbNgC/+Y3u\n38lJ4JOf1HV64YXZ+bZuBTZv1v0xNgYcPKjTH3kEGB3V33ds3gysWaP7/fnnZ6c/8ogek64uHa+u\nBrZtA8rLgePHdZsd+/bpOjlNim+9Vff/iRP6+15f+ALwj/84O75xo8bNqVPzQtg93o5779V9dObM\n3Pm+/nXdr2++qeMNDboOb7wxf5m1tRrj587peEUF0Namx7LQ178O/PCHs+P19cCmTXpcCn360xov\njvJy/S2/GtHHHwd+/OP50wvddpt++61boVBIY/TkybnT/Ar09uwB3n774ssEgC9/GXjmmcXNu307\n8P77i5v3Im8jdhUeg0uwYM3/174G/OhHi1vQli2aji7Gtm0ai0uprU3TscUIOuZ+Hn0UePbZOZME\n+vKuK3qiprERGBq6+Hx33z17zi6lnTv1euYRWLB93XVAPL7067BYd94JvPXW0i/3asThpYhG9Rq3\nGFVVeq1aapEIsARPIV02nzhcccrLNS+1XMJh4AqaMV8TYjHNrxMRERHRilaMT25/EcCnjTFfs+P/\nCkC7MeYJzzym2NabaDE6OjrQ0dGx9AuemdFKEblIJZgxWihjjN4ULiSZ1OXW1GihYpB0Wit0cjkt\neF9oHZJJrSQoKwPWrg2eN5vVSpOJCZ23pWXhdT1zRm9wGxq0IiNouSMjutxsViuPWluDlzk6qhVW\ngBbkBMlktJJkakr3U11d8Prm87rMoSEtnNq1K3i5AwNaeZDLaaVW0NuL83ndr+fO6TpUVASvrzFa\nQZBKacHIxo26vn4mJ/X3BwfR8Zd/iY6nnw5e10RCKwUB3a9btgB+/dLNzOgx6O8HLlzQdWifX3fp\n6u7WiqFQSAuLw2H/Y5tKAX19WilljBYoBu2DfF4LRvv69Bh86lPB8T0woPMlk1r5c+ONun2FjNHt\nOnZMt6uyUivm/NZ1ZgY4fVrXNZPRGGxr8//9XA7o6dHKh0xGK26C4mBsTOebmtJz5oYbgOuv91/X\nyUk9Z+Jx3R8PPui/TEBj26l8aG3VWPTrGzid1njp7tblh0K6b4McOQL09uoxaG/XdMZPMqn7oL9f\n521q0spRP6mUVnYmk0BVFTpefRUd3/mO/7z9/brcREJ/e8+e4HXt69PKqnxet3/nTt3HhSYn9Twc\nHtY0pqFBK4L9GKMVpE7F0sMPB6dbg4NagZ9K6T5oadFzrNDMjKaZH3ygsRsKaQV1kI8+0m1zKuiD\n3lWSTmusfPihprOVlVpxHjTv++9rOi+iFbFr1/rPm0ppfPX16b594IHgc3FwUI+BMZq2bNvmn3bl\n8zpvV5euQzSqFcdBOjv1vC0r0zRm1Sr/45BM6jIHB3Udamv1AQI/MzNa8N/To2nWvn3+8QLocert\n1XVtaNAK+crK+XkFY/SadPSoxlksFpzGALNpzPR0cLwAur/6+vTYptN6HgT1QTw+rst1zu/16/3T\nGEDTobNn9Txzjm2Qvj6NRRFNY9au9U9nMxndB04aEw5rBVqQkydn0/kdO/yvSYCm2fG4Hlsnb7J+\nvf+8k5MaL6mUruOuXVrJ4WdoSI9vMqnzBqUFgB6rY8c0zd+wQePA79hmMhovo6O6X1evDj62xmjM\nOg873HNP8O+Pj8/u1/Jyvc745WOM0bTt9Gm95kWjc9LOeXHb26vzOWln0PUrl9O0uKtL43D1amCh\nF66eOjWb51q/Xuf3MzWlx2FkRNd9oTxXMqlpcjis53drq+an/CQSutzpaV2HW28NXm48rtsVjepD\nHUFp3NSUxoGTzldVLZz37ezU41ZRoel3UFowPq7xkk7rORCU73XyBmfP6nhd3cJ55Hhc5wd0fwWl\nGzMzug69vbrtW7cGLzOd1nQT0HOmttY/LQA0ZkZH9VhEIpqfDTIxMZvfuO463+Pa0dGBjqee0uMw\nMqLrUlsbHLOAHq+xMd2va9Zo7PjJZjWdSaV0H6xbF7xMQM/FdFp/u6Ym+NhOTupys1m9JizUf3w6\nrXEYiWgas9C92uio7odwWPN7VVXB846P67mwapXefwXJZmevn84yg/pjNmY2vurrg+9TgNn7ykhE\nj1fQOTszo+lnX58eg7Vrg9NuQK9dmYzu/+rq4HV1jq2T31jonM1m9fwqL9e8bHl5cHowOanbls3q\n7wfsg46ODnR861u6vk6aEXRs83k9XyYndV1raxcuC4jHddsaGhY+BlNTuq75vJ6vC/X1nc1qfAOa\nvix0DEZHNY2NRHQfBB1bY3Qd4nHdnoWOQT4/+9BPTY2ub1D+LJ/X45pM6jYttF1TU3q9d7Y/6N1G\nxui5mEhoGtbcHBxbgO4D5x60ujo4XnI5TecmJnR5C6VbxszmzdauDU63nO2amNB5KyqC79UA3a7e\n3osfAwAisuK6Jekwxuyz477dkizX+hERERERERERERHR4q2kwu0yAKegL5TsB/AOgMeMMSeWdcWI\niIiIiIiIiIiIqGgUXZ/bxpi8iPx7AC8DCAF4mgXbRERERERERERERORVdE9uExERERERERERERFd\nzAJviStOIrJPRE6KyIci8p+We31o5RGRp0VkQESOeKbVicjLInJKRF4SkVrP354UkY9E5ISIPOCZ\nvktEjthY/p5nelREnrX/8xsRCXizEdHiicg6EXlNRI6LyFERecJOZ+xS0RKRchF5W0QO2bh9yk5n\n3FLRE5GQiBwUkRfsOOOWip6InBOR9226+46dxtiloiYitSLyMxuHx0VkD+OWip2IbLFp7UH7PS4i\nTzB2qdiJyH8UkWM25v7Gxtmyxm1JFW6LSAjAfwfwaQC3AnhMRG5e3rWiFegn0Bj0+jaAV4wxNwF4\nDcCTACAiWwF8CcAtAB4E8D9E3NcY/xDAV40xWwBsERFnmV8FMGqMaQPwPQB/cTU3hlaMHIBvGmNu\nBfAJAH9o00/GLhUtY0wawCeNMTsB7ADwoIi0g3FLpeEbAD7wjDNuqRTMALjXGLPTGNNupzF2qdh9\nH8CLxphbAGwHcBKMWypyxpgPbVq7C8BuABMAngNjl4qYiLQA+CMAu4wxt0G7u34Myxy3JVW4DaAd\nwEfGmPPGmCyAZwE8vMzrRCuMMeZXAMYKJj8M4K/t8F8D+Lwd/hyAZ40xOWPMOQAfAWgXkesArDLG\nHLDz/S/P/3iX9ffQl6sSXRFjTNwYc9gOXwBwAsA6MHapyBljJu1gOTTzZMC4pSInIusAfAbAjz2T\nGbdUCgTz7xEZu1S0RKQGwN3GmJ8AgI3HcTBuqbT8DoAzxphuMHap+JUBqBKRMIAKAL1Y5rgttcLt\nVgDdnvEeO41ouTUZYwYALUQE0GSnF8Zsr53WCo1fhzeW3f8xxuQBJESk/uqtOq00InID9CnY/QCa\nGbtUzES7djgEIA7g5zYDxLilYvddAH8MrYxxMG6pFBgAPxeRAyLyuJ3G2KVithHAsIj8xHbv8CMR\nqQTjlkrL7wH4WzvM2KWiZYzpA/BfAXRBY3DcGPMKljluS61wm6hULOWbWuXisxAtjohUQ2s/v2Gf\n4C6MVcYuFRVjzIztlmQdtJb/VjBuqYiJyEMABmxrmYXiiXFLxehO20T+M9AuzO4G01wqbmEAuwD8\nwMbuBLR5POOWSoKIRKBPt/7MTmLsUtESkdXQJ6s3AGiBPsH9L7HMcVtqhdu9ALwdia+z04iW24CI\nNAOAbV4xaKf3ArjeM58Ts0HT5/yPiJQBqDHGjF69VaeVwjYb+nsAzxhjnreTGbtUEowxSQCvA9gH\nxi0VtzsBfE5EOgH8HYBPicgzAOKMWyp2xph++z0E4P9Au4VkmkvFrAdAtzHmXTv+D9DCbsYtlYoH\nAbxnjBm244xdKma/A6DTGDNqn6p+DsAdWOa4LbXC7QMANovIBhGJAngUwAvLvE60Mgnm1h69AOBf\n2+HfB/C8Z/qj9m2vGwFsBvCObaYxLiLttjP9rxT8z+/b4X8B7YyfaCn8TwAfGGO+75nG2KWiJSJr\nnDdti0gFgPuh/cUzbqloGWP+xBiz3hizCZpXfc0Y82UA/xeMWypiIlJpW3hBRKoAPADgKJjmUhGz\nzeC7RWSLnXQfgONg3FLpeAxaGe5g7FIx6wKwV0RiNt7ug75AfVnjVoxZyifFrz4R2Qd9G3IIwNPG\nmD9f5lWiFUZE/hbAvQAaAAwAeAr6ZMvPoLVL5wF8yRiTsPM/CX3baxbaFcTLdvpuAH8FIAZ9u/c3\n7PRyAM8A2AlgBMCjtuN9ossmIncCeAN6k2rs508AvAPgp2DsUhESkY9BXyYSsp//bYz5ju1zjXFL\nRU9E7gHwLWPM5xi3VOzsTedz0DxCGMDfGGP+nLFLxU5EtkNf4BsB0AngD6AvPGPcUlGz/cOfB7DJ\nGJOy05jmUlETkaegD3BkARwC8DiAVVjGuC25wm0iIiIiIiIiIiIiolLrloSIiIiIiIiIiIiIiIXb\nRERERERERERERFR6WLhNRERERERERERERCWHhdtEREREREREREREVHJYuE1EREREREREREREJYeF\n20RERERERERERERUcli4TURERER0mUTkV/Z7g4g8tsTLftLvt4iIiIiISIkxZrnXgYiIiIiopInI\nvQC+ZYz57CX8T5kxJr/A31PGmFVLsX5ERERERNciPrlNRERERHSZRCRlB/8MwF0iclBEviEiIRH5\nCxF5W0QOi8i/sfPfIyJviMjzAI7bac+JyAEROSoij9tpfwagwi7vmYLfgoj8Fzv/+yLyJc+yfyEi\nPxORE87/ERERERFdq8LLvQJERERERCXMaQb5beiT258DAFuYnTDG7BGRKIC3RORlO+9OALcaY7rs\n+B8YYxIiEgNwQET+wRjzpIj8oTFmV+FvicgXAdxmjPmYiDTZ//mlnWcHgK0A4vY37zDG/PoqbTsR\nERER0bLik9tEREREREvvAQBfEZFDAN4GUA+gzf7tHU/BNgD8BxE5DGA/gHWe+YLcCeDvAMAYMwjg\ndQAf9yy732jfg4cB3HDlm0JEREREVJz45DYRERER0dITAH9kjPn5nIki9wCYKBj/FIA9xpi0iPwC\nQMyzjMX+liPtGc6D+X0iIiIiuobxyW0iIiIiosvnFCynAHhf/vgSgH8nImEAEJE2Ean0+f9aAGO2\nYPtmAHs9f8s4/1/wW28C+D3br3cjgLsBvLME20JEREREVFL4JAcRERER0eVz+tw+AmDGdkPyV8aY\n74vIDQAOiogAGATweZ///2cA/1ZEjgM4BeA3nr/9CMAREXnPGPNl57eMMc+JyF4A7wOYAfDHxphB\nEbklYN2IiIiIiK5Jot3xERERERERERERERGVDnZLQkREREREREREREQlh4XbRERERERERERERFRy\nWLhNRERERERERERERCWHhdtEREREREREREREVHJYuE1EREREREREREREJYeF20RERERERERERERU\ncli4TUREREREREREREQlh4XbRERERERERERERFRy/j+kw1BRccQMxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c7c045090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "#niter = 30000\n",
    "test_interval = 500\n",
    "n_hype_configs = len(hype_configs.keys())\n",
    "pid = 1\n",
    "for hype in CV_perf_hype.keys(): \n",
    "    CV_perf = CV_perf_hype[hype]\n",
    "    n_CV_configs = len(CV_perf)\n",
    "    for fid in fid_list:        \n",
    "        train_loss_list = CV_perf[fid]['train_loss']\n",
    "        test_loss_list = CV_perf[fid]['test_loss']\n",
    "        \n",
    "        for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "            #plt.figure()\n",
    "            ax1 = plt.subplot(n_hype_configs,n_CV_configs,pid)            \n",
    "            ax1.plot(arange(niter), train_loss, label='train')\n",
    "            ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test', linewidth='3')                            \n",
    "            ax1.set_xlabel('iteration')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_title('fid: {}, hyp: {}, Test loss: {:.2f}'.format(fid, hype, test_loss[-1]))\n",
    "            ax1.legend(loc=1)\n",
    "            #ax1.set_ylim(0,10)\n",
    "        pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp11'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'R_HC'\n",
    "batch_size = 256\n",
    "encoding_layer = 'code'\n",
    "weight_layers = 'code'\n",
    "multi_task = False\n",
    "\n",
    "if modality in ['L_HC', 'R_HC']:\n",
    "    snap_iter = 10000\n",
    "    \n",
    "    if modality == 'R_HC':\n",
    "        output_node_size = 16471\n",
    "    else: \n",
    "        output_node_size = 16086\n",
    "        \n",
    "    hype_configs = {\n",
    "                   'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':8000,'out':output_node_size},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "\n",
    "elif modality in ['CT']:  \n",
    "    snap_iter = 100000\n",
    "    \n",
    "    hype_configs = {\n",
    "                    'hyp1':{'node_sizes':{'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-4, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "elif modality in ['HC_CT']:  \n",
    "    snap_iter = 20000\n",
    "    hype_configs = {\n",
    "                 'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':500,'out_HC':16086,'out_CT':686},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "hype = 'hyp1'\n",
    "pretrain_preproc = 'ae_preproc_sparse_HC_10k_CT_100k_{}'.format(hype)\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "\n",
    "# Save either L or R HC encodings. Turn on CT + CS save only for one of these. \n",
    "# (Also decide if you want to copy CT raw scores or encodings)\n",
    "save_encodings = True\n",
    "save_scores = False\n",
    "CT_encodings = True\n",
    "\n",
    "for cohort in ['inner_train', 'inner_test','outer_test']:\n",
    "    data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "    test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)       \n",
    "    input_nodes = ['X_{}'.format(modality)]\n",
    "    #input_nodes = ['X_L_HC','X_CT']\n",
    "    net_file = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "    with open(net_file, 'w') as f:\n",
    "        f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "\n",
    "    test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "    with open(test_filename_txt, 'w') as f:\n",
    "        f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "    sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "    if modality == 'CT':\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "    else:\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "        \n",
    "    print 'Check Sparsity for model: {}'.format(model_file)\n",
    "    \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    \n",
    "    encodings_hdf_file = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,pretrain_preproc)\n",
    "    \n",
    "    if save_encodings:    \n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        input_data.create_dataset(input_nodes[0],data=encodings)           \n",
    "        input_data.close()\n",
    "        \n",
    "    if save_scores:\n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        if not CT_encodings: #copy raw scores for CT instead of encodings\n",
    "            CT_data = load_data(data_path, 'X_CT','no_preproc')                    \n",
    "            input_data.create_dataset('X_CT', data=CT_data)    \n",
    "            \n",
    "        adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "        mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "        input_data.create_dataset('y', data=adas_scores)           \n",
    "        input_data.create_dataset('y3', data=mmse_scores)    \n",
    "        input_data.create_dataset('dx_cat3', data=dx_labels)\n",
    "        input_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting TSNE embeddings \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "exp_name = 'Exp11'\n",
    "#preproc = 'no_preproc'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "Clinical_Scale = 'ADAS13_DX' \n",
    "batch_size = 256\n",
    "snap_interval = 2000\n",
    "snap_start = 2000\n",
    "encoding_layer = 'ff3'\n",
    "weight_layers = 'ff3'\n",
    "cohort = 'outer_test'\n",
    "multi_task = False\n",
    "\n",
    "modality = 'HC_CT'\n",
    "snap_end = 21000\n",
    "\n",
    "\n",
    "hype_configs = {\n",
    "            'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-2}},\n",
    "}\n",
    "\n",
    "hype = 'hyp1'\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "tr = hype_configs[hype]['tr']\n",
    "\n",
    "data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "test_net_path = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)       \n",
    "#input_node = 'X_{}'.format(modality)\n",
    "\n",
    "net_file = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)\n",
    "with open(net_file, 'w') as f:\n",
    "    #f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "\n",
    "test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "with open(test_filename_txt, 'w') as f:\n",
    "    f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "n_snaps = len(np.arange(snap_start,snap_end,snap_interval))\n",
    "tsne_encodings = {}\n",
    "net_weights = {}\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    print sn, snap_iter\n",
    "    model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    net_weights[snap_iter] = results['wt_dict']\n",
    "    print encodings.shape\n",
    "\n",
    "    adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "    mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "    dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "    tsne_model = TSNE(n_components=2, random_state=0, init='pca')\n",
    "    tsne_encodings[snap_iter] = tsne_model.fit_transform(encodings) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "plot_encodings = True\n",
    "plot_wts = False\n",
    "\n",
    "color_scores = dx_labels\n",
    "cm = plt.get_cmap('RdYlBu') \n",
    "marker_size = 100\n",
    "marker_size = (np.mod(color_scores+1,2)+1)*50\n",
    "\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    plt.subplot(np.ceil(n_snaps/2.0),2,sn+1)\n",
    "    if plot_encodings:\n",
    "        plt.scatter(tsne_encodings[snap_iter][:,0],tsne_encodings[snap_iter][:,1],c=color_scores,s=marker_size,cmap=cm)\n",
    "    if plot_wts:\n",
    "        plt.imshow(net_weights[snap_iter][weight_layers])\n",
    "        \n",
    "    plt.title(snap_iter)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp6_MC 1 CT\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.50590760870891749, 0.28744168272913057, 0.5115302569987521, 0.42628113845552956, 0.50056826161794232]\n",
      "ADAS mse: [56.210850724275389, 75.301038966454016, 57.524482073133989, 79.095532289495296, 50.715837720160238]\n",
      "ADAS means: 0.446345789702, 63.7695483547\n",
      "\n",
      "MMSE corr: [0.47374635791011838, 0.35826143989007675, 0.3162182009918163, 0.40438301158121165, 0.53835441612432067]\n",
      "MMSE mse: [5.5793522275995091, 6.4613264992226238, 6.2308157022118715, 6.5502360433720241, 6.0275701850635892]\n",
      "MMSE means: 0.4181926853, 6.16986013149\n",
      "\n",
      "opt_snap: {1: 7, 2: 7, 3: 5, 4: 7, 5: 7}\n"
     ]
    }
   ],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "#exp_name = 'Exp11'\n",
    "#niter = 20000\n",
    "#modality = 'CT'\n",
    "#start_fold = 1\n",
    "#n_folds = 10\n",
    "#fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "#preproc = 'no_preproc'\n",
    "batch_size = 256\n",
    "snap_interval = 5000\n",
    "snap_start = 5000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "#Clinical_Scale = 'ADAS13'\n",
    "\n",
    "#MC_list = np.arange(1,11,1)\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "idx = 0  \n",
    "df_perf_dict_adas = {}\n",
    "df_perf_dict_mmse = {}\n",
    "df_perf_dict_opt = {}\n",
    "\n",
    "for mc in MC_list:\n",
    "    print exp_name, mc, modality\n",
    "\n",
    "    for hype in hype_configs.keys():      \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "\n",
    "        for fid in fid_list:\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "            #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "            #with open(test_filename_txt, 'w') as f:\n",
    "            #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                    f.write(str(adninet_ff_HC(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC']\n",
    "                elif modality == 'CT':\n",
    "                    f.write(str(adninet_ff_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_CT_AAL']\n",
    "                elif modality == 'HC_CT':\n",
    "                    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "                elif modality == 'HC_CT_unified_hyp1':\n",
    "                    f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_HC_CT']\n",
    "                else:\n",
    "                    print 'Wrong modality'\n",
    "\n",
    "            #print 'Hype # {}, MC # {}, Fold # {}, Clinical_Scale {}'.format(hype, mc, fid, Clinical_Scale)\n",
    "            data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            if Clinical_Scale == 'ADAS13':\n",
    "                act_scores = load_data(data_path, 'adas','no_preproc')\n",
    "            elif Clinical_Scale == 'MMSE': \n",
    "                act_scores = load_data(data_path, 'mmse','no_preproc')\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                act_scores_adas13 = load_data(data_path, 'adas','no_preproc')\n",
    "                act_scores_mmse = load_data(data_path, 'mmse','no_preproc')\n",
    "            else:\n",
    "                print 'unknown clinical scale'\n",
    "\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort)\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            sub.call([\"cp\", baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort), baseline_dir + \n",
    "                      'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)])\n",
    "            #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "            #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "\n",
    "            if Clinical_Scale in ['ADAS13', 'MMSE']:                \n",
    "                multi_task = False\n",
    "                cs_list = ['opt']\n",
    "                iter_euLoss = []\n",
    "                iter_r = []        \n",
    "                iter_pred_scores = []\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = np.squeeze(results['X_out'])            \n",
    "                    iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                    iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "                fold_r[config_idx] = np.array(iter_r)\n",
    "                fold_act_scores[fid] = act_scores\n",
    "                fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                multi_task = True\n",
    "                cs_list = ['adas','mmse']\n",
    "                iter_euLoss_adas13 = []\n",
    "                iter_r_adas13 = []        \n",
    "                iter_pred_scores_adas13 = []\n",
    "                iter_euLoss_mmse = []\n",
    "                iter_r_mmse = []        \n",
    "                iter_pred_scores_mmse = []\n",
    "\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = results['X_out']   \n",
    "                    encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                    encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                    iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                    iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                    iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                    iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "                fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "                fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "                fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "                fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "                fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "                fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "                fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "\n",
    "    \n",
    "    if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "        fold_perf_dict = {'fold_r':fold_r,'fold_euLoss':fold_euLoss,'fold_act_scores':fold_act_scores,'fold_pred_scores':fold_pred_scores}\n",
    "    else: \n",
    "        fold_perf_dict = {'fold_r_adas13':fold_r_adas13,'fold_r_mmse':fold_r_mmse,'fold_euLoss_adas13':fold_euLoss_adas13,'fold_euLoss_mmse':fold_euLoss_mmse,\n",
    "                         'fold_act_scores_adas13':fold_act_scores_adas13,'fold_act_scores_mmse':fold_act_scores_mmse,\n",
    "                          'fold_pred_scores_adas13':fold_pred_scores_adas13,'fold_pred_scores_mmse':fold_pred_scores_mmse}\n",
    "        \n",
    "    opt_metric = 'euLoss' #euLoss or corr\n",
    "    task_weights = {'ADAS':1,'MMSE':0}\n",
    "    save_multitask_results = False\n",
    "    CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/'  \n",
    "    save_path = '{}{}_{}_NN_{}'.format(CV_model_dir,exp_name,mc,modality)\n",
    "\n",
    "    results = compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path)  \n",
    "    \n",
    "    # populate the perf dictionary for 10 MC x 10 folds\n",
    "    model_choice = 'APANN'    \n",
    "    for fid in fid_list: \n",
    "        for cs in cs_list:            \n",
    "            r_valid = results['{}_r'.format(cs)][fid-1]\n",
    "            MSE_valid = results['{}_mse'.format(cs)][fid-1]\n",
    "            RMSE_valid = results['{}_rmse'.format(cs)][fid-1]\n",
    "        \n",
    "            if cs == 'adas':\n",
    "                df_perf_dict_adas[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                     'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                \n",
    "            if cs == 'mmse':\n",
    "                df_perf_dict_mmse[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                     'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                \n",
    "            if cs == 'opt':\n",
    "                df_perf_dict_opt[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                     'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "            idx+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Save df style dictionaly for seaborn plots\n",
    "cohort = 'ADNI1'\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_tuned_2.pkl'.format(exp_name, cohort,modality)                \n",
    "pickleIt(df_perf_dict_adas,df_perf_dict_path)\n",
    "# df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE_tuned.pkl'.format(exp_name, cohort,modality)  \n",
    "# pickleIt(df_perf_dict_mmse,df_perf_dict_path)\n",
    "\n",
    "print 'saving results at: {}'.format(CV_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 25)\n",
    "n_rows = n_folds\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "plot_perf = False\n",
    "\n",
    "if multi_task:\n",
    "    euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "    corrs = [fold_r_adas13,fold_r_mmse]\n",
    "else:\n",
    "    euLosses = [fold_euLoss]\n",
    "    corrs = [fold_r]\n",
    "    \n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        \n",
    "        if plot_perf:\n",
    "            plt.figure(fid)\n",
    "            plt.subplot(n_rows,n_cols,1)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "            plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "            plt.legend()\n",
    "            plt.subplot(n_rows,n_cols,2)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "            plt.title('correlation, fid: {}'.format(fid))\n",
    "            plt.legend(loc=2)\n",
    "\n",
    "print preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path):\n",
    "    fold_r_dict = {}\n",
    "    fold_euLoss_dict = {}\n",
    "    fold_act_scores_dict = {}\n",
    "    fold_pred_scores_dict = {}\n",
    "\n",
    "    if multi_task:\n",
    "        fold_r_dict['ADAS'] = fold_perf_dict['fold_r_adas13']\n",
    "        fold_r_dict['MMSE'] = fold_perf_dict['fold_r_mmse']\n",
    "        fold_euLoss_dict['ADAS'] = fold_perf_dict['fold_euLoss_adas13']\n",
    "        fold_euLoss_dict['MMSE'] = fold_perf_dict['fold_euLoss_mmse']\n",
    "        fold_act_scores_dict['ADAS'] = fold_perf_dict['fold_act_scores_adas13']\n",
    "        fold_act_scores_dict['MMSE'] = fold_perf_dict['fold_act_scores_mmse']\n",
    "        fold_pred_scores_dict['ADAS'] = fold_perf_dict['fold_pred_scores_adas13']\n",
    "        fold_pred_scores_dict['MMSE'] = fold_perf_dict['fold_pred_scores_mmse']\n",
    "        \n",
    "        NN_results = computePerfMetrics(fold_r_dict, fold_euLoss_dict, fold_act_scores_dict, fold_pred_scores_dict, opt_metric, hype_configs, \n",
    "                                        Clinical_Scale,task_weights)\n",
    "        adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "        mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "        adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "        mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "        adas_rmse = NN_results['opt_ADAS']['CV_RMSE'].values()\n",
    "        mmse_rmse = NN_results['opt_MMSE']['CV_RMSE'].values()\n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['opt_ADAS']['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['opt_ADAS']['actual_CV_scores']\n",
    "        \n",
    "        print 'ADAS corr: {}'.format(adas_r)\n",
    "        print 'ADAS mse: {}'.format(adas_mse)\n",
    "        print 'ADAS means: {}, {}'.format(np.mean(adas_r),np.mean(adas_mse))\n",
    "        print ''\n",
    "        print 'MMSE corr: {}'.format(mmse_r)\n",
    "        print 'MMSE mse: {}'.format(mmse_mse)\n",
    "        print 'MMSE means: {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse))\n",
    "        print ''\n",
    "        #print 'opt_hyp: {}'.format(opt_hyp)\n",
    "        #print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        \n",
    "        return {'adas_r':adas_r, 'adas_mse':adas_mse, 'adas_rmse':adas_rmse, 'mmse_r':mmse_r, 'mmse_mse':mmse_mse, 'mmse_rmse':mmse_rmse,\n",
    "               'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        NN_results = computeSingleTaskPerfMetrics(fold_perf_dict, opt_metric, hype_configs)\n",
    "        opt_r = NN_results['opt_r'].values()        \n",
    "        opt_mse = NN_results['opt_mse'].values()        \n",
    "        opt_rmse = NN_results['opt_rmse'].values()        \n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['actual_CV_scores']\n",
    "        print 'opt corr: {}'.format(opt_r)\n",
    "        print 'opt mse: {}'.format(opt_mse)\n",
    "        print 'means: {}, {}'.format(np.mean(opt_r),np.mean(opt_mse))\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        print ''\n",
    "    \n",
    "        return {'opt_r':opt_r, 'opt_mse':opt_mse, 'opt_rmse':opt_rmse,'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    \n",
    "\n",
    "\n",
    "    if save_multitask_results:\n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')        \n",
    "        save_path = save_path + '_' + st + '.pkl' \n",
    "        print 'saving results at: {}'.format(save_path)\n",
    "        output = open(save_path, 'wb')\n",
    "        pickle.dump(NN_results, output)\n",
    "        output.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = np.squeeze(results['predicted_CV_scores'][7])\n",
    "b = np.squeeze(results['actual_CV_scores'][7])\n",
    "\n",
    "print a , b                               \n",
    "print stats.pearsonr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    \n",
    "     # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "\n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    opt_rmse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "\n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    opt_rmse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "\n",
    "    print 'Clinical_Scale: {}'.format(Clinical_Scale)\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        print 'This function does not work for single clinical scale models. Use the code from the next cell'\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "\n",
    "        fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "        fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "        fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "        fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "\n",
    "        for fid in fid_hype_map.keys():\n",
    "            r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "            r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "            r_perf_array = np.array(fid_r_perf[fid])\n",
    "            euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "            euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "            euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "            if opt_metric == 'euLoss':\n",
    "                h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "            else:\n",
    "                h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "\n",
    "            opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "            opt_snap[fid] = snp\n",
    "\n",
    "            opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "            opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "            opt_rmse_adas[fid] = np.sqrt(2*euLoss_perf_array_adas[h,snp]) #RMSE\n",
    "            opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "            opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "            opt_rmse_mmse[fid] = np.sqrt(2*euLoss_perf_array_mmse[h,snp]) #RMSE\n",
    "\n",
    "            actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "            opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "            actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "            opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "        opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'CV_RMSE':opt_rmse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "        opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'CV_RMSE':opt_rmse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_snap':opt_snap, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "def computeSingleTaskPerfMetrics(fold_perf_dict,opt_metric,hype_configs):\n",
    "    corrs = fold_perf_dict['fold_r']\n",
    "    euLosses = fold_perf_dict['fold_euLoss']\n",
    "    act_scores = fold_perf_dict['fold_act_scores']\n",
    "    pred_scores = fold_perf_dict['fold_pred_scores']\n",
    "\n",
    "    \n",
    "    NN_multitask_results = {}\n",
    "   \n",
    "    snap_array = np.arange(snap_start,niter+1,snap_start)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = {}\n",
    "    opt_mse = {}\n",
    "    opt_rmse = {}\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "    actual_scores = {}\n",
    "    opt_pred_scores = {}\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "        # if want to find best hyp from mse values\n",
    "        if opt_metric == 'euLoss':\n",
    "            h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        else:\n",
    "            h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "            \n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        r = r_perf_array[h,snp]        \n",
    "        opt_r[fid] = r\n",
    "        opt_mse[fid] = 2*eu_loss\n",
    "        opt_rmse[fid] = np.sqrt(2*eu_loss)\n",
    "        #print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)        \n",
    "        opt_snap[fid] = snp\n",
    "        opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "        actual_scores[fid] = fold_act_scores[fid]\n",
    "        opt_pred_scores[fid] = fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp]\n",
    "\n",
    "    #print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r.values()), np.mean(opt_mse.values()))\n",
    "    #print opt_r\n",
    "    #print opt_mse\n",
    "    return {'opt_r':opt_r,'opt_mse':opt_mse,'opt_rmse':opt_rmse,'opt_hype':opt_hype,'opt_snap':opt_snap, 'actual_scores':actual_scores,'opt_pred_scores':opt_pred_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "\n",
    "model_file = 'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-05-06-17-14-49.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {4:0,5:1}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_ADAS13_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(5.64658243068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update fold performance for multitask\n",
    "print NN_results['opt_ADAS'].keys()\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "#model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl'\n",
    "model_file_soa = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-23-52.pkl'\n",
    "print 'current state of art: ' + model_file_soa\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file_soa,'rb'))\n",
    "\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "#load old file\n",
    "# model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-02-48.pkl'\n",
    "# print 'updated fold result file: ' + model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "idx_pairs = {1:1,3:3}\n",
    "CV_data = update_multifold_perf(boxplots_dir + model_file_soa, NN_results, idx_pairs)\n",
    "\n",
    "print \"\"\n",
    "print 'updated CV_data'\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "#print 'saving results at: {}'.format(save_name)\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(CV_data, output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():        \n",
    "        for idx in idx_pairs.keys():            \n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "\n",
    "    return CV_data\n",
    "\n",
    "def update_multifold_perf(saved_perf_file, new_perf_dict, idx_pairs):    \n",
    "    perf_metrics = ['CV_r', 'actual_CV_scores', 'CV_MSE', 'predicted_CV_scores']    \n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        if key == 'opt_hype':\n",
    "            for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                CV_data[key][idx_key] = new_perf_dict[key][idx_val]\n",
    "                \n",
    "        else:\n",
    "            for pm in perf_metrics:\n",
    "                for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                    CV_data[key][pm][idx_key] = new_perf_dict[key][pm][idx_val]\n",
    "    \n",
    "    return CV_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "model_files = ['Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl',             \n",
    "             'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-05-23-07-31-29.pkl']\n",
    "\n",
    "#'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-03-19-29-16_Fold9_10.pkl'\n",
    "\n",
    "Clinical_Scale = 'ADAS'\n",
    "perf_array = []\n",
    "for mf in model_files:\n",
    "    CV_data = pickle.load(open(boxplots_dir + mf,'rb'))['opt_{}'.format(Clinical_Scale)]        \n",
    "    perf_array.append(CV_data['CV_r'].values())\n",
    "\n",
    "print (np.max(np.array(perf_array),axis=0))\n",
    "print np.mean(np.max(np.array(perf_array),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "\n",
    "\n",
    "save_detailed_results = True\n",
    "multi_task = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results    \n",
    "    if not multi_task:\n",
    "        NN_results = {}\n",
    "        NN_results['CV_MSE'] = opt_mse\n",
    "        NN_results['CV_r'] = opt_r\n",
    "        NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "        NN_results['actual_CV_scores'] = actual_scores\n",
    "        NN_results['tid_snap_config_dict'] = opt_hype\n",
    "        print opt_r\n",
    "        print opt_mse\n",
    "        \n",
    "    else:\n",
    "        NN_results = NN_multitask_results\n",
    "        print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "        print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "        print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "        print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])\n",
    "        \n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_BOTH_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    for modality in modalities:\n",
    "        in_data = load_data(in_data_path, 'Fold_{}_{}'.format(fid,modality), preproc)         \n",
    "\n",
    "        # HDF5 is pretty efficient, but can be further compressed.\n",
    "        comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "        with h5py.File(out_data_path, 'a') as f:      \n",
    "            if modality == 'X_R_CT': #Fix the typo            \n",
    "                modality = 'X_CT'            \n",
    "\n",
    "            f.create_dataset('{}'.format(modality), data=in_data, **comp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp6'\n",
    "exp_name_out = 'Exp6_MC'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "modalities = ['X_L_HC','X_R_HC','X_R_CT','adas','mmse','dx']\n",
    "dataset = 'ADNI1'\n",
    "preproc = 'no_preproc' #binary labels\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "\n",
    "for mc in np.arange(1,3,1):\n",
    "    for fid in np.arange(1,11,1):\n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_train_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_valid_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_ADAS13_NN_valid_MC_{}.h5'.format(exp_name,dataset,mc)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name_out)\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC: fid[1:4] 8000; fid[5,6] 6000, fid[7,8,9]: 8000 fid 10: 6000\n",
    "CT: fid[1:5] 12000 fid[6:10] 6000\n",
    "\n",
    "#spawn net\n",
    "ADNI_FF_train_hyp1_CT.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "modality = 'HC_CT'\n",
    "pretrain_snap_HC = 4000\n",
    "pretrain_snap_CT = 6000\n",
    "n_folds = 10\n",
    "\n",
    "#Custom snaps per fold\n",
    "#HC_iter = [8000,8000,8000,8000,6000,6000,8000,8000,8000,6000]\n",
    "#CT_iter = [12000,12000,12000,12000,12000,6000,6000,6000,6000,6000]\n",
    "\n",
    "hyp = 'hyp2'\n",
    "for fid in np.arange(1,n_folds+1,1):\n",
    "    print 'fid: {}'.format(fid)\n",
    "    #for AE_branch in ['CT','R_HC','L_HC']:\n",
    "    for AE_branch in ['CT','HC']:\n",
    "        print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "        if AE_branch == 'L_HC':\n",
    "            params_FF = ['L_ff1', 'L_ff2']            \n",
    "            AE_iter = 12000\n",
    "        elif AE_branch == 'R_HC':\n",
    "            params_FF = ['R_ff1', 'R_ff2']            \n",
    "            AE_iter = 12000\n",
    "        elif AE_branch == 'HC':\n",
    "            params_FF = ['L_ff1','R_ff1']\n",
    "            AE_iter = pretrain_snap_HC\n",
    "        elif AE_branch == 'CT':\n",
    "            #params_FF = ['ff1', 'ff2']\n",
    "            params_FF = ['ff1']\n",
    "            AE_iter = pretrain_snap_HC\n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            #Only use this during 1 of the modalities to avoid overwritting\n",
    "            print 'Spawning new net'\n",
    "            pretrain_net = caffe.Net(baseline_dir + 'API/data/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(1,hyp,modality), caffe.TRAIN)\n",
    "        else:\n",
    "            print 'Wrong AE branch'\n",
    "\n",
    "        # conv_params = {name: (weights, biases)}\n",
    "        conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "        for conv in params_FF:\n",
    "            print '{} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "        # Review AE net params \n",
    "        #fid for pretain is 1 because it's same definition for all the folds.\n",
    "        net_file = baseline_dir + 'API/data/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(1,hyp,AE_branch)\n",
    "        #model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/Exp11_{}_{}_iter_{}.caffemodel'.format(fid,hyp,AE_branch,AE_iter) \n",
    "\n",
    "        AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "        #params_AE = ['encoder1', 'code']\n",
    "        params_AE = params_FF #if you are using pretrained NN\n",
    "        \n",
    "        # fc_params = {name: (weights, biases)}\n",
    "        fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "        for fc in params_AE:\n",
    "            print '{} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "        #transplant net parameters\n",
    "        for pr, pr_conv in zip(params_AE, params_FF):\n",
    "            conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "            conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "        save_net = True\n",
    "        if save_net:\n",
    "            save_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_{}_{}_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'.format(fid,hyp,modality,pretrain_snap_HC,pretrain_snap_CT)\n",
    "            print \"Saving net to \" + save_path\n",
    "            pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_data['opt_ADAS']['CV_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print n_snaps/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
