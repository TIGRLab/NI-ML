{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from docopt import docopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_from_logs(train_file, valid_file, log_title):\n",
    "    '''\n",
    "    Training Log Header:\n",
    "    #Iters Seconds TrainingLoss LearningRate\n",
    "\n",
    "    Valid Log Header:\n",
    "    #Iters Seconds TestAccuracy TestLoss\n",
    "\n",
    "    :param train_file:\n",
    "    :param valid_file:\n",
    "    :return:\n",
    "    '''\n",
    "    #tX = np.loadtxt(train_file, skiprows=1)    \n",
    "    #vX = np.loadtxt(valid_file, skiprows=1)\n",
    "    tX = np.genfromtxt(train_file, dtype=float, delimiter=',',skip_header=1)     \n",
    "    vX = np.genfromtxt(valid_file, dtype=float, delimiter=',',skip_header=1) \n",
    "    \n",
    "    t_iters = tX[:, 0]\n",
    "    v_iters = vX[:, 0]\n",
    "    seconds = tX[:, 1]\n",
    "\n",
    "    # Training loss and validation accuracy:    \n",
    "    #plt.subplot(211)    \n",
    "    p1, = plt.plot(t_iters, tX[:, 3],  label=\"Training Loss\")\n",
    "    p2, = plt.plot(v_iters, vX[:, 3],  linewidth=2, label=\"Validation Accuracy\")\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.title(log_title)\n",
    "    plt.legend()\n",
    "    #plt.xlim([1000,15000])\n",
    "    #plt.legend(bbox_to_anchor=(0.,1.02, 1., 0.102), loc=3, ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "    \n",
    "    # Learning rate:\n",
    "    #plt.subplot(212)\n",
    "    #p3, = plt.plot(t_iters, tX[:, 2], label=\"Learning Rate: \" + log_title)\n",
    "    #plt.xlabel('Iterations')\n",
    "    #plt.ylabel('Learning Rate')\n",
    "    #plt.legend(loc=1)\n",
    "    #plt.ylim([0,0.01])\n",
    "    #plt.grid()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "baseline_dir= '/projects/nikhil/ADNI_prediction/caffe_output/'\n",
    "n_folds = 10\n",
    "n_cols = 2\n",
    "n_rows = (n_folds+1)/n_cols\n",
    "tid = 1\n",
    "modality = 'HC_CT'\n",
    "\n",
    "for lid in np.arange(1,n_folds+1,1):\n",
    "    plt.subplot(n_rows,n_cols,lid)\n",
    "    train_file = '{}OuterFold{}/caffe.INFO.{}_ff_OF{}_{}_T{}'.format(baseline_dir,lid,'train',lid,modality,tid)\n",
    "    valid_file = '{}OuterFold{}/caffe.INFO.{}_ff_OF{}_{}_T{}'.format(baseline_dir,lid,'test',lid,modality,tid)\n",
    "    log_title = 'Inner Fold Train/Valid Loss (Outer Fold: {})'.format(lid)\n",
    "    plot_from_logs(train_file, valid_file, log_title)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Features from a trained net via forward pass\n",
    "import os\n",
    "import sys\n",
    "#from docopt import docopt\n",
    "import numpy as np\n",
    "import tables as tb\n",
    "import caffe\n",
    "import h5py as h5\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "\n",
    "def extract_features(net_file, model_file, target_file, data_path, input_nodes, batch_size):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)    \n",
    "    \n",
    "    print net.blobs.items()[0]\n",
    "    print net.blobs.items()[1]\n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = batch_size    \n",
    "    N = load_data(data_path, input_nodes[0]).shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    code_layer = net.blobs['clas']\n",
    "    out_shape = code_layer.data.shape    \n",
    "    X_out = np.zeros(shape=(N, out_shape[1]))\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node))\n",
    "        data_layers.append(net.blobs.items()[i][1])    \n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "    \n",
    "    net.reshape()\n",
    "        \n",
    "    print 'Extracting features from data...'\n",
    "    print 'X_out.shape: {}'.format(X_out.shape)\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "\n",
    "    #np.save(target_file, X_out)\n",
    "    #print 'Saved to {}'.format(target_file)\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess as sub\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import collections\n",
    "\n",
    "baseline_dir= '/projects/nikhil/ADNI_prediction/'\n",
    "caffe_output_path = baseline_dir + 'caffe_output/'\n",
    "input_data_dir_path = baseline_dir + 'input_datasets/'\n",
    "target_file = caffe_output_path + 'test_1'\n",
    "modality = 'L_HC'\n",
    "innerLoop = False\n",
    "outerLoop = False\n",
    "\n",
    "n_folds = 10\n",
    "tid = 3\n",
    "batch_size = 256\n",
    "\n",
    "X_out = []\n",
    "labels = []\n",
    "CV_MSE = []\n",
    "\n",
    "for lid in np.arange(1,n_folds+1,1):\n",
    "    model_file = '{}OuterFold{}/_iter_10000.caffemodel_{}_T{}'.format(caffe_output_path,lid,modality,tid)\n",
    "    net_file = '{}OuterFold{}/net_partition.prototxt_{}_T{}'.format(caffe_output_path,lid,modality,tid)\n",
    "    \n",
    "    fold_dir = '{}OuterFold{}'.format(caffe_output_path,lid)\n",
    "    if innerLoop: #Inner fold Validation\n",
    "        sub.call([\"cp\", fold_dir+\"/test_inner_val_partition.txt\", fold_dir+\"/test.txt\"])\n",
    "        input_data = '{}inflated_datasets/HC_CT_inflated_CV_OuterFold_{}_valid_InnerFold_1_partition_ROI_74.h5'.format(input_data_dir_path,lid)\n",
    "        \n",
    "        if modality == 'HC':\n",
    "                input_nodes = ['Fold_{}_X_{}'.format(lid,'L_HC'),'Fold_{}_X_{}'.format(lid,'R_HC')]\n",
    "        elif modality == 'HC_CT':\n",
    "            input_nodes = ['Fold_{}_X_{}'.format(lid,'L_HC'),'Fold_{}_X_{}'.format(lid,'R_HC'),\n",
    "                           'Fold_{}_X_{}'.format(lid,'R_CT')]  #Typo R_CT while creating folds with partition\n",
    "        else:\n",
    "            input_nodes = ['Fold_{}_X_{}'.format(lid,modality)]\n",
    "        \n",
    "        labels_dataset_name = 'Fold_{}_y'.format(lid)\n",
    "        label_dataset = h5.File(input_data)                \n",
    "\n",
    "    elif outerLoop: #Outer fold Validation (fused preferably)\n",
    "        sub.call([\"cp\", fold_dir+\"/test_outer_val_partition_fused.txt\", fold_dir+\"/test.txt\"])\n",
    "        input_data = '{}HC_CT_inflated_CV_OuterFolds_valid_partition_fused.h5'.format(input_data_dir_path,lid)\n",
    "        \n",
    "        if modality == 'HC':\n",
    "                input_nodes = ['Fold_{}_X_{}'.format(lid,'L_HC'),'Fold_{}_X_{}'.format(lid,'R_HC')]\n",
    "        elif modality == 'HC_CT':\n",
    "            input_nodes = ['Fold_{}_X_{}'.format(lid,'L_HC'),'Fold_{}_X_{}'.format(lid,'R_HC'),\n",
    "                           'Fold_{}_X_{}'.format(lid,'R_CT')]  #Typo R_CT while creating folds with partition\n",
    "        else:\n",
    "            input_nodes = ['Fold_{}_X_{}'.format(lid,modality)]\n",
    "        \n",
    "        labels_dataset_name = 'Fold_{}_y'.format(lid)\n",
    "        label_dataset = h5.File(input_data)        \n",
    "    \n",
    "    else: #Heldout testset\n",
    "        sub.call([\"cp\", fold_dir+\"/test_heldout.txt\", fold_dir+\"/test.txt\"])\n",
    "        input_data = '{}HC_CT_inflated_CV_OuterFolds_test_partition_fused.h5'.format(input_data_dir_path,lid)\n",
    "        \n",
    "        if modality == 'HC':\n",
    "                input_nodes = ['test_X_{}'.format(lid,'L_HC'),'test_X_{}'.format(lid,'R_HC')]\n",
    "        elif modality == 'HC_CT':\n",
    "            input_nodes = ['test_X_{}'.format(lid,'L_HC'),'test_X_{}'.format(lid,'R_HC'),\n",
    "                           'test_X_{}'.format(lid,'R_CT')]  #Typo R_CT while creating folds with partition\n",
    "        else:\n",
    "            input_nodes = ['test_X_{}'.format(lid,modality)]\n",
    "        \n",
    "        labels_dataset_name = 'test_y'.format(lid)\n",
    "        label_dataset = h5.File(input_data) \n",
    "        \n",
    "        \n",
    "    predicted_scores = extract_features(net_file, model_file, target_file, input_data, input_nodes, batch_size)\n",
    "    actual_scores = label_dataset[labels_dataset_name][:]\n",
    "    X_out.append(predicted_scores)\n",
    "    labels.append(actual_scores)\n",
    "    CV_MSE.append(mse(predicted_scores,actual_scores))\n",
    "    label_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#boxplot_config_MSE = collections.defaultdict(list)\n",
    "method_labels = ['L_HC', 'R_HC', 'HC', 'HC_CT']\n",
    "boxplot_config_MSE[modality].append(CV_MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/ipykernel/__main__.py:11: FutureWarning: \n",
      "The default value for 'return_type' will change to 'axes' in a future release.\n",
      " To use the future behavior now, set return_type='axes'.\n",
      " To keep the previous behavior and silence this warning, set return_type='dict'.\n"
     ]
    }
   ],
   "source": [
    "#Plot CV MSE performance\n",
    "from matplotlib.artist import setp\n",
    "import pandas as pd\n",
    "plt.figure()\n",
    "font_small = 12\n",
    "font_med = 16\n",
    "font_large = 24\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "df_array = pd.DataFrame(dict([ (k,pd.Series(v[0])) for k,v in boxplot_config_MSE.iteritems() ]))\n",
    "bplot = df_array.boxplot(column=method_labels, fontsize=font_large)\n",
    "#plt.xlabel('Method',fontsize=font_large)\n",
    "plt.ylabel('MSE',fontsize=font_large)\n",
    "plt.xticks(fontsize=font_med)\n",
    "plt.yticks(fontsize=font_small)\n",
    "setp(bplot['boxes'], linewidth=2)\n",
    "setp(bplot['whiskers'], linewidth=2)\n",
    "setp(bplot['fliers'], linewidth=2)\n",
    "setp(bplot['medians'], linewidth=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil/.conda/envs/snowflakes/lib/python2.7/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "n_rows = 2\n",
    "n_cols = (n_folds+1)/n_rows\n",
    "plt.figure()\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "font_small = 8\n",
    "font_med = 16\n",
    "font_large = 24\n",
    "\n",
    "if innerLoop:\n",
    "    loop_name = 'Inner Fold' \n",
    "else:\n",
    "    loop_name = 'Outer Fold'\n",
    "\n",
    "for i in np.arange(n_folds):\n",
    "    x = np.squeeze(X_out[i])\n",
    "    y = np.squeeze(labels[i])\n",
    "\n",
    "    plt.subplot(n_rows,n_cols,i+1)\n",
    "    plt.scatter(x, y, c='crimson',s=20)\n",
    "    fit = np.polyfit(x,y,1)\n",
    "    fit_fn = np.poly1d(fit) \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    if p_value < 0.0001:\n",
    "        p_value_sig = '<0.0001'\n",
    "    else:\n",
    "        p_value_sig = str(p_value)\n",
    "        \n",
    "    label_str = 'r-value: {:04.2f}'.format(r_value) + '\\n' + 'p-value: ' + p_value_sig + '\\n' + 'std_err: {:04.2f}'.format(std_err) \n",
    "    # fit_fn is now a function which takes in x and returns an estimate for y\n",
    "    plt.plot(x, fit_fn(x),linewidth=3, label=label_str)\n",
    "    \n",
    "    title_str = '{}: {} Validation'.format(loop_name, i+1)\n",
    "    plt.title(title_str,fontsize=font_med)\n",
    "    plt.xlabel('Actual Score',fontsize=font_med)\n",
    "    plt.ylabel('Predicted Score',fontsize=font_med)            \n",
    "    plt.legend(fontsize=font_med,loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e301efa9225e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mtest_fused_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseline_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexemplar_train_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtest_fused_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_X_L_HC'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheldout_train_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mL_HC_offset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mtest_fused_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_X_R_HC'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheldout_train_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mL_HC_offset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mL_HC_offset\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mR_HC_offset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mtest_fused_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_X_CT'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheldout_train_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mL_HC_offset\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mR_HC_offset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "#Create Train + Val subset for \"Exemplar Model\" training\n",
    "baseline_dir= '/projects/nikhil/ADNI_prediction/input_datasets/inflated_datasets/'\n",
    "exemplar_train_file = 'HC_CT_inflated_Exemplar_train_partition_subsets.h5'\n",
    "lid=1\n",
    "input_data_train_X = load_data(baseline_dir + 'HC_CT_inflated_CV_subsets.h5','Fold_{}_train_X'.format(lid))\n",
    "input_data_train_y = load_data(baseline_dir + 'HC_CT_inflated_CV_subsets.h5','Fold_{}_train_y'.format(lid))\n",
    "\n",
    "input_data_valid_X = load_data(baseline_dir + 'HC_CT_inflated_CV_subsets.h5','Fold_{}_valid_X'.format(lid))\n",
    "input_data_valid_y = load_data(baseline_dir + 'HC_CT_inflated_CV_subsets.h5','Fold_{}_valid_y'.format(lid))\n",
    "\n",
    "heldout_train_X = np.vstack((input_data_train_X, input_data_valid_X))\n",
    "heldout_train_y = np.concatenate((input_data_train_y, input_data_valid_y))\n",
    "\n",
    "L_HC_offset=11427\n",
    "R_HC_offset=10519\n",
    "\n",
    "test_fused_data = h5.File(baseline_dir + exemplar_train_file,'a')\n",
    "test_fused_data.create_dataset('test_X_L_HC'.format(f+1),data=heldout_train_X[:,:L_HC_offset])\n",
    "test_fused_data.create_dataset('test_X_R_HC'.format(f+1),data=heldout_train_X[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "test_fused_data.create_dataset('test_X_CT'.format(f+1),data=heldout_train_X[:,L_HC_offset+R_HC_offset:])    \n",
    "test_fused_data.create_dataset('test_y',data=heldout_train_y)\n",
    "test_fused_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "from scipy.stats.mstats_basic import mquantiles\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "import tables as tb\n",
    "import sys\n",
    "import os\n",
    "#import lmdb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import describe\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "#from activations import visualize_activations, hinton\n",
    "from scipy.spatial.distance import dice\n",
    "\n",
    "def get3DVol(HC_input, HC_shape, input_mask):\n",
    "    flatvol = np.zeros(np.prod(HC_shape))\n",
    "    flatvol[input_mask] = HC_input\n",
    "    vol = flatvol.reshape(-1, HC_shape[2]).T\n",
    "    return vol\n",
    "\n",
    "def plot_slices(slice_list, baseline_shape, baseline_mask, llimit=0.01, ulimit=0.99, xmin=200, xmax=1600):\n",
    "    \"\"\"\n",
    "    Plot dem slices.\n",
    "    :param slice_list:\n",
    "    :param llimit:\n",
    "    :param ulimit:\n",
    "    :param num_slices:\n",
    "    :param xmin:\n",
    "    :param xmax:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_slices = len(slice_list)\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    cols = 2\n",
    "    rows = num_slices / cols\n",
    "    plt.cla()\n",
    "    for j, input in enumerate(slice_list):\n",
    "        quantiles = mquantiles(input[0], [llimit, ulimit])\n",
    "        wt_vol = get3DVol(input[0], baseline_shape, baseline_mask)\n",
    "        plt.subplot(rows, cols, j + 1)\n",
    "        im = plt.imshow(wt_vol[:, xmin:xmax], cmap=plt.cm.Reds, aspect='auto', interpolation='none', vmin=-.06, vmax=0.06)\n",
    "        plt.grid()\n",
    "        plt.title(input[1])\n",
    "        plt.colorbar()\n",
    "        im.set_clim(quantiles[0], quantiles[1])\n",
    "        plt.axis('off')\n",
    "        \n",
    "def getDice(X,X_hat):    \n",
    "    X.astype(int)\n",
    "    X_hat_r = np.round(X_hat)\n",
    "    X_hat_r.astype(int)\n",
    "    d=[]\n",
    "    for i in np.arange(X.shape[0]):\n",
    "        d.append(dice(X[i,:],X_hat_r[i,:]))\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "#layer_names = ['encoder1','encoder2','encoder3','code']\n",
    "layer_names = ['encoder1','encoder2','code']\n",
    "\n",
    "act_title = 'test_35_T10k.h5'\n",
    "\n",
    "layer_acts = []\n",
    "input_file = '/projects/nikhil/miccai/visuals/train_logs/' + act_title\n",
    "input_data = h5.File(input_file, 'r')\n",
    "for layer in layer_names:\n",
    "    layer_acts.append(input_data[layer][:])\n",
    "\n",
    "X_hat = input_data['output_Sigmoid'][:]\n",
    "input_data.close()\n",
    "\n",
    "sampx = np.shape(X_hat)[0]\n",
    "\n",
    "sim=0\n",
    "# simulation\n",
    "if sim==1:\n",
    "    input_file = '/projects/nikhil/miccai/input_data_comb/HC_sim_cat4_data_2.h5'\n",
    "    input_data = h5.File(input_file, 'r')\n",
    "    features = input_data['train_data_1'][:]\n",
    "    labels = input_data['train_classes_1'][:]\n",
    "    input_data.close()\n",
    "\n",
    "    ind0 = np.where(labels[:sampx] == 0)[0]\n",
    "    ind1 = np.where(labels[:sampx] == 1)[0]\n",
    "    ind2 = np.where(labels[:sampx] == 2)[0]\n",
    "    ind3 = np.where(labels[:sampx] == 3)[0]\n",
    "    \n",
    "else:\n",
    "    input_file = '/projects/nikhil/miccai/visuals/train_logs/ad_cn_test.h5'\n",
    "    input_data = h5.File(input_file, 'r')\n",
    "    features = input_data['l_hc_features'][:]\n",
    "    labels = input_data['label'][:]\n",
    "    input_data.close()\n",
    "    ind0 = np.where(labels[:sampx] == 0)[0]\n",
    "    ind1 = np.where(labels[:sampx] == 1)[0]\n",
    "\n",
    "X = features[:sampx,:]\n",
    "\n",
    "#recon_dice = getDice(X, X_hat)\n",
    "#print \"mean dice scores of the test sample reconstructions: \" + str(np.mean(recon_dice))\n",
    "\n",
    "    \n",
    "plt.figure()\n",
    "for i in np.arange(len(layer_acts)):\n",
    "    activations = layer_acts[i]\n",
    "    print layer_names[i] + \" :\" + str(np.mean(activations))\n",
    "    if activations.shape[1] != 1:\n",
    "        tsne = TSNE(n_components=2, random_state=0, init='pca')\n",
    "        proj = tsne.fit_transform(activations.astype(float))\n",
    "    else:\n",
    "        proj = activations\n",
    "    \n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.scatter(proj[ind0, 0], proj[ind0, 1], c='mediumturquoise', alpha=0.7,label='AD')\n",
    "    plt.scatter(proj[ind1, 0], proj[ind1, 1], c='slategray', alpha=0.7,label='CN')\n",
    "    \n",
    "    if sim==1:\n",
    "        plt.scatter(proj[ind2, 0], proj[ind2, 1], c='mediumpurple', alpha=0.7,label='grp3')\n",
    "        plt.scatter(proj[ind3, 0], proj[ind3, 1], c='darksalmon', alpha=0.7,label='grp4')\n",
    "        \n",
    "        \n",
    "    plt.title(layer_names[i] + ' layer activations')\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize inputs vs their reconstructions:\n",
    "mappings = tb.open_file('/projects/nikhil/miccai/visuals/train_logs/data_mappings.h5', 'r')\n",
    "baseline_mask = mappings.get_node('/r_datamask')[:]\n",
    "volmask = mappings.get_node('/r_volmask')[:]\n",
    "mappings.close()\n",
    "baseline_shape = volmask.shape\n",
    "\n",
    "plot_list = []\n",
    "for x in range(6):\n",
    "    i = np.random.random_integers(sampx)\n",
    "    plot_list.append((X[i], 'X {}'.format(i)))\n",
    "    plot_list.append((np.round(X_hat[i]), 'X_hat {}'.format(i)))\n",
    "    \n",
    "plot_slices(plot_list, baseline_shape, baseline_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=0.7\n",
    "plt.figure()\n",
    "plt.hist(np.sum(features[labels==0],axis=1),alpha=a,normed=1,bins=50,label='AD')\n",
    "plt.hist(np.sum(features[labels==1],axis=1),alpha=a,normed=1,bins=50,label='CN')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simulation\n",
    "input_file = '/projects/nikhil/miccai/input_data_comb/HC_sim_cat4_data_2.h5'\n",
    "input_data = h5.File(input_file, 'r')\n",
    "features_all = input_data['train_data_1'][:]\n",
    "labels = input_data['train_classes_1'][:]\n",
    "input_data.close()\n",
    "#Missing HC cases :-P \n",
    "vols = np.sum(features_all,axis=1)\n",
    "empty_rows = np.where(vols!=0)[0]\n",
    "features = features_all[empty_rows,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.externals import joblib\n",
    "n_components=64\n",
    "train_pca = RandomizedPCA(n_components=n_components).fit(features)\n",
    "features_loadings = train_pca.transform(features)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_dice=[]\n",
    "for i in np.arange(5):\n",
    "    print i        \n",
    "    feature_recon = np.dot(features_loadings[:,:i],train_pca.components_[:i,:]) + train_pca.mean_\n",
    "    pca_dice.append(np.nanmean(getDice(features,feature_recon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "36000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
