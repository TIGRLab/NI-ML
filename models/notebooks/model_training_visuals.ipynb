{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from docopt import docopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_from_logs(train_file, valid_file, log_title):\n",
    "    '''\n",
    "    Training Log Header:\n",
    "    #Iters Seconds TrainingLoss LearningRate\n",
    "\n",
    "    Valid Log Header:\n",
    "    #Iters Seconds TestAccuracy TestLoss\n",
    "\n",
    "    :param train_file:\n",
    "    :param valid_file:\n",
    "    :return:\n",
    "    '''\n",
    "    #tX = np.loadtxt(train_file, skiprows=1)    \n",
    "    #vX = np.loadtxt(valid_file, skiprows=1)\n",
    "    tX = np.genfromtxt(train_file, dtype=float, delimiter=',',skip_header=1)     \n",
    "    vX = np.genfromtxt(valid_file, dtype=float, delimiter=',',skip_header=1) \n",
    "    \n",
    "    t_iters = tX[:, 0]\n",
    "    v_iters = vX[:, 0]\n",
    "    seconds = tX[:, 1]\n",
    "\n",
    "    # Training loss and validation accuracy:    \n",
    "    #plt.subplot(211)    \n",
    "    p1, = plt.plot(t_iters, tX[:, 3],  label=\"Training Loss\")\n",
    "    p2, = plt.plot(v_iters, vX[:, 3],  linewidth=2, label=\"Validation Accuracy\")\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.title(log_title)\n",
    "    plt.legend()\n",
    "    #plt.xlim([1000,15000])\n",
    "    #plt.legend(bbox_to_anchor=(0.,1.02, 1., 0.102), loc=3, ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "    \n",
    "    # Learning rate:\n",
    "    #plt.subplot(212)\n",
    "    #p3, = plt.plot(t_iters, tX[:, 2], label=\"Learning Rate: \" + log_title)\n",
    "    #plt.xlabel('Iterations')\n",
    "    #plt.ylabel('Learning Rate')\n",
    "    #plt.legend(loc=1)\n",
    "    #plt.ylim([0,0.01])\n",
    "    #plt.grid()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "baseline_dir= '/projects/nikhil/ADNI_prediction/caffe_output/'\n",
    "n_folds = 10\n",
    "n_cols = 2\n",
    "n_rows = n_folds/n_cols\n",
    "\n",
    "for lid in np.arange(1,n_folds+1,1):\n",
    "    plt.subplot(n_rows,n_cols,lid)\n",
    "    train_file = '{}OuterFold{}/caffe.INFO.{}_ff_OF{}_T1'.format(baseline_dir,lid,'train',lid)\n",
    "    valid_file = '{}OuterFold{}/caffe.INFO.{}_ff_OF{}_T1'.format(baseline_dir,lid,'test',lid)\n",
    "    log_title = 'Inner Fold Train/Valid Loss (Outer Fold: {})'.format(lid)\n",
    "    plot_from_logs(train_file, valid_file, log_title)\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Features from a trained net via forward pass\n",
    "import os\n",
    "import sys\n",
    "#from docopt import docopt\n",
    "import numpy as np\n",
    "import tables as tb\n",
    "import caffe\n",
    "import h5py as h5\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "\n",
    "def extract_features(net_file, model_file, target_file, data_path, input_node):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)    \n",
    "    \n",
    "    X = load_data(data_path, input_node)    \n",
    "    BATCH_SIZE = 128\n",
    "    print 'X shape: {}'.format(X.shape)\n",
    "    N = X.shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    code_layer = net.blobs['clas']\n",
    "    out_shape = code_layer.data.shape\n",
    "    print 'out_shape: {}'.format(out_shape)\n",
    "    X_out = np.zeros(shape=(N, out_shape[1]))\n",
    "\n",
    "    data_layer = net.blobs.items()[0][1]\n",
    "    #print 'net.blobs.items()'\n",
    "    #print net.blobs.items()\n",
    "    #print net.blobs.items()[0][1]\n",
    "    \n",
    "    data_layer.reshape(BATCH_SIZE, X.shape[1]) # TODO: only works for 2-D inputs\n",
    "    net.reshape()\n",
    "        \n",
    "    print 'Extracting features from data...'\n",
    "    print 'X_out.shape: {}'.format(X_out.shape)\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        print '.',\n",
    "        X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "        batch_sampx = X_b.shape[0]\n",
    "        # Pad last batch with zeros\n",
    "        if X_b.shape[0] < BATCH_SIZE:\n",
    "            print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "            X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "        data_layer.data[...] = X_b\n",
    "        net.forward()\n",
    "        X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "\n",
    "    #np.save(target_file, X_out)\n",
    "    #print 'Saved to {}'.format(target_file)\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (7292, 22025)\n",
      "out_shape: (128, 1)\n",
      "Extracting features from data...\n",
      "X_out.shape: (7292, 1)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Zero-padding last batch with 4 rows\n",
      "X shape: (7315, 22025)\n",
      "out_shape: (128, 1)\n",
      "Extracting features from data...\n",
      "X_out.shape: (7315, 1)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Zero-padding last batch with 109 rows\n",
      "X shape: (7315, 22025)\n",
      "out_shape: (128, 1)\n",
      "Extracting features from data...\n",
      "X_out.shape: (7315, 1)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Zero-padding last batch with 109 rows\n",
      "X shape: (7315, 22025)\n",
      "out_shape: (128, 1)\n",
      "Extracting features from data...\n",
      "X_out.shape: (7315, 1)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Zero-padding last batch with 109 rows\n",
      "X shape: (7315, 22025)\n",
      "out_shape: (128, 1)\n",
      "Extracting features from data...\n",
      "X_out.shape: (7315, 1)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Zero-padding last batch with 109 rows\n",
      "X shape: (7315, 22025)\n",
      "out_shape: (128, 1)\n",
      "Extracting features from data...\n",
      "X_out.shape: (7315, 1)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Zero-padding last batch with 109 rows\n",
      "X shape: (7315, 22025)\n",
      "out_shape: (128, 1)\n",
      "Extracting features from data...\n",
      "X_out.shape: (7315, 1)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Zero-padding last batch with 109 rows\n",
      "X shape: (7315, 22025)\n",
      "out_shape: (128, 1)\n",
      "Extracting features from data...\n",
      "X_out.shape: (7315, 1)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Zero-padding last batch with 109 rows\n",
      "X shape: (7315, 22025)\n",
      "out_shape: (128, 1)\n",
      "Extracting features from data...\n",
      "X_out.shape: (7315, 1)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Zero-padding last batch with 109 rows\n",
      "X shape: (7315, 22025)\n",
      "out_shape: (128, 1)\n",
      "Extracting features from data...\n",
      "X_out.shape: (7315, 1)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Zero-padding last batch with 109 rows\n"
     ]
    }
   ],
   "source": [
    "baseline_dir= '/projects/nikhil/ADNI_prediction/'\n",
    "caffe_output_path = baseline_dir + 'caffe_output/'\n",
    "input_data_dir_path = baseline_dir + 'input_datasets/'\n",
    "target_file = caffe_output_path + 'test_1'\n",
    "innerLoop = True\n",
    "n_folds = 10\n",
    "X_out = []\n",
    "labels = []\n",
    "for lid in np.arange(1,n_folds+1,1):\n",
    "    model_file = '{}OuterFold{}/_iter_10000.caffemodel'.format(caffe_output_path,lid)\n",
    "    net_file = '{}OuterFold{}/net.prototxt'.format(caffe_output_path,lid)\n",
    "        \n",
    "    if innerLoop: #Inner fold Validation\n",
    "        input_data = '{}HC_CT_inflated_CV_OuterFold_{}_valid_InnerFold_1.h5'.format(input_data_dir_path,lid)\n",
    "        data_layer = 'Fold_{}_X'.format(lid)\n",
    "        labels_dataset_name = 'Fold_{}_y'.format(lid)\n",
    "        label_dataset = h5.File(input_data)                \n",
    "\n",
    "    else: #Outer fold Validation    \n",
    "        input_data = '{}HC_CT_inflated_CV_OuterFolds_valid.h5'.format(input_data_dir_path,lid)\n",
    "        data_layer = 'Fold_{}_X'.format(lid)\n",
    "        labels_dataset_name = 'Fold_{}_y'.format(lid)\n",
    "        label_dataset = h5.File(input_data)        \n",
    "        \n",
    "\n",
    "    X_out.append(extract_features(net_file, model_file, target_file, input_data, data_layer))\n",
    "    labels.append(label_dataset[labels_dataset_name][:])\n",
    "    label_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_cols = 2\n",
    "n_rows = n_folds/n_cols\n",
    "plt.figure()\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "font_small = 8\n",
    "font_med = 16\n",
    "font_large = 24\n",
    "\n",
    "if innerLoop:\n",
    "    loop_name = 'Inner Fold' \n",
    "else:\n",
    "    loop_name = 'Outer Fold'\n",
    "\n",
    "for i in np.arange(n_folds):\n",
    "    x = np.squeeze(X_out[i])\n",
    "    y = np.squeeze(labels[i])\n",
    "\n",
    "    plt.subplot(n_rows,n_cols,i+1)\n",
    "    plt.scatter(x, y, c='crimson',s=20)\n",
    "    fit = np.polyfit(x,y,1)\n",
    "    fit_fn = np.poly1d(fit) \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    if p_value < 0.0001:\n",
    "        p_value_sig = '<0.0001'\n",
    "    else:\n",
    "        p_value_sig = str(p_value)\n",
    "        \n",
    "    label_str = 'r-value: {:04.2f}'.format(r_value) + '\\n' + 'p-value: ' + p_value_sig + '\\n' + 'std_err: {:04.2f}'.format(std_err) \n",
    "    # fit_fn is now a function which takes in x and returns an estimate for y\n",
    "    plt.plot(x, fit_fn(x),linewidth=3, label=label_str)\n",
    "    \n",
    "    title_str = '{}: {} Validation'.format(loop_name, i+1)\n",
    "    plt.title(title_str,fontsize=font_large)\n",
    "    plt.xlabel('Actual Score',fontsize=font_large)\n",
    "    plt.ylabel('Predicted Score',fontsize=font_large)            \n",
    "    plt.legend(fontsize=font_med,loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "from scipy.stats.mstats_basic import mquantiles\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "import tables as tb\n",
    "import sys\n",
    "import os\n",
    "#import lmdb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import describe\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "#from activations import visualize_activations, hinton\n",
    "from scipy.spatial.distance import dice\n",
    "\n",
    "def get3DVol(HC_input, HC_shape, input_mask):\n",
    "    flatvol = np.zeros(np.prod(HC_shape))\n",
    "    flatvol[input_mask] = HC_input\n",
    "    vol = flatvol.reshape(-1, HC_shape[2]).T\n",
    "    return vol\n",
    "\n",
    "def plot_slices(slice_list, baseline_shape, baseline_mask, llimit=0.01, ulimit=0.99, xmin=200, xmax=1600):\n",
    "    \"\"\"\n",
    "    Plot dem slices.\n",
    "    :param slice_list:\n",
    "    :param llimit:\n",
    "    :param ulimit:\n",
    "    :param num_slices:\n",
    "    :param xmin:\n",
    "    :param xmax:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_slices = len(slice_list)\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    cols = 2\n",
    "    rows = num_slices / cols\n",
    "    plt.cla()\n",
    "    for j, input in enumerate(slice_list):\n",
    "        quantiles = mquantiles(input[0], [llimit, ulimit])\n",
    "        wt_vol = get3DVol(input[0], baseline_shape, baseline_mask)\n",
    "        plt.subplot(rows, cols, j + 1)\n",
    "        im = plt.imshow(wt_vol[:, xmin:xmax], cmap=plt.cm.Reds, aspect='auto', interpolation='none', vmin=-.06, vmax=0.06)\n",
    "        plt.grid()\n",
    "        plt.title(input[1])\n",
    "        plt.colorbar()\n",
    "        im.set_clim(quantiles[0], quantiles[1])\n",
    "        plt.axis('off')\n",
    "        \n",
    "def getDice(X,X_hat):    \n",
    "    X.astype(int)\n",
    "    X_hat_r = np.round(X_hat)\n",
    "    X_hat_r.astype(int)\n",
    "    d=[]\n",
    "    for i in np.arange(X.shape[0]):\n",
    "        d.append(dice(X[i,:],X_hat_r[i,:]))\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "#layer_names = ['encoder1','encoder2','encoder3','code']\n",
    "layer_names = ['encoder1','encoder2','code']\n",
    "\n",
    "act_title = 'test_35_T10k.h5'\n",
    "\n",
    "layer_acts = []\n",
    "input_file = '/projects/nikhil/miccai/visuals/train_logs/' + act_title\n",
    "input_data = h5.File(input_file, 'r')\n",
    "for layer in layer_names:\n",
    "    layer_acts.append(input_data[layer][:])\n",
    "\n",
    "X_hat = input_data['output_Sigmoid'][:]\n",
    "input_data.close()\n",
    "\n",
    "sampx = np.shape(X_hat)[0]\n",
    "\n",
    "sim=0\n",
    "# simulation\n",
    "if sim==1:\n",
    "    input_file = '/projects/nikhil/miccai/input_data_comb/HC_sim_cat4_data_2.h5'\n",
    "    input_data = h5.File(input_file, 'r')\n",
    "    features = input_data['train_data_1'][:]\n",
    "    labels = input_data['train_classes_1'][:]\n",
    "    input_data.close()\n",
    "\n",
    "    ind0 = np.where(labels[:sampx] == 0)[0]\n",
    "    ind1 = np.where(labels[:sampx] == 1)[0]\n",
    "    ind2 = np.where(labels[:sampx] == 2)[0]\n",
    "    ind3 = np.where(labels[:sampx] == 3)[0]\n",
    "    \n",
    "else:\n",
    "    input_file = '/projects/nikhil/miccai/visuals/train_logs/ad_cn_test.h5'\n",
    "    input_data = h5.File(input_file, 'r')\n",
    "    features = input_data['l_hc_features'][:]\n",
    "    labels = input_data['label'][:]\n",
    "    input_data.close()\n",
    "    ind0 = np.where(labels[:sampx] == 0)[0]\n",
    "    ind1 = np.where(labels[:sampx] == 1)[0]\n",
    "\n",
    "X = features[:sampx,:]\n",
    "\n",
    "#recon_dice = getDice(X, X_hat)\n",
    "#print \"mean dice scores of the test sample reconstructions: \" + str(np.mean(recon_dice))\n",
    "\n",
    "    \n",
    "plt.figure()\n",
    "for i in np.arange(len(layer_acts)):\n",
    "    activations = layer_acts[i]\n",
    "    print layer_names[i] + \" :\" + str(np.mean(activations))\n",
    "    if activations.shape[1] != 1:\n",
    "        tsne = TSNE(n_components=2, random_state=0, init='pca')\n",
    "        proj = tsne.fit_transform(activations.astype(float))\n",
    "    else:\n",
    "        proj = activations\n",
    "    \n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.scatter(proj[ind0, 0], proj[ind0, 1], c='mediumturquoise', alpha=0.7,label='AD')\n",
    "    plt.scatter(proj[ind1, 0], proj[ind1, 1], c='slategray', alpha=0.7,label='CN')\n",
    "    \n",
    "    if sim==1:\n",
    "        plt.scatter(proj[ind2, 0], proj[ind2, 1], c='mediumpurple', alpha=0.7,label='grp3')\n",
    "        plt.scatter(proj[ind3, 0], proj[ind3, 1], c='darksalmon', alpha=0.7,label='grp4')\n",
    "        \n",
    "        \n",
    "    plt.title(layer_names[i] + ' layer activations')\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize inputs vs their reconstructions:\n",
    "mappings = tb.open_file('/projects/nikhil/miccai/visuals/train_logs/data_mappings.h5', 'r')\n",
    "baseline_mask = mappings.get_node('/r_datamask')[:]\n",
    "volmask = mappings.get_node('/r_volmask')[:]\n",
    "mappings.close()\n",
    "baseline_shape = volmask.shape\n",
    "\n",
    "plot_list = []\n",
    "for x in range(6):\n",
    "    i = np.random.random_integers(sampx)\n",
    "    plot_list.append((X[i], 'X {}'.format(i)))\n",
    "    plot_list.append((np.round(X_hat[i]), 'X_hat {}'.format(i)))\n",
    "    \n",
    "plot_slices(plot_list, baseline_shape, baseline_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=0.7\n",
    "plt.figure()\n",
    "plt.hist(np.sum(features[labels==0],axis=1),alpha=a,normed=1,bins=50,label='AD')\n",
    "plt.hist(np.sum(features[labels==1],axis=1),alpha=a,normed=1,bins=50,label='CN')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simulation\n",
    "input_file = '/projects/nikhil/miccai/input_data_comb/HC_sim_cat4_data_2.h5'\n",
    "input_data = h5.File(input_file, 'r')\n",
    "features_all = input_data['train_data_1'][:]\n",
    "labels = input_data['train_classes_1'][:]\n",
    "input_data.close()\n",
    "#Missing HC cases :-P \n",
    "vols = np.sum(features_all,axis=1)\n",
    "empty_rows = np.where(vols!=0)[0]\n",
    "features = features_all[empty_rows,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.externals import joblib\n",
    "n_components=64\n",
    "train_pca = RandomizedPCA(n_components=n_components).fit(features)\n",
    "features_loadings = train_pca.transform(features)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_dice=[]\n",
    "for i in np.arange(5):\n",
    "    print i        \n",
    "    feature_recon = np.dot(features_loadings[:,:i],train_pca.components_[:i,:]) + train_pca.mean_\n",
    "    pca_dice.append(np.nanmean(getDice(features,feature_recon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "36000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
