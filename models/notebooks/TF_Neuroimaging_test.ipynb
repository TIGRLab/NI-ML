{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import h5py as h5\n",
    "import SimpleITK as sitk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Data: http://repo.mouseimaging.ca/repo/for_Nikhil/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model Architecture Defs\n",
    "#MLP\n",
    "def build_model(X_,_dropout,node_sizes,net_type):\n",
    "    if net_type == 'mlpnet':\n",
    "        model = mlpnet(X_,_dropout,node_sizes)\n",
    "    elif net_type == 'convnet':\n",
    "        model = convnet(X_,_dropout,node_sizes)\n",
    "    else: \n",
    "        print('unknown net architecture type')\n",
    "    return model\n",
    "\n",
    "def mlpnet(image,_dropout,node_sizes):\n",
    "    l1 = mlp(image,node_sizes['input'],node_sizes['l1'],name='l1')\n",
    "    l1 = tf.nn.dropout(l1,_dropout)\n",
    "    l2 = mlp(l1,node_sizes['l1'],node_sizes['l2'],name='l2')\n",
    "    l2 = tf.nn.dropout(l2,1)\n",
    "#     l3 = mlp(l2,node_sizes['l2'],node_sizes['l3'],name='l3')\n",
    "#     l3 = tf.nn.dropout(l3,1)\n",
    "    l4 = mlp(l2,node_sizes['l2'],node_sizes['output'],name='l4')\n",
    "    return l4\n",
    "\n",
    "def mlp(input_,input_dim,output_dim,name=\"mlp\"):\n",
    "    with tf.variable_scope(name):\n",
    "        #w = tf.get_variable('w',[input_dim,output_dim],tf.float32,tf.random_normal_initializer(mean = 0.001,stddev=0.02))\n",
    "        w = tf.get_variable('w',[input_dim,output_dim],tf.float32,tf.contrib.layers.xavier_initializer())\n",
    "        return tf.nn.relu(tf.matmul(input_,w))\n",
    "    \n",
    "#Convnet\n",
    "def convnet(X_,_dropout,node_sizes):\n",
    "    #image_dim = int(np.sqrt(node_sizes['input']))\n",
    "    image_dim = node_sizes['input']\n",
    "    final_image_vector_size = node_sizes['input'][0]*node_sizes['input'][1]/node_sizes['max_pool_redux_factor']\n",
    "    \n",
    "    X_image = tf.reshape(X_, [-1,image_dim[0],image_dim[1],1])\n",
    "    l1 = conv(X_image,node_sizes['kernel_dim'], 1, node_sizes['l1'], name='l1')  \n",
    "    l1 = tf.nn.dropout(l1,_dropout)\n",
    "    l2 = conv(l1,node_sizes['kernel_dim'], node_sizes['l1'], node_sizes['l2'], name='l2')\n",
    "    l2 = tf.nn.dropout(l2,_dropout)\n",
    "    \n",
    "    #reshape to vector (reduced image size*number of filters)\n",
    "    l2_flat = tf.reshape(l2, [-1, final_image_vector_size*node_sizes['l2']])    \n",
    "    l3 = mlp(l2_flat,final_image_vector_size*node_sizes['l2'],node_sizes['output'],name='mlp')    \n",
    "    #l3 = tf.nn.dropout(l3,_dropout) #Doesnt work well\n",
    "    return l3\n",
    "    \n",
    "def conv(input_, kernel_dim, in_channels, output_dim,name=\"conv\"):\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('w',[kernel_dim, kernel_dim, in_channels, output_dim],\n",
    "                            tf.float32,tf.random_normal_initializer(mean = 0.01,stddev=0.02))\n",
    "        b = tf.get_variable('b',[output_dim],tf.float32,tf.constant_initializer(0.1))                        \n",
    "        h_conv = tf.nn.relu(conv2d(input_, W) + b)\n",
    "        h_pool = max_pool_2x2(h_conv)\n",
    "        return h_pool\n",
    "    \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    \n",
    "def next_batch(s,e,inputs,labels):\n",
    "    input1 = inputs[s:e]\n",
    "    y = np.reshape(labels[s:e],(len(range(s,e)),2))\n",
    "    return input1,y\n",
    "\n",
    "def compute_accuracy(prediction,labels):\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    \n",
    "    return acc\n",
    "\n",
    "# Extract 2D slices OR 1D labels from volumes \n",
    "def getSliceInfoFrom3DVolume(vol_list,slice_dim,label_slices):\n",
    "    slicex = vol_list[0].shape[slice_dim]\n",
    "    #slice_info_list = []\n",
    "    for v,vol in enumerate(vol_list):\n",
    "        sub_slices = np.split(vol,slicex,axis=slice_dim)\n",
    "        sub_slices_vectorized = np.reshape(sub_slices,[-1,slicex])\n",
    "        \n",
    "        #If volume is a segmented image then count tumor voxels. \n",
    "        if label_slices:\n",
    "            label_count = np.sum(sub_slices_vectorized,axis=0) > 0\n",
    "            if v ==0:\n",
    "                out_array = label_count\n",
    "            else:\n",
    "                out_array = np.concatenate((out_array,label_count))\n",
    "        \n",
    "        #Else just list 2D slices\n",
    "        else:\n",
    "            if v ==0:\n",
    "                out_array = sub_slices_vectorized\n",
    "            else:\n",
    "                out_array = np.hstack((out_array,sub_slices_vectorized))\n",
    "        \n",
    "    return np.squeeze(out_array)\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_dir = '/Users/nikhil/projects/mice_tumors/'\n",
    "\n",
    "image_label_pairs = {\n",
    "\"B3_M6_null_062615_HR_MUT_small.npy\": \"B3_M6_null_062615_HR_tumor_labels_small.npy\",\n",
    "\"B3_C2_R_062215_HR_MUT_small.npy\": \"B3_C2_R_062215_HR_tumor_labels_small.npy\", \n",
    "\"B3_M7_LLR_062715_HR_MUT_small.npy\": \"B3_M7_LLR_062715_HR_tumor_labels_small.npy\" ,\n",
    "\"B3_C5_LR_063015_HR_MUT_small.npy\":\"B3_C5_LR_063015_HR_tumor_labels_small.npy\" , \n",
    "\"B3_M7_LLR_070215_HR_MUT_small.npy\":\"B3_M7_LLR_070215_HR_tumor_labels_small.npy\" ,\n",
    "\"B3_M3_LL_062315_HR_MUT_small.npy\":\"B3_M3_LL_062315_HR_tumor_labels_small.npy\" , \n",
    "\"B3_M8_LRR_062715_HR_MUT_small.npy\":\"B3_M8_LRR_062715_HR_tumor_labels_small.npy\" ,\n",
    "\"B3_M4_RR_062615_HR_MUT_small.npy\":\"B3_M4_RR_062615_HR_tumor_labels_small.npy\" ,\n",
    "}\n",
    "\n",
    "no_tumor_images = [\"B10_C14_W3.hdr_WT_small.npy\",\"B10_C2_021016_HR_W3.hdr_WT_small.npy\",\n",
    "\"B10_C15_W3.hdr_WT_small.npy\",\"B10_C3_021016_HR_W3.hdr_WT_small.npy\",\"B10_C1_021016_HR_W3.hdr_WT_small.npy\"]\n",
    "\n",
    "sample_vol_img_list = []\n",
    "sample_vol_label_list = []\n",
    "\n",
    "for key in image_label_pairs.keys():\n",
    "    sample_vol_img_file = '{}input_data/images/MUT/{}'.format(baseline_dir,key)\n",
    "    sample_vol_label_file = '{}input_data/labels/{}'.format(baseline_dir,image_label_pairs[key])\n",
    "\n",
    "    sample_vol_img = np.load(sample_vol_img_file)\n",
    "    sample_vol_label = np.load(sample_vol_label_file)\n",
    "\n",
    "    #Crop to square slices\n",
    "    sample_vol_img = sample_vol_img1[:24,:,:24]\n",
    "    sample_vol_label = sample_vol_label1[:24,:,:24]\n",
    "\n",
    "    #List all files\n",
    "    sample_vol_img_list.append(sample_vol_img)\n",
    "    sample_vol_label_list.append(sample_vol_label)\n",
    "\n",
    "print len(sample_vol_img_list), len(sample_vol_label_list)\n",
    "image = getSliceInfoFrom3DVolume(sample_vol_img_list,1,False).T\n",
    "segs = getSliceInfoFrom3DVolume(sample_vol_label_list,1,False).T.astype(int)\n",
    "labels = getSliceInfoFrom3DVolume(sample_vol_label_list,1,True)\n",
    "print image.shape, segs.shape, labels.shape\n",
    "\n",
    "sample_vol_img_list = []\n",
    "for no_tumor in no_tumor_images:\n",
    "    sample_vol_img_file = '{}input_data/images/no_tumor/{}'.format(baseline_dir,no_tumor)\n",
    "    sample_vol_img = np.load(sample_vol_img_file)\n",
    "    sample_vol_img = sample_vol_img1[:24,:,:24]\n",
    "    sample_vol_img_list.append(sample_vol_img)\n",
    "    \n",
    "print len(sample_vol_img_list)\n",
    "image_no_tumor = getSliceInfoFrom3DVolume(sample_vol_img_list,1,False).T\n",
    "    \n",
    "print image_no_tumor.shape\n",
    "\n",
    "image = np.vstack((image,image_no_tumor))\n",
    "labels = np.concatenate((labels,np.zeros(image_no_tumor.shape[0])))\n",
    "\n",
    "print image.shape, segs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot image and label slices\n",
    "plt.figure(figsize=(20,10))\n",
    "example_images = 5\n",
    "for p, idx in enumerate(np.random.randint(0,segs.shape[0],example_images)):\n",
    "    plt.subplot(example_images/5,5,p+1)\n",
    "    plt.imshow(np.reshape(image[idx,:]-10000*segs[idx,:],[24,24]))\n",
    "    plt.colorbar()\n",
    "    plt.title('idx:{}, label:{}'.format(idx,labels[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If KF doesn't exist then create train-val-test sets\n",
    "\n",
    "n = image.shape[0]\n",
    "d = image.shape[1]\n",
    "\n",
    "#shuffle data\n",
    "\n",
    "idx = np.arange(n)\n",
    "np.random.shuffle(idx)\n",
    "X = image[idx,:]\n",
    "y_raw = labels[idx]\n",
    "\n",
    "#One-hot\n",
    "y = np.vstack((y_raw==True,y_raw==False)).T.astype(int)\n",
    "print X.shape, y.shape\n",
    "\n",
    "#split data\n",
    "n_train = int(0.8*n)\n",
    "n_val = int(0.1*n)\n",
    "\n",
    "X_train = X[:n_train]\n",
    "X_val   = X[n_train:n_train+n_val]\n",
    "X_test  = X[n_train+n_val:]\n",
    "\n",
    "#Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y[:n_train]\n",
    "y_val   = y[n_train:n_train+n_val]\n",
    "y_test  = y[n_train+n_val:]\n",
    "\n",
    "print(len(y_train),len(y_val),len(y_test))\n",
    "print(np.sum(y[:n_train,0]==1)/float(len(y_train)),np.sum(y[n_train:n_train+n_val,0]==1)/float(len(y_val)),\n",
    "      np.sum(y[n_train+n_val:,0]==1)/float(len(y_test)))\n",
    "print(np.sum(y[:n_train,0]==0)/float(len(y_train)),np.sum(y[n_train:n_train+n_val,0]==0)/float(len(y_val)),\n",
    "      np.sum(y[n_train+n_val:,0]==0)/float(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Run TF\n",
    "batch_size = 100\n",
    "num_epochs = 200\n",
    "n_folds = 1\n",
    "lr = 0.0001\n",
    "keep_p = 0.5\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for fid in range(1,n_folds+1,1):        \n",
    "    node_sizes = {'input':[24,24],'l1':20,'l2':20,'l3':0,'output':2,'kernel_dim':5,'max_pool_redux_factor':16}\n",
    "    with tf.Graph().as_default():    \n",
    "        data = tf.placeholder(tf.float32,shape=([None,node_sizes['input'][0]*node_sizes['input'][1]]),name='input_data')        \n",
    "        labels = tf.placeholder(tf.float32,shape=([None,2]),name='gt')                \n",
    "        dropout_f = tf.placeholder(\"float\")\n",
    "\n",
    "        with tf.variable_scope(\"micenet\") as scope:\n",
    "            model1= build_model(data,dropout_f,node_sizes,'convnet')            \n",
    "            \n",
    "        \n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model1,labels=labels))         \n",
    "        acc = compute_accuracy(model1,labels)\n",
    "        \n",
    "#         t_vars = tf.trainable_variables()\n",
    "#         d_vars  = [var for var in t_vars if 'l' in var.name]\n",
    "#         batch = tf.Variable(0)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "        #optimizer = tf.train.RMSPropOptimizer(0.0001,momentum=0.9,epsilon=1e-6).minimize(loss)\n",
    "\n",
    "        # Launch the graph\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            #sess.run(init)\n",
    "            tf.global_variables_initializer().run()\n",
    "            # Training cycle\n",
    "            for epoch in range(num_epochs):\n",
    "                avg_loss = 0.\n",
    "                avg_acc = 0.\n",
    "                total_batch = int(X_train.shape[0]/batch_size)\n",
    "                start_time = time.time()\n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "                    s  = i * batch_size\n",
    "                    e = (i+1) *batch_size\n",
    "                    \n",
    "                    # Fit training using batch data\n",
    "                    tr_X,tr_y = next_batch(s,e,X_train,y_train)  \n",
    "                    #print(input1.shape, y.shape)\n",
    "                                \n",
    "                    _,tr_loss,tr_acc=sess.run([optimizer,loss,acc], \n",
    "                                                  feed_dict={data:tr_X,labels:tr_y,dropout_f:keep_p})\n",
    "                    \n",
    "                    predict=model1.eval(feed_dict={data:tr_X,dropout_f:1.0})                    \n",
    "                    tr_acc = compute_accuracy(predict,tr_y).eval()                \n",
    "                                        \n",
    "                    #print(tr_acc.eval())\n",
    "#                     if math.isnan(tr_acc) and epoch != 0:\n",
    "#                         print('tr_acc %0.2f' % tr_acc)\n",
    "#                         pdb.set_trace()\n",
    "\n",
    "                    avg_loss += tr_loss\n",
    "                    avg_acc +=tr_acc*100\n",
    "                                        \n",
    "                #print('epoch %d loss %0.2f' %(epoch,avg_loss/total_batch))\n",
    "                duration = time.time() - start_time\n",
    "                #print('epoch %d  time: %f loss %0.5f acc %0.2f' %(epoch,duration,avg_loss/(total_batch),avg_acc/total_batch))\n",
    "                train_loss.append(avg_loss/(total_batch))\n",
    "                train_acc.append(avg_acc/total_batch)\n",
    "                \n",
    "                #Check performance on validation set                                                                                \n",
    "                predict=model1.eval(feed_dict={data:X_val,dropout_f:1.0})                \n",
    "                va_acc = compute_accuracy(predict,y_val).eval()*100.0\n",
    "                valid_acc.append(va_acc)\n",
    "                \n",
    "                # Test model                                    \n",
    "                predict= model1.eval(feed_dict={data:X_test,dropout_f:1.0})                \n",
    "                te_acc = compute_accuracy(predict,y_test).eval()*100\n",
    "                test_acc.append(te_acc)\n",
    "                \n",
    "                \n",
    "                if epoch%10 == 0:\n",
    "                    print('Accuracy validation set %0.2f' % ((va_acc)))\n",
    "\n",
    "            #Accuracy of the entire training set at the end of training            \n",
    "            predict= model1.eval(feed_dict={data:X_train, dropout_f:1.0})                \n",
    "            tr_acc = compute_accuracy(predict,y_train).eval()*100\n",
    "            print('Accuracy training set %0.2f' % (tr_acc))\n",
    "            print('Accuracy validation set %0.2f' % (va_acc))\n",
    "\n",
    "\n",
    "            # Test model                                    \n",
    "            predict= model1.eval(feed_dict={data:X_test,dropout_f:1.0})                \n",
    "            te_acc = compute_accuracy(predict,y_test)\n",
    "            #test_acc.append(te_acc)\n",
    "            print('Accuracy test set %0.2f' % (100 * te_acc.eval()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Acc and Loss\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(train_loss, 'g-')\n",
    "ax2.plot(train_acc, 'b--')\n",
    "ax2.plot(valid_acc, 'r-')\n",
    "ax2.plot(test_acc, 'k-')\n",
    "\n",
    "\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('train_loss', color='g')\n",
    "ax2.set_ylabel('val_acc', color='r')\n",
    "ax1.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
