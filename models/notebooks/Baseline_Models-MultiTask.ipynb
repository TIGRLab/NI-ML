{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import re\n",
    "import collections\n",
    "import tables as tb\n",
    "from math import isnan\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data imports\n",
    "my_name = 'Exp6'\n",
    "cohort = 'ADNI1'\n",
    "clinical_scale = 'ADAS13andMMSE'\n",
    "exp_name = '{}_{}_{}'.format(my_name,cohort,clinical_scale)\n",
    "atlas = 'AAL'\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "HC_L_data_path = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_left_{}.pkl'.format(cohort,my_name)\n",
    "HC_R_data_path = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_right_{}.pkl'.format(cohort,my_name)\n",
    "CT_data_path = baseline_dir + 'CT/civet_out/{}_subject_ROI_CT_dict_{}.pkl'.format(cohort,atlas)\n",
    "CT_unique_ROIs_path = baseline_dir + 'CT/civet_out/ADNI_unique_ROIs_{}.pkl'.format(atlas)\n",
    "sub_CS1_data_path = baseline_dir + 'CS/{}_BL_PTID_ADAS13_dict.pkl'.format(cohort)\n",
    "sub_CS2_data_path = baseline_dir + 'CS/{}_BL_PTID_MMSE_dict.pkl'.format(cohort)\n",
    "sub_DX_data_path = baseline_dir + 'CS/{}_BL_PTID_DX_bl_dict.pkl'.format(cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common HC keys: 686\n",
      "HC_L_keys: 686, HC_R_keys: 686, CT_keys: 806\n"
     ]
    }
   ],
   "source": [
    "#Fused labels and volumes\n",
    "sub_HC_L_dict = pickle.load( open(HC_L_data_path, \"rb\" ) )\n",
    "sub_HC_R_dict = pickle.load( open(HC_R_data_path, \"rb\" ) )\n",
    "\n",
    "HC_L_total_vol_dict = {}\n",
    "HC_R_total_vol_dict = {}\n",
    "\n",
    "HC_common_keys = set(sub_HC_L_dict.keys()) & set(sub_HC_R_dict.keys())\n",
    "print 'Common HC keys: {}'.format(len(HC_common_keys))\n",
    "for key in HC_common_keys:\n",
    "    fused_vol = stats.mode(sub_HC_L_dict[key],axis=0)[0] #Left HC\n",
    "    HC_L_total_vol_dict[key] = np.sum(fused_vol)\n",
    "    fused_vol = stats.mode(sub_HC_R_dict[key],axis=0)[0] #Right HC\n",
    "    HC_R_total_vol_dict[key] = np.sum(fused_vol)\n",
    "    \n",
    "#Mean CT values for AAL\n",
    "sub_CT_dict = pickle.load( open(CT_data_path, \"rb\" ) )\n",
    "unique_ROIs = pickle.load( open(CT_unique_ROIs_path, \"rb\" ) )\n",
    "#unique_ROIs = sub_CT_dict['137_S_0459'][0].keys()\n",
    "CT_mean_dict = {}\n",
    "for key in sub_CT_dict.keys():\n",
    "    CT_roi_dict = sub_CT_dict[key][0]\n",
    "    mean_CT_list = []\n",
    "    for roi in unique_ROIs:\n",
    "        #ignore roi list (mainly the background ones...)\n",
    "        if not roi in [0]:\n",
    "            mean_CT_list.append(np.mean(CT_roi_dict[roi]))\n",
    "                \n",
    "    CT_mean_dict[key] = mean_CT_list\n",
    "    \n",
    "print 'HC_L_keys: {}, HC_R_keys: {}, CT_keys: {}'.format(len(HC_L_total_vol_dict), len(HC_R_total_vol_dict), len(CT_mean_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dump the unique ordered list of CT ROIs that is consistent across ADNI1 and 2. \n",
    "sub_CT_dict = pickle.load( open(CT_data_path, \"rb\" ) )\n",
    "unique_ROIs = sub_CT_dict['137_S_0459'][0].keys()\n",
    "print len(unique_ROIs)\n",
    "pickleIt(unique_ROIs,baseline_dir + 'CT/civet_out/ADNI_unique_ROIs_{}.pkl'.format(atlas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of common keys across modalities: 669\n",
      "X shape: (669, 80), y shape: (669, 2)\n"
     ]
    }
   ],
   "source": [
    "# ADAS / MMSE score dict\n",
    "sub_CS_dict = pickle.load( open(sub_CS1_data_path, \"rb\" ) )\n",
    "sub_CS1_dict_clean = filter(lambda k: not isnan(sub_CS_dict[k]), sub_CS_dict) #remove NaNs\n",
    "sub_CS1_dict_clean = {k: sub_CS_dict[k] for k in sub_CS_dict if not isnan(sub_CS_dict[k])}\n",
    "\n",
    "sub_CS_dict = pickle.load( open(sub_CS2_data_path, \"rb\" ) )\n",
    "sub_CS2_dict_clean = filter(lambda k: not isnan(sub_CS_dict[k]), sub_CS_dict) #remove NaNs\n",
    "sub_CS2_dict_clean = {k: sub_CS_dict[k] for k in sub_CS_dict if not isnan(sub_CS_dict[k])}\n",
    "\n",
    "#Dx score dict (for stratified k-fold)\n",
    "sub_DX_dict = pickle.load( open(sub_DX_data_path, \"rb\" ) )\n",
    "#sub_DX_dict_clean = filter(lambda k: not isnan(sub_DX_dict[k]), sub_DX_dict) #remove NaNs\n",
    "#sub_DX_dict_clean = {k: sub_DX_dict[k] for k in sub_DX_dict if not isnan(sub_DX_dict[k])}\n",
    "\n",
    "common_keys = list(set(HC_L_total_vol_dict.keys()) & set(HC_R_total_vol_dict.keys()) \n",
    "                   & set(CT_mean_dict.keys()) & set(sub_CS1_dict_clean.keys())& set(sub_CS1_dict_clean.keys()))\n",
    "print '# of common keys across modalities: {}'.format(len(common_keys))\n",
    "X = []\n",
    "y = []\n",
    "y_dx = [] #Used for creating balanced k-folds\n",
    "for key in common_keys:\n",
    "    X.append(np.array([HC_L_total_vol_dict[key]] + [HC_R_total_vol_dict[key]] + CT_mean_dict[key]))\n",
    "    y.append([sub_CS1_dict_clean[key],sub_CS2_dict_clean[key]])\n",
    "    if key in sub_DX_dict:\n",
    "        if sub_DX_dict[key] in ['EMCI','LMCI']:\n",
    "            y_dx.append('MCI')\n",
    "        elif sub_DX_dict[key] in ['CN','SMC']:   \n",
    "            y_dx.append('CN')\n",
    "        elif sub_DX_dict[key] in ['AD']:  \n",
    "            y_dx.append('AD')\n",
    "        else:\n",
    "            print \"Unknown Dx\"\n",
    "    else:\n",
    "        print \"No Dx found for {}\".format(key)\n",
    "        y_dx.append('CN') #Two subjects without Dx \n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X shape: {}, y shape: {}'.format(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(669, 2)\n"
     ]
    }
   ],
   "source": [
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fce49632bb40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified KFold\n",
      "DX distribution (AD,CN,LMCI): (15, 20, 33)\n",
      "y_train_mean: [ 18.13529118  26.88685524], y_test_mean: [ 17.1075      26.82352941]\n",
      "DX distribution (AD,CN,LMCI): (15, 20, 33)\n",
      "y_train_mean: [ 18.02096506  26.89517471], y_test_mean: [ 18.11794118  26.75      ]\n",
      "DX distribution (AD,CN,LMCI): (15, 20, 33)\n",
      "y_train_mean: [ 17.99108153  26.88851913], y_test_mean: [ 18.38205882  26.80882353]\n",
      "DX distribution (AD,CN,LMCI): (15, 20, 33)\n",
      "y_train_mean: [ 18.00266223  26.87021631], y_test_mean: [ 18.27970588  26.97058824]\n",
      "DX distribution (AD,CN,LMCI): (15, 20, 33)\n",
      "y_train_mean: [ 17.92397671  26.87354409], y_test_mean: [ 18.97514706  26.94117647]\n",
      "DX distribution (AD,CN,LMCI): (14, 20, 33)\n",
      "y_train_mean: [ 18.05310631  26.88870432], y_test_mean: [ 17.83059701  26.80597015]\n",
      "DX distribution (AD,CN,LMCI): (14, 20, 32)\n",
      "y_train_mean: [ 18.12268657  26.85406302], y_test_mean: [ 17.19151515  27.12121212]\n",
      "DX distribution (AD,CN,LMCI): (14, 20, 32)\n",
      "y_train_mean: [ 18.05631841  26.89054726], y_test_mean: [ 17.79787879  26.78787879]\n",
      "DX distribution (AD,CN,LMCI): (14, 19, 32)\n",
      "y_train_mean: [ 17.95518212  26.8692053 ], y_test_mean: [ 18.73369231  26.98461538]\n",
      "DX distribution (AD,CN,LMCI): (14, 19, 32)\n",
      "y_train_mean: [ 18.04682119  26.88741722], y_test_mean: [ 17.88215385  26.81538462]\n",
      "(669, 80) (669, 2)\n",
      "Saving exp_setup to: /projects/nikhil/ADNI_prediction/input_datasets/exp_data/CV_Exp6_concat.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create folds for CV (default Stratified based on DX)\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "#Encode Dx labels to integers map: {AD:0,CN:1,LMCI:2}\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_dx)\n",
    "y_dx_int = le.transform(y_dx) \n",
    "\n",
    "stratified_KF = True #Something is wrong with ADNI2 dx data... \n",
    "if not stratified_KF:\n",
    "    print 'KFold'\n",
    "    kf = KFold(len(y), n_folds=10)\n",
    "else: \n",
    "    print 'Stratified KFold'\n",
    "    kf = StratifiedKFold(y_dx_int, n_folds=10)\n",
    "    \n",
    "for train_index, test_index in kf:    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print 'DX distribution (AD,CN,LMCI): {}'.format((np.sum(y_dx_int[test_index]==0),np.sum(y_dx_int[test_index]==1),\n",
    "                                              np.sum(y_dx_int[test_index]==2)))\n",
    "    print 'y_train_mean: {}, y_test_mean: {}'.format(np.mean(y_train,axis=0),np.mean(y_test,axis=0))\n",
    "\n",
    "print X.shape, y.shape\n",
    "save_experimental_setup = True  #Saves X, y, and KF\n",
    "if save_experimental_setup:\n",
    "    save_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/CV_{}.pkl'.format(exp_name)    \n",
    "    exp_setup = {'X': X, 'y': y, 'y_dx': y_dx, 'kf':kf,'common_subs':common_keys,'exp_name':exp_name}    \n",
    "    pickleIt(exp_setup, save_path)\n",
    "    print 'Saving exp_setup to: {}'.format(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# QC plots\n",
    "n_feat = X.shape[1]\n",
    "print X.shape\n",
    "\n",
    "feat_corr  = []\n",
    "for col in np.arange(n_feat):\n",
    "    feat_corr.append(stats.pearsonr(X[:,col],y)[0])\n",
    "\n",
    "print 'L_HC corr: {}, R_HC_corr: {}'.format(feat_corr[0],feat_corr[1]) \n",
    "print 'HC Correlation in each fold'\n",
    "fid = 1\n",
    "for train_index, test_index in kf:    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_L_HC_vol_mean = np.mean(X_train[:,0])\n",
    "    train_L_HC_vol_std = np.std(X_train[:,0])\n",
    "    train_R_HC_vol_mean = np.mean(X_train[:,1])\n",
    "    train_R_HC_vol_std = np.std(X_train[:,1])\n",
    "    \n",
    "    test_L_HC_vol_mean = np.mean(X_test[:,0])\n",
    "    test_L_HC_vol_std = np.std(X_test[:,0])\n",
    "    test_R_HC_vol_mean = np.mean(X_test[:,1])\n",
    "    test_R_HC_vol_std = np.std(X_test[:,1])\n",
    "    \n",
    "    train_L_HC_corr = stats.pearsonr(X_train[:,0],y_train)[0]\n",
    "    train_R_HC_corr = stats.pearsonr(X_train[:,1],y_train)[0]\n",
    "    test_L_HC_corr = stats.pearsonr(X_test[:,0],y_test)[0]\n",
    "    test_R_HC_corr = stats.pearsonr(X_test[:,1],y_test)[0]\n",
    "    print fid\n",
    "    fid+=1\n",
    "    print 'test L_HC vol, std, corr: {}, {}, {}'.format(test_L_HC_vol_mean,test_L_HC_vol_std,test_L_HC_corr)\n",
    "    print 'test R_HC vol, std, corr: {}, {}, {}'.format(test_R_HC_vol_mean,test_R_HC_vol_std,test_R_HC_corr)\n",
    "    print 'train L_HC vol, std, corr: {}, {}, {}'.format(train_L_HC_vol_mean,train_L_HC_vol_std,train_L_HC_corr)\n",
    "    print 'train R_HC vol, std, corr: {}, {}, {}'.format(train_R_HC_vol_mean,train_R_HC_vol_std,train_R_HC_corr)\n",
    "    \n",
    "\n",
    "alpha = 0.5\n",
    "plt.figure()\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('HC_L total volume')\n",
    "plt.hist(X[y_dx_int==0,0],bins=100,alpha=alpha,label='AD: {}'.format(np.mean(X[y_dx_int==0,0])))\n",
    "plt.hist(X[y_dx_int==1,0],bins=100,alpha=alpha,label='CN: {}'.format(np.mean(X[y_dx_int==1,0])))\n",
    "plt.hist(X[y_dx_int==2,0],bins=100,alpha=alpha,label='MCI: {}'.format(np.mean(X[y_dx_int==2,0])))\n",
    "plt.legend()\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('HC_R total volume')\n",
    "plt.hist(X[y_dx_int==0,1],bins=100,alpha=alpha,label='AD: {}'.format(np.mean(X[y_dx_int==0,1])))\n",
    "plt.hist(X[y_dx_int==1,1],bins=100,alpha=alpha,label='CN: {}'.format(np.mean(X[y_dx_int==1,1])))\n",
    "plt.hist(X[y_dx_int==2,1],bins=100,alpha=alpha,label='MCI: {}'.format(np.mean(X[y_dx_int==2,1])))\n",
    "plt.legend()\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('CT mean per ROI')\n",
    "plt.bar(np.arange(n_feat-2),np.mean(X[:,2:],axis=0))\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('Correlation with CS for each feature')\n",
    "plt.bar(np.arange(n_feat),feat_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(669, 80) (669, 2)\n",
      "ADNI1\n",
      "Left HC\n",
      "AD: 3602\n",
      "MCI: 3755\n",
      "CN: 3878\n",
      "Right HC\n",
      "AD: 3612\n",
      "MCI: 3765\n",
      "CN: 3929\n",
      "CT\n",
      "AD: 2.89622991643\n",
      "MCI: 2.98721598902\n",
      "CN: 3.06806548109\n"
     ]
    }
   ],
   "source": [
    "# Load save experimental setup\n",
    "cohort = 'ADNI1'\n",
    "exp_name = 'Exp6'\n",
    "exp_setup_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/CV_{}_{}_ADAS13andMMSE.pkl'.format(exp_name,cohort)\n",
    "exp_setup = pickle.load( open(exp_setup_path, \"rb\" ) )\n",
    "\n",
    "X = exp_setup['X']\n",
    "y = exp_setup['y']\n",
    "\n",
    "print X.shape, y.shape\n",
    "y_dx = exp_setup['y_dx']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_dx)\n",
    "y_dx_int = le.transform(y_dx) \n",
    "print cohort\n",
    "print 'Left HC'\n",
    "print 'AD: {}'.format(int(np.mean(X[y_dx_int==0,0])))\n",
    "print 'MCI: {}'.format(int(np.mean(X[y_dx_int==2,0])))\n",
    "print 'CN: {}'.format(int(np.mean(X[y_dx_int==1,0])))\n",
    "print 'Right HC'\n",
    "print 'AD: {}'.format(int(np.mean(X[y_dx_int==0,1])))\n",
    "print 'MCI: {}'.format(int(np.mean(X[y_dx_int==2,1])))\n",
    "print 'CN: {}'.format(int(np.mean(X[y_dx_int==1,1])))\n",
    "print  'CT'\n",
    "print 'AD: {}'.format((np.mean(X[y_dx_int==0,2:])))\n",
    "print 'MCI: {}'.format((np.mean(X[y_dx_int==2,2:])))\n",
    "print 'CN: {}'.format((np.mean(X[y_dx_int==1,2:])))\n",
    "\n",
    "#ADNI1: CT\n",
    "#AD: 2.89622991643\n",
    "#MCI: 2.98721598902\n",
    "#CN: 3.06806548109\n",
    "#ADNI2: CT\n",
    "#AD: 2.79175707773\n",
    "#MCI: 2.92246056401\n",
    "#CN: 2.9658935139\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configs for K-fold validations (nested)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import MultiTaskLasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import grid_search\n",
    "import datetime\n",
    "import time\n",
    "import collections\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import ipyparallel as ipp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import KFold\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pick model with its configs/hyper-params\n",
    "model_list = ['LR_L1']\n",
    "model_choice_id = 0\n",
    "model_choice = model_list[model_choice_id]\n",
    "modality = 'HC_CT'\n",
    "\n",
    "if model_choice == 'LR_L1':\n",
    "    model_clf = MultiTaskLasso()\n",
    "    hyper_params = {'alpha':[0.2, 0.1, 0.05, 0.01]} \n",
    "    scale_data = True #Scales HC and CT features\n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = True     \n",
    "    \n",
    "else:\n",
    "    print \"Unknown model choice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (669, 80)\n",
      "y shape (669, 2)\n",
      "most frequent hp:{'alpha': array([ 0.2])}\n",
      "CV MSE (mean, median, std_err): 30.19,29.51, 2.05\n"
     ]
    }
   ],
   "source": [
    "# Train and Test models\n",
    "from functools import partial #Parallelize!!! \n",
    "\n",
    "# Load save experimental setup\n",
    "cohort = 'ADNI1and2'\n",
    "exp_name = 'Exp6'\n",
    "exp_setup_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/CV_Exp6_ADNI1_ADAS13andMMSE.pkl'\n",
    "exp_setup = pickle.load( open(exp_setup_path, \"rb\" ) )\n",
    "\n",
    "X_raw = exp_setup['X']\n",
    "\n",
    "if modality == 'HC_CT':\n",
    "    X_modality = X_raw\n",
    "elif modality == 'HC':\n",
    "    X_modality = X_raw[:,:2]\n",
    "elif modality == 'CT':\n",
    "    X_modality = X_raw[:,2:]\n",
    "else:\n",
    "    print \"Wrong modality selected...\"\n",
    "    \n",
    "if scale_data:\n",
    "    X = preprocessing.scale(X_modality)\n",
    "else:\n",
    "    X = X_modality\n",
    "\n",
    "print 'X shape {}'.format(X.shape)\n",
    "\n",
    "y = exp_setup['y']\n",
    "print 'y shape {}'.format(y.shape)\n",
    "kf = exp_setup['kf']\n",
    "exp_name = exp_setup['exp_name']\n",
    "\n",
    "#Some paths to store models and performance stats\n",
    "CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/output/'\n",
    "saved_model_name =''\n",
    "save_model_path = CV_model_dir + exp_name + '_' + model_choice\n",
    "load_model_path = CV_model_dir + saved_model_name\n",
    "\n",
    "# train a new classifer? Or load a single pretrained classifier based on most frequent hyperparams found previously?\n",
    "# This will NOT load K different classifiers each for outer-CV fold  \n",
    "train_clf = True\n",
    "save_model = False #do you really want to save all classifiers per each fold? (default false) \n",
    "\n",
    "if train_clf:\n",
    "    # Create list of all the fold-subsets (needed for parallelization)\n",
    "    X_train = []\n",
    "    X_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []    \n",
    "    for train, valid in kf:        \n",
    "        X_train.append(X[train])\n",
    "        X_valid.append(X[valid])\n",
    "        y_train.append(y[train])\n",
    "        y_valid.append(y[valid])\n",
    "\n",
    "    CV_r_train=[] #pearson r score for each outer fold on train set\n",
    "    CV_r_valid=[] #pearson r score for each outer fold on validation set\n",
    "\n",
    "    CV_R2_train=[] #R2 score for each outer fold on train set\n",
    "    CV_R2_valid=[] #R2 score for each outer fold on validation set\n",
    "\n",
    "    CV_MSE_train=[] #MSE for each outer fold on train set\n",
    "    CV_MSE_valid=[] #MSE for each outer fold on validation set\n",
    "    \n",
    "    predicted_CV_scores = []\n",
    "    actual_CV_scores = []\n",
    "    \n",
    "    # Parallization configs for ipython notebook cluster    \n",
    "    rc = ipp.Client()\n",
    "    dview = rc[:]\n",
    "    dview.push(dict(computeOuterFold = computeOuterFold))\n",
    "    dview.push(dict(innerCVLoop = innerCVLoop))\n",
    "    mapfunc = partial(computeOuterFold, model_clf=model_clf, hyper_params=hyper_params, inner_loop=inner_loop, \n",
    "                  save_model=save_model, save_model_path=save_model_path)\n",
    "    parallel_result = dview.map_sync(mapfunc, X_train, y_train, X_valid, y_valid)    \n",
    "    \n",
    "    hp_dict = collections.defaultdict(list)\n",
    "    for pr in parallel_result:\n",
    "        CV_r_train.append(pr['r_train'])\n",
    "        CV_r_valid.append(pr['r_valid'])\n",
    "        CV_R2_train.append(pr['R2_train'])\n",
    "        CV_R2_valid.append(pr['R2_valid'])\n",
    "        CV_MSE_train.append(pr['MSE_train'])\n",
    "        CV_MSE_valid.append(pr['MSE_valid'])\n",
    "        predicted_CV_scores.append(pr['predicted_fold_score'])\n",
    "        actual_CV_scores.append(pr['actual_fold_scores'])\n",
    "        \n",
    "        for hp in hyper_params:\n",
    "            hp_dict[hp].append(pr['hp_dict'][hp])\n",
    "            \n",
    "    #Find out most frequent hyper-params during cross-val    \n",
    "    hp_mode = {}\n",
    "    for hp in hyper_params:\n",
    "        hp_mode[hp] = mode(hp_dict[hp])[0][0]\n",
    "\n",
    "    print 'most frequent hp:' + str(hp_mode)\n",
    "    \n",
    "else: \n",
    "    #Grabs the best classifer as a result of N-fold nested CV along with the MSE and R2 stats of the outerloop\n",
    "    print \"Loading previously saved model: \"\n",
    "    f = open(load_model_path)\n",
    "    result = pickle.load(f)\n",
    "    test_clf = result['best_clf']\n",
    "    CV_r_valid = result['CV_r']\n",
    "    CV_R2_valid = result['CV_R2']\n",
    "    CV_MSE_valid = result['CV_MSE']\n",
    "    f.close()\n",
    "\n",
    "#print 'CV r (mean, median, std_err): ' + '{:04.2f},{:04.2f},{:04.2f}'.format(np.mean(zip(*CV_r_valid)[0]),np.median(zip(*CV_r_valid)[0]),stats.sem(zip(*CV_r_valid)[0]))\n",
    "#print 'CV R2 (mean, median, std_err): ' + '{:04.2f},{:04.2f}, {:04.2f}'.format(np.mean(CV_R2_valid),np.median(CV_R2_valid),stats.sem(CV_R2_valid))\n",
    "print 'CV MSE (mean, median, std_err): ' + '{:04.2f},{:04.2f}, {:04.2f}'.format(np.mean(CV_MSE_valid),np.median(CV_MSE_valid),stats.sem(CV_MSE_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV R2 (mean, median, std_err): 0.00,0.00, 0.00\n"
     ]
    }
   ],
   "source": [
    "print 'CV R2 (mean, median, std_err): ' + '{:04.2f},{:04.2f}, {:04.2f}'.format(np.mean(CV_R2_valid),np.median(CV_R2_valid),stats.sem(CV_R2_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean r (ADAS, MMSE): 0.563720411063,0.553491570843\n",
      "stdev r (ADAS, MMSE): 0.0718909860065,0.00928711319124\n"
     ]
    }
   ],
   "source": [
    "print 'mean r (ADAS, MMSE): {},{}'.format(np.mean(zip(*zip(*CV_r_valid)[0])[0]), np.mean(zip(*zip(*CV_r_valid)[1])[0]))\n",
    "print 'stdev r (ADAS, MMSE): {},{}'.format(np.std(zip(*zip(*CV_r_valid)[0])[0]), np.std(zip(*zip(*CV_r_valid)[1])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute single model from the most frequent hyper-params (for across dataset testing)\n",
    "\n",
    "#test_clf = Lasso(alpha=0.1)    \n",
    "#test_clf = SVR(kernel='rbf',C=10)\n",
    "test_clf = RandomForestRegressor(n_estimators=100,min_samples_split=8,n_jobs=4)\n",
    "\n",
    "save_CV_perf = True    \n",
    "if save_CV_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    save_model_filename = save_model_path + '_' + modality + '_' + st + '.pkl'\n",
    "    classifier_model_and_stats = {'best_clf':test_clf, 'CV_R2':CV_R2_valid, 'CV_MSE':CV_MSE_valid, 'CV_r': CV_r_valid,\n",
    "                                 'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    pickleIt(classifier_model_and_stats,save_model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting stuff...\n",
    "boxplot_config_r = {}\n",
    "boxplot_config_R2 = {}\n",
    "boxplot_config_MSE = {}\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/output/'\n",
    "scale = 'ADAS13'\n",
    "cohort = 'ADNI1'\n",
    "if scale == 'ADAS13':\n",
    "    if cohort == 'ADNI1':    \n",
    "        baseline_models = {'LR_HC*':'Exp6_ADNI1_ADAS13_LR_L1_HC_2016-04-21-15-09-33.pkl',\n",
    "                           'LR_CT*':'Exp6_ADNI1_ADAS13_LR_L1_CT_2016-04-21-15-13-04.pkl',\n",
    "                           'LR_HC_CT*':'Exp6_ADNI1_ADAS13_LR_L1_HC_CT_2016-04-21-15-04-00.pkl',\n",
    "                           'SVR_HC*':'Exp6_ADNI1_ADAS13_SVR_HC_2016-04-21-15-14-29.pkl',\n",
    "                           'SVR_CT*':'Exp6_ADNI1_ADAS13_SVR_CT_2016-04-21-15-17-53.pkl',\n",
    "                           'SVR_HC_CT*':'Exp6_ADNI1_ADAS13_SVR_HC_CT_2016-04-21-15-28-22.pkl', \n",
    "                           'RFR_HC':'Exp6_ADNI1_ADAS13_RFR_HC_2016-04-21-15-40-32.pkl',\n",
    "                           'RFR_CT':'Exp6_ADNI1_ADAS13_RFR_CT_2016-04-21-15-38-44.pkl',\n",
    "                           'RFR_HC_CT':'Exp6_ADNI1_ADAS13_RFR_HC_CT_2016-04-21-15-31-53.pkl'}\n",
    "        \n",
    "    if cohort == 'ADNI2':\n",
    "        baseline_models = {'LR_HC*':'Exp6_ADNI2_ADAS13_LR_L1_HC_2016-04-21-15-07-08.pkl',\n",
    "                           'LR_CT*':'Exp6_ADNI2_ADAS13_LR_L1_CT_2016-04-21-15-13-32.pkl',\n",
    "                           'LR_HC_CT*':'Exp6_ADNI2_ADAS13_LR_L1_HC_CT_2016-04-21-15-04-35.pkl',\n",
    "                           'SVR_HC*':'Exp6_ADNI2_ADAS13_SVR_HC_2016-04-21-15-14-13.pkl',\n",
    "                           'SVR_CT*':'Exp6_ADNI2_ADAS13_SVR_CT_2016-04-21-15-20-53.pkl',\n",
    "                           'SVR_HC_CT*':'Exp6_ADNI2_ADAS13_SVR_HC_CT_2016-04-21-15-26-27.pkl', \n",
    "                           'RFR_HC':'Exp6_ADNI2_ADAS13_RFR_HC_2016-04-21-15-42-00.pkl',\n",
    "                           'RFR_CT':'Exp6_ADNI2_ADAS13_RFR_CT_2016-04-21-15-36-31.pkl',\n",
    "                           'RFR_HC_CT':'Exp6_ADNI2_ADAS13_RFR_HC_CT_2016-04-21-15-34-16.pkl'}\n",
    "if scale == 'MMSE':\n",
    "    baseline_models = {'LR_L1*': 'LR_L1_MMSE_2015-12-29-16-41-41.pkl',\n",
    "                    'SVR*':'RFR_MMSE_2015-12-29-17-35-51.pkl', \n",
    "                    'RFR':'SVR_MMSE_2015-12-29-17-40-24.pkl'}\n",
    "\n",
    "# Not plotted:\n",
    "# 'LR_L1': 'LR_L1_ADAS13_2015-11-06-12-25-21.pkl'\n",
    "# 'LR_L1_infl':'LR_L1_ADAS13_inflated_train_parallel_2015-11-27-15-05-17.pkl'\n",
    "r_means =  {}  \n",
    "r_medians =  {}\n",
    "r_sems =  {}\n",
    "mse_means =  {}\n",
    "mse_medians =  {}\n",
    "mse_sems =  {}\n",
    "predicted_CV_scores = {}\n",
    "actual_CV_scores = {}\n",
    "keys_used = []\n",
    "for key,val in baseline_models.iteritems():\n",
    "    pkl_file = open(boxplots_dir + val, 'rb')\n",
    "    saved_data = pickle.load(pkl_file)\n",
    "    pkl_file.close()            \n",
    "    boxplot_config_r[key] = (zip(*saved_data['CV_r'])[0])\n",
    "    boxplot_config_MSE[key] = (saved_data['CV_MSE'])\n",
    "    print 'key: {}, mean: {}, median: {}, std: {}'.format(key,np.mean(zip(*saved_data['CV_r'])[0]),\n",
    "                                              np.median(zip(*saved_data['CV_r'])[0]),\n",
    "                                              np.std(zip(*saved_data['CV_r'])[0]))\n",
    "    r_means[key] = (np.mean(zip(*saved_data['CV_r'])[0]))\n",
    "    r_medians[key] = (np.median(zip(*saved_data['CV_r'])[0]))\n",
    "    r_sems[key] = (stats.sem(zip(*saved_data['CV_r'])[0]))        \n",
    "\n",
    "    mse_means[key] = (np.mean(saved_data['CV_MSE']))\n",
    "    mse_sems[key] = (stats.sem(saved_data['CV_MSE']))        \n",
    "    if key in ['LR_HC_CT*','SVR_HC_CT*','RFR_HC_CT']:\n",
    "        predicted_CV_scores[key] = (saved_data['predicted_CV_scores'])\n",
    "        actual_CV_scores[key] = (saved_data['actual_CV_scores'])\n",
    "        \n",
    "# Since ANN model perfs are saved differently :-/\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "\n",
    "if cohort == 'ADNI1':\n",
    "    ANN_models ={'CT':'Exp4_ADNI1_ADAS13_NN_CT_2016-03-03-21-54-55.pkl',\n",
    "                 'HC':'Exp4_ADNI1_ADAS13_NN_HC_2016-03-04-00-24-38.pkl',\n",
    "                 'HC_CT':'Exp4_ADNI1_ADAS13_NN_HC_CT_2016-03-03-14-34-34.pkl'}\n",
    "    \n",
    "if cohort == 'ADNI2':\n",
    "    ANN_models ={'CT':'Exp5_ADNI2_ADAS13_NN_CT_2016-04-06-15-27-44.pkl',\n",
    "                 'HC':'Exp5_ADNI2_ADAS13_NN_HC_2016-04-06-15-39-46.pkl',\n",
    "                 'HC_CT':'Exp5_ADNI2_ADAS13_NN_HC_CT_2016-04-07-10-44-33.pkl'}\n",
    "\n",
    "for key,val in ANN_models.iteritems():\n",
    "    pkl_file = open(boxplots_dir + val, 'rb')\n",
    "    saved_data = pickle.load(pkl_file)               \n",
    "    boxplot_config_r['ANN_'+key] = (saved_data['CV_r'])\n",
    "    boxplot_config_MSE['ANN_'+key] = (saved_data['CV_MSE'])\n",
    "    pkl_file.close()\n",
    "    print 'key: {}, mean: {}, median: {}, std: {}'.format(key,np.mean(saved_data['CV_r']),\n",
    "                                              np.median(saved_data['CV_r']),\n",
    "                                              np.std(saved_data['CV_r']))\n",
    "    r_means['ANN_'+key] = (np.mean(saved_data['CV_r']))\n",
    "    r_sems['ANN_'+key] = (stats.sem(saved_data['CV_r']))\n",
    "\n",
    "    mse_means['ANN_'+key] = (np.mean(saved_data['CV_MSE']))\n",
    "    mse_sems['ANN_'+key] = (stats.sem(saved_data['CV_MSE']))\n",
    "    if key == 'HC_CT':\n",
    "        predicted_CV_scores['ANN_' + key] = (saved_data['predicted_CV_scores'])\n",
    "        actual_CV_scores['ANN_' + key] = (saved_data['actual_CV_scores'])\n",
    "\n",
    "print zip(r_means, r_sems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot foldwise results\n",
    "# plt.rcParams['figure.figsize'] = (20, 10)\n",
    "# fids = np.arange(1,11,1)\n",
    "# print boxplot_config_r.keys()\n",
    "# for key in boxplot_config_r.keys():\n",
    "#     if key in ['ANN_CT','ANN_HC','ANN_HC_CT']:\n",
    "#         custom_marker = 'd'    \n",
    "#     else:\n",
    "#         custom_marker = 'o'\n",
    "#     plt.subplot(2,1,1)\n",
    "#     plt.plot(fids,boxplot_config_r[key][0],linestyle='None',marker=custom_marker,label=key)\n",
    "#     plt.legend(bbox_to_anchor=[0.5, -0.1], loc='center', ncol=6)    \n",
    "#     plt.xlim(0,11)\n",
    "\n",
    "#     plt.subplot(2,1,2)\n",
    "#     plt.plot(fids,boxplot_config_MSE[key][0],linestyle='None',marker=custom_marker,label=key)\n",
    "#     plt.legend(bbox_to_anchor=[0.5, -0.1], loc='center', ncol=6)\n",
    "#     plt.xlim(0,11)\n",
    "\n",
    "# modality = 'ANN_HC_CT'\n",
    "    \n",
    "# print 'corr: {}:{}'.format(modality,boxplot_config_r[modality][0][4:8])\n",
    "# print 'MSE: {}:{}'.format(modality,boxplot_config_MSE[modality][0][4:8])\n",
    "\n",
    "boxplot_config_r['LR_HC_CT*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Boxplots for CV statistics (r, mse, R2)\n",
    "stat_measure_list = [boxplot_config_r,boxplot_config_MSE] #boxplot_config_MSE\n",
    "stat_measure_names = ['Pearson r', 'RMSE'] #'MSE'\n",
    "from matplotlib.artist import setp\n",
    "font_small = 25\n",
    "font_med = 30\n",
    "font_large = 40\n",
    "plt.rcParams['figure.figsize'] = (40, 40)\n",
    "#custom_cols = ['LR_L1*','LR_L1_infl*', 'SVR*', 'SVR_infl*', 'RFR', 'RFR_infl','ANN_HC_CT']\n",
    "custom_cols = ['LR_HC*','LR_CT*','LR_HC_CT*','SVR_HC*','SVR_CT*','SVR_HC_CT*','RFR_HC','RFR_CT','RFR_HC_CT',\n",
    "               'ANN_HC','ANN_CT','ANN_HC_CT']\n",
    "boxplot = False\n",
    "errorbarplot = True\n",
    "scatterplot = True\n",
    "\n",
    "my_colors = ['steelblue', 'olivedrab', 'cadetblue', 'salmon']\n",
    "#plt.figure()\n",
    "ax1 = plt.subplot2grid((6,2), (0,0), colspan=2,rowspan=2)\n",
    "ax2 = plt.subplot2grid((6,2), (2,0), colspan=2,rowspan=2)\n",
    "ax3 = plt.subplot2grid((6,2), (4,0))\n",
    "ax4 = plt.subplot2grid((6,2), (4,1))\n",
    "ax5 = plt.subplot2grid((6,2), (5,0))\n",
    "ax6 = plt.subplot2grid((6,2), (5,1))\n",
    "\n",
    "ax_list = [ax1,ax2,ax3,ax4,ax5,ax6]\n",
    "\n",
    "if errorbarplot:\n",
    "    custom_r_means = []    \n",
    "    custom_r_sems = []    \n",
    "    custom_mse_means = []\n",
    "    custom_mse_sems = []\n",
    "    for c, col in enumerate(custom_cols):\n",
    "        custom_r_means.append(r_means[col])        \n",
    "        custom_r_sems.append(r_sems[col])\n",
    "        custom_mse_means.append(mse_means[col])        \n",
    "        custom_mse_sems.append(mse_sems[col])\n",
    "        \n",
    "    x_r = np.array([0,1,2,4,5,6,8,9,10,12,13,14])  # padding to better appearance \n",
    "    x_mse = x_r \n",
    "    #plt.bar(x, np.array(custom_r_means), yerr=np.array(custom_r_sems))\n",
    "    width = 0.25                    # bar width\n",
    "\n",
    "    #fig, ax = plt.subplots()\n",
    "    plot_corr = True\n",
    "    if plot_corr: \n",
    "\n",
    "        rects1 = ax1.bar(x_r[:3], custom_r_means[:3],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[0],        # bar colour\n",
    "                        yerr=custom_r_sems[:3],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4},       # error-bar width\n",
    "                        label = 'Linear Regression')\n",
    "\n",
    "        rects2 = ax1.bar(x_r[3:6], custom_r_means[3:6],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[1],        # bar colour\n",
    "                        yerr=custom_r_sems[3:6],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4},\n",
    "                        label = 'Support Vector Regression')\n",
    "\n",
    "        rects3 = ax1.bar(x_r[6:9], custom_r_means[6:9],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[2],        # bar colour\n",
    "                        yerr=custom_r_sems[6:9],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4},\n",
    "                         label = 'Random Forest')\n",
    "\n",
    "        rects4 = ax1.bar(x_r[9:], custom_r_means[9:],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[3],        # bar colour\n",
    "                        yerr=custom_r_sems[9:],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4},\n",
    "                        label = 'Artificial Neural Network')\n",
    "        autolabel(rects1,ax1)\n",
    "        autolabel(rects2,ax1)\n",
    "        autolabel(rects3,ax1)\n",
    "        autolabel(rects4,ax1)\n",
    "        #Plot annotations \n",
    "        ax1.set_ylabel('Correlation',fontsize=font_large)\n",
    "        ax1.set_xlim(-0.5,max(x_r)+1)\n",
    "        ax1.set_ylim(0,1)\n",
    "        ax1.set_xticks(x_r)\n",
    "        ax1.set_xticklabels(custom_cols)\n",
    "        ax1.tick_params(labelsize=font_small)\n",
    "        #ax1.set_yticks(fontsize=font_med)\n",
    "        ax1.legend(fontsize=font_med, loc=2)\n",
    "    \n",
    "    plot_mse = True\n",
    "    if plot_mse: \n",
    "        rects5 = ax2.bar(x_mse[:3], custom_mse_means[:3],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[0],        # bar colour\n",
    "                        yerr=custom_mse_sems[:3],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4})\n",
    "\n",
    "        rects6 = ax2.bar(x_mse[3:6], custom_mse_means[3:6],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[1],        # bar colour\n",
    "                        yerr=custom_mse_sems[3:6],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4})\n",
    "\n",
    "        rects7 = ax2.bar(x_mse[6:9], custom_mse_means[6:9],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[2],        # bar colour\n",
    "                        yerr=custom_mse_sems[6:9],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4})\n",
    "\n",
    "        rects8 = ax2.bar(x_mse[9:], custom_mse_means[9:],                  # data\n",
    "                        width,                          # bar width\n",
    "                        color=my_colors[3],        # bar colour\n",
    "                        yerr=custom_mse_sems[9:],               # data for error bars\n",
    "                        error_kw={'ecolor':'darkslategrey',    # error-bars colour\n",
    "                                 'linewidth':4})\n",
    "        autolabel(rects5,ax2)\n",
    "        autolabel(rects6,ax2)\n",
    "        autolabel(rects7,ax2)\n",
    "        autolabel(rects8,ax2)\n",
    "\n",
    "        #Plot annotations \n",
    "        ax2.set_ylabel('MSE',fontsize=font_large)\n",
    "        ax2.set_xlim(-0.5,max(x_r)+1)\n",
    "        #ax2.set_ylim(0,100)\n",
    "        ax2.set_xticks(x_r)\n",
    "        ax2.set_xticklabels(custom_cols)\n",
    "        ax2.tick_params(labelsize=font_small)\n",
    "        #ax1.set_yticks(fontsize=font_med)\n",
    "        ax2.legend(fontsize=font_med, loc=2)\n",
    "    \n",
    "if scatterplot:\n",
    "    #plt.figure()\n",
    "    \n",
    "    for k, key in enumerate(['LR_HC_CT*','SVR_HC_CT*', 'RFR_HC_CT', 'ANN_HC_CT']):\n",
    "        x = np.hstack(actual_CV_scores[key])\n",
    "        y = np.hstack(predicted_CV_scores[key])\n",
    "        ax_offset = 2\n",
    "        #plt.subplot(2,2,k+1)        \n",
    "        ax_list[k+ax_offset].scatter(x, y, c=my_colors[k], s=40)\n",
    "        fit = np.polyfit(x,y,1)\n",
    "        fit_fn = np.poly1d(fit) \n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "        if p_value < 0.0001:\n",
    "            p_value_sig = '<0.0001'\n",
    "        else:\n",
    "            p_value_sig = str(p_value)\n",
    "\n",
    "        #Plot annotations\n",
    "        label_str = 'model: {}'.format(key) + '\\n' + 'r-value: {:04.2f}'.format(r_value) + '\\n' + 'p-value: ' + p_value_sig + '\\n' + 'std_err: {:04.2f}'.format(std_err) \n",
    "        # fit_fn is now a function which takes in x and returns an estimate for y\n",
    "        ax_list[k+ax_offset].plot(x, fit_fn(x),linewidth=3, c=my_colors[k], label=label_str)\n",
    "        #plt.title(model_choice,fontsize=font_large)\n",
    "        ax_list[k+ax_offset].set_ylim(0,40)\n",
    "        ax_list[k+ax_offset].set_xlabel('Actual Score',fontsize=font_large)\n",
    "        ax_list[k+ax_offset].set_ylabel('Predicted Score',fontsize=font_large)            \n",
    "        ax_list[k+ax_offset].legend(fontsize=font_small,loc=2)\n",
    "        ax_list[k+ax_offset].tick_params(labelsize=font_med)\n",
    "\n",
    "#Save figure\n",
    "save_fig = True\n",
    "if save_fig:\n",
    "    print 'Save image path: ' + '{}{}_{}_ADAS13_PerfPlots.png'.format(boxplots_dir, exp_name, cohort)\n",
    "    box_fig = plt.gcf()\n",
    "    box_fig.savefig('{}{}_{}_ADAS13_PerfPlots.png'.format(boxplots_dir, exp_name, cohort), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some Defs\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()\n",
    "\n",
    "#Outer Fold Computation (need the imports inside def if you want to parallelize!)\n",
    "def computeOuterFold(train_X, train_y, valid_X, valid_y, model_clf, hyper_params, inner_loop, save_model, save_model_path):\n",
    "    import collections\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import MultiTaskLasso\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn import grid_search\n",
    "    import datetime\n",
    "    import time\n",
    "    import collections\n",
    "    from scipy.stats import mode\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from scipy import stats\n",
    "    \n",
    "    print 'Starting Outerfold computation'\n",
    "    \n",
    "    hp_dict = collections.defaultdict(list) #store best hyper-parameters for each fold\n",
    "    \n",
    "    if inner_loop:     \n",
    "        print 'Starting InnerFold computation'\n",
    "        save_model_path_fold = save_model_path + '_fold_' \n",
    "        clf = innerCVLoop(model_clf,hyper_params,train_X, train_y,save_model,save_model_path_fold)\n",
    "        for hp in hyper_params:\n",
    "            hp_dict[hp].append(clf.best_estimator_.get_params()[hp])\n",
    "            \n",
    "        print 'Ending InnerFold computation'\n",
    "\n",
    "    else:\n",
    "        clf = model_clf\n",
    "        clf.fit(fold_X,fold_y)\n",
    "        \n",
    "    #CV_scores    \n",
    "    r_train = (stats.pearsonr(clf.predict(train_X)[:,0],train_y[:,0]),stats.pearsonr(clf.predict(train_X)[:,1],train_y[:,1]))\n",
    "    r_valid = (stats.pearsonr(clf.predict(valid_X)[:,0],valid_y[:,0]),stats.pearsonr(clf.predict(train_X)[:,1],train_y[:,1]))\n",
    "        \n",
    "    R2_train = 0 #clf.score(train_X,train_y) \n",
    "    R2_valid = 0 #clf.score(valid_X,valid_y)\n",
    "        \n",
    "    MSE_train = mse(clf.predict(train_X),train_y)\n",
    "    MSE_valid = mse(clf.predict(valid_X),valid_y)\n",
    "    \n",
    "    print 'Ending OuterFold computation'\n",
    "    \n",
    "    return {'r_train':r_train, 'r_valid':r_valid, 'R2_train':R2_train, 'R2_valid':R2_valid,\n",
    "            'MSE_train':MSE_train, 'MSE_valid':MSE_valid, 'hp_dict':hp_dict, \n",
    "            'predicted_fold_score': clf.predict(valid_X), 'actual_fold_scores':valid_y}\n",
    "\n",
    "#Inner Fold Computation (need the imports inside def if you want to parallelize!)\n",
    "def innerCVLoop(model_clf,hyper_params,fold_X, fold_y,save_model,save_model_path):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import MultiTaskLasso\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn import grid_search\n",
    "    clf = grid_search.GridSearchCV(model_clf, hyper_params,cv=3,verbose=0)\n",
    "    clf.fit(fold_X, fold_y)\n",
    "    #Save classifier\n",
    "    if save_model:\n",
    "        save_model(clf,save_model_path)\n",
    "        \n",
    "    return clf\n",
    "\n",
    "def autolabel(rects,_ax):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        _ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                '{:03.2f}'.format(height),\n",
    "                ha='center',            # vertical alignment\n",
    "                va='bottom',             # horizontal alignment\n",
    "                fontsize = 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = [x for x in y_dx if x == 'No_DX']\n",
    "print matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CS_vals = sub_CS_dict_clean.values()\n",
    "plt.hist(CS_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohort = 'ADNI2'\n",
    "exp_setup_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/CV_Exp5_{}_ADAS13.pkl'.format(cohort)\n",
    "exp_setup = pickle.load( open(exp_setup_path, \"rb\" ) )\n",
    "kf = exp_setup['kf']\n",
    "adni2_sub_list = exp_setup['common_subs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_keys = list(set(adni1_sub_list) & set(adni2_sub_list))\n",
    "print len(adni1_sub_list), len(adni2_sub_list)\n",
    "print common_keys\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/output/'\n",
    "adni1_RFR_file = 'Exp4_ADNI1_ADAS13_RFR_HC_CT_2016-02-29-15-24-23.pkl'\n",
    "adni2_RFR_file = 'Exp5_ADNI2_ADAS13_RFR_HC_CT_2016-04-04-16-18-06.pkl'\n",
    "pkl_file = open(boxplots_dir + adni1_RFR_file, 'rb')\n",
    "saved_data = pickle.load(pkl_file)\n",
    "pkl_file.close()            \n",
    "\n",
    "adni1_act_scores = np.hstack(saved_data['actual_CV_scores'])\n",
    "adni1_prd_scores = np.hstack(saved_data['predicted_CV_scores'])\n",
    "\n",
    "pkl_file = open(boxplots_dir + adni2_RFR_file, 'rb')\n",
    "saved_data = pickle.load(pkl_file)\n",
    "pkl_file.close()            \n",
    "\n",
    "adni2_act_scores = np.hstack(saved_data['actual_CV_scores'])\n",
    "adni2_prd_scores = np.hstack(saved_data['predicted_CV_scores'])\n",
    "\n",
    "# print np.hstack(adni1_act_scores)\n",
    "plt.subplot(2,1,1)\n",
    "plt.scatter(adni1_act_scores,adni1_prd_scores,c='olivedrab')\n",
    "plt.title('ADNI1 Perf')\n",
    "plt.xlabel('Actual ADAS scores')\n",
    "plt.ylabel('Predicted ADAS scores')\n",
    "plt.subplot(2,1,2)\n",
    "plt.scatter(adni2_act_scores,adni2_prd_scores,c='olivedrab')\n",
    "plt.xlabel('Actual ADAS scores')\n",
    "plt.ylabel('Predicted ADAS scores')\n",
    "plt.title('ADNI2 Perf')\n",
    "\n",
    "box_fig = plt.gcf()\n",
    "box_fig.savefig('/projects/nikhil/ADNI_prediction/input_datasets/CS/adas_correlations.png', bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "node_sizes1 = {'L_ff1':50,'R_ff1':50,'L_ff2':50,'R_ff2':50,'ff1':100,'ff2':100,'ff3':20}\n",
    "node_sizes2 = {'L_ff1':50,'R_ff1':50,'L_ff2':50,'R_ff2':50,'ff1':100,'ff2':100,'ff3':40}\n",
    "dr1 = {'HC':0.25,'CT':0.25}\n",
    "dr2 = {'HC':0.5,'CT':0.5}\n",
    "\n",
    "list1 = ['node_sizes1','node_sizes2']\n",
    "list2 = ['dr1','dr2']\n",
    "\n",
    "s=[list1,list2 ]\n",
    "hype_combs = list(itertools.product(*s))\n",
    "hype_configs = {}\n",
    "for c, comb in enumerate(hype_combs):\n",
    "    hype_configs['hyp{}'.format(c)] = {'node_sizes':comb}\n",
    "    \n",
    "    hype_configs = {'hyp1':{'node_sizes':{'L_ff1':50,'R_ff1':50,'L_ff2':50,'R_ff2':50,'ff1':100,'ff2':100,'ff3':20},\n",
    "                       'dr':{'HC':0.25,'CT':0.25}},\n",
    "                'hyp2':{'node_sizes':{'L_ff1':50,'R_ff1':50,'L_ff2':50,'R_ff2':50,'ff1':100,'ff2':100,'ff3':40},\n",
    "                       'dr':{'HC':0.5,'CT':0.5}}\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.title('HC_L total volume')\n",
    "plt.hist(X[y_dx_int==0,0],bins=100,alpha=alpha,label='AD: {}'.format(np.mean(X[y_dx_int==0,0])))\n",
    "plt.hist(X[y_dx_int==1,0],bins=100,alpha=alpha,label='CN: {}'.format(np.mean(X[y_dx_int==1,0])))\n",
    "plt.hist(X[y_dx_int==2,0],bins=100,alpha=alpha,label='MCI: {}'.format(np.mean(X[y_dx_int==2,0])))\n",
    "plt.legend()\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('HC_R total volume')\n",
    "plt.hist(X[y_dx_int==0,1],bins=100,alpha=alpha,label='AD: {}'.format(np.mean(X[y_dx_int==0,1])))\n",
    "plt.hist(X[y_dx_int==1,1],bins=100,alpha=alpha,label='CN: {}'.format(np.mean(X[y_dx_int==1,1])))\n",
    "plt.hist(X[y_dx_int==2,1],bins=100,alpha=alpha,label='MCI: {}'.format(np.mean(X[y_dx_int==2,1])))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X[y_dx_int==0,1].shape\n",
    "print X[y_dx_int==1,1].shape\n",
    "print X[y_dx_int==2,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print int(np.mean(X[y_dx_int==0,0]))*unit_vol, int(np.mean(X[y_dx_int==0,1]))*unit_vol\n",
    "print int(np.mean(X[y_dx_int==1,0]))*unit_vol, int(np.mean(X[y_dx_int==1,1]))*unit_vol\n",
    "print int(np.mean(X[y_dx_int==2,0]))*unit_vol, int(np.mean(X[y_dx_int==2,1]))*unit_vol\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    unit_vol = 1.2*0.94*0.94 \n",
    "print unit_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.ones((3,2))\n",
    "b = np.ones((4,2))\n",
    "c = np.vstack((a,b))\n",
    "print c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
