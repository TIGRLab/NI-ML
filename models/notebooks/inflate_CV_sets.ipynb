{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "#from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import KFold\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import ipyparallel as ipp\n",
    "from functools import partial\n",
    "import collections\n",
    "import tables as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasets\n",
    "\n",
    "#input data\n",
    "master_dataframe = '/projects/nikhil/ADNI_prediction/input_datasets/master_fused.pkl'\n",
    "train_val_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_train_plus_val.pkl'\n",
    "test_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_test.pkl'\n",
    "\n",
    "#Candidate label dictionaries\n",
    "sub_HC_vol_left_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC/subject_HC_vol_dictionary_train_val_left.pkl'\n",
    "sub_HC_vol_right_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC/subject_HC_vol_dictionary_train_val_right.pkl'\n",
    "\n",
    "sub_CT_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/subject_roi_ct_data/ADNI1_subject_ROI_CT_dict_JDV.pkl'\n",
    "#k-fold indices (from a saved file)\n",
    "#kf_file = \"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_train_valid_KFold_idx.pkl\"\n",
    "kf_file = \"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_train_valid_KFold_UIDs.pkl\"\n",
    "\n",
    "#save hdf_file for inflated sets\n",
    "out_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC_CT_fused_CV_subsets_JDV.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grab CV data with specific feature columes (independent vars) and specific clinical scale (dependent var)\n",
    "def load_CV_data(sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, in_file, clinical_scale, kf_file, hdf_file):    \n",
    "    # Grab subject_IDs from sub_HC / sub_CT dictionaries\n",
    "    # Grab clinical score for each subject from master_csv table (infile). \n",
    "    # Filter out NANs\n",
    "    # Loop through K-folds (kf_file) and append candidate labels + CT values\n",
    "    # Note: UID = PTID + IID (036_S_0976_I65091)\n",
    "    drawSamples = False  #Draw samples of point estimates\n",
    "    computeTrainingFolds = False\n",
    "    computeValidFolds = True\n",
    "    \n",
    "    csv_data = pd.read_pickle(in_file)\n",
    "    subject_PTIDs = csv_data.PTID\n",
    "    subject_IIDs = csv_data.IID\n",
    "    \n",
    "     # Pick any UID to generate roi_list common across all subjects to stay consistents while tranforming dictionary into array\n",
    "    ordered_roi_list = sub_CT_dict['40817'][0].keys()\n",
    "    \n",
    "    # ignore the \"0\" idx along with the 4 missing rois from the mean CT value csv\n",
    "    #ignore_roi_list = [0,29,30,39,40]\n",
    "    #for roi in ignore_roi_list:\n",
    "    #    ordered_roi_list.remove(roi)\n",
    "            \n",
    "    #clinical_scores = csv_data[csv_data.PTID.isin(subject_ids)][['UID',clinical_scale]]\n",
    "    clinical_scores = csv_data[['PTID','IID',clinical_scale]]\n",
    "    #print clinical_scores\n",
    "    #remove NANs        \n",
    "    clinical_scores = clinical_scores[np.isfinite(clinical_scores[clinical_scale])]    \n",
    "    clinical_scores['UID'] = clinical_scores['PTID'] + '_' + clinical_scores['IID']\n",
    "    #print clinical_scores\n",
    "    #print len(clinical_scores)    \n",
    "    \n",
    "    sub_clinical_scores_dict = dict(zip(clinical_scores['UID'],clinical_scores[clinical_scale]))    \n",
    "      \n",
    "    #print list(clinical_scores['UID'])\n",
    "    \n",
    "    # K-folds\n",
    "    kf = pickle.load( open(kf_file, \"rb\" ) )\n",
    "    train_fold_list= kf['train_UIDs']\n",
    "    val_fold_list= kf['valid_UIDs']\n",
    "    \n",
    "     \n",
    "    #for train, valid in kf:  \n",
    "    #    train_fold_list.append(train)\n",
    "        \n",
    "    X_train = []\n",
    "    X_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    runParallel = False\n",
    "    if runParallel: #parallelized version:        \n",
    "        rc = ipp.Client()\n",
    "        #rc.block = False\n",
    "        dview = rc[:]\n",
    "        print dview\n",
    "        dview.push(dict(inflate_fold = inflate_fold))        \n",
    "        dview.push(dict(inflate_subject_samples = inflate_subject_samples))        \n",
    "        mapfunc = partial(inflate_fold, sub_HC_L_dict=sub_HC_L_dict, sub_HC_R_dict=sub_HC_R_dict, sub_CT_dict=sub_CT_dict, \n",
    "                  ordered_roi_list=ordered_roi_list, sub_clinical_scores_dict=sub_clinical_scores_dict)\n",
    "\n",
    "        parallel_result = dview.map_sync(mapfunc, train_fold_list)  \n",
    "        return parallel_result\n",
    "    \n",
    "    else:\n",
    "        fold = 0 \n",
    "        uid_sampx_dict_list = []\n",
    "        uid_sampx_list_list = []\n",
    "        \n",
    "        if computeTrainingFolds: \n",
    "            for train in train_fold_list:      \n",
    "                fold+=1\n",
    "                print 'Staring fold # {}'.format(fold)\n",
    "                print 'Starting train subset'\n",
    "                uid_sampx_dict = collections.OrderedDict()\n",
    "                uid_sampx_list = collections.OrderedDict()\n",
    "                X_train_PE = []\n",
    "                y_train_PE = []\n",
    "                for t, tr in enumerate(train):                  \n",
    "                    uid = tr             \n",
    "                    #print uid\n",
    "                    result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict, drawSamples)\n",
    "                    sub_X = result['sub_X']\n",
    "                    sub_y = result['sub_y']\n",
    "                \n",
    "                    if drawSamples:\n",
    "                        if t == 0:                \n",
    "                            X_train_stack = sub_X\n",
    "                            y_train_stack = sub_y\n",
    "                        else:\n",
    "                            print X_train_stack, y_train_stack\n",
    "                            print sub_X, sub_y\n",
    "                            X_train_stack = np.vstack((X_train_stack,sub_X))\n",
    "                            y_train_stack = np.concatenate((y_train_stack,sub_y))\n",
    "                        \n",
    "                    else:\n",
    "                        X_train_PE.append(sub_X)\n",
    "                        y_train_PE.append(sub_y)\n",
    "                \n",
    "                    #uid_sampx_dict[uid] = len(sub_y)\n",
    "                    uid_sampx_dict[uid] = 1\n",
    "                \n",
    "                if not drawSamples:   \n",
    "                    X_train_stack  = np.squeeze(np.array(X_train_PE))\n",
    "                    y_train_stack  = np.array(y_train_PE)\n",
    "                \n",
    "                input_data = h5.File(hdf_file, 'a')\n",
    "                input_data.create_dataset('Fold_{}_train_X'.format(fold),data=X_train_stack)    \n",
    "                input_data.create_dataset('Fold_{}_train_y'.format(fold),data=y_train_stack)                    \n",
    "                input_data.close()\n",
    "                                \n",
    "                print 'Ending train subset'\n",
    "                uid_sampx_dict_list.append(uid_sampx_dict)\n",
    "                uid_sampx_list_list.append(uid_sampx_list)\n",
    "                \n",
    "        if computeValidFolds:    \n",
    "            #val_score_list = []\n",
    "            fold = 0 \n",
    "            for valid in val_fold_list:\n",
    "                fold+=1\n",
    "                #uid = val\n",
    "                #val_score_list.append(sub_clinical_scores_dict[uid])\n",
    "                #print val_score_list\n",
    "            \n",
    "                # This is optional - validation by default should be on \"fused features\"\n",
    "                print 'Starting valid subset'\n",
    "                X_valid_PE = []\n",
    "                y_valid_PE = []\n",
    "                for v, val in enumerate(valid):\n",
    "                    #print valid\n",
    "                    uid = val\n",
    "                    result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict, drawSamples)\n",
    "                    sub_X = result['sub_X']\n",
    "                    sub_y = result['sub_y']\n",
    "                \n",
    "                    if drawSamples:\n",
    "                        if v == 0:                \n",
    "                            X_valid_stack = sub_X\n",
    "                            y_valid_stack = sub_y\n",
    "                        else:\n",
    "                            X_valid_stack = np.vstack((X_valid_stack,sub_X))\n",
    "                            y_valid_stack = np.concatenate((y_valid_stack,sub_y))     \n",
    "                    else:\n",
    "                        X_valid_PE.append(sub_X)\n",
    "                        y_valid_PE.append(sub_y)\n",
    "        \n",
    "                print 'Ending valid subset' \n",
    "                if not drawSamples:   \n",
    "                    X_valid_stack  = np.squeeze(np.array(X_valid_PE))\n",
    "                    y_valid_stack  = np.array(y_valid_PE)\n",
    "                \n",
    "                input_data = h5.File(hdf_file, 'a')            \n",
    "                input_data.create_dataset('Fold_{}_valid_X'.format(fold),data=X_valid_stack)    \n",
    "                input_data.create_dataset('Fold_{}_valid_y'.format(fold),data=y_valid_stack)    \n",
    "                input_data.close()\n",
    "            \n",
    "        #Save uid--> sampx list of dictionaries per fold\n",
    "        f = open(hdf_file+'.pkl', 'wb')\n",
    "        pickle.dump(uid_sampx_dict_list, f)\n",
    "        f.close()\n",
    "        \n",
    "        print 'All folds done!'\n",
    "\n",
    "\n",
    "def inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_cScores_dict, drawSamples):\n",
    "    import numpy as np\n",
    "    import h5py as h5    \n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    from sklearn.cross_validation import KFold\n",
    "    import pickle\n",
    "    import re\n",
    "    import random\n",
    "    import collections\n",
    "    #UID = PTID + IID (PTID:[HC_vols], IID:{ROI:CT})\n",
    "    uid = uid.strip()\n",
    "    #print 'uid: {}'.format(uid)\n",
    "    ptid_re = re.compile('\\d*(_S_)\\d*')\n",
    "    iid_re = re.compile('(?<=I)\\d*')\n",
    "    ptid = re.search(ptid_re, uid).group(0).strip()\n",
    "    iid = re.search(iid_re, uid).group(0).strip()\n",
    "    missing_data = False\n",
    "    #min_CT_sampx = 132\n",
    "    min_CT_sampx = 14\n",
    "    MC = False\n",
    "    \n",
    "    \n",
    "    if ptid in sub_HC_L_dict.keys():\n",
    "        sub_HC_L = np.asarray(sub_HC_L_dict[ptid][0])\n",
    "    else: \n",
    "        print \"missing HC_L entry for: {}\".format(uid) \n",
    "        missing_data = True\n",
    "    \n",
    "    if ptid in sub_HC_R_dict.keys():\n",
    "        sub_HC_R = np.asarray(sub_HC_R_dict[ptid][0])\n",
    "    else: \n",
    "        print \"missing HC_R entry for: {}\".format(uid)\n",
    "        missing_data = True\n",
    "        \n",
    "    if iid in sub_CT_dict.keys():\n",
    "        sub_CT_all_rois = sub_CT_dict[iid][0]              \n",
    "    else: \n",
    "        print \"missing CT entry for: {}\".format(uid) \n",
    "        missing_data = True\n",
    "        \n",
    "    if not missing_data:\n",
    "        sub_CScore = sub_cScores_dict[uid]\n",
    "        sub_CT_sampx_dict = collections.OrderedDict()\n",
    "        if drawSamples:                    \n",
    "            min_sampx = np.min([len(sub_HC_L),len(sub_HC_R),min_CT_sampx])\n",
    "            \n",
    "            #select samples \n",
    "            sub_HC_L_sampx = random.sample(sub_HC_L, min_sampx)\n",
    "            sub_HC_R_sampx = random.sample(sub_HC_R, min_sampx)\n",
    "        \n",
    "            #Draw equal number of samples per roi\n",
    "            \n",
    "            for roi in ordered_roi_list:\n",
    "                #print roi\n",
    "                if roi not in sub_CT_sampx_dict:\n",
    "                    sub_CT_sampx_dict[roi]=[]\n",
    "                \n",
    "                sub_CT_roi = np.squeeze(sub_CT_all_rois[roi])\n",
    "                #print sub_CT_roi.shape\n",
    "                if len(sub_CT_roi) >= min_CT_sampx:\n",
    "                    #Do you want averaged out samples or true thickness samples\n",
    "                    if MC: \n",
    "                        MC_mult = int(0.5*len(sub_CT_roi)) #Pool for averaged out samples \n",
    "                        CT_MC_sampx = []\n",
    "                        for i in np.arange(min_sampx): #Generate CT samples \n",
    "                            CT_MC_sampx.append(np.mean(random.sample(sub_CT_roi, MC_mult))) #Average out individual samples\n",
    "\n",
    "                        sub_CT_sampx_dict[roi].append(CT_MC_sampx)      \n",
    "                    else:\n",
    "                        # Draw true samples\n",
    "                        sub_CT_sampx_dict[roi].append(random.sample(sub_CT_roi, min_sampx))  \n",
    "                        #sub_CT_sampx_dict[roi].append(np.mean(sub_CT_roi))\n",
    "                else:\n",
    "                    print \"Wrong value for the min_CT_sampx\"\n",
    "                    \n",
    "            #Clinical Score            \n",
    "            sub_y = np.tile(sub_CScore, min_sampx)\n",
    "        \n",
    "        # Or just collect point esimates (fused labels + mean thickness values)\n",
    "        else:\n",
    "            #select point-estimates\n",
    "            min_sampx = 1\n",
    "            sub_HC_L_sampx = stats.mode(sub_HC_L)[0]\n",
    "            sub_HC_R_sampx = stats.mode(sub_HC_R)[0]\n",
    "            \n",
    "            for roi in ordered_roi_list:                \n",
    "                sub_CT_roi = np.squeeze(sub_CT_all_rois[roi])                \n",
    "                sub_CT_sampx_dict[roi] = np.mean(sub_CT_roi)\n",
    "            #Clinical Score            \n",
    "            sub_y = sub_CScore\n",
    "            \n",
    "            \n",
    "        # Convert samples or a mean vector to a numpy array   \n",
    "        sub_CT_sampx = np.zeros((min_sampx, len(ordered_roi_list)))\n",
    "        for col, roi in enumerate(ordered_roi_list):\n",
    "            sub_CT_sampx[:,col] = np.asarray(sub_CT_sampx_dict[roi],dtype=float)\n",
    "        \n",
    "        sub_X = np.hstack((sub_HC_L_sampx,sub_HC_R_sampx,sub_CT_sampx))\n",
    "        \n",
    "    else:\n",
    "        sub_X = []\n",
    "        sub_y = []\n",
    "    \n",
    "    return {'sub_X': sub_X, 'sub_y':sub_y}\n",
    "\n",
    "#If you want only inflate training subset (this is used for parallel implementation)\n",
    "\n",
    "def inflate_fold(train,sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict):\n",
    "    import numpy as np\n",
    "    import h5py as h5    \n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    from sklearn.cross_validation import KFold\n",
    "    import pickle\n",
    "    import re\n",
    "    import random\n",
    "    import collections\n",
    "    print 'Starting train subset'\n",
    "    for t, tr in enumerate(train):            \n",
    "        uid = subject_uids[t]\n",
    "        result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict)\n",
    "        sub_X = result['sub_X']\n",
    "        sub_y = result['sub_y']\n",
    "        if t == 0:                \n",
    "            X_train_stack = sub_X\n",
    "            y_train_stack = sub_y\n",
    "        else:\n",
    "            X_train_stack = np.vstack((X_train_stack,sub_X))\n",
    "            y_train_stack = np.concatenate((y_train_stack,sub_y))\n",
    "        \n",
    "    print 'Ending train subset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = np.random.randn(100)\n",
    "print np.mean(tmp)\n",
    "sampx1=[]\n",
    "sampx2=[]\n",
    "for i in np.arange(2):\n",
    "    sampx1.append(np.mean(random.sample(tmp, 75)))\n",
    "\n",
    "for i in np.arange(50):\n",
    "    sampx2.append(np.mean(random.sample(tmp, 2)))\n",
    "\n",
    "print (np.array(sampx1))\n",
    "print np.mean(np.array(sampx2))\n",
    "\n",
    "int(10.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#kf = pickle.load( open(kf_file, \"rb\" ) )\n",
    "#master_csv = pickle.load( open(master_dataframe, \"rb\" ) )\n",
    "#train_val_data = pickle.load( open(train_val_file, \"rb\" ) )\n",
    "#test_val_data = pickle.load( open(test_file, \"rb\" ) )\n",
    "\n",
    "sub_HC_L_dict = pickle.load( open(sub_HC_vol_left_file, \"rb\" ) )\n",
    "sub_HC_R_dict = pickle.load( open(sub_HC_vol_right_file, \"rb\" ) )\n",
    "\n",
    "sub_CT_dict = pickle.load( open(sub_CT_file, \"rb\" ) )\n",
    "#train_val_data = pickle.load( open(train_val_file, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ordered_roi_list = sub_CT_dict['40817'][0].keys()\n",
    "ignore_roi_list = [0,29,30,39,40]\n",
    "for roi in ignore_roi_list:\n",
    "    ordered_roi_list.remove(roi)\n",
    "    \n",
    "print ordered_roi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "All folds done!\n"
     ]
    }
   ],
   "source": [
    "clinical_scale = 'ADAS13'\n",
    "CV_inflated_data = load_CV_data(sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, train_val_file, clinical_scale, kf_file, out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['002_S_0729_I118682', '002_S_0955_I118689', '005_S_0221_I72128', '005_S_0610_I32667', '005_S_0814_I74591', '007_S_1339_I56319', '012_S_0803_I118716', '014_S_0519_I39647', '014_S_0520_I39660', '021_S_0642_I33452', '022_S_0096_I59456', '022_S_0750_I59552', '027_S_1385_I47574', '029_S_0845_I64867', '031_S_0321_I65383', '031_S_1209_I67441', '032_S_0677_I119102', '032_S_0718_I119105', '033_S_0567_I119126', '033_S_0724_I119128', '036_S_0760_I38652', '037_S_0182_I65134', '062_S_0535_I50426', '062_S_0768_I50506', '067_S_0059_I119188', '067_S_0076_I119190', '067_S_0110_I119194', '067_S_0336_I119202', '068_S_0473_I140334', '072_S_1380_I119226', '073_S_0909_I119235', '094_S_1015_I40763', '098_S_0160_I65739', '098_S_0172_I65757', '098_S_0288_I323256', '098_S_0884_I56026', '099_S_0040_I34607', '100_S_0006_I33025', '100_S_0035_I33074', '100_S_0069_I33105', '100_S_0892_I66120', '100_S_1286_I66144', '116_S_0370_I59777', '116_S_1249_I59647', '128_S_0528_I119401', '128_S_0740_I119406', '129_S_1246_I69733', '132_S_0339_I94847', '133_S_1031_I119640', '136_S_0107_I119701', '136_S_1227_I63838', '137_S_0366_I46608', '137_S_1041_I43071', '941_S_1197_I66462', '062_S_1182_I50566', '131_S_0384_I48002', '131_S_0436_I48020', '133_S_0629_I119520', '941_S_1311_I97327']\n"
     ]
    }
   ],
   "source": [
    "kf = pickle.load( open(kf_file, \"rb\" ) )\n",
    "train_fold_list= kf['train_UIDs']\n",
    "val_fold_list= kf['valid_UIDs']\n",
    "\n",
    "print val_fold_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(sub_HC_L_dict.keys()), len(sub_HC_R_dict.keys()), len(sub_CT_dict)\n",
    "clinical_scale = 'ADAS13'\n",
    "subject_ids = sub_HC_L_dict.keys()\n",
    "csv_data = pd.read_pickle(master_dataframe)\n",
    "clinical_scores = csv_data[csv_data.PTID.isin(subject_ids)][['UID',clinical_scale]]\n",
    "#remove NANs\n",
    "clinical_scores = clinical_scores[np.isfinite(clinical_scores[clinical_scale])]\n",
    "sub_clinical_scores_dict = dict(zip(clinical_scores['UID'],clinical_scores[clinical_scale]))  \n",
    "subject_uids = sub_clinical_scores_dict.keys()\n",
    "\n",
    "print len(subject_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_id_list = list(train_val_data.IID)\n",
    "iid_re = re.compile('(?<=I)\\d*')    \n",
    "ordered_roi_list = sub_CT_dict['40817'][0].keys()\n",
    "sub_roi_mat_mean = np.zeros((len(sub_id_list), len(ordered_roi_list)))\n",
    "sub_roi_mat_std = np.zeros((len(sub_id_list), len(ordered_roi_list)))\n",
    "sub_roi_mat_sampx = np.zeros((len(sub_id_list), len(ordered_roi_list)))\n",
    "\n",
    "for s,sub in enumerate(sub_id_list):\n",
    "    iid = re.search(iid_re, sub).group(0).strip()\n",
    "    for r,roi in enumerate(ordered_roi_list):        \n",
    "        sub_roi_vals = np.squeeze(sub_CT_dict[iid][0][roi])\n",
    "        sub_roi_mat_mean[s,r] = np.mean(sub_roi_vals)\n",
    "        sub_roi_mat_std[s,r] = np.std(sub_roi_vals)\n",
    "        sub_roi_mat_sampx[s,r] = random.sample(sub_roi_vals, 1)[0]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for r,roi in enumerate(ordered_roi_list):      \n",
    "    plt.subplot(8,10,r+1)\n",
    "    plt.hist(sub_roi_mat_sampx[:,r]-sub_roi_mat[:,r])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateInnerFold(OF_id, fold_name_prefix, out_file_prefix, in_file, n_innerFolds):\n",
    "    import numpy as np\n",
    "    import h5py as h5    \n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    from sklearn.cross_validation import KFold\n",
    "    import pickle\n",
    "    import re\n",
    "    import random\n",
    "    import collections\n",
    "    # Load data\n",
    "    X_name = fold_name_prefix + '_X'\n",
    "    y_name = fold_name_prefix + '_y'\n",
    "    input_data = h5.File(in_file, 'r')\n",
    "    X_raw = input_data[X_name][:]\n",
    "    y = input_data[y_name][:]\n",
    "    input_data.close()\n",
    "\n",
    "    print X_raw.shape\n",
    "    #Remove ROIs from ignore list\n",
    "    L_HC_offset = 11427\n",
    "    R_HC_offset = 10519\n",
    "        \n",
    "    ignore_list_CT_idx = list(L_HC_offset + R_HC_offset + np.array([0,29,30,37,38]))\n",
    "    #X = np.delete(X_raw, np.s_[ignore_list_CT_idx], 1)\n",
    "    X = X_raw\n",
    "    print X.shape\n",
    "\n",
    "    #Sample / Shuffle Data\n",
    "    sampx = len(y)\n",
    "    kf = KFold(sampx, n_folds=n_innerFolds,shuffle=True)\n",
    "\n",
    "    #Save Data\n",
    "    split_HC_CT = True #Split the HC and CT data layer to allow partitioned model\n",
    "    k=0\n",
    "    for train, valid in kf:\n",
    "        k+=1\n",
    "        out_file_train = out_file_prefix + 'train_InnerFold_{}_partition_ROI_74.h5'.format(k)\n",
    "        out_file_valid = out_file_prefix + 'valid_InnerFold_{}_partition_ROI_74.h5'.format(k)\n",
    "        #Save Train\n",
    "        output_data = h5.File(out_file_train, 'a')\n",
    "        X_all = X[train]\n",
    "        if split_HC_CT:\n",
    "            output_data.create_dataset('Fold_{}_X_L_HC'.format(OF_id),data=X_all[:,:L_HC_offset])\n",
    "            output_data.create_dataset('Fold_{}_X_R_HC'.format(OF_id),data=X_all[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "            output_data.create_dataset('Fold_{}_X_R_CT'.format(OF_id),data=X_all[:,L_HC_offset+R_HC_offset:])\n",
    "        else:\n",
    "            output_data.create_dataset('Fold_{}_X'.format(OF_id),data=X[train])\n",
    "\n",
    "        output_data.create_dataset('Fold_{}_y'.format(OF_id),data=y[train])\n",
    "        output_data.close()\n",
    "\n",
    "        # Save valid\n",
    "        output_data = h5.File(out_file_valid, 'a')\n",
    "        X_all = X[valid]\n",
    "        if split_HC_CT:\n",
    "            output_data.create_dataset('Fold_{}_X_L_HC'.format(OF_id),data=X_all[:,:L_HC_offset])\n",
    "            output_data.create_dataset('Fold_{}_X_R_HC'.format(OF_id),data=X_all[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "            output_data.create_dataset('Fold_{}_X_R_CT'.format(OF_id),data=X_all[:,L_HC_offset+R_HC_offset:])\n",
    "        else:\n",
    "            output_data.create_dataset('Fold_{}_X'.format(OF_id),data=X[valid])\n",
    "\n",
    "        output_data.create_dataset('Fold_{}_y'.format(OF_id),data=y[valid])\n",
    "        output_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirectView [0, 1, 2, 3, 4]>\n"
     ]
    }
   ],
   "source": [
    "outer_CV_fold_file = '/projects/nikhil/ADNI_prediction/input_datasets/inflated_datasets_jdv/HC_CT_inflated_CV_subsets_JDV.h5'\n",
    "single_CV_fold_dir = '/projects/nikhil/ADNI_prediction/input_datasets/inflated_datasets_jdv/'\n",
    "\n",
    "n_innerFolds = 5\n",
    "\n",
    "outer_folds = np.arange(1,11,1)\n",
    "fold_name_prefix_list = []\n",
    "single_CV_fold_file_list = []\n",
    "for of in outer_folds:\n",
    "    fold_name_prefix_list.append('Fold_{}_train'.format(str(of)))\n",
    "    single_CV_fold_file_list.append('{}HC_CT_inflated_CV_OuterFold_{}_'.format(single_CV_fold_dir,str(of)))\n",
    "    \n",
    "    #generateInnerFold(outer_CV_fold_file, str(of), fold_name_prefix, n_innerFolds, single_CV_fold_file)\n",
    "\n",
    "runParallel = True\n",
    "if runParallel: #parallelized version:        \n",
    "        rc = ipp.Client()\n",
    "        #rc.block = False\n",
    "        dview = rc[:]\n",
    "        print dview\n",
    "        dview.push(dict(generateInnerFold = generateInnerFold))                   \n",
    "        mapfunc = partial(generateInnerFold, in_file=outer_CV_fold_file, n_innerFolds=n_innerFolds)\n",
    "\n",
    "        parallel_result = dview.map_sync(mapfunc, outer_folds, fold_name_prefix_list, single_CV_fold_file_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this data for computing subject wise performance during outerloop cross-validation + held-out testset\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "in_file = 'HC_CT_fused_CV_subsets_JDV.h5'\n",
    "\n",
    "CV = True\n",
    "L_HC_offset=11427\n",
    "R_HC_offset=10519\n",
    "\n",
    "subset = 'valid'\n",
    "CV_partition_file = 'HC_CT_fused_CV_OuterFolds_{}_partition.h5'.format(subset)\n",
    "if CV:\n",
    "    CV_fused_data = h5.File(baseline_dir + in_file,'a')\n",
    "    for f in np.arange(10):\n",
    "        X = CV_fused_data['Fold_{}_{}_X'.format(f+1,subset)]\n",
    "        y = CV_fused_data['Fold_{}_{}_y'.format(f+1,subset)]\n",
    "        CV_fused_data.create_dataset('Fold_{}_X_L_HC'.format(f+1),data=X[:,:L_HC_offset])\n",
    "        CV_fused_data.create_dataset('Fold_{}_X_R_HC'.format(f+1),data=X[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "        CV_fused_data.create_dataset('Fold_{}_X_R_CT'.format(f+1),data=X[:,L_HC_offset+R_HC_offset:])#Typo : R_CT\n",
    "        CV_fused_data.create_dataset('Fold_{}_y'.format(f+1),data=y)\n",
    "    \n",
    "    CV_fused_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Restructure OuterFold dataset h5 into two separate train and valid h5 files with identical dataset name\n",
    "single_CV_fold_dir = '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "\n",
    "out_file_train = single_CV_fold_dir + 'HC_CT_inflated_CV_OuterFolds_train.h5'\n",
    "out_file_valid = single_CV_fold_dir + 'HC_CT_inflated_CV_OuterFolds_valid.h5'\n",
    "\n",
    "input_data = h5.File(single_CV_fold_dir + 'HC_CT_inflated_CV_subsets.h5', 'r')\n",
    "out_data_train = h5.File(out_file_train, 'a')\n",
    "out_data_valid = h5.File(out_file_valid, 'a')\n",
    "                         \n",
    "for OF_id in np.arange(1,11,1):\n",
    "    dataset_train_X = 'Fold_{}_train_X'.format(OF_id)\n",
    "    dataset_train_y = 'Fold_{}_train_y'.format(OF_id)\n",
    "    dataset_valid_X = 'Fold_{}_valid_X'.format(OF_id)\n",
    "    dataset_valid_y = 'Fold_{}_valid_y'.format(OF_id)\n",
    "                     \n",
    "    data_train_X = input_data[dataset_train_X][:]\n",
    "    data_train_y = input_data[dataset_train_y][:]\n",
    "    data_valid_X = input_data[dataset_valid_X][:]\n",
    "    data_valid_y = input_data[dataset_valid_y][:]\n",
    "                \n",
    "    out_data_train.create_dataset('Fold_{}_X'.format(OF_id),data=data_train_X)    \n",
    "    out_data_train.create_dataset('Fold_{}_y'.format(OF_id),data=data_train_y)    \n",
    "    out_data_valid.create_dataset('Fold_{}_X'.format(OF_id),data=data_valid_X)    \n",
    "    out_data_valid.create_dataset('Fold_{}_y'.format(OF_id),data=data_valid_y)    \n",
    "    \n",
    "input_data.close()\n",
    "out_data_train.close()\n",
    "out_data_valid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_CT_dict = pickle.load( open(sub_CT_file, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate map between AAL atlas ROI : index based on\n",
    "AAL_roi_map_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL_ROI_IDx'\n",
    "AAL_roi_Name_featID_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL_ROI_Name_featIDx.pkl'\n",
    "ordered_roi_list = sub_CT_dict['40817'][0].keys()\n",
    "\n",
    "\n",
    "ignore_roi_list = [0,29,30,39,40]\n",
    "for roi in ignore_roi_list:\n",
    "    ordered_roi_list.remove(roi)\n",
    "\n",
    "ordered_roi_idx_dict = {}\n",
    "for i,idx in enumerate(ordered_roi_list):\n",
    "    ordered_roi_idx_dict[idx]=i\n",
    "\n",
    "print ordered_roi_idx_dict\n",
    "#data = pd.read_csv(AAL_roi_map_file,delim_whitespace=True)\n",
    "print data.columns\n",
    "\n",
    "data['feature_id'] = data['Ind'].map(ordered_roi_idx_dict) \n",
    "print data\n",
    "#data = data[~np.isnan(data['feature_id'])]\n",
    "#print data\n",
    "\n",
    "roi_name_featIDx_Dict = data.set_index('feature_id')['Name'].to_dict()\n",
    "od = collections.OrderedDict(sorted(roi_name_featIDx_Dict.items())) #order by feature colume index\n",
    "print od\n",
    "\n",
    "#f = open(AAL_roi_Name_featID_file, 'wb')\n",
    "#pickle.dump(od, f)\n",
    "#f.close()\n",
    "\n",
    "#print 'total number of ROIs {}'.format(len(ordered_roi_list))\n",
    "#ignore_roi_list = [0,29,30,39,40]\n",
    "\n",
    "#for roi in ignore_roi_list:\n",
    "#    print 'Ignore ROI index {}'.format(ordered_roi_list.index(roi))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print arr.shape\n",
    "l = [1,2]\n",
    "arr_trunc = np.delete(arr, np.s_[l], 1)\n",
    "print arr_trunc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create inflated HC total vol dataset (no voxel wise features)\n",
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "\n",
    "baseline_dir= '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "in_file = 'HC_CT_inflated_CV_subsets_MC.h5'\n",
    "out_file = 'total_HC_vol_CT_inflated_CV_subsets_ROI_74_MC.h5'\n",
    "\n",
    "L_HC_offset=11427\n",
    "R_HC_offset=10519\n",
    "\n",
    "ignore_cols = False\n",
    "for lid in np.arange(1,11,1):\n",
    "    print 'Starting Fold {}'.format(lid)\n",
    "    out_train_X_raw = load_data(baseline_dir + in_file,'Fold_{}_train_X'.format(lid))\n",
    "    out_train_y = load_data(baseline_dir + in_file,'Fold_{}_train_y'.format(lid))\n",
    "\n",
    "    #out_valid_X_raw = load_data(baseline_dir + in_file,'Fold_{}_valid_X'.format(lid))\n",
    "    #out_valid_y = load_data(baseline_dir + in_file,'Fold_{}_valid_y'.format(lid))\n",
    "\n",
    "    #if you want to remove some CT columes (74 connundrum)\n",
    "    if ignore_cols:\n",
    "        ignore_list_CT_idx = list(L_HC_offset + R_HC_offset + np.array([0,29,30,37,38]))\n",
    "        out_train_X = np.delete(out_train_X_raw, np.s_[ignore_list_CT_idx], 1)\n",
    "        #out_valid_X = np.delete(out_valid_X_raw, np.s_[ignore_list_CT_idx], 1)\n",
    "    else:\n",
    "        out_train_X = out_train_X_raw\n",
    "        #out_valid_X = out_valid_X_raw\n",
    "\n",
    "\n",
    "    out_data = h5.File(baseline_dir + out_file,'a')\n",
    "    #Train\n",
    "    out_data.create_dataset('Fold_{}_train_X_L_HC'.format(lid),data=np.sum(out_train_X[:,:L_HC_offset],axis=1))\n",
    "    out_data.create_dataset('Fold_{}_train_X_R_HC'.format(lid),data=np.sum(out_train_X[:,L_HC_offset:L_HC_offset+R_HC_offset],axis=1))\n",
    "    out_data.create_dataset('Fold_{}_train_X_CT'.format(lid),data=out_train_X[:,L_HC_offset+R_HC_offset:])\n",
    "    out_data.create_dataset('Fold_{}_train_y'.format(lid),data=out_train_y)\n",
    "    #Valid\n",
    "    #out_data.create_dataset('Fold_{}_valid_X_L_HC'.format(lid),data=np.sum(out_valid_X[:,:L_HC_offset],axis=1))\n",
    "    #out_data.create_dataset('Fold_{}_valid_X_R_HC'.format(lid),data=np.sum(out_valid_X[:,L_HC_offset:L_HC_offset+R_HC_offset],axis=1))\n",
    "    #out_data.create_dataset('Fold_{}_valid_X_CT'.format(lid),data=out_valid_X[:,L_HC_offset+R_HC_offset:])\n",
    "    #out_data.create_dataset('Fold_{}_valid_y'.format(lid),data=out_valid_y)\n",
    "    out_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
