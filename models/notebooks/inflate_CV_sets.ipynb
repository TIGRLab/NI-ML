{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "#from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import KFold\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasets\n",
    "\n",
    "#input data\n",
    "master_dataframe = '/projects/nikhil/ADNI_prediction/input_datasets/master_fused.pkl'\n",
    "train_val_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_train_plus_val.pkl'\n",
    "test_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_test.pkl'\n",
    "\n",
    "#Candidate label dictionaries\n",
    "sub_HC_vol_left_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC/subject_HC_vol_dictionary_train_val_left.pkl'\n",
    "sub_HC_vol_right_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC/subject_HC_vol_dictionary_train_val_right.pkl'\n",
    "sub_CT_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/subject_roi_ct_data/ADNI1_subject_ROI_CT_dict.pkl'\n",
    "#k-fold indices (from a saved file)\n",
    "kf_file = \"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_train_valid_KFold_idx.pkl\"\n",
    "\n",
    "#save hdf_file for inflated sets\n",
    "out_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC_CT_inflated_CV_subsets.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grab CV data with specific feature columes (independent vars) and specific clinical scale (dependent var)\n",
    "def load_CV_data(sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, in_file, clinical_scale, kf_file, hdf_file):    \n",
    "    # Grab subject_IDs from sub_HC / sub_CT dictionaries\n",
    "    # Grab clinical score for each subject from master_csv table (infile). \n",
    "    # Filter out NANs\n",
    "    # Loop through K-folds (kf_file) and append candidate labels + CT values\n",
    "    # Note: UID = PTID + IID (036_S_0976_I65091)\n",
    "    \n",
    "    subject_ids = sub_HC_L_dict.keys()\n",
    "     # Pick any UID to generate roi_list common across all subjects to stay consistents while tranforming dictionary into array\n",
    "    ordered_roi_list = sub_CT_dict['40817'][0].keys()\n",
    "    # ignore the \"0\" idx along with the 4 missing rois from the mean CT value csv\n",
    "    ignore_roi_list = [0,29,30,39,40]\n",
    "    for roi in ignore_roi_list:\n",
    "        ordered_roi_list.remove(roi)\n",
    "    \n",
    "    csv_data = pd.read_pickle(in_file)\n",
    "    clinical_scores = csv_data[csv_data.PTID.isin(subject_ids)][['UID',clinical_scale]]\n",
    "    #remove NANs\n",
    "    clinical_scores = clinical_scores[np.isfinite(clinical_scores[clinical_scale])]\n",
    "    sub_clinical_scores_dict = dict(zip(clinical_scores['UID'],clinical_scores[clinical_scale]))  \n",
    "    subject_uids = sub_clinical_scores_dict.keys()\n",
    "    \n",
    "    # K-folds\n",
    "    kf = pickle.load( open(kf_file, \"rb\" ) )\n",
    "    X_train = []\n",
    "    X_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    fold = 0 \n",
    "    for train, valid in kf:      \n",
    "        fold+=1\n",
    "        print 'Staring fold # {}'.format(fold)\n",
    "        print 'Starting train subset'\n",
    "        for t, tr in enumerate(train):            \n",
    "            uid = subject_uids[t]\n",
    "            result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict)\n",
    "            sub_X = result['sub_X']\n",
    "            sub_y = result['sub_y']\n",
    "            if t == 0:                \n",
    "                X_train_stack = sub_X\n",
    "                y_train_stack = sub_y\n",
    "            else:\n",
    "                X_train_stack = np.vstack((X_train_stack,sub_X))\n",
    "                y_train_stack = np.concatenate((y_train_stack,sub_y))\n",
    "        \n",
    "        print 'Ending train subset'\n",
    "        print 'Starting valid subset'\n",
    "        for v, val in enumerate(valid):\n",
    "            uid = subject_uids[val]\n",
    "            result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict)\n",
    "            sub_X = result['sub_X']\n",
    "            sub_y = result['sub_y']\n",
    "            if v == 0:                \n",
    "                X_valid_stack = sub_X\n",
    "                y_valid_stack = sub_y\n",
    "            else:\n",
    "                X_valid_stack = np.vstack((X_valid_stack,sub_X))\n",
    "                y_valid_stack = np.concatenate((y_valid_stack,sub_y))     \n",
    "        \n",
    "        print 'Ending valid subset'\n",
    "        input_data = h5.File(hdf_file, 'a')\n",
    "        input_data.create_dataset('Fold_{}_train_X'.format(fold),data=X_train_stack)    \n",
    "        input_data.create_dataset('Fold_{}_train_y'.format(fold),data=y_train_stack)    \n",
    "        input_data.create_dataset('Fold_{}_valid_X'.format(fold),data=X_valid_stack)    \n",
    "        input_data.create_dataset('Fold_{}_valid_y'.format(fold),data=y_valid_stack)    \n",
    "        input_data.close()\n",
    "        \n",
    "        \n",
    "        #X_train.append(X_train_stack)\n",
    "        #X_valid.append(X_valid_stack)\n",
    "        #y_train.append(y_train_stack)\n",
    "        #y_valid.append(y_valid_stack)\n",
    "        \n",
    "    print 'All folds done!'\n",
    "    \n",
    "    \n",
    "    # Return train and validation lists comprising all folds as well as unsplit data\n",
    "    #return {'X_train':X_train,'X_valid':X_valid,'y_train':y_train,'y_valid':y_valid}\n",
    "\n",
    "\n",
    "def inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_cScores_dict):\n",
    "    #UID = PTID + IID (PTID:[HC_vols], IID:{ROI:CT})\n",
    "    uid = uid.strip()\n",
    "    #print 'uid: {}'.format(uid)\n",
    "    ptid_re = re.compile('\\d*(_S_)\\d*')\n",
    "    iid_re = re.compile('(?<=I)\\d*')\n",
    "    ptid = re.search(ptid_re, uid).group(0).strip()\n",
    "    iid = re.search(iid_re, uid).group(0).strip()\n",
    "    missing_data = False\n",
    "    min_CT_sampx = 132\n",
    "    \n",
    "    if ptid in sub_HC_L_dict.keys():\n",
    "        sub_HC_L = np.asarray(sub_HC_L_dict[ptid][0])\n",
    "    else: \n",
    "        print \"missing HC_L entry for: {}\".format(uid) \n",
    "        missing_data = True\n",
    "    \n",
    "    if ptid in sub_HC_R_dict.keys():\n",
    "        sub_HC_R = np.asarray(sub_HC_R_dict[ptid][0])\n",
    "    else: \n",
    "        print \"missing HC_R entry for: {}\".format(uid)\n",
    "        missing_data = True\n",
    "        \n",
    "    if iid in sub_CT_dict.keys():\n",
    "        sub_CT_all_rois = sub_CT_dict[iid][0]              \n",
    "    else: \n",
    "        print \"missing CT entry for: {}\".format(uid) \n",
    "        missing_data = True\n",
    "        \n",
    "    if not missing_data:  \n",
    "        sub_CScore = sub_cScores_dict[uid]        \n",
    "        min_sampx = np.min([len(sub_HC_L),len(sub_HC_R),min_CT_sampx])\n",
    "        #print min_sampx\n",
    "        \n",
    "        #select samples \n",
    "        sub_HC_L_sampx = random.sample(sub_HC_L, min_sampx)\n",
    "        sub_HC_R_sampx = random.sample(sub_HC_R, min_sampx)\n",
    "        \n",
    "        #Draw equal number of samples per roi\n",
    "        sub_CT_sampx_dict = collections.OrderedDict()\n",
    "        for roi in ordered_roi_list:\n",
    "            #print roi\n",
    "            if roi not in sub_CT_sampx_dict:\n",
    "                sub_CT_sampx_dict[roi]=[]\n",
    "                \n",
    "            sub_CT_roi = np.squeeze(sub_CT_all_rois[roi])\n",
    "            #print sub_CT_roi.shape\n",
    "            if len(sub_CT_roi) >= min_CT_sampx:\n",
    "                sub_CT_sampx_dict[roi].append(random.sample(sub_CT_roi, min_sampx))            \n",
    "                \n",
    "        #print 'sub_CT_sampx_dict: {}'.format(len(sub_CT_sampx_dict))\n",
    "        sub_CT_sampx = np.zeros((min_sampx, len(ordered_roi_list)))\n",
    "        for col, roi in enumerate(ordered_roi_list):\n",
    "            sub_CT_sampx[:,col] = np.asarray(sub_CT_sampx_dict[roi],dtype=float)\n",
    "            \n",
    "        \n",
    "        #print 'sub_CT_sampx.shape :{}'.format(sub_CT_sampx.shape)\n",
    "        \n",
    "        sub_X = np.hstack((sub_HC_L_sampx,sub_HC_R_sampx,sub_CT_sampx))\n",
    "        sub_y = np.tile(sub_CScore, min_sampx)\n",
    "        \n",
    "    else:\n",
    "        sub_X = []\n",
    "        sub_y = []\n",
    "    \n",
    "    return {'sub_X': sub_X, 'sub_y':sub_y}\n",
    "\n",
    "#Load test data: Need to change this and collect Test data per subject with similar features length\n",
    "def load_test_data(in_file, feature_cols, clinical_scale):\n",
    "\n",
    "    data = pd.read_pickle(in_file)\n",
    "    data_trunc = data[clinical_scale + feature_cols]\n",
    "    # remove nans \n",
    "    data_trunc = data_trunc[np.isfinite(data_trunc[clinical_scale[0]])]\n",
    "    X = np.asarray(data_trunc[feature_cols],dtype=float)\n",
    "    y = np.asarray(data_trunc[clinical_scale[0]],dtype=float)\n",
    "    return {'X':X, 'y':y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = pickle.load( open(kf_file, \"rb\" ) )\n",
    "master_csv = pickle.load( open(master_dataframe, \"rb\" ) )\n",
    "#train_val_data = pickle.load( open(train_val_file, \"rb\" ) )\n",
    "#test_val_data = pickle.load( open(test_file, \"rb\" ) )\n",
    "\n",
    "sub_HC_L_dict = pickle.load( open(sub_HC_vol_left_file, \"rb\" ) )\n",
    "sub_HC_R_dict = pickle.load( open(sub_HC_vol_right_file, \"rb\" ) )\n",
    "sub_CT_dict = pickle.load( open(sub_CT_file, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trial run..\n",
    "print len(master_csv),len(kf.idxs),len(train_val_data),len(sub_HC_vol_left_dict),len(sub_HC_vol_right_dict),len(sub_CT_dict)\n",
    "uid  =' 002_S_0559_I118676'\n",
    "clinical_scale = 'ADAS13'\n",
    "subject_ids = sub_HC_L_dict.keys()\n",
    "clinical_scores = master_csv[csv_data.PTID.isin(subject_ids)][['UID',clinical_scale]]\n",
    "#remove NANs\n",
    "clinical_scores = clinical_scores[np.isfinite(clinical_scores[clinical_scale])]\n",
    "sub_clinical_scores_dict = dict(zip(clinical_scores['UID'],clinical_scores[clinical_scale]))  \n",
    "ordered_roi_list = sub_CT_dict['40817'][0].keys()\n",
    "result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict)\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring fold # 1\n",
      "Starting train subset\n",
      "Ending train subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Staring fold # 2\n",
      "Starting train subset\n",
      "Ending train subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Staring fold # 3\n",
      "Starting train subset\n",
      "Ending train subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Staring fold # 4\n",
      "Starting train subset\n",
      "Ending train subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Staring fold # 5\n",
      "Starting train subset\n",
      "Ending train subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Staring fold # 6\n",
      "Starting train subset\n",
      "Ending train subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Staring fold # 7\n",
      "Starting train subset\n",
      "Ending train subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Staring fold # 8\n",
      "Starting train subset\n",
      "Ending train subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Staring fold # 9\n",
      "Starting train subset\n",
      "Ending train subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "Staring fold # 10\n",
      "Starting train subset\n",
      "Ending train subset\n",
      "Starting valid subset\n",
      "Ending valid subset\n",
      "All folds done!\n"
     ]
    }
   ],
   "source": [
    "clinical_scale = 'ADAS13'\n",
    "test = load_CV_data(sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, master_dataframe, clinical_scale, kf_file, out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_csv[master_csv.PTID=='114_S_0166']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateInnerFold(in_file, OF_id, fold_name_prefix, n_innerFolds, out_file_prefix):\n",
    "    # Load data\n",
    "    X_name = fold_name_prefix + '_X'\n",
    "    y_name = fold_name_prefix + '_y'\n",
    "    input_data = h5.File(in_file, 'r')\n",
    "    X_raw = input_data[X_name][:]\n",
    "    y = input_data[y_name][:]\n",
    "    input_data.close()\n",
    "\n",
    "    print X_raw.shape\n",
    "    #Remove ROIs from ignore list\n",
    "    L_HC_offset = 11427\n",
    "    R_HC_offset = 10519\n",
    "        \n",
    "    ignore_list_CT_idx = list(L_HC_offset + R_HC_offset + np.array([0,29,30,37,38]))\n",
    "    #X = np.delete(X_raw, np.s_[ignore_list_CT_idx], 1)\n",
    "    X = X_raw\n",
    "    print X.shape\n",
    "\n",
    "    #Sample / Shuffle Data\n",
    "    sampx = len(y)\n",
    "    kf = KFold(sampx, n_folds=n_innerFolds,shuffle=True)\n",
    "\n",
    "    #Save Data\n",
    "    split_HC_CT = True #Split the HC and CT data layer to allow partitioned model\n",
    "    k=0\n",
    "    for train, valid in kf:\n",
    "        k+=1\n",
    "        out_file_train = out_file_prefix + 'train_InnerFold_{}_partition_ROI_74.h5'.format(k)\n",
    "        out_file_valid = out_file_prefix + 'valid_InnerFold_{}_partition_ROI_74.h5'.format(k)\n",
    "        #Save Train\n",
    "        output_data = h5.File(out_file_train, 'a')\n",
    "        X_all = X[train]\n",
    "        if split_HC_CT:\n",
    "            output_data.create_dataset('Fold_{}_X_L_HC'.format(OF_id),data=X_all[:,:L_HC_offset])\n",
    "            output_data.create_dataset('Fold_{}_X_R_HC'.format(OF_id),data=X_all[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "            output_data.create_dataset('Fold_{}_X_R_CT'.format(OF_id),data=X_all[:,L_HC_offset+R_HC_offset:])\n",
    "        else:\n",
    "            output_data.create_dataset('Fold_{}_X'.format(OF_id),data=X[train])\n",
    "\n",
    "        output_data.create_dataset('Fold_{}_y'.format(OF_id),data=y[train])\n",
    "        output_data.close()\n",
    "\n",
    "        # Save valid\n",
    "        output_data = h5.File(out_file_valid, 'a')\n",
    "        X_all = X[valid]\n",
    "        if split_HC_CT:\n",
    "            output_data.create_dataset('Fold_{}_X_L_HC'.format(OF_id),data=X_all[:,:L_HC_offset])\n",
    "            output_data.create_dataset('Fold_{}_X_R_HC'.format(OF_id),data=X_all[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "            output_data.create_dataset('Fold_{}_X_R_CT'.format(OF_id),data=X_all[:,L_HC_offset+R_HC_offset:])\n",
    "        else:\n",
    "            output_data.create_dataset('Fold_{}_X'.format(OF_id),data=X[valid])\n",
    "\n",
    "        output_data.create_dataset('Fold_{}_y'.format(OF_id),data=y[valid])\n",
    "        output_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36456, 22020)\n",
      "(36456, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n",
      "(36572, 22020)\n"
     ]
    }
   ],
   "source": [
    "outer_CV_fold_file = '/projects/nikhil/ADNI_prediction/input_datasets/inflated_datasets/HC_CT_inflated_CV_subsets.h5'\n",
    "single_CV_fold_dir = '/projects/nikhil/ADNI_prediction/input_datasets/inflated_datasets/'\n",
    "\n",
    "n_innerFolds = 5\n",
    "\n",
    "outer_folds = np.arange(1,11,1)\n",
    "for of in outer_folds:\n",
    "    fold_name_prefix = 'Fold_{}_train'.format(str(of))\n",
    "    single_CV_fold_file = '{}HC_CT_inflated_CV_OuterFold_{}_'.format(single_CV_fold_dir,str(of))\n",
    "    generateInnerFold(outer_CV_fold_file, str(of), fold_name_prefix, n_innerFolds, single_CV_fold_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outer_CV_fold_file = '/projects/nikhil/ADNI_prediction/input_datasets/inflated_datasets/HC_CT_inflated_CV_subsets.h5'\n",
    "X_name = fold_name_prefix + '_X'\n",
    "input_data = h5.File(outer_CV_fold_file, 'r')\n",
    "X = np.array(input_data[X_name][:])\n",
    "print X.shape\n",
    "#Remove ROIs from ignore list\n",
    "HC_offset = 11427 + 10519\n",
    "ignore_list_CT_idx = list(HC_offset + np.array([0,29,30,37,38]))\n",
    "np.delete(X, np.s_[ignore_list_CT_idx], 1)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Restructure OuterFold dataset h5 into two separate train and valid h5 files with identical dataset name\n",
    "single_CV_fold_dir = '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "\n",
    "out_file_train = single_CV_fold_dir + 'HC_CT_inflated_CV_OuterFolds_train.h5'\n",
    "out_file_valid = single_CV_fold_dir + 'HC_CT_inflated_CV_OuterFolds_valid.h5'\n",
    "\n",
    "input_data = h5.File(single_CV_fold_dir + 'HC_CT_inflated_CV_subsets.h5', 'r')\n",
    "out_data_train = h5.File(out_file_train, 'a')\n",
    "out_data_valid = h5.File(out_file_valid, 'a')\n",
    "                         \n",
    "for OF_id in np.arange(1,11,1):\n",
    "    dataset_train_X = 'Fold_{}_train_X'.format(OF_id)\n",
    "    dataset_train_y = 'Fold_{}_train_y'.format(OF_id)\n",
    "    dataset_valid_X = 'Fold_{}_valid_X'.format(OF_id)\n",
    "    dataset_valid_y = 'Fold_{}_valid_y'.format(OF_id)\n",
    "                     \n",
    "    data_train_X = input_data[dataset_train_X][:]\n",
    "    data_train_y = input_data[dataset_train_y][:]\n",
    "    data_valid_X = input_data[dataset_valid_X][:]\n",
    "    data_valid_y = input_data[dataset_valid_y][:]\n",
    "                \n",
    "    out_data_train.create_dataset('Fold_{}_X'.format(OF_id),data=data_train_X)    \n",
    "    out_data_train.create_dataset('Fold_{}_y'.format(OF_id),data=data_train_y)    \n",
    "    out_data_valid.create_dataset('Fold_{}_X'.format(OF_id),data=data_valid_X)    \n",
    "    out_data_valid.create_dataset('Fold_{}_y'.format(OF_id),data=data_valid_y)    \n",
    "    \n",
    "input_data.close()\n",
    "out_data_train.close()\n",
    "out_data_valid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_CT_dict = pickle.load( open(sub_CT_file, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ordered_roi_list = sub_CT_dict['40817'][0].keys()\n",
    "print 'total number of ROIs {}'.format(len(ordered_roi_list))\n",
    "ignore_roi_list = [0,29,30,39,40]\n",
    "\n",
    "for roi in ignore_roi_list:\n",
    "    print 'Ignore ROI index {}'.format(ordered_roi_list.index(roi))\n",
    "\n",
    "\n",
    "#for roi in ignore_roi_list:\n",
    "#    ordered_roi_list.remove(roi)\n",
    "#print \"\"\n",
    "#print ordered_roi_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print arr.shape\n",
    "l = [1,2]\n",
    "arr_trunc = np.delete(arr, np.s_[l], 1)\n",
    "print arr_trunc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(10 + np.array([0,29,30,37,38]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
