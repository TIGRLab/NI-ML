{
 "metadata": {
  "name": "",
  "signature": "sha256:f767ad190c00cf8d764e1b3e468b4db9335fdcc2ccb1f7c2934bdc31a01e96c2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Basic Imports\n",
      "import numpy as np\n",
      "import h5py as h5\n",
      "from sklearn.externals import joblib\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "from scipy import stats\n",
      "from sklearn.cross_validation import KFold\n",
      "import pickle\n",
      "import re\n",
      "import random\n",
      "import collections"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#datasets\n",
      "\n",
      "#input data\n",
      "master_dataframe = '/projects/francisco/data/ADNI/master_fused.pkl'\n",
      "train_val_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_train_plus_val.pkl'\n",
      "test_file = '/projects/francisco/data/ADNI/cli_ct_seg_fused_test.pkl'\n",
      "\n",
      "#Candidate label dictionaries\n",
      "sub_HC_vol_left_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC/subject_HC_vol_dictionary_train_val_left.pkl'\n",
      "sub_HC_vol_right_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC/subject_HC_vol_dictionary_train_val_right.pkl'\n",
      "sub_CT_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/subject_roi_ct_data/ADNI1_subject_ROI_CT_dict.pkl'\n",
      "#k-fold indices (from a saved file)\n",
      "kf_file = \"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_train_valid_KFold_idx.pkl\"\n",
      "\n",
      "#save hdf_file for inflated sets\n",
      "out_file = '/projects/nikhil/ADNI_prediction/input_datasets/test_1.hdf'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Grab CV data with specific feature columes (independent vars) and specific clinical scale (dependent var)\n",
      "def load_CV_data(sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, in_file, clinical_scale, kf_file, hdf_file):    \n",
      "    # Grab subject_IDs from sub_HC / sub_CT dictionaries\n",
      "    # Grab clinical score for each subject from master_csv table (infile). \n",
      "    # Filter out NANs\n",
      "    # Loop through K-folds (kf_file) and append candidate labels + CT values\n",
      "    # Note: UID = PTID + IID (036_S_0976_I65091)\n",
      "    \n",
      "    subject_ids = sub_HC_L_dict.keys()\n",
      "     # Pick any UID to generate roi_list common across all subjects to stay consistents while tranforming dictionary into array\n",
      "    ordered_roi_list = sub_CT_dict['40817'][0].keys()\n",
      "    csv_data = pd.read_pickle(in_file)\n",
      "    clinical_scores = csv_data[csv_data.PTID.isin(subject_ids)][['UID',clinical_scale]]\n",
      "    #remove NANs\n",
      "    clinical_scores = clinical_scores[np.isfinite(clinical_scores[clinical_scale])]\n",
      "    sub_clinical_scores_dict = dict(zip(clinical_scores['UID'],clinical_scores[clinical_scale]))  \n",
      "    subject_uids = sub_clinical_scores_dict.keys()\n",
      "    \n",
      "    # K-folds\n",
      "    kf = pickle.load( open(kf_file, \"rb\" ) )\n",
      "    X_train = []\n",
      "    X_valid = []\n",
      "    y_train = []\n",
      "    y_valid = []\n",
      "    fold = 0 \n",
      "    for train, valid in kf:      \n",
      "        fold+=1\n",
      "        print 'Staring fold # {}'.format(fold)\n",
      "        print 'Starting train subset'\n",
      "        for t, tr in enumerate(train):            \n",
      "            uid = subject_uids[t]\n",
      "            result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict)\n",
      "            sub_X = result['sub_X']\n",
      "            sub_y = result['sub_y']\n",
      "            if t == 0:                \n",
      "                X_train_stack = sub_X\n",
      "                y_train_stack = sub_y\n",
      "            else:\n",
      "                X_train_stack = np.vstack((X_train_stack,sub_X))\n",
      "                y_train_stack = np.concatenate((y_train_stack,sub_y))\n",
      "        \n",
      "        print 'Ending train subset'\n",
      "        print 'Starting valid subset'\n",
      "        for v, val in enumerate(valid):\n",
      "            uid = subject_uids[val]\n",
      "            result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict)\n",
      "            sub_X = result['sub_X']\n",
      "            sub_y = result['sub_y']\n",
      "            if v == 0:                \n",
      "                X_valid_stack = sub_X\n",
      "                y_valid_stack = sub_y\n",
      "            else:\n",
      "                X_valid_stack = np.vstack((X_valid_stack,sub_X))\n",
      "                y_valid_stack = np.concatenate((y_valid_stack,sub_y))     \n",
      "        \n",
      "        print 'Ending valid subset'\n",
      "        input_data = h5.File(hdf_file, 'a')\n",
      "        input_data.create_dataset('Fold_{}_train_X'.format(fold),data=X_train_stack)    \n",
      "        input_data.create_dataset('Fold_{}_train_y'.format(fold),data=y_train_stack)    \n",
      "        input_data.create_dataset('Fold_{}_valid_X'.format(fold),data=X_valid_stack)    \n",
      "        input_data.create_dataset('Fold_{}_valid_y'.format(fold),data=y_valid_stack)    \n",
      "        input_data.close()\n",
      "        \n",
      "        \n",
      "        #X_train.append(X_train_stack)\n",
      "        #X_valid.append(X_valid_stack)\n",
      "        #y_train.append(y_train_stack)\n",
      "        #y_valid.append(y_valid_stack)\n",
      "        \n",
      "    print 'All folds done!'\n",
      "    \n",
      "    \n",
      "    # Return train and validation lists comprising all folds as well as unsplit data\n",
      "    #return {'X_train':X_train,'X_valid':X_valid,'y_train':y_train,'y_valid':y_valid}\n",
      "\n",
      "\n",
      "def inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_cScores_dict):\n",
      "    #UID = PTID + IID (PTID:[HC_vols], IID:{ROI:CT})\n",
      "    uid = uid.strip()\n",
      "    #print 'uid: {}'.format(uid)\n",
      "    ptid_re = re.compile('\\d*(_S_)\\d*')\n",
      "    iid_re = re.compile('(?<=I)\\d*')\n",
      "    ptid = re.search(ptid_re, uid).group(0).strip()\n",
      "    iid = re.search(iid_re, uid).group(0).strip()\n",
      "    missing_data = False\n",
      "    min_CT_sampx = 10\n",
      "    \n",
      "    if ptid in sub_HC_L_dict.keys():\n",
      "        sub_HC_L = np.asarray(sub_HC_L_dict[ptid][0])\n",
      "    else: \n",
      "        print \"missing HC_L entry for: {}\".format(uid) \n",
      "        missing_data = True\n",
      "    \n",
      "    if ptid in sub_HC_R_dict.keys():\n",
      "        sub_HC_R = np.asarray(sub_HC_R_dict[ptid][0])\n",
      "    else: \n",
      "        print \"missing HC_R entry for: {}\".format(uid)\n",
      "        missing_data = True\n",
      "        \n",
      "    if iid in sub_CT_dict.keys():\n",
      "        sub_CT_all_rois = sub_CT_dict[iid][0]              \n",
      "    else: \n",
      "        print \"missing CT entry for: {}\".format(uid) \n",
      "        missing_data = True\n",
      "        \n",
      "    if not missing_data:  \n",
      "        sub_CScore = sub_cScores_dict[uid]        \n",
      "        min_sampx = np.min([len(sub_HC_L),len(sub_HC_R),min_CT_sampx])\n",
      "        #print min_sampx\n",
      "        \n",
      "        #select samples \n",
      "        sub_HC_L_sampx = random.sample(sub_HC_L, min_sampx)\n",
      "        sub_HC_R_sampx = random.sample(sub_HC_R, min_sampx)\n",
      "        \n",
      "        #Draw equal number of samples per roi\n",
      "        sub_CT_sampx_dict = collections.OrderedDict()\n",
      "        for roi in ordered_roi_list:\n",
      "            #print roi\n",
      "            if roi not in sub_CT_sampx_dict:\n",
      "                sub_CT_sampx_dict[roi]=[]\n",
      "                \n",
      "            sub_CT_roi = np.squeeze(sub_CT_all_rois[roi])\n",
      "            #print sub_CT_roi.shape\n",
      "            if len(sub_CT_roi) >= min_CT_sampx:\n",
      "                sub_CT_sampx_dict[roi].append(random.sample(sub_CT_roi, min_sampx))            \n",
      "                \n",
      "        #print 'sub_CT_sampx_dict: {}'.format(len(sub_CT_sampx_dict))\n",
      "        sub_CT_sampx = np.zeros((min_sampx, len(ordered_roi_list)))\n",
      "        for col, roi in enumerate(ordered_roi_list):\n",
      "            sub_CT_sampx[:,col] = np.asarray(sub_CT_sampx_dict[roi],dtype=float)\n",
      "            \n",
      "        \n",
      "        #print 'sub_CT_sampx.shape :{}'.format(sub_CT_sampx.shape)\n",
      "        \n",
      "        sub_X = np.hstack((sub_HC_L_sampx,sub_HC_R_sampx,sub_CT_sampx))\n",
      "        sub_y = np.tile(sub_CScore, min_sampx)\n",
      "        \n",
      "    else:\n",
      "        sub_X = []\n",
      "        sub_y = []\n",
      "    \n",
      "    return {'sub_X': sub_X, 'sub_y':sub_y}\n",
      "\n",
      "#Load test data\n",
      "def load_test_data(in_file, feature_cols, clinical_scale):\n",
      "\n",
      "    data = pd.read_pickle(in_file)\n",
      "    data_trunc = data[clinical_scale + feature_cols]\n",
      "    # remove nans \n",
      "    data_trunc = data_trunc[np.isfinite(data_trunc[clinical_scale[0]])]\n",
      "    X = np.asarray(data_trunc[feature_cols],dtype=float)\n",
      "    y = np.asarray(data_trunc[clinical_scale[0]],dtype=float)\n",
      "    return {'X':X, 'y':y}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kf = pickle.load( open(kf_file, \"rb\" ) )\n",
      "master_csv = pickle.load( open(master_dataframe, \"rb\" ) )\n",
      "#train_val_data = pickle.load( open(train_val_file, \"rb\" ) )\n",
      "#test_val_data = pickle.load( open(test_file, \"rb\" ) )\n",
      "\n",
      "sub_HC_L_dict = pickle.load( open(sub_HC_vol_left_file, \"rb\" ) )\n",
      "sub_HC_R_dict = pickle.load( open(sub_HC_vol_right_file, \"rb\" ) )\n",
      "sub_CT_dict = pickle.load( open(sub_CT_file, \"rb\" ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(master_csv),len(kf.idxs),len(train_val_data),len(sub_HC_vol_left_dict),len(sub_HC_vol_right_dict),len(sub_CT_dict)\n",
      "uid  =' 002_S_0559_I118676'\n",
      "clinical_scale = 'ADAS13'\n",
      "subject_ids = sub_HC_L_dict.keys()\n",
      "clinical_scores = master_csv[csv_data.PTID.isin(subject_ids)][['UID',clinical_scale]]\n",
      "#remove NANs\n",
      "clinical_scores = clinical_scores[np.isfinite(clinical_scores[clinical_scale])]\n",
      "sub_clinical_scores_dict = dict(zip(clinical_scores['UID'],clinical_scores[clinical_scale]))  \n",
      "ordered_roi_list = sub_CT_dict['40817'][0].keys()\n",
      "result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict)\n",
      "print result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "698 581 588 588 588 3585\n",
        "26"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "79\n",
        "79\n",
        "(26, 79)\n",
        "{'sub_X': array([[ 0.     ,  0.     ,  0.     , ...,  4.29186,  3.62227,  4.24065],\n",
        "       [ 0.     ,  0.     ,  0.     , ...,  4.20074,  3.90388,  3.94995],\n",
        "       [ 0.     ,  0.     ,  0.     , ...,  4.24083,  3.60309,  3.72378],\n",
        "       ..., \n",
        "       [ 0.     ,  0.     ,  0.     , ...,  4.44469,  4.20018,  4.48717],\n",
        "       [ 0.     ,  0.     ,  0.     , ...,  4.45053,  3.70272,  4.25988],\n",
        "       [ 0.     ,  0.     ,  0.     , ...,  4.32983,  3.47878,  4.2267 ]]), 'sub_y': array([ 8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
        "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.])}\n"
       ]
      }
     ],
     "prompt_number": 250
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clinical_scale = 'ADAS13'\n",
      "test = load_CV_data(sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, master_dataframe, clinical_scale, kf_file, out_file)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subject_ids = sub_HC_vol_left_dict.keys()\n",
      "#print len(subject_ids)\n",
      "#print subject_ids[:3]\n",
      "clinical_scale = 'ADAS13'\n",
      "subject_ids = sub_HC_vol_left_dict.keys()\n",
      "clinical_scores = master_csv[master_csv.PTID.isin(subject_ids)][['PTID',clinical_scale]]\n",
      "    #remove NANs\n",
      "clinical_scores = clinical_scores[np.isfinite(clinical_scores[clinical_scale])]\n",
      "#print clinical_scores\n",
      "\n",
      "uid = '002_S_0295_I118671'\n",
      "id_image = re.compile('\\d*(_S_)\\d*')\n",
      "img = re.search(id_image, uid).group(0)\n",
      "print img"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "002_S_0295\n"
       ]
      }
     ],
     "prompt_number": 120
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "master_csv[master_csv.PTID=='114_S_0166']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>UID</th>\n",
        "      <th>RID</th>\n",
        "      <th>PTID</th>\n",
        "      <th>IID</th>\n",
        "      <th>SPLIT</th>\n",
        "      <th>VISCODE</th>\n",
        "      <th>COLPROT</th>\n",
        "      <th>ORIGPROT</th>\n",
        "      <th>DX_bl</th>\n",
        "      <th>AGE</th>\n",
        "      <th>...</th>\n",
        "      <th>R_HC_10509</th>\n",
        "      <th>R_HC_10510</th>\n",
        "      <th>R_HC_10511</th>\n",
        "      <th>R_HC_10512</th>\n",
        "      <th>R_HC_10513</th>\n",
        "      <th>R_HC_10514</th>\n",
        "      <th>R_HC_10515</th>\n",
        "      <th>R_HC_10516</th>\n",
        "      <th>R_HC_10517</th>\n",
        "      <th>R_HC_10518</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>497</th>\n",
        "      <td> 114_S_0166_I39810</td>\n",
        "      <td> 166</td>\n",
        "      <td> 114_S_0166</td>\n",
        "      <td> I39810</td>\n",
        "      <td> train</td>\n",
        "      <td> bl</td>\n",
        "      <td> ADNI1</td>\n",
        "      <td> ADNI1</td>\n",
        "      <td> CN</td>\n",
        "      <td> 72.5</td>\n",
        "      <td>...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>1 rows \u00d7 22039 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 278,
       "text": [
        "                   UID  RID        PTID     IID  SPLIT VISCODE COLPROT  \\\n",
        "497  114_S_0166_I39810  166  114_S_0166  I39810  train      bl   ADNI1   \n",
        "\n",
        "    ORIGPROT DX_bl   AGE     ...     R_HC_10509  R_HC_10510  R_HC_10511  \\\n",
        "497    ADNI1    CN  72.5     ...              0           0           0   \n",
        "\n",
        "     R_HC_10512  R_HC_10513  R_HC_10514  R_HC_10515  R_HC_10516  R_HC_10517  \\\n",
        "497           0           0           0           0           0           0   \n",
        "\n",
        "     R_HC_10518  \n",
        "497           0  \n",
        "\n",
        "[1 rows x 22039 columns]"
       ]
      }
     ],
     "prompt_number": 278
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}