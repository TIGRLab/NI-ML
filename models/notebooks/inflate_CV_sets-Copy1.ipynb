{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "#from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import KFold\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import ipyparallel as ipp\n",
    "from functools import partial\n",
    "import collections\n",
    "import tables as tb\n",
    "from math import isnan\n",
    "\n",
    "%matplotlib inline\n",
    "#plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasets\n",
    "cohort = 'ADNI1'\n",
    "exp_name = 'Exp6'\n",
    "atlas = 'SpecCluster'\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "HC_L_data_path = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_left_{}.pkl'.format(cohort,exp_name)\n",
    "HC_R_data_path = baseline_dir + 'HC/subject_HC_vol_dictionary_{}_right_{}.pkl'.format(cohort,exp_name)\n",
    "CT_data_path = baseline_dir + 'CT/civet_out/{}_subject_ROI_CT_dict_{}.pkl'.format(cohort,atlas)\n",
    "CS_data_path = baseline_dir + 'CS/{}_BL_PTID_MMSE_dict.pkl'.format(cohort)\n",
    "CT_unique_ROIs_path = baseline_dir + 'CT/civet_out/ADNI_unique_ROIs_{}.pkl'.format(atlas)\n",
    "#k-fold indices (from a saved file)\n",
    "#kf_file = \"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_train_valid_KFold_idx.pkl\"\n",
    "#kf_file = \"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_train_valid_KFold_UIDs.pkl\"\n",
    "exp_setup_file = baseline_dir + 'exp_data/CV_{}_{}_ADAS13.pkl'.format(exp_name,cohort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grab CV data with specific feature columes (independent vars) and specific clinical scale (dependent var)\n",
    "def load_CV_data(sub_list, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, sub_CS_dict, clinical_scale, kf, hdf_file, cohort):\n",
    "    # Use sub list to verify dictionary entry exists for each in the sub_HC / sub_CT /sub_CS dictionaries\n",
    "    # Grab clinical score for each subject from master_csv table (infile) / or grab it from the sub_CS_dict\n",
    "    # Filter out NANs \n",
    "    # Loop through K-folds (kf_file) and append candidate labels + CT values\n",
    "    # Note: UID = PTID + IID (036_S_0976_I65091)\n",
    "    # cohort = Train, Valid, or Heldout \n",
    "    # Pick any UID to generate roi_list common across all subjects to stay consistents while tranforming dictionary into array\n",
    "    #ordered_roi_list = sub_CT_dict['137_S_0459'][0].keys()\n",
    "    ordered_roi_list = pickle.load( open(CT_unique_ROIs_path, \"rb\" ) )\n",
    "    \n",
    "    # ignore the \"0\" idx along with the 4 missing rois from the mean CT value csv\n",
    "    #ignore_roi_list = [0,29,30,39,40]\n",
    "    \n",
    "    if len(ordered_roi_list) < 100: #AAL\n",
    "        ignore_roi_list = [0] #-1 (and -1+1000) index for the ROIs in the middle\n",
    "    else: #SpecCluster\n",
    "        ignore_roi_list = [-1,999] #-1 (and -1+1000) index for the ROIs in the middle\n",
    "        \n",
    "    for roi in ignore_roi_list:\n",
    "        ordered_roi_list.remove(roi)\n",
    "            \n",
    "    use_CS_dict = True\n",
    "    if use_CS_dict:\n",
    "        sub_clinical_scores_dict = sub_CS_dict\n",
    "    else:\n",
    "        csv_data = pd.read_pickle(in_file)\n",
    "        subject_PTIDs = csv_data.PTID\n",
    "        subject_IIDs = csv_data.IID\n",
    "        #clinical_scores = csv_data[csv_data.PTID.isin(subject_ids)][['UID',clinical_scale]]\n",
    "        clinical_scores = csv_data[['PTID','IID',clinical_scale]]\n",
    "        #print clinical_scores\n",
    "        #remove NANs        \n",
    "        clinical_scores = clinical_scores[np.isfinite(clinical_scores[clinical_scale])]    \n",
    "        #clinical_scores['UID'] = clinical_scores['PTID'] + '_' + clinical_scores['IID']\n",
    "        sub_clinical_scores_dict = dict(zip(clinical_scores['PTID'],clinical_scores[clinical_scale]))    \n",
    "      \n",
    "    #print list(clinical_scores['UID'])\n",
    "    \n",
    "    # K-fold list\n",
    "    #kf = pickle.load( open(kf_file, \"rb\" ) )\n",
    "    train_fold_list = []\n",
    "    val_fold_list = []\n",
    "    for train, test in kf:\n",
    "        tmp_train_list = []\n",
    "        for t in train:\n",
    "            tmp_train_list.append(sub_list[t])\n",
    "        train_fold_list.append(tmp_train_list)\n",
    "        \n",
    "        tmp_val_list = []\n",
    "        for t in test:\n",
    "            tmp_val_list.append(sub_list[t])\n",
    "        val_fold_list.append(tmp_val_list)\n",
    "            \n",
    "    if cohort == 'train':        \n",
    "        #train_fold_list= kf['train_UIDs']       \n",
    "        computeTrainingFolds = True\n",
    "        drawSamples = True  #Draw samples or point estimates?\n",
    "        \n",
    "    elif cohort == 'valid':                \n",
    "        #val_fold_list= kf['valid_UIDs']  \n",
    "        computeTrainingFolds = False\n",
    "        computeValidFolds = True\n",
    "        drawSamples = False\n",
    "        \n",
    "    else:        \n",
    "        #heldout_list= kf['heldout_UIDs']\n",
    "        computeTrainingFolds = False\n",
    "        computeValidFolds = False\n",
    "        drawSamples = False\n",
    "        \n",
    "    X_train = []\n",
    "    X_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    \n",
    "    runParallel = False\n",
    "    if runParallel: #parallelized version -- doesnt work :-(   \n",
    "        rc = ipp.Client()\n",
    "        #rc.block = False\n",
    "        dview = rc[:]\n",
    "        print dview\n",
    "        dview.push(dict(inflate_fold = inflate_fold))        \n",
    "        dview.push(dict(inflate_subject_samples = inflate_subject_samples))        \n",
    "        mapfunc = partial(inflate_fold, sub_HC_L_dict=sub_HC_L_dict, sub_HC_R_dict=sub_HC_R_dict, sub_CT_dict=sub_CT_dict, \n",
    "                  ordered_roi_list=ordered_roi_list, sub_clinical_scores_dict=sub_clinical_scores_dict)\n",
    "\n",
    "        parallel_result = dview.map_sync(mapfunc, train_fold_list)  \n",
    "        return parallel_result\n",
    "    \n",
    "    else:\n",
    "        fold = 0 \n",
    "        uid_sampx_dict_list = []\n",
    "        \n",
    "        if computeTrainingFolds:             \n",
    "            for train in train_fold_list:  \n",
    "                #train = train_fold_list[-1]\n",
    "                #fold = 10\n",
    "                fold+=1                \n",
    "                print 'Starting train subset'\n",
    "                print 'Staring fold # {}'.format(fold)\n",
    "                uid_sampx_dict = collections.OrderedDict() #keep track of number of samples generated per subject\n",
    "                X_train_PE = []\n",
    "                y_train_PE = []\n",
    "                #print 'train:{}'.format(train)\n",
    "                for t, tr in enumerate(train):                  \n",
    "                    uid = tr             \n",
    "                    #print uid\n",
    "                    result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, sub_CS_dict, ordered_roi_list, sub_clinical_scores_dict, drawSamples)\n",
    "                    sub_X = result['sub_X']\n",
    "                    sub_y = result['sub_y']\n",
    "                \n",
    "                    if drawSamples:\n",
    "                        if t == 0:                \n",
    "                            X_train_stack = sub_X\n",
    "                            y_train_stack = sub_y\n",
    "                        else:\n",
    "                            X_train_stack = np.vstack((X_train_stack,sub_X))\n",
    "                            y_train_stack = np.concatenate((y_train_stack,sub_y))\n",
    "                            \n",
    "                        uid_sampx_dict[uid] = len(sub_y)\n",
    "                         \n",
    "                    else: #collect Point Esimtates\n",
    "                        X_train_PE.append(sub_X)\n",
    "                        y_train_PE.append(sub_y)\n",
    "                        uid_sampx_dict[uid] = 1\n",
    "                \n",
    "                if not drawSamples:   \n",
    "                    X_train_stack  = np.squeeze(np.array(X_train_PE))\n",
    "                    y_train_stack  = np.array(y_train_PE)\n",
    "                \n",
    "                input_data = h5.File(hdf_file, 'a')\n",
    "                if clinical_scale == 'ADAS13':\n",
    "                    input_data.create_dataset('Fold_{}_train_X'.format(fold),data=X_train_stack)    \n",
    "                    input_data.create_dataset('Fold_{}_train_y'.format(fold),data=y_train_stack)                    \n",
    "                else:\n",
    "                    input_data.create_dataset('Fold_{}_train_y3'.format(fold),data=y_train_stack)                    \n",
    "                input_data.close()\n",
    "                                \n",
    "                print 'Ending train subset'\n",
    "                uid_sampx_dict_list.append(uid_sampx_dict)\n",
    "                \n",
    "        #validation by default should be on \"fused features\"                \n",
    "        elif computeValidFolds:    \n",
    "            fold = 0 \n",
    "            for valid in val_fold_list:\n",
    "                fold+=1\n",
    "                print 'Starting valid subset'\n",
    "                X_valid_PE = []\n",
    "                y_valid_PE = []\n",
    "                for v, val in enumerate(valid):\n",
    "                    #print valid\n",
    "                    uid = val\n",
    "                    result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, sub_CS_dict, ordered_roi_list, sub_clinical_scores_dict, drawSamples)\n",
    "                    sub_X = result['sub_X']\n",
    "                    sub_y = result['sub_y']\n",
    "                \n",
    "                    if drawSamples:\n",
    "                        if v == 0:                \n",
    "                            X_valid_stack = sub_X\n",
    "                            y_valid_stack = sub_y\n",
    "                        else:\n",
    "                            X_valid_stack = np.vstack((X_valid_stack,sub_X))\n",
    "                            y_valid_stack = np.concatenate((y_valid_stack,sub_y))     \n",
    "                    else:\n",
    "                        X_valid_PE.append(sub_X)\n",
    "                        y_valid_PE.append(sub_y)\n",
    "        \n",
    "                print 'Ending valid subset' \n",
    "                if not drawSamples:   \n",
    "                    X_valid_stack  = np.squeeze(np.array(X_valid_PE))\n",
    "                    y_valid_stack  = np.array(y_valid_PE)\n",
    "                \n",
    "                input_data = h5.File(hdf_file, 'a')            \n",
    "                if clinical_scale == 'ADAS13':                    \n",
    "                    input_data.create_dataset('Fold_{}_valid_X'.format(fold),data=X_valid_stack)    \n",
    "                    input_data.create_dataset('Fold_{}_valid_y'.format(fold),data=y_valid_stack)    \n",
    "                else:\n",
    "                    input_data.create_dataset('Fold_{}_valid_y3'.format(fold),data=y_valid_stack)    \n",
    "                input_data.close()\n",
    "        \n",
    "        else:\n",
    "            print \"starting heldout set\"\n",
    "            X_heldout_PE = []\n",
    "            y_heldout_PE = []\n",
    "            for h, held in enumerate(heldout_list):                    \n",
    "                uid = held\n",
    "                result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, sub_CS_dict, ordered_roi_list, sub_clinical_scores_dict, drawSamples)\n",
    "                sub_X = result['sub_X']\n",
    "                sub_y = result['sub_y']\n",
    "                                \n",
    "                X_heldout_PE.append(sub_X)\n",
    "                y_heldout_PE.append(sub_y)\n",
    "        \n",
    "                X_heldout_stack  = np.squeeze(np.array(X_heldout_PE))\n",
    "                y_heldout_stack  = np.array(y_heldout_PE)\n",
    "                \n",
    "                input_data = h5.File(hdf_file, 'a')            \n",
    "                input_data.create_dataset('heldout_X',data=X_heldout_stack)    \n",
    "                input_data.create_dataset('heldout_y',data=y_heldout_stack)    \n",
    "                input_data.close()\n",
    "                \n",
    "            print 'Ending heldout subset' \n",
    "            \n",
    "        #Save uid--> sampx list of dictionaries per fold\n",
    "        f = open(hdf_file+'.pkl', 'wb')\n",
    "        pickle.dump(uid_sampx_dict_list, f)\n",
    "        f.close()\n",
    "        \n",
    "        print 'All folds done!'\n",
    "\n",
    "        \n",
    "def inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, sub_CS_dict, ordered_roi_list, sub_cScores_dict, drawSamples):\n",
    "    import numpy as np\n",
    "    import h5py as h5    \n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    from sklearn.cross_validation import KFold\n",
    "    import pickle\n",
    "    import re\n",
    "    import random\n",
    "    import collections\n",
    "    #UID = PTID + IID (PTID:[HC_vols], IID:{ROI:CT})\n",
    "    uid = uid.strip()\n",
    "    #print 'uid: {}'.format(uid)\n",
    "    ptid_re = re.compile('\\d*(_S_)\\d*')\n",
    "    iid_re = re.compile('(?<=I)\\d*')\n",
    "    ptid = re.search(ptid_re, uid).group(0).strip()\n",
    "    #iid = re.search(iid_re, uid).group(0).strip()\n",
    "    missing_data = False\n",
    "    min_CT_sampx = 65 #exp6\n",
    "    #min_CT_sampx = 60 #exp11\n",
    "    #min_CT_sampx = 5\n",
    "    MC = True\n",
    "    CS_ONLY = True\n",
    "    \n",
    "    #print ptid\n",
    "    if ptid in sub_HC_L_dict.keys():\n",
    "        sub_HC_L = np.asarray(sub_HC_L_dict[ptid])\n",
    "        \n",
    "    else: \n",
    "        print \"missing HC_L entry for: {}\".format(uid) \n",
    "        missing_data = True\n",
    "    \n",
    "    if ptid in sub_HC_R_dict.keys():\n",
    "        sub_HC_R = np.asarray(sub_HC_R_dict[ptid])\n",
    "        \n",
    "    else: \n",
    "        print \"missing HC_R entry for: {}\".format(uid)\n",
    "        missing_data = True\n",
    "        \n",
    "    if ptid in sub_CT_dict.keys():\n",
    "        sub_CT_all_rois = sub_CT_dict[ptid][0]            \n",
    "    else: \n",
    "        print \"missing CT entry for: {}\".format(uid) \n",
    "        missing_data = True\n",
    "        \n",
    "    if not missing_data:\n",
    "        sub_CScore = sub_cScores_dict[ptid]\n",
    "        sub_CT_sampx_dict = collections.OrderedDict()\n",
    "        if drawSamples:                    \n",
    "            min_sampx = np.min([sub_HC_L.shape[0],sub_HC_R.shape[0],min_CT_sampx])\n",
    "            #print 'min_sampx {}'.format(min_sampx)\n",
    "            if not CS_ONLY:\n",
    "                if MC: \n",
    "                    #select averaged out samples \n",
    "                    MC_mult = np.max([1,int(0.5*(np.min([sub_HC_L.shape[0],sub_HC_R.shape[0]])))]) #Pool for averaged out samples \n",
    "                    HC_L_MC_sampx_list = []\n",
    "                    HC_R_MC_sampx_list = []\n",
    "                    \n",
    "                    for i in np.arange(min_sampx): #Generate CT samples \n",
    "                        #select samples \n",
    "                        sub_HC_L_sampx_array = random.sample(sub_HC_L, MC_mult)\n",
    "                        sub_HC_R_sampx_array = random.sample(sub_HC_R, MC_mult)\n",
    "                        HC_L_MC_sampx_list.append(np.squeeze(stats.mode(sub_HC_L_sampx_array)[0]))\n",
    "                        HC_R_MC_sampx_list.append(np.squeeze(stats.mode(sub_HC_R_sampx_array)[0]))\n",
    "\n",
    "                    sub_HC_L_sampx = np.array(HC_L_MC_sampx_list)\n",
    "                    sub_HC_R_sampx = np.array(HC_R_MC_sampx_list)   \n",
    "            \n",
    "                else:\n",
    "                    #select any samples \n",
    "                    sub_HC_L_sampx = random.sample(sub_HC_L, min_sampx)\n",
    "                    sub_HC_R_sampx = random.sample(sub_HC_R, min_sampx)\n",
    "\n",
    "                #Draw equal number of samples per roi            \n",
    "                for roi in ordered_roi_list:\n",
    "                    #print roi\n",
    "                    if roi not in sub_CT_sampx_dict:\n",
    "                        sub_CT_sampx_dict[roi]=[]\n",
    "\n",
    "                    sub_CT_roi = np.squeeze(sub_CT_all_rois[roi])\n",
    "                    #print sub_CT_roi.shape\n",
    "                    if len(sub_CT_roi) >= min_CT_sampx:\n",
    "                        #Do you want averaged out samples or true thickness samples\n",
    "                        if MC: \n",
    "                            MC_mult = int(0.5*len(sub_CT_roi)) #Pool for averaged out samples \n",
    "                            CT_MC_sampx = []\n",
    "                            for i in np.arange(min_sampx): #Generate CT samples \n",
    "                                CT_MC_sampx.append(np.mean(random.sample(sub_CT_roi, MC_mult))) #Average out individual samples\n",
    "\n",
    "                            sub_CT_sampx_dict[roi].append(CT_MC_sampx)      \n",
    "                        else:\n",
    "                            # Draw true samples\n",
    "                            sub_CT_sampx_dict[roi].append(random.sample(sub_CT_roi, min_sampx))  \n",
    "                            #sub_CT_sampx_dict[roi].append(np.mean(sub_CT_roi))\n",
    "                    else:\n",
    "                        print \"Wrong value for the min_CT_sampx\"\n",
    "            \n",
    "            #Clinical Score            \n",
    "            sub_y = np.tile(sub_CScore, min_sampx)\n",
    "        \n",
    "        # Or just collect point esimates (fused labels + mean thickness values)\n",
    "        else:\n",
    "            #select point-estimates\n",
    "            min_sampx = 1\n",
    "            #print sub_HC_L\n",
    "            #print sub_HC_R\n",
    "            sub_HC_L_sampx = stats.mode(sub_HC_L)[0]\n",
    "            sub_HC_R_sampx = stats.mode(sub_HC_R)[0]\n",
    "            \n",
    "            for roi in ordered_roi_list:                \n",
    "                sub_CT_roi = np.squeeze(sub_CT_all_rois[roi])                \n",
    "                sub_CT_sampx_dict[roi] = np.mean(sub_CT_roi)\n",
    "            #Clinical Score            \n",
    "            sub_y = sub_CScore\n",
    "            \n",
    "        if not CS_ONLY:    \n",
    "            # Convert samples or a mean vector to a numpy array   \n",
    "            sub_CT_sampx = np.zeros((min_sampx, len(ordered_roi_list)))\n",
    "            for col, roi in enumerate(ordered_roi_list):\n",
    "                sub_CT_sampx[:,col] = np.asarray(sub_CT_sampx_dict[roi],dtype=float)\n",
    "\n",
    "            #print sub_HC_L_sampx.shape, sub_HC_R_sampx.shape, sub_CT_sampx.shape\n",
    "            sub_X = np.hstack((sub_HC_L_sampx,sub_HC_R_sampx,sub_CT_sampx))\n",
    "        else:\n",
    "            sub_X = []\n",
    "        \n",
    "    else:\n",
    "        sub_X = []\n",
    "        sub_y = []\n",
    "    \n",
    "    return {'sub_X': sub_X, 'sub_y':sub_y}\n",
    "\n",
    "\n",
    "#If you want only inflate training subset (this is used for parallel implementation)\n",
    "def inflate_fold(train,sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict):\n",
    "    import numpy as np\n",
    "    import h5py as h5    \n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    from sklearn.cross_validation import KFold\n",
    "    import pickle\n",
    "    import re\n",
    "    import random\n",
    "    import collections\n",
    "    print 'Starting train subset'\n",
    "    for t, tr in enumerate(train):            \n",
    "        uid = subject_uids[t]\n",
    "        result = inflate_subject_samples(uid, sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, ordered_roi_list, sub_clinical_scores_dict)\n",
    "        sub_X = result['sub_X']\n",
    "        sub_y = result['sub_y']\n",
    "        if t == 0:                \n",
    "            X_train_stack = sub_X\n",
    "            y_train_stack = sub_y\n",
    "        else:\n",
    "            X_train_stack = np.vstack((X_train_stack,sub_X))\n",
    "            y_train_stack = np.concatenate((y_train_stack,sub_y))\n",
    "        \n",
    "    print 'Ending train subset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if you have list of file names you can grab PTID and IID like this:\n",
    "\n",
    "# ID patterns\n",
    "id_participant = re.compile(r\"\"\"\n",
    " (?<=ADNI_)      # Match the first string after ADNI_\n",
    " (.*?)          # Lazy quantifier so it only grabs the first immediate match.\n",
    " (?=_MR)        # End at the _MR\n",
    "\"\"\", re.VERBOSE)\n",
    "\n",
    "id_image = re.compile('(?<=S)\\d+_(.*?)(?=_)')\n",
    "\n",
    "# part > img id lookup\n",
    "participants = {}\n",
    "for f in filenames:\n",
    "    try:\n",
    "        id = re.search(id_participant, f).group(0)\n",
    "        img = re.search(id_image, f).group(1)\n",
    "        participants[id] = img\n",
    "    except:\n",
    "        print f\n",
    "print '{} unique mappings found'.format(len(participants.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load dictionaries... \n",
    "sub_HC_L_dict = pickle.load( open(HC_L_data_path, \"rb\" ) )\n",
    "sub_HC_R_dict = pickle.load( open(HC_R_data_path, \"rb\" ) )\n",
    "\n",
    "sub_CT_dict = pickle.load( open(CT_data_path, \"rb\" ) )\n",
    "sub_CS_dict =  pickle.load( open(CS_data_path, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab common subjects from all feature dictionaries\n",
    "#sub_CS_dict_clean = filter(lambda k: not isnan(sub_CS_dict[k]), sub_CS_dict) #remove NaNs\n",
    "#sub_CS_dict_clean = {k: sub_CS_dict[k] for k in sub_CS_dict if not isnan(sub_CS_dict[k])}\n",
    "#sub_list = list(set(sub_HC_L_dict.keys()) & set(sub_HC_R_dict.keys()) & set(sub_CT_dict.keys()) & set(sub_CS_dict_clean.keys()))\n",
    "print len(set(sub_HC_L_dict.keys())), len(set(sub_HC_R_dict.keys())), len(set(sub_CT_dict.keys())), len(set(sub_CS_dict.keys())) \n",
    "#print len(sub_list)\n",
    "print sub_HC_L_dict.values()[0][0].shape, sub_HC_R_dict.values()[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Configs for generating fold\n",
    "clinical_scale = 'MMSE'\n",
    "exp_setup = pickle.load( open(exp_setup_file, \"rb\" ) )\n",
    "kf = exp_setup['kf']\n",
    "sub_list = exp_setup['common_subs']\n",
    "#ordered_roi_list = sub_CT_dict['40817'][0].keys() #keeps order consistent while appending CT cols across subjects\n",
    "cohort = 'valid'\n",
    "#save hdf_file for inflated / fused sets\n",
    "#out_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC_CT_fused_CV_subsets_C688_valid_test.h5'\n",
    "out_file = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp6_ADNI1_ADAS13_NN_{}.h5'.format(cohort)\n",
    "\n",
    "CV_inflated_data = load_CV_data(sub_list,sub_HC_L_dict, sub_HC_R_dict, sub_CT_dict, sub_CS_dict, clinical_scale, kf, out_file, cohort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, X shape:(139, 33243), y shape: (139,)\n"
     ]
    }
   ],
   "source": [
    "# ordered_roi_list = sub_CT_dict['137_S_0459'][0].keys()\n",
    "# atlas_data = sub_CT_dict['137_S_0459'][0]\n",
    "# roi_vert_count = {}\n",
    "# for roi in ordered_roi_list:\n",
    "#     roi_vert_count[roi] = len(atlas_data[roi][0])\n",
    "\n",
    "# print np.min(roi_vert_count.values())\n",
    "\n",
    "#Combine ADNI1 and ADNI2 CV data --> fold by fold concatination. \n",
    "cohort = 'valid'\n",
    "CS_only = False\n",
    "adni1_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp6_ADNI1_ADAS13_NN_{}.h5'.format(cohort)\n",
    "adni2_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp11_ADNI2_ADAS13_NN_{}.h5'.format(cohort)\n",
    "adni1and2_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp14_ADNI1and2_ADAS13_MMSE_NN_{}.h5'.format(cohort)\n",
    "for fold in np.arange(1,2,1):\n",
    "    if not CS_only:\n",
    "        adni1_data_X = load_data(adni1_path, 'Fold_{}_{}_X'.format(fold,cohort))\n",
    "        adni1_data_y = load_data(adni1_path, 'Fold_{}_{}_y'.format(fold,cohort))\n",
    "        adni2_data_X = load_data(adni2_path, 'Fold_{}_{}_X'.format(fold,cohort))\n",
    "        adni2_data_y = load_data(adni2_path, 'Fold_{}_{}_y'.format(fold,cohort))\n",
    "        adni1and2_data_X = np.vstack((adni1_data_X,adni2_data_X))\n",
    "        adni1and2_data_y = np.concatenate((adni1_data_y,adni2_data_y))\n",
    "\n",
    "        print 'fold: {}, X shape:{}, y shape: {}'.format(fold, adni1and2_data_X.shape, adni1and2_data_y.shape)\n",
    "        comb_data = h5.File(adni1and2_path, 'a')            \n",
    "        comb_data.create_dataset('Fold_{}_{}_X'.format(fold,cohort),data=adni1and2_data_X)            \n",
    "        comb_data.create_dataset('Fold_{}_{}_y'.format(fold,cohort),data=adni1and2_data_y)    \n",
    "    else:\n",
    "        adni1_data_y = load_data(adni1_path, 'Fold_{}_{}_y3'.format(fold,cohort))\n",
    "        adni2_data_y = load_data(adni2_path, 'Fold_{}_{}_y3'.format(fold,cohort))        \n",
    "        adni1and2_data_y = np.concatenate((adni1_data_y,adni2_data_y))    \n",
    "        print 'fold: {}, y shape: {}'.format(fold, adni1and2_data_y.shape)\n",
    "        comb_data = h5.File(adni1and2_path, 'a')\n",
    "        comb_data.create_dataset('Fold_{}_y3'.format(fold,cohort),data=adni1and2_data_y)        \n",
    "        \n",
    "    comb_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adni1_data_y3 = load_data(adni1_path, 'Fold_{}_y3'.format(fold,cohort))\n",
    "adni2_data_y3 = load_data(adni2_path, 'Fold_{}_y3'.format(fold,cohort))\n",
    "\n",
    "print adni1_data_y3\n",
    "print adni2_data_y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# QC the empirical samples\n",
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "def ES_QC(PE_data_X, PE_data_y, ES_data_X, ES_data_y,show_plots):    \n",
    "    #local params\n",
    "    sampx_PE = PE_data_X.shape[0]\n",
    "    sampx_ES = ES_data_X.shape[0]\n",
    "    featx = 80 #688\n",
    "    L_HC_offset = 16086 #12488 #9732 #11076 #11427\n",
    "    R_HC_offset = 16471 #12263 #8953 #10161 #10519\n",
    "    HC_CT_offset = 78 #686\n",
    "    \n",
    "    #compare the mean and stdev\n",
    "    PE_X_HC_L = np.sum(PE_data_X[:,:L_HC_offset],axis=1)\n",
    "    PE_X_HC_R = np.sum(PE_data_X[:,L_HC_offset:L_HC_offset+R_HC_offset],axis=1)\n",
    "    PE_X_CT = PE_data_X[:,L_HC_offset+R_HC_offset:]   \n",
    "    \n",
    "    ES_X_HC_L = np.sum(ES_data_X[:,:L_HC_offset],axis=1)\n",
    "    ES_X_HC_R = np.sum(ES_data_X[:,L_HC_offset:L_HC_offset+R_HC_offset],axis=1)\n",
    "    ES_X_CT = ES_data_X[:,L_HC_offset+R_HC_offset:]   \n",
    "    \n",
    "    PE_X_HC_L = PE_X_HC_L.reshape(sampx_PE,1)\n",
    "    PE_X_HC_R = PE_X_HC_R.reshape(sampx_PE,1)\n",
    "    ES_X_HC_L = ES_X_HC_L.reshape(sampx_ES,1)\n",
    "    ES_X_HC_R = ES_X_HC_R.reshape(sampx_ES,1)\n",
    "           \n",
    "    PE_feat = np.hstack((PE_X_HC_L,PE_X_HC_R,PE_X_CT))\n",
    "    ES_feat = np.hstack((ES_X_HC_L,ES_X_HC_R,ES_X_CT))\n",
    "    \n",
    "    print PE_feat.shape, ES_feat.shape\n",
    "    \n",
    "    PE_X_mean = np.mean(PE_feat,axis=0)\n",
    "    ES_X_mean = np.mean(ES_feat,axis=0)\n",
    "    \n",
    "    PE_X_std = np.std(PE_feat,axis=0)\n",
    "    ES_X_std = np.std(ES_feat,axis=0)\n",
    "    \n",
    "    #compare the correlations with the score    \n",
    "    PE_corr = []\n",
    "    ES_corr = []\n",
    "    feat_idx = np.arange(featx)\n",
    "    for col in feat_idx:\n",
    "        PE_corr.append(*zip(stats.pearsonr(PE_feat[:,col],PE_data_y))[0])\n",
    "        ES_corr.append(*zip(stats.pearsonr(ES_feat[:,col],ES_data_y))[0])\n",
    "    \n",
    "    PE_corr = np.squeeze(np.array(PE_corr))\n",
    "    ES_corr = np.squeeze(np.array(ES_corr))\n",
    "    \n",
    "    print 'PE: HC_L mean vol={}, std={}, corr={}'.format(PE_X_mean[0],PE_X_std[0],PE_corr[0])\n",
    "    print 'PE: HC_R mean vol={}, std={}, corr={}'.format(PE_X_mean[1],PE_X_std[1],PE_corr[1])\n",
    "    print 'ES: HC_L mean vol={}, std={}, corr={}'.format(ES_X_mean[0],ES_X_std[0],ES_corr[0])\n",
    "    print 'ES: HC_R mean vol={}, std={}, corr={}'.format(ES_X_mean[1],ES_X_std[1],ES_corr[1])\n",
    "    \n",
    "    if show_plots:\n",
    "        fig, ax = plt.subplots()    \n",
    "        bar_width = 1\n",
    "        opacity = 0.4\n",
    "        error_config = {'ecolor': '0.3'}\n",
    "\n",
    "        plt.subplot(3,1,1)\n",
    "        mean_PE_plt = plt.plot(feat_idx[2:],PE_X_mean[2:],label='PE_mean')\n",
    "        mean_ES_plt = plt.plot(feat_idx[2:],ES_X_mean[2:],label='ES_mean')\n",
    "        diff_mean_plt = plt.bar(feat_idx[2:], PE_X_mean[2:]-ES_X_mean[2:], bar_width,\n",
    "                         alpha=opacity,                     \n",
    "                         yerr=PE_X_std[2:],\n",
    "                         error_kw=error_config,\n",
    "                         label='PE_mean - ES_mean (PE_std)')\n",
    "        plt.ylabel('Stats (mean)')    \n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(3,1,2)\n",
    "        diff_std_plt = plt.bar(feat_idx[2:], PE_X_std[2:]-ES_X_std[2:] , bar_width,\n",
    "                         alpha=opacity,\n",
    "                         color='r',                     \n",
    "                         label='PE_std - ES_std')\n",
    "\n",
    "        #Correlation\n",
    "        plt.subplot(3,1,3)\n",
    "        r_PE_plt = plt.plot(feat_idx[2:], PE_corr[2:], label='PE_corr')\n",
    "        r_ES_plt = plt.plot(feat_idx[2:], ES_corr[2:], label='ES_corr')\n",
    "        diff_corr_plt = plt.bar(feat_idx[2:], PE_corr[2:] - ES_corr[2:], bar_width,\n",
    "                         alpha=opacity,                     \n",
    "                         label='PE_corr - ES_corr')\n",
    "\n",
    "        plt.xlabel('PE vs ES')\n",
    "        plt.ylabel('Stats (corr)')    \n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "#fid = 2\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/'\n",
    "#ES_file = baseline_dir + 'caffe_input/CV_Exp2_ADNI1_ADAS13_NN_OuterFold_{}_train_InnerFold_1.h5'.format(fid)\n",
    "ES_file = baseline_dir + 'CV_Exp11_ADNI2_ADAS13_NN_train.h5'\n",
    "PE_file = baseline_dir + 'CV_Exp11_ADNI2_ADAS13_NN_valid.h5'\n",
    "\n",
    "show_plots = False\n",
    "for fid in np.arange(1,11,1):\n",
    "    print fid\n",
    "    PE_data_X = load_data(PE_file,'Fold_{}_valid_X'.format(fid))\n",
    "    #PE_data_y = load_data(PE_file,'Fold_{}_valid_y'.format(fid))\n",
    "    PE_data_y = load_data(PE_file,'Fold_{}_y3'.format(fid))\n",
    "\n",
    "    #ES_data_X_L_HC = load_data(ES_file,'Fold_{}_X_L_HC'.format(fid))\n",
    "    #ES_data_X_R_HC = load_data(ES_file,'Fold_{}_X_R_HC'.format(fid))\n",
    "    #ES_data_X_CT = load_data(ES_file,'Fold_{}_X_R_CT'.format(fid))\n",
    "\n",
    "    #ES_data_X = np.hstack((ES_data_X_L_HC,ES_data_X_R_HC,ES_data_X_CT))\n",
    "\n",
    "    ES_data_X = load_data(ES_file,'Fold_{}_train_X'.format(fid))\n",
    "    ES_data_y = load_data(ES_file,'Fold_{}_train_y3'.format(fid))\n",
    "\n",
    "    ES_QC(PE_data_X, PE_data_y, ES_data_X, ES_data_y,show_plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for r,roi in enumerate(ordered_roi_list):      \n",
    "    plt.subplot(8,10,r+1)\n",
    "    plt.hist(sub_roi_mat_sampx[:,r]-sub_roi_mat[:,r])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "def generateInnerFold(OF_id, fold_name_prefix, out_file_prefix, in_file, n_innerFolds):\n",
    "    import numpy as np\n",
    "    import h5py as h5    \n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    from sklearn.cross_validation import KFold\n",
    "    import pickle\n",
    "    import re\n",
    "    import random\n",
    "    import collections\n",
    "    from sklearn import preprocessing\n",
    "    import tables as tb\n",
    "    \n",
    "    CT_SCALED = False #Scale CT modality to have values between [0,1] per dimension\n",
    "    CS_ONLY = False\n",
    "    \n",
    "    # Load data\n",
    "    data = tb.open_file(in_file, 'r')\n",
    "    if not CS_ONLY:\n",
    "        X_name = fold_name_prefix + '_X'    \n",
    "        y_name = fold_name_prefix + '_y'\n",
    "        \n",
    "        X_raw = data.get_node('/' + X_name)[:]\n",
    "        X = X_raw\n",
    "        print X.shape\n",
    "    else:        \n",
    "        y_name = fold_name_prefix + '_y3'\n",
    "        \n",
    "    y = data.get_node('/' + y_name)[:]\n",
    "    data.close()\n",
    "        \n",
    "    #Remove ROIs from ignore list       \n",
    "    L_HC_offset = 16086 #12488 #9732 #11076 #11427\n",
    "    R_HC_offset = 16471 #12263 #8953 #10161 #10519\n",
    "    sampx = len(y) \n",
    "    #ignore_list_CT_idx = list(L_HC_offset + R_HC_offset + np.array([0,29,30,37,38]))\n",
    "    #X = np.delete(X_raw, np.s_[ignore_list_CT_idx], 1)\n",
    "    \n",
    "    split_HC_CT = True #Split the HC and CT data layer to allow partitioned model\n",
    "    #Just create single fold (this avoids mixing of samples across the subjects)\n",
    "    if n_innerFolds == 1:\n",
    "        print \"Creating single fold...\"\n",
    "        k=1\n",
    "        train = np.arange(1,int(0.8*sampx),1)\n",
    "        valid = np.arange(int(0.8*sampx),sampx,1)\n",
    "        # Need to shuffle manually since no longer calling KFold\n",
    "        #np.random.shuffle(train)\n",
    "        #np.random.shuffle(valid)\n",
    "        \n",
    "        out_file_train = out_file_prefix + 'train_InnerFold_{}.h5'.format(k)\n",
    "        out_file_valid = out_file_prefix + 'valid_InnerFold_{}.h5'.format(k)\n",
    "        #Save Train\n",
    "        output_data = h5.File(out_file_train, 'a')        \n",
    "        print out_file_train\n",
    "        if not CS_ONLY:    \n",
    "            X_all = X[train]\n",
    "            X_L_HC = X_all[:,:L_HC_offset]\n",
    "            X_R_HC = X_all[:,L_HC_offset:L_HC_offset+R_HC_offset]\n",
    "            X_CT = X_all[:,L_HC_offset+R_HC_offset:]\n",
    "            if CT_SCALED:\n",
    "                #scaler = preprocessing.MinMaxScaler()\n",
    "                print 'train'\n",
    "                print 'before scaling' + np.mean(X_CT, axis=1)\n",
    "                scaler = preprocessing.StandardScaler()\n",
    "                X_CT_scaler = scaler.fit_transform(X_CT) \n",
    "                print 'after scaling' + np.mean(X_CT_scaler, axis=1)\n",
    "            else:\n",
    "                X_CT_scaler = X_CT\n",
    "                \n",
    "            if split_HC_CT:\n",
    "                output_data.create_dataset('Fold_{}_X_L_HC'.format(OF_id),data=X_L_HC)\n",
    "                output_data.create_dataset('Fold_{}_X_R_HC'.format(OF_id),data=X_R_HC)\n",
    "                output_data.create_dataset('Fold_{}_X_R_CT'.format(OF_id),data=X_CT_scaler)\n",
    "            else:\n",
    "                output_data.create_dataset('Fold_{}_X'.format(OF_id),data=X[train])\n",
    "                \n",
    "            output_data.create_dataset('Fold_{}_y'.format(OF_id),data=y[train])\n",
    "        \n",
    "        else:\n",
    "            output_data.create_dataset('Fold_{}_y3'.format(OF_id),data=y[train])\n",
    "        output_data.close()\n",
    "\n",
    "        # Save valid\n",
    "        output_data = h5.File(out_file_valid, 'a')\n",
    "        if not CS_ONLY:    \n",
    "            X_all = X[valid]\n",
    "            X_L_HC = X_all[:,:L_HC_offset]\n",
    "            X_R_HC = X_all[:,L_HC_offset:L_HC_offset+R_HC_offset]\n",
    "            X_CT = X_all[:,L_HC_offset+R_HC_offset:]\n",
    "            if CT_SCALED:\n",
    "                print 'valid'\n",
    "                print 'before scaling' + np.mean(X_CT, axis=1)\n",
    "                scaler = preprocessing.StandardScaler()\n",
    "                X_CT_scaler = scaler.fit_transform(X_CT) \n",
    "                print 'after scaling' + np.mean(X_CT_scaler, axis=1) \n",
    "            else:\n",
    "                X_CT_scaler = X_CT\n",
    "                \n",
    "            if split_HC_CT:\n",
    "                output_data.create_dataset('Fold_{}_X_L_HC'.format(OF_id),data=X_L_HC)\n",
    "                output_data.create_dataset('Fold_{}_X_R_HC'.format(OF_id),data=X_R_HC)\n",
    "                output_data.create_dataset('Fold_{}_X_R_CT'.format(OF_id),data=X_CT_scaler)\n",
    "            else:\n",
    "                output_data.create_dataset('Fold_{}_X'.format(OF_id),data=X[valid])\n",
    "                \n",
    "            output_data.create_dataset('Fold_{}_y'.format(OF_id),data=y[valid])    \n",
    "            \n",
    "        else:\n",
    "            output_data.create_dataset('Fold_{}_y3'.format(OF_id),data=y[valid])\n",
    "        output_data.close()\n",
    "        \n",
    "    else: #Sample / Shuffle Data (not a good idea)\n",
    "        kf = KFold(sampx, n_folds=n_innerFolds,shuffle=True)\n",
    "        k=0\n",
    "        for train, valid in kf:\n",
    "            k+=1\n",
    "            out_file_train = out_file_prefix + 'train_InnerFold_{}.h5'.format(k)\n",
    "            out_file_valid = out_file_prefix + 'valid_InnerFold_{}.h5'.format(k)\n",
    "            #Save Train\n",
    "            output_data = h5.File(out_file_train, 'a')\n",
    "            X_all = X[train]\n",
    "            if split_HC_CT:\n",
    "                output_data.create_dataset('Fold_{}_X_L_HC'.format(OF_id),data=X_all[:,:L_HC_offset])\n",
    "                output_data.create_dataset('Fold_{}_X_R_HC'.format(OF_id),data=X_all[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "                output_data.create_dataset('Fold_{}_X_R_CT'.format(OF_id),data=X_all[:,L_HC_offset+R_HC_offset:])\n",
    "            else:\n",
    "                output_data.create_dataset('Fold_{}_X'.format(OF_id),data=X[train])\n",
    "\n",
    "            output_data.create_dataset('Fold_{}_y'.format(OF_id),data=y[train])\n",
    "            output_data.close()\n",
    "\n",
    "            # Save valid\n",
    "            output_data = h5.File(out_file_valid, 'a')\n",
    "            X_all = X[valid]\n",
    "            if split_HC_CT:\n",
    "                output_data.create_dataset('Fold_{}_X_L_HC'.format(OF_id),data=X_all[:,:L_HC_offset])\n",
    "                output_data.create_dataset('Fold_{}_X_R_HC'.format(OF_id),data=X_all[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "                output_data.create_dataset('Fold_{}_X_R_CT'.format(OF_id),data=X_all[:,L_HC_offset+R_HC_offset:])\n",
    "            else:\n",
    "                output_data.create_dataset('Fold_{}_X'.format(OF_id),data=X[valid])\n",
    "\n",
    "            output_data.create_dataset('Fold_{}_y'.format(OF_id),data=y[valid])\n",
    "            output_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirectView [0, 1, 2, 3]>\n"
     ]
    }
   ],
   "source": [
    "outer_CV_fold_file = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp14_ADNI1and2_ADAS13_MMSE_NN_train.h5'\n",
    "single_CV_fold_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/caffe_input/'\n",
    "\n",
    "n_innerFolds = 1\n",
    "\n",
    "outer_folds = np.arange(1,11,1)\n",
    "fold_name_prefix_list = []\n",
    "single_CV_fold_file_list = []\n",
    "for of in outer_folds:\n",
    "    fold_name_prefix_list.append('Fold_{}_train'.format(str(of)))\n",
    "    single_CV_fold_file_list.append('{}CV_Exp14_ADNI1and2_ADAS13_MMSE_NN_OuterFold_{}_'.format(single_CV_fold_dir,str(of)))    \n",
    "    #generateInnerFold(outer_CV_fold_file, str(of), fold_name_prefix, n_innerFolds, single_CV_fold_file)\n",
    "\n",
    "runParallel = True\n",
    "if runParallel: #parallelized version:        \n",
    "        rc = ipp.Client()\n",
    "        #rc.block = False\n",
    "        dview = rc[:]\n",
    "        print dview\n",
    "        dview.push(dict(generateInnerFold = generateInnerFold))                   \n",
    "        mapfunc = partial(generateInnerFold, in_file=outer_CV_fold_file, n_innerFolds=n_innerFolds)\n",
    "\n",
    "        parallel_result = dview.map_sync(mapfunc, outer_folds, fold_name_prefix_list, single_CV_fold_file_list)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this data for computing subject wise performance during outerloop cross-validation + held-out testset\n",
    "# Partition the validation folds\n",
    "from sklearn import preprocessing\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/'\n",
    "in_file = 'CV_Exp14_ADNI1and2_ADAS13_MMSE_NN_valid.h5'\n",
    "out_file = 'CV_Exp14_ADNI1and2_ADAS13_MMSE_NN_valid.h5'\n",
    "\n",
    "CV = True\n",
    "L_HC_offset = 16086 #12488 #9732 #11076 #11427\n",
    "R_HC_offset = 16471 #12263 #8953 #10161 #10519\n",
    "\n",
    "subset = 'valid'\n",
    "CS_ONLY = False\n",
    "CT_SCALED = False\n",
    "if CV:\n",
    "    CV_fused_data = h5.File(baseline_dir + in_file,'a')\n",
    "    #CV_valid_norm_data = h5.File(baseline_dir + out_file,'a')\n",
    "    for f in np.arange(10):\n",
    "        X = CV_fused_data['Fold_{}_{}_X'.format(f+1,subset)][:]\n",
    "        \n",
    "        if not CS_ONLY:\n",
    "            y = CV_fused_data['Fold_{}_{}_y'.format(f+1,subset)][:]\n",
    "            \n",
    "            CV_fused_data.create_dataset('Fold_{}_X_L_HC'.format(f+1),data=X[:,:L_HC_offset])\n",
    "            CV_fused_data.create_dataset('Fold_{}_X_R_HC'.format(f+1),data=X[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "            X_CT = X[:,L_HC_offset+R_HC_offset:]\n",
    "            if CT_SCALED:\n",
    "                #scaler = preprocessing.MinMaxScaler()\n",
    "                scaler = preprocessing.StandardScaler()\n",
    "                X_CT_scaled = scaler.fit_transform(X_CT)\n",
    "                #X_CT_scaled = preprocessing.scale(X_CT)\n",
    "            else:\n",
    "                X_CT_scaled = X_CT\n",
    "            #print np.mean(X_CT,axis=0)        \n",
    "            #print np.mean(X_CT_scaled,axis=0)\n",
    "            CV_fused_data.create_dataset('Fold_{}_X_R_CT'.format(f+1),data=X_CT_scaled)#Typo : R_CT        \n",
    "            CV_fused_data.create_dataset('Fold_{}_y'.format(f+1),data=y) \n",
    "            \n",
    "        else:\n",
    "            y = CV_fused_data['Fold_{}_{}_y3'.format(f+1,subset)][:]\n",
    "            CV_fused_data.create_dataset('Fold_{}_y3'.format(f+1),data=y)\n",
    "    \n",
    "    #CV_valid_norm_data.close()\n",
    "    CV_fused_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outer_CV_fold_file = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp2_ADNI1_ADAS13_NN_train.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate map between AAL atlas ROI : index based on\n",
    "AAL_roi_map_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL_ROI_IDx'\n",
    "AAL_roi_Name_featID_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL_ROI_Name_featIDx.pkl'\n",
    "ordered_roi_list = sub_CT_dict['40817'][0].keys()\n",
    "\n",
    "\n",
    "ignore_roi_list = [0,29,30,39,40]\n",
    "for roi in ignore_roi_list:\n",
    "    ordered_roi_list.remove(roi)\n",
    "\n",
    "ordered_roi_idx_dict = {}\n",
    "for i,idx in enumerate(ordered_roi_list):\n",
    "    ordered_roi_idx_dict[idx]=i\n",
    "\n",
    "print ordered_roi_idx_dict\n",
    "#data = pd.read_csv(AAL_roi_map_file,delim_whitespace=True)\n",
    "print data.columns\n",
    "\n",
    "data['feature_id'] = data['Ind'].map(ordered_roi_idx_dict) \n",
    "print data\n",
    "#data = data[~np.isnan(data['feature_id'])]\n",
    "#print data\n",
    "\n",
    "roi_name_featIDx_Dict = data.set_index('feature_id')['Name'].to_dict()\n",
    "od = collections.OrderedDict(sorted(roi_name_featIDx_Dict.items())) #order by feature colume index\n",
    "print od\n",
    "\n",
    "#f = open(AAL_roi_Name_featID_file, 'wb')\n",
    "#pickle.dump(od, f)\n",
    "#f.close()\n",
    "\n",
    "#print 'total number of ROIs {}'.format(len(ordered_roi_list))\n",
    "#ignore_roi_list = [0,29,30,39,40]\n",
    "\n",
    "#for roi in ignore_roi_list:\n",
    "#    print 'Ignore ROI index {}'.format(ordered_roi_list.index(roi))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print arr.shape\n",
    "l = [1,2]\n",
    "arr_trunc = np.delete(arr, np.s_[l], 1)\n",
    "print arr_trunc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create inflated HC total vol dataset (no voxel wise features)\n",
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "\n",
    "baseline_dir= '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "in_file = 'HC_CT_inflated_CV_subsets_MC5.h5'\n",
    "out_file = 'total_HC_vol_CT_inflated_CV_subsets_MC5.h5'\n",
    "\n",
    "L_HC_offset=11427\n",
    "R_HC_offset=10519\n",
    "\n",
    "ignore_cols = False\n",
    "for lid in np.arange(6,11,1):\n",
    "    print 'Starting Fold {}'.format(lid)\n",
    "    out_train_X_raw = load_data(baseline_dir + in_file,'Fold_{}_train_X'.format(lid))\n",
    "    out_train_y = load_data(baseline_dir + in_file,'Fold_{}_train_y'.format(lid))\n",
    "\n",
    "    #out_valid_X_raw = load_data(baseline_dir + in_file,'Fold_{}_valid_X'.format(lid))\n",
    "    #out_valid_y = load_data(baseline_dir + in_file,'Fold_{}_valid_y'.format(lid))\n",
    "\n",
    "    #if you want to remove some CT columes (74 connundrum)\n",
    "    if ignore_cols:\n",
    "        ignore_list_CT_idx = list(L_HC_offset + R_HC_offset + np.array([0,29,30,37,38]))\n",
    "        out_train_X = np.delete(out_train_X_raw, np.s_[ignore_list_CT_idx], 1)\n",
    "        #out_valid_X = np.delete(out_valid_X_raw, np.s_[ignore_list_CT_idx], 1)\n",
    "    else:\n",
    "        out_train_X = out_train_X_raw\n",
    "        #out_valid_X = out_valid_X_raw\n",
    "\n",
    "\n",
    "    out_data = h5.File(baseline_dir + out_file,'a')\n",
    "    #Train\n",
    "    out_data.create_dataset('Fold_{}_train_X_L_HC'.format(lid),data=np.sum(out_train_X[:,:L_HC_offset],axis=1))\n",
    "    out_data.create_dataset('Fold_{}_train_X_R_HC'.format(lid),data=np.sum(out_train_X[:,L_HC_offset:L_HC_offset+R_HC_offset],axis=1))\n",
    "    out_data.create_dataset('Fold_{}_train_X_CT'.format(lid),data=out_train_X[:,L_HC_offset+R_HC_offset:])\n",
    "    out_data.create_dataset('Fold_{}_train_y'.format(lid),data=out_train_y)\n",
    "    #Valid\n",
    "    #out_data.create_dataset('Fold_{}_valid_X_L_HC'.format(lid),data=np.sum(out_valid_X[:,:L_HC_offset],axis=1))\n",
    "    #out_data.create_dataset('Fold_{}_valid_X_R_HC'.format(lid),data=np.sum(out_valid_X[:,L_HC_offset:L_HC_offset+R_HC_offset],axis=1))\n",
    "    #out_data.create_dataset('Fold_{}_valid_X_CT'.format(lid),data=out_valid_X[:,L_HC_offset+R_HC_offset:])\n",
    "    #out_data.create_dataset('Fold_{}_valid_y'.format(lid),data=out_valid_y)\n",
    "    out_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sanity checks \n",
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "baseline_dir= '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "in_file1 = 'HC_CT_fused_CV_subsets_C688_valid_test.h5'\n",
    "in_file2 = 'HC_CT_fused_CV_subsets_C688_valid.h5'\n",
    "\n",
    "x1 = load_data(baseline_dir + in_file1,'Fold_1_valid_X')\n",
    "y1 = load_data(baseline_dir + in_file1,'Fold_1_valid_y')\n",
    "\n",
    "x2 = load_data(baseline_dir + in_file2,'Fold_1_valid_X')\n",
    "y2 = load_data(baseline_dir + in_file2,'Fold_1_valid_y')\n",
    "\n",
    "print zip(y1,y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_setup_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/CV_Exp2_ADNI1_ADAS13.pkl'\n",
    "exp_setup = pickle.load( open(exp_setup_path, \"rb\" ) )\n",
    "kf = exp_setup['kf']\n",
    "sub_list = exp_setup['common_subs']\n",
    "train_fold_list=[]\n",
    "val_fold_list=[]\n",
    "for train, test in kf:\n",
    "        tmp_train_list = []\n",
    "        for t in train:\n",
    "            tmp_train_list.append(sub_list[t])\n",
    "        train_fold_list.append(tmp_train_list)\n",
    "        \n",
    "        tmp_val_list = []\n",
    "        for t in test:\n",
    "            tmp_val_list.append(sub_list[t])\n",
    "        val_fold_list.append(tmp_val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Copy hdf5 dataset\n",
    "local_root = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/caffe_input/'\n",
    "\n",
    "copy_from = 'CV_Exp12_ADNI1and2_ADAS13_MMSE_NN_OuterFold_{}_{}_InnerFold_1.h5'\n",
    "copy_to = 'CV_Exp13_ADNI1and2_ADAS13_MMSE_NN_OuterFold_{}_{}_InnerFold_1.h5'\n",
    "\n",
    "cohorts = ['train', 'valid']\n",
    "datasets = ['Fold_{}_X_L_HC','Fold_{}_X_R_HC','Fold_{}_X_R_CT']\n",
    "\n",
    "for cohort in cohorts:\n",
    "    for fid in range(2,11,1):\n",
    "        in_data_path = local_root + copy_from.format(fid, cohort)\n",
    "        out_data_path = local_root + copy_to.format(fid, cohort)        \n",
    "        out_data_file = h5.File(out_data_path, 'a')\n",
    "        \n",
    "        for dataset in datasets:\n",
    "            in_data = load_data(in_data_path, dataset.format(fid))            \n",
    "            out_data_file.create_dataset(dataset.format(fid),data=in_data)  \n",
    "            \n",
    "        out_data_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Delete hdf5 dataset! (works sometimes -- possibly with py tables datasets)\n",
    "local_root = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/caffe_input/'\n",
    "\n",
    "for fid in range(1,11,1):\n",
    "    file_name = 'CV_Exp12_ADNI1and2_ADAS13_MMSE_NN_OuterFold_{}_valid_InnerFold_1.h5'.format(fid)\n",
    "    dataset = 'Fold_{}_y3'.format(fid)\n",
    "    print local_root + file_name, dataset\n",
    "    with h5.File(local_root + file_name,  \"a\") as f:\n",
    "        f.__delitem__(dataset)\n",
    "        \n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
