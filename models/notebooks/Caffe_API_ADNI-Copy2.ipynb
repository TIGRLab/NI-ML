{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import caffe\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(2)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "# Some defs to load data and extract encodings from trained net\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    #print net.blobs.items()[0]\n",
    "    #print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        #print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        #print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        #print net.blobs[input_node].shape\n",
    "        \n",
    "        data_layers.append(net.blobs[input_node])    \n",
    "        #print 'X_list shape: {}'.format(X_list[i].shape)\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "    \n",
    "     \n",
    "    net.reshape()            \n",
    "    #print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        #print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1, concat_param=dict(axis=1))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_CT_SpecCluster_dyn,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT_SpecCluster_dyn, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT_unified(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_HC_CT,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_HC_CT,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_HC_CT,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_HC_CT,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_HC_CT, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['HC_CT_ff'], param=dict(lr_mult=lr['HC_CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC_CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=4, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas, n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT_SpecCluster_dyn, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    #n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "#     n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,n.ff1, concat_param=dict(axis=1))\n",
    "    #n.concat = L.Concat(n.X_L_HC,n.X_R_HC,n.X_CT, concat_param=dict(axis=1))\n",
    "    \n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=lr['COMB']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.dropC1 = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    n.dropC2 = L.Dropout(n.ff4, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2) #This is done automatically by caffe! \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff4, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        #n.ff2_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2ADAS13 = L.ReLU(n.ff2_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff4, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        #n.ff2_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2MMSE = L.ReLU(n.ff2_MMSE, in_place=True)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        n.ff1_DX = L.InnerProduct(n.ff3, num_output=node_sizes['DX_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1DX = L.ReLU(n.ff1_DX, in_place=True)\n",
    "        \n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_DX = L.InnerProduct(n.ff1_DX, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.DX_loss = L.SoftmaxWithLoss(n.output_DX, n.dx_cat3,loss_weight=tr['DX'])  \n",
    "    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_CT, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.dropC1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "        \n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_R_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))\n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.code = L.InnerProduct(n.X_L_HC, num_output=node_sizes['code'], weight_filler=dict(type='gaussian',std=.1, sparse=int(0.1*node_sizes['code'])))       \n",
    "        n.NLinEn1 = L.ReLU(n.code, in_place=True)\n",
    "        n.drop1 = L.Dropout(n.code, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "        \n",
    "    elif modality =='HC_CT': #multimodal AE - experimental stage\n",
    "        HC_node_split = 0.8\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_L_HC = L.InnerProduct(n.X_L_HC, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.9*node_sizes['En1'])))        \n",
    "        n.NLinEn_L_HC = L.ReLU(n.encoder_L_HC, in_place=True)\n",
    "        \n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder_CT = L.InnerProduct(n.X_CT, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='gaussian',std=.1, sparse=int(0.2*0.1*node_sizes['En1'])))        \n",
    "        n.NLinEn_CT = L.ReLU(n.encoder_CT, in_place=True)\n",
    "        \n",
    "        #Concat         \n",
    "        n.encoder1 = L.Concat(n.encoder_L_HC,n.encoder_CT, concat_param=dict(axis=1))\n",
    "        \n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers (or common encoder layers for multimodal) \n",
    "    \n",
    "#     n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.Sigmoid(n.encoder2, in_place=True)\n",
    "    #code layer\n",
    "#    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='xavier'))  \n",
    "    \n",
    "#     n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "    \n",
    "    #Decoder layers\n",
    "    if modality == 'CT':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)                    \n",
    "    \n",
    "    elif modality =='R_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "#         n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#         n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "        n.output = L.InnerProduct(n.code, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    elif modality =='HC_CT':        \n",
    "        n.decoder_L_HC = L.InnerProduct(n.code, num_output=int(HC_node_split*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_L_CT = L.ReLU(n.decoder_L_HC, in_place=True)\n",
    "        n.decoder_CT = L.InnerProduct(n.code, num_output=int((1-HC_node_split)*node_sizes['En1']), weight_filler=dict(type='xavier'))  \n",
    "        n.NLinDe_CT = L.ReLU(n.decoder_CT, in_place=True)\n",
    "        \n",
    "        n.output_L_HC = L.InnerProduct(n.decoder_L_HC, num_output=node_sizes['out_HC'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_HC = L.SigmoidCrossEntropyLoss(n.output_L_HC, n.X_L_HC)\n",
    "        \n",
    "        n.output_CT = L.InnerProduct(n.decoder_CT, num_output=node_sizes['out_CT'], weight_filler=dict(type='xavier'))\n",
    "        n.loss_CT = L.EuclideanLoss(n.output_CT, n.X_CT)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    return n.to_proto()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode):\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 20\n",
    "    \n",
    "    if multimodal_autoencode:\n",
    "        train_loss_HC = zeros(niter)\n",
    "        test_loss_HC = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_loss_CT = zeros(niter)\n",
    "        test_loss_CT = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "        for it in range(niter):            \n",
    "            solver.step(1)  # SGD by Caffe \n",
    "            train_loss_HC[it] = solver.net.blobs['loss_HC'].data\n",
    "            train_loss_CT[it] = solver.net.blobs['loss_CT'].data\n",
    "            \n",
    "            if it % test_interval == 0:                        \n",
    "                t_loss_HC = 0\n",
    "                t_loss_CT = 0                                \n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()   \n",
    "                    t_loss_HC += solver.test_nets[0].blobs['loss_HC'].data\n",
    "                    t_loss_CT += solver.test_nets[0].blobs['loss_CT'].data\n",
    "                \n",
    "                test_loss_HC[it // test_interval] = t_loss_HC/(test_iter)\n",
    "                test_loss_CT[it // test_interval] = t_loss_CT/(test_iter)\n",
    "                print 'HC Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_HC[it], np.sum(train_loss_HC)/it, test_loss_HC[it // test_interval])\n",
    "                print 'CT Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_loss_CT[it], np.sum(train_loss_CT)/it, test_loss_CT[it // test_interval])\n",
    "                \n",
    "        perf = {'train_loss':[train_loss_HC, train_loss_CT],'test_loss':[test_loss_HC,test_loss_CT]}\n",
    "    else: \n",
    "        \n",
    "        #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "        # losses will also be stored in the log\n",
    "        test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "        if not multi_task:\n",
    "            train_loss = zeros(niter)\n",
    "            test_loss = zeros(int(np.ceil(niter / test_interval)))  \n",
    "\n",
    "        else: \n",
    "            if multi_label:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_DX_loss = zeros(niter)\n",
    "                test_DX_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "            else:\n",
    "                train_ADAS13_loss = zeros(niter)\n",
    "                test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "                train_MMSE_loss = zeros(niter)\n",
    "                test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "\n",
    "        #output = zeros((niter, batch_size))\n",
    "        #solver.restore()\n",
    "        #the main solver loop\n",
    "        for it in range(niter):\n",
    "            #solver.net.forward()\n",
    "            solver.step(1)  # SGD by Caffe    \n",
    "            # store the train loss\n",
    "            if not multi_task:\n",
    "                train_loss[it] = solver.net.blobs['loss'].data        \n",
    "            else: \n",
    "                if multi_label:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_DX_loss[it] = solver.net.blobs['DX_loss'].data\n",
    "                else:\n",
    "                    train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "                    train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "            # store the output on the first test batch\n",
    "            # (start the forward pass at conv1 to avoid loading new data)\n",
    "            #solver.test_nets[0].forward()\n",
    "            #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "            # run a full test every so often\n",
    "            # (Caffe can also do this for us and write to a log, but we show here\n",
    "            #  how to do it directly in Python, where more complicated things are easier.)\n",
    "            if it % test_interval == 0:        \n",
    "                t_loss = 0\n",
    "                t_ADAS13_loss = 0\n",
    "                t_MMSE_loss = 0\n",
    "                t_DX_loss = 0\n",
    "                correct = 0\n",
    "                for test_it in range(test_iter):\n",
    "                    solver.test_nets[0].forward()                \n",
    "                    if not multi_task:\n",
    "                        t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                        if multi_label:\n",
    "                            correct += sum(solver.test_nets[0].blobs['output'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "                    else: \n",
    "                        if multi_label:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_DX_loss += solver.test_nets[0].blobs['DX_loss'].data\n",
    "                            correct += sum(solver.test_nets[0].blobs['output_DX'].data.argmax(1)\n",
    "                               == solver.test_nets[0].blobs['dx_cat3'].data)\n",
    "\n",
    "                        else:\n",
    "                            t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                            t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "                if not multi_task:\n",
    "                    test_loss[it // test_interval] = t_loss/(test_iter)\n",
    "                    if multi_label:\n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                    print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                        it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval], test_acc[it // test_interval])\n",
    "                else:\n",
    "                    if multi_label:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_DX_loss[it // test_interval] = t_DX_loss/(test_iter) \n",
    "                        test_acc[it // test_interval] = float(correct)/(test_iter*batch_size)\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'DX Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                            it, train_DX_loss[it], np.sum(train_DX_loss)/it, test_DX_loss[it // test_interval],test_acc[it // test_interval])\n",
    "                    else:\n",
    "                        test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                        test_MMSE_loss[it // test_interval] = t_MMSE_loss/(test_iter)             \n",
    "\n",
    "                        print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                        print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                            it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "        if not multi_task:\n",
    "            perf = {'train_loss':[train_loss],'test_loss':[test_loss],'test_acc':[test_acc]}\n",
    "        else:\n",
    "            if multi_label:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_DX_loss],'test_loss':[test_ADAS13_loss,test_DX_loss],'test_acc':[test_acc]}\n",
    "            else:\n",
    "                perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "                \n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,snap_prefix):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    \n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    #s.solver_type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 100000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 4000\n",
    "    s.snapshot_prefix = snap_prefix\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MC # 1, Hype # hyp5, Fold # 6\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold6/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (194.300003052,inf), test loss: 166.799511719\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (464.853027344,inf), test loss: 482.257244873\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (49.1363296509,73.9603316956), test loss: 45.3179003\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.39552116394,138.242866991), test loss: 3.6642472595\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (45.9784698486,58.7179810033), test loss: 35.7412334919\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.59878385067,70.9422994511), test loss: 3.19602302015\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (11.1180915833,53.4221251853), test loss: 41.9773237705\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.566712021828,48.4562381521), test loss: 3.73033487499\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.8626899719,50.7470128083), test loss: 38.4812057018\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (5.28840827942,37.2160809183), test loss: 3.61600563824\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (36.1396827698,48.9409116056), test loss: 43.2236183643\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.21693587303,30.4535078602), test loss: 3.56943354011\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (53.0308837891,47.6238636827), test loss: 40.6334705353\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.73681116104,25.9446102102), test loss: 3.51731606424\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (22.905456543,46.5995837822), test loss: 39.1961075306\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.07099437714,22.7128891277), test loss: 3.14281142652\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (30.6177501678,45.7013058368), test loss: 40.5474942923\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (5.21383476257,20.2821334518), test loss: 3.81129433513\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (22.2116928101,44.9214147282), test loss: 37.8712503433\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.92576599121,18.3884143648), test loss: 3.00413440466\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (35.6270370483,44.2440753816), test loss: 41.0349291325\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.96594190598,16.865762875), test loss: 3.49001784325\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (39.8803443909,43.6481204906), test loss: 34.909015131\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.63302350044,15.6193808781), test loss: 2.76577986181\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (64.6874084473,43.0498023202), test loss: 41.6829235792\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.69325184822,14.5762078639), test loss: 3.65035407841\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (29.4933414459,42.4795616962), test loss: 32.1999046326\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.99185991287,13.6902906265), test loss: 2.78751585484\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (107.08555603,41.9358302892), test loss: 37.6756560564\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (6.58363485336,12.9283019078), test loss: 3.24315774143\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (27.8980293274,41.3528257165), test loss: 28.0540476322\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.74412727356,12.262179235), test loss: 2.8696804285\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (41.6568450928,40.8321606197), test loss: 35.6459698677\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.61828708649,11.6768440032), test loss: 3.31542223096\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (21.3319664001,40.291891087), test loss: 33.2640294075\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.46753120422,11.156378586), test loss: 3.35757366717\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (33.8180465698,39.7797017701), test loss: 36.2564508915\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.17114067078,10.6928226515), test loss: 3.37597765923\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (2.94910097122,39.2532490096), test loss: 34.6364315987\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.76091980934,10.2745245222), test loss: 3.38613099456\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (8.96743583679,38.7424888385), test loss: 32.1738919258\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.30612397194,9.89535986679), test loss: 2.74363139868\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (32.4674377441,38.2254565262), test loss: 34.7144785404\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.724713146687,9.55009554929), test loss: 3.37133358121\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (36.8732299805,37.7109383597), test loss: 30.3713521957\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.49211299419,9.23374606796), test loss: 2.59201981127\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (24.9993476868,37.2196240308), test loss: 36.1821274519\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (5.43910884857,8.94246959912), test loss: 3.23044140339\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (20.2074661255,36.7461076823), test loss: 28.9475207329\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.25871992111,8.67422356725), test loss: 2.61348581314\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (22.9400901794,36.2796139349), test loss: 34.5155874729\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.8400914669,8.42561878377), test loss: 3.05630600452\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (14.3700904846,35.8244001536), test loss: 24.912916851\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.689457178116,8.19596181009), test loss: 2.6332464397\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (20.6076316833,35.3923712355), test loss: 32.7419933319\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.95002222061,7.98190802589), test loss: 3.10119840205\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (33.2994651794,34.9640615851), test loss: 24.1620827198\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.22567176819,7.78171026174), test loss: 2.83401319683\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (14.2850446701,34.5529885398), test loss: 33.4892656803\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.81399583817,7.5944012927), test loss: 3.17252813876\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (2.4607129097,34.166340019), test loss: 30.3562355042\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.57115101814,7.41787534024), test loss: 3.18054317534\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (13.4467926025,33.7941737412), test loss: 30.9812318802\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.42777609825,7.2526179571), test loss: 2.84610089064\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (7.81917762756,33.4287966354), test loss: 33.346322155\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.955008029938,7.09706964687), test loss: 3.3520550251\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (14.3941497803,33.0819109592), test loss: 30.5267456532\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.79179143906,6.95099542171), test loss: 2.62299738079\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (24.4378395081,32.7544938486), test loss: 35.1631865978\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.18383824825,6.81286517339), test loss: 3.33088556528\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (14.0766363144,32.4246796861), test loss: 28.831679678\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.11976122856,6.68175910689), test loss: 2.60331015289\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (9.85554599762,32.1188851877), test loss: 34.7188425064\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.07891082764,6.55723709442), test loss: 3.1573316142\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (49.9034614563,31.8225274543), test loss: 28.6809212685\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.88346862793,6.43896699949), test loss: 2.69496738613\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (27.506029129,31.5359644352), test loss: 33.5321867943\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.3864210844,6.32662609786), test loss: 3.07288036346\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.8492908478,31.2570902045), test loss: 25.8833965302\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.483721613884,6.21975478691), test loss: 2.58422335982\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (13.5674419403,30.9928468333), test loss: 33.5781995773\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.25424361229,6.11823073268), test loss: 3.06970767379\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (13.9521751404,30.7333972417), test loss: 30.6076315165\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.773230969906,6.02106240593), test loss: 3.06739860177\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (18.280216217,30.480643926), test loss: 33.0751748085\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.16019868851,5.92814802072), test loss: 3.10037689209\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (12.9131803513,30.241519807), test loss: 29.6167198181\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.47806072235,5.83916513344), test loss: 3.13209031224\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (5.87284755707,30.0091242103), test loss: 30.3763937473\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.33913779259,5.75374561947), test loss: 2.72802331746\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (23.7991104126,29.7815560601), test loss: 31.9946748257\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.98917901516,5.67171698836), test loss: 3.30259725749\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (22.2324638367,29.561021362), test loss: 30.9784824371\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.29103755951,5.59365860327), test loss: 2.58600387573\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (25.4880142212,29.349418002), test loss: 32.0697621584\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.2774245739,5.51849435501), test loss: 2.99699038863\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (24.7945671082,29.1381427167), test loss: 31.0017591476\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.41809129715,5.44618860191), test loss: 2.56456434727\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (19.7009277344,28.9336464172), test loss: 33.173728466\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.21682584286,5.37652257665), test loss: 2.99602172971\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (8.48047161102,28.7389004036), test loss: 28.8681987762\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.32729530334,5.30889109774), test loss: 2.75720514059\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (14.7313556671,28.5473317793), test loss: 32.5406610012\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.43101119995,5.24412730864), test loss: 2.91792471707\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (9.97698974609,28.3558341825), test loss: 26.1085828304\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.58795106411,5.18122961038), test loss: 2.61136526167\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (25.4514274597,28.1734272463), test loss: 34.847268486\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.21078765392,5.12119652582), test loss: 2.97688102424\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (36.825881958,27.9976968927), test loss: 30.7560242176\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.13917922974,5.06311823043), test loss: 3.01251032054\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (48.5213851929,27.8194658937), test loss: 34.5595073462\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.66565442085,5.00672515275), test loss: 2.96591542661\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (6.14412212372,27.647906632), test loss: 30.8945374012\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.79501521587,4.95197647096), test loss: 3.0922031939\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (8.05576133728,27.4814606327), test loss: 32.284378767\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.02967643738,4.89895606072), test loss: 2.59727312326\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.5673141479,27.3173285563), test loss: 31.5529200077\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.38278341293,4.84767174837), test loss: 3.15699913502\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (12.8507213593,27.1541492444), test loss: 31.6850818157\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.779753684998,4.79773978488), test loss: 2.43144589067\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (9.53195095062,26.997449812), test loss: 33.9009752989\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.91070663929,4.74980405207), test loss: 2.97051517665\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.6666660309,26.8424764693), test loss: 31.5782378674\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.61868882179,4.70311448631), test loss: 2.70402904451\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (15.4036808014,26.6892608997), test loss: 33.6152967691\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.06881761551,4.65764054999), test loss: 2.91704010218\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (45.5506019592,26.5420003879), test loss: 26.930761528\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.23896217346,4.61341123976), test loss: 2.583348912\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (13.3072376251,26.3952896395), test loss: 35.1427924156\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.55763030052,4.5702958187), test loss: 2.84465168267\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (22.0822944641,26.2510562809), test loss: 26.0838269711\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.65727639198,4.52843983655), test loss: 2.61142958999\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.1309690475,26.1086367788), test loss: 33.8747242689\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.26900243759,4.48779434215), test loss: 3.01417051852\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (10.5345954895,25.9712220458), test loss: 30.0130549908\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.640540003777,4.44832777927), test loss: 3.00278155804\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (5.95701789856,25.8322422836), test loss: 35.0537923336\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.87086105347,4.40988189296), test loss: 3.02663049102\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (4.28012943268,25.6972562022), test loss: 30.5482977867\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.25812137127,4.37232832212), test loss: 3.00762408078\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (6.81280183792,25.566001085), test loss: 33.3741394997\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.34325361252,4.3354522134), test loss: 2.64570816904\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (5.25895023346,25.4360190203), test loss: 32.5151225567\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.297538548708,4.29962140998), test loss: 3.08043697476\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (14.0599927902,25.3048955566), test loss: 30.5942294598\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.14468383789,4.26462823646), test loss: 2.35384878367\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (10.7988672256,25.1788361172), test loss: 34.3642516375\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.35835289955,4.23067441443), test loss: 2.97203692198\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (10.6611566544,25.0559722068), test loss: 29.6127403259\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.15930497646,4.19761814907), test loss: 2.58072798401\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (12.4570808411,24.9304479232), test loss: 35.9240034103\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.70428287983,4.16513175456), test loss: 2.76274287701\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.1857557297,24.8097022413), test loss: 26.4826039314\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.9266242981,4.1334106366), test loss: 2.60846392512\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (35.6479644775,24.6908987912), test loss: 34.0649008751\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.19386577606,4.1022187509), test loss: 2.81923651397\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (25.8111248016,24.5720102295), test loss: 25.2604605675\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.54674482346,4.0717855533), test loss: 2.6460771054\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (19.4367980957,24.4538267111), test loss: 34.8245635509\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.63714849949,4.04202879917), test loss: 2.94071445167\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.0818119049,24.3388266538), test loss: 29.0018451691\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.81445145607,4.01301016882), test loss: 2.83699521124\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (24.8089561462,24.2255039455), test loss: 34.3930913925\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.40348100662,3.98473482244), test loss: 2.75344335586\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (25.1067504883,24.1124770108), test loss: 32.138032937\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (4.1401848793,3.95691160062), test loss: 3.07524817288\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (19.8230361938,24.0027520229), test loss: 32.7080332279\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.05113399029,3.92951612917), test loss: 2.4990801394\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (19.2549858093,23.8924161902), test loss: 34.0334204674\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (3.03203678131,3.90273629763), test loss: 2.95184858441\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (24.5539054871,23.7834550935), test loss: 31.4539967537\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (4.62360715866,3.87647439282), test loss: 2.53287213743\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (20.3545608521,23.6755588457), test loss: 36.5120992661\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.51023244858,3.85082566586), test loss: 2.98955798745\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (13.3154716492,23.5700941876), test loss: 28.96361866\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.26437950134,3.82557554376), test loss: 2.5834115833\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (25.7719955444,23.4645590061), test loss: 35.8414404869\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.890293598175,3.80103932558), test loss: 2.76408279091\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (16.3705921173,23.3604539354), test loss: 26.9591944218\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.376628398895,3.77679135944), test loss: 2.47952633798\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (7.52539634705,23.2586172991), test loss: 35.5475032806\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.19111061096,3.75285950756), test loss: 2.92025654539\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (13.5139751434,23.1572043377), test loss: 30.238691926\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.31283962727,3.72949192224), test loss: 2.87010164857\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (20.5525379181,23.0550914066), test loss: 34.6276372433\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.8179666996,3.70642446426), test loss: 2.9093630746\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (4.87929534912,22.9553981169), test loss: 28.8051948309\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.30454730988,3.68404420974), test loss: 2.81794797182\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (6.70605325699,22.8578934083), test loss: 32.9088430405\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.41405534744,3.66193957335), test loss: 2.7345325619\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.9852199554,22.7592609228), test loss: 31.9033145189\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.16637301445,3.6403154723), test loss: 3.00330499709\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (7.82692050934,22.6625192578), test loss: 33.4035945415\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.7648422718,3.61893151137), test loss: 2.49926240593\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (22.7175598145,22.5679103559), test loss: 34.0097890854\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.517116189,3.59781186679), test loss: 2.87054120302\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (13.4297266006,22.4720098384), test loss: 32.9230064392\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.21621727943,3.57714258643), test loss: 2.51720055342\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (19.8734455109,22.3769342704), test loss: 35.3061330318\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.47411966324,3.556714828), test loss: 2.84925802201\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (10.7372922897,22.2835114356), test loss: 29.4758786678\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.589737892151,3.53678428191), test loss: 2.63093877733\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (33.472026825,22.191792929), test loss: 42.2637814522\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.67809867859,3.51723588966), test loss: 3.20930259824\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (8.40497016907,22.0995265553), test loss: 27.2732860565\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.522607028484,3.4979265645), test loss: 2.56348228455\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (10.222410202,22.0089183769), test loss: 34.4256023884\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.58791196346,3.47876573454), test loss: 2.81739965528\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (9.52032852173,21.9184971571), test loss: 29.5939870358\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.45495319366,3.46002689098), test loss: 2.75713835061\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (13.1979026794,21.8284217436), test loss: 36.4153205872\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.68165683746,3.44155339707), test loss: 2.93552483618\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (6.47042751312,21.7387125575), test loss: 35.0375475407\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.69661206007,3.4233185566), test loss: 3.02222123295\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (5.85422849655,21.6511105548), test loss: 35.2131126404\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.51092362404,3.40544584164), test loss: 2.68321375549\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (11.5426959991,21.5631104176), test loss: 31.2656824589\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.967348396778,3.38787582197), test loss: 3.00977166295\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (16.4854469299,21.4768308304), test loss: 33.5269165039\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.71524548531,3.37056369806), test loss: 2.41959929615\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (20.5338840485,21.3906838884), test loss: 35.0428064346\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (4.20693016052,3.35332647847), test loss: 2.92474318743\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (16.9198055267,21.3052361389), test loss: 32.6968506336\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (3.77578163147,3.33645717919), test loss: 2.60200209767\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (21.946975708,21.2194224458), test loss: 36.6777044058\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.19881916046,3.31969034455), test loss: 2.85162305385\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (15.5630578995,21.1344319388), test loss: 28.5064593315\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.66009366512,3.30332624671), test loss: 2.58610170111\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (11.2436580658,21.051588879), test loss: 35.9775125265\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.26965391636,3.28721063133), test loss: 2.7642931208\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (9.64709854126,20.9674400918), test loss: 27.4704711437\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.55904912949,3.27129391327), test loss: 2.60040686727\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (14.1021137238,20.8853157499), test loss: 35.518474865\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.021474123,3.25560676339), test loss: 2.93207053393\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.91766166687,20.8034669038), test loss: 30.2631196022\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.426787257195,3.23992683194), test loss: 2.85374453664\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (18.1313533783,20.721347049), test loss: 37.5846246243\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.01230216026,3.22460424612), test loss: 2.92398103476\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (2.84572553635,20.6394687684), test loss: 30.383302331\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.527316987514,3.2094089313), test loss: 2.84285777211\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (9.90997982025,20.5587004132), test loss: 34.7716014862\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.13279438019,3.19450621479), test loss: 2.6000848338\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.5361690521,20.4792791366), test loss: 38.1286942482\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.297315716743,3.1798428948), test loss: 3.3904342711\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (8.62520694733,20.3990550032), test loss: 32.8500323296\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.65984630585,3.16531144809), test loss: 2.40333878472\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (5.57918548584,20.3204758943), test loss: 36.116991663\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.88779258728,3.15090873522), test loss: 2.91800633371\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (20.0275306702,20.2416655791), test loss: 31.3371004105\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.56502199173,3.13666178427), test loss: 2.6994674921\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (13.4798879623,20.1627863595), test loss: 37.6157930136\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.47343230247,3.12262079994), test loss: 2.77859022841\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (16.9217414856,20.0846698835), test loss: 28.7320791245\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.740764975548,3.10875194092), test loss: 2.55781438053\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (7.74742507935,20.0073002459), test loss: 37.0574653625\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.77836298943,3.0950579561), test loss: 2.78166579008\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (6.44243717194,19.9303152988), test loss: 26.595937109\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.473714947701,3.08160265249), test loss: 2.66164882183\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (7.6771273613,19.8542282575), test loss: 36.8371391535\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.36413764954,3.06824572302), test loss: 2.92881832123\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (11.303653717,19.7784563681), test loss: 29.5962149382\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.33280348778,3.05502150538), test loss: 2.77572949529\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (4.78046512604,19.7025811216), test loss: 38.0688349247\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.469304263592,3.04191246915), test loss: 2.84346078932\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (7.92956876755,19.6267574793), test loss: 31.8639679432\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.38651931286,3.0289624325), test loss: 2.92008066177\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (21.0354785919,19.5518975845), test loss: 35.4152832985\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.86156654358,3.01625448772), test loss: 2.63508942127\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (10.0422029495,19.4777327107), test loss: 36.3106320143\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.245532542467,3.00360757691), test loss: 2.96358699948\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (7.05862236023,19.4032965337), test loss: 33.7888118267\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.941293001175,2.99122995092), test loss: 2.59558154792\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (11.0678081512,19.329984261), test loss: 37.4138206005\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.731504559517,2.97888279978), test loss: 3.00984177589\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (5.12191438675,19.2572060707), test loss: 30.1977108002\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.40869617462,2.96662020667), test loss: 2.66700443625\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (11.2425785065,19.1840883109), test loss: 37.8642889738\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.03359127045,2.95452409673), test loss: 2.77971625328\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (4.6339225769,19.1108594443), test loss: 28.7516816616\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.426964938641,2.9425331076), test loss: 2.48338137567\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (10.6811542511,19.038809305), test loss: 38.6850265265\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.64679145813,2.9307529085), test loss: 2.82432480454\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (6.50586223602,18.9675434442), test loss: 30.5320626736\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.10978007317,2.91907883228), test loss: 2.95740901828\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (9.31681442261,18.8960110411), test loss: 36.8517390251\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.543618679047,2.90757869736), test loss: 2.95389994532\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (6.2522482872,18.825134944), test loss: 29.3541723728\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.45274651051,2.89606099304), test loss: 2.7288615644\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (3.91981887817,18.7548434542), test loss: 35.9636643887\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.862292170525,2.88473905783), test loss: 2.71989646703\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (11.1318225861,18.6843378639), test loss: 33.1540330172\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.50756847858,2.8734741235), test loss: 2.91684926152\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.09712600708,18.6139891658), test loss: 38.2170879841\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.467584252357,2.86234534812), test loss: 2.70745761544\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.0454416275,18.544429703), test loss: 35.8767860413\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.651806116104,2.85135825498), test loss: 2.85069017708\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (15.5560073853,18.4755249777), test loss: 34.7886045456\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.74879705906,2.84051816973), test loss: 2.72598387897\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.60309934616,18.4069989754), test loss: 37.4355618477\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.09552979469,2.82976276374), test loss: 2.89329507649\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (6.5632648468,18.3385533388), test loss: 31.0382224083\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.05131220818,2.81902666819), test loss: 2.7329480052\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.40677165985,18.2706045416), test loss: 38.4926746845\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.64752805233,2.80846849774), test loss: 2.88905321062\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (14.5884513855,18.2026046482), test loss: 29.7785130739\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.97499656677,2.79795031368), test loss: 2.66368827224\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.07279348373,18.134962814), test loss: 36.7484320641\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.02648675442,2.78760981313), test loss: 2.89100062549\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.21722984314,18.0680634994), test loss: 31.777383852\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.420759081841,2.77733128219), test loss: 2.72717019916\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (5.83605575562,18.0012985077), test loss: 37.59494133\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.31635499001,2.76720936082), test loss: 2.91639281809\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (4.53483915329,17.9354670787), test loss: 31.6972794056\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.73792552948,2.75715208928), test loss: 2.89015482068\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (6.19167137146,17.8696079942), test loss: 36.713124752\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.2438185215,2.7470865421), test loss: 2.76761009023\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (8.13541603088,17.8041215206), test loss: 33.6256843567\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.458800077438,2.73722231557), test loss: 3.05179033279\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (5.0655207634,17.7382396507), test loss: 35.5679748535\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.8496260643,2.72735128999), test loss: 2.50457457602\n",
      "\n",
      "MC # 1, Hype # hyp5, Fold # 7\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold7/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (340.61026001,inf), test loss: 198.407148743\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (307.707702637,inf), test loss: 345.649601746\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (22.5416641235,78.2233921075), test loss: 43.5050322533\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.92117214203,27.0479031614), test loss: 3.35484783649\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (20.7844982147,61.880521744), test loss: 36.3519142151\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.71884393692,14.9572900577), test loss: 2.59481167197\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (13.8647594452,56.1971464445), test loss: 41.5737896919\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.89709997177,10.928733136), test loss: 3.52711181641\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.1203136444,53.3570706989), test loss: 39.2993581772\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.35047340393,8.92064969909), test loss: 3.17350856662\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (30.1814479828,51.5640603594), test loss: 41.6640860558\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.33021879196,7.7179224853), test loss: 3.41444570124\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (26.7052631378,50.2727859556), test loss: 39.4125750542\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.87484288216,6.90784588925), test loss: 3.47696956396\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (42.7299461365,49.3071181372), test loss: 43.137649107\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.70805954933,6.32422418174), test loss: 3.25492464304\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (56.6541099548,48.5368788593), test loss: 40.2152573586\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.28167915344,5.87991483778), test loss: 3.32597669363\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (86.7945251465,47.9368900273), test loss: 38.6249830246\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.87608385086,5.53400403464), test loss: 2.70010253489\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (16.9855041504,47.3839114362), test loss: 41.0586764812\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.47166109085,5.25669400895), test loss: 3.29036016464\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (48.8020935059,46.9203031577), test loss: 40.2291043282\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.92759132385,5.02766434112), test loss: 2.54611661434\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (85.9226074219,46.4684843216), test loss: 43.5418997288\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.94527649879,4.83461969557), test loss: 3.61341216564\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (66.3956604004,46.052949978), test loss: 38.9306758881\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.1675992012,4.66859179178), test loss: 2.4044321537\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (24.7727088928,45.6630390507), test loss: 41.4638424873\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (6.06028699875,4.52470116608), test loss: 3.45305099487\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (25.5584182739,45.2998590108), test loss: 35.923990345\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.67011284828,4.39959817808), test loss: 2.38426442444\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (33.9006462097,44.9535743009), test loss: 40.6556813717\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.44809007645,4.2893789985), test loss: 3.27807308733\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (23.2653045654,44.5853680817), test loss: 34.8211647034\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.545219600201,4.18941484965), test loss: 2.58339983523\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (52.0397720337,44.2259808792), test loss: 40.7224206924\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.82677865028,4.09769786675), test loss: 3.24503584504\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (14.9616756439,43.8616444566), test loss: 33.7171973467\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.65839385986,4.01297838543), test loss: 2.46133867502\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (17.8785686493,43.5231565529), test loss: 38.4931177139\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.4119989872,3.93626820276), test loss: 3.18653065115\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (102.233474731,43.1633108373), test loss: 28.903692627\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.06978464127,3.86599668504), test loss: 2.4019787848\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (45.9457817078,42.7927332545), test loss: 35.7968974113\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.37797772884,3.80033977876), test loss: 3.04721725285\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (94.2484359741,42.4007507859), test loss: 27.0722588062\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.49086141586,3.73876592193), test loss: 2.80405815244\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (30.2191333771,41.9985241968), test loss: 34.8401540041\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (6.56924724579,3.68150982069), test loss: 3.05992636979\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (20.6733703613,41.6080244473), test loss: 30.8180989265\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.20129609108,3.62735085102), test loss: 3.1363859266\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (42.6160202026,41.1696106172), test loss: 33.9898125172\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.76718556881,3.57752579783), test loss: 3.17140609026\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (19.5727729797,40.6918854893), test loss: 30.2680467606\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.62436151505,3.53073598557), test loss: 3.17021777928\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (10.5369329453,40.1954956015), test loss: 28.4951305628\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.1329972744,3.48656702154), test loss: 2.58260877281\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (18.7200317383,39.7029367181), test loss: 30.0190393448\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.91598320007,3.44420785882), test loss: 3.24774197638\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (18.2806396484,39.2144591122), test loss: 28.8583682537\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.77290344238,3.40337471416), test loss: 2.37611907423\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (64.76512146,38.7397026488), test loss: 32.1507686377\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.00122213364,3.36563541478), test loss: 3.42998216152\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (7.80558156967,38.265439632), test loss: 27.8911715984\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.12891054153,3.3301881999), test loss: 2.22401088476\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (40.602645874,37.804439261), test loss: 32.3837778091\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.87194788456,3.29636300005), test loss: 3.26893574744\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (15.4225006104,37.3428446172), test loss: 26.4330796719\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.51164126396,3.26335315808), test loss: 2.25502607375\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (13.1148786545,36.9063871011), test loss: 31.9708536863\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.96607232094,3.23173400123), test loss: 3.17321686149\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (26.7327232361,36.485791435), test loss: 26.3537234306\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.82266545296,3.20174223987), test loss: 2.44687940329\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (19.705871582,36.0678679475), test loss: 33.6101092339\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.24632072449,3.17310320691), test loss: 3.23503401875\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (8.77564334869,35.6731295798), test loss: 25.3030865669\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.585079550743,3.14593630282), test loss: 2.53662313819\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (20.0001602173,35.2826224184), test loss: 32.6660556078\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.26421606541,3.11969869661), test loss: 3.14291266352\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (16.3598022461,34.9117810677), test loss: 22.730716753\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.66761064529,3.09399745764), test loss: 2.450798738\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (6.24247360229,34.5527730002), test loss: 32.4370703697\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.23603689671,3.06916119577), test loss: 2.99122406691\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (23.6413002014,34.202538894), test loss: 21.5182552814\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.89989078045,3.04536935202), test loss: 2.47404781878\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (12.4284267426,33.8677240169), test loss: 32.485439682\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.28996419907,3.02276329457), test loss: 3.10990879536\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (39.6748580933,33.5437422823), test loss: 27.9402528763\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.86396980286,3.00099401151), test loss: 3.0310023129\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (22.8341712952,33.2262936288), test loss: 32.4260227203\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.09725260735,2.97951093573), test loss: 3.11526107192\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (17.6789340973,32.9255877962), test loss: 27.116693902\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.11387777328,2.95843579573), test loss: 3.04727541208\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (48.9524993896,32.6327048619), test loss: 27.8822466612\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.65552425385,2.93832469238), test loss: 2.48616831303\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (13.1147346497,32.3437092128), test loss: 28.3892067432\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.92260015011,2.91874017746), test loss: 3.15038767159\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.8005409241,32.0677849024), test loss: 28.3055418015\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.518716096878,2.89998753935), test loss: 2.34868764281\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (4.76909351349,31.7956440606), test loss: 30.1893582821\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.831794559956,2.88151792376), test loss: 3.30192774832\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (15.8596057892,31.5349483293), test loss: 27.294550705\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.26642131805,2.86346651157), test loss: 2.21924605668\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (6.0132818222,31.283661328), test loss: 31.9845518112\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.298045754433,2.84566930115), test loss: 3.19457091093\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (8.00095748901,31.0324303695), test loss: 27.7131376743\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.05165803432,2.8285951639), test loss: 2.25187099278\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.2634515762,30.7930512489), test loss: 32.0566072464\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.10979247093,2.81217134605), test loss: 3.08089464605\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (30.211271286,30.5580356926), test loss: 28.5827765942\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.05760025978,2.79606990517), test loss: 2.37915951312\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (11.7738628387,30.3288160136), test loss: 32.9869574785\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.475261986256,2.78022021189), test loss: 3.17092028856\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (35.2632141113,30.1079383074), test loss: 25.6183346748\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.06191325188,2.76446099003), test loss: 2.51386713386\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (17.457452774,29.889664113), test loss: 33.1873240948\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.57668089867,2.74931800668), test loss: 3.04160137326\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (9.35769271851,29.6752657384), test loss: 23.5623106718\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.00451469421,2.73459465696), test loss: 2.39826506972\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.9515514374,29.4700707511), test loss: 31.1277076721\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.29805088043,2.72035501878), test loss: 2.90299200416\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (33.2327613831,29.2653554221), test loss: 21.5937668562\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.38854503632,2.70623474758), test loss: 2.32693119943\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (7.08733940125,29.0682921135), test loss: 31.9840588093\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.09316647053,2.69213532534), test loss: 3.02396195233\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.6943035126,28.8759521658), test loss: 26.8707927704\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.987330079079,2.6785161489), test loss: 2.81100837886\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (17.672410965,28.6829082207), test loss: 33.85116992\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.75627732277,2.66517356326), test loss: 3.02185036093\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (8.25022888184,28.4980462669), test loss: 26.231231451\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.48972964287,2.65220344349), test loss: 2.94616280198\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.7323417664,28.3152737318), test loss: 27.5034967661\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.594438552856,2.63964839868), test loss: 2.46864885986\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (8.08148765564,28.1380036735), test loss: 28.1087357283\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.54792261124,2.62700413193), test loss: 3.0410194546\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (14.2637071609,27.9634976223), test loss: 29.6661314964\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.09832715988,2.61452412381), test loss: 2.28411283791\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (13.9334001541,27.7902077302), test loss: 30.002494669\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.89725613594,2.60231901505), test loss: 3.1480504185\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.9878807068,27.6206865749), test loss: 28.5409639359\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.55627346039,2.59051798511), test loss: 2.28790243864\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.7682981491,27.4570395192), test loss: 30.8160613537\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.75397849083,2.57904167575), test loss: 3.0690102458\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (12.0561847687,27.2938882468), test loss: 28.7416574955\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.549793183804,2.56757147725), test loss: 2.22772372365\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (9.54583454132,27.1361078506), test loss: 31.8124302864\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.20909059048,2.5561623569), test loss: 2.98879567087\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (9.99986457825,26.9791258415), test loss: 27.0720716953\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.15217685699,2.54497440837), test loss: 2.30663805008\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (21.2853393555,26.8229263657), test loss: 35.5355595112\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.3150241375,2.5339957671), test loss: 3.12978557348\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (17.0482635498,26.6729080789), test loss: 25.743755722\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.634813666344,2.5233621766), test loss: 2.42858936787\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (18.0126991272,26.5231943236), test loss: 33.6123252153\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.49642562866,2.51288060366), test loss: 2.98170036077\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (11.2386302948,26.3775218173), test loss: 25.0448321342\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.353354752064,2.5023830881), test loss: 2.45656756759\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.7431602478,26.2334437156), test loss: 31.2308307171\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (4.46371936798,2.49202040686), test loss: 2.8057418108\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.4037971497,26.0879017949), test loss: 21.9367455482\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.714561104774,2.48174805743), test loss: 2.26460133493\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (6.30493545532,25.9487877647), test loss: 31.9557202816\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.299588322639,2.47182287198), test loss: 2.90613889694\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (16.4621677399,25.8116794891), test loss: 26.3919331074\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.39879155159,2.46218599499), test loss: 2.74296947122\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.42536640167,25.675786385), test loss: 32.0891306877\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.20380938053,2.45253656342), test loss: 2.97413898781\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (6.91985368729,25.5422182323), test loss: 25.7699237108\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.28569471836,2.44282300489), test loss: 2.78899366856\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (24.3412017822,25.4094142561), test loss: 30.9121377468\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.65444564819,2.43337393358), test loss: 2.57735395432\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (24.5891780853,25.2778093116), test loss: 28.6688493252\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.58932471275,2.42403070543), test loss: 2.98916567266\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (13.8329086304,25.1515349348), test loss: 28.8221411705\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.27014613152,2.41501344495), test loss: 2.42252005041\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.3094902039,25.0236463021), test loss: 30.2178051949\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.77113622427,2.40596802375), test loss: 3.13086749017\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (13.4083433151,24.8988141154), test loss: 28.8151345253\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.17072868347,2.39693424983), test loss: 2.31944172084\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (10.2686634064,24.775217578), test loss: 30.5278204918\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.3857858181,2.3880191158), test loss: 3.01595590413\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (8.17712306976,24.651095174), test loss: 30.641813612\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.4208739996,2.37925369562), test loss: 2.23164207041\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (3.32763648033,24.5316461804), test loss: 31.3844186783\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.865377187729,2.37063098607), test loss: 2.92094446123\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (20.939447403,24.4129018654), test loss: 28.1789262295\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.12734460831,2.36231332931), test loss: 2.38190333843\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (12.6254768372,24.2953300634), test loss: 33.8215259075\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.978604912758,2.35384939744), test loss: 3.00389918089\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (10.7062072754,24.1790487281), test loss: 25.93288517\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.63701248169,2.34546354182), test loss: 2.4169510141\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (20.0179824829,24.0630664817), test loss: 33.1160612583\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.570532500744,2.33716290779), test loss: 2.82312491536\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (17.9822597504,23.9495687379), test loss: 25.1364703178\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.32605409622,2.32907494722), test loss: 2.49591389596\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (36.567035675,23.8387766903), test loss: 32.9235499144\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.74012613297,2.32121703739), test loss: 2.79793460369\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (12.1577625275,23.7268270839), test loss: 22.5557708263\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.873813331127,2.31322552106), test loss: 2.2870441556\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (6.22666931152,23.6170394135), test loss: 31.3563925266\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.53819227219,2.3052736914), test loss: 2.83398229927\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.28312301636,23.5081538011), test loss: 26.7182450294\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.63664865494,2.29751178282), test loss: 2.65682519078\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (8.31092643738,23.3996366686), test loss: 33.315993166\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.04673671722,2.28977315289), test loss: 2.89948880076\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (7.95496606827,23.2944749855), test loss: 25.6923265934\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.366736590862,2.28221167954), test loss: 2.75392711759\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (13.8601102829,23.1884811612), test loss: 32.4625577927\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.33012270927,2.27480103903), test loss: 2.80944577605\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (15.1372346878,23.0846626518), test loss: 27.4354054451\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.53834319115,2.26723759942), test loss: 2.93118881285\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (7.66164016724,22.9810852763), test loss: 31.108533287\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.313341975212,2.25983355635), test loss: 2.51362124234\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (25.5540046692,22.8774357937), test loss: 29.2740990162\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.07108664513,2.25246671084), test loss: 2.98278186321\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (9.79862785339,22.7766211798), test loss: 29.4703910112\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.837964832783,2.24523557899), test loss: 2.34017234445\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (8.9946231842,22.6766123054), test loss: 30.303576076\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.695647358894,2.23816770703), test loss: 2.93826927841\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (2.6485440731,22.576885941), test loss: 30.4783875465\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.12007880211,2.23112160737), test loss: 2.28601154089\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (21.7624835968,22.4786566253), test loss: 31.7732722759\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.04657423496,2.22400176497), test loss: 2.87829413414\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (14.5103845596,22.3805854099), test loss: 28.4281162739\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.12530374527,2.21702436284), test loss: 2.42796129137\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (6.85416221619,22.2835985202), test loss: 33.7499529839\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.887218594551,2.21011660579), test loss: 2.91026442051\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (3.82464671135,22.1884398158), test loss: 26.0619281769\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.627916455269,2.20332523097), test loss: 2.44367070869\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (8.08472061157,22.0925615031), test loss: 32.8706880569\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.62885642052,2.19657642102), test loss: 2.84770461321\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (12.6475133896,21.9983903122), test loss: 26.3857628822\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.807775616646,2.18981846064), test loss: 2.54848804772\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (6.75830221176,21.9050626613), test loss: 32.0399098635\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.424109041691,2.18311452058), test loss: 2.73705117628\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (3.52344512939,21.8114203764), test loss: 23.3416926861\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.788117289543,2.17650333719), test loss: 2.31160625666\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (6.94078445435,21.7197604932), test loss: 32.6122133732\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.94793713093,2.16994460903), test loss: 2.80429219306\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (6.80023860931,21.6281111704), test loss: 27.0290827513\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.98440861702,2.16351258304), test loss: 2.6648355931\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (10.3825492859,21.5377068498), test loss: 32.1922913551\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.42145681381,2.15709295136), test loss: 2.86771108508\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (16.3241558075,21.448049383), test loss: 25.9391602039\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.89173746109,2.15067023211), test loss: 2.83070971668\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (10.05900383,21.3587300499), test loss: 33.3277523994\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.14719200134,2.14431399452), test loss: 2.83198057711\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (3.31049108505,21.2698368321), test loss: 27.1837143421\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.28236293793,2.13801786979), test loss: 2.90200341344\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (5.33712482452,21.1826282697), test loss: 31.0346450329\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.134040907025,2.13181188209), test loss: 2.61154867709\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (7.42900562286,21.0949863207), test loss: 29.2068717957\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.49124264717,2.12565068482), test loss: 2.97058953047\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (4.47860145569,21.008484796), test loss: 31.7667444229\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.904872179031,2.11946574946), test loss: 2.40776408315\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (15.4894351959,20.9229372663), test loss: 30.8684284687\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.771325707436,2.11338987912), test loss: 2.97782151401\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (11.4401102066,20.8370793938), test loss: 33.4112215042\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.614229917526,2.10732592282), test loss: 2.32825353295\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (5.40172767639,20.7524449186), test loss: 32.261102438\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.780858933926,2.10128300767), test loss: 2.8759891659\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (12.3199872971,20.6680584563), test loss: 29.1484245539\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.847221970558,2.09543822289), test loss: 2.44569008946\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (5.37098789215,20.5848103812), test loss: 34.0352526665\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.4051322937,2.08947990458), test loss: 2.85835927427\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (5.36488056183,20.5021064205), test loss: 28.5634269238\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.519131302834,2.08361312708), test loss: 2.50590046942\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (22.6316738129,20.4194254334), test loss: 33.590380168\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.884372770786,2.0777630614), test loss: 2.74159971178\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (8.803606987,20.3373368435), test loss: 26.3806622505\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.75048041344,2.07196409022), test loss: 2.51531387568\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (10.7366580963,20.2562596208), test loss: 33.1232357979\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.77625143528,2.06628252238), test loss: 2.69583047181\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (5.68683815002,20.1754360035), test loss: 24.1894114017\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.389252394438,2.06062853586), test loss: 2.34709853083\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (9.98613357544,20.0957396284), test loss: 31.9778229713\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.58620715141,2.05493166395), test loss: 2.77934395969\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (7.68520355225,20.0160459195), test loss: 28.2352840424\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.772772014141,2.04929038022), test loss: 2.76734318733\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (14.8126564026,19.9363459154), test loss: 33.0680063486\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.09812664986,2.0436949014), test loss: 2.87844999135\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (5.2160487175,19.8579237704), test loss: 26.2640390873\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.11398267746,2.0381352875), test loss: 2.78598015606\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (8.25151729584,19.7794720433), test loss: 33.4373079062\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.03971910477,2.0327012593), test loss: 2.88749435842\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (11.3239498138,19.7022421869), test loss: 28.078387928\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.718271911144,2.02719767528), test loss: 2.92672198713\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (10.7688360214,19.6254897024), test loss: 30.5099112988\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.68829107285,2.02175454424), test loss: 2.53592614532\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.03605937958,19.5479047428), test loss: 30.8946885347\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.01858067513,2.01632718058), test loss: 2.92779810429\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.80413389206,19.4717569427), test loss: 32.6625198364\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.634166121483,2.01091076336), test loss: 2.42755010426\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.24302482605,19.3962298913), test loss: 29.466449213\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.81290483475,2.00569180061), test loss: 2.92486251593\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (10.2859697342,19.3211447879), test loss: 31.0645661592\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.63676381111,2.0004127215), test loss: 2.31362397149\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.5206990242,19.2468422382), test loss: 34.0655668259\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.15080416203,1.99513075386), test loss: 2.86736319065\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (8.0054473877,19.1723354204), test loss: 28.5714152575\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.55120444298,1.98989305664), test loss: 2.38210935593\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (8.90257263184,19.0979456808), test loss: 34.2606251001\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.66458773613,1.98466637243), test loss: 2.89881550372\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (12.2313318253,19.0251323348), test loss: 27.9492456913\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.882255315781,1.97953934034), test loss: 2.5210534513\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.94486761093,18.952068533), test loss: 34.3020986557\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.70148652792,1.97444804094), test loss: 2.86779360622\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (13.4895915985,18.8800167101), test loss: 27.2950559139\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.91469192505,1.9693331477), test loss: 2.52751915604\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (9.16535663605,18.8080440361), test loss: 33.4134467125\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.07974910736,1.96425645048), test loss: 2.7131944567\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.51388168335,18.7358497849), test loss: 25.5979547977\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.0699955225,1.95918952912), test loss: 2.38910563737\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.41168689728,18.664757506), test loss: 32.6507560253\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.625057518482,1.95416119864), test loss: 2.71968099773\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (7.57915306091,18.5940584311), test loss: 27.8110502243\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.338063180447,1.94930157498), test loss: 2.81085458696\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (7.53505325317,18.5242282899), test loss: 32.8194465637\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.24391150475,1.94434198224), test loss: 2.85836803168\n",
      "\n",
      "MC # 1, Hype # hyp5, Fold # 8\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold8/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (356.536895752,inf), test loss: 185.191178131\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (331.47668457,inf), test loss: 396.98150177\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (63.3342590332,96.5720136361), test loss: 43.3270641327\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.23512506485,57.4629018673), test loss: 3.49171306193\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (48.5721054077,72.6746387565), test loss: 34.557345438\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.03032803535,30.2859374749), test loss: 2.89295694828\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (10.3791456223,64.6171856682), test loss: 41.625177002\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.96450090408,21.2377323405), test loss: 3.6630992949\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (42.1087646484,60.6586543745), test loss: 38.6952011108\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.65574085712,16.7163385065), test loss: 3.50708819777\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (11.6422538757,58.1547760219), test loss: 40.6934082985\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.74772918224,14.0066189484), test loss: 3.69271749258\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (36.6059036255,56.4321728577), test loss: 42.1800817013\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (4.7467455864,12.1986542894), test loss: 3.75073330998\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (110.47177124,55.1636949529), test loss: 37.3482017517\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (7.09734535217,10.9061130595), test loss: 2.97653575242\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (97.321105957,54.1143424745), test loss: 45.744307518\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.95509004593,9.93258635714), test loss: 3.95018953085\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (62.6323623657,53.3380352481), test loss: 38.6295247078\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.26756477356,9.17315855137), test loss: 2.76974369586\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (25.3848724365,52.6593424347), test loss: 45.7821037292\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.08058071136,8.56236304421), test loss: 3.9103413105\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (91.990020752,52.1146532759), test loss: 34.3769127369\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.48558473587,8.06372958394), test loss: 2.85858257115\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (112.854324341,51.6282014589), test loss: 43.6954767227\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.82616615295,7.65028710296), test loss: 3.78575926423\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (35.7816429138,51.1623698641), test loss: 32.9813458443\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.99970555305,7.29760248926), test loss: 3.00563490391\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (45.8687019348,50.7368974806), test loss: 41.7174001694\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.10632610321,6.99510903477), test loss: 3.38984777927\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (49.5850830078,50.32731867), test loss: 32.1320755243\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.27283191681,6.73204159912), test loss: 2.66671847403\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (18.1684150696,49.9337601877), test loss: 39.314508009\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.71157932281,6.49936626866), test loss: 3.63383378386\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (24.6013069153,49.5949528211), test loss: 36.0752617359\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.74999535084,6.29445201575), test loss: 3.13011268675\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (30.9730453491,49.2500282561), test loss: 36.9661706924\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.12432074547,6.11222061415), test loss: 3.51793425679\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (46.464012146,48.9132820894), test loss: 37.5048428535\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (5.02776145935,5.94846096249), test loss: 3.45949169695\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (34.0452919006,48.556180193), test loss: 32.7198121071\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.639747798443,5.79978708988), test loss: 2.93206532896\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (41.5678329468,48.1940409939), test loss: 39.8461478472\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.14839482307,5.6646180973), test loss: 3.59151801467\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (12.0400180817,47.8397247759), test loss: 33.4632492065\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.862960040569,5.53968776319), test loss: 2.7224437058\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (15.3197689056,47.4946091194), test loss: 40.1267292023\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.729385137558,5.42548052841), test loss: 3.80701798797\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (27.8498344421,47.134588855), test loss: 31.3809139729\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.34738850594,5.32014592597), test loss: 2.75922479928\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (40.0083580017,46.7660682287), test loss: 39.8386273384\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.29265213013,5.22225853394), test loss: 3.4990223825\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (71.0010070801,46.3927640689), test loss: 28.7778893471\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.47040820122,5.13034452709), test loss: 2.70283590257\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (44.010269165,45.9834366453), test loss: 37.3953119278\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.4777765274,5.0429883058), test loss: 3.30174437165\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (70.3511505127,45.5926716713), test loss: 26.1490804195\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.40755534172,4.96023650931), test loss: 2.67458527535\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (15.8868703842,45.1774746332), test loss: 33.6373801231\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.72450590134,4.88106891379), test loss: 3.44187255204\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (23.2892856598,44.7499330954), test loss: 26.1680003643\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (7.43193244934,4.80830338561), test loss: 2.99525897801\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (14.3219900131,44.2991703405), test loss: 31.0639103413\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.14746618271,4.73922971146), test loss: 3.46799207926\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (29.61977005,43.8399064652), test loss: 30.254774642\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.83179831505,4.67301824342), test loss: 3.3860675931\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (44.0223731995,43.3739284086), test loss: 31.5364900589\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.5361187458,4.61001508248), test loss: 3.1419116497\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (14.3085670471,42.9033104445), test loss: 31.3324046612\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.25971198082,4.54892914109), test loss: 3.42456557751\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (33.0157661438,42.4429734223), test loss: 25.5296086073\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.16631245613,4.48962627691), test loss: 2.54969254434\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (31.1416130066,41.9882765967), test loss: 31.9625640154\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.77585577965,4.43318234137), test loss: 3.64322793484\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (27.192855835,41.5331482443), test loss: 24.8391131878\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.91766357422,4.3791087027), test loss: 2.46673981771\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (20.2666606903,41.0907536515), test loss: 32.1665957451\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.571357965469,4.32739799275), test loss: 3.43667843044\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (25.1498470306,40.6510200642), test loss: 25.1295500278\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.88371419907,4.27787206921), test loss: 2.62304101735\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (19.3214683533,40.2181649657), test loss: 32.074311018\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.57149410248,4.23014976035), test loss: 3.46240005642\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (23.7508087158,39.8039225105), test loss: 24.2449292183\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (6.35623502731,4.18411056731), test loss: 2.68838098645\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (19.7379684448,39.3986363153), test loss: 29.7475840569\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.49962997437,4.1391001395), test loss: 3.51534924507\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (56.1002464294,39.0018277203), test loss: 22.8277346849\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.56510269642,4.09621181488), test loss: 2.52645660564\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (13.4605026245,38.6175667077), test loss: 30.5671212435\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.92466282845,4.0556129529), test loss: 3.50366771817\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (21.749294281,38.2466554706), test loss: 29.1224967003\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.96823143959,4.01623219814), test loss: 3.40026887357\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (3.7906460762,37.8760002892), test loss: 30.1248955011\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.49670702219,3.97813646636), test loss: 3.49799777269\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (11.8400993347,37.5246426242), test loss: 30.1378489256\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.96719360352,3.94112318051), test loss: 3.60267956853\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.0463266373,37.1788072054), test loss: 25.7105716228\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.80365133286,3.90485732437), test loss: 2.75207143277\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (40.0427398682,36.8447389936), test loss: 31.9624100208\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.29712200165,3.87043292242), test loss: 3.72800841928\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (10.6318473816,36.5192903726), test loss: 26.5299595356\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.4986975193,3.83686081804), test loss: 2.43110322058\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (10.0567169189,36.2045977803), test loss: 31.214846468\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.289281129837,3.80450961412), test loss: 3.51357435286\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (37.0060844421,35.8966218344), test loss: 26.2052374601\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.66366255283,3.77360132936), test loss: 2.64111146629\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.4412651062,35.5940502525), test loss: 31.885807693\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.1167974472,3.7431434468), test loss: 3.56822390258\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (18.1098194122,35.3048581481), test loss: 25.769046402\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.92458975315,3.7133518274), test loss: 2.70791538358\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (21.0196533203,35.020152933), test loss: 30.658229351\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.03528928757,3.68439262927), test loss: 3.39342937768\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (34.7630882263,34.7402307942), test loss: 23.7219249725\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.61949694157,3.65644768065), test loss: 2.52398594171\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.4356136322,34.470508598), test loss: 31.0368808746\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.36522817612,3.62956260735), test loss: 3.41423689201\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.2611675262,34.2062769489), test loss: 29.4797437906\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.92990398407,3.60352823), test loss: 3.31127943248\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (17.5361061096,33.9466788513), test loss: 30.6419597149\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.92185115814,3.57797141929), test loss: 3.48092114627\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (6.92294883728,33.6962114712), test loss: 28.9076547623\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.47545695305,3.55303075583), test loss: 3.41235112548\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (12.7286396027,33.4503330743), test loss: 26.2029050112\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.749786257744,3.52845250908), test loss: 2.71021480858\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (11.2935209274,33.2081780657), test loss: 31.4530122757\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (4.33427524567,3.50455460501), test loss: 3.64330236316\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (15.9321203232,32.9721010659), test loss: 26.3321827292\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.15607738495,3.48166295388), test loss: 2.51720827818\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (4.88261318207,32.744104576), test loss: 31.7984504223\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.67365694046,3.45932678113), test loss: 3.5553312242\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (10.0907793045,32.5162910579), test loss: 28.6092846632\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.57971596718,3.43750188513), test loss: 2.56500728428\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (18.1945056915,32.2956025603), test loss: 33.1819916964\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (5.42149162292,3.4161350251), test loss: 3.54180275202\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (13.7626399994,32.0792124802), test loss: 26.1129086494\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.15673685074,3.39481887583), test loss: 2.66314093173\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (51.0212249756,31.8658399931), test loss: 31.8003628492\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.32651638985,3.37431417351), test loss: 3.35839947015\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (19.8728256226,31.655481247), test loss: 25.1632739067\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (0.989690303802,3.35413061261), test loss: 2.58850353956\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (15.6931018829,31.4517764779), test loss: 31.0648668766\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.19845104218,3.33458421797), test loss: 3.33768950701\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.9524278641,31.2505362505), test loss: 29.380021739\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.21746599674,3.31573109235), test loss: 3.2038716495\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (4.84482765198,31.0518739988), test loss: 30.8929858208\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.704831957817,3.29688915837), test loss: 3.433698304\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (8.4319896698,30.8599232313), test loss: 28.6914783955\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.54142689705,3.27845677388), test loss: 3.27969626486\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (13.9332332611,30.6693159848), test loss: 26.446132946\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.04470252991,3.26024072486), test loss: 2.81160456389\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (7.29027986526,30.4794001594), test loss: 29.9618666887\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.99832344055,3.24245490605), test loss: 3.38803234398\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (8.72238063812,30.2960161924), test loss: 28.1420958996\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.43625116348,3.22529519586), test loss: 2.53826265931\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (48.3076095581,30.1166945169), test loss: 31.7738130569\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.22742843628,3.20855293581), test loss: 3.53186161518\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (22.1709899902,29.9365473283), test loss: 26.9246474981\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.12994813919,3.19197475691), test loss: 2.41798436642\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (14.0947952271,29.7626085624), test loss: 32.1650612593\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.555912673473,3.17554239796), test loss: 3.37220418751\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (16.9580764771,29.5890591155), test loss: 27.55924263\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.37332844734,3.1593551259), test loss: 2.56206682324\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (22.0203857422,29.4185429065), test loss: 32.3920736551\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.43874430656,3.14350258069), test loss: 3.3858900398\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (18.4607200623,29.2502321359), test loss: 25.7477262974\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.43563079834,3.12810265864), test loss: 2.66498250365\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (9.79182910919,29.0866618471), test loss: 30.7325544357\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.14884400368,3.11304215338), test loss: 3.23112338185\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (14.4634304047,28.9228713003), test loss: 24.4693565845\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.75114250183,3.0982961889), test loss: 2.51186811924\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (7.82046985626,28.7631035199), test loss: 31.0507784367\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.90675497055,3.08370992934), test loss: 3.35000367463\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (30.2256355286,28.6062938447), test loss: 29.1980117559\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.15702652931,3.06908465865), test loss: 3.31672653556\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (10.0694990158,28.4493579537), test loss: 32.4771720886\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.73082113266,3.05485147972), test loss: 3.42046349049\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (11.1455936432,28.294450553), test loss: 32.3590428114\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.780682861805,3.04079864943), test loss: 3.41500072181\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (14.5724563599,28.143249275), test loss: 28.3151199341\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (4.09069919586,3.02718078056), test loss: 2.50042446777\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (14.6852340698,27.9934218709), test loss: 31.7742639542\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.642850577831,3.01382587862), test loss: 3.5182688266\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (6.93005084991,27.8454608702), test loss: 27.3874047041\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.21375596523,3.00057233243), test loss: 2.3973315686\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (16.4895858765,27.7007855679), test loss: 31.9238783598\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.54801487923,2.98744765093), test loss: 3.40095651746\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (10.8329267502,27.5570110468), test loss: 28.924467802\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.457031488419,2.97441421455), test loss: 2.68123585582\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (15.3052406311,27.4126313815), test loss: 32.7260679483\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.61126279831,2.96164472393), test loss: 3.34104881883\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (17.1097660065,27.2730428727), test loss: 26.6825733662\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.99130105972,2.94925239374), test loss: 2.57197140157\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (8.05119228363,27.1351435295), test loss: 33.9523855686\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.417433023453,2.93705936367), test loss: 3.35031214356\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (20.2932415009,26.9972110702), test loss: 25.1734536648\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.73363757133,2.92508314319), test loss: 2.48379675746\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (11.8749704361,26.8628120193), test loss: 31.6956816435\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.71858370304,2.91300909581), test loss: 3.31155593991\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (22.3196144104,26.7284699151), test loss: 29.6923959255\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.974871873856,2.90117604479), test loss: 3.08324898779\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (22.0304069519,26.5957150539), test loss: 32.3899026394\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (4.4136300087,2.88952723426), test loss: 3.3984588027\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (18.47697258,26.4638035643), test loss: 30.7110897064\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (3.7184472084,2.87805229419), test loss: 3.39211879969\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (9.14328765869,26.3353775191), test loss: 28.6162680626\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.511352777481,2.86676206855), test loss: 2.78515661061\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.78377342224,26.2063206933), test loss: 32.7147153378\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.354017227888,2.85579648742), test loss: 3.53052021265\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (8.75148963928,26.0800329473), test loss: 29.1106878757\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.0649960041,2.8448673079), test loss: 2.53811990023\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (17.55676651,25.9559666795), test loss: 33.0262262583\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.63377308846,2.83386567264), test loss: 3.40571822524\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (14.6457891464,25.8308726925), test loss: 28.1831971645\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.05529117584,2.82311415012), test loss: 2.47951519713\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (20.9453048706,25.7071218691), test loss: 36.2297458172\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.75714349747,2.81243906676), test loss: 3.49366119951\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.90599155426,25.5853019162), test loss: 27.5329390049\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.718162953854,2.80201521906), test loss: 2.57805413604\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (21.0716991425,25.4649930317), test loss: 32.9709063053\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.18595910072,2.79182451849), test loss: 3.25519528389\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.1322116852,25.345897249), test loss: 25.6803411484\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.14404416084,2.781670923), test loss: 2.41908379197\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (13.2637310028,25.228269278), test loss: 32.7912390709\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (3.67134332657,2.77164505488), test loss: 3.31300489306\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (12.032491684,25.1113851285), test loss: 30.374008894\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (4.58836460114,2.7615823275), test loss: 3.07042993903\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (33.2100372314,24.9943020516), test loss: 33.302766037\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.83218765259,2.75164623449), test loss: 3.44325659871\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (5.90492296219,24.8791579816), test loss: 29.9501230001\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.07761907578,2.74195957813), test loss: 3.1795832783\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (12.4327945709,24.7660117412), test loss: 29.1477328777\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.09954440594,2.73242298778), test loss: 2.83686796427\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (12.650844574,24.6525577781), test loss: 31.9638394833\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.777063369751,2.72306662669), test loss: 3.45621604025\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.56935548782,24.5413702237), test loss: 29.7450434685\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.60124313831,2.7136153091), test loss: 2.56487250775\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (23.4768466949,24.430440509), test loss: 32.8370010376\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (3.26640939713,2.704321928), test loss: 3.44450762272\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (8.85274982452,24.3201074893), test loss: 30.06468606\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.69179368019,2.69508232553), test loss: 2.6503393054\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (6.28412437439,24.2101993644), test loss: 35.7080083847\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.996465325356,2.68595324556), test loss: 3.33628596961\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (6.4658613205,24.1024366875), test loss: 28.9681378365\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.987905800343,2.67694960657), test loss: 2.62308844924\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (4.7311706543,23.9947780759), test loss: 36.1213712454\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.75478386879,2.6682427284), test loss: 3.35617205203\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (8.37785816193,23.8887069552), test loss: 28.2457798004\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.666825771332,2.65947456675), test loss: 2.69247853607\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (4.43989086151,23.7840482555), test loss: 33.5671951056\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.97046422958,2.65069207685), test loss: 3.23851255774\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (7.55639266968,23.6787233963), test loss: 27.1134807348\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.14143490791,2.64203055731), test loss: 2.87194443345\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (3.92167639732,23.5742579812), test loss: 34.59819417\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.517676353455,2.63343838131), test loss: 3.37876313925\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.55111312866,23.4707801828), test loss: 30.3837788105\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.816706418991,2.62501513364), test loss: 3.18087911308\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (23.4066848755,23.3687842242), test loss: 35.8545428991\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.07646942139,2.61677505541), test loss: 3.38926504403\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (9.63363456726,23.2674639557), test loss: 32.0134725332\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.56387114525,2.60853777932), test loss: 3.34611558616\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (8.17350769043,23.1669009718), test loss: 30.467116642\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.17821717262,2.60034957702), test loss: 2.64693722427\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (3.32083201408,23.067147418), test loss: 33.8274644852\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.05301332474,2.59216602018), test loss: 3.44849835634\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (8.31129074097,22.9670001218), test loss: 29.8138790131\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.90271067619,2.58407192287), test loss: 2.52166680396\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (7.43861818314,22.8684090381), test loss: 35.1934381962\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.33532738686,2.57612769024), test loss: 3.37235241532\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (3.79505681992,22.7710295862), test loss: 30.3264034271\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.61824607849,2.56829058186), test loss: 2.65840286799\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (4.76875448227,22.6735576613), test loss: 35.6555910587\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (3.35918664932,2.56061610355), test loss: 3.33217525482\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (8.58066177368,22.5778004651), test loss: 29.1649885654\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.96949589252,2.55285428312), test loss: 2.70716600418\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (4.50559329987,22.4820761802), test loss: 34.0022106409\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.75579500198,2.54514855894), test loss: 3.16950794756\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (21.4914646149,22.3870730217), test loss: 27.1290707588\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.24596333504,2.53748662543), test loss: 2.52663370967\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (3.95563864708,22.2918609869), test loss: 35.1900289774\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.95064437389,2.52995620877), test loss: 3.34991589487\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (6.31063365936,22.1982984525), test loss: 31.5177679062\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.29848146439,2.52245056005), test loss: 3.00488404632\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (10.0739049911,22.1050755945), test loss: 35.2227851391\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.774311542511,2.5152018554), test loss: 3.44526283741\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (8.28877162933,22.0128515714), test loss: 33.6826468945\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (3.2837562561,2.50791409028), test loss: 3.37983987331\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (5.19541168213,21.9216599007), test loss: 30.620798111\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.525152504444,2.50056370092), test loss: 2.81702980697\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (14.6609582901,21.8302349247), test loss: 34.6705444813\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.44979333878,2.49333764695), test loss: 3.48251906633\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.49730682373,21.7391115959), test loss: 30.2464529037\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.49388229847,2.48612996682), test loss: 2.33853843361\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.8854970932,21.6490013462), test loss: 36.8649335861\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.76472234726,2.47907040918), test loss: 3.35121637881\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (9.41487979889,21.560162026), test loss: 36.3056785583\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.286765873432,2.47212772834), test loss: 2.99597381353\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (2.87457251549,21.4713667185), test loss: 37.0110206127\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.576133966446,2.46521031513), test loss: 3.53865852654\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (7.94363832474,21.3832865928), test loss: 29.285226202\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.600371003151,2.45825692864), test loss: 2.63697454929\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (9.2383480072,21.2959899354), test loss: 34.5795284271\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.10354399681,2.45138576096), test loss: 3.14410795271\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (4.30237770081,21.2084832135), test loss: 27.7299292088\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.69907617569,2.44454900646), test loss: 2.52510212809\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (3.29365921021,21.1219400998), test loss: 34.7369644642\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.934596419334,2.43780328739), test loss: 3.33084080517\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.9092464447,21.0363014254), test loss: 32.4116104841\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.922508716583,2.43116133318), test loss: 3.01697255969\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (3.33433318138,20.9507345946), test loss: 34.8066528559\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.74365186691,2.42465668645), test loss: 3.37562958598\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (6.20965862274,20.8664824523), test loss: 31.7562488556\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.26668167114,2.4180644171), test loss: 3.1561360538\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (15.4612340927,20.7824638553), test loss: 30.6445420027\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.79002314806,2.4114790479), test loss: 2.89395830631\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (10.2504701614,20.6988043115), test loss: 34.439982605\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (3.66818237305,2.40497363195), test loss: 3.43290913105\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (7.4123840332,20.6151590431), test loss: 31.2118515968\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.04798865318,2.39851752244), test loss: 2.5847989589\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (6.74777126312,20.5326510466), test loss: 37.6545653105\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.04241847992,2.39213997492), test loss: 3.46871995926\n",
      "\n",
      "MC # 1, Hype # hyp5, Fold # 9\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold9/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (374.780639648,inf), test loss: 206.506642151\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (293.753112793,inf), test loss: 361.234472656\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (39.3378105164,74.2782501717), test loss: 43.5563551426\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.52762031555,66.328594038), test loss: 3.58197354674\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (46.2497444153,60.0706685143), test loss: 33.4340451241\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.59902739525,34.9134576266), test loss: 2.97186075449\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (55.0645446777,55.202570144), test loss: 42.9882317066\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.07512187958,24.4311396457), test loss: 3.81810326576\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.2274188995,52.7757994497), test loss: 38.5131674528\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.05952656269,19.1940972601), test loss: 3.73766088188\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (48.1195755005,51.146622212), test loss: 42.621715498\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (7.52784061432,16.0520943878), test loss: 3.79081802368\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (42.9297027588,50.0379566089), test loss: 41.4187176704\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.506712555885,13.950063717), test loss: 3.87519172132\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (58.4087524414,49.1221847642), test loss: 38.6157266021\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (5.59435081482,12.44775491), test loss: 3.07564911842\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (40.6990661621,48.3836005206), test loss: 43.0820374727\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.48947381973,11.3182200541), test loss: 3.87312389314\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (12.7041168213,47.7613176253), test loss: 39.3075627327\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.632587432861,10.434875237), test loss: 2.89256395698\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (5.06895017624,47.2627153531), test loss: 42.8259199142\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.631171941757,9.72865138484), test loss: 4.02923659086\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (18.2315673828,46.7743363263), test loss: 35.9970046043\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.82291245461,9.1493006903), test loss: 2.95455560386\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (38.1215057373,46.3115805888), test loss: 43.0783146858\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.19095897675,8.66428795539), test loss: 3.66011461616\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (112.520065308,45.8668908012), test loss: 33.8261854887\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (6.91955137253,8.25293560352), test loss: 3.06938841343\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (54.6597747803,45.4038168593), test loss: 41.6239113808\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.68562698364,7.89721519095), test loss: 3.52603145838\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (127.51084137,44.9806887321), test loss: 31.9432633042\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.3346657753,7.58729515617), test loss: 2.79349509478\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (24.7812385559,44.5406955642), test loss: 37.9742908478\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (5.04823160172,7.31448185947), test loss: 3.68807896972\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (68.9479980469,44.1007946573), test loss: 34.4823691368\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.64106893539,7.07184899107), test loss: 3.29063060284\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (31.4492950439,43.6416572125), test loss: 35.7700067043\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.14683914185,6.85593588908), test loss: 3.56731067896\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (53.6989746094,43.1884385675), test loss: 34.0394032955\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.36627006531,6.65939453297), test loss: 3.50716724396\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (140.14743042,42.7113940867), test loss: 30.7381047487\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.17070770264,6.48028671708), test loss: 2.8078158915\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (12.3246860504,42.232153157), test loss: 35.0048668385\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.52148938179,6.31576748764), test loss: 3.39593468308\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (18.2233695984,41.7526824732), test loss: 31.5192230225\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.15014648438,6.16389732137), test loss: 2.68417506814\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (59.7980918884,41.2884560675), test loss: 37.1017310619\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (5.69182872772,6.02424988561), test loss: 3.78331667781\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (76.9026947021,40.8171075418), test loss: 31.5419680595\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.04827260971,5.89506087199), test loss: 2.66491077542\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (44.7600212097,40.3496606351), test loss: 34.8453439713\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.04064595699,5.77345358554), test loss: 3.56123252511\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (50.7402572632,39.8801855792), test loss: 28.5538546085\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (5.70009326935,5.66009576535), test loss: 2.78316190243\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (28.9721336365,39.4184627286), test loss: 36.6231854439\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.44501638412,5.55290225553), test loss: 3.52002439499\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (33.5670852661,38.971796251), test loss: 27.1901708126\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.45908403397,5.45124052542), test loss: 2.66142402142\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (17.624792099,38.540039298), test loss: 34.9438575745\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.69861698151,5.35698325571), test loss: 3.56529975235\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (15.3729534149,38.1167431986), test loss: 24.7353786945\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.85502398014,5.26796069783), test loss: 2.49122924209\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (24.3785552979,37.7089116531), test loss: 34.0158662796\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.79638576508,5.18360225686), test loss: 3.4264703989\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (9.26491165161,37.3029601794), test loss: 30.8623934984\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.32177209854,5.10455180202), test loss: 3.32775936127\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (24.2496604919,36.9096858582), test loss: 32.4334719419\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.24403476715,5.02884290021), test loss: 3.39326574206\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (22.9962329865,36.5363202083), test loss: 30.4265112758\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.11224198341,4.95613432554), test loss: 3.34513705671\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (51.1992454529,36.1736841243), test loss: 26.7046812057\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.95326519012,4.887923859), test loss: 2.6352641046\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (37.8173980713,35.8168685646), test loss: 35.799327898\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.14294505119,4.82299933577), test loss: 3.52603244483\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (24.8168315887,35.4774877169), test loss: 29.1213848114\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.23385763168,4.7616119414), test loss: 2.54262695909\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (27.6703605652,35.1424442479), test loss: 33.5819729328\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.649329543114,4.70280843944), test loss: 3.55317291617\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (20.0332775116,34.816066157), test loss: 28.1618545294\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.77800428867,4.64657055773), test loss: 2.42847941071\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (4.52410364151,34.5065588827), test loss: 34.4255455971\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.28667664528,4.59218437643), test loss: 3.44746578932\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (17.9528942108,34.2058976296), test loss: 28.3231394768\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.3921431303,4.54023497855), test loss: 2.6517668575\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (34.791305542,33.9103425049), test loss: 34.9340689421\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.53388786316,4.49089417595), test loss: 3.45658290386\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (13.6604242325,33.6264944503), test loss: 27.6674838543\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.52922964096,4.44376945663), test loss: 2.72607621551\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (66.6585922241,33.3523332588), test loss: 32.8053788185\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.34208774567,4.39857173652), test loss: 3.27893454283\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (18.136341095,33.0765064595), test loss: 23.9599752426\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.1974363327,4.354677118), test loss: 2.6370574668\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (7.33238887787,32.8174123419), test loss: 33.6257137299\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.80548143387,4.31230552273), test loss: 3.36241769791\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (13.2704696655,32.5658683414), test loss: 30.2966662884\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.8776845932,4.27099749694), test loss: 3.46775258482\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (13.7267341614,32.3162374537), test loss: 34.2345173478\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.0692448616,4.23175985488), test loss: 3.4217636466\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (20.2017936707,32.0769021226), test loss: 32.2202924728\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.763636469841,4.19421935743), test loss: 3.473501122\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (4.2633190155,31.8464941035), test loss: 30.0346536875\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.837864816189,4.15795871819), test loss: 2.64733016491\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (19.978105545,31.6138107998), test loss: 33.1270051479\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.54824256897,4.12271745494), test loss: 3.58913921714\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (21.7941131592,31.3924765584), test loss: 28.9738485336\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.97915828228,4.08812252971), test loss: 2.38749540895\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.757475853,31.173474021), test loss: 34.302600193\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.13537597656,4.05448156481), test loss: 3.42366325855\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (7.19376850128,30.9595240903), test loss: 28.4142562866\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.91723704338,4.02214665786), test loss: 2.56562084258\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (19.4334583282,30.7515097396), test loss: 35.7128093719\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.48026752472,3.99120666441), test loss: 3.52064841986\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (18.2908573151,30.5513589165), test loss: 27.78780756\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.52485656738,3.9611275024), test loss: 2.7093046084\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (17.0997905731,30.3490195741), test loss: 35.7634386301\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.13828659058,3.93173356988), test loss: 3.34689622372\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.0834388733,30.1530205655), test loss: 25.1995637417\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.34226369858,3.90283193788), test loss: 2.55136862397\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (9.67492389679,29.9613236375), test loss: 33.4191558123\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.88594222069,3.87448662079), test loss: 3.37194129825\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (25.4315052032,29.7735346002), test loss: 31.4362510204\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.69443702698,3.84745386732), test loss: 3.3667047739\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (6.87626457214,29.5882466132), test loss: 33.5674455166\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.37357771397,3.82096797541), test loss: 3.44751452208\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (14.6723051071,29.410122127), test loss: 29.8774588108\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.584670364857,3.7954099891), test loss: 3.3758902058\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (17.4870872498,29.23039718), test loss: 28.4135594845\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.11839747429,3.77054065997), test loss: 2.60467220247\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (7.98777294159,29.0550361312), test loss: 31.7092280388\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.52605724335,3.74606695605), test loss: 3.34685242474\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (39.0054397583,28.8856738274), test loss: 30.6766561031\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.33224654198,3.72173393802), test loss: 2.48186859339\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (9.75854969025,28.7158720571), test loss: 32.6760317802\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.02286851406,3.69827215174), test loss: 3.51085068583\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (24.9761505127,28.5484216175), test loss: 31.9672267914\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.00120997429,3.67547879084), test loss: 2.43231508434\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (13.8220682144,28.3871615742), test loss: 32.499409008\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.21629524231,3.65334812736), test loss: 3.36724483669\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (7.53625679016,28.2254905216), test loss: 30.1308001995\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.11045646667,3.63181849498), test loss: 2.55502824187\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (18.2944335938,28.0666856848), test loss: 34.6385160923\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.12375938892,3.61048208842), test loss: 3.36980645061\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (9.5202999115,27.9124945852), test loss: 27.2788876534\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.80739307404,3.589344397), test loss: 2.53921442479\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (11.8356723785,27.7589386481), test loss: 34.167998457\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.39318180084,3.56879859251), test loss: 3.24865559042\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (18.46667099,27.6058818082), test loss: 24.8619450569\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.39615249634,3.54874774827), test loss: 2.38915606886\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (4.49791383743,27.4577561573), test loss: 34.4262165785\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.766744375229,3.52930776354), test loss: 3.26527061909\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (23.1468658447,27.3110904178), test loss: 29.3443900585\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.84038901329,3.51030536254), test loss: 3.10795758665\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (12.7271194458,27.165058873), test loss: 33.577934885\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.702670812607,3.49146440328), test loss: 3.26384541988\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (23.8778076172,27.0230793134), test loss: 29.5539577007\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (5.37988471985,3.47285625814), test loss: 3.1756308794\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (5.80355644226,26.8819220794), test loss: 29.4824806929\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.354293882847,3.4544497325), test loss: 2.5915497154\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (7.7454457283,26.7408310332), test loss: 33.6165491104\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.50322055817,3.43660468007), test loss: 3.37807231694\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (13.5597877502,26.6039642923), test loss: 31.3933430433\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (3.56259346008,3.41932768275), test loss: 2.44745962322\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (20.9208621979,26.47041926), test loss: 34.3841161966\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.515369534492,3.40236315606), test loss: 3.46950125247\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.3578338623,26.3339465208), test loss: 29.4449156284\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.561290323734,3.38553149009), test loss: 2.29744323194\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (46.9467468262,26.2028027169), test loss: 34.864544189\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.51314258575,3.3688667501), test loss: 3.33412502408\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.2518959045,26.0714840275), test loss: 29.6781389236\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.717735111713,3.3523543988), test loss: 2.49583370388\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (26.5478687286,25.9411522109), test loss: 35.3768623233\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.16930603981,3.33620526017), test loss: 3.32323446572\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.12513494492,25.8129399496), test loss: 28.0152173996\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.35406923294,3.32061884841), test loss: 2.61687508225\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (9.33012390137,25.6885092369), test loss: 33.5721812248\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.996311903,3.30529299171), test loss: 3.1179884702\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.3905572891,25.5621551367), test loss: 26.6127911568\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.24615156651,3.29016383552), test loss: 2.72568413466\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (5.85942602158,25.4391332154), test loss: 33.8272122383\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.09872341156,3.27503356466), test loss: 3.13142217696\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (23.8921451569,25.3167314941), test loss: 29.8508289337\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.27823925018,3.26005425896), test loss: 3.1415695101\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (7.91289234161,25.1950806884), test loss: 34.9717790484\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.68043756485,3.2454312371), test loss: 3.23127798438\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (9.81379985809,25.0750666123), test loss: 30.3606445789\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.633747756481,3.2311909013), test loss: 3.33080829978\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (13.1603908539,24.9580730316), test loss: 31.2122387409\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.70194965601,3.21721551447), test loss: 2.46668596864\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (17.4027442932,24.8397663259), test loss: 32.636297226\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.13595104218,3.20347489619), test loss: 3.35843699574\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (7.07078123093,24.7242748785), test loss: 30.9320680618\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.69870996475,3.1897298421), test loss: 2.24645880461\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (5.05750417709,24.6101664468), test loss: 33.6618298531\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.30895876884,3.1760331647), test loss: 3.19173855186\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (14.2955141068,24.4955404436), test loss: 30.2611960888\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.10208392143,3.16263611703), test loss: 2.43455117345\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (5.32900238037,24.3822221494), test loss: 38.1218970299\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.923207163811,3.14949043017), test loss: 3.32221262455\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (10.4590778351,24.2716750786), test loss: 28.8000095367\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.3904941082,3.13660897851), test loss: 2.56528596729\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (7.2856593132,24.1606901034), test loss: 36.2436244488\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.33563613892,3.12405461661), test loss: 3.10007960796\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (14.2393121719,24.0519388431), test loss: 25.8061827183\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.853439152241,3.11145520637), test loss: 2.45395381451\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (8.99929046631,23.94458315), test loss: 34.3056340218\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.55077910423,3.09883170458), test loss: 3.06658922434\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (13.0547389984,23.8363781934), test loss: 31.9061536312\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.48392820358,3.08651111226), test loss: 2.9864453584\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (13.9629096985,23.7291428677), test loss: 34.7922719479\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.6990377903,3.07434617893), test loss: 3.18227193058\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (9.92970180511,23.6242815339), test loss: 30.050034523\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.47515249252,3.06248441586), test loss: 3.08620423079\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (8.26949882507,23.5196584098), test loss: 30.7404176712\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.16317176819,3.05077375434), test loss: 2.55799048543\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (11.7255811691,23.4163863171), test loss: 31.6508588314\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.15544164181,3.03917811422), test loss: 3.15510584116\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.01045036316,23.3142037008), test loss: 33.1216643333\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.14060652256,3.02751331528), test loss: 2.44458094835\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (18.8641395569,23.2123705906), test loss: 31.8825474262\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.78257894516,3.0160687415), test loss: 3.34215590656\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (3.63749170303,23.1105373851), test loss: 32.5540901661\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.691919207573,3.00476725653), test loss: 2.34800199866\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (8.41670417786,23.0102746705), test loss: 33.8604295492\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.950389742851,2.99372043654), test loss: 3.2095923692\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (39.1747360229,22.9117848164), test loss: 34.0343009949\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.3253891468,2.98291244639), test loss: 2.56844149828\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (16.4362411499,22.8130350597), test loss: 36.0281965733\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.93306660652,2.97215088806), test loss: 3.20061359107\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (6.70191764832,22.7157421474), test loss: 28.7690516949\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.24486207962,2.96132581647), test loss: 2.57790031731\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (13.5466241837,22.618824381), test loss: 35.4300127029\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (4.57667779922,2.95062717894), test loss: 3.07647975385\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (26.1574935913,22.5217479702), test loss: 25.2965524197\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.90054965019,2.94007478715), test loss: 2.37372446954\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (9.97641944885,22.4261897407), test loss: 35.2278952599\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.95835280418,2.92979459912), test loss: 3.09498831928\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (12.4477996826,22.3324288011), test loss: 29.5120254278\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.41623544693,2.91966375503), test loss: 2.94423752427\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (3.23734974861,22.2378747821), test loss: 34.8773062229\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.921064019203,2.90963277757), test loss: 3.12436396331\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (15.1660022736,22.1453234942), test loss: 30.184555459\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.06506872177,2.89949100927), test loss: 3.11356975585\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (6.5797495842,22.0522072628), test loss: 33.7605933189\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.821915209293,2.88950686003), test loss: 2.5940637216\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (14.5836868286,21.959744444), test loss: 33.3335778713\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.02918243408,2.87962963906), test loss: 3.24135310948\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (12.6216259003,21.8680747993), test loss: 33.4533011675\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.26793456078,2.86999190515), test loss: 2.43634959757\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (10.8342065811,21.7783583222), test loss: 34.5667841434\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.4701256752,2.86048367604), test loss: 3.2305537343\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (10.9509210587,21.6879198695), test loss: 29.7376151085\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.58936262131,2.85111321416), test loss: 2.20225084126\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (4.0373840332,21.5990071666), test loss: 35.4214137554\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (4.87543821335,2.84167804333), test loss: 3.18688714802\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (30.3328495026,21.5105607441), test loss: 30.5520167828\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.75202298164,2.83224637879), test loss: 2.53020476103\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (11.0466804504,21.4216130109), test loss: 36.5082135201\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.07755684853,2.82298131417), test loss: 3.12810347825\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (20.1362190247,21.3338473706), test loss: 30.9379703522\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.00277972221,2.81387451174), test loss: 2.58075053692\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (2.3742146492,21.2471499509), test loss: 35.5167257786\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.207663372159,2.80486764308), test loss: 3.01582039297\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (13.6699323654,21.160636197), test loss: 28.6827272892\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.36461853981,2.7960766427), test loss: 2.84005763829\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (7.47204256058,21.075164792), test loss: 35.2904619217\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.06524300575,2.7872373224), test loss: 3.05785115361\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (7.47707080841,20.9904616693), test loss: 29.6488512993\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.46991610527,2.77834786528), test loss: 3.04724695086\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (4.68030929565,20.9051300204), test loss: 37.5940014124\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (4.36230134964,2.76963785429), test loss: 3.08089881241\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (6.03690576553,20.8205552663), test loss: 33.3641077995\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.04598617554,2.76099744989), test loss: 3.19114608765\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.11997032166,20.7371614943), test loss: 33.4078864098\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.63327240944,2.75248967875), test loss: 2.42730175257\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (10.9579229355,20.6541720039), test loss: 33.1660263062\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.334361493587,2.74419470334), test loss: 3.25946997404\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (6.23269367218,20.5720314505), test loss: 32.8197435856\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (3.30286574364,2.73588187066), test loss: 2.26224875748\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (10.33801651,20.4903097554), test loss: 34.8453724861\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.1144528389,2.72745676369), test loss: 3.13368874192\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (6.20675325394,20.4083898195), test loss: 32.1769618034\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.17557406425,2.71919960209), test loss: 2.43729402423\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (11.5109882355,20.3270296173), test loss: 39.3542015553\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.35549092293,2.71104127272), test loss: 3.2863258034\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (4.43675613403,20.246478836), test loss: 30.9466515541\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.613606870174,2.70299833644), test loss: 2.54240639508\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (6.31557798386,20.166824504), test loss: 36.7576640844\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.34590399265,2.69513247277), test loss: 2.9847415477\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (5.08944511414,20.0875400895), test loss: 27.2799073696\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.62078744173,2.68723286209), test loss: 2.46684063375\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (6.80310058594,20.0088089574), test loss: 35.7242129803\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (3.07955956459,2.67932782754), test loss: 2.96566028595\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (9.28255558014,19.9302705868), test loss: 31.1763950348\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.62206351757,2.67144966522), test loss: 2.96758654714\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.06381130219,19.8516816607), test loss: 36.8564317703\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.26959335804,2.6636895717), test loss: 3.13049708903\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (9.49648761749,19.7741159448), test loss: 31.4834388733\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.54396009445,2.65609777732), test loss: 3.01488095522\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (7.1837220192,19.6976394295), test loss: 36.8863901854\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.204420521855,2.64859608441), test loss: 2.80217306614\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (5.364382267,19.6210877773), test loss: 32.2983440876\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.22084712982,2.64115167248), test loss: 3.15087427497\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (5.37306928635,19.5452508131), test loss: 35.3884786129\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.693844020367,2.63359449419), test loss: 2.4998986721\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (4.07849597931,19.4695150735), test loss: 35.4515603542\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.18544614315,2.62615571265), test loss: 3.278047508\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.66975402832,19.3938883572), test loss: 35.0239253044\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.22863984108,2.61876737096), test loss: 2.50163274407\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.48340129852,19.3190519881), test loss: 35.7162586212\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.34964132309,2.61154935441), test loss: 3.15303720236\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (6.30103063583,19.2453485499), test loss: 33.9545929909\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.88621270657,2.60438172636), test loss: 2.5234403491\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (4.07314062119,19.1713463392), test loss: 37.5289315701\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.40712308884,2.59731358058), test loss: 3.08727511168\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.26519107819,19.0981749628), test loss: 29.9711949825\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (3.5814704895,2.59013426642), test loss: 2.584063752\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (8.2563533783,19.0253842772), test loss: 36.5606801748\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.45394682884,2.58305766123), test loss: 2.92210178375\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (8.59279251099,18.9525825758), test loss: 26.9731778622\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (3.17645382881,2.57602267342), test loss: 2.41562733799\n",
      "\n",
      "MC # 1, Hype # hyp5, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold10/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (297.844573975,inf), test loss: 151.576277924\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (309.823516846,inf), test loss: 376.503781128\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (16.7969474792,63.2692064381), test loss: 45.5682344913\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.18105125427,35.8068025363), test loss: 3.32885812521\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (29.8077583313,53.7297603402), test loss: 36.4776353836\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.3556292057,19.3307374296), test loss: 2.71508308649\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (111.594955444,50.4074984341), test loss: 43.4659390926\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.85148358345,13.814775409), test loss: 3.53561844826\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (25.7244586945,48.7336032531), test loss: 39.0684707165\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.5858335495,11.0557629081), test loss: 3.57157675624\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (7.08026504517,47.5069298944), test loss: 41.3544065475\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.32832527161,9.39742845945), test loss: 3.5074251771\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (26.9904823303,46.5756304223), test loss: 40.9940813065\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.755931377411,8.28499391116), test loss: 3.80958422124\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (39.4378356934,45.7741997197), test loss: 37.7724299908\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.98721241951,7.48906150476), test loss: 2.66722809076\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (23.2498626709,45.027539997), test loss: 42.5602671146\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.1347682476,6.88834192888), test loss: 3.60734321773\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (15.8402748108,44.3825982717), test loss: 38.7396533012\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.72234964371,6.41368037883), test loss: 2.546225667\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (17.8908462524,43.8524433654), test loss: 42.6768693924\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.40358448029,6.03543770921), test loss: 3.82038051486\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (21.1912994385,43.281421022), test loss: 36.8983262539\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.12036061287,5.72384002174), test loss: 2.49737517983\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (35.5204925537,42.7035466394), test loss: 41.3328364372\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.70969414711,5.46168165874), test loss: 3.58322551847\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (32.2844200134,42.0132413477), test loss: 32.1122577429\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.57757735252,5.23874469613), test loss: 2.8237604782\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (28.8357601166,41.3075761806), test loss: 38.8209394932\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.34923505783,5.04584236432), test loss: 3.62203886509\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (35.0109901428,40.6307930305), test loss: 29.8933067322\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.07706308365,4.87369156758), test loss: 2.81482630074\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (19.8245391846,39.9969077456), test loss: 37.9434835911\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.19776725769,4.72449757354), test loss: 3.42943950891\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (9.80560684204,39.3519901233), test loss: 27.6460060358\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.32858645916,4.59149842962), test loss: 2.60357732326\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (20.3718681335,38.7154156976), test loss: 34.7990038872\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.63761234283,4.471887488), test loss: 3.24468457103\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (58.4055252075,38.0831302638), test loss: 31.4315579176\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.88578414917,4.36336304641), test loss: 3.53929523081\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (7.36426925659,37.467323669), test loss: 34.1396862984\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.71418142319,4.26459417781), test loss: 3.44078728855\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (19.9381427765,36.8855744323), test loss: 31.1437130451\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.03351676464,4.17305191863), test loss: 3.57746812403\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (24.6593132019,36.3365509082), test loss: 28.2902321339\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.55987238884,4.09022573445), test loss: 2.77131296396\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (68.3399581909,35.7969149952), test loss: 31.5873650551\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.87367248535,4.01467817784), test loss: 3.59244854152\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (16.7988491058,35.2794077185), test loss: 29.7039544344\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.16688418388,3.94434879605), test loss: 2.60961701274\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (101.164581299,34.7712990561), test loss: 33.6985209942\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.58003568649,3.87866044223), test loss: 3.70678257644\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (5.83751726151,34.2905698023), test loss: 29.8732925177\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.80813908577,3.81740285396), test loss: 2.39663787335\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (17.4466972351,33.8374225779), test loss: 36.4813315392\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.11653327942,3.75932108778), test loss: 3.50184209049\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (7.90254306793,33.4070873361), test loss: 29.2882669449\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.56580615044,3.705533171), test loss: 2.69326221049\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (8.45680999756,32.9886713067), test loss: 35.6353267431\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.14163398743,3.65564091568), test loss: 3.65335905552\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.0594654083,32.5943595703), test loss: 28.8258361578\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.70697808266,3.6084739033), test loss: 2.81189383566\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (22.1571121216,32.1994226856), test loss: 36.1417434216\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (4.41097831726,3.56339217605), test loss: 3.3838767603\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (22.7915611267,31.8342210547), test loss: 28.4230626106\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.42027258873,3.52054541706), test loss: 2.78088214397\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (8.09967422485,31.4821823375), test loss: 35.1107308388\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.56296050549,3.47953439367), test loss: 3.21902324855\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (55.9770507812,31.1464211794), test loss: 31.5965473652\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.91579127312,3.44086498217), test loss: 3.30693808645\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (12.5718708038,30.8192922609), test loss: 35.6155715466\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.76587605476,3.40477120623), test loss: 3.34724878073\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (8.64871692657,30.5084280007), test loss: 31.1659411907\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (0.906584143639,3.37018172252), test loss: 3.49155600071\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (13.3961210251,30.1972283909), test loss: 35.388136363\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.612490534782,3.3368101964), test loss: 3.16776179671\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (51.69140625,29.9097524563), test loss: 30.6159385204\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.78587675095,3.30495147978), test loss: 3.53827106059\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (21.0405197144,29.6297805983), test loss: 30.1839387417\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (6.98434829712,3.2740685618), test loss: 2.54621028304\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (35.5679550171,29.3564962541), test loss: 34.1817620277\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.37409222126,3.24441820539), test loss: 3.57710932195\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (32.3663673401,29.0939015844), test loss: 30.9651799202\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.07171714306,3.21669135901), test loss: 2.31148165464\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (49.2963371277,28.8408012241), test loss: 36.0026589394\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.8272395134,3.18986637807), test loss: 3.55787552595\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (37.0630531311,28.585826519), test loss: 29.3172911167\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.45450210571,3.16362841245), test loss: 2.47393083274\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (16.5636634827,28.346288692), test loss: 35.114316988\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.677971184254,3.13824075433), test loss: 3.46919044256\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (12.076789856,28.1149712355), test loss: 29.8335743904\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.317694842815,3.11335616526), test loss: 2.74832754135\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (11.699754715,27.8858472017), test loss: 35.9514511585\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.625170767307,3.08969203471), test loss: 3.36140156984\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (15.5352964401,27.6657936698), test loss: 28.8368955612\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.45223331451,3.06733335564), test loss: 2.79959710389\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (53.1100387573,27.451518467), test loss: 37.3918823242\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.16684103012,3.04544484271), test loss: 3.23900462389\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (16.6236515045,27.2366247513), test loss: 29.0066804886\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.810612082481,3.02400250057), test loss: 2.88458608836\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (17.4388160706,27.0333934133), test loss: 35.91720438\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (4.08086299896,3.00313278846), test loss: 3.2961076498\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (17.0000076294,26.8353270789), test loss: 29.5980070114\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.89934778214,2.98272631934), test loss: 3.45412797928\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (6.0060749054,26.6377279944), test loss: 35.9259740353\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.951068341732,2.9629844233), test loss: 3.36146268547\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (14.6697044373,26.4467895605), test loss: 30.6252938271\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.24993538857,2.94414337987), test loss: 3.44991169274\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (41.4108123779,26.2588449837), test loss: 33.0546319723\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.23586893082,2.92570529431), test loss: 2.69362372458\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (30.6135940552,26.0726242222), test loss: 33.1333126307\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.11753821373,2.90765077422), test loss: 3.48427634835\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (8.15608024597,25.8943446692), test loss: 33.0472729683\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.16934168339,2.88980098059), test loss: 2.49996839166\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (15.1147527695,25.7197752997), test loss: 35.3387852907\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.19863533974,2.87240940926), test loss: 3.5263002038\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (27.8056583405,25.5455197632), test loss: 31.5253937006\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.31219530106,2.85559117145), test loss: 2.3762337774\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (8.59902477264,25.376468972), test loss: 34.2675391197\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.930514514446,2.83934334399), test loss: 3.31425080299\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (18.276222229,25.2091775346), test loss: 34.4786879063\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.398211359978,2.82346510645), test loss: 2.72677758783\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.9790153503,25.0446739824), test loss: 34.2826526165\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.08582758904,2.80795548774), test loss: 3.34348258227\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (6.6058549881,24.8862909854), test loss: 30.4705792904\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.55273532867,2.79247757912), test loss: 2.7232313931\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (19.0153369904,24.7297900438), test loss: 35.9707771778\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.926183462143,2.77743035772), test loss: 2.96206185222\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (14.8481016159,24.5728982943), test loss: 29.132653594\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.22511601448,2.76270538181), test loss: 2.53370900601\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (12.7886476517,24.4213842182), test loss: 36.3983856201\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (4.75994777679,2.74851618852), test loss: 3.19435731173\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (10.7925243378,24.2701839606), test loss: 31.5187060595\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.12101745605,2.73455644983), test loss: 3.29605274498\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (5.32029628754,24.1221306384), test loss: 36.5667429924\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.11690211296,2.72079082997), test loss: 3.3329621911\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (7.92295074463,23.979489975), test loss: 30.5200356007\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.46421933174,2.70712606363), test loss: 3.35691917092\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (14.5379295349,23.8373437557), test loss: 32.0957576752\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.87614238262,2.69383446305), test loss: 2.62606999278\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (13.7627792358,23.6951489186), test loss: 36.2231225967\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.82367658615,2.68079102737), test loss: 3.47500367463\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (8.4481678009,23.5580451875), test loss: 34.3153047562\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.950472295284,2.66814751306), test loss: 2.5700184077\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (6.49868965149,23.4200791454), test loss: 33.362335062\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.0542703867,2.65586470536), test loss: 3.52158361077\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.4343833923,23.2854538704), test loss: 34.3080845356\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.03707098961,2.6436494624), test loss: 2.4190793246\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (14.1852416992,23.1547700584), test loss: 36.0969261646\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.69132566452,2.63145603525), test loss: 3.37340785265\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (11.7392854691,23.0244482034), test loss: 34.1594442844\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.1699025631,2.61954028759), test loss: 2.71597178727\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (20.8186836243,22.8939967102), test loss: 36.7556223869\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.12304782867,2.6078412525), test loss: 3.42353077829\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.8907690048,22.7679049763), test loss: 31.8789758205\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.879482448101,2.59643393097), test loss: 2.79935141504\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (9.11940193176,22.64134548), test loss: 37.1111283302\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.907008767128,2.58535754592), test loss: 3.01961732507\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (6.96458625793,22.5173983329), test loss: 30.3494213581\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.839255094528,2.57433307682), test loss: 2.55668507218\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (6.24156665802,22.3964987203), test loss: 36.5812552691\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.02926063538,2.56329442919), test loss: 3.00841424465\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (8.69923877716,22.276257324), test loss: 32.6214151382\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.31882047653,2.55251284572), test loss: 3.16844385266\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (9.80619812012,22.1562100277), test loss: 37.2129277468\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.931985080242,2.54189877591), test loss: 3.23980439901\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.74604082108,22.0395432922), test loss: 31.0353449821\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.956750452518,2.53158116224), test loss: 3.28715710342\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (12.0914106369,21.9217975457), test loss: 35.653728199\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.20715045929,2.52152638764), test loss: 2.80502865911\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (2.17238736153,21.8066457336), test loss: 31.486017561\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.72499704361,2.51148910588), test loss: 3.33786326945\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (8.11969470978,21.6939546277), test loss: 36.1825994015\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.61434149742,2.50140705837), test loss: 2.73247308582\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (20.2432498932,21.5816293385), test loss: 33.9882374763\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.25107765198,2.49157912692), test loss: 3.44870883226\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (7.35054349899,21.4693116544), test loss: 35.1282121658\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.01485955715,2.48185324079), test loss: 2.40337500274\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (12.1608715057,21.3605328695), test loss: 36.2639323711\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.12418603897,2.47239680412), test loss: 3.38414067924\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (8.2520570755,21.2509584106), test loss: 36.7633423805\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.83286142349,2.4631512035), test loss: 2.72241513431\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (9.74738693237,21.143953727), test loss: 37.648085928\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (4.2580871582,2.45388831682), test loss: 3.49565557241\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (8.5517244339,21.0386821968), test loss: 32.864541626\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.4412560463,2.44462918196), test loss: 2.77950008512\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (11.204659462,20.9334468109), test loss: 39.1969309092\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.02528142929,2.43561477472), test loss: 3.27268672585\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (16.4985866547,20.8287826906), test loss: 33.5910271645\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.0838906765,2.42674536419), test loss: 2.82434561253\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (4.22554397583,20.7265271857), test loss: 38.2279222727\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.993974804878,2.41805767867), test loss: 3.04697267264\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (6.11153125763,20.6236539268), test loss: 35.4362106562\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.21617245674,2.40950052905), test loss: 3.12333517075\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (4.39600086212,20.5230203525), test loss: 38.7543149233\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.41871905327,2.40092109869), test loss: 3.22724577188\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (12.0132379532,20.4234809811), test loss: 31.8969852209\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.5256125927,2.39243343163), test loss: 3.16422129869\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (17.1854858398,20.3244694784), test loss: 38.8214326859\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.878970980644,2.38398992831), test loss: 3.26895292401\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (4.18400859833,20.2256644305), test loss: 32.190280509\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.29650342464,2.37581894937), test loss: 3.29525718689\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (8.33367538452,20.1297308316), test loss: 37.2122423649\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.839593172073,2.36772774139), test loss: 2.68339712024\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (4.43406867981,20.0328124644), test loss: 34.9564686537\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.987597465515,2.3597990258), test loss: 3.42115930021\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.05883407593,19.9381625304), test loss: 36.970176363\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.79175853729,2.35177829862), test loss: 2.44415881634\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (5.65254640579,19.8440296081), test loss: 39.3880228519\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.18263268471,2.34390748322), test loss: 3.47228435874\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.86099529266,19.7503138093), test loss: 37.3441842079\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.19523072243,2.33608404691), test loss: 2.69043987393\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (3.89130520821,19.6571300694), test loss: 37.6584477425\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.85593271255,2.32847072054), test loss: 3.43397910297\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (10.7196893692,19.5656600916), test loss: 36.7883143902\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.86997997761,2.32096879324), test loss: 2.9397734046\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (9.11673069,19.4737626959), test loss: 38.7510956764\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.99570381641,2.31354211187), test loss: 3.30526652932\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (8.33957862854,19.383697638), test loss: 34.825099659\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.02329039574,2.30604331637), test loss: 2.9110462606\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (7.55303859711,19.2942216226), test loss: 38.7746831417\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.792696475983,2.29871424336), test loss: 2.98991336524\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (17.8107833862,19.2046786371), test loss: 33.4582954407\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.95263421535,2.29136587683), test loss: 2.79892813563\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (7.09029340744,19.1164771713), test loss: 39.5073540211\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.979879438877,2.28421796767), test loss: 3.25011987686\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (8.65313625336,19.0293750714), test loss: 36.4281861782\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.254942238331,2.2771841569), test loss: 3.67747285366\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (8.46408653259,18.9422371851), test loss: 40.2108967304\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.2166159153,2.27020196729), test loss: 3.36532704532\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (9.87558555603,18.8563482109), test loss: 32.6688282013\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.329497307539,2.26318203748), test loss: 3.40257215202\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (12.8046951294,18.7713348429), test loss: 38.5157932758\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.09784126282,2.25627413197), test loss: 2.84802180827\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.59008693695,18.6857295622), test loss: 36.1662751198\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.95250320435,2.24942133616), test loss: 3.3394890666\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (5.2240190506,18.6014224787), test loss: 38.9343791485\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.00497102737,2.24273943563), test loss: 2.7285735935\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (21.5240039825,18.5178912188), test loss: 39.6248268127\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.19651794434,2.23613072904), test loss: 3.60867219269\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (7.11090564728,18.4347876962), test loss: 37.2626319408\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.27114892006,2.22951973561), test loss: 2.57162634134\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (6.26475811005,18.3526059289), test loss: 38.5048772812\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.84272193909,2.22292823749), test loss: 3.33382891417\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (4.0545592308,18.2711792912), test loss: 39.4975773335\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (0.301471352577,2.21635442308), test loss: 2.92046734393\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (4.61448001862,18.1895609292), test loss: 39.8612523079\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.60198366642,2.20987090886), test loss: 3.35532064289\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (11.8203897476,18.1092856492), test loss: 35.5812969685\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.67691779137,2.20351775445), test loss: 2.88858576417\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.47084999084,18.0293181509), test loss: 41.3624505281\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.75442624092,2.19730720119), test loss: 2.98981217593\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (11.3425245285,17.9502296882), test loss: 33.4144274473\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.24343752861,2.19107984289), test loss: 2.70763787031\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (3.59812116623,17.8716402501), test loss: 40.1938020229\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.450333535671,2.18479601541), test loss: 3.12959523648\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.61870288849,17.7936507429), test loss: 33.8498701334\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.33765351772,2.17864989132), test loss: 3.20262684822\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (7.82542037964,17.7155452643), test loss: 40.1995250225\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.634796202183,2.17252333401), test loss: 3.28556607813\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (9.30556488037,17.6383884334), test loss: 32.5464365005\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.83392739296,2.16650810265), test loss: 3.28594278991\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (5.30124378204,17.5617547634), test loss: 38.0524098396\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.642580628395,2.16060558967), test loss: 2.93234034181\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (6.19019699097,17.4859481203), test loss: 35.1198197961\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.599020123482,2.15470467975), test loss: 3.33787613809\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (4.9755525589,17.4107803103), test loss: 40.4082326889\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.37314176559,2.1487563019), test loss: 2.7562265873\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.19864082336,17.3360516235), test loss: 36.1314301014\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.18017172813,2.14290068101), test loss: 3.43488104939\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (10.4611463547,17.2613490344), test loss: 40.8871157646\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.44749999046,2.13706827524), test loss: 2.49723440707\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.08259391785,17.1875474428), test loss: 39.0968035698\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.02264463902,2.13133660522), test loss: 3.34224129021\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (4.65488243103,17.1142217578), test loss: 39.2067497253\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.5174870491,2.12575729666), test loss: 2.88986813128\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (8.69518852234,17.0416879388), test loss: 40.2175295353\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.88448858261,2.12014603974), test loss: 3.45016178787\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (2.42037177086,16.9696498917), test loss: 36.6425808907\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.707604527473,2.11448613029), test loss: 2.94900898039\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (6.08800029755,16.8977378054), test loss: 40.420023632\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.80493891239,2.1089677508), test loss: 2.94340316653\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (9.27694129944,16.8261542239), test loss: 35.3413341999\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.713822424412,2.10341862257), test loss: 2.74769510329\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.88444662094,16.7553322388), test loss: 40.6082658768\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.13462018967,2.0979472558), test loss: 2.97281761467\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (12.2488594055,16.6850422526), test loss: 35.7564074039\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.896609246731,2.09264331359), test loss: 3.15667468309\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (13.8505401611,16.6155388729), test loss: 41.0670506001\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.985847592354,2.08726139195), test loss: 3.29224341512\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (10.6390209198,16.5466594541), test loss: 32.2676602364\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.37346935272,2.08188375629), test loss: 3.27157453001\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.48489713669,16.4777453214), test loss: 37.9856546402\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.45033359528,2.07657462121), test loss: 2.85074248016\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (9.38257789612,16.4092738792), test loss: 35.8706532478\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.844590067863,2.0713045429), test loss: 3.35974942148\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (5.80492019653,16.3415471968), test loss: 40.2832300186\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.05823409557,2.06610512854), test loss: 2.82246859968\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (6.57346534729,16.274061396), test loss: 36.1948317528\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.35747885704,2.0610627114), test loss: 3.41294897199\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.16591691971,16.2072543453), test loss: 38.6932451725\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.868929862976,2.05593873312), test loss: 2.4839980945\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (14.537153244,16.141164607), test loss: 41.478512907\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.24575138092,2.05082029753), test loss: 3.48771847785\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (14.6610546112,16.0750178414), test loss: 38.9719974518\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.835363149643,2.04574437883), test loss: 2.81587232351\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.64627170563,16.0093133821), test loss: 42.3983323097\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.589338958263,2.04072068129), test loss: 3.50577029884\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (1.80187523365,15.9444700383), test loss: 35.6828803062\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.304391443729,2.03575538952), test loss: 2.87670186982\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.57647109032,15.8797925658), test loss: 43.0803308964\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.1408264637,2.03092813808), test loss: 3.2769212395\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (4.27897405624,15.8158143822), test loss: 36.6702059746\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.23234295845,2.02600911299), test loss: 2.93906530142\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.62548208237,15.7524591967), test loss: 42.0833335638\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.827771782875,2.02111248413), test loss: 2.97903357893\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.41732788086,15.6891494342), test loss: 38.4751609802\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.30016088486,2.01625397596), test loss: 3.25896646082\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (11.1987199783,15.6263349104), test loss: 42.159207058\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.05406904221,2.01145975429), test loss: 3.23695048392\n",
      "run time for single CV loop: 7094.44466996\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 6\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold6/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (194.300003052,inf), test loss: 167.575107956\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (464.853027344,inf), test loss: 487.93910675\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (48.3897323608,90.5747538891), test loss: 45.0715424538\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.75237774849,193.118406998), test loss: 3.69243501425\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (47.3347053528,66.9632389131), test loss: 35.566833353\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.60533094406,98.3914305646), test loss: 3.08231788278\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (10.5560245514,58.8867379996), test loss: 42.0821226597\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (0.859730303288,66.7999513849), test loss: 3.72705917358\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (31.8766345978,54.8972142494), test loss: 39.1333122253\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (5.46545362473,51.0152786949), test loss: 3.48390845954\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (36.1570777893,52.3708111494), test loss: 44.0291914463\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.62410545349,41.5377365296), test loss: 3.52793948054\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (54.2598266602,50.6363178879), test loss: 41.5506569862\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (4.16952943802,35.2206186671), test loss: 3.50999445617\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (24.1561546326,49.3592272406), test loss: 40.563485837\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.00280332565,30.6945401169), test loss: 3.1096342504\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (31.6205673218,48.3147477808), test loss: 41.6549487114\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (5.17509365082,27.2959302195), test loss: 3.81147832274\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (23.9112186432,47.4601598987), test loss: 39.7393900394\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.27185726166,24.6510703144), test loss: 2.97825139165\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (37.8203277588,46.7600349924), test loss: 42.4337948799\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.19600057602,22.5287365162), test loss: 3.672430861\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (43.8465194702,46.1852674373), test loss: 37.3050221205\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.83099579811,20.79278212), test loss: 2.85169446468\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (70.390083313,45.6376393182), test loss: 43.7999934673\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.09558868408,19.3427245345), test loss: 3.66401377022\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (34.1985816956,45.1454105369), test loss: 34.9780771732\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (4.60545015335,18.1127319897), test loss: 2.91685780883\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (115.101600647,44.7000855374), test loss: 40.4993805408\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (6.36559009552,17.0570934838), test loss: 3.35666355491\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (31.5706253052,44.2331014971), test loss: 30.8567579746\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.29131746292,16.1374923565), test loss: 2.86062256098\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (49.3990859985,43.8433627707), test loss: 38.8757565737\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (5.03769159317,15.3308581776), test loss: 3.44187110662\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (26.1402511597,43.4404664024), test loss: 36.4407550812\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.73812627792,14.6156391193), test loss: 3.36890106201\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.6786613464,43.079766007), test loss: 40.3268161297\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.39753055573,13.9796129634), test loss: 3.33198765814\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (4.20068216324,42.7028151927), test loss: 38.0372510433\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.72262454033,13.4074972888), test loss: 3.39411000609\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (14.4652605057,42.3421261627), test loss: 36.336347723\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.35024499893,12.8895264495), test loss: 2.89560430646\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (33.3549880981,41.969881818), test loss: 37.7109425545\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.805568575859,12.4188974143), test loss: 3.48073909283\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (44.289894104,41.5919683581), test loss: 34.6641308546\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.32698225975,11.9880764699), test loss: 2.67478329539\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (33.3794555664,41.2217615731), test loss: 39.2110852003\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (5.59237480164,11.5916371847), test loss: 3.31652513742\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (21.4807243347,40.8611135013), test loss: 32.3417391777\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.48824644089,11.2267884887), test loss: 2.62355757952\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (18.0662117004,40.4895979522), test loss: 36.9854907036\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.01214170456,10.8885206637), test loss: 3.09708653092\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (23.2044620514,40.1088651835), test loss: 27.3955656052\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.557664036751,10.5756471312), test loss: 2.53263189197\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (25.6662864685,39.732137622), test loss: 34.2757427692\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.12678050995,10.2833300765), test loss: 3.06886748374\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (42.4819259644,39.3412895711), test loss: 25.3190963268\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.61988639832,10.0099048692), test loss: 2.72472662926\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (19.2529258728,38.9473899489), test loss: 33.5793009996\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.17886066437,9.75374930993), test loss: 3.10483054519\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (2.19169187546,38.5623685792), test loss: 31.1527696133\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.22935938835,9.51210923231), test loss: 3.03973483443\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (15.3359870911,38.1819538265), test loss: 31.7279246807\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.34666264057,9.28537175853), test loss: 2.79020201564\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (13.7197656631,37.7940917192), test loss: 34.2364977837\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.05777311325,9.0715458694), test loss: 3.27773382366\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (6.85231351852,37.4118545062), test loss: 30.4791245699\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.3244972229,8.87033473901), test loss: 2.52992100865\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (37.6137924194,37.0406863558), test loss: 33.3256927252\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.58674240112,8.67963093233), test loss: 3.18744844198\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (11.5407857895,36.6569883152), test loss: 28.3709485531\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.47633552551,8.49876128093), test loss: 2.505550614\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (13.3163871765,36.2941063807), test loss: 34.5418720722\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.16543889046,8.32700867797), test loss: 3.15711166859\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (43.5446548462,35.9359455421), test loss: 28.2469089508\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.5196223259,8.1637579467), test loss: 2.5882604599\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (23.9378929138,35.5877380952), test loss: 33.1745130539\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.875531196594,8.00881850733), test loss: 3.10606446564\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (16.2060241699,35.2443659616), test loss: 25.4619550228\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.43849760294,7.86148344271), test loss: 2.50250629783\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (14.168627739,34.9158608827), test loss: 32.0402581215\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.2620677948,7.72149974912), test loss: 3.13714620769\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (16.3533153534,34.5914028188), test loss: 29.912644577\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.787123501301,7.58763808104), test loss: 2.99684717059\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (20.6052284241,34.2755224177), test loss: 31.8782601357\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.30493319035,7.45995486263), test loss: 3.17159473896\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (16.4923229218,33.9750431833), test loss: 29.4918728828\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.9831764698,7.33781911586), test loss: 3.13300088644\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (7.96465301514,33.684981052), test loss: 29.3045002937\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.41044712067,7.22079855579), test loss: 2.75930182338\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (22.8857307434,33.4020629978), test loss: 32.0017187834\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.15421295166,7.10870251319), test loss: 3.33759766519\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (25.9506378174,33.1287709836), test loss: 30.5117702961\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.68472886086,7.00200936531), test loss: 2.48393524587\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (28.7877902985,32.8672597475), test loss: 32.006187439\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.32342422009,6.89958244412), test loss: 3.0981935516\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (27.4413414001,32.6085859593), test loss: 30.271000576\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.6641740799,6.80116398653), test loss: 2.58258898556\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (23.1184997559,32.359287158), test loss: 32.8833797455\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.25920987129,6.70664989318), test loss: 3.11856647134\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (10.9199028015,32.1232966851), test loss: 28.5884246349\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.65082359314,6.61523190479), test loss: 2.70480472147\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (16.8091945648,31.8942433261), test loss: 32.2269510031\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.02329921722,6.5276491689), test loss: 3.14303013086\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (10.3187332153,31.667661324), test loss: 26.3238895416\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.740449547768,6.44300902965), test loss: 2.62703509927\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (31.9377670288,31.453233022), test loss: 32.9178559065\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.697610855103,6.36222703902), test loss: 3.21505245864\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (33.0224914551,31.2478909605), test loss: 31.0752752781\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.45310640335,6.28427320755), test loss: 3.08335038722\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (67.3782424927,31.0424077713), test loss: 33.503403616\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.1650416851,6.20874910848), test loss: 3.20071142912\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (7.88388776779,30.8458767273), test loss: 31.1427397728\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.79128754139,6.13571938173), test loss: 3.2140628159\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (6.74750852585,30.6576754281), test loss: 31.2061710596\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.83461213112,6.06510657567), test loss: 2.68624160141\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (11.257183075,30.4744313123), test loss: 31.6426259518\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.4630959034,5.99688068492), test loss: 3.31192898154\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (17.7115211487,30.2941763409), test loss: 31.0611185551\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.883426010609,5.9308153796), test loss: 2.4797581166\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (10.7091627121,30.1223255902), test loss: 33.6005286217\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.870487570763,5.86737091991), test loss: 3.10970982909\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (21.831867218,29.9543096504), test loss: 31.3124432325\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.38696098328,5.80577337803), test loss: 2.66524024755\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (20.1830329895,29.7887919913), test loss: 33.1214475632\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.80337572098,5.7459131489), test loss: 3.11583056301\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (53.4214477539,29.6329483483), test loss: 26.7555001736\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.60889291763,5.68790513963), test loss: 2.61061641425\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.3268127441,29.4794259817), test loss: 33.4893692493\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.48514318466,5.63143826788), test loss: 3.05903922021\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (29.6041316986,29.329489342), test loss: 26.1516938448\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.02169823647,5.57669500525), test loss: 2.6502103433\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (16.1944885254,29.1831485318), test loss: 33.9488174438\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.91259217262,5.52378482413), test loss: 3.32388832569\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (15.2340621948,29.0435097485), test loss: 30.715588212\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.47375357151,5.47249473336), test loss: 3.2097779274\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (6.4740114212,28.9035446287), test loss: 34.7799007177\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.18476700783,5.42261830562), test loss: 3.33360059857\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (7.11841201782,28.7673757218), test loss: 30.8929563284\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.16332089901,5.37403357401), test loss: 3.15489903986\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (8.58544540405,28.6386327117), test loss: 31.9859766483\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.59150075912,5.32653990231), test loss: 2.66323218197\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (5.50498485565,28.5122741884), test loss: 32.2824426174\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.367075741291,5.28039208278), test loss: 3.24073658884\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.6349983215,28.3849776076), test loss: 30.3498024225\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.51314282417,5.23541621698), test loss: 2.42826426178\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.26471138,28.2647381832), test loss: 34.2438134909\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.899191021919,5.19202741537), test loss: 3.10337757766\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (11.9721660614,28.1483164944), test loss: 31.0687461376\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.38997936249,5.14977304136), test loss: 2.64654397368\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (20.9003639221,28.0302869147), test loss: 33.7439881563\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.84591543674,5.10837604996), test loss: 2.96604118645\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (11.3438777924,27.9168538797), test loss: 26.2469907761\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.72540855408,5.06805621323), test loss: 2.59478185177\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (36.3662185669,27.8080449969), test loss: 33.6555438519\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.22412276268,5.02852765592), test loss: 3.12530391812\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (35.4824333191,27.700206641), test loss: 25.1398843765\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.00070238113,4.9900314854), test loss: 2.73985551894\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (20.1861743927,27.592590375), test loss: 34.812197113\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.71060347557,4.95243774513), test loss: 3.21580148339\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (6.78326129913,27.4899402357), test loss: 29.4597746372\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.24545478821,4.91606050904), test loss: 3.0244255662\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (28.4230117798,27.3891036961), test loss: 33.8291770935\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.3859462738,4.88047773082), test loss: 2.94460279047\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (42.2584381104,27.2889319972), test loss: 31.9683597565\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (5.49017333984,4.84568859447), test loss: 3.21464858949\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (27.3997421265,27.1935991806), test loss: 31.3842985153\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.3795940876,4.81148966687), test loss: 2.60228150785\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (20.5539970398,27.0987828216), test loss: 33.4633473873\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.80271291733,4.77807262694), test loss: 3.14155711234\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (26.4391174316,27.0056687607), test loss: 29.9271398067\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (5.73112440109,4.74543788596), test loss: 2.51908669472\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (29.2930202484,26.9138769783), test loss: 34.8386406898\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.86063361168,4.71360836219), test loss: 3.13210531473\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (13.4308824539,26.8258392817), test loss: 29.6230446815\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.78734946251,4.68254455828), test loss: 2.59966237843\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (30.2825241089,26.7371434022), test loss: 33.8209036589\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.908276975155,4.65218877875), test loss: 2.98265782744\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (23.2235546112,26.6499053302), test loss: 26.7820626736\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.523141980171,4.62240145418), test loss: 2.52410128862\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.54687690735,26.5669862088), test loss: 34.0577227831\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.56155490875,4.5930618004), test loss: 3.14081030935\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (15.6983699799,26.485359498), test loss: 30.614817524\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.78495073318,4.56440484205), test loss: 3.0190564841\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (27.6346359253,26.4027622136), test loss: 33.4235934973\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.48368477821,4.53625080193), test loss: 3.15679927617\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (6.04552030563,26.3230601068), test loss: 29.4445333481\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.24115931988,4.50902236676), test loss: 2.98898141533\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (12.1639614105,26.2465737797), test loss: 30.2625693798\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.32179546356,4.48228577722), test loss: 2.7793292731\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (16.2856864929,26.1680464094), test loss: 31.7067982674\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.52548968792,4.45599651937), test loss: 3.22177982926\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (11.2840137482,26.0921683486), test loss: 31.9161983967\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (3.74907803535,4.43022836481), test loss: 2.53240617961\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (30.673664093,26.019746929), test loss: 31.9321326494\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.49004352093,4.40473095725), test loss: 3.02202497125\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (20.5657939911,25.9466774721), test loss: 30.7656876087\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.6049349308,4.37985499868), test loss: 2.52293397188\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (24.0203170776,25.8734160674), test loss: 33.5263529301\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.73932504654,4.35535953962), test loss: 3.01678370535\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (13.7526569366,25.8030837452), test loss: 28.9478236198\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.598889827728,4.33159382193), test loss: 2.62476908267\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (55.190700531,25.7351709122), test loss: 33.4493632793\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (3.67505073547,4.3083002125), test loss: 3.04598110318\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (21.8732719421,25.6647911124), test loss: 26.9679708958\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.747449696064,4.28529542826), test loss: 2.56877118349\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (13.6048994064,25.5987826438), test loss: 32.7718488455\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.78265535831,4.26261488602), test loss: 3.0541479975\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (15.0026597977,25.5327231956), test loss: 30.3910828829\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.99536728859,4.2403672497), test loss: 2.94285364747\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (26.6935405731,25.4674258523), test loss: 33.6258951902\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (3.02993369102,4.21855855253), test loss: 3.11135127246\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (6.35106754303,25.4023214556), test loss: 31.7386443138\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.09455394745,4.19708656103), test loss: 3.13172982037\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (6.48100137711,25.3401705996), test loss: 31.3043990135\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.94527363777,4.1761608676), test loss: 2.75235136896\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (18.2426223755,25.2772261596), test loss: 30.9632469416\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.5715045929,4.15558623906), test loss: 3.21588925123\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (26.4838256836,25.215125071), test loss: 31.1263755322\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.92398905754,4.13532198103), test loss: 2.40069462657\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (24.4345493317,25.1556915294), test loss: 33.0519348621\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (5.34389352798,4.11529992703), test loss: 3.02244787216\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (19.9472160339,25.097030847), test loss: 30.9338323116\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (5.97893428802,4.0956603433), test loss: 2.56576618701\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (43.7926712036,25.0377038688), test loss: 32.9885377884\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.70637440681,4.07616528042), test loss: 3.00340040475\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (14.139582634,24.9790972384), test loss: 26.8458758831\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.0087634325,4.05729930799), test loss: 2.54186565131\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (16.8661117554,24.9234474276), test loss: 32.6109483004\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.33544766903,4.03877840406), test loss: 2.91272374392\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (18.5833091736,24.8660172339), test loss: 26.9050671101\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.72250247002,4.02047686155), test loss: 2.55204100013\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (12.2568569183,24.8101236645), test loss: 33.2947522879\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.18032360077,4.00247359506), test loss: 3.14598129392\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (4.30405235291,24.7567692814), test loss: 30.2946496964\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.00097954273,3.98454275018), test loss: 3.03633961827\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (17.4104175568,24.7029507332), test loss: 35.0517398357\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.98937702179,3.96700046171), test loss: 3.15007761121\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (4.82920837402,24.6482336741), test loss: 30.8521906614\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.629377305508,3.94967435773), test loss: 2.9991030246\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (17.5407924652,24.595997465), test loss: 31.5287009239\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.16657352448,3.93286340173), test loss: 2.57619318962\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (19.0002861023,24.545328533), test loss: 33.1636115551\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.308489531279,3.91626161952), test loss: 3.22778933048\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (26.900680542,24.4926235029), test loss: 30.2051040649\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.51108884811,3.89988588966), test loss: 2.33652255237\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (5.29128742218,24.4425716181), test loss: 34.0968972445\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.82905888557,3.88364687617), test loss: 3.01889997423\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (40.8045387268,24.3933046663), test loss: 29.7002353668\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (4.22293424606,3.86767614849), test loss: 2.53047554642\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (20.4989280701,24.3435091264), test loss: 33.5347488642\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.32826757431,3.85191029397), test loss: 2.91699951589\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (20.6429405212,24.2939170164), test loss: 26.1336487293\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.99917191267,3.83639028788), test loss: 2.51847028732\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (14.7780361176,24.2466010398), test loss: 33.0948579311\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (3.45594358444,3.8212590599), test loss: 3.0112539351\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (11.2264328003,24.1983717543), test loss: 25.6019745588\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.60783970356,3.80626047674), test loss: 2.54331732988\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (18.1001033783,24.1511668042), test loss: 34.149417448\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.52036213875,3.79149481405), test loss: 3.08281256557\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (13.5284938812,24.1053487756), test loss: 29.0112981319\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.26992464066,3.77689980612), test loss: 2.89268352389\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (8.46160507202,24.0599684087), test loss: 34.4483138084\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.851793408394,3.76244193282), test loss: 2.95488861203\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (13.0426197052,24.0140380058), test loss: 31.4461283445\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.33647084236,3.74814253587), test loss: 3.10951370299\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (29.6125049591,23.9690121191), test loss: 31.5718555927\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.32852268219,3.7342177631), test loss: 2.57267881632\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (32.2088546753,23.9256878353), test loss: 33.5149786949\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.672398090363,3.72049715176), test loss: 3.07869265974\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (11.9280738831,23.8805381789), test loss: 29.9563554287\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.936176717281,3.70694581964), test loss: 2.44195304364\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.31817626953,23.8369323598), test loss: 34.4778419495\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.996233046055,3.69356264204), test loss: 3.07500112653\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (9.52684211731,23.7951818947), test loss: 28.7673186779\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.75086021423,3.68021516728), test loss: 2.48989285529\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (18.4206466675,23.7530182933), test loss: 33.8940836668\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.63046216965,3.6671118563), test loss: 2.91373960376\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (10.4510011673,23.7096618099), test loss: 26.3665110588\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.10576820374,3.65411597641), test loss: 2.43894138038\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (30.2613697052,23.6687153722), test loss: 33.8014278412\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.75680911541,3.64151353483), test loss: 3.04732352793\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (30.5858154297,23.6287484596), test loss: 30.6799244881\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (4.09770727158,3.62906284028), test loss: 2.96784240603\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (26.1496448517,23.5867691692), test loss: 33.1381747007\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.31556844711,3.61669664137), test loss: 3.05075636953\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (8.15029811859,23.5468004271), test loss: 28.9933534622\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.67567491531,3.60441784586), test loss: 2.85285228193\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (6.41135644913,23.507624149), test loss: 30.643647337\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.84656739235,3.59235029132), test loss: 2.68474301845\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (14.9521389008,23.4682809586), test loss: 31.7542375088\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.04933786392,3.58037622544), test loss: 3.12043288946\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (20.3109970093,23.4281645594), test loss: 33.1877967358\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.23259735107,3.56852921636), test loss: 2.57747123092\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (11.0225477219,23.3900940102), test loss: 32.0616631985\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.897766232491,3.55701229444), test loss: 2.9541053921\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (39.518196106,23.3517283684), test loss: 31.2149940968\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (3.06190037727,3.54558267146), test loss: 2.49747703969\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (12.3968772888,23.3133143178), test loss: 33.5314797878\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.97335743904,3.53427775341), test loss: 2.97678358555\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (20.6777801514,23.2764398377), test loss: 28.7876833916\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.19781112671,3.52305696163), test loss: 2.54167015553\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (20.9069709778,23.2397022502), test loss: 33.5031781673\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.75185441971,3.51197802334), test loss: 2.90160792172\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (23.2857227325,23.2026059126), test loss: 26.8415194988\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.96700620651,3.50097059463), test loss: 2.51492655277\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (12.8862094879,23.1656716275), test loss: 32.7885743618\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.54810500145,3.49020842759), test loss: 2.98329707533\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (14.33020401,23.1304723782), test loss: 29.9104999542\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.583956360817,3.47961307928), test loss: 2.81770709753\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (7.43840265274,23.0937418808), test loss: 33.2615160465\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.98720192909,3.46913266507), test loss: 2.9818618089\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (7.31129360199,23.0580198626), test loss: 30.8002026558\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.765693128109,3.45875055314), test loss: 3.07811035514\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (13.9000167847,23.0237987292), test loss: 32.3942749977\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.57702565193,3.44838274044), test loss: 2.7063114956\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (18.7465419769,22.9894418953), test loss: 30.9311303854\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.946295619011,3.43820338444), test loss: 3.17537677288\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (11.6777162552,22.9535165951), test loss: 31.3342614651\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.10261011124,3.42803745383), test loss: 2.34285267889\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 7\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold7/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (340.61026001,inf), test loss: 197.951126099\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (307.707702637,inf), test loss: 350.762393188\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (17.207868576,103.512232209), test loss: 44.7469647408\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.77755212784,49.8650819151), test loss: 3.1900806278\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (20.8570709229,74.9321796579), test loss: 36.9366650581\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.4649014473,26.2830575576), test loss: 2.34918626547\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (14.1160526276,65.1168191223), test loss: 42.094347477\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.93649196625,18.4038440751), test loss: 3.20649958849\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.9725265503,60.2507002819), test loss: 40.0865262508\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.07582902908,14.4626348454), test loss: 2.94641987085\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (30.4712619781,57.2527106903), test loss: 42.2818804741\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.36183214188,12.0986103544), test loss: 3.25109345317\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (27.1161117554,55.1651861575), test loss: 40.1825279236\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.13184475899,10.5164485581), test loss: 3.19336811602\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (43.0007781982,53.644140336), test loss: 43.8928024769\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.48395562172,9.38283484229), test loss: 3.12232159376\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (57.723651886,52.469007236), test loss: 41.2129043579\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.96867752075,8.53023016316), test loss: 3.15482069552\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (90.1259002686,51.5618248934), test loss: 39.6196824074\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.86591720581,7.86709496677), test loss: 2.59123395383\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (18.220911026,50.7717025161), test loss: 42.0015570641\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.37924551964,7.33559886077), test loss: 3.15531077385\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (49.9805145264,50.1248529659), test loss: 41.3997795105\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.92040491104,6.89970683118), test loss: 2.41881604791\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (87.286781311,49.5321725757), test loss: 44.632544899\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.00500202179,6.53413295336), test loss: 3.3902882278\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (67.9901275635,49.0091366512), test loss: 40.5345340967\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.81790959835,6.2226303887), test loss: 2.26287060976\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (26.340555191,48.5380018081), test loss: 42.5095021248\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (5.95378303528,5.95471332997), test loss: 3.27560569644\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (28.0778408051,48.1182022393), test loss: 37.6856725216\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.88722968102,5.72184489789), test loss: 2.27886156738\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (37.2098121643,47.7360764485), test loss: 41.7582268238\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.9423418045,5.51786026541), test loss: 3.17965331078\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (23.28748703,47.3496374331), test loss: 36.7383246422\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.382171392441,5.33623882592), test loss: 2.44500896335\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (55.6944084167,46.990835142), test loss: 42.3012980461\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.54146790504,5.17327695279), test loss: 3.14155604839\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (16.1519393921,46.6406353699), test loss: 36.3808915854\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.69749546051,5.02569643618), test loss: 2.37414759994\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (18.6772727966,46.3328041245), test loss: 40.2983101845\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.32473599911,4.89268996958), test loss: 3.06158510745\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (106.966934204,46.016902394), test loss: 31.9716050148\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.41880369186,4.77194403438), test loss: 2.29981631339\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (54.7339134216,45.7034406948), test loss: 38.0803788185\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.29695165157,4.66112076838), test loss: 2.97995243967\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (104.989250183,45.3835692981), test loss: 30.2389442444\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (4.02394962311,4.55840027022), test loss: 2.62258763313\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (32.9785346985,45.0636927405), test loss: 37.4033105135\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.98868942261,4.46328689089), test loss: 3.01004253328\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (21.4028358459,44.7683026751), test loss: 34.1728680611\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.38299870491,4.37474177254), test loss: 2.9174864158\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (60.9308013916,44.4597329967), test loss: 37.9972986698\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.77840304375,4.29309780748), test loss: 3.05159811676\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (31.5394668579,44.1538720329), test loss: 35.1230696678\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.61474275589,4.21692080862), test loss: 3.03069134057\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (17.0284461975,43.8317145017), test loss: 33.607651782\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.13589119911,4.14544866704), test loss: 2.56367409527\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (28.8394107819,43.5052069395), test loss: 34.9408524513\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.78228139877,4.07772478523), test loss: 3.07711282969\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (26.7826919556,43.1590844987), test loss: 34.1118609428\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.89870285988,4.01352994905), test loss: 2.32978420258\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (87.0792922974,42.8180373929), test loss: 36.8855410099\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.37326145172,3.9536961224), test loss: 3.27923457623\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (14.3802833557,42.4617424663), test loss: 32.6631548882\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.74417638779,3.89758474529), test loss: 2.16670868099\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (40.2708358765,42.1011346724), test loss: 35.2508097172\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.32586669922,3.84440008975), test loss: 3.15643690526\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (18.1509971619,41.7247310013), test loss: 29.3561415195\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.62535977364,3.7933097994), test loss: 2.21019626409\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (5.5762348175,41.351073564), test loss: 32.9960472107\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.20927667618,3.7447531865), test loss: 3.07374721467\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (40.6933517456,40.980315294), test loss: 28.05215168\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.92729187012,3.69883996389), test loss: 2.423419635\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (25.4486446381,40.5926888311), test loss: 34.1439014673\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.40378403664,3.65518644943), test loss: 3.14590482116\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (18.425403595,40.2105940262), test loss: 26.7188391209\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.576839327812,3.61391925997), test loss: 2.47629005909\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (21.8435592651,39.8172269193), test loss: 32.4976743221\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.24249875546,3.57450688953), test loss: 3.08373513296\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (17.2634010315,39.4279241335), test loss: 22.6739217997\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.36475038528,3.53646043951), test loss: 2.404954575\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (10.8246955872,39.0372178087), test loss: 30.2675475836\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.07096242905,3.50009526628), test loss: 3.0047961086\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (26.9010791779,38.6476326106), test loss: 21.0021844149\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.80401885509,3.46536923515), test loss: 2.45765880644\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (13.1133241653,38.2642612876), test loss: 30.0995053768\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.18001616001,3.4326041786), test loss: 3.16601066589\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (40.3151245117,37.8824717364), test loss: 25.7552206278\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.83861637115,3.40130349931), test loss: 2.9530254066\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (21.3971862793,37.5031508741), test loss: 30.0967981339\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.44875717163,3.37093173702), test loss: 3.17558827996\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (16.5798492432,37.1357343865), test loss: 26.2127921581\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.93882393837,3.34154767619), test loss: 3.0522895813\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (39.622718811,36.7757266653), test loss: 26.3951073885\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.83152198792,3.31355090137), test loss: 2.62977458835\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (14.020524025,36.4195821124), test loss: 27.1479212761\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.06806468964,3.28663518992), test loss: 3.17683323026\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (12.4424934387,36.0758783331), test loss: 27.1744196892\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.609100103378,3.26100101585), test loss: 2.42501228154\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (5.73634767532,35.7369045868), test loss: 29.47977705\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.08480584621,3.23610030046), test loss: 3.31687700748\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (20.5281162262,35.4100526924), test loss: 26.5654709339\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.3111436367,3.21190494927), test loss: 2.26677724719\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (6.38928365707,35.0951148757), test loss: 30.9712419987\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.280597507954,3.18836009459), test loss: 3.25252010822\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (7.74830055237,34.7836634413), test loss: 26.5872221947\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.17028653622,3.16590580521), test loss: 2.32679498792\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (12.3145303726,34.4865034442), test loss: 30.7082296848\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.20769572258,3.14450916987), test loss: 3.16509339809\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (30.3814086914,34.1960065986), test loss: 26.7599032402\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.19246983528,3.12373705629), test loss: 2.42394488454\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (12.5505142212,33.9135874425), test loss: 31.8975794077\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.471322178841,3.10348847311), test loss: 3.22370596826\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (35.4441833496,33.6423708742), test loss: 25.1116592407\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.04131555557,3.08361847472), test loss: 2.53187274933\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (18.5199794769,33.377678095), test loss: 32.5214111567\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.91350078583,3.06464107068), test loss: 3.16697728932\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.2586288452,33.1192395493), test loss: 23.0183487892\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.03604674339,3.04633733591), test loss: 2.46935438961\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.4733562469,32.8729303345), test loss: 30.1366906643\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.66448187828,3.02879207321), test loss: 3.03861311674\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (33.8666000366,32.6288103154), test loss: 21.4997141838\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.99864339828,3.01151184843), test loss: 2.38655522168\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (7.20921230316,32.3951416616), test loss: 31.3797670841\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.35841214657,2.99443004168), test loss: 3.16893945038\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.0211019516,32.1694245573), test loss: 26.4779785633\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.1051005125,2.97807945888), test loss: 2.92952657938\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (18.6185684204,31.9453449758), test loss: 32.6892554283\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.61805546284,2.96215351917), test loss: 3.2161938116\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (9.6189289093,31.7315310889), test loss: 25.6586401463\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.43980622292,2.94684185908), test loss: 3.07272898555\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.3746509552,31.5216166548), test loss: 27.0277112246\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.56740295887,2.93202114414), test loss: 2.61585041881\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (9.59824466705,31.3187608059), test loss: 27.5113089919\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.48906564713,2.91722802587), test loss: 3.1774400562\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (12.4799928665,31.1215985617), test loss: 28.9321021557\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.02632474899,2.90281203102), test loss: 2.40447096825\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (16.8203659058,30.9277620369), test loss: 29.9495875597\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.36558818817,2.88878056678), test loss: 3.29531665444\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (13.7503328323,30.7391844888), test loss: 27.8521578074\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.4127395153,2.87534517746), test loss: 2.38913282156\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.6068487167,30.5579404143), test loss: 30.3033088207\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.46114063263,2.86235502401), test loss: 3.22015465498\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (11.3262586594,30.3780491149), test loss: 28.3454459667\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.573483347893,2.84940262839), test loss: 2.35165421516\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (11.2784433365,30.2065995836), test loss: 31.2413922787\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.56793022156,2.83673594051), test loss: 3.15570902824\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (10.0952615738,30.037575742), test loss: 26.5522111416\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.08471000195,2.82441256337), test loss: 2.38949360847\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (25.6287269592,29.8705437252), test loss: 33.9770036221\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.6575152874,2.81239271222), test loss: 3.27084563971\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (23.5628623962,29.7109165335), test loss: 25.8594436646\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.791034936905,2.80089293593), test loss: 2.51895014644\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (25.9334526062,29.5524611492), test loss: 32.7095208645\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.52766704559,2.78952743621), test loss: 3.15329540074\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (16.050699234,29.3997122452), test loss: 23.6936486483\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.631511867046,2.77830760076), test loss: 2.50112001002\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (14.9805793762,29.2504497252), test loss: 31.0738285065\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (5.45212745667,2.76732082186), test loss: 3.00836725831\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (13.7082204819,29.1007585051), test loss: 21.7968211174\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.939851403236,2.75646619049), test loss: 2.35089284778\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (8.51323890686,28.9586174536), test loss: 31.53352108\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.409506767988,2.74616376046), test loss: 3.11880701333\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (18.6596221924,28.8189997439), test loss: 26.6785685062\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.388262450695,2.73612968247), test loss: 2.91215097904\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (6.67168521881,28.6813663276), test loss: 32.2062534809\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.62217509747,2.72617547892), test loss: 3.1786489889\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (6.59654283524,28.5484872563), test loss: 26.01640625\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.72184228897,2.71626351996), test loss: 2.97559002638\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (31.2091197968,28.4173963775), test loss: 31.0971833229\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.55469274521,2.70665960267), test loss: 2.72958751917\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (28.3999519348,28.2877987589), test loss: 27.9969777346\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.93249225616,2.69724589135), test loss: 3.18512254357\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (16.5169715881,28.1642087209), test loss: 27.8956235409\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.29937291145,2.68826650946), test loss: 2.53402743936\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (17.0679416656,28.0393468294), test loss: 29.711482048\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.852917313576,2.67922971364), test loss: 3.28626577258\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (13.5482196808,27.9193943507), test loss: 28.0775876045\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.52348041534,2.6703064213), test loss: 2.37077856809\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (11.7266635895,27.8022952412), test loss: 30.4801329613\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.70844984055,2.66157673957), test loss: 3.18761636019\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (8.25873088837,27.6842716173), test loss: 29.5377985954\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.42192387581,2.65300077773), test loss: 2.31655707359\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (5.20189905167,27.5718581025), test loss: 30.874834466\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.939768195152,2.64473541052), test loss: 3.13442158103\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (33.8522872925,27.4604735219), test loss: 26.7813322067\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.86206793785,2.63674051662), test loss: 2.39201630056\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (15.8279781342,27.3511380141), test loss: 33.2258756638\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.04714035988,2.62867662035), test loss: 3.23319159746\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (14.5959978104,27.2445164858), test loss: 25.3551743031\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.89791369438,2.62068531024), test loss: 2.44108366072\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (24.964673996,27.1387120085), test loss: 32.7569242001\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.596785783768,2.61284569026), test loss: 3.06159774661\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (24.2492599487,27.035411563), test loss: 24.5621145248\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.92441058159,2.60532444024), test loss: 2.52488546669\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (50.5441017151,26.9354639446), test loss: 30.8013678551\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (4.43287801743,2.59806345843), test loss: 2.98277542293\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (21.7692527771,26.8342447892), test loss: 22.0739204407\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.51193118095,2.59064009826), test loss: 2.33859701753\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (8.56929016113,26.7373941293), test loss: 31.4019867897\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.92898917198,2.58333644704), test loss: 3.04549892247\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (15.459066391,26.6421491977), test loss: 26.6115136623\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.15637898445,2.57625532867), test loss: 2.84253846109\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (8.73128890991,26.5459987188), test loss: 32.7363415241\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.43774747849,2.56920301588), test loss: 3.12075069249\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (15.7852535248,26.4548744432), test loss: 25.7296162367\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.574356019497,2.56247376611), test loss: 2.97428353131\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (16.3159370422,26.362575788), test loss: 32.3715138912\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.63047862053,2.55582703368), test loss: 2.99551467746\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (19.2624130249,26.2736862333), test loss: 27.3097410679\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.60957145691,2.54911197104), test loss: 3.14666539431\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (11.3989343643,26.1858049744), test loss: 28.8769699097\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.468991428614,2.54252331382), test loss: 2.54473356307\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (35.9373703003,26.0973213953), test loss: 29.1315202713\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.14936888218,2.53597974986), test loss: 3.20250050426\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (10.3005886078,26.0125341597), test loss: 27.9449036837\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.09653532505,2.52974413352), test loss: 2.35385946631\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (12.160982132,25.9289175598), test loss: 29.8074620485\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.23460769653,2.52361954673), test loss: 3.16846110225\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (5.3404250145,25.8456610231), test loss: 28.8804722309\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.66990852356,2.51755085922), test loss: 2.32066203654\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (29.5876731873,25.7654679478), test loss: 30.9525502205\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.17537808418,2.5114007129), test loss: 3.09553419948\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (19.7966632843,25.6852507199), test loss: 26.5335235596\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.30473482609,2.50542954122), test loss: 2.37708057165\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (12.1847724915,25.6058689542), test loss: 33.3954804897\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.15076637268,2.49956366246), test loss: 3.13774796724\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (10.3237142563,25.529188613), test loss: 25.3365965366\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.07939577103,2.49391931352), test loss: 2.4470182538\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (10.0360460281,25.4516688489), test loss: 31.4758434772\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.03099799156,2.4882554719), test loss: 3.02047921419\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (8.68780899048,25.3762938512), test loss: 24.9096372604\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.53433310986,2.48259259537), test loss: 2.51320700049\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.81723880768,25.303053324), test loss: 31.100368309\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.413565039635,2.47696780902), test loss: 2.95165398717\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (6.90274715424,25.2281479173), test loss: 21.8996879578\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.13190329075,2.47147724981), test loss: 2.30049638748\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (11.8329782486,25.1567607677), test loss: 31.0801055193\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.17448282242,2.46618284935), test loss: 3.00531822741\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (17.777256012,25.0852200614), test loss: 26.4444713593\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.72629702091,2.46096770268), test loss: 2.75371370912\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (9.90837860107,25.014829832), test loss: 31.8577485085\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.98697328568,2.45575352409), test loss: 3.05441473722\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (22.7081832886,24.94624315), test loss: 25.9666414976\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.66560077667,2.45048850772), test loss: 2.99082843065\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (16.9146842957,24.8777683087), test loss: 32.7489446402\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (3.42111754417,2.44536713059), test loss: 3.02170171291\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (3.57477736473,24.8098001705), test loss: 27.0054385662\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.10505378246,2.44036543086), test loss: 3.11019250751\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (9.94258022308,24.7443787116), test loss: 28.1684130192\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.442365378141,2.43550572093), test loss: 2.55742911249\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (17.6031036377,24.6774361235), test loss: 28.2674861193\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.1374797821,2.43061037023), test loss: 3.16740439236\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.44013834,24.6132575025), test loss: 28.8799501896\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.12027537823,2.42569021872), test loss: 2.35151741803\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (17.2397518158,24.5502201299), test loss: 30.5822726965\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.28659260273,2.42092693806), test loss: 3.1684222281\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (28.9954376221,24.4859293385), test loss: 29.177302742\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.51990544796,2.41617846322), test loss: 2.2347125262\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (9.49542999268,24.4243026414), test loss: 30.8710626602\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.841939091682,2.41157154384), test loss: 3.06022382379\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (23.9405822754,24.3621172168), test loss: 26.6287077427\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.9372934103,2.40709456009), test loss: 2.32175401747\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (6.20728063583,24.3015787843), test loss: 33.0112081051\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.86789894104,2.40248265827), test loss: 3.06362790465\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (6.00426006317,24.2420555002), test loss: 25.3358934879\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.07999968529,2.39796569547), test loss: 2.37532809079\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (52.4602661133,24.1823821125), test loss: 31.7247700691\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.99467086792,2.39347768341), test loss: 2.9539978236\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (14.4045906067,24.1234522016), test loss: 24.5475227356\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.82886123657,2.38915339708), test loss: 2.44305433333\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (19.2579936981,24.066082989), test loss: 31.4390434027\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.37793111801,2.38492145146), test loss: 2.90652464628\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (8.18202972412,24.0080654446), test loss: 21.9972474098\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.711377263069,2.38065457629), test loss: 2.28599244654\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (10.6498746872,23.9527135348), test loss: 30.9791826725\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.88183069229,2.37637522526), test loss: 2.94928673804\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (13.3025474548,23.8972452448), test loss: 26.1969570637\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.08183538914,2.37220112638), test loss: 2.75626445413\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (20.6241550446,23.8411426557), test loss: 31.7488617182\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.67255377769,2.36805240832), test loss: 2.99574024379\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (11.5081672668,23.7871975315), test loss: 25.7482088566\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.77110469341,2.36404783923), test loss: 2.94676221609\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (20.3413772583,23.7324722252), test loss: 32.1401233673\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.18193137646,2.36004114944), test loss: 3.01705197021\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (29.6684398651,23.6797202605), test loss: 26.4143559456\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.53725397587,2.35602790768), test loss: 3.04548909068\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (15.6216497421,23.6277136526), test loss: 28.2237884998\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.26799821854,2.35202727891), test loss: 2.47220892906\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.24912261963,23.5739370915), test loss: 28.6654555917\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.727612316608,2.34808036968), test loss: 3.02707565129\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.51781177521,23.5230346379), test loss: 28.953456831\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.905443668365,2.3442692153), test loss: 2.36756451875\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (9.18920135498,23.4718903246), test loss: 28.4941250563\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.67771553993,2.34054143201), test loss: 3.14824652821\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (13.4396333694,23.4213216777), test loss: 27.7850219727\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.87997031212,2.33678132138), test loss: 2.22389067113\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (7.15385770798,23.3722278127), test loss: 31.4826819897\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.13663840294,2.33300242313), test loss: 3.00136561692\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (21.6387138367,23.3229201782), test loss: 25.4609859467\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (3.15896034241,2.32930921932), test loss: 2.21770556569\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (14.1614093781,23.2731551927), test loss: 32.8337001204\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.83507966995,2.3256186592), test loss: 3.05347727239\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (17.6769943237,23.2259324633), test loss: 26.1744825363\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.2133243084,2.3221011744), test loss: 2.39648561478\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.0368475914,23.1770613289), test loss: 31.2854195118\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.567351520061,2.31851817243), test loss: 2.95920585245\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (28.6838684082,23.1303903392), test loss: 24.7354557037\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (4.62438392639,2.31493408049), test loss: 2.39814299196\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (19.9673557281,23.0842257521), test loss: 31.6950386047\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.03719711304,2.31140508608), test loss: 2.87985682487\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (12.8855962753,23.036580123), test loss: 21.9488659859\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.78172338009,2.3078945555), test loss: 2.23461441249\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (12.5813531876,22.9911799191), test loss: 30.7225197792\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.21780276299,2.30447639364), test loss: 2.89548805356\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (24.6937770844,22.9454277655), test loss: 25.9635195017\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.13568031788,2.30116946816), test loss: 2.74503636956\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (16.094543457,22.9006783588), test loss: 30.4527756214\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (4.1043548584,2.2978012434), test loss: 2.92040609419\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 8\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold8/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (356.536895752,inf), test loss: 185.120584106\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (331.47668457,inf), test loss: 398.351437378\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (63.1105194092,118.918241079), test loss: 45.8175076485\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.8274834156,91.2130318137), test loss: 3.29210572541\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (48.4293060303,84.3060524223), test loss: 34.6920692444\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.82339596748,47.138515403), test loss: 2.64025068879\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (10.0162239075,72.3860285932), test loss: 41.6050123215\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.4132475853,32.4412460921), test loss: 3.44090799093\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (42.4384155273,66.509381852), test loss: 39.0288604975\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.75338876247,25.0966827692), test loss: 3.14465126395\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (11.5427341461,62.8935772033), test loss: 40.9179176092\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.06967639923,20.6938822115), test loss: 3.33080985397\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (37.8497848511,60.4507942819), test loss: 42.5318518639\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (4.56753063202,17.7585618671), test loss: 3.51174211204\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (112.704360962,58.6793633276), test loss: 37.7539580822\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (6.50796222687,15.6635974987), test loss: 2.84251730442\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (98.6635513306,57.2682933804), test loss: 46.2504387856\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.56321024895,14.0902197916), test loss: 3.62528633475\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (62.711265564,56.2225063782), test loss: 39.2609598637\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.09882259369,12.8659562001), test loss: 2.68242935538\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (26.1815490723,55.341988088), test loss: 46.3130947113\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.79404735565,11.8856347105), test loss: 3.6775192976\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (92.5269927979,54.6451096809), test loss: 35.2847445965\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.62058496475,11.0849846345), test loss: 2.78913654089\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (114.996353149,54.0396943379), test loss: 44.3079204559\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.97064971924,10.4198017401), test loss: 3.58826294541\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (36.4376792908,53.4811846547), test loss: 34.0109988213\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.14262866974,9.85387110996), test loss: 2.77949070334\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (47.4570922852,52.9890266905), test loss: 42.5687590599\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.82088971138,9.36915008699), test loss: 3.29910771847\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (50.7324905396,52.5360674268), test loss: 33.5343277931\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.2732322216,8.9481541266), test loss: 2.52849115729\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (18.2473602295,52.1186022815), test loss: 40.41126194\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.05003738403,8.57790670668), test loss: 3.46269254088\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (25.5352592468,51.7760287749), test loss: 37.6358138561\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.82794141769,8.25206889315), test loss: 2.96186750531\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (31.3495635986,51.4417842275), test loss: 38.3997636795\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.02864336967,7.96278788721), test loss: 3.35506480038\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (51.7597618103,51.1265856801), test loss: 39.1935760498\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (5.32866191864,7.70394752027), test loss: 3.31628690958\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (35.3831176758,50.8054218533), test loss: 35.1568926334\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (0.736753702164,7.47031833869), test loss: 2.82993580699\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (45.0846557617,50.492991776), test loss: 41.9576893806\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.13336706161,7.2584724443), test loss: 3.4462906152\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (15.1996030807,50.2014574324), test loss: 36.5695055246\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.964229762554,7.06391923776), test loss: 2.67261829972\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (20.4138469696,49.9331934405), test loss: 42.4360300064\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.826341509819,6.88690414067), test loss: 3.67070633173\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (36.0474624634,49.6583471377), test loss: 34.9188823223\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.2805942297,6.72445739495), test loss: 2.7462036401\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (42.9119949341,49.3847992272), test loss: 42.6235630989\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.2102432251,6.57462633002), test loss: 3.44184435606\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (81.9522018433,49.1178456705), test loss: 32.3755503893\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.73291444778,6.43551906559), test loss: 2.65212420821\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (49.2831306458,48.826313459), test loss: 41.2721302986\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.9245827198,6.30528429458), test loss: 3.25987946987\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (90.289680481,48.5648047193), test loss: 30.5851588488\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.66261959076,6.18304225252), test loss: 2.61821698248\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (15.2503814697,48.2850862684), test loss: 37.4600003719\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.1596763134,6.06777754033), test loss: 3.30903994143\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (35.6138153076,48.0273018284), test loss: 30.7592556\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (7.86318874359,5.96019767087), test loss: 2.81720559597\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (18.6187095642,47.7582662974), test loss: 35.3877669811\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.23458862305,5.85821327736), test loss: 3.26959019303\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (36.3127288818,47.4826416473), test loss: 34.5765243053\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.93611574173,5.76140565573), test loss: 3.1937332809\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (52.3486785889,47.2010102233), test loss: 36.6904092312\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.14046502113,5.66997027818), test loss: 3.01145018637\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (33.5407333374,46.9097857068), test loss: 35.8472505093\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.00749397278,5.58218629191), test loss: 3.25986441076\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (35.3323860168,46.6190580452), test loss: 31.4187417507\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.20708942413,5.49769332769), test loss: 2.52068837285\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (47.0721282959,46.3319370816), test loss: 37.35874753\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.71726322174,5.41757138903), test loss: 3.52481301129\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (45.4821243286,46.0299442163), test loss: 29.6563191414\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.65225934982,5.34079396439), test loss: 2.3826379776\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (31.1841583252,45.725432527), test loss: 36.5424081326\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.555388152599,5.26701961856), test loss: 3.31307500005\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (20.0706062317,45.404233171), test loss: 27.114526844\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.05152595043,5.19619980949), test loss: 2.45520653874\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (31.7282047272,45.0707139952), test loss: 35.6385346889\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.36213302612,5.12782391547), test loss: 3.28716108203\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (27.8154945374,44.7321408401), test loss: 25.2237127781\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (5.91138839722,5.06157531082), test loss: 2.50945671797\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (29.0678348541,44.3898323117), test loss: 32.0433327913\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.86938858032,4.99734517232), test loss: 3.21312672496\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (74.0404205322,44.0390523289), test loss: 23.3613703012\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.46076107025,4.93574161978), test loss: 2.34842632562\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (21.120464325,43.6805567271), test loss: 30.5477565527\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.93220043182,4.8767769529), test loss: 3.20648633242\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (19.1424770355,43.3181805599), test loss: 27.9258139133\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.91562962532,4.8194697623), test loss: 3.05701392889\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (7.70513343811,42.9421643834), test loss: 28.6942094326\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.486879020929,4.76409476553), test loss: 3.15865668729\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.9996833801,42.5731300355), test loss: 29.7830892563\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.66449403763,4.71011601733), test loss: 3.32774553522\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (19.5557022095,42.1970719342), test loss: 25.1621507168\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.91621541977,4.6576341263), test loss: 2.60958574414\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (30.9605102539,41.8265121208), test loss: 32.1171703815\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.88862514496,4.60736222348), test loss: 3.46485536695\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (13.2827396393,41.4545360235), test loss: 25.0004719496\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.52092850208,4.55852740232), test loss: 2.33091003299\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (18.0309257507,41.0861828424), test loss: 30.9819392681\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.460229426622,4.5113461896), test loss: 3.3641739428\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (32.9569473267,40.7192075594), test loss: 24.2086700201\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.67967176437,4.46615623509), test loss: 2.53443659544\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.4985179901,40.3551449071), test loss: 31.1347739935\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.26914274693,4.4219834298), test loss: 3.44713993371\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (23.6390304565,40.0021348894), test loss: 23.9446409225\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.06338167191,4.37892202748), test loss: 2.62346935868\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (18.8325252533,39.6542068092), test loss: 29.5165118217\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.944840312004,4.33728714567), test loss: 3.27094824314\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (39.1738014221,39.3114335358), test loss: 22.5964785576\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.65871739388,4.29712566873), test loss: 2.47211197764\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (15.3976612091,38.9784959285), test loss: 29.4863109112\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.27694559097,4.2584900046), test loss: 3.29978827089\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (12.6587791443,38.6514054553), test loss: 28.0833380222\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.15629839897,4.22119521817), test loss: 3.17857918739\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (20.2341098785,38.3309471741), test loss: 29.1492616177\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.4996547699,4.1848405022), test loss: 3.36440554261\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (7.46310520172,38.0216041638), test loss: 27.9560257912\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.43843090534,4.14954333829), test loss: 3.31468484551\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (16.9180526733,37.719551267), test loss: 25.3147530079\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.682966828346,4.11502076748), test loss: 2.69505924582\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (9.88395786285,37.4233524779), test loss: 31.1651182175\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (4.48906707764,4.08164678296), test loss: 3.55672415197\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (16.5342254639,37.1357479761), test loss: 25.7818569303\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.34197425842,4.04961893329), test loss: 2.49190474749\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (5.81939554214,36.8574966008), test loss: 31.3151763678\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.66447329521,4.01852357694), test loss: 3.52943542004\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (15.0107278824,36.5829202165), test loss: 26.3721983194\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.39712941647,3.98836277033), test loss: 2.54953548759\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (22.3915672302,36.3169518041), test loss: 32.4264126301\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (6.0204744339,3.95890318577), test loss: 3.54352512956\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (14.2623157501,36.0584444138), test loss: 25.5906041622\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.22706818581,3.92991200164), test loss: 2.66817967892\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (51.0894622803,35.8057706108), test loss: 31.3349137306\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.48634505272,3.90194165927), test loss: 3.36884068549\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (20.8020896912,35.5584545826), test loss: 24.6593168736\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.08193063736,3.87464647262), test loss: 2.64184423983\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (19.9409122467,35.3203031162), test loss: 30.6647645235\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.30727815628,3.84827390551), test loss: 3.32023470998\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.7545661926,35.0862105903), test loss: 29.2643961906\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.29168200493,3.82285071904), test loss: 3.23217884302\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (6.235019207,34.8568718147), test loss: 30.6689112186\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.66236770153,3.79773905551), test loss: 3.43703071475\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (11.618555069,34.6370725352), test loss: 28.4259912729\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.42864990234,3.77318753187), test loss: 3.29947262406\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.4351139069,34.4206005011), test loss: 26.0010852337\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.35295939445,3.74920743619), test loss: 2.80205378532\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (7.613260746,34.2072253087), test loss: 29.6938632965\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.97445940971,3.72580043898), test loss: 3.40345822424\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.4430541992,34.0018738705), test loss: 27.3984238148\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.39872598648,3.70331309693), test loss: 2.56942924708\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (59.1339492798,33.8028570564), test loss: 30.753499198\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.35101747513,3.68146123941), test loss: 3.53089758754\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (27.8541316986,33.6034402422), test loss: 27.0982304335\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.30732631683,3.65992189707), test loss: 2.45360462815\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (21.180809021,33.4130152603), test loss: 31.1908965111\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.71379584074,3.63877897362), test loss: 3.41391038001\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (23.1097755432,33.2245699613), test loss: 27.1283602238\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.99969625473,3.61798785415), test loss: 2.63831969798\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (27.3000469208,33.0405479382), test loss: 31.7413267732\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.84648251534,3.59777989742), test loss: 3.38185714483\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (19.7398433685,32.8609073242), test loss: 25.8028823376\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.28043580055,3.57818426901), test loss: 2.75530014932\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (9.46385192871,32.6864596402), test loss: 29.9297634602\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.50541305542,3.55912366283), test loss: 3.23545024693\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (19.2833156586,32.5140499597), test loss: 24.6590184927\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.60996556282,3.54053313367), test loss: 2.62641558945\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (9.63373565674,32.3457866011), test loss: 30.8950207233\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.92304992676,3.52222195721), test loss: 3.34380558878\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (34.6106033325,32.1824740996), test loss: 29.6918092489\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.16310238838,3.50405046626), test loss: 3.41291584373\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (13.5626144409,32.0205717752), test loss: 32.0730992794\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.86549663544,3.48636786991), test loss: 3.44116611481\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.5108184814,31.861711932), test loss: 31.5078150272\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.907610416412,3.46902748713), test loss: 3.47445779145\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (16.7975101471,31.7078891985), test loss: 27.4814352989\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (4.53847980499,3.45230826852), test loss: 2.54551401138\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (18.7929515839,31.5561084494), test loss: 31.2649937153\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.621125817299,3.43592753585), test loss: 3.57996045947\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (6.86445093155,31.4065821051), test loss: 27.1233458519\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.13504564762,3.41978292022), test loss: 2.45791139603\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (20.3444137573,31.2626604032), test loss: 30.6945168495\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.02378892899,3.40384076997), test loss: 3.44374165833\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (11.5667324066,31.120170602), test loss: 27.2390019417\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.382539123297,3.38814450824), test loss: 2.64119323194\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (18.9492340088,30.9782113021), test loss: 31.8348407984\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.14483714104,3.37278128676), test loss: 3.35801357627\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (19.8730239868,30.8420444084), test loss: 26.3324223518\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.17414975166,3.35797595844), test loss: 2.67429343462\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (9.71397781372,30.7087853472), test loss: 30.6520929813\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.542864322662,3.34346966328), test loss: 3.26858236194\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (38.9238357544,30.5750288821), test loss: 25.2242719173\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.83421707153,3.32920137417), test loss: 2.60485773087\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (15.0749473572,30.4461509859), test loss: 31.131834507\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.57281816006,3.31498472046), test loss: 3.30812735558\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (23.7333450317,30.3182747447), test loss: 29.9888416529\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.988678276539,3.30099461051), test loss: 3.18950601816\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (20.5557937622,30.192557853), test loss: 31.5188472271\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (4.83268976212,3.2873797809), test loss: 3.36202968061\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (20.8952674866,30.0692438644), test loss: 30.3788534403\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (4.10572385788,3.27400269777), test loss: 3.47444274127\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (16.7961349487,29.9496306696), test loss: 27.1596337557\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.752326726913,3.26092434723), test loss: 2.76887280792\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (13.3372116089,29.8300633288), test loss: 31.9535759211\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.321105599403,3.24823063692), test loss: 3.59407048821\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (9.6187877655,29.7130156303), test loss: 28.1100018978\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.92052221298,3.23561515655), test loss: 2.57474324107\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (20.1054420471,29.5998841121), test loss: 31.5467376471\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.82587909698,3.22301411164), test loss: 3.44365485013\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (21.7618274689,29.4865095058), test loss: 26.084166193\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.196393013,3.21070811038), test loss: 2.5009133637\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (20.6066207886,29.3743251265), test loss: 33.9781910896\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.73945879936,3.19857196276), test loss: 3.51826877296\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (11.2489299774,29.2657530334), test loss: 26.6168466568\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.914974451065,3.18683385242), test loss: 2.67152209878\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (29.0929908752,29.1584368177), test loss: 31.7833525658\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.31767868996,3.17533093633), test loss: 3.31422999501\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (17.50730896,29.0521052566), test loss: 25.1807672024\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.22828197479,3.16393432188), test loss: 2.52787221819\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (19.355682373,28.9491644287), test loss: 31.109678793\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (5.3266916275,3.15272834235), test loss: 3.27099542916\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (19.8968200684,28.8470121955), test loss: 29.7765967369\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (4.89344882965,3.14151475531), test loss: 3.10088785589\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (54.218875885,28.7454413302), test loss: 31.5251854658\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.38092899323,3.13049526087), test loss: 3.3493176192\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (8.83339691162,28.6461031183), test loss: 29.3703642607\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.22619819641,3.11986774213), test loss: 3.25452594161\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (16.3195819855,28.5495723213), test loss: 26.7242162228\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.1856751442,3.10942414181), test loss: 2.80291612148\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (21.364408493,28.4521053121), test loss: 30.7190760136\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.35792803764,3.09916491115), test loss: 3.50535131693\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (9.0204372406,28.3576397214), test loss: 27.605098629\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.04133272171,3.08889106835), test loss: 2.51629775465\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (38.6066322327,28.2644183644), test loss: 31.7459029675\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (4.47520589828,3.07875655063), test loss: 3.47312979698\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (13.3216896057,28.1715911164), test loss: 26.8650500774\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.57268214226,3.06879627672), test loss: 2.52389379144\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.94682407379,28.0801334151), test loss: 32.4732030869\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.01205539703,3.05895407166), test loss: 3.35366610289\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.8225870132,27.9913894902), test loss: 27.3589078426\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.01596474648,3.04934104637), test loss: 2.63588958085\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.74648046494,27.9027179279), test loss: 33.1661459684\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.92776226997,3.0400488872), test loss: 3.35134731531\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (9.70270347595,27.8152402847), test loss: 27.1985975742\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.659480333328,3.03071342656), test loss: 2.7470651418\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (9.21952819824,27.7305998323), test loss: 31.1218683481\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.9010027647,3.02140638367), test loss: 3.22525066733\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (12.020359993,27.6455889161), test loss: 24.9016944885\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.47838699818,3.01224649931), test loss: 2.85671631694\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (5.15006637573,27.5611062929), test loss: 32.7705868006\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.797952532768,3.00320661186), test loss: 3.31017699838\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (15.3662910461,27.4789102212), test loss: 29.1883504868\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.00810372829,2.99444179228), test loss: 3.27061573267\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (51.6272583008,27.3982703464), test loss: 32.9986288548\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.09961676598,2.98587389048), test loss: 3.32180155441\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (20.9212017059,27.317165765), test loss: 30.227054143\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.19217252731,2.97731390001), test loss: 3.40424247086\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (22.5065326691,27.2388051829), test loss: 27.9526751041\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.62476706505,2.96885484519), test loss: 2.57688920498\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (4.63065338135,27.1606925342), test loss: 31.5916453362\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.973983943462,2.96037625261), test loss: 3.48034622669\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (13.5468006134,27.0828863804), test loss: 27.3282462597\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.54155445099,2.95206328614), test loss: 2.42099457383\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (9.15160560608,27.0066709707), test loss: 31.2110667944\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.38335323334,2.94397575415), test loss: 3.3695288837\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (7.49413394928,26.9323642878), test loss: 28.6261929512\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.90375721455,2.93602255049), test loss: 2.61865928918\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (8.90535354614,26.8575050503), test loss: 31.5465510368\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (3.42840838432,2.92823472684), test loss: 3.3172393553\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (15.867193222,26.7845084265), test loss: 26.8526560783\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.79162430763,2.92043439233), test loss: 2.68795580268\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.53390598297,26.7124439608), test loss: 30.6460151672\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.61382865906,2.91262677902), test loss: 3.16405332983\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (38.8114471436,26.6408194444), test loss: 24.8498488188\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.25713205338,2.90496298713), test loss: 2.53244263232\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (9.05864715576,26.5690857583), test loss: 31.7712973595\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.75821280479,2.89738426466), test loss: 3.2453430593\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (5.09563350677,26.4998869819), test loss: 29.3812562704\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.41118645668,2.88996614585), test loss: 3.1170853734\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (10.3090362549,26.4305445003), test loss: 32.0442238808\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.0492939949,2.88279584863), test loss: 3.35212156773\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (12.6160449982,26.362062424), test loss: 30.5056127548\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.82949233055,2.87559331886), test loss: 3.43986096978\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (9.14106845856,26.2956478679), test loss: 27.7688817501\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.68023443222,2.86835970322), test loss: 2.69637622237\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (33.9991645813,26.2292080978), test loss: 32.4963706493\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (3.27436947823,2.86126894248), test loss: 3.4994381547\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (6.17504310608,26.1620099654), test loss: 27.6833148956\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.07649362087,2.85419394269), test loss: 2.30794104189\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.85459280014,26.0972487782), test loss: 31.8460077286\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (3.08061385155,2.84737514494), test loss: 3.35540831983\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (24.1252403259,26.0341060773), test loss: 28.3804585695\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.472809910774,2.84069047534), test loss: 2.60791718364\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (5.59373950958,25.969251465), test loss: 32.512674427\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.567669212818,2.8339998915), test loss: 3.44337735474\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (20.0915470123,25.9071599383), test loss: 26.8744416714\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.95754468441,2.82732940852), test loss: 2.63639594615\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (16.6606521606,25.8447501007), test loss: 31.1532077074\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.32692575455,2.82069623361), test loss: 3.14674753249\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (9.00054931641,25.782653334), test loss: 24.9424406052\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.47606468201,2.81414408112), test loss: 2.52031081617\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (5.46427345276,25.721525107), test loss: 31.3784111977\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.859318435192,2.80773715343), test loss: 3.19367041066\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (22.1151676178,25.661961915), test loss: 29.6049933434\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.47246873379,2.80146297996), test loss: 3.09603767097\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (8.90437221527,25.6017646107), test loss: 31.9512384653\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.63380670547,2.79532907919), test loss: 3.32717999518\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (8.42308425903,25.5428983782), test loss: 29.1935401917\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.04628181458,2.78914573641), test loss: 3.20695922971\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (30.7724304199,25.4851731889), test loss: 27.0879261971\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.04799306393,2.78292621252), test loss: 2.72699852586\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (29.428062439,25.4272620605), test loss: 31.5088239908\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.96398735046,2.77683318879), test loss: 3.46893793643\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (12.7618522644,25.369032016), test loss: 27.9742622375\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.33781027794,2.77077366178), test loss: 2.47810597569\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (16.6924781799,25.3129830498), test loss: 32.3419674397\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.49642133713,2.76490215665), test loss: 3.4688255161\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 9\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold9/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (374.780639648,inf), test loss: 207.356188965\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (293.753112793,inf), test loss: 362.23236084\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (37.5271835327,93.9432869678), test loss: 43.1848176003\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.39419698715,106.573863753), test loss: 3.48042613864\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (45.7195243835,69.6767610338), test loss: 33.206282568\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.63186836243,55.0730584951), test loss: 2.90172912478\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (54.2889785767,61.4584205901), test loss: 42.7794484615\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.30890870094,37.8797687421), test loss: 3.69407914281\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (21.1570529938,57.3807236215), test loss: 38.3572309017\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.13520586491,29.287403978), test loss: 3.61481227577\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (48.0349502563,54.7695078174), test loss: 42.5057252407\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (7.86949920654,24.1331543906), test loss: 3.62918454707\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (44.5721359253,53.0232152188), test loss: 41.0101010323\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.489140748978,20.6907414759), test loss: 3.78907746077\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (58.2852478027,51.6667783249), test loss: 38.6412986755\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (5.84696054459,18.2327837896), test loss: 3.0212143302\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (40.7016143799,50.6154583259), test loss: 43.0106721401\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.49650812149,16.3865818954), test loss: 3.78721490502\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (12.5952014923,49.7683186781), test loss: 39.6211983681\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (0.597836196423,14.9447504648), test loss: 2.86608309746\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (5.43736171722,49.110449724), test loss: 42.9130485535\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.522811174393,13.7920720212), test loss: 3.93426809311\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (18.7809295654,48.5164327048), test loss: 36.6559887886\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.88228034973,12.8482074488), test loss: 2.96689030528\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (38.7191200256,47.9860980516), test loss: 43.420085907\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.41756343842,12.0581247795), test loss: 3.62370288372\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (113.897468567,47.5037403526), test loss: 34.6483107567\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (6.53916883469,11.3883937898), test loss: 2.99615104198\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (57.9048080444,47.0273810001), test loss: 42.5481577396\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.75311970711,10.8111480422), test loss: 3.47859067023\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (134.613845825,46.6130385996), test loss: 33.3781884074\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.87186932564,10.3088371427), test loss: 2.75091900229\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (27.7393779755,46.2232096606), test loss: 39.3631224632\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.97919511795,9.86843943147), test loss: 3.55886926055\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (75.2789154053,45.8583732334), test loss: 35.841625905\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.77268671989,9.4784991825), test loss: 3.23171714544\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (32.7124328613,45.4875771163), test loss: 37.5641424179\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.396961689,9.13183450317), test loss: 3.47222080827\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (63.0523071289,45.1367056122), test loss: 35.5402323246\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.64645147324,8.81866616517), test loss: 3.43444347382\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (149.189666748,44.7733634872), test loss: 33.7729385376\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.55478191376,8.53543215614), test loss: 2.78663670421\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (13.2714643478,44.4152459425), test loss: 36.60369277\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.30842018127,8.27691110954), test loss: 3.3552100718\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (20.9188499451,44.0577223235), test loss: 34.5979636192\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.77461004257,8.04050382481), test loss: 2.66505947411\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (71.5191040039,43.7190632621), test loss: 39.241121912\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (5.91356801987,7.82398214059), test loss: 3.72147132754\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (86.2025604248,43.3671532171), test loss: 34.6529681206\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.50559997559,7.62481712051), test loss: 2.67996982932\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (51.884853363,43.0106343667), test loss: 36.4645469904\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.39242208004,7.43895390748), test loss: 3.51206317544\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (59.5785865784,42.6446538955), test loss: 30.5025658607\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (5.81891775131,7.26670187132), test loss: 2.75035033524\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (29.3814411163,42.2727363543), test loss: 38.0239532709\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.64624738693,7.10492377447), test loss: 3.45966652632\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (37.8094940186,41.9019023525), test loss: 28.4920804977\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.12210178375,6.95260245774), test loss: 2.6084368825\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (16.1572723389,41.5361978718), test loss: 35.2163347721\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.49545097351,6.81072472107), test loss: 3.33695388734\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (21.4930419922,41.1600822309), test loss: 25.2220870018\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.88598012924,6.67678672776), test loss: 2.46142163575\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (28.7928009033,40.782845006), test loss: 33.264065218\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.85068011284,6.54970160393), test loss: 3.2008345753\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (17.9484024048,40.3958191886), test loss: 30.0880183697\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.25449991226,6.43038843279), test loss: 3.12860321999\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (23.1652603149,40.0086064675), test loss: 31.2764909744\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.4808306694,6.31645778545), test loss: 3.20512115955\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (26.7349834442,39.6296409272), test loss: 30.3925366879\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.05393648148,6.20748206156), test loss: 3.21556335092\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (65.7814102173,39.2552596349), test loss: 26.6207940578\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.82449865341,6.10474636272), test loss: 2.55042852163\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (32.6120071411,38.8759006732), test loss: 35.1952624083\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.61512172222,6.0067932395), test loss: 3.3989282757\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (32.4403190613,38.5079032308), test loss: 28.6792010307\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.39792013168,5.91368167243), test loss: 2.46575573683\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (27.126947403,38.1372252879), test loss: 33.1882347107\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.619459152222,5.82465655242), test loss: 3.4330694437\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (19.7741699219,37.7733092953), test loss: 27.1803081989\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.54185009003,5.73959357095), test loss: 2.32636806667\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (6.12133073807,37.4237797708), test loss: 33.6727370262\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.76333117485,5.65755311163), test loss: 3.3671667695\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (18.0165214539,37.0845384598), test loss: 27.6351089478\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.34253835678,5.57931987907), test loss: 2.54114782214\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (37.6770782471,36.749393482), test loss: 34.007268703\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.51712238789,5.50479200809), test loss: 3.33234475255\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (13.9272756577,36.4258167737), test loss: 27.1505049229\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.46852850914,5.43341816601), test loss: 2.63894775212\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (69.9727859497,36.1109628985), test loss: 31.6856087446\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.17692470551,5.36517263753), test loss: 3.10858035535\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (17.2165908813,35.7960661739), test loss: 23.6516198635\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.24846363068,5.29932500896), test loss: 2.5331433937\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (8.94309425354,35.4981678539), test loss: 32.5039312363\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (4.03908538818,5.23583146447), test loss: 3.23618088961\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (14.626996994,35.2106665343), test loss: 29.812571764\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.15759778023,5.17445466183), test loss: 3.30666834712\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.3679542542,34.9271018026), test loss: 32.7915902615\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.02775144577,5.11596475686), test loss: 3.29137065262\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (23.0403633118,34.6554785396), test loss: 30.9864718914\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.600548386574,5.05993389504), test loss: 3.37531442046\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (5.57714319229,34.3946059834), test loss: 28.4621441364\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.62873852253,5.00600100563), test loss: 2.5621040836\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (18.0923690796,34.1332467319), test loss: 33.150502491\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.34089660645,4.95397173094), test loss: 3.48957619965\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (24.1285057068,33.8864848961), test loss: 28.4796656609\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.12561893463,4.90326156767), test loss: 2.33160168827\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (14.571680069,33.6450156041), test loss: 33.5320442677\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.05310678482,4.85428504665), test loss: 3.39656550884\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (7.47095775604,33.4113782066), test loss: 27.3011641026\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.04084968567,4.80724740509), test loss: 2.49927811325\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (20.4868755341,33.1850349911), test loss: 35.496185112\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.4780548811,4.76221605245), test loss: 3.44952242076\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (22.5086097717,32.969213993), test loss: 27.7610738993\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.88690948486,4.71866204884), test loss: 2.65216899514\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (18.137670517,32.7536983825), test loss: 35.1693380833\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.03481960297,4.67646151267), test loss: 3.25692199767\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.9174804688,32.5460741401), test loss: 25.290486598\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.08316993713,4.63525322655), test loss: 2.5281032443\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.6784219742,32.3457463718), test loss: 33.0936189771\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.0756547451,4.59517778139), test loss: 3.30759662986\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (27.6579322815,32.1515301291), test loss: 31.4967442274\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.67118835449,4.55686306079), test loss: 3.26631318927\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (7.7114739418,31.9613482798), test loss: 32.7649288654\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.27449274063,4.51957082534), test loss: 3.3947745204\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (20.1226787567,31.7802278345), test loss: 29.784223032\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.605609953403,4.48362530039), test loss: 3.34097374082\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (16.4312343597,31.598775771), test loss: 27.6520934343\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.08902001381,4.44885173975), test loss: 2.60978787541\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (8.14278411865,31.4229940308), test loss: 31.793372345\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.78526115417,4.41487626213), test loss: 3.32314213514\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (43.6430511475,31.2557191615), test loss: 30.2760051727\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.54191637039,4.38141279061), test loss: 2.48694010079\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.6830291748,31.0897337926), test loss: 32.8511957407\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.98218524456,4.34920125586), test loss: 3.52526147664\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (22.9919319153,30.9272076423), test loss: 30.8456954956\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.1893787384,4.31800488981), test loss: 2.46493052542\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (16.8746910095,30.7723500418), test loss: 32.3859528542\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.64639282227,4.28783498507), test loss: 3.38843331337\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (8.09716033936,30.6174749383), test loss: 29.2179232121\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.29630613327,4.25858118908), test loss: 2.59270503819\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (19.6837043762,30.4664467936), test loss: 34.2297152042\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.3016513586,4.2298495492), test loss: 3.38902021646\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.1157855988,30.3225640427), test loss: 27.3270401001\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.94698572159,4.20156437501), test loss: 2.57384337783\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (12.8277626038,30.1805590311), test loss: 33.8456995964\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.40290331841,4.17419740496), test loss: 3.28360410333\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (18.8021850586,30.0396574222), test loss: 24.8890144348\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.46297287941,4.1476041151), test loss: 2.44532780349\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (5.73261117935,29.9048875687), test loss: 33.3644648075\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.833074808121,4.12188573375), test loss: 3.3046100229\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (27.6285381317,29.7720103219), test loss: 29.897824192\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.95960617065,4.09683761302), test loss: 3.19883212447\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (16.6186332703,29.6399942399), test loss: 32.9104059458\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.692845344543,4.07219018394), test loss: 3.34409382939\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (22.9637336731,29.514426972), test loss: 29.8640933514\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (5.18130874634,4.04802295841), test loss: 3.24777424335\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (6.30878925323,29.3909292594), test loss: 28.2251543522\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.320024520159,4.02428466139), test loss: 2.59285458326\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.13028907776,29.2677227475), test loss: 33.8445281982\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.32546973228,4.00131526065), test loss: 3.43539519608\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (14.5123939514,29.14965522), test loss: 29.8704550743\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (4.05234718323,3.97915563701), test loss: 2.49588246047\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (29.4438781738,29.0358681037), test loss: 32.9084328175\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.649788081646,3.95748646282), test loss: 3.47624153495\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (14.6075344086,28.9180348136), test loss: 28.3335251808\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.61566734314,3.93613769235), test loss: 2.35635424703\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (70.8770141602,28.8081085419), test loss: 34.278366375\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.09663653374,3.9151217504), test loss: 3.41447681487\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (12.6647806168,28.6983942174), test loss: 28.7413062572\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.899314165115,3.8944276188), test loss: 2.58659133464\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (40.181678772,28.5907726074), test loss: 33.7627624512\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.35619318485,3.87430285765), test loss: 3.32542252541\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (8.8908996582,28.4851102428), test loss: 27.9480436802\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.85245490074,3.85491812986), test loss: 2.66650346816\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (11.0275058746,28.384134669), test loss: 32.1618117809\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.03376948833,3.83593798299), test loss: 3.19495168924\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (15.4640684128,28.2812101913), test loss: 25.6610177517\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.25420486927,3.8172793237), test loss: 2.7987890929\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (4.29515028,28.1816977149), test loss: 33.1589289665\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.9844455719,3.79876504311), test loss: 3.25118611753\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (24.4601631165,28.0843166808), test loss: 30.4901083946\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.20261764526,3.78053049034), test loss: 3.31754481196\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (11.5513820648,27.9882252315), test loss: 33.7872681379\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.36853218079,3.76283250088), test loss: 3.35392716825\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (12.2347240448,27.8940857535), test loss: 30.3842378855\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.850376605988,3.74564953746), test loss: 3.41162679791\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (25.3235473633,27.8035083289), test loss: 29.3667077065\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.881015837193,3.72885682565), test loss: 2.51787270606\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (27.3199806213,27.7112086269), test loss: 32.2631965876\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.12627601624,3.71239414039), test loss: 3.47783207893\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.2695541382,27.621594654), test loss: 29.2004595757\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.07261776924,3.69604102616), test loss: 2.31485900283\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (7.19489955902,27.5349314644), test loss: 32.846855545\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.96372699738,3.67983006151), test loss: 3.31758976579\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (18.0739593506,27.4480547466), test loss: 28.0022369385\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.08519399166,3.6640676498), test loss: 2.47757103443\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (5.5536904335,27.3629123987), test loss: 35.3787401676\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.15659117699,3.64866822087), test loss: 3.4294662714\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (10.1666536331,27.2809858976), test loss: 27.760570097\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.56583344936,3.63363952822), test loss: 2.63882076293\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (14.2084884644,27.1976536287), test loss: 34.6427535057\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.50024008751,3.6189918917), test loss: 3.19899811745\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (16.7233505249,27.1163474036), test loss: 25.1560932159\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.970881342888,3.60442339022), test loss: 2.53617326319\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (8.47902774811,27.0385792653), test loss: 33.0313091278\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.58486652374,3.58988513305), test loss: 3.21973531544\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (20.549243927,26.9602951727), test loss: 30.9957048416\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.77886366844,3.57576660941), test loss: 3.163687554\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (18.9287490845,26.8821351834), test loss: 33.4497020245\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.84462475777,3.56188622927), test loss: 3.3381801188\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (13.5054016113,26.8072477474), test loss: 29.5362897873\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.40964412689,3.54844368011), test loss: 3.28874263763\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (14.4434223175,26.7318898378), test loss: 28.1618843317\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.84218454361,3.53516449952), test loss: 2.62207596004\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (18.6756019592,26.6574878367), test loss: 31.3302379131\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.48448252678,3.5220754549), test loss: 3.28322175145\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (7.06978750229,26.5859958299), test loss: 30.6362547398\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.57324457169,3.50899581355), test loss: 2.48369790912\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (24.7225093842,26.5152510598), test loss: 31.7310258389\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (3.23024344444,3.49620836706), test loss: 3.46851287186\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (8.40902709961,26.4436803033), test loss: 30.9273762226\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.910779953003,3.48363780084), test loss: 2.40936143398\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (10.4592914581,26.3745708009), test loss: 32.1426363707\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.08334064484,3.47144732443), test loss: 3.3270690918\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (61.9483299255,26.3071516135), test loss: 30.4005817413\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.99805736542,3.45950106532), test loss: 2.54240824282\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (41.9452095032,26.237985961), test loss: 33.5431531429\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (6.24784040451,3.44766366393), test loss: 3.27097311914\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (12.5137577057,26.1719882307), test loss: 27.2969421864\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.79570508003,3.43578878362), test loss: 2.59641193151\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (22.0834350586,26.1069627887), test loss: 32.9239950657\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (5.55516624451,3.42410264217), test loss: 3.20145897865\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (45.7320671082,26.0412481748), test loss: 24.3715076447\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.40532875061,3.41263527163), test loss: 2.40894938409\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (19.725440979,25.977643641), test loss: 33.2767447472\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.69115686417,3.40156194711), test loss: 3.24514265656\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (30.5867576599,25.9160892661), test loss: 29.4817669868\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (4.13221359253,3.39067663074), test loss: 3.13433266282\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (3.58975934982,25.8518367306), test loss: 33.0306023121\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.784404158592,3.37985638123), test loss: 3.29081279039\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (21.3334579468,25.7913882754), test loss: 29.6109003186\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.39381980896,3.3690084911), test loss: 3.28218571842\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (19.2082290649,25.730340005), test loss: 29.6651083469\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.9342417717,3.35832516709), test loss: 2.55437851548\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (21.0153179169,25.6700356757), test loss: 32.4313249588\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (0.973834216595,3.34785685826), test loss: 3.38801347017\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (25.74335289,25.6106867136), test loss: 30.2010687113\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.90232205391,3.33769797137), test loss: 2.47854001224\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (13.2825679779,25.5535561107), test loss: 32.7485977411\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.5661457777,3.32771280451), test loss: 3.38138050139\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (18.5113792419,25.4946673566), test loss: 27.5085366726\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.76095485687,3.31781986123), test loss: 2.26243170798\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (11.8875865936,25.4376015128), test loss: 33.7300976515\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (6.29399299622,3.30796557744), test loss: 3.33499115407\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (50.4814987183,25.3820275139), test loss: 28.2569720745\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (3.58654594421,3.29808775218), test loss: 2.53413153887\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (27.8004455566,25.3255957756), test loss: 33.0650567532\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (3.54867219925,3.28851627162), test loss: 3.24949678481\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (34.8254356384,25.2704095349), test loss: 27.8878969669\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.49864530563,3.27908367771), test loss: 2.60522083044\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (5.46159744263,25.2169015146), test loss: 32.0701833725\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.299098342657,3.26983004185), test loss: 3.11542077661\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (31.9982376099,25.1622218589), test loss: 26.7569556713\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.24573564529,3.26078438068), test loss: 2.91689434648\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (6.97835588455,25.1085375244), test loss: 32.6801720619\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.13532042503,3.25173420567), test loss: 3.19603140354\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (19.2635765076,25.0571988462), test loss: 29.2876824379\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.67557477951,3.24262455369), test loss: 3.22686335742\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (20.4153499603,25.0047926474), test loss: 33.9255952477\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.16887712479,3.23377496534), test loss: 3.25148759186\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (14.2933921814,24.9524917181), test loss: 30.3030865431\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.85925996304,3.22502498613), test loss: 3.31708302796\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.64439678192,24.9024839101), test loss: 29.6977930546\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.03489732742,3.21649320613), test loss: 2.46262558699\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (22.0208969116,24.8514788568), test loss: 31.688371706\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.544614315033,3.20810935457), test loss: 3.4195812434\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (7.44847917557,24.8011196424), test loss: 29.6530816078\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (3.00533223152,3.19976435098), test loss: 2.29047383666\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (16.801738739,24.7529596228), test loss: 32.0776031494\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.77972745895,3.19133592831), test loss: 3.24193989635\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.51635694504,24.7042375367), test loss: 28.1489314556\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.55149769783,3.18309898303), test loss: 2.39141177684\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (21.9129714966,24.6550972301), test loss: 35.1285857201\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.808817029,3.17500046588), test loss: 3.38540329933\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (8.61439990997,24.6077779278), test loss: 27.4289386272\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.973513782024,3.16708378171), test loss: 2.55756872296\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (12.8385066986,24.56052901), test loss: 33.8511325955\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.1687476635,3.15931940683), test loss: 3.14205375463\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (12.1366224289,24.5128920756), test loss: 24.9749537468\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.73872148991,3.15153586809), test loss: 2.4955478698\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (9.66980648041,24.4673181325), test loss: 32.8581457615\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (4.09168434143,3.14378396638), test loss: 3.13736102283\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (14.0084381104,24.4217599052), test loss: 29.5110177517\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.396594703197,3.13603634723), test loss: 3.06857543886\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (14.1837129593,24.3753353288), test loss: 33.3329602718\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.610454559326,3.12848708744), test loss: 3.27623787522\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (20.4190254211,24.3307521908), test loss: 29.4613042355\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.14431929588,3.1211627453), test loss: 3.16348209381\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (17.6440887451,24.2872760618), test loss: 28.9014913559\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.476488918066,3.11391245633), test loss: 2.63570803255\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (42.943939209,24.2419062253), test loss: 30.5498635769\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.0684928894,3.10670347243), test loss: 3.31260095835\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (22.9985942841,24.1988354711), test loss: 30.5249420166\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.39484596252,3.09943902483), test loss: 2.47505878359\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (6.10391044617,24.1554067653), test loss: 31.4259445548\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.32244968414,3.09224474055), test loss: 3.39714755416\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (12.39590168,24.1121554003), test loss: 29.8442463875\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (3.23327159882,3.08519049067), test loss: 2.38082070649\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (7.19487667084,24.0693198462), test loss: 31.9522859573\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.08308005333,3.07831486938), test loss: 3.27255558372\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (11.3366231918,24.0283680127), test loss: 29.8314315796\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.41602182388,3.07154302322), test loss: 2.49106295705\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (12.3186607361,23.9856697891), test loss: 33.0342393875\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.78278541565,3.06481672449), test loss: 3.20353089273\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.56251907349,23.9445103271), test loss: 26.8484435081\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (4.24453783035,3.05801115328), test loss: 2.57772640586\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (14.8883991241,23.9037080796), test loss: 32.659487009\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.84755992889,3.05127511844), test loss: 3.07510681152\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (20.4374675751,23.8628022256), test loss: 24.1851338387\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (4.24593496323,3.04469296761), test loss: 2.39975378215\n",
      "\n",
      "MC # 1, Hype # hyp4, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold10/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (297.844573975,inf), test loss: 151.720201492\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (309.823516846,inf), test loss: 377.849987793\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (17.326171875,72.6368049731), test loss: 45.9275990963\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.91930043697,72.8840518938), test loss: 3.1381452322\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (30.6185302734,58.9501422575), test loss: 37.3995792389\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.51831400394,37.8332862506), test loss: 2.54923231602\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (113.799316406,54.2478508808), test loss: 44.1006977081\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.64642381668,26.1297713236), test loss: 3.31638695598\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (24.8662796021,51.9104078478), test loss: 40.0354860783\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.44399690628,20.2784591577), test loss: 3.32599615157\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (8.03699111938,50.3196404944), test loss: 42.2499114513\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.54731011391,16.7630129816), test loss: 3.32359226346\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (29.3194026947,49.1836182172), test loss: 41.8761583805\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.670378088951,14.4137425337), test loss: 3.56629670262\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (40.5546569824,48.2731145423), test loss: 39.4380401611\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.80100178719,12.7360494353), test loss: 2.54871074259\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (25.2616386414,47.4789182764), test loss: 43.9540229797\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.93863391876,11.4746666473), test loss: 3.43013950288\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (18.6952800751,46.8290651557), test loss: 40.8525165081\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.2689743042,10.4880580809), test loss: 2.4337074399\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (18.131439209,46.3332387398), test loss: 44.4787755966\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.28541302681,9.70113204935), test loss: 3.63547506332\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (20.3368759155,45.8134550217), test loss: 39.540040493\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.77759337425,9.0555431493), test loss: 2.38540712595\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (38.4277763367,45.3162636224), test loss: 43.2066419601\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.76825964451,8.51583978224), test loss: 3.46930462718\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (35.7945556641,44.8220836354), test loss: 35.5311447144\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.71466112137,8.05775871459), test loss: 2.63489435911\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (31.2195224762,44.3334791267), test loss: 41.8194699764\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.62985253334,7.66395955431), test loss: 3.51659013629\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (45.2499389648,43.8738758535), test loss: 33.9209388733\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.43585515022,7.31860278473), test loss: 2.65219206214\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (25.1795082092,43.4655123286), test loss: 40.4704900742\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.07029724121,7.01815162438), test loss: 3.15634480864\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (10.6740264893,43.0294030032), test loss: 32.0417254925\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.10703730583,6.7516151992), test loss: 2.46796140671\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (19.7507381439,42.5876634709), test loss: 37.807270813\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.49370408058,6.51324158583), test loss: 3.03304331899\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (71.9917984009,42.1372318964), test loss: 34.5464738369\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.33671307564,6.29877507914), test loss: 3.25232410878\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (6.91546821594,41.6763442775), test loss: 36.3088398933\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.56954455376,6.1045255986), test loss: 3.24434704483\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (15.6023349762,41.2218711366), test loss: 33.577645731\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.00131428242,5.92709162471), test loss: 3.38822228909\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (51.4837799072,40.7856243286), test loss: 30.6615307093\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.17640829086,5.7666637234), test loss: 2.59025136456\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (80.6380157471,40.3247919923), test loss: 34.0662883282\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (4.21890735626,5.62013041824), test loss: 3.4190231204\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (17.7144489288,39.8588873119), test loss: 31.1614075422\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.23738455772,5.48430852334), test loss: 2.43887995481\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (123.342460632,39.3794307201), test loss: 36.079183054\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.10647010803,5.3584202621), test loss: 3.5957300365\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (9.26874637604,38.9030440265), test loss: 30.8753970146\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.76537394524,5.2411090932), test loss: 2.28140915483\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (23.1141014099,38.433651614), test loss: 35.7898396492\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.06159949303,5.13125814196), test loss: 3.49747394323\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (10.4493732452,37.9760191981), test loss: 28.4872321844\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.86300373077,5.02929596719), test loss: 2.56302699745\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (10.7530536652,37.5125319569), test loss: 35.0606087446\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.15616393089,4.93447676693), test loss: 3.60731294155\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (17.9050598145,37.0589248401), test loss: 27.9478026152\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.77752006054,4.84524949121), test loss: 2.72377225757\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (20.8728218079,36.5948503412), test loss: 34.9189229488\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (4.409055233,4.76104500034), test loss: 3.3146674633\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (26.3527488708,36.1542008611), test loss: 26.9438123226\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.51576638222,4.68136005003), test loss: 2.71674842983\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (10.7110414505,35.7213341499), test loss: 33.1983447313\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.47470068932,4.60600146126), test loss: 3.19822950959\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (57.1255493164,35.3049444337), test loss: 30.2686840057\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.88966989517,4.5350161325), test loss: 3.26798792779\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (13.0677957535,34.8935725081), test loss: 33.6824668884\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.6653727293,4.46853575663), test loss: 3.297567904\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (16.1539230347,34.4989277799), test loss: 28.7205729246\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.01072406769,4.40534621365), test loss: 3.46636127234\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (15.1002311707,34.1032625916), test loss: 33.4905795813\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.555812180042,4.34508381459), test loss: 3.10903037339\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (54.435508728,33.735255129), test loss: 29.7466458797\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.60335206985,4.28772310299), test loss: 3.5702587679\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (19.0232810974,33.3778890364), test loss: 29.1236388683\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (7.3799123764,4.23290871019), test loss: 2.55436995625\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (34.8750762939,33.0323624359), test loss: 34.023053813\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.3368332386,4.18060729042), test loss: 3.645745942\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (36.1277351379,32.7006006256), test loss: 29.5101160526\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.842040538788,4.13150372826), test loss: 2.34955560565\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (54.4366645813,32.3817789611), test loss: 34.3586698532\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.09795331955,4.08448296948), test loss: 3.61883225143\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (35.4217796326,32.0643707253), test loss: 28.4689097404\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.81265163422,4.0391733002), test loss: 2.49327720255\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (17.4586181641,31.7677745977), test loss: 33.979658556\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.73996257782,3.99562343123), test loss: 3.5490971148\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (12.0888404846,31.4842188228), test loss: 28.9889220715\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.233707606792,3.9535699335), test loss: 2.78381272554\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (13.7794351578,31.2071573435), test loss: 35.1751610279\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.668852090836,3.91363301916), test loss: 3.44549862742\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (14.7877473831,30.9423336912), test loss: 28.3918184757\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.85637044907,3.87585251082), test loss: 2.86876061857\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (56.1945953369,30.6867909588), test loss: 35.5009410381\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.08658599854,3.83930309782), test loss: 3.27572352886\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (19.2298679352,30.4336780508), test loss: 28.8716840267\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.98106944561,3.80397597842), test loss: 2.99796161801\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (17.6389541626,30.1973900417), test loss: 35.3633646727\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (4.08644723892,3.7698552433), test loss: 3.3806312561\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (17.8068790436,29.9706221073), test loss: 30.1585660219\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.2188243866,3.73690440363), test loss: 3.61142156422\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (7.43028831482,29.7471434918), test loss: 35.5164273262\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.03081250191,3.70520931618), test loss: 3.46323302686\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (17.72448349,29.5332317467), test loss: 30.7846879005\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.29598295689,3.67506923146), test loss: 3.62473148704\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (45.9226646423,29.3252611624), test loss: 32.0479256153\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.38544797897,3.64588113111), test loss: 2.7020935595\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (34.6374015808,29.1209189616), test loss: 33.2241024971\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.43513345718,3.61758406332), test loss: 3.6408514142\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (8.41114521027,28.9290318907), test loss: 32.2557187319\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.41132116318,3.58994969429), test loss: 2.5439458549\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (17.8761844635,28.7438890069), test loss: 35.2510411739\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.52645373344,3.56325265838), test loss: 3.7046343416\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (28.3710231781,28.5604564518), test loss: 30.6039091587\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.28098678589,3.53754784296), test loss: 2.44373311251\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (12.3613729477,28.3845627987), test loss: 33.9140627861\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.05413150787,3.51289985987), test loss: 3.48314960897\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.4716968536,28.2119373659), test loss: 33.7517905235\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.413965255022,3.48896093008), test loss: 2.81047120392\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (22.7645797729,28.0432798782), test loss: 33.2176862478\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.93035620451,3.46576032901), test loss: 3.47346680909\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (6.31684494019,27.8849295604), test loss: 29.4467937946\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.98749017715,3.44291166779), test loss: 2.81672996581\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (18.588727951,27.7305575351), test loss: 35.4237189293\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.12629961967,3.42085545462), test loss: 3.13095653653\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (15.768611908,27.576626478), test loss: 28.4719602585\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.46727991104,3.39944463339), test loss: 2.67787794471\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (13.9041061401,27.4298245302), test loss: 36.0897971153\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (5.10040616989,3.3789480374), test loss: 3.34050715864\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (10.6007347107,27.2843073544), test loss: 32.2745762587\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.30716848373,3.35892314138), test loss: 3.56917911172\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (7.2432308197,27.1421192455), test loss: 36.2467171192\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.27055954933,3.33937550792), test loss: 3.46978346109\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (11.1753015518,27.0091420833), test loss: 31.2104345441\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.46442437172,3.32010610438), test loss: 3.58565322161\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (21.3044395447,26.8777302394), test loss: 30.483925724\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.64353609085,3.30148215492), test loss: 2.64874971509\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (24.9704666138,26.7465745365), test loss: 34.682508707\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.16824626923,3.28333464558), test loss: 3.64949257672\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (11.264881134,26.6218761904), test loss: 32.7959256172\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.27747416496,3.2658677111), test loss: 2.61530404985\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (9.19482040405,26.4968348742), test loss: 32.8885967731\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.12739670277,3.24894008392), test loss: 3.71580309272\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.49741745,26.375283109), test loss: 33.0252295017\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.24905538559,3.23228877029), test loss: 2.48504296541\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (21.4417152405,26.2612242457), test loss: 34.6308961868\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.79612278938,3.21579300907), test loss: 3.52623817325\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.5222434998,26.1480451664), test loss: 31.8579296589\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.01890945435,3.19981883596), test loss: 2.77592201829\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (18.1088104248,26.0345907877), test loss: 35.326041317\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.77632522583,3.18420539177), test loss: 3.55427884459\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (16.2340335846,25.9271890452), test loss: 31.0856498718\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.16686415672,3.16914446238), test loss: 2.89450165629\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.189581871,25.8187552995), test loss: 35.6095634937\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.3065237999,3.15452156482), test loss: 3.1502371639\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (4.91148900986,25.7127958452), test loss: 29.4399469376\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.811209321022,3.14011615779), test loss: 2.68327500522\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (7.86021995544,25.612805656), test loss: 34.9504417419\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.63818502426,3.1257686359), test loss: 3.15011953712\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (9.63084983826,25.5139310308), test loss: 32.7131662369\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.14964270592,3.11185488931), test loss: 3.43130123317\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (13.0640735626,25.415525846), test loss: 34.9708147049\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.27847933769,3.09827887123), test loss: 3.3451797545\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (19.9496669769,25.3215152323), test loss: 31.2131973267\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.953473687172,3.08516828103), test loss: 3.55693529248\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (17.8121910095,25.2258786066), test loss: 31.9094518185\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (3.29434919357,3.07238473901), test loss: 2.73641601205\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (6.90218877792,25.1331820857), test loss: 31.4468790054\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.62936091423,3.05977176765), test loss: 3.56384959817\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (13.4997215271,25.0452138888), test loss: 32.4911655903\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (4.8285908699,3.04711395252), test loss: 2.63114590049\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (29.8042869568,24.9578150584), test loss: 33.8253871679\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (3.65826821327,3.03492586139), test loss: 3.61725231707\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (7.20379447937,24.8702032052), test loss: 31.8020693779\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.911273300648,3.02292587833), test loss: 2.38730569929\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (23.9272232056,24.7870830427), test loss: 34.7150296688\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.50098395348,3.01138877821), test loss: 3.52592450678\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (15.5395107269,24.7022042429), test loss: 32.4789879799\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.56245422363,3.0000315939), test loss: 2.65613577068\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (18.7240581512,24.6200912098), test loss: 35.1279462099\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (5.71970272064,2.98882476737), test loss: 3.58643242717\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (14.5478429794,24.541651401), test loss: 29.7954509735\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.83768677711,2.97756276542), test loss: 2.76817537248\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (16.6183242798,24.4632240545), test loss: 35.7719238997\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.97494888306,2.96674155455), test loss: 3.36909062266\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (19.9903564453,24.3854331294), test loss: 30.375562191\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.26386642456,2.95615863795), test loss: 2.80509343743\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.1640558243,24.3107318344), test loss: 35.7154040456\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.28104996681,2.94588340239), test loss: 3.16847493052\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (12.1624736786,24.234729367), test loss: 34.3963513613\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.0720102787,2.93571457628), test loss: 3.34866916537\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (8.07100486755,24.1611972611), test loss: 36.1259422064\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.00498962402,2.92564285807), test loss: 3.28967413008\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (17.3748626709,24.0905835712), test loss: 31.3548400879\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.259557724,2.91566070079), test loss: 3.40632166266\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (20.722026825,24.0202603556), test loss: 36.0621645689\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.00323295593,2.90586735427), test loss: 3.31459001601\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (8.26455497742,23.9496750878), test loss: 31.7785681725\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.22009885311,2.89642494671), test loss: 3.52121696472\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.89477348328,23.8828156999), test loss: 32.9958941936\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.755001604557,2.88719128094), test loss: 2.57277852595\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (3.36313199997,23.8131552175), test loss: 33.3715794802\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.950938761234,2.87804216152), test loss: 3.61550434828\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.98428726196,23.747601716), test loss: 32.3241063595\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.05676579475,2.86892882683), test loss: 2.35570361912\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (13.3846912384,23.6832449364), test loss: 35.8665534258\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.76753950119,2.85993991561), test loss: 3.56002880037\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (14.7434873581,23.6194935415), test loss: 30.5005916595\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (3.52773499489,2.85112485785), test loss: 2.42969679534\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (5.27219676971,23.555694461), test loss: 34.4198742867\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.27842140198,2.84259883319), test loss: 3.48276088238\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (28.3866043091,23.4947170261), test loss: 33.6715491295\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.6325712204,2.83427433794), test loss: 2.82455289364\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (36.3706169128,23.4313794225), test loss: 34.7823898792\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.75389802456,2.82594248513), test loss: 3.38196426034\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (16.6724014282,23.371752897), test loss: 30.1273516655\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.64228689671,2.81768385925), test loss: 2.85226031244\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (10.3249082565,23.3130770995), test loss: 34.9603850365\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.11657941341,2.80954320149), test loss: 3.13722291589\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (29.1375045776,23.2540288608), test loss: 29.160946703\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.70385456085,2.80148070378), test loss: 2.73907897174\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (14.091170311,23.1966080428), test loss: 36.0440429211\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.08786845207,2.79375832117), test loss: 3.30893390179\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (18.8544273376,23.140334268), test loss: 31.6504564762\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.39963799715,2.78615187875), test loss: 3.58991293311\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (26.686378479,23.0821333905), test loss: 36.6506237507\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (3.19656324387,2.77859016148), test loss: 3.40531280339\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (17.9213981628,23.0274604495), test loss: 31.0665904999\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.498280823231,2.77103331421), test loss: 3.53570308089\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (17.2832107544,22.9739354355), test loss: 32.1590187073\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.05900382996,2.76355628154), test loss: 2.63430144191\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (10.8501005173,22.9190517167), test loss: 35.0848884106\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.15518188477,2.75623184823), test loss: 3.55452878773\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (15.9035663605,22.8665595264), test loss: 33.4643307209\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.43682289124,2.74921486399), test loss: 2.55287564397\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (45.8251800537,22.8143643158), test loss: 34.878416729\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.28536462784,2.7422379628), test loss: 3.64885182381\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (17.0044937134,22.7611167596), test loss: 32.3467122555\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.14456999302,2.73526608804), test loss: 2.43355294466\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.099937439,22.7109815903), test loss: 34.3286312342\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (5.63348865509,2.728409428), test loss: 3.40024591386\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (4.55029678345,22.6615655678), test loss: 33.0866858959\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (0.414708077908,2.72151030434), test loss: 2.71776664704\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (8.51334190369,22.6109502824), test loss: 34.6173494816\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.89150285721,2.71476466314), test loss: 3.38053253293\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (20.1613426208,22.5622587124), test loss: 30.6792252064\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.35446381569,2.70827556658), test loss: 2.77435935289\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (18.8893585205,22.5133558715), test loss: 36.8908961535\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.91044712067,2.70186457104), test loss: 3.10629127622\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (21.4373130798,22.46452006), test loss: 29.3673718452\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.63521647453,2.69547780295), test loss: 2.65722800195\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (7.89662456512,22.418080659), test loss: 35.7376505494\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.985213756561,2.68906053597), test loss: 3.21634081453\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (17.3550491333,22.3723044055), test loss: 32.255174017\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.55803060532,2.6827781292), test loss: 3.40328394175\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (21.6128616333,22.3254490507), test loss: 35.4396457434\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.970047771931,2.67657148693), test loss: 3.32506492883\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.76766109467,22.2799263575), test loss: 31.3485283613\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.44187259674,2.67057602357), test loss: 3.45593571961\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (11.8155088425,22.2341636627), test loss: 31.7486801147\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.34471154213,2.66461380627), test loss: 2.73368034363\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (8.4790058136,22.1888554088), test loss: 33.5752285004\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.983013272285,2.65873195522), test loss: 3.56569599211\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (13.0391559601,22.1460849045), test loss: 33.6819018364\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (3.97789382935,2.65280939064), test loss: 2.57554452419\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (11.3549461365,22.1032873366), test loss: 33.8161883831\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.4007871151,2.64697749408), test loss: 3.5771720767\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (19.3318023682,22.0592556896), test loss: 33.1055958748\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.73139417171,2.64120931673), test loss: 2.36152214706\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (14.3828716278,22.0171143263), test loss: 34.5962809563\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.24894285202,2.63564222886), test loss: 3.44104763567\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (14.4865512848,21.9741499077), test loss: 32.9766672611\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.11890149117,2.63012929795), test loss: 2.68287784159\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (11.1354846954,21.9317919103), test loss: 35.3565963268\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.70245528221,2.62466688036), test loss: 3.49343166351\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (6.39796161652,21.8919559756), test loss: 30.8276228428\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.784217774868,2.61912464429), test loss: 2.78468104601\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (12.3413696289,21.8517180509), test loss: 34.872117424\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.02935051918,2.61375094567), test loss: 3.05227475762\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (14.5059328079,21.810399977), test loss: 29.7637135506\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.17672610283,2.60837023299), test loss: 2.66799455583\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (7.03414201736,21.771000906), test loss: 35.0676822662\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.69163608551,2.60316254874), test loss: 3.06670975685\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (31.2667884827,21.7307149528), test loss: 33.0480005264\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.08263897896,2.59805742487), test loss: 3.26604878008\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (19.2834320068,21.6910739349), test loss: 34.9977922916\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.37826550007,2.59295501254), test loss: 3.2535235256\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (18.0966053009,21.6537490296), test loss: 30.5348082066\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.89010822773,2.58780893172), test loss: 3.38782721162\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (13.93475914,21.6157491467), test loss: 30.939000845\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.80158162117,2.58276517481), test loss: 2.69036547244\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (13.81848526,21.5767249518), test loss: 31.9951578617\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.746669769287,2.57773432793), test loss: 3.46723703444\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (14.0479393005,21.5399285509), test loss: 33.3907803059\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.84421432018,2.57288237835), test loss: 2.61626417935\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (14.0717601776,21.5016663559), test loss: 33.51925807\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.19248270988,2.56812530979), test loss: 3.57074392438\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (11.1949081421,21.4643928495), test loss: 32.1901690006\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.701831459999,2.56335023371), test loss: 2.35349860638\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (34.4791030884,21.4291655504), test loss: 35.0871418476\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.14974927902,2.55853186302), test loss: 3.48145217001\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (40.1520767212,21.3934627951), test loss: 32.0126074791\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.09449219704,2.55381563153), test loss: 2.60253049135\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (7.70456075668,21.3566078654), test loss: 35.238138032\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.990954935551,2.54910881081), test loss: 3.43105084002\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (3.99494361877,21.3219140127), test loss: 30.6253641129\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.407408475876,2.54455660007), test loss: 2.73747573867\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (18.1534004211,21.2858926606), test loss: 36.2110198975\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.907770991325,2.54009657308), test loss: 3.30786745846\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (8.21796035767,21.2506824645), test loss: 30.7075249195\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.60122275352,2.53562684363), test loss: 2.78384170085\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.45522260666,21.217059659), test loss: 35.0707175016\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.00084495544,2.53108089187), test loss: 3.06298910379\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (9.89641189575,21.1831605327), test loss: 34.2927436829\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.37301445007,2.52664544007), test loss: 3.29132849574\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (26.1082038879,21.1486639795), test loss: 35.9657292366\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.76702356339,2.52223592899), test loss: 3.20788939893\n",
      "run time for single CV loop: 7088.54415989\n",
      "\n",
      "MC # 2, Hype # hyp5, Fold # 6\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold6/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (338.877075195,inf), test loss: 186.169059372\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (311.927490234,inf), test loss: 379.569902039\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (38.196434021,94.0420123262), test loss: 48.7957295418\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.69945645332,59.1914949121), test loss: 3.79408472776\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (53.8889083862,70.1756789708), test loss: 42.9225452423\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.44418859482,31.2858900713), test loss: 3.5657679826\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (12.6882009506,62.1354228697), test loss: 48.754877615\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (5.76721000671,21.9679165305), test loss: 3.86305891871\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (19.7089958191,58.216408509), test loss: 43.2660615444\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.00883865356,17.3039193949), test loss: 3.7167329967\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (26.6181240082,55.7853530714), test loss: 46.9084047318\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.12829470634,14.5080006377), test loss: 3.27598220706\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (42.6787796021,54.1400355194), test loss: 47.5284802437\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.06953537464,12.6453972906), test loss: 3.79403477907\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (93.5795211792,52.9619629551), test loss: 46.0774487495\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.22099590302,11.3079978987), test loss: 2.9189558506\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (38.2117004395,51.956855664), test loss: 47.9441894054\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (5.80775165558,10.3010718091), test loss: 4.04547836185\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (20.4343452454,51.2544122075), test loss: 44.7932062626\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.73431587219,9.51748469045), test loss: 2.93556078076\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (32.2272491455,50.622105109), test loss: 47.1807459831\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.987295746803,8.88472422096), test loss: 4.05806108117\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (34.8232917786,50.1490952464), test loss: 41.8351449013\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (6.99084854126,8.36995899884), test loss: 2.92885140181\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (35.1360816956,49.7062203046), test loss: 48.7064984798\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.19019603729,7.93968362223), test loss: 3.72421017289\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (33.2345428467,49.301455508), test loss: 40.4587303638\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.9314160347,7.57207094895), test loss: 3.15276532471\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (52.8588180542,48.9244309713), test loss: 47.4079389572\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.94727778435,7.25629093191), test loss: 3.56344555914\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (63.6063270569,48.572270854), test loss: 36.8789529324\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.36230385303,6.98013678324), test loss: 3.09865256548\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (16.3629837036,48.2471990706), test loss: 46.5478396893\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.13352870941,6.73578186296), test loss: 3.6009749949\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (26.059928894,47.9675376936), test loss: 41.3153439522\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.91481113434,6.51899827657), test loss: 3.51817080975\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (15.7253417969,47.6863671816), test loss: 46.8268221855\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.99154496193,6.32627285179), test loss: 3.28854675293\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (28.1599464417,47.4139359078), test loss: 44.4602855682\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.26957654953,6.15334966839), test loss: 3.66538167298\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (37.2513618469,47.1512492332), test loss: 42.8118054867\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.69320178032,5.99492680453), test loss: 2.72443098128\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (46.2640914917,46.8671658094), test loss: 46.9684090614\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (6.22683811188,5.85020215172), test loss: 3.86692339182\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (84.3102111816,46.6191516591), test loss: 42.17789011\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.25016593933,5.71772884587), test loss: 2.6999699533\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (8.81948184967,46.347670715), test loss: 45.1240769863\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.99146223068,5.59418971316), test loss: 3.80597748756\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (35.863609314,46.0986733021), test loss: 38.075747776\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (6.75956296921,5.48088971564), test loss: 2.75509679317\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (18.4501686096,45.8284309662), test loss: 45.4051949501\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.13251280785,5.37518841958), test loss: 3.57879294753\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (26.4143409729,45.5541416194), test loss: 36.9854343414\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.674970269203,5.27558345759), test loss: 2.95237799585\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (31.7984352112,45.2698486618), test loss: 43.5863256454\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.46491336823,5.18290109607), test loss: 3.45289981663\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (25.253534317,44.9746830473), test loss: 32.8831077576\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.15452432632,5.0952382869), test loss: 2.75420218706\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (33.311882019,44.6798995088), test loss: 42.0541212082\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.44482064247,5.01179896302), test loss: 3.50006793141\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (27.225189209,44.3844974927), test loss: 36.3536612988\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.87572228909,4.93348395454), test loss: 3.48528463542\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (23.2814540863,44.0723290492), test loss: 40.9819675922\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.21104025841,4.85980492358), test loss: 3.45482640266\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (19.0636138916,43.7505052531), test loss: 38.7638647556\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.82275390625,4.79051489896), test loss: 3.60265638232\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (102.75315094,43.4212478045), test loss: 36.6118012905\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (5.22670030594,4.72424104809), test loss: 2.65735681355\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (28.7952346802,43.0664259103), test loss: 41.4121605873\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.932587862015,4.66067626533), test loss: 3.91228551269\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (33.6351623535,42.718496635), test loss: 35.2591576099\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.96220302582,4.60036055918), test loss: 2.60182254314\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (14.3374919891,42.3564735079), test loss: 38.9498635292\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.25227046013,4.54207359683), test loss: 3.65791827142\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (37.5330505371,41.9948164677), test loss: 31.0137917042\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.12919926643,4.4869944544), test loss: 2.66819371581\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (12.8520898819,41.6212123764), test loss: 39.3558255196\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.2467571497,4.43479286993), test loss: 3.60119873285\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (22.4009819031,41.2433157014), test loss: 29.9636025667\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.273062467575,4.38458917021), test loss: 2.78943124115\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (27.6007499695,40.8570853676), test loss: 37.3601921797\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.83843147755,4.33642197832), test loss: 3.47966932952\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (21.8549690247,40.4691997041), test loss: 26.6180018425\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.40278911591,4.29001539947), test loss: 2.58641158342\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (17.0262870789,40.0861309955), test loss: 35.8397060633\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.39723157883,4.24479730781), test loss: 3.57720160186\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (15.8484859467,39.7059740586), test loss: 30.8297312737\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.80972635746,4.20168195128), test loss: 3.56329783648\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (10.7500171661,39.3223242344), test loss: 34.9209774494\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.89761197567,4.16033309604), test loss: 3.58990026414\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (13.8139543533,38.9432847832), test loss: 33.0849854469\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.47641253471,4.12095847813), test loss: 3.6771417588\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (67.7924346924,38.5691026215), test loss: 30.4836651325\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (4.18668651581,4.08291300684), test loss: 2.68395684361\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (29.4793281555,38.1935245467), test loss: 36.0744434357\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.0663599968,4.04584260204), test loss: 3.9242909193\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (21.4726905823,37.8302593388), test loss: 30.0079406261\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.74700117111,4.00983421018), test loss: 2.54973067045\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (20.5767154694,37.4698908116), test loss: 35.2554158211\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.07307434082,3.97445061174), test loss: 3.5470471859\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (25.2245883942,37.115841118), test loss: 28.0466676712\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.69266867638,3.94037308918), test loss: 2.66754338443\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (12.5191793442,36.7673714876), test loss: 36.2981918573\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.47829914093,3.90778739791), test loss: 3.58203246295\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (18.4532566071,36.4262669733), test loss: 26.7984168291\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.30799269676,3.87611305868), test loss: 2.71006014943\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.5282468796,36.0874382048), test loss: 35.5412036777\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.53829419613,3.84519806203), test loss: 3.43415565491\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (10.8750133514,35.7561081732), test loss: 24.3000008106\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.2881770134,3.81505770883), test loss: 2.48678211272\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (12.2485876083,35.4348861045), test loss: 34.780673027\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.86956858635,3.78518618236), test loss: 3.52473756671\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (18.0992698669,35.1187744091), test loss: 29.4888244629\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.906665921211,3.75639772724), test loss: 3.47392712533\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (31.336681366,34.806689), test loss: 34.0327033758\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.74292063713,3.7283610888), test loss: 3.49801442027\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.0331630707,34.5023616366), test loss: 31.455879271\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.49860048294,3.70136514144), test loss: 3.58666123748\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (49.6572532654,34.2057630659), test loss: 29.8888979435\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.4810321331,3.67501979983), test loss: 2.63354200125\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (31.9326667786,33.9110517239), test loss: 35.2179601192\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (6.97203540802,3.64918386167), test loss: 3.79699664712\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (38.9885520935,33.6253204786), test loss: 28.9496331453\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.02887678146,3.62373606957), test loss: 2.445270513\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (7.61292552948,33.3435288629), test loss: 35.8084412813\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.34825372696,3.59860519889), test loss: 3.41839270592\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.1844444275,33.0664617359), test loss: 28.291449213\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.12832558155,3.57423117129), test loss: 2.58368726373\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (7.72311830521,32.7941950281), test loss: 36.8345915318\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.09376072884,3.55074014352), test loss: 3.50740743876\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (16.2417125702,32.5275921916), test loss: 26.9948229313\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.72337591648,3.52774798127), test loss: 2.64827038795\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (8.32668781281,32.2621582511), test loss: 35.9001182795\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.726617455482,3.5051172164), test loss: 3.3079223305\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (4.23592615128,32.0024792711), test loss: 24.4443004131\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.780646085739,3.4828670913), test loss: 2.42476346493\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (16.6136760712,31.7498223447), test loss: 34.9490556717\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.04311656952,3.46068984512), test loss: 3.45366461277\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (15.1094121933,31.4995859841), test loss: 29.5051715374\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.36880683899,3.43916187065), test loss: 3.35356488526\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (10.3963956833,31.2520062265), test loss: 34.8686924934\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.45233464241,3.41800507405), test loss: 3.35399761051\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (9.76553344727,31.0102955964), test loss: 30.8225199699\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.53287506104,3.39752366996), test loss: 3.45493123531\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (5.9223575592,30.7725517236), test loss: 30.0569477081\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.4590909481,3.37743245853), test loss: 2.50707381964\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.8803863525,30.5379472699), test loss: 34.7388506651\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.03018045425,3.35749612722), test loss: 3.59478468597\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (17.6214466095,30.3091137508), test loss: 30.1113432407\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.75945997238,3.33794866626), test loss: 2.41833802313\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (8.92146205902,30.0844073689), test loss: 36.0817978382\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.24433922768,3.31857124136), test loss: 3.32828158438\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (9.25397491455,29.8619469996), test loss: 29.7928830147\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.16840982437,3.29959584821), test loss: 2.61381624937\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.894033432,29.6428027325), test loss: 37.2462268114\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.5071144104,3.28118612275), test loss: 3.39303373694\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (11.6291122437,29.4281965369), test loss: 27.6190579414\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.30884218216,3.26306568682), test loss: 2.57940688133\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.9462308884,29.214737732), test loss: 37.0943558216\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.77651810646,3.24527061455), test loss: 3.11627185345\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (9.1304807663,29.0058623609), test loss: 25.2830367804\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.52645206451,3.22767693151), test loss: 2.35463900864\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (15.8195447922,28.801653216), test loss: 35.6159464359\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.3901925087,3.21004704486), test loss: 3.29846764803\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (19.4991397858,28.5986238915), test loss: 29.0989030123\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.21242785454,3.19285656371), test loss: 3.28614891768\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (13.1704454422,28.3977205516), test loss: 34.7489157677\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.6982831955,3.17595018484), test loss: 3.24800964594\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (5.53809642792,28.2007968834), test loss: 31.8510605574\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.715465724468,3.15942529101), test loss: 3.38553281724\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (13.4085674286,28.0069258864), test loss: 31.2407980919\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.06334972382,3.14326368195), test loss: 2.52039068341\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (6.64872169495,27.8154193635), test loss: 35.9949533939\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.0123115778,3.12718016305), test loss: 3.48937629163\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (7.2220416069,27.6272911226), test loss: 31.2172724962\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.760693788528,3.11132847221), test loss: 2.37866768837\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (12.7595844269,27.4422596687), test loss: 36.5592472553\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.45969057083,3.09563245501), test loss: 3.2540335834\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (10.1564702988,27.258497614), test loss: 30.8828551292\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.48125505447,3.08019440819), test loss: 2.55953212976\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (16.6312103271,27.0776358064), test loss: 37.9714392185\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.52013409138,3.06520264012), test loss: 3.33697763085\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (4.60896968842,26.9011005659), test loss: 28.991262579\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.15128743649,3.05042169357), test loss: 2.58628479838\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (6.32763767242,26.7255485715), test loss: 37.7054296494\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.598164081573,3.03594920536), test loss: 3.06922557056\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (4.52750396729,26.553346689), test loss: 26.5144467354\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.59194612503,3.02158104113), test loss: 2.31809311509\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (30.226108551,26.3841949744), test loss: 36.6451874852\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.47047042847,3.00718315826), test loss: 3.21419309825\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (13.3322849274,26.2158576435), test loss: 30.1297708988\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (4.8820066452,2.99312381657), test loss: 3.03497440219\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (10.6919956207,26.0494326827), test loss: 34.9168446779\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.83794176579,2.97927281314), test loss: 3.18313957155\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (7.68859958649,25.8860470793), test loss: 31.622296834\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.626609861851,2.96570166437), test loss: 3.27529051304\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (20.3819541931,25.7249157691), test loss: 32.4706667185\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.63818454742,2.95241572292), test loss: 2.66140326113\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (6.2766251564,25.5657899362), test loss: 37.0752749503\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.876588463783,2.93918467897), test loss: 3.42198233604\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (7.68153572083,25.4088121003), test loss: 32.6181483984\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.41817903519,2.92609904156), test loss: 2.356944561\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (10.3925619125,25.2541847774), test loss: 37.685936594\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (5.44841432571,2.9131462224), test loss: 3.24601768851\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (23.3600330353,25.1005234055), test loss: 30.4745396137\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.1872317791,2.90030041965), test loss: 2.44518574476\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (11.64041996,24.9486746321), test loss: 38.4766644001\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.87006497383,2.88784026572), test loss: 3.28960593641\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (4.71209144592,24.7999640818), test loss: 29.3960424423\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.36652398109,2.87551284962), test loss: 2.53306580484\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.90559005737,24.6518370296), test loss: 39.4695659876\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.96806025505,2.86348150363), test loss: 3.08743073642\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (4.14414310455,24.5064437213), test loss: 27.8118180275\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.78715133667,2.85144276687), test loss: 2.37228364348\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (4.91701889038,24.3635321515), test loss: 37.7853649616\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.754523992538,2.83950904041), test loss: 3.09540237188\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (9.40488815308,24.2222922498), test loss: 30.9225399494\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.36614155769,2.82774609059), test loss: 3.04246584177\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (9.8201417923,24.0822099304), test loss: 36.008306551\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.01659429073,2.81625093013), test loss: 3.1636005342\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (8.52418327332,23.9439930929), test loss: 31.7478800058\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.758330702782,2.80489817974), test loss: 3.23098604977\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (7.15623950958,23.8074243977), test loss: 32.775130558\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.92209649086,2.79377072855), test loss: 2.63927813172\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (13.4207611084,23.6727803935), test loss: 37.097088623\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.6326379776,2.78271356167), test loss: 3.30479176641\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (5.68020486832,23.5395361475), test loss: 33.7936788082\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.46158504486,2.77174874884), test loss: 2.37396854162\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (9.44508266449,23.4080976766), test loss: 39.4609978199\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (3.78908038139,2.76086170173), test loss: 3.30265481472\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (13.7318372726,23.2774190009), test loss: 30.8152921677\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.27386891842,2.7500824409), test loss: 2.39521615803\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (10.8342266083,23.1482796784), test loss: 39.2622718811\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.04165709019,2.73955497952), test loss: 3.19257565141\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (4.24867916107,23.021435713), test loss: 29.4677549601\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.768987238407,2.72914368579), test loss: 2.49951040745\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (3.84436631203,22.8949909558), test loss: 40.4299398661\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.82119655609,2.71897084203), test loss: 3.12720584869\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (8.83058261871,22.7704661678), test loss: 28.6090859413\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (4.85762405396,2.70876710567), test loss: 2.48117373884\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (3.54264116287,22.6475602814), test loss: 38.8508228779\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.69430243969,2.69860856433), test loss: 3.0707627058\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (16.7695198059,22.5255989885), test loss: 31.91147089\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.970395445824,2.68855861676), test loss: 3.04597959816\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (4.54048347473,22.4046759583), test loss: 38.2065089703\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.4089653492,2.67877602883), test loss: 3.21553632021\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (5.50938034058,22.285524017), test loss: 31.8903862\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (3.07717990875,2.66911224506), test loss: 3.18451489806\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (9.62010288239,22.1681988151), test loss: 34.2000169516\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.710233688354,2.65966442106), test loss: 2.66022251248\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.23601055145,22.0524220542), test loss: 37.896505475\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.16445922852,2.65023443137), test loss: 3.26692716777\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (9.37054157257,21.9378031636), test loss: 34.5887942314\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (3.35696744919,2.64085365081), test loss: 2.36697340012\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.99583625793,21.8243307613), test loss: 39.9158998013\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.50345551968,2.63154063031), test loss: 3.30807510316\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (11.7364044189,21.711446113), test loss: 31.0714881897\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.41518735886,2.62234800951), test loss: 2.34456386268\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (3.47005081177,21.599927569), test loss: 39.6924973488\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.853040337563,2.61334738785), test loss: 3.15325627327\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (5.1703004837,21.4903733022), test loss: 29.9466971397\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.620051264763,2.60441694041), test loss: 2.51723474562\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (6.81877374649,21.3812619734), test loss: 40.7113520622\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.32289028168,2.59566115731), test loss: 3.11129573956\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (3.69360923767,21.2735529344), test loss: 28.9350686073\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (3.27729105949,2.58686989768), test loss: 2.47609908581\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (9.77813434601,21.167355659), test loss: 39.2154989004\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.202439785,2.57815890607), test loss: 3.04416141212\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (5.92006778717,21.061672946), test loss: 32.5437782764\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.09344768524,2.56949259066), test loss: 3.05072838068\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (5.86557388306,20.9571097354), test loss: 38.117928791\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.3739221096,2.56102889858), test loss: 3.16796047539\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (6.17300653458,20.8535608151), test loss: 31.8007169962\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.2512512207,2.55262244513), test loss: 3.0765059799\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (7.88502788544,20.751225141), test loss: 34.4104988575\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.216721177101,2.54442546251), test loss: 2.67916054726\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (6.07508468628,20.6499605864), test loss: 37.2822817802\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.12865114212,2.53620602081), test loss: 3.20820149183\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (1.90899169445,20.5497329625), test loss: 35.3080626965\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.366044074297,2.52802310257), test loss: 2.40146895647\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (6.48568534851,20.4508907086), test loss: 40.3281692743\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.520139396191,2.51997475505), test loss: 3.28044649959\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (9.48114490509,20.3527774447), test loss: 31.505394125\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.06346035004,2.51200065425), test loss: 2.33459269702\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (4.05295753479,20.2558259464), test loss: 40.2792129993\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.88372147083,2.50418616476), test loss: 3.17995356321\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (10.5113983154,20.1602718259), test loss: 30.9228327751\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.950745403767,2.49641788573), test loss: 2.568285577\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (2.64663338661,20.0649965097), test loss: 40.8120822906\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.724025607109,2.48876903453), test loss: 3.08295212984\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (3.87536406517,19.9709149516), test loss: 29.4302158833\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.60934174061,2.48110006976), test loss: 2.46444484293\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.66096353531,19.8780194464), test loss: 39.3083093405\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.36833524704,2.47352391675), test loss: 2.98060843647\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (6.74740743637,19.7856840359), test loss: 33.0961747646\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.22840809822,2.46596885671), test loss: 3.06698745787\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (11.3094425201,19.6943339825), test loss: 38.322765398\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.72629499435,2.45853968613), test loss: 3.14083844423\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (6.16825294495,19.6038008938), test loss: 31.5033051014\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.662542998791,2.45115443587), test loss: 3.01421081722\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (4.75089931488,19.5143420414), test loss: 34.7395013094\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.85327148438,2.44399837348), test loss: 2.6795488447\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (9.19239425659,19.4257396863), test loss: 37.3736248255\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.922312796116,2.43677904227), test loss: 3.1732193917\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (2.66132068634,19.3379171103), test loss: 35.5807822704\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.427486658096,2.42958158756), test loss: 2.47942206264\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.41085243225,19.2509590032), test loss: 40.2648880959\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.453588694334,2.42247804057), test loss: 3.25854252577\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.78712749481,19.1643328802), test loss: 32.2932027817\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.08100438118,2.41541198256), test loss: 2.31049254239\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.97767496109,19.0786898028), test loss: 40.6607872009\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.34255552292,2.40849578385), test loss: 3.13424684405\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (2.83968305588,18.9942532771), test loss: 33.0587700844\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.693827390671,2.40165702608), test loss: 2.69136314988\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.74723625183,18.9105162028), test loss: 40.4938924074\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.783382534981,2.39491031141), test loss: 3.05048967898\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (2.76497864723,18.8277943541), test loss: 29.7022647381\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.81676745415,2.3881477393), test loss: 2.48463663161\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (5.11885261536,18.7460934876), test loss: 39.2137326717\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.39053559303,2.38146625698), test loss: 2.87767998874\n",
      "\n",
      "MC # 2, Hype # hyp5, Fold # 7\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold7/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (405.785858154,inf), test loss: 226.689992523\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (280.079742432,inf), test loss: 350.214596558\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (62.1672401428,128.888503901), test loss: 46.510246563\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.696007728577,84.0960658068), test loss: 3.41160791963\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (42.8297195435,87.3950273323), test loss: 39.7304316521\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.54109191895,43.6411242142), test loss: 3.17095255256\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (67.8595352173,73.3747497629), test loss: 42.6579508305\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.30401730537,30.1504579607), test loss: 3.2720691517\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (34.5379333496,66.2224864054), test loss: 42.5796509743\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.56981694698,23.4013590067), test loss: 3.50189671516\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (17.6415843964,62.0291720819), test loss: 40.0462749481\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.56674909592,19.360637232), test loss: 2.82066773772\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (53.4868278503,59.1377019965), test loss: 44.0059976578\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (7.47280693054,16.6675629961), test loss: 3.63597405553\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (24.1599597931,57.062845388), test loss: 43.3694444656\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.86866879463,14.7414931892), test loss: 2.63836708963\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (39.3506126404,55.4808537402), test loss: 43.98719244\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.13623857498,13.2995257284), test loss: 3.48100199103\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (102.057991028,54.2018185204), test loss: 41.2077362537\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.6353905201,12.1764808532), test loss: 2.94380282462\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (44.0695343018,53.1684319634), test loss: 45.2882015228\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.81832504272,11.28028092), test loss: 3.36460050344\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (55.2987060547,52.3079639254), test loss: 39.2300089836\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (5.595703125,10.5460288134), test loss: 2.80822401643\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (23.3251667023,51.6116558136), test loss: 44.4781963348\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.07357883453,9.93537682374), test loss: 3.41268803179\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (42.0221748352,50.9970659491), test loss: 40.5400639772\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.20382642746,9.41998968756), test loss: 3.07345811725\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (38.3289337158,50.4463391624), test loss: 41.3696577549\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (0.776647865772,8.98034339274), test loss: 3.2146005705\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (42.2608337402,49.9596632247), test loss: 41.7169802189\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.97179031372,8.59781439957), test loss: 3.55420042276\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (50.6950187683,49.5019398826), test loss: 38.7987097263\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.69609928131,8.26478467902), test loss: 2.77893840373\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (47.6109924316,49.088162863), test loss: 42.8287608147\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.99644339085,7.97106318965), test loss: 3.56199271679\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (69.3809280396,48.7032200542), test loss: 41.5933542728\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.15772819519,7.71005425155), test loss: 2.64721637666\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (32.3091049194,48.3684444726), test loss: 42.2030054092\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.86258029938,7.47701548372), test loss: 3.53144139051\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (53.9043731689,48.0644894763), test loss: 38.9665421963\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (9.74004650116,7.26960769573), test loss: 2.68254466355\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (31.7443828583,47.7466903611), test loss: 43.7926206589\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.785892963409,7.08119034628), test loss: 3.3911244601\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (23.3657875061,47.4585073496), test loss: 37.9481147766\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.674015045166,6.9102404921), test loss: 2.91741755605\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (114.065429688,47.1635759636), test loss: 41.8497900963\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (7.62600803375,6.75509568918), test loss: 3.23458659649\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (34.4457931519,46.8684040619), test loss: 38.5813364506\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.47884082794,6.61299029976), test loss: 3.18835792542\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (30.761838913,46.5706889143), test loss: 39.3425161839\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.6147851944,6.48251733149), test loss: 3.19344067276\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (15.0891113281,46.2639505472), test loss: 38.6301506996\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.5136153698,6.3617149261), test loss: 3.40577098727\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (51.857749939,45.9873579467), test loss: 35.9980910778\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.7541577816,6.2509780046), test loss: 2.80185075402\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (15.1180839539,45.697675236), test loss: 39.7802930832\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.8691893816,6.14844781986), test loss: 3.56621928215\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (60.766204834,45.405370942), test loss: 37.568319273\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (6.12669467926,6.05319949075), test loss: 2.68694263995\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (14.8221797943,45.1129945652), test loss: 38.0702054977\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.78317403793,5.96353003321), test loss: 3.41656399071\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (38.9097633362,44.8114389321), test loss: 35.6567656994\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.36646199226,5.87991402399), test loss: 2.83592548072\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (34.3174095154,44.5070231228), test loss: 40.2168599129\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.89477181435,5.80152232766), test loss: 3.3500541836\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (18.4875335693,44.1990190844), test loss: 33.0453744888\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.29163479805,5.72696320688), test loss: 2.86314638257\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (42.9209671021,43.8966099852), test loss: 38.5373571873\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.50789499283,5.6572003393), test loss: 3.34927937388\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (9.43602752686,43.5846340813), test loss: 33.9213272572\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.16710662842,5.59091204764), test loss: 3.0502825141\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (17.8294239044,43.2649291827), test loss: 36.0578054905\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.37272119522,5.52849744407), test loss: 3.18145286739\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (56.5598106384,42.9372569494), test loss: 34.5771967888\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (6.45164442062,5.46791522081), test loss: 3.44399931431\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (17.8860168457,42.5908878477), test loss: 31.5342847824\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.57945764065,5.40985255127), test loss: 2.68103881776\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (17.3063964844,42.2430418832), test loss: 35.1148236275\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.8064160347,5.35419372807), test loss: 3.52800568342\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (56.6783905029,41.8961225127), test loss: 31.7508585453\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.95989990234,5.3003100867), test loss: 2.5797806263\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (19.9363708496,41.5329740024), test loss: 33.1163303852\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.79757881165,5.24743877157), test loss: 3.47978724837\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (31.4473686218,41.1680140503), test loss: 30.3093213081\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.54061508179,5.19674220629), test loss: 2.54696346223\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (31.8905220032,40.7731404324), test loss: 35.533273077\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (8.09798526764,5.14720851606), test loss: 3.33773886561\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (14.4324398041,40.3748071108), test loss: 28.9440243721\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (5.23215103149,5.09814279523), test loss: 2.72446727753\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (10.3321752548,39.978148414), test loss: 33.9760387897\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.59954583645,5.05032253169), test loss: 3.10173446536\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (37.3988685608,39.5765147671), test loss: 30.6861670017\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.248937875032,5.00300530491), test loss: 3.23440598249\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (9.71955490112,39.1822537008), test loss: 33.181521821\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.83557319641,4.95712701552), test loss: 3.0769624114\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.2675437927,38.794527076), test loss: 29.6373923779\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.950005292892,4.91137834925), test loss: 3.24627638757\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (29.7659912109,38.4135855589), test loss: 28.3519803047\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.67900919914,4.86681198176), test loss: 2.51353475451\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (34.8980064392,38.0338770872), test loss: 30.6476369381\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.60098159313,4.8231910259), test loss: 3.44750351012\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (9.97444343567,37.6616288157), test loss: 29.0944901466\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.1530354023,4.78079151942), test loss: 2.36934117079\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (11.299071312,37.2990230592), test loss: 29.8286167145\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.950566709042,4.73880298316), test loss: 3.24003376067\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.1419849396,36.9371451952), test loss: 29.6273546219\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.37091255188,4.69797861575), test loss: 2.59449434876\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.0869026184,36.5845254213), test loss: 33.5472790718\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.04533433914,4.65784675571), test loss: 3.21355962604\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (12.9889593124,36.2402638346), test loss: 28.4501588821\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.914605617523,4.61851962377), test loss: 2.58382976502\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (17.0169696808,35.9017769314), test loss: 34.0869940281\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.50966501236,4.58035048908), test loss: 3.13552028835\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.0976657867,35.5663253815), test loss: 31.1684841633\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.40813732147,4.54384811575), test loss: 3.05595469177\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (11.547996521,35.2361846238), test loss: 33.896439147\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.516403615475,4.50776500933), test loss: 3.18740973175\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (3.58255910873,34.9156565541), test loss: 29.6563203812\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.802417159081,4.47248546106), test loss: 3.27868559062\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (47.2566452026,34.6028847239), test loss: 31.1609333754\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (5.53879356384,4.43810945274), test loss: 2.4425873071\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (5.16279697418,34.2927672036), test loss: 30.6101024628\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.92461633682,4.40411112745), test loss: 3.47758120298\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (15.4045562744,33.9911778244), test loss: 29.9752382278\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.39744973183,4.37078604785), test loss: 2.36399637014\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (29.5761413574,33.697062615), test loss: 30.1614504814\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.17643237114,4.3379784373), test loss: 3.20706148148\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.811504364,33.4081250208), test loss: 30.682294035\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.69697916508,4.30600990947), test loss: 2.41429485381\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (9.92303752899,33.1224659087), test loss: 34.2148423672\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.73802256584,4.27474995907), test loss: 3.29333592653\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (7.36894845963,32.8448317117), test loss: 28.8363463879\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.00159454346,4.24441404895), test loss: 2.50037881285\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (10.516793251,32.5740758142), test loss: 33.6458188534\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.623414039612,4.21453245186), test loss: 3.1741931513\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (10.4944286346,32.3052584223), test loss: 31.592058444\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.25864005089,4.18534280607), test loss: 3.22437552959\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (4.38157367706,32.0425447762), test loss: 34.1490246296\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.10659170151,4.15662200435), test loss: 3.24393496811\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (3.72567033768,31.7864197053), test loss: 30.6590368032\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.750120520592,4.12832820448), test loss: 3.28079417795\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (17.385093689,31.5350591629), test loss: 29.4077153206\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.86064338684,4.10072790359), test loss: 2.53206091672\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (17.2811737061,31.2868481337), test loss: 30.9090117455\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.62982213497,4.07365686984), test loss: 3.40778733194\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.432226181,31.0422919573), test loss: 30.5377055883\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.73873996735,4.04745289359), test loss: 2.36324535459\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (4.32193756104,30.8042562058), test loss: 30.598329854\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.7431075573,4.02157278236), test loss: 3.15328570753\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (31.8145751953,30.5691776413), test loss: 32.188147068\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.80980920792,3.9963895569), test loss: 2.44777129441\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (29.5164318085,30.3382886272), test loss: 34.1770524502\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (4.56751823425,3.97152099534), test loss: 3.28131106943\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (14.9467983246,30.1123799529), test loss: 28.9186536789\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.06878519058,3.94703409704), test loss: 2.51483518183\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (7.82894134521,29.8894603336), test loss: 34.1931376457\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.49798727036,3.92288663543), test loss: 3.28915184736\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (14.3477973938,29.6704505653), test loss: 32.0242852211\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.40739989281,3.89942902465), test loss: 3.135083884\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (14.688876152,29.4532279887), test loss: 35.0051115036\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.34649276733,3.87641501829), test loss: 3.24777657837\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (9.33266925812,29.2410982735), test loss: 29.5858708143\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (4.51093482971,3.85393819798), test loss: 3.17660667598\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (11.78881073,29.0339066311), test loss: 31.588649416\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.7407336235,3.8318612439), test loss: 2.5699113518\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (8.49272632599,28.8268635465), test loss: 30.9908242464\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.0033249855,3.81011957947), test loss: 3.40582479835\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (13.4705562592,28.6248427484), test loss: 30.9773237228\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.2944483757,3.78873171372), test loss: 2.34320952892\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (7.23230266571,28.4264148555), test loss: 30.9801781654\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.16216349602,3.76750580098), test loss: 3.10403509885\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (19.6177806854,28.2303611993), test loss: 32.143003273\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.59233427048,3.74677509786), test loss: 2.48008846939\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (19.6457767487,28.036102277), test loss: 34.9364910603\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.89386320114,3.72637122308), test loss: 3.22769298553\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (18.4490146637,27.8449875529), test loss: 29.5012333393\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.03196167946,3.70657672942), test loss: 2.45714906156\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (3.11450886726,27.6584202732), test loss: 34.8719713688\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.597145080566,3.68698020065), test loss: 3.24902504832\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (6.91548299789,27.473004668), test loss: 31.9300188541\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.8642783165,3.66787280059), test loss: 3.08886677623\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.6112670898,27.2908788083), test loss: 35.0660590649\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.49724292755,3.64886125417), test loss: 3.19510519207\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (10.0626182556,27.1117711761), test loss: 30.0863850832\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.26724004745,3.63014477712), test loss: 3.10925519466\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (7.04295635223,26.9350148179), test loss: 32.1189792633\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.955567955971,3.61159498885), test loss: 2.53339208066\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (7.65421152115,26.7603298401), test loss: 31.1236947536\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.26747989655,3.59355633232), test loss: 3.26371158957\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (7.71241188049,26.5871455999), test loss: 32.4447677612\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.01763415337,3.57577742635), test loss: 2.32424745858\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (4.88624668121,26.4174114149), test loss: 31.7481294632\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.855323314667,3.55831194363), test loss: 3.06789296269\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (28.4342803955,26.2509632115), test loss: 36.083188343\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.90975928307,3.54125740262), test loss: 2.48565871418\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (3.35257124901,26.0848980583), test loss: 35.4901294708\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.01068842411,3.52430554857), test loss: 3.16944127381\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (4.73104000092,25.9221939372), test loss: 29.9468988419\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.03436028957,3.5075056836), test loss: 2.41536425948\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (16.7415466309,25.7618981648), test loss: 35.888515377\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.31251907349,3.49091052472), test loss: 3.18444775492\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (18.432012558,25.6027990391), test loss: 33.20201931\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.871214210987,3.47462954482), test loss: 3.05363162458\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (4.73329114914,25.4445577584), test loss: 35.8861898661\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.27433347702,3.4585678413), test loss: 3.1331751436\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (2.21232485771,25.2894831592), test loss: 30.5814162016\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.19577479362,3.44291581139), test loss: 3.05836507678\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (7.69998168945,25.1371189058), test loss: 32.8048623562\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.52433943748,3.42744636114), test loss: 2.52119654119\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (13.6026000977,24.9856680729), test loss: 31.4256795883\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.889652967453,3.41221141371), test loss: 3.21162568629\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (6.78877973557,24.8366765611), test loss: 31.9844574928\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.80572557449,3.39710423653), test loss: 2.25867951214\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (5.87814188004,24.6893597959), test loss: 32.5860724926\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.41572785378,3.38210177605), test loss: 3.03803957105\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (13.9070053101,24.5445462908), test loss: 32.1815851212\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.29391527176,3.36737935276), test loss: 2.37825392634\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (12.0788040161,24.3999098549), test loss: 36.4604470253\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.46444988251,3.35282716724), test loss: 3.09977241158\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (14.878367424,24.2575874938), test loss: 30.6040241718\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.80143404007,3.33861895833), test loss: 2.39721919894\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (6.31901931763,24.1172579243), test loss: 36.5955743551\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.01619458199,3.32454382109), test loss: 3.13969083577\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (20.608379364,23.9787439246), test loss: 31.7740078449\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.62403512001,3.31076766788), test loss: 3.02419345975\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (11.1866989136,23.8420682008), test loss: 36.4177644253\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.07304286957,3.29707182621), test loss: 3.09566777349\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (4.96048307419,23.7067602743), test loss: 31.1421046019\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.62303376198,3.28344058711), test loss: 3.00040236712\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (10.2657108307,23.5734899144), test loss: 32.9266853809\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (3.60435366631,3.2700175452), test loss: 2.48772624135\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.09382009506,23.4408217856), test loss: 32.7751795769\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.11911201477,3.25678261211), test loss: 3.18035801053\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.93252086639,23.3094584443), test loss: 33.7458440781\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.09121847153,3.24376042588), test loss: 2.23106175512\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (7.14352464676,23.1802337241), test loss: 33.4547994614\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (4.25453567505,3.23098754603), test loss: 3.03675798476\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.95134019852,23.0529185229), test loss: 34.8137287617\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.886818766594,3.2183330251), test loss: 2.40846486688\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (8.04819488525,22.9263503542), test loss: 37.144376111\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.96938073635,3.20587426479), test loss: 3.06892843544\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (8.17309093475,22.8016908465), test loss: 30.4661821842\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.92559158802,3.19348054137), test loss: 2.36709801406\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (4.87594270706,22.6785011604), test loss: 37.1064834595\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.53829526901,3.18112463892), test loss: 3.11173521578\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (4.88709068298,22.5564095683), test loss: 31.5436905384\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.243693590164,3.16901543663), test loss: 2.96563462466\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (23.4789924622,22.4351994626), test loss: 38.1194602966\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.08041453362,3.15701176927), test loss: 3.10181422085\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (18.5229625702,22.3153678541), test loss: 31.5306716919\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.68516850471,3.14529670871), test loss: 2.96759713143\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (1.96005892754,22.1972129978), test loss: 33.4478347301\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.175547122955,3.1336501333), test loss: 2.51773370504\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (5.15343952179,22.0802321742), test loss: 31.7840399504\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.59724712372,3.12226068054), test loss: 3.1395907104\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (5.17640638351,21.965076945), test loss: 32.9829439163\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.3057615757,3.11086869822), test loss: 2.22804517448\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (12.1052093506,21.8505938109), test loss: 35.0847878456\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.08450829983,3.09956127311), test loss: 3.02050225735\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (7.79565000534,21.7376223872), test loss: 32.8131904602\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.72195959091,3.08837477591), test loss: 2.35035851002\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (5.15628433228,21.6253533304), test loss: 38.0490502834\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.845731973648,3.07736476018), test loss: 3.0217474848\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (5.65015888214,21.5140478706), test loss: 32.3239696503\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.574010849,3.06649121229), test loss: 2.39655293077\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (6.50256061554,21.4043066645), test loss: 37.6685377121\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.4204531908,3.0557802692), test loss: 3.06085742116\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.11566352844,21.2961556445), test loss: 33.889222908\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.389429211617,3.04525180945), test loss: 3.07776174843\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (4.85212564468,21.1885575922), test loss: 37.99106884\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.11595118046,3.03477827721), test loss: 3.03017876297\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (2.89289712906,21.0822379869), test loss: 31.7161007643\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (3.25942134857,3.02435684044), test loss: 2.92942143679\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (3.75328826904,20.9772973026), test loss: 34.0840503693\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.3677482605,3.01397991095), test loss: 2.45598348081\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (7.19841861725,20.8729445178), test loss: 32.9327541351\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.65819883347,3.00380336812), test loss: 3.11010630727\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (6.16365480423,20.7692330704), test loss: 33.9425076485\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.954203307629,2.99367231277), test loss: 2.21901310533\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (4.20533943176,20.6668628961), test loss: 35.206931138\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.932174086571,2.9837801268), test loss: 3.02924489826\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (2.43656420708,20.5660866863), test loss: 34.2366280079\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.32881736755,2.97398521661), test loss: 2.36396446824\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (8.95321941376,20.4659832018), test loss: 39.2205841064\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.814897894859,2.96430908303), test loss: 2.95832119435\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (5.32291221619,20.3670474302), test loss: 30.9713097572\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.53677105904,2.95466601763), test loss: 2.27437080443\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (7.86660766602,20.2689375962), test loss: 38.0918455124\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.79223442078,2.94503626602), test loss: 3.06202831119\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.89247465134,20.1718185785), test loss: 33.0622933865\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.561043441296,2.93557599036), test loss: 2.87277392745\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (15.1106472015,20.0754247418), test loss: 37.7325151443\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.77034330368,2.9261766251), test loss: 2.99666229486\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (8.26395702362,19.9796660777), test loss: 34.614136219\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.12425422668,2.91697519354), test loss: 2.92468866706\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (5.2473897934,19.885259816), test loss: 34.5495923996\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.50745511055,2.90782524855), test loss: 2.3567208156\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (7.40749692917,19.7918837231), test loss: 32.154941988\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.13974666595,2.89885820758), test loss: 3.04658259153\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.70934104919,19.6993858007), test loss: 33.5064252377\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.96140098572,2.8898942354), test loss: 2.22732085437\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (1.87529802322,19.6078179339), test loss: 36.1301619053\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.86558198929,2.88095897237), test loss: 2.99776407778\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (8.25402069092,19.5169987829), test loss: 32.4796267986\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.63355922699,2.87213435171), test loss: 2.31799472719\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (3.14314293861,19.4268605893), test loss: 38.4728640079\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (4.62357854843,2.86340697577), test loss: 2.88693575561\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (8.5940322876,19.3373804023), test loss: 31.1973851204\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.38702833652,2.85475098241), test loss: 2.23090107441\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.335521698,19.2488556552), test loss: 38.0525996208\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.97289383411,2.84625129414), test loss: 3.06345813572\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (4.15254116058,19.1616291611), test loss: 31.8658653259\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.624761819839,2.8378183036), test loss: 2.88293998092\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.26028776169,19.0747453349), test loss: 38.0814930677\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.37573242188,2.82952416989), test loss: 2.97469774187\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (6.8104019165,18.9890484253), test loss: 32.0622228146\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.743385970592,2.82119608193), test loss: 2.87058862746\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (2.88203644753,18.9039901556), test loss: 34.7103157043\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.95652365685,2.81291980489), test loss: 2.30310435891\n",
      "\n",
      "MC # 2, Hype # hyp5, Fold # 8\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold8/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (360.829986572,inf), test loss: 201.383940506\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (277.46105957,inf), test loss: 334.530319214\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (26.4284362793,100.988918193), test loss: 37.5402156115\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.517048835754,39.1017146989), test loss: 3.23893590271\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (35.9746780396,74.1901419907), test loss: 33.6036883354\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.30467319489,21.07645044), test loss: 3.21271921694\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (2.69907903671,65.2142385592), test loss: 36.915579319\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.1175866127,15.0580596868), test loss: 3.43567231297\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (13.7975721359,60.7930436269), test loss: 35.2270074368\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.1161441803,12.0408038207), test loss: 3.45886234045\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (119.128692627,58.1340063749), test loss: 39.3256243229\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.07185935974,10.2346981666), test loss: 2.94413294792\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (24.2862987518,56.2351770484), test loss: 41.0820083141\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.78323125839,9.03456759573), test loss: 3.39890161753\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (35.0718154907,54.8589638718), test loss: 40.8903720498\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.73772931099,8.17211229573), test loss: 3.03277015239\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (43.8867950439,53.7760386914), test loss: 40.2524776936\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.62682819366,7.52777946967), test loss: 3.3341734767\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (54.2318687439,52.8701617148), test loss: 37.3333647251\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.66483306885,7.02545546967), test loss: 2.8295927465\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (46.9198684692,52.1306845075), test loss: 39.2785668373\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.28032255173,6.62132835759), test loss: 3.19285021424\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (36.9025382996,51.5247319811), test loss: 35.2294965744\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.89256286621,6.29145059402), test loss: 2.80235958099\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (33.9091720581,50.9872988463), test loss: 37.0685603142\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.69390368462,6.01649780605), test loss: 3.36944439411\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (81.2824249268,50.5081527812), test loss: 32.2132479191\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.5140414238,5.78624821087), test loss: 2.60924461782\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (43.0569648743,50.0674772812), test loss: 35.0219380379\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (0.449076443911,5.58565864128), test loss: 3.12049239874\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (60.2214660645,49.6486460536), test loss: 31.4022685051\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.83053731918,5.41212955812), test loss: 3.17201567292\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (27.2043991089,49.2339156714), test loss: 35.4202438354\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.54871273041,5.25651746197), test loss: 3.0696074605\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (49.8584098816,48.8618419398), test loss: 34.1651313782\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.57472395897,5.11620551141), test loss: 3.13122656643\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (24.1254348755,48.5208114504), test loss: 37.7769278049\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.56181073189,4.99135959896), test loss: 2.8031339258\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (38.1431121826,48.1848128149), test loss: 38.2227908611\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.26366281509,4.87977139405), test loss: 3.38462927938\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (30.5279312134,47.8479819076), test loss: 37.5267084599\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.04680395126,4.78023219452), test loss: 2.66781488657\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (44.5242729187,47.5179809906), test loss: 35.433925581\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.19767451286,4.6873766747), test loss: 3.00570133626\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (46.305519104,47.1734545464), test loss: 33.0372064114\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.75088357925,4.59867324175), test loss: 2.69441983849\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (53.4308624268,46.8138652045), test loss: 35.3056447506\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.34939801693,4.51532153996), test loss: 2.89610168636\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (5.15047597885,46.4570936907), test loss: 31.0375887394\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.74313545227,4.43688505557), test loss: 2.58143874854\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (12.8358955383,46.0998340428), test loss: 32.107174933\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.703015804291,4.36400749858), test loss: 3.30558240414\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (45.7721443176,45.7126577972), test loss: 26.6330265522\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.04174423218,4.29619319075), test loss: 2.89102199376\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (39.6657752991,45.3165437024), test loss: 29.5788717985\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (4.5071811676,4.23331201369), test loss: 3.18129865527\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (28.1349525452,44.8946902607), test loss: 26.7149274111\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.93905639648,4.17331357866), test loss: 2.95959864259\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (36.1767539978,44.4212854784), test loss: 29.2040673733\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.59755563736,4.11667513387), test loss: 2.70332308114\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (24.1445007324,43.9237872046), test loss: 30.995182848\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.65425539017,4.0626061897), test loss: 3.06180521995\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (35.8857879639,43.4267249578), test loss: 29.3714653969\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.59956347942,4.00999828461), test loss: 2.44624477625\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (14.2576503754,42.9251860811), test loss: 31.0275355816\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.26152706146,3.96040752179), test loss: 3.20737742186\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (26.1850414276,42.4135240835), test loss: 26.4705331326\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.64514684677,3.91321192908), test loss: 2.48303967491\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (28.800994873,41.9039268229), test loss: 29.0185105324\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.69682860374,3.86880344808), test loss: 3.02319841087\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (18.1739120483,41.3948608302), test loss: 25.0506354809\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.15833950043,3.82623121891), test loss: 2.55370385349\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (22.8236961365,40.8794238361), test loss: 27.5860118389\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.49840068817,3.78515905503), test loss: 2.89138676524\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (13.4734153748,40.3696998635), test loss: 22.7083891392\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.724592089653,3.74546064589), test loss: 2.38971335292\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (32.4331703186,39.8727804613), test loss: 26.5164969683\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.53398776054,3.70626067971), test loss: 3.06370897889\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (10.5490608215,39.3839516296), test loss: 22.5385774612\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.52238023281,3.66890719438), test loss: 3.02370294034\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (7.93627357483,38.9003512531), test loss: 26.1526938677\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.37711644173,3.63298312272), test loss: 3.13137580752\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (11.9976606369,38.4313924941), test loss: 23.0726408005\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.11855244637,3.59881674202), test loss: 3.02779467553\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (24.1486186981,37.9770919237), test loss: 28.8133935452\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.6302280426,3.56557966212), test loss: 2.65933587551\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (29.1414413452,37.5263356959), test loss: 26.775713706\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.30002808571,3.53327325472), test loss: 3.14734019935\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (10.4460554123,37.0883726863), test loss: 28.6127729893\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (5.23048496246,3.50202570946), test loss: 2.5266970396\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (44.9696159363,36.6632335388), test loss: 27.339674592\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.64008665085,3.47090511657), test loss: 2.99104453027\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (16.7555007935,36.2490244247), test loss: 27.8447828293\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.37538409233,3.44122409789), test loss: 2.58525970578\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (14.3415842056,35.8425665957), test loss: 28.319408226\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.7114995718,3.4124973085), test loss: 2.9326772362\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (12.0499639511,35.4499516639), test loss: 26.0221665382\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.74850821495,3.38504800865), test loss: 2.55873185396\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (14.982591629,35.0696842446), test loss: 28.7766955376\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.72609353065,3.35847495402), test loss: 3.14978488386\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (45.2422447205,34.6942189928), test loss: 25.1767641783\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.47239005566,3.33242757451), test loss: 2.90800859034\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (9.66677570343,34.329515165), test loss: 28.0288736582\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (6.51117897034,3.30690579315), test loss: 3.23386229575\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (5.80964946747,33.9738956485), test loss: 23.3985205889\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.52765655518,3.28158109698), test loss: 2.88164269477\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (23.9499359131,33.6274068737), test loss: 28.3974442005\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.0147690773,3.25730289435), test loss: 2.76246280372\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.7708740234,33.2859718927), test loss: 28.0556723595\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.80387973785,3.23366803766), test loss: 3.00825216472\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (10.6475381851,32.9561856206), test loss: 29.1945705414\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.62445282936,3.21094407513), test loss: 2.42527461052\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (14.7969293594,32.6349735561), test loss: 31.8200483561\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.47893601656,3.18880646126), test loss: 3.02699320316\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (23.8933620453,32.3171177511), test loss: 28.2438510895\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.90637350082,3.16713076322), test loss: 2.43478153646\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (7.89543533325,32.0082351664), test loss: 30.5646397352\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (4.1353931427,3.14554514961), test loss: 2.96266973913\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (16.4830741882,31.7054646647), test loss: 26.1794438839\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.66753721237,3.1243620107), test loss: 2.58350128531\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (4.75240802765,31.4083432168), test loss: 29.7519746065\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.671697497368,3.10390937366), test loss: 3.06761985421\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (9.24273300171,31.116469743), test loss: 24.0196720123\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.43640995026,3.08389448401), test loss: 2.31281348765\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (5.88951063156,30.8330450471), test loss: 29.5090124607\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.888130247593,3.06456869637), test loss: 2.98501428962\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (32.6530838013,30.5562696541), test loss: 25.6588287592\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.46599531174,3.04573653617), test loss: 2.92841889858\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (8.6432094574,30.2826409682), test loss: 29.4910801888\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.504526436329,3.02714265456), test loss: 3.03979755193\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (16.1356220245,30.0164414546), test loss: 24.5272025585\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.04634380341,3.0086576054), test loss: 2.92438619733\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (6.69059658051,29.7532827603), test loss: 31.8688892365\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.14439535141,2.99051130957), test loss: 2.69798094332\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (33.7506370544,29.4964371356), test loss: 27.344009161\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.05187797546,2.97291040545), test loss: 3.07867914736\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (11.2664451599,29.2428822937), test loss: 30.8253680706\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.6206741333,2.95561848332), test loss: 2.40965977609\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (7.60945653915,28.9965210796), test loss: 30.0748199701\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.23384213448,2.93893253195), test loss: 2.85904583335\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (3.93910551071,28.7545553643), test loss: 31.046663332\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.26820540428,2.92260179609), test loss: 2.51454334557\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (17.9763355255,28.5163649248), test loss: 31.4801270008\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.23938989639,2.90643884495), test loss: 2.8750137955\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (8.55939483643,28.283454525), test loss: 26.4372760773\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.82201230526,2.89032226963), test loss: 2.48067042828\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (28.6089820862,28.0535691802), test loss: 31.0849281788\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.887818276882,2.87455283431), test loss: 3.02408679724\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (7.29588937759,27.8273792034), test loss: 25.6902526855\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.21090650558,2.85921333265), test loss: 2.97859092355\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (4.00638437271,27.6050918973), test loss: 30.9757368803\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.60774517059,2.8441076736), test loss: 3.02488273084\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (4.20258045197,27.3885618566), test loss: 24.4356249332\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.21412205696,2.82944600046), test loss: 2.84486313164\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (24.2325172424,27.1757142908), test loss: 33.1627251625\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.40766811371,2.8151726811), test loss: 2.79654574394\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (4.75020885468,26.9660630145), test loss: 28.4130048752\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.38508176804,2.80099349074), test loss: 2.9284189105\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (20.6281356812,26.7603698487), test loss: 31.7913583755\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.29295611382,2.78690707378), test loss: 2.39763502479\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.303273201,26.5570803237), test loss: 32.2216567039\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.12041199207,2.77301196968), test loss: 2.8891407907\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (10.6579942703,26.3573635561), test loss: 30.7898307085\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.19912576675,2.75950733698), test loss: 2.47297124863\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.3616447449,26.1608331621), test loss: 33.5013455391\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.93534767628,2.74620348491), test loss: 2.91628764868\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.60328722,25.9687715288), test loss: 27.1258115768\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.69536232948,2.73324109309), test loss: 2.38716909587\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (9.04214954376,25.7796562745), test loss: 34.1073724508\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.348660618067,2.72063885091), test loss: 2.96890017688\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (15.8396968842,25.5939343675), test loss: 24.1268941879\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.51174545288,2.70811045299), test loss: 2.25567054749\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (9.37047958374,25.4107108027), test loss: 32.9793208599\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.23589468002,2.69563724348), test loss: 3.07196322381\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (6.56296920776,25.2296842242), test loss: 24.4363112688\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.20694637299,2.68332411189), test loss: 2.69952949286\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (16.1133728027,25.0519400744), test loss: 32.1851925373\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.85964930058,2.67124970968), test loss: 2.91947589815\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (6.9900727272,24.8766700479), test loss: 25.9150732994\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.62003493309,2.65950886277), test loss: 2.82258961201\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (3.57380819321,24.7049662485), test loss: 34.7212391853\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.754259049892,2.64795304484), test loss: 2.64542512298\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (17.7498512268,24.5358858779), test loss: 29.3810430527\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.32580113411,2.63673070007), test loss: 2.97564020157\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (7.20081853867,24.3696813397), test loss: 31.8371320248\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.548493027687,2.6255412268), test loss: 2.38664582074\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (10.2771043777,24.2058676195), test loss: 32.2583364725\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.6598136425,2.61434945186), test loss: 2.83089262247\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (7.69412899017,24.0436558183), test loss: 30.2238422871\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.02593708038,2.60343981986), test loss: 2.46698575318\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.60440444946,23.8836954449), test loss: 35.3389977455\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.36780142784,2.5925826071), test loss: 2.96878364682\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (14.5262928009,23.7265141058), test loss: 26.7981245995\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.24797070026,2.58206472641), test loss: 2.44898298085\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (2.40330982208,23.572248088), test loss: 33.997726059\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.395200371742,2.57173005308), test loss: 2.98968260139\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (5.48882198334,23.4198647989), test loss: 25.1096802711\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.340975821018,2.56165875028), test loss: 2.65229337513\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (6.47840690613,23.270079744), test loss: 32.9717998981\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (3.17219996452,2.5516002892), test loss: 3.023070319\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (5.33499336243,23.1221964647), test loss: 26.2342880726\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.998889446259,2.54143792002), test loss: 2.77147869468\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (10.6230354309,22.9758242386), test loss: 34.6121386528\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.85394406319,2.53158801656), test loss: 2.85972086191\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (12.7689476013,22.8312614803), test loss: 29.4766289711\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.40608215332,2.52185714107), test loss: 2.93230709434\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.73052787781,22.6888459351), test loss: 33.8390436649\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.41580021381,2.51240136924), test loss: 2.45433337986\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (3.95456290245,22.5493386902), test loss: 33.4145174026\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.19123530388,2.50305047696), test loss: 2.87966528535\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (9.41316223145,22.4112175962), test loss: 32.3770992279\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.53732645512,2.49392099285), test loss: 2.40029158741\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (15.167427063,22.2754998014), test loss: 35.1634160042\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.645628273487,2.48478161792), test loss: 2.89240523577\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (9.33074378967,22.1413202036), test loss: 27.6860628605\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.08328485489,2.47560886419), test loss: 2.37645869851\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (4.88503360748,22.0081095561), test loss: 36.728415513\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.310817480087,2.46667953181), test loss: 3.0363776505\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (13.7020397186,21.8767036538), test loss: 25.0301396847\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.70918893814,2.45784275352), test loss: 2.3247985363\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (4.35794830322,21.7472451024), test loss: 34.4911538601\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.98203492165,2.44923875132), test loss: 3.07183454633\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (6.24411869049,21.6202722178), test loss: 24.742366457\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.71016335487,2.4407524051), test loss: 2.68325812519\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (7.14350557327,21.4943516735), test loss: 33.3720796108\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.824345886707,2.43246164913), test loss: 2.76341515779\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (7.04517507553,21.3704834269), test loss: 27.2038938999\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.979032516479,2.4241267959), test loss: 2.72559751868\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (3.90412449837,21.2481821212), test loss: 35.6720349312\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.426108926535,2.41572534998), test loss: 2.56047217995\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (5.62405395508,21.126500133), test loss: 30.5993548393\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.25823879242,2.40761359073), test loss: 2.89889407158\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (4.31344175339,21.0062712644), test loss: 32.4212081194\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.82449281216,2.39952766413), test loss: 2.3670111835\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (4.25411605835,20.8881763727), test loss: 33.9235684156\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.59729129076,2.39164459877), test loss: 2.89579691887\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (6.37695121765,20.7719592827), test loss: 30.2605567932\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.51077914238,2.38389737205), test loss: 2.42333270609\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (7.1357088089,20.6566481473), test loss: 36.3033197403\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.94358205795,2.37630870823), test loss: 2.90103718042\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (6.10018539429,20.5430289492), test loss: 27.3547863483\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.98337584734,2.36864011294), test loss: 2.42052591592\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.47681427,20.4310083515), test loss: 34.8549927235\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.61494851112,2.36096185319), test loss: 2.94218080938\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.83862113953,20.3194830431), test loss: 25.8407969236\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.05547809601,2.35350262865), test loss: 2.59248795807\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (4.208568573,20.2090812414), test loss: 34.1453718662\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.49397826195,2.34607722934), test loss: 3.01431215703\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (6.85907697678,20.1005331513), test loss: 25.8385985851\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.23328018188,2.33885202145), test loss: 2.83567077518\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (4.8875207901,19.9936132453), test loss: 34.8176276922\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.624792933464,2.33168168908), test loss: 2.82796738744\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (4.60993003845,19.8876410022), test loss: 30.0818283796\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.4643137455,2.32469080451), test loss: 2.87061320543\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (3.42033863068,19.7831455359), test loss: 34.6837464333\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (4.25108003616,2.3176581862), test loss: 2.44355962574\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (17.4454994202,19.6798839627), test loss: 34.0137932777\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.0253674984,2.31057624265), test loss: 2.83221503496\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (7.22992897034,19.577110035), test loss: 32.626519537\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.735354244709,2.30368847971), test loss: 2.38177517056\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.348736763,19.4755251441), test loss: 36.1500585556\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.716624140739,2.29682255216), test loss: 2.85367712975\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (6.84659481049,19.3754303729), test loss: 28.1286403656\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.22775769234,2.29010759202), test loss: 2.44878011644\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (4.03747224808,19.2767433135), test loss: 37.9299841881\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.929404675961,2.2835520894), test loss: 2.98139347434\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (5.00611782074,19.178939923), test loss: 26.0732461452\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.60007494688,2.27703959764), test loss: 2.36166167259\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (3.83344912529,19.0824614115), test loss: 36.1795627594\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (5.16604328156,2.27049059212), test loss: 3.14143802524\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (1.84330153465,18.9868844914), test loss: 26.3370491743\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.690546154976,2.26398249298), test loss: 2.61792024821\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (6.71659183502,18.8919960447), test loss: 33.7911305428\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.768973350525,2.25759122585), test loss: 2.72879119813\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.23389911652,18.7980628407), test loss: 29.8293406963\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.71755623817,2.25124018449), test loss: 2.71485096514\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (11.4570732117,18.7054836413), test loss: 36.4351882935\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.0714404583,2.24499092096), test loss: 2.55897601545\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (3.25313711166,18.6141509952), test loss: 32.7841051579\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.160559237003,2.2388999673), test loss: 2.94366506338\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (4.48565340042,18.5236016838), test loss: 33.1520648241\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.2385468483,2.23285178623), test loss: 2.33213926852\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (2.86847114563,18.4341157806), test loss: 34.9649330616\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (3.7213010788,2.22669185682), test loss: 2.8141780436\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (7.52667999268,18.3456052188), test loss: 30.2401943207\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.633303940296,2.22068403409), test loss: 2.41947830319\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.12532520294,18.2575784101), test loss: 36.560013485\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.56693571806,2.21470473515), test loss: 2.85003936589\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (4.92542982101,18.1704308264), test loss: 28.7810398102\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.11804151535,2.2088097315), test loss: 2.42134120762\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (3.27926397324,18.0844215503), test loss: 35.9156835556\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.830923140049,2.20297935709), test loss: 2.93335125744\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (6.53977394104,17.9996968282), test loss: 26.5504410744\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.376444399357,2.19731274844), test loss: 2.62687090039\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (2.95242285728,17.9156437816), test loss: 34.6577198982\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.472680330276,2.19163237146), test loss: 3.0153076902\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (11.172410965,17.8324990282), test loss: 26.3126300335\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.53449368477,2.1858836935), test loss: 2.7639459908\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (5.44084024429,17.7500230292), test loss: 35.8516709805\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.99449563026,2.18029678192), test loss: 2.87694332302\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (9.92221736908,17.6682718738), test loss: 30.6801868677\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.792983293533,2.17470145965), test loss: 2.85727069378\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (4.92843294144,17.5873243835), test loss: 36.225245285\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.00074720383,2.16919702683), test loss: 2.42358503342\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (4.29020500183,17.5072590117), test loss: 34.9146671772\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.86836731434,2.16374825603), test loss: 2.79878993034\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (2.4296476841,17.4283076436), test loss: 33.7932653904\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.47679054737,2.15843549433), test loss: 2.48021743\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (6.02797603607,17.3500181561), test loss: 36.347417593\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.91772460938,2.15308752962), test loss: 2.86245235205\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (2.0546913147,17.2725222316), test loss: 28.488506937\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.537858843803,2.14770257632), test loss: 2.44385513514\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (10.9827537537,17.1957390679), test loss: 35.137988615\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.86962556839,2.14246453433), test loss: 2.81212715805\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.2281088829,17.1194586336), test loss: 28.3980816841\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.55454492569,2.13723110068), test loss: 2.67582680881\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.37240934372,17.0439204549), test loss: 36.198436451\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.26449012756,2.1320184669), test loss: 2.96107322872\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (3.54230213165,16.969319481), test loss: 25.8718528271\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.68344020844,2.12690589288), test loss: 2.64416175187\n",
      "\n",
      "MC # 2, Hype # hyp5, Fold # 9\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold9/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (263.128814697,inf), test loss: 192.555650711\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (337.560058594,inf), test loss: 365.147773743\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (41.0794677734,72.2040627146), test loss: 45.9977640629\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.42075538635,60.6245552664), test loss: 3.45245028138\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (60.6479454041,58.7651377442), test loss: 36.1419057369\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.91470599174,31.9809186309), test loss: 2.99414755702\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (26.3414745331,54.2546421854), test loss: 45.3296287298\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.39774894714,22.4233633267), test loss: 3.55705132484\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (22.0513420105,52.0216293265), test loss: 40.7190768242\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.47082519531,17.647275513), test loss: 3.64758296907\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (18.7404594421,50.5798959908), test loss: 46.9814643383\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.39833211899,14.7871419803), test loss: 3.49145711362\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (24.2893295288,49.5261604343), test loss: 44.4759290218\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (5.00914669037,12.8805404844), test loss: 3.77189000249\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (30.6120128632,48.7510128976), test loss: 43.8191616297\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.38926887512,11.5170396516), test loss: 2.86546450257\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (57.9128341675,48.0519106411), test loss: 45.3367694855\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.92286586761,10.4918568655), test loss: 3.99299960732\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (89.8810043335,47.5431249684), test loss: 42.6760155678\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.69805240631,9.69383139351), test loss: 2.94335567355\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (30.610200882,47.0472468937), test loss: 43.7112689495\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.2887018919,9.05336320374), test loss: 3.95878696442\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (42.053440094,46.6649867374), test loss: 39.4833686829\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (8.83120727539,8.53312819257), test loss: 2.89209849238\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (27.8176059723,46.2827936938), test loss: 45.6414758205\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.59169173241,8.09883713939), test loss: 3.66091752648\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (39.0453414917,45.9226447354), test loss: 37.1709940434\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.86177110672,7.72971938366), test loss: 3.24858366251\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (46.5907974243,45.5680936461), test loss: 43.310864687\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.68861675262,7.41372512811), test loss: 3.29469442666\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (34.1848678589,45.2061130382), test loss: 33.0652486324\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (5.23537349701,7.13847452864), test loss: 2.85566132665\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (50.2516975403,44.87729407), test loss: 41.6588350296\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (5.38682889938,6.89532635988), test loss: 3.42788684964\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (15.1065349579,44.5615985813), test loss: 37.0239866734\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.763923764229,6.68108045512), test loss: 3.47771943808\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (69.0085296631,44.2381282765), test loss: 41.406051898\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.35602807999,6.49188560795), test loss: 3.39310817719\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (23.6558380127,43.903060714), test loss: 40.9464037895\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (0.621400535107,6.32029743999), test loss: 3.7365912497\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (43.6399726868,43.5642497828), test loss: 38.4398966312\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.13585639,6.166199935), test loss: 2.92456020713\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (36.4963340759,43.200122243), test loss: 41.7874725819\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.50476360321,6.02480210161), test loss: 3.85178785324\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (91.5114974976,42.8543321782), test loss: 37.3975568056\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.76431369781,5.89501490869), test loss: 2.76754463911\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (28.9746284485,42.488824304), test loss: 39.6578927994\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.69281172752,5.77515125973), test loss: 3.72847445011\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (48.6791610718,42.1217370492), test loss: 33.491801548\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.6820473671,5.66526703352), test loss: 2.79783933163\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (94.278678894,41.7421980158), test loss: 39.6576966286\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (6.02705717087,5.56419183172), test loss: 3.55794789195\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (46.1661834717,41.3407963277), test loss: 31.3304010391\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.98952722549,5.46827747328), test loss: 3.02726542354\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (44.5851402283,40.9192547084), test loss: 37.4268202782\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (6.81291866302,5.37828409554), test loss: 3.20759113431\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (22.4661102295,40.479413704), test loss: 27.7597170353\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.75395011902,5.29275555324), test loss: 2.73558684587\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (47.5105743408,40.0396487713), test loss: 35.4218771935\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.81233358383,5.21103259075), test loss: 3.32129305005\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (22.1586074829,39.5969216212), test loss: 31.1410268307\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.73954725266,5.13418649182), test loss: 3.26155552566\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (21.6223011017,39.1418298857), test loss: 34.6086842537\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (5.88816070557,5.06085398912), test loss: 3.25430438519\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (7.60185909271,38.6844275797), test loss: 34.7305623531\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.618737220764,4.98991130125), test loss: 3.64702332616\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (45.7311248779,38.2233728986), test loss: 29.5481563807\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.23256874084,4.92199912625), test loss: 2.69277687669\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (18.4275760651,37.755267823), test loss: 35.5432653904\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.61302530766,4.85608890877), test loss: 3.67147027254\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (18.431098938,37.2980397121), test loss: 30.2409309387\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.48876643181,4.79129517254), test loss: 2.54861309826\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (12.5704536438,36.8394695866), test loss: 34.2380855322\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (6.55518245697,4.72676214769), test loss: 3.5876321733\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (20.3756313324,36.3865543555), test loss: 27.8380657911\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.26334953308,4.66456409092), test loss: 2.6042181924\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (13.5880641937,35.9413954687), test loss: 35.3494068146\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.11520338058,4.60536414396), test loss: 3.42459582984\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (10.1385498047,35.5092326484), test loss: 27.5754184723\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.11324810982,4.54824576681), test loss: 2.80907485485\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (25.3449344635,35.0825106586), test loss: 35.765319109\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.51435184479,4.49300937108), test loss: 3.32953931689\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (16.9135379791,34.6699484462), test loss: 25.9067550659\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (5.17288160324,4.44008612018), test loss: 2.69741032869\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (13.6406154633,34.2685627849), test loss: 33.984543848\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (5.15469551086,4.38859071801), test loss: 3.35638064742\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (10.8420314789,33.8777215335), test loss: 30.3380640984\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.69591391087,4.33931024642), test loss: 3.33260597587\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (19.0638618469,33.4969235312), test loss: 33.8686452866\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.531699299812,4.29192643725), test loss: 3.38801636398\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (18.0590515137,33.1286703112), test loss: 31.1213315487\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.57439136505,4.24657840616), test loss: 3.42831326127\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.1647996902,32.7696006845), test loss: 29.7695884705\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.489783167839,4.20276041254), test loss: 2.74284998775\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.2212505341,32.4182600409), test loss: 33.8208789825\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.45537865162,4.160369381), test loss: 3.63690089881\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (6.47355651855,32.0791363164), test loss: 30.5405978203\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.827825784683,4.11883485369), test loss: 2.58620212078\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.8809776306,31.7466018446), test loss: 33.7707369328\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.459386378527,4.07859174833), test loss: 3.50851693749\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (15.874879837,31.4214516314), test loss: 28.3979990005\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.52710211277,4.03998327216), test loss: 2.42641918659\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (27.0365524292,31.1077855582), test loss: 34.8276496887\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.52085757256,4.00293580443), test loss: 3.38650358319\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (32.354511261,30.8041802197), test loss: 30.0982982874\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.98647499084,3.96707871107), test loss: 2.70711808354\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (3.2788169384,30.501026894), test loss: 36.2397727489\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.779967665672,3.9318811546), test loss: 3.42113633156\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (1.94281256199,30.2088890005), test loss: 27.2071602821\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.59318685532,3.89758853237), test loss: 2.69415694475\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (5.93476486206,29.9208324619), test loss: 33.7662485838\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.01359534264,3.86393643538), test loss: 3.3298586309\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (15.3430185318,29.6411331812), test loss: 30.1961125374\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.67085409164,3.83161007106), test loss: 3.20196844041\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (10.6592817307,29.3676617424), test loss: 35.347726202\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.30194115639,3.80017091195), test loss: 3.37009230256\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (4.10016441345,29.1016732831), test loss: 30.2527719021\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.733918070793,3.76970527933), test loss: 3.39942963123\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (8.15649795532,28.8388464572), test loss: 30.8981519699\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.36322784424,3.7401284583), test loss: 2.84290273041\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (15.0754890442,28.5811334541), test loss: 31.3622404337\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.62805080414,3.7110805182), test loss: 3.47991900444\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (11.8908138275,28.3309129853), test loss: 31.6435201645\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (5.41766643524,3.68241653856), test loss: 2.57452872694\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.8178844452,28.0840108495), test loss: 33.6804851532\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.85719335079,3.65444473257), test loss: 3.41913734972\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (5.55805015564,27.8430548415), test loss: 29.3236789227\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.21068620682,3.62745674278), test loss: 2.39911011755\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.2306442261,27.6065114797), test loss: 34.4025713921\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (4.27720928192,3.60132114426), test loss: 3.26849464476\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (10.2327508926,27.3758511284), test loss: 35.1892187119\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.09833693504,3.5756912985), test loss: 2.79627734423\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (9.92703533173,27.146071578), test loss: 35.2534649134\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.555687010288,3.5504107131), test loss: 3.30883889496\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (16.9228210449,26.9238510768), test loss: 28.3202679634\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.48074328899,3.52563496722), test loss: 2.74434956312\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (24.6527767181,26.7052839145), test loss: 34.1683612347\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.97446966171,3.50137532825), test loss: 3.21126219332\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (14.0321235657,26.490140757), test loss: 30.1688240051\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.58374357224,3.47772437095), test loss: 3.13811922669\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (9.17151927948,26.2780297213), test loss: 34.8631037712\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.70893788338,3.45474366567), test loss: 3.33603977561\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (8.13704872131,26.0714576058), test loss: 29.1417477608\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.4927508533,3.4321939612), test loss: 3.26279346347\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (4.85750341415,25.8662973177), test loss: 34.324341774\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.589547932148,3.41010829221), test loss: 2.94421525002\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (7.66372299194,25.6669063191), test loss: 30.6314079762\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.5091509819,3.38849814788), test loss: 3.31321018338\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.19319534302,25.472746405), test loss: 33.0146373749\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.27856111526,3.36701104935), test loss: 2.65972789228\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (14.8929405212,25.2785785576), test loss: 32.040892005\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.881774306297,3.34606769076), test loss: 3.40526764691\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (15.247013092,25.0876386952), test loss: 32.1304893017\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.9203234911,3.32558706272), test loss: 2.48445101976\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (8.41103744507,24.9005116358), test loss: 34.5827557087\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.213940382,3.30553280164), test loss: 3.25358005166\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (9.8781337738,24.7173152873), test loss: 33.6029635429\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.1841211319,3.28599485021), test loss: 2.65192022622\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (6.15919303894,24.5377118889), test loss: 35.7283925533\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.9322681427,3.26677153221), test loss: 3.27956884801\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (16.5491771698,24.3617101243), test loss: 28.4365759373\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.00384759903,3.24779386778), test loss: 2.6933714211\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (6.33259963989,24.186138844), test loss: 36.1011567593\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.15287065506,3.22903334458), test loss: 3.12259531319\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (8.84687900543,24.0131677174), test loss: 25.6014316082\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.972612380981,3.21057387097), test loss: 2.56565904468\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (14.9192609787,23.8436774595), test loss: 35.5296002626\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.63974547386,3.19271370901), test loss: 3.28034728318\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (9.76807022095,23.6786909951), test loss: 29.1545812607\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.24989938736,3.17518830915), test loss: 3.19416861534\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (5.09658098221,23.5152257142), test loss: 38.8923438072\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.81965851784,3.15799639053), test loss: 3.4112278223\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (3.00573539734,23.3546993278), test loss: 30.9937175989\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.82545924187,3.14097738659), test loss: 3.29581838548\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (20.3186683655,23.196275657), test loss: 34.1222180843\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.30754137039,3.12406255171), test loss: 2.61942318976\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (13.5562782288,23.0386315498), test loss: 32.0299621105\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (3.51780390739,3.10746740772), test loss: 3.36080006063\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (10.446554184,22.8843598831), test loss: 33.7977944851\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.12216925621,3.09123223307), test loss: 2.50914470553\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (7.48681974411,22.7335823061), test loss: 36.3407730579\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.721428990364,3.07537334878), test loss: 3.29609138072\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (22.9329547882,22.5852424473), test loss: 36.2600450516\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.55632734299,3.05988405589), test loss: 2.76477657259\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (15.435459137,22.438200907), test loss: 36.8126058102\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.80572676659,3.0445474195), test loss: 3.2609303683\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (9.995262146,22.2932352435), test loss: 29.4655834675\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.31781554222,3.02927796328), test loss: 2.67586637437\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (6.88777351379,22.1498390066), test loss: 35.835492754\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (4.78561925888,3.01420833007), test loss: 3.15830406249\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.9862594604,22.0088304107), test loss: 26.4049645901\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.00036478043,2.99939474779), test loss: 2.47379528284\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (7.08770179749,21.8705566216), test loss: 36.5869985104\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.895728290081,2.98502930591), test loss: 3.2842936635\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (4.76415348053,21.7347828481), test loss: 30.0142561436\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.79183328152,2.97089243991), test loss: 3.15226884931\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (4.55903482437,21.5990946803), test loss: 41.0118037224\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.08241343498,2.95693334368), test loss: 3.47200129032\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (5.86048316956,21.4662532744), test loss: 31.3941303253\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.28891134262,2.94302782051), test loss: 3.31861671209\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (9.76760578156,21.3356375508), test loss: 34.9305649996\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.09477233887,2.9293370247), test loss: 2.72877783775\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (10.2066631317,21.2061998188), test loss: 33.3390926361\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.895681977272,2.91583791227), test loss: 3.36613029838\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (12.9677343369,21.078769287), test loss: 34.7391277313\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.33512866497,2.9026626755), test loss: 2.47111992538\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (8.0260219574,20.9527578869), test loss: 38.0490133286\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.68905568123,2.88964631069), test loss: 3.2647529453\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (6.00372648239,20.8282708021), test loss: 35.6452476501\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.29171252251,2.87693183598), test loss: 2.78486228287\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (10.9066085815,20.7059476513), test loss: 38.168313694\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (3.03220796585,2.86431645141), test loss: 3.33894459009\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (5.06595611572,20.5856822537), test loss: 30.0407488823\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.701108515263,2.85168911522), test loss: 2.6664467752\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (3.00348854065,20.4667401506), test loss: 37.8289417267\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.301799118519,2.8393474526), test loss: 3.17430091798\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (7.76862573624,20.3485412544), test loss: 27.148354435\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.92171132565,2.82717764809), test loss: 2.46315917373\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (5.2462682724,20.2318775597), test loss: 38.4094528198\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.46277809143,2.81525251565), test loss: 3.22823627144\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (7.75653648376,20.117339477), test loss: 31.1899598122\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.01437902451,2.8035019651), test loss: 3.06466635764\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (3.30576848984,20.0038370075), test loss: 38.5902859211\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.93714809418,2.79189521917), test loss: 3.34835520387\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (2.27562785149,19.8927371311), test loss: 31.4303331852\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.07434511185,2.78026063054), test loss: 3.28926581442\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (5.61370563507,19.7828306078), test loss: 35.362618494\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.10502099991,2.7688704606), test loss: 2.91431919932\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (8.55679416656,19.6732332056), test loss: 33.4791497946\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.03229188919,2.75759137371), test loss: 3.30038368404\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (10.5363750458,19.5648836786), test loss: 36.9713133812\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.780628979206,2.74646832118), test loss: 2.44314061552\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (3.62181210518,19.4583743281), test loss: 38.2640582085\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.663375020027,2.73547914353), test loss: 3.29631882608\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.35758781433,19.3534248828), test loss: 35.470331192\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.037971735,2.72478287457), test loss: 2.6951748848\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (14.1879281998,19.25040282), test loss: 38.8486241579\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.771844029427,2.71412305481), test loss: 3.30602808297\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (7.34007644653,19.1484230227), test loss: 30.6771837234\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.37672591209,2.70346681534), test loss: 2.65909339786\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (8.87029838562,19.046304236), test loss: 39.3278893709\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.32468676567,2.69299714277), test loss: 3.25940901041\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (3.13507223129,18.9456093485), test loss: 28.5036270618\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.551036059856,2.68263790509), test loss: 2.5920737654\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.55768680573,18.84665973), test loss: 39.1301415443\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (3.46257400513,2.67249635477), test loss: 3.21326949\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (17.6501903534,18.7494022402), test loss: 34.8614919186\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.52691960335,2.66254078395), test loss: 3.20055932105\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (6.98361539841,18.6531206949), test loss: 38.4256093025\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.733615040779,2.65263660431), test loss: 3.33557656854\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (5.18055534363,18.5575771412), test loss: 30.2234119415\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.671568274498,2.64272975392), test loss: 3.05486032069\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (7.09919548035,18.4627583558), test loss: 36.3014415264\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.94520187378,2.63302541084), test loss: 2.91404711902\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (5.7829208374,18.369147843), test loss: 36.112416172\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.54979133606,2.62333005545), test loss: 3.34531833977\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (3.2699136734,18.2766321276), test loss: 36.229466629\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.924471139908,2.61391379852), test loss: 2.67706240714\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (4.10795307159,18.1858724048), test loss: 38.176674509\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.364476919174,2.60457231951), test loss: 3.33842751384\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (3.76100754738,18.0957026347), test loss: 34.185016489\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.434795826674,2.5954028007), test loss: 2.59599032998\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (4.8416724205,18.0064067818), test loss: 38.8781684399\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.914854168892,2.5862419049), test loss: 3.29906885922\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (10.7923364639,17.9184462203), test loss: 31.1639907837\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.62570738792,2.57711003483), test loss: 2.67434759438\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (10.9440250397,17.8308030538), test loss: 41.480442977\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.828353881836,2.56812767359), test loss: 3.33522860557\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.5982208252,17.7443963247), test loss: 31.4213703632\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.50304794312,2.55927080467), test loss: 2.61206910908\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (6.14864492416,17.6592577367), test loss: 39.594055891\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.62116682529,2.55051127899), test loss: 3.21321927309\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (6.03129673004,17.5749288082), test loss: 31.647419405\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.36158072948,2.54195764566), test loss: 2.98955917954\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.40715360641,17.4912443228), test loss: 38.6940089703\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.795293986797,2.5334116906), test loss: 3.3618580848\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (3.75252771378,17.4086294134), test loss: 31.2403765202\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.0675880909,2.52488147864), test loss: 3.15766193867\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (3.47803544998,17.3269348221), test loss: 36.1277916908\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.12139034271,2.5164885836), test loss: 2.94096550941\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (5.19149494171,17.2459011065), test loss: 33.0966537476\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.39820146561,2.50812777362), test loss: 3.28643441796\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (5.21337080002,17.165961623), test loss: 37.9600040913\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.27700710297,2.49997798377), test loss: 2.69228210747\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (7.85788393021,17.0867435727), test loss: 36.7240999699\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.23914754391,2.49189270566), test loss: 3.34217615724\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.49282073975,17.0080712147), test loss: 35.6721292019\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.46054160595,2.4839337394), test loss: 2.59577164203\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (1.76611614227,16.9307099268), test loss: 38.7675323963\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.53135621548,2.47594815116), test loss: 3.23875041902\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (8.8760061264,16.8543877448), test loss: 32.9401724815\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.879642426968,2.46804504149), test loss: 2.74462811351\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (6.46224117279,16.7783269058), test loss: 40.2729682446\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (4.57569313049,2.4602233014), test loss: 3.25752547383\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (6.38661909103,16.7029403626), test loss: 33.5643181324\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.444146871567,2.45248319367), test loss: 2.79912791252\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (10.8816738129,16.6282498161), test loss: 40.9511561632\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.757449388504,2.44484761012), test loss: 3.22292459011\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (9.82736587524,16.5545489506), test loss: 33.6017103672\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.09432435036,2.43736863809), test loss: 3.11791518927\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (6.03385162354,16.4817595112), test loss: 39.2986395836\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.846009135246,2.42991215742), test loss: 3.30677936375\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (3.89659881592,16.4098636088), test loss: 30.7335731983\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.3816857338,2.42243254387), test loss: 3.09043873847\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (7.96502017975,16.3385222709), test loss: 36.8659204006\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.7050075531,2.41505182636), test loss: 2.9901819706\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (7.11490821838,16.2672551681), test loss: 33.1487629414\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.67117893696,2.40771816935), test loss: 3.22314893603\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (5.56220054626,16.1969059733), test loss: 37.9562564373\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.5944275856,2.40053771217), test loss: 2.7533844471\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.56662321091,16.1276322754), test loss: 33.6307104826\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.560774683952,2.39342721287), test loss: 3.28931493163\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (4.01854562759,16.058925159), test loss: 37.3127779961\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.460568130016,2.38641586128), test loss: 2.7048535645\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (6.21378517151,15.9912401217), test loss: 38.9488714695\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.82914209366,2.3793599333), test loss: 3.23691179752\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (7.68776893616,15.9240674903), test loss: 35.8222457886\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.1666996479,2.37240552819), test loss: 2.74120399505\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (5.01409435272,15.8567879836), test loss: 40.5043542862\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.314209163189,2.3654597733), test loss: 3.30206478536\n",
      "\n",
      "MC # 2, Hype # hyp5, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold10/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (317.909362793,inf), test loss: 167.240895081\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (310.756530762,inf), test loss: 374.665864563\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (41.0241088867,62.3588274994), test loss: 47.6913712502\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.7847738266,78.6804188882), test loss: 3.34221080542\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (38.0826187134,54.0968555117), test loss: 34.8919039488\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.40967178345,41.0620580626), test loss: 2.83711479455\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (32.2377204895,51.2945644353), test loss: 46.1768464565\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.46504473686,28.5075681827), test loss: 3.36723260283\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (22.6414527893,49.9208749511), test loss: 38.057433176\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.05506706238,22.2280223744), test loss: 3.47161354423\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (52.8780593872,48.9744541103), test loss: 46.4380066395\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.94256639481,18.4666919355), test loss: 3.38066232204\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (34.3646621704,48.2585936488), test loss: 42.1014603615\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.90527844429,15.9601236629), test loss: 3.48536135703\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (13.6742982864,47.717149103), test loss: 42.4269216299\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.89170455933,14.1687960324), test loss: 2.78996914327\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (30.6825561523,47.195993126), test loss: 44.5196608543\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.16638457775,12.8244020408), test loss: 3.55975946784\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (38.5259399414,46.7938843216), test loss: 41.1918039799\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (6.03574752808,11.7792651334), test loss: 2.7951457262\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (50.691696167,46.391451146), test loss: 43.5477304459\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.2059340477,10.9397296694), test loss: 3.7753963232\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (39.4012794495,46.0610248954), test loss: 36.8560730696\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (5.07410430908,10.2578790988), test loss: 2.86888992786\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (29.7246284485,45.7135088674), test loss: 45.0338160515\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.22540068626,9.68896071678), test loss: 3.517881459\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (80.8025970459,45.3863519718), test loss: 35.1855212212\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (7.79827022552,9.20622360761), test loss: 3.13789018989\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (41.148475647,45.0386570921), test loss: 43.7768045902\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.68161523342,8.79301465765), test loss: 3.24229083955\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (39.884853363,44.6771320079), test loss: 31.332098341\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.29987335205,8.43419345445), test loss: 2.76725360155\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (13.1595449448,44.3284581844), test loss: 42.1214112043\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.30761241913,8.11802505508), test loss: 3.26692127585\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (19.9193153381,43.9963959965), test loss: 33.8373644829\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.83658576012,7.83972897856), test loss: 3.38052451909\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (52.4137954712,43.6409408763), test loss: 41.4614706993\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.99024915695,7.59325352303), test loss: 3.30053356886\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (44.9307174683,43.2692964923), test loss: 36.9513364315\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.47200870514,7.3722767296), test loss: 3.38209067881\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (23.1080989838,42.890319199), test loss: 36.4012111187\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (5.30014705658,7.17163512965), test loss: 2.74507426023\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (23.303609848,42.4755645052), test loss: 38.9025877953\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.0160857439,6.98875655182), test loss: 3.46242126524\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (23.4167098999,42.0657645146), test loss: 34.6203586102\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.48991060257,6.82196486281), test loss: 2.73566855192\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (30.3665180206,41.6123149864), test loss: 37.2635709763\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (5.45131540298,6.66827576507), test loss: 3.69243611693\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (13.7316246033,41.1577881645), test loss: 30.2868805408\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.38448023796,6.52801109562), test loss: 2.80781796277\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (6.30240297318,40.6795774286), test loss: 37.537972641\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.07339668274,6.39714304793), test loss: 3.40830366611\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (39.5724868774,40.1986755599), test loss: 28.4316202641\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (4.95588827133,6.27377915772), test loss: 2.97536370754\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (37.3815994263,39.6974203848), test loss: 36.0196633101\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.53340387344,6.15802280168), test loss: 3.12162263989\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (22.3073806763,39.185595322), test loss: 25.1547784567\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.78946375847,6.04829349921), test loss: 2.67362745404\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (6.28428936005,38.6813338035), test loss: 35.5234528065\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.16211080551,5.94326193063), test loss: 3.14850270152\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (19.1862468719,38.1814138147), test loss: 27.8588161945\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.13449764252,5.84371638516), test loss: 3.33356266022\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (16.6882381439,37.6763759665), test loss: 34.2727725863\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.8068022728,5.74950034498), test loss: 3.20544215888\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (34.1923904419,37.1810698181), test loss: 29.3991410017\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.1758890152,5.65984927863), test loss: 3.2866500169\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (26.528793335,36.6979646392), test loss: 29.8779745817\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.75464820862,5.57368623056), test loss: 2.54244461358\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (26.52044487,36.2132085892), test loss: 31.9135394096\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.00423169136,5.49090148749), test loss: 3.35086768866\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (11.8327236176,35.7485219309), test loss: 30.2273738384\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.26305961609,5.41164283651), test loss: 2.52232500613\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (35.3340797424,35.2934146667), test loss: 32.4662586451\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (5.09718322754,5.33501992593), test loss: 3.45142039955\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (12.0403709412,34.8510964255), test loss: 28.9484627724\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.40422439575,5.26262618714), test loss: 2.69690938294\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (3.66653966904,34.4191793764), test loss: 34.6582788706\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.47276473045,5.19329020551), test loss: 3.26050464809\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (12.2532262802,34.0063137095), test loss: 27.138854599\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.92621779442,5.12670607402), test loss: 2.77731886208\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (18.906036377,33.6017974634), test loss: 33.8300414801\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.921918451786,5.06298671997), test loss: 3.0642834112\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (9.70997047424,33.2063762457), test loss: 24.9139990807\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.55310034752,5.00148175984), test loss: 2.59970113635\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (18.7480583191,32.8291994992), test loss: 35.001717627\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.05509996414,4.94200529564), test loss: 3.19746789336\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (21.0181922913,32.4619182591), test loss: 27.6998490095\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.25316905975,4.8845280258), test loss: 3.43044584692\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (9.97173118591,32.1001877926), test loss: 34.7971075296\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.20127749443,4.82971030392), test loss: 3.24200130403\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (13.9469308853,31.7525383814), test loss: 28.9807520628\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.19073987007,4.77730043431), test loss: 3.32958076596\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (10.5113554001,31.4173058179), test loss: 30.715486908\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.22583806515,4.726593443), test loss: 2.49646113366\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (42.2542457581,31.0852873879), test loss: 31.4164980888\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.97195851803,4.67729665835), test loss: 3.29450209439\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (5.068816185,30.7647457544), test loss: 30.8095026493\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.51215553284,4.62952338415), test loss: 2.48189926744\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (31.8786334991,30.4528767228), test loss: 32.9423661947\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.76770114899,4.58299164052), test loss: 3.32146914899\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (9.25037956238,30.146701556), test loss: 30.0488426685\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.973017930984,4.53840688725), test loss: 2.68806932718\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (13.8505516052,29.848002618), test loss: 35.3613092542\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.58564829826,4.49536966325), test loss: 3.19124239683\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (5.23862934113,29.5602717321), test loss: 27.6799453259\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.76267004013,4.45365613149), test loss: 2.70084006786\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (16.1143035889,29.2770901596), test loss: 33.962532711\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.33509260416,4.41326069659), test loss: 3.02036935389\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.5234680176,28.9989858495), test loss: 25.4745397091\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.68230974674,4.37377757152), test loss: 2.55284736156\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (23.6301403046,28.7300077279), test loss: 34.6229192734\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (5.73916959763,4.33520206317), test loss: 3.16841937006\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (4.51100540161,28.4654240222), test loss: 27.2167297006\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.479777336121,4.29741384638), test loss: 3.37223379016\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (18.8902206421,28.2038436938), test loss: 34.5680212855\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.5810418129,4.26103357142), test loss: 3.2267069608\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (4.83841466904,27.9501861517), test loss: 28.6971057415\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.02116298676,4.22609733231), test loss: 3.28883144557\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (6.50399398804,27.7042508479), test loss: 31.5436892986\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.864355802536,4.19189473955), test loss: 2.46043447554\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (27.2928371429,27.4597917899), test loss: 31.0021650553\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.920201301575,4.15839808861), test loss: 3.23438473344\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (5.44503641129,27.2217997833), test loss: 31.6994271755\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.95964241028,4.12570926536), test loss: 2.43174553514\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (27.0292053223,26.9883262884), test loss: 34.1515978336\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (5.65310621262,4.09346816646), test loss: 3.1931881994\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.0836648941,26.7574061235), test loss: 30.8111358643\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.889787077904,4.06219649251), test loss: 2.61249815524\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (16.4130592346,26.5308190478), test loss: 35.574144125\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.78758192062,4.03186740493), test loss: 3.10633286238\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (4.94760608673,26.31041758), test loss: 28.0301259518\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.09654140472,4.00229004542), test loss: 2.62708691359\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (14.2912425995,26.093401104), test loss: 34.6442574739\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.474428236485,3.97344300312), test loss: 2.93506944329\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (14.6076812744,25.8798073045), test loss: 25.8614720345\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.24160194397,3.94508077502), test loss: 2.49479270577\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (6.78189086914,25.6715184988), test loss: 34.8591549635\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.21733760834,3.91720836495), test loss: 3.09078689516\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (3.71013212204,25.4659569365), test loss: 26.9226538301\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (0.32504221797,3.88965688666), test loss: 3.28525303006\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (21.5221538544,25.262492123), test loss: 35.0521527529\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.32809138298,3.86300602954), test loss: 3.16448707134\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (3.48599934578,25.0641444827), test loss: 28.4715547562\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.52593779564,3.83731112924), test loss: 3.21262037158\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (3.57518720627,24.8715178912), test loss: 32.7646624088\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.859744906425,3.81205799904), test loss: 2.47131738663\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (6.74125480652,24.6802763271), test loss: 31.1739326239\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.36418247223,3.78727194874), test loss: 3.16693470329\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (9.69653511047,24.4938490267), test loss: 32.7739497423\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (4.76093864441,3.76289615532), test loss: 2.39328744113\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (5.55208730698,24.3102711086), test loss: 34.815860343\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.44043946266,3.73880226977), test loss: 3.08505144119\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (30.367275238,24.1293121573), test loss: 31.8286057472\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.59228527546,3.71530261537), test loss: 2.56325573027\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (17.4465789795,23.9503319555), test loss: 35.9699460506\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.36708283424,3.69242599208), test loss: 3.07597664595\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (7.76474761963,23.7764700227), test loss: 28.582007885\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.34715747833,3.67005878711), test loss: 2.59306572676\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (10.3142967224,23.6050507948), test loss: 35.7851762772\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.613451838493,3.64818619939), test loss: 2.83972399086\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (22.0173950195,23.436404369), test loss: 26.6412790298\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.89825463295,3.62660343289), test loss: 2.45678192973\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (9.76046276093,23.2709142613), test loss: 35.3831539869\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.33367729187,3.60528010725), test loss: 3.02517907023\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (6.31738138199,23.1074450203), test loss: 27.0295083642\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.41374242306,3.5841972418), test loss: 3.19678917229\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (16.4701766968,22.9455022128), test loss: 35.8276241183\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.38828086853,3.563706121), test loss: 3.10278815925\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (6.34882593155,22.7868909424), test loss: 28.3870029807\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.18386387825,3.54388219983), test loss: 3.14830755591\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (5.89319515228,22.6322065763), test loss: 34.0280090809\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.957613348961,3.52433287851), test loss: 2.49263970554\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.29779434204,22.4780893567), test loss: 31.3018739462\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.22066307068,3.50509281488), test loss: 3.11404024512\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (7.63018226624,22.3273065796), test loss: 33.8881088495\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (4.89091491699,3.48603076916), test loss: 2.36857712567\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (4.30445146561,22.1782222297), test loss: 36.6924487352\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.68460154533,3.46714899961), test loss: 3.01108320951\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (23.6993484497,22.0307010555), test loss: 32.6999250412\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.87727236748,3.44862769109), test loss: 2.5524689436\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (5.94200992584,21.8845355278), test loss: 37.6610454082\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.21082496643,3.43055765021), test loss: 3.09160369039\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (6.86567401886,21.7417262383), test loss: 29.2171082973\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.86723899841,3.41283387745), test loss: 2.58599829525\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (6.3950715065,21.6007901273), test loss: 36.5188892722\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.824976623058,3.39543764429), test loss: 2.76056670845\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (14.5746564865,21.4622406415), test loss: 27.3349668503\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.45018267632,3.37825570715), test loss: 2.442336303\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (7.80457830429,21.3253727464), test loss: 36.2099189281\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.85739040375,3.36121385577), test loss: 2.95299628377\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (13.0703325272,21.1899033265), test loss: 27.407366395\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.76098394394,3.34432362222), test loss: 3.14622918367\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (11.3528366089,21.0554417439), test loss: 36.6509854794\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.97350955009,3.32784631896), test loss: 3.06270385981\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (6.60202121735,20.9233385104), test loss: 28.7197519064\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.10191130638,3.31182250588), test loss: 3.12529319823\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (3.87254095078,20.7938211447), test loss: 34.6613945723\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.5770483017,3.2959835491), test loss: 2.49724934399\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (4.43446397781,20.6648598111), test loss: 31.7803970337\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (3.56187844276,3.2803971213), test loss: 3.07481058091\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (5.96852874756,20.5383372632), test loss: 34.6397148132\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.35629868507,3.26484011864), test loss: 2.34667008817\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (5.39594841003,20.4130431555), test loss: 37.5033891439\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.960973382,3.24947126617), test loss: 2.99339427948\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (13.8849401474,20.2889344906), test loss: 33.3845393658\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.20973944664,3.23430940329), test loss: 2.53994597942\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (5.36741352081,20.1661022071), test loss: 38.8551266193\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.7839499712,3.21953609455), test loss: 3.10945649445\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (6.00107097626,20.045853925), test loss: 29.7538958073\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.43607378006,3.20498874428), test loss: 2.55484496504\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.62674093246,19.9272484394), test loss: 37.1474261522\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.16073489189,3.19075264354), test loss: 2.69836132079\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (10.9908084869,19.8104601727), test loss: 27.944333601\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.07669138908,3.1766484847), test loss: 2.41404146403\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (7.60072469711,19.6950987854), test loss: 36.8003467321\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.12570118904,3.16260628291), test loss: 2.8865497008\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (12.7400522232,19.5809856866), test loss: 27.8028367996\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.23516654968,3.14870160126), test loss: 3.11935456693\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (15.7682170868,19.4679205235), test loss: 37.4210261703\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.26667618752,3.13508350559), test loss: 3.04153864682\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (5.12825345993,19.3565866204), test loss: 29.3032753944\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.12495446205,3.12183553928), test loss: 3.08780637383\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (2.35151100159,19.247598155), test loss: 35.0162498474\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.22469472885,3.10869984688), test loss: 2.47751551867\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (2.91684865952,19.1391596746), test loss: 32.0068223238\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (3.71768164635,3.09581268365), test loss: 3.04024343193\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (2.66485643387,19.0327290143), test loss: 35.176156354\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.7109991312,3.08289144158), test loss: 2.33534513414\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.69775295258,18.9274232728), test loss: 37.2431022406\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.70525932312,3.07010773082), test loss: 2.9844832629\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (6.35188436508,18.8229727824), test loss: 34.4590993881\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.13109147549,3.05750742493), test loss: 2.54365104437\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (4.20380163193,18.7195451907), test loss: 39.3018784046\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.49393844604,3.0452217691), test loss: 3.09233044088\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.9880900383,18.6180615937), test loss: 30.3655708313\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.27152991295,3.03309240058), test loss: 2.54081151485\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (7.22571611404,18.5177602478), test loss: 37.7007779837\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.47614753246,3.02121812127), test loss: 2.65732929558\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.54230165482,18.4186338255), test loss: 28.4877975464\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.940729796886,3.00938200699), test loss: 2.40556883365\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (6.41382598877,18.320529022), test loss: 36.994974494\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.20971429348,2.99757831317), test loss: 2.84377067685\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (8.24494552612,18.2232524023), test loss: 28.1208603382\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (4.20796918869,2.98593893682), test loss: 3.07322436571\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (12.8224096298,18.1266311519), test loss: 37.8392249584\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.57471489906,2.97440039354), test loss: 3.00514947474\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (3.76083755493,18.0312082807), test loss: 29.9667475939\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.06164717674,2.96318420902), test loss: 3.06688403487\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (4.96716308594,17.9374662519), test loss: 35.6226037979\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.04244792461,2.95204708213), test loss: 2.50153661817\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (3.30977702141,17.8440863837), test loss: 32.1754962921\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.88022375107,2.94113479956), test loss: 3.02340049744\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (2.31346869469,17.7523418795), test loss: 35.3497589111\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.90999889374,2.93017472014), test loss: 2.31767838895\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (11.9878253937,17.6615470178), test loss: 37.1004375219\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.45014047623,2.91930515828), test loss: 2.981595245\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (4.75354146957,17.5711330964), test loss: 34.7278466702\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.909875810146,2.90857892454), test loss: 2.51817860305\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (5.99302577972,17.4817320694), test loss: 39.1213392019\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.567232370377,2.89809455819), test loss: 3.08798656166\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.56341743469,17.3936326533), test loss: 31.0088699341\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.777026534081,2.88771141056), test loss: 2.53605369329\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (8.51181411743,17.306473739), test loss: 38.6554551363\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.48313295841,2.87754529624), test loss: 2.68744914234\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (3.17900109291,17.2201438875), test loss: 28.9172745228\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.51927280426,2.8673674063), test loss: 2.40100876242\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (4.12640762329,17.1346313223), test loss: 37.2182398081\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.03099417686,2.85721478281), test loss: 2.81470327005\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (4.8188328743,17.0498501059), test loss: 28.5302185535\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (4.41951036453,2.84720453878), test loss: 2.98942917883\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (9.20847702026,16.9655793963), test loss: 38.2029446363\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.925134539604,2.83723710033), test loss: 2.98505663872\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (2.31688070297,16.8824775547), test loss: 30.5050675392\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.01374089718,2.82755807965), test loss: 3.0494816497\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.30151891708,16.8008056635), test loss: 35.9579166889\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.19780790806,2.81795132814), test loss: 2.50221190006\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (5.2750787735,16.7195328182), test loss: 32.4400801182\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.32921648026,2.80855114912), test loss: 3.01175216436\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (3.22388696671,16.6395316933), test loss: 35.7331336021\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.44408679008,2.7990834841), test loss: 2.31217636168\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.55714941025,16.5603726676), test loss: 37.4340206146\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.765483379364,2.78967963093), test loss: 2.98774900436\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (4.89433336258,16.4816560511), test loss: 34.9179442883\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (4.83168125153,2.7804519994), test loss: 2.48759156018\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.57675361633,16.4037443431), test loss: 38.4985071778\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.678105473518,2.77130676485), test loss: 3.06393252015\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.67223024368,16.3270465113), test loss: 31.4045180798\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.53894507885,2.76229859206), test loss: 2.52346235812\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (4.07425880432,16.2513110675), test loss: 38.7487706184\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.60322070122,2.75350930124), test loss: 2.69182718992\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (2.1756541729,16.1762789215), test loss: 29.255644536\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.851551532745,2.74468738833), test loss: 2.39825931266\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.33022451401,16.102075199), test loss: 37.291524744\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.963435769081,2.73587525715), test loss: 2.79506878331\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (5.17454862595,16.028524364), test loss: 29.2648793221\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.85917520523,2.72721143775), test loss: 2.91664011478\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (3.54250073433,15.9553821205), test loss: 38.3353456736\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.700276732445,2.71856351634), test loss: 2.94566567242\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (2.47530460358,15.8832252173), test loss: 30.7975955725\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.46182489395,2.71018600068), test loss: 3.02241228223\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (4.61157178879,15.8122414706), test loss: 36.5273502588\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.40120601654,2.70184437217), test loss: 2.48383230418\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (6.98339176178,15.741499181), test loss: 32.5627830505\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.87260782719,2.69367580841), test loss: 2.99311480224\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (3.50399112701,15.6717197564), test loss: 36.2054635048\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.58666110039,2.68542496557), test loss: 2.3153965801\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.06702947617,15.6025981435), test loss: 37.7461830735\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.14318275452,2.67720881217), test loss: 3.04286286831\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (6.94494438171,15.5337413507), test loss: 34.9182833672\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (5.00110626221,2.66914038604), test loss: 2.52333774939\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.82964324951,15.4654250672), test loss: 39.1112563014\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.793723762035,2.6611299502), test loss: 3.09119692147\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (4.26829481125,15.3979527784), test loss: 31.6902276993\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.609397530556,2.65322832304), test loss: 2.5299583137\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.24955177307,15.33114808), test loss: 38.992803669\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.66784095764,2.64551203896), test loss: 2.67904685438\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (3.2352347374,15.2649339978), test loss: 29.8775529385\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.4836564064,2.63775729412), test loss: 2.41355197132\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (5.20381641388,15.1993836773), test loss: 37.4900138378\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.734499871731,2.63001195007), test loss: 2.78729139268\n",
      "run time for single CV loop: 7092.73421097\n",
      "\n",
      "MC # 2, Hype # hyp4, Fold # 6\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold6/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (338.877075195,inf), test loss: 186.159300995\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (311.927490234,inf), test loss: 381.099449158\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (48.1746025085,126.969295214), test loss: 51.5347848892\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.75967478752,110.148338076), test loss: 3.66176593155\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (52.7894096375,87.0263013124), test loss: 42.9089101791\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.52683877945,56.7370192057), test loss: 3.29042612016\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (12.026966095,73.275557525), test loss: 48.647812748\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (5.20177125931,38.9081777359), test loss: 3.69291591644\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (19.9524002075,66.5154769011), test loss: 43.3618143082\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.98514008522,29.9895344802), test loss: 3.51301946044\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (26.305727005,62.3875933922), test loss: 46.956572628\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.994361042976,24.6395030095), test loss: 3.10882190764\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (43.3703231812,59.6204332991), test loss: 47.4532477379\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.10268700123,21.073895908), test loss: 3.6445751369\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (93.1436080933,57.6496130857), test loss: 46.2118009567\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.04154586792,18.5215936564), test loss: 2.77545034885\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (38.0135154724,56.0561101584), test loss: 47.9405888557\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (5.66586208344,16.6047573614), test loss: 3.81835687459\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (20.217376709,54.9021061977), test loss: 45.1279859543\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.09473061562,15.1140223408), test loss: 2.83560741246\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (31.4894599915,53.906692383), test loss: 47.0501994133\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.0292494297,13.9169084015), test loss: 3.78219535351\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (35.4296226501,53.1318706525), test loss: 42.1512925148\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (6.9715795517,12.9403159149), test loss: 2.83732919693\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (35.6559181213,52.4385044143), test loss: 48.8055578232\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.09943652153,12.1257504928), test loss: 3.55172821581\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (33.3121032715,51.8407056421), test loss: 40.7940390587\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.60173153877,11.4339907491), test loss: 2.93213051558\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (53.8556137085,51.3088556464), test loss: 47.8896012306\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.67466831207,10.8415045695), test loss: 3.40528844595\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (63.5197296143,50.8300442731), test loss: 37.3790242672\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.33470237255,10.3262365696), test loss: 2.91087062061\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (16.360748291,50.4006109782), test loss: 46.9909937859\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.82013702393,9.87350363774), test loss: 3.48874833584\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (26.8771018982,50.0366538624), test loss: 42.0598725319\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.00887227058,9.47355014562), test loss: 3.38868448734\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (16.1184062958,49.6881106239), test loss: 47.6498914719\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.16516065598,9.1184898626), test loss: 3.20130282044\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (29.4706268311,49.3646767298), test loss: 45.0709770679\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.74580478668,8.80071889228), test loss: 3.57203991413\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (36.6024932861,49.0651028495), test loss: 44.0546331882\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.55872011185,8.512572553), test loss: 2.65270472765\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (47.3186187744,48.7579067244), test loss: 47.8438611984\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (6.39965820312,8.25106193096), test loss: 3.70916510224\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (87.6360473633,48.5000035078), test loss: 44.0679701328\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.64931583405,8.01280443936), test loss: 2.6816907227\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (8.76280975342,48.2291306688), test loss: 46.0361249924\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.35959219933,7.79318995976), test loss: 3.64669636488\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (40.0497512817,47.9981742325), test loss: 40.4113767624\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (7.21777868271,7.59247557985), test loss: 2.69832569957\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (18.9074745178,47.7572057937), test loss: 47.0071501732\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.23874020576,7.40714095532), test loss: 3.43494609594\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (31.5268554688,47.5256289935), test loss: 39.3029353142\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.887798547745,7.23451394199), test loss: 2.78647250533\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (37.1060791016,47.2952859202), test loss: 46.1018931389\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.41937732697,7.07465151881), test loss: 3.25742427111\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (28.8008804321,47.0650928455), test loss: 35.7605426311\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.11315727234,6.92486633726), test loss: 2.61142096519\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (35.7340698242,46.8447874833), test loss: 44.8038770199\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.26452159882,6.78385296225), test loss: 3.35876973271\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (35.6326370239,46.6360614477), test loss: 39.4417728424\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.01956963539,6.65187977486), test loss: 3.2861010015\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (26.710855484,46.4204721595), test loss: 44.7004227638\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.51351606846,6.52806805913), test loss: 3.26383003294\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (24.4521942139,46.2049293161), test loss: 42.1313033104\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (7.07822036743,6.41155884442), test loss: 3.45899008512\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (111.053092957,45.9912494157), test loss: 41.7245334625\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (5.29903888702,6.30082321004), test loss: 2.5640206188\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (37.9331970215,45.7605349246), test loss: 44.9640861511\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.3350982666,6.19541220486), test loss: 3.61914027631\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (38.0776367188,45.5451968917), test loss: 41.3345123291\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.29243803024,6.09557344225), test loss: 2.53519285321\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (27.3002529144,45.3193800006), test loss: 42.8509191513\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.01758956909,5.99978665996), test loss: 3.46443313956\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (56.8680648804,45.1017028093), test loss: 37.0387481689\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.5145008564,5.90887470405), test loss: 2.55224047899\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (16.9187908173,44.8713580538), test loss: 43.5072285652\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.24707448483,5.82242084687), test loss: 3.30541608334\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (30.7797164917,44.6374399506), test loss: 35.9424899101\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.254277855158,5.73916246633), test loss: 2.61878180802\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (40.0594100952,44.3946649995), test loss: 42.1553487301\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.19079566002,5.65951442386), test loss: 3.13453035355\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (33.4923248291,44.1452455181), test loss: 32.1343512535\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.49686980247,5.58281379114), test loss: 2.41507388949\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (36.460018158,43.8925714577), test loss: 40.1717965126\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.99050831795,5.50850917318), test loss: 3.25975243151\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (19.8743724823,43.6379684853), test loss: 34.7931499481\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.75156462193,5.43725638846), test loss: 3.250372383\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (27.1148929596,43.3712586027), test loss: 39.4547914505\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.624806284904,5.36885884904), test loss: 3.25660814047\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (26.4251060486,43.0961562319), test loss: 37.285522747\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.142172575,5.30325963791), test loss: 3.44945779145\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (95.4177627563,42.8122167388), test loss: 35.6408064842\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (4.51801586151,5.24002465363), test loss: 2.51213618815\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (36.797454834,42.5140876318), test loss: 39.549179697\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.43002319336,5.17870057675), test loss: 3.60862242579\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (24.5790596008,42.2165849435), test loss: 34.7317258835\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.74028992653,5.11946285566), test loss: 2.45282411128\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (25.4357318878,41.9073072751), test loss: 37.6076415539\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.47994804382,5.061831682), test loss: 3.39718854427\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (51.5004272461,41.5941259033), test loss: 30.8761571884\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.31121349335,5.00622395094), test loss: 2.56586548388\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (15.4936437607,41.2707451159), test loss: 37.7328446388\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.61570692062,4.95286260357), test loss: 3.35963501334\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (21.0240516663,40.9410731927), test loss: 29.3201179981\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.78104436398,4.90102099159), test loss: 2.60372553617\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (14.5463085175,40.6028210869), test loss: 35.9839895844\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.3087631464,4.85075399405), test loss: 3.18784500659\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (10.7081222534,40.2605788898), test loss: 26.3028323174\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.09229969978,4.80200555334), test loss: 2.45136128068\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.0558776855,39.9173798062), test loss: 34.2471440792\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.80035710335,4.75426077571), test loss: 3.35873340964\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (15.5337934494,39.572316985), test loss: 29.2236761093\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.02131867409,4.7082647441), test loss: 3.40028962791\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (40.3560409546,39.2230509766), test loss: 33.4385102987\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.84623551369,4.66369707161), test loss: 3.40034707189\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.7561349869,38.8740925497), test loss: 31.4368663669\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.47201824188,4.62076284697), test loss: 3.54277429581\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (50.9417648315,38.5258983902), test loss: 29.5960619926\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.29853475094,4.57909218777), test loss: 2.60829661489\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (19.4434299469,38.1771281377), test loss: 34.3061646938\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (7.06719970703,4.53859304205), test loss: 3.66705679297\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (39.9689826965,37.8345205352), test loss: 29.4093753576\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.86340999603,4.49908799632), test loss: 2.48937924355\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (7.96554803848,37.4941161257), test loss: 34.0245776415\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.31544864178,4.46049552196), test loss: 3.41409683228\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.0153179169,37.1581740955), test loss: 28.0181571484\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.870979785919,4.42310116902), test loss: 2.65417974591\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (6.50478649139,36.8265356476), test loss: 35.5866912842\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.25496733189,4.38715654281), test loss: 3.47668999434\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (15.7210454941,36.5009079518), test loss: 26.7194410324\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.74883580208,4.3521795125), test loss: 2.68431344926\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (9.52132987976,36.1780251475), test loss: 34.8744004726\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.08419287205,4.31803811946), test loss: 3.27120470107\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (4.26635074615,35.8620240094), test loss: 24.7553563833\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.891426324844,4.28476187569), test loss: 2.50734682679\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (18.1600284576,35.5540165657), test loss: 34.3464709044\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.62886333466,4.25199426241), test loss: 3.48133241236\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (19.0515480042,35.2505661569), test loss: 28.7002673149\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.21490192413,4.22027609105), test loss: 3.4753588587\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (10.8840532303,34.9515819818), test loss: 33.809416914\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.5230101347,4.18930337514), test loss: 3.43611502349\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (8.18985748291,34.6599477574), test loss: 30.2498455524\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.1859254837,4.15944348659), test loss: 3.56309250593\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (7.58014011383,34.3736060478), test loss: 30.011669302\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.1842751503,4.13028269785), test loss: 2.58325235248\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.299741745,34.0929635596), test loss: 34.0468269348\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.6186196804,4.10161778907), test loss: 3.64504770339\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (21.71733284,33.8193182954), test loss: 30.2317930222\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.55041456223,4.07366895172), test loss: 2.48586864471\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (10.0107297897,33.5510275239), test loss: 34.645312953\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.22547078133,4.04617904836), test loss: 3.40259477198\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (6.55894374847,33.2875761993), test loss: 28.894486618\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.87111330032,4.01934726306), test loss: 2.64254289269\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (18.9587955475,33.0292188231), test loss: 36.3201708794\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.62868952751,3.99343948858), test loss: 3.47431286275\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (12.0200872421,32.7769117718), test loss: 27.5630903244\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.33506727219,3.96811746812), test loss: 2.68016374111\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (13.4056158066,32.5275593506), test loss: 36.3855121851\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.59405303001,3.94329018308), test loss: 3.2087915957\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (10.3293428421,32.2841228981), test loss: 25.5801584721\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.60939145088,3.91893856668), test loss: 2.50374261141\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (12.818195343,32.0466195789), test loss: 35.651828146\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.26077985764,3.89478145954), test loss: 3.44367677569\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (19.5480766296,31.8121184933), test loss: 29.1131265163\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.22393894196,3.87128889979), test loss: 3.48510524035\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (12.2555561066,31.5812057143), test loss: 34.7064055443\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.04420566559,3.84827261555), test loss: 3.39667887092\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (7.54418373108,31.3551436598), test loss: 30.8890637636\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.02082467079,3.82595597452), test loss: 3.53304228187\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (10.9596443176,31.1331020233), test loss: 30.8627083778\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.4581053257,3.80409986704), test loss: 2.62222313881\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (8.60432338715,30.9147978244), test loss: 35.0043930292\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.45140349865,3.78248780841), test loss: 3.59543839097\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (8.68343925476,30.701197021), test loss: 30.7471138477\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.76890194416,3.76134272317), test loss: 2.4402759701\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (11.3750648499,30.4915255494), test loss: 35.5157168865\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.38719058037,3.7404405066), test loss: 3.38306238651\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (12.9208488464,30.2846978918), test loss: 29.2670269012\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.37189650536,3.71993632831), test loss: 2.57429382205\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (19.5991153717,30.0813150822), test loss: 37.4507014275\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.05525112152,3.70009682033), test loss: 3.47334029078\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (8.12118339539,29.8827183741), test loss: 28.2921432972\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.66676831245,3.68060729999), test loss: 2.64899526238\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (7.64683628082,29.6859357165), test loss: 37.5931283712\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.658845186234,3.66145817128), test loss: 3.22473765314\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (5.18584156036,29.4934810757), test loss: 26.1140443325\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.40239286423,3.64263135104), test loss: 2.46738144457\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (29.3369865417,29.3051275379), test loss: 36.6090967894\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.52004897594,3.6238723441), test loss: 3.38047097623\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (14.3664808273,29.1185735293), test loss: 29.7731871367\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (4.62527275085,3.60555767824), test loss: 3.26178273857\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (9.44953918457,28.9348644539), test loss: 35.0915487528\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.84743106365,3.58755074337), test loss: 3.33865395486\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (10.5402097702,28.7546895501), test loss: 30.7544925451\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.831815600395,3.57005136784), test loss: 3.46493023932\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (22.4905548096,28.5774267742), test loss: 31.0436299801\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.93371152878,3.55284769463), test loss: 2.72652834654\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (8.42457485199,28.4029531354), test loss: 36.0700360656\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.08564066887,3.53580947297), test loss: 3.56259006262\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (9.71081256866,28.2315740659), test loss: 31.5605920315\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.55713534355,3.51907000873), test loss: 2.38838584721\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.32847976685,28.063035977), test loss: 37.2335628986\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (6.29825735092,3.50246770639), test loss: 3.40580309629\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (31.6743946075,27.8965636431), test loss: 28.7135932446\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.41690397263,3.48607439955), test loss: 2.43810245991\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (12.8934984207,27.7323711326), test loss: 38.4686353683\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.72790884972,3.47025799666), test loss: 3.46090410948\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (6.14405441284,27.572062311), test loss: 28.7471559048\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.40578508377,3.45465158394), test loss: 2.6098570168\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (7.3604259491,27.4127913046), test loss: 38.9396099091\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.72747635841,3.43932823649), test loss: 3.24819566011\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (4.54865074158,27.2567926883), test loss: 26.8915870667\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.991439700127,3.42411024547), test loss: 2.49298329949\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (5.14396381378,27.1036569682), test loss: 37.6320487499\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.19473218918,3.40902825813), test loss: 3.31767298281\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (7.54873752594,26.9525069215), test loss: 30.8391063929\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.55463814735,3.39417879267), test loss: 3.27748184204\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (9.98219299316,26.8032598792), test loss: 36.0686647415\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.786509394646,3.37965141724), test loss: 3.32112684548\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (13.4663076401,26.6565670561), test loss: 31.0051494598\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.888289570808,3.36547715163), test loss: 3.43421207666\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (5.95854377747,26.5118122308), test loss: 31.4873990536\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.48010277748,3.35149176787), test loss: 2.70500160754\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (14.7407855988,26.3696222672), test loss: 36.4703789711\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.02020335197,3.33764175962), test loss: 3.50552078485\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (8.355219841,26.2294586591), test loss: 32.6976924419\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (4.42226028442,3.32398528373), test loss: 2.38443167806\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (9.70841026306,26.0914865836), test loss: 38.5325274944\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (4.66859102249,3.31038786305), test loss: 3.42683291733\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (20.0086193085,25.9547646203), test loss: 29.3679252148\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.78543972969,3.2969862937), test loss: 2.3549987793\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (14.8342933655,25.820216791), test loss: 39.0203790188\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.10099768639,3.28400691853), test loss: 3.35689518899\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.26816129684,25.6883776401), test loss: 29.3384937286\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.657123148441,3.27118072553), test loss: 2.58597542942\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (4.79201126099,25.557266115), test loss: 39.4579486609\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.92877578735,3.25858288132), test loss: 3.29929751456\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (12.5254383087,25.4286835683), test loss: 27.699634552\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (5.17505311966,3.24604555216), test loss: 2.56433398724\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (3.46264410019,25.3021503009), test loss: 38.3787726879\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.21405410767,3.23358239924), test loss: 3.30867996514\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (25.5500240326,25.1769537161), test loss: 31.7066029787\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.1633002758,3.22129137429), test loss: 3.26063877344\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (9.07877445221,25.0531428787), test loss: 37.1657257557\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.79381620884,3.20926993661), test loss: 3.33218465745\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (8.13025474548,24.9314293927), test loss: 31.2069415092\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (3.7691078186,3.19751302282), test loss: 3.38926248848\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (10.3169107437,24.8115436362), test loss: 32.341341424\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.70746666193,3.18591313517), test loss: 2.6770603776\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (11.6456375122,24.6935557551), test loss: 37.0071549177\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.28484940529,3.17440075571), test loss: 3.47444096804\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (11.3457422256,24.5771246628), test loss: 33.4124763727\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (4.29952526093,3.16300224971), test loss: 2.37958056331\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (12.0167036057,24.4623090993), test loss: 39.3481872082\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.37074708939,3.15165156039), test loss: 3.46530041397\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (17.4060726166,24.3481841912), test loss: 30.1112947702\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.58858382702,3.14049141098), test loss: 2.33162183166\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (4.28778171539,24.235947155), test loss: 39.8508129597\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.764363467693,3.12964694151), test loss: 3.32774791718\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (8.57055473328,24.1260393779), test loss: 29.7645219564\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.02885079384,3.11890162658), test loss: 2.58345150799\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (12.0122966766,24.0164888449), test loss: 39.9292114496\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.97753763199,3.10831699284), test loss: 3.29337442815\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (6.10468196869,23.9087537984), test loss: 28.1625490427\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (3.58766722679,3.09777025494), test loss: 2.57512901425\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (19.0026035309,23.8028257892), test loss: 38.9700311661\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.97149014473,3.08729754562), test loss: 3.2936888963\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (7.7435464859,23.6974450391), test loss: 32.4887279034\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.138692379,3.07692599925), test loss: 3.25065599382\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (7.73376846313,23.593613986), test loss: 37.5000157833\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.25729084015,3.06678962661), test loss: 3.30662938803\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.03277778625,23.4911801669), test loss: 31.3245377779\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.88064908981,3.05682856521), test loss: 3.30104455054\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (12.1717376709,23.3900903531), test loss: 32.8224868298\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.165952950716,3.04705358217), test loss: 2.68500259221\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (6.72068500519,23.2903644749), test loss: 37.2341149569\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.39625811577,3.03729546708), test loss: 3.4589122504\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (2.23992300034,23.1918592047), test loss: 34.1787230015\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.542135119438,3.02758509416), test loss: 2.4220704183\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (8.14911079407,23.0948866295), test loss: 39.8909703732\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.350063204765,3.01800112715), test loss: 3.45845131874\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (16.5637760162,22.9983708288), test loss: 30.6655520201\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.61371433735,3.0085235112), test loss: 2.31827974617\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (3.75111484528,22.9034769718), test loss: 40.3477161884\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.01807498932,2.99932490048), test loss: 3.35313311815\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (14.293211937,22.8103533823), test loss: 30.4959167957\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.96666491032,2.99018369436), test loss: 2.59864589572\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (7.73774242401,22.7173823026), test loss: 40.184712863\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.434000849724,2.98115032066), test loss: 3.26257238984\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (6.88708686829,22.6259667388), test loss: 28.8002307415\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.987403512,2.97215235743), test loss: 2.55726062953\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (13.3488483429,22.5358883595), test loss: 39.1823014736\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.90155363083,2.96322368065), test loss: 3.20901820362\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (6.93325138092,22.4463702618), test loss: 33.011441803\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.57047867775,2.95438286275), test loss: 3.23402204216\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (16.1343383789,22.3580342715), test loss: 37.6071070194\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.45144689083,2.94569824327), test loss: 3.27845949531\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (6.33556175232,22.2707067519), test loss: 31.355774498\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.709195494652,2.93713562454), test loss: 3.24568077028\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.90509653091,22.1844998448), test loss: 33.2409986019\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.38763308525,2.92878996883), test loss: 2.67663007975\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (12.1766853333,22.0994476772), test loss: 37.5996142387\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.876490950584,2.92039878126), test loss: 3.43075704575\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (2.52423667908,22.015281879), test loss: 34.7524199486\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.67750197649,2.912046686), test loss: 2.50039128363\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (7.46331214905,21.9321760835), test loss: 40.0265616417\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.615054786205,2.90379982755), test loss: 3.44296567738\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.71154022217,21.8492409808), test loss: 31.112552762\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.36052179337,2.89561851852), test loss: 2.2972145617\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (5.53079938889,21.7678431844), test loss: 40.6439726114\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.88304042816,2.88767635892), test loss: 3.30520795286\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (6.7775850296,21.6877055609), test loss: 31.2880245209\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.76360166073,2.87979869624), test loss: 2.61466428041\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (9.61919021606,21.6079466564), test loss: 40.080293417\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.851890623569,2.87201425423), test loss: 3.25082308799\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.0484828949,21.5293750305), test loss: 29.2252596855\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.09783387184,2.86422633196), test loss: 2.56302061975\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (7.13582372665,21.4518729151), test loss: 39.3624816895\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.86076641083,2.85650711809), test loss: 3.10769658089\n",
      "\n",
      "MC # 2, Hype # hyp4, Fold # 7\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold7/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (405.785858154,inf), test loss: 226.78717041\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (280.079742432,inf), test loss: 350.325880432\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (95.0267791748,153.557963907), test loss: 63.8354697704\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.644030928612,118.02773578), test loss: 3.26155081093\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (45.1315193176,102.793611304), test loss: 41.204454565\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.42735862732,60.5717118177), test loss: 2.93182625175\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (67.665222168,83.7102338978), test loss: 42.8559217453\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.23601031303,41.3916524964), test loss: 3.09017402381\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (33.9242706299,74.0138261693), test loss: 42.9166512489\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.5304671526,31.7971123239), test loss: 3.28624905944\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (17.5018997192,68.3047594292), test loss: 40.4316724777\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.4174836874,26.0475291183), test loss: 2.64416776597\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (54.8716239929,64.4116900576), test loss: 44.3643242836\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (7.02973031998,22.2156138874), test loss: 3.41796965301\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (24.6089515686,61.6297348232), test loss: 43.8414807796\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.48338723183,19.4764249423), test loss: 2.51173580587\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (38.9616775513,59.5244965092), test loss: 44.3295358658\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.15814185143,17.4253263104), test loss: 3.32219437659\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (102.373657227,57.8447888765), test loss: 41.8813094139\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.33989715576,15.8290341428), test loss: 2.68818775713\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (44.6754646301,56.4971208673), test loss: 45.6799836159\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.85327219963,14.5540047908), test loss: 3.2364895463\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (56.1112670898,55.385580459), test loss: 39.9679787636\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.91514348984,13.5100333842), test loss: 2.67935162783\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (23.6954269409,54.4860570906), test loss: 44.9797111988\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.99065899849,12.6415024972), test loss: 3.20532777011\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (42.5870780945,53.7048421496), test loss: 41.2482321739\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.20973038673,11.9075027192), test loss: 2.96477122307\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (39.3467254639,53.0175183291), test loss: 42.0493267059\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (0.822400689125,11.2801742715), test loss: 3.08635186553\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (42.8041038513,52.4185052429), test loss: 42.4451896191\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.71395301819,10.7350735023), test loss: 3.37930013537\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (52.3794250488,51.8681741596), test loss: 39.7242259979\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.2596373558,10.2597094317), test loss: 2.65891459584\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (49.2871322632,51.3787537585), test loss: 43.6500718117\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.9880900383,9.84027545669), test loss: 3.43775918782\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (72.4940948486,50.9324106553), test loss: 42.7541456699\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.03390359879,9.46748166977), test loss: 2.53894994259\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (33.1910018921,50.549541871), test loss: 43.0487828255\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.75257658958,9.13441652093), test loss: 3.39693539739\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (56.0316848755,50.2098495059), test loss: 40.3339361191\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (9.3663520813,8.83669207247), test loss: 2.57888035476\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (32.9902191162,49.8659879691), test loss: 44.8537078857\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.757032155991,8.56646823183), test loss: 3.2776419878\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (25.9202041626,49.5624348432), test loss: 39.2735773087\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.639103889465,8.32105985377), test loss: 2.79951605797\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (118.064285278,49.2640614282), test loss: 43.5662654877\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (7.15036439896,8.09785042678), test loss: 3.12620271146\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (36.4058265686,48.9851425249), test loss: 40.4076776028\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (5.27624750137,7.89335132024), test loss: 3.05647439957\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (32.199508667,48.726493285), test loss: 41.2282624245\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.49510002136,7.70534072613), test loss: 3.09948424697\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (15.008477211,48.4709496703), test loss: 41.260451889\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.28408479691,7.53155069568), test loss: 3.30720645785\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (59.4376068115,48.2566560434), test loss: 38.8085636139\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.69860124588,7.37181766531), test loss: 2.66918568611\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (15.4239759445,48.0373246986), test loss: 42.56838727\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.70501875877,7.22386816258), test loss: 3.46786366403\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (62.1969985962,47.8225956568), test loss: 41.0468845367\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (6.1150636673,7.08646592147), test loss: 2.58821313083\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (17.9961090088,47.6179543172), test loss: 41.3509831429\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.65622735023,6.95768442144), test loss: 3.34428248703\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (43.4230804443,47.4127317045), test loss: 39.0937160492\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.4078373909,6.83775992645), test loss: 2.6853826791\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (47.9773101807,47.2135660222), test loss: 43.8532783508\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.54609870911,6.72557102266), test loss: 3.2889403075\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (27.7789916992,47.0185432105), test loss: 37.5852696896\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.1370267868,6.6197305492), test loss: 2.77155139446\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (53.0135574341,46.8415199894), test loss: 42.2626185417\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.45082569122,6.521056777), test loss: 3.21247006059\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (10.0870685577,46.6643661321), test loss: 38.9777530193\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.25092172623,6.42803384845), test loss: 3.01001093984\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (25.7578811646,46.488577575), test loss: 39.7303800583\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.32627058029,6.34103792027), test loss: 3.11016043723\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (73.080947876,46.307755061), test loss: 39.8066201687\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (6.32755279541,6.25789041765), test loss: 3.37324490547\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (31.3014564514,46.1127359434), test loss: 37.0181091785\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.59424901009,6.17927391025), test loss: 2.62845090032\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (9.47546577454,45.9238252638), test loss: 40.5565342903\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.95725011826,6.10506164627), test loss: 3.48047216535\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (81.1200714111,45.745808937), test loss: 38.4590743065\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (5.44986438751,6.03453572564), test loss: 2.60591607392\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (30.5201320648,45.5534479864), test loss: 39.0502819061\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.95423126221,5.96702847131), test loss: 3.4572261095\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (23.3962631226,45.3745689254), test loss: 36.8257474899\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.43194770813,5.90354312073), test loss: 2.59629377127\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (68.7021408081,45.1863226862), test loss: 41.5282963753\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (9.0181646347,5.84310246333), test loss: 3.31935503185\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (28.6827125549,44.9963365527), test loss: 36.4787480831\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (5.77564716339,5.78471811042), test loss: 2.82979541123\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (21.1004714966,44.8035191456), test loss: 39.2139805794\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.962266922,5.7292410627), test loss: 3.11496132314\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (72.2573318481,44.6018517157), test loss: 36.2835846901\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.222536474466,5.67567558939), test loss: 3.10279371738\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (29.1183700562,44.3984741636), test loss: 36.8644003868\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (6.31248950958,5.62480883572), test loss: 3.08707118034\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (27.425693512,44.192843091), test loss: 36.3230938911\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.22144150734,5.57521024052), test loss: 3.29621511102\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (21.7049694061,43.9846865995), test loss: 33.6601188183\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.5701520443,5.52786543265), test loss: 2.59552204311\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (39.1813011169,43.7616822942), test loss: 36.7483545303\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.79721212387,5.4822533126), test loss: 3.45277660489\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (23.6874599457,43.5219993192), test loss: 33.8602188587\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.09167993069,5.43833711371), test loss: 2.60136470199\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (16.8615283966,43.2622414064), test loss: 34.1736031055\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.737695097923,5.3951549239), test loss: 3.3206175983\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (15.475189209,42.9884167588), test loss: 32.5779922009\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.95216035843,5.35363047071), test loss: 2.64528810084\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (27.8802528381,42.7095223528), test loss: 36.1431773186\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.53771305084,5.31301585796), test loss: 3.24027397037\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (33.7569274902,42.4234112343), test loss: 30.9265304089\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.68630123138,5.27318819128), test loss: 2.68677319884\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (27.5508747101,42.1351915576), test loss: 33.9838108063\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.57630062103,5.23433244056), test loss: 3.10679289699\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.0958938599,41.8407749513), test loss: 30.6723533154\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.3428106308,5.19664832764), test loss: 2.92493985593\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (17.8728656769,41.534288761), test loss: 32.5045208454\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.04593396187,5.15923683244), test loss: 3.02183309197\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.0104522705,41.2256753455), test loss: 30.5649757624\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.849843084812,5.12236413392), test loss: 3.2332095176\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (73.5955352783,40.9109477941), test loss: 28.4919409752\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (7.07340145111,5.08632968759), test loss: 2.47097322047\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (10.0981807709,40.5884077586), test loss: 30.8582131147\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.67797875404,5.05042493109), test loss: 3.34828392863\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (20.3132381439,40.2655643673), test loss: 29.4488971233\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.98310112953,5.01495618111), test loss: 2.42191693485\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (41.6052360535,39.9397436037), test loss: 29.7814410686\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.34588336945,4.9796844462), test loss: 3.18850188851\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (22.4732398987,39.6140520647), test loss: 29.3010089397\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.07383537292,4.94492787799), test loss: 2.35438021719\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (9.27289581299,39.2851424729), test loss: 32.8811720848\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.78262710571,4.91056232173), test loss: 3.16877277195\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (14.3770046234,38.9576773518), test loss: 27.2163307667\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.76990175247,4.87659925131), test loss: 2.44142363369\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (12.6760044098,38.6325421899), test loss: 32.082762289\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.02213001251,4.84263756783), test loss: 2.92333243042\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (10.4794912338,38.3068176615), test loss: 29.8171212673\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.66868662834,4.80918843983), test loss: 2.92484796941\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (4.58286094666,37.9846706886), test loss: 32.2801229239\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.40528297424,4.77601089804), test loss: 2.95459886491\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (3.47491908073,37.6673819592), test loss: 28.3936352491\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.525976896286,4.7429220849), test loss: 3.05397785008\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (16.3902740479,37.3544314567), test loss: 28.7019691467\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (4.05529594421,4.71045355067), test loss: 2.37748007923\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (18.9303588867,37.044401065), test loss: 29.1363020301\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.7157484293,4.67824486335), test loss: 3.23849074244\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (11.6065301895,36.7384363874), test loss: 30.0466355801\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.49934625626,4.64679848861), test loss: 2.25765316933\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (7.0866727829,36.4393886012), test loss: 29.0425546169\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.50245928764,4.6154357829), test loss: 3.02432794571\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (33.8046302795,36.1435802553), test loss: 31.064008832\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (4.26518058777,4.58468957303), test loss: 2.33169008791\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (28.5675430298,35.8533555379), test loss: 33.195515871\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.71629858017,4.55426631131), test loss: 3.07592530698\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (16.7972545624,35.569415637), test loss: 28.391771102\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.57849192619,4.52423742715), test loss: 2.37579478621\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (8.07123470306,35.2895410226), test loss: 33.716504097\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.02066302299,4.49454151638), test loss: 3.06037341654\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (14.8330774307,35.0150640723), test loss: 31.0073226452\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.40246987343,4.46548971901), test loss: 2.91040463746\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (17.1120948792,34.7440761611), test loss: 34.2777644157\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.78540611267,4.43695547614), test loss: 3.05407873392\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (9.25039100647,34.4796307384), test loss: 28.3874005556\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (5.51101112366,4.40890794259), test loss: 3.02894631624\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (10.6429824829,34.2214187409), test loss: 31.4820196152\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.86925935745,4.3813003551), test loss: 2.4426530093\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (12.0590457916,33.9651445794), test loss: 29.8176499367\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.93842625618,4.35410731048), test loss: 3.28429188728\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (13.8026599884,33.7151535531), test loss: 31.3537241459\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.46002292633,4.32739943904), test loss: 2.26006590724\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (12.1865730286,33.470445478), test loss: 29.6372733355\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.16477155685,4.30096172392), test loss: 3.01921819896\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (21.202911377,33.2297856301), test loss: 32.1620097637\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.40505254269,4.27505915474), test loss: 2.3658831358\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (19.0285186768,32.9918393322), test loss: 34.1752255201\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.8700056076,4.24959795241), test loss: 3.11643320322\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (18.5615081787,32.7589978183), test loss: 29.5919270039\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.34829378128,4.22483874501), test loss: 2.38221544027\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (4.91422224045,32.5316463816), test loss: 35.1746340513\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.524164915085,4.20033164194), test loss: 3.10908584446\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (6.95926713943,32.3066006132), test loss: 32.0376990318\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.27019333839,4.17645020118), test loss: 3.02271246612\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (14.1981830597,32.0862844434), test loss: 35.5455823183\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.33435153961,4.1527590467), test loss: 3.10081018209\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (10.7157459259,31.870049042), test loss: 29.5182305217\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.13836538792,4.129505181), test loss: 3.06661383212\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (4.761531353,31.657759733), test loss: 32.0616353989\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.18725919724,4.10652978094), test loss: 2.47851298451\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (9.27296829224,31.448180493), test loss: 30.4527331591\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.2756626606,4.08414465285), test loss: 3.24509145916\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (14.4268264771,31.2414824302), test loss: 31.9217394829\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.94562959671,4.06211331085), test loss: 2.26724028736\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (7.77252197266,31.0393092543), test loss: 30.573155725\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.943287372589,4.04049795502), test loss: 3.04313255996\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (40.3608551025,30.8412435617), test loss: 33.8325195312\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.42901587486,4.01938343045), test loss: 2.39634517133\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (2.81623387337,30.6443973191), test loss: 35.139090693\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.02072560787,3.99846553966), test loss: 3.12802311778\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (4.90171337128,30.4518352928), test loss: 29.9731741905\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.13125264645,3.97781882397), test loss: 2.40068781078\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (20.3142433167,30.2627411638), test loss: 36.0066794395\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.43447828293,3.95748528554), test loss: 3.16500065327\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (17.184419632,30.075785171), test loss: 33.1065348625\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.860998690128,3.93752267444), test loss: 3.07537022233\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (5.84954500198,29.8905608181), test loss: 36.2629799485\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.34176409245,3.91788818895), test loss: 3.13073852211\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (2.32626056671,29.7096330014), test loss: 30.4758494616\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.08764445782,3.89875977465), test loss: 3.0781994611\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (10.3126277924,29.5321160337), test loss: 33.443631506\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.4808614254,3.87985054509), test loss: 2.51670391411\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (14.2142410278,29.3560468237), test loss: 31.0297691345\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.01586031914,3.86126827341), test loss: 3.2764347434\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (9.30779361725,29.1831889045), test loss: 32.3003042221\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.75905942917,3.84289774729), test loss: 2.26952626109\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (6.2802028656,29.0131036678), test loss: 31.6508100271\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.28263831139,3.82472856767), test loss: 3.06328171194\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (11.8751535416,28.8460616493), test loss: 32.5097789764\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.35542058945,3.80687771412), test loss: 2.39657214731\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (12.7942638397,28.6801292604), test loss: 36.1360803485\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.71550738811,3.78928189193), test loss: 3.13290190101\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (15.555683136,28.5169533604), test loss: 30.3571393013\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.5311293602,3.77212012287), test loss: 2.42533562779\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (6.33630371094,28.3568906531), test loss: 36.7870640993\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.11936664581,3.75514949786), test loss: 3.17393044382\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (27.6812438965,28.1988943897), test loss: 32.120291996\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.71661925316,3.73853289614), test loss: 3.10760654807\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (15.9515886307,28.0434016177), test loss: 37.1527130604\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.08820581436,3.72206656573), test loss: 3.13921811581\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (6.0240893364,27.8902527471), test loss: 30.9881949425\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.86405706406,3.70575340467), test loss: 3.08150132\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (13.4828166962,27.7395187118), test loss: 32.958943224\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (3.90600538254,3.68968143547), test loss: 2.52311928272\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.79848146439,27.5901989447), test loss: 32.188929987\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.04433298111,3.67386377626), test loss: 3.2616332978\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (8.24809741974,27.4425340904), test loss: 32.963603878\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.12369608879,3.65834441044), test loss: 2.26674661189\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (12.0938587189,27.2979033895), test loss: 32.7295442581\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (4.64390659332,3.64313594452), test loss: 3.09176590741\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (6.91219854355,27.1557934606), test loss: 35.5882544041\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.957129478455,3.6280804588), test loss: 2.41813066304\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (10.5228996277,27.0144157599), test loss: 37.1092443228\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.05362844467,3.61326700156), test loss: 3.13995700479\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.5258808136,26.8759024637), test loss: 30.8202288628\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.04229617119,3.59859508895), test loss: 2.42244772464\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.54536914825,26.7393782271), test loss: 37.2221722126\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (3.06421422958,3.58403447666), test loss: 3.1631070286\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (5.17068004608,26.6045833876), test loss: 32.0782156944\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.262671709061,3.5697243615), test loss: 3.07502288222\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (33.1653060913,26.4709309296), test loss: 37.5117624879\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.23540019989,3.55559596113), test loss: 3.15658292174\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (24.5627918243,26.3390939351), test loss: 31.7596997499\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.09049153328,3.5418343227), test loss: 3.08141571581\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (3.50226259232,26.2097392287), test loss: 33.6930208206\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.335736632347,3.52816436042), test loss: 2.55027046204\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (7.07529878616,26.0815012947), test loss: 31.9356636524\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (3.05755925179,3.51480378613), test loss: 3.26970458031\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.70719718933,25.9556431872), test loss: 33.2261768818\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.46365118027,3.50147115563), test loss: 2.26425134689\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (15.5738677979,25.8312050005), test loss: 34.0934779644\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.17811989784,3.48830939414), test loss: 3.11799389422\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (8.90251159668,25.7084507021), test loss: 32.8792016983\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.51891040802,3.47526610334), test loss: 2.39820697755\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (7.34762239456,25.5867312899), test loss: 37.7730755568\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.868104338646,3.46246174672), test loss: 3.13723376095\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (10.40346241,25.4663013041), test loss: 31.1932427406\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.7941391468,3.44983600421), test loss: 2.41730103344\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (7.03055286407,25.3479941594), test loss: 37.6709202051\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.64389789104,3.43741525775), test loss: 3.16289199591\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (11.5295438766,25.2317089084), test loss: 32.5742248058\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.630863904953,3.42521224271), test loss: 3.09291311353\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (5.67648124695,25.1158470322), test loss: 38.1225711346\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.30423748493,3.41309103747), test loss: 3.12944567204\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (4.49297523499,25.0020104147), test loss: 31.8542052746\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (3.56831383705,3.40105779044), test loss: 3.07461385727\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (7.78696250916,24.8899431337), test loss: 33.7367938519\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.48257112503,3.38911440305), test loss: 2.49540456533\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (9.00938224792,24.7786402285), test loss: 33.4577976942\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.63928925991,3.37737589584), test loss: 3.24911171198\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (7.37374639511,24.6681481419), test loss: 33.3075572014\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.1138985157,3.3657465132), test loss: 2.25409291685\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (3.79882144928,24.5596556901), test loss: 34.8428388357\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.07945501804,3.35441003009), test loss: 3.13099846244\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (3.28328800201,24.4530426575), test loss: 35.6085837841\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.38657784462,3.34317456043), test loss: 2.41949014366\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (15.524394989,24.34716098), test loss: 38.5403875828\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.09323644638,3.33209243946), test loss: 3.09333635867\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (9.2184715271,24.242786703), test loss: 31.0391554832\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.46771335602,3.32107002407), test loss: 2.33610749841\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (9.18377685547,24.1396533284), test loss: 37.8371172428\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (3.10878181458,3.31012418217), test loss: 3.18042092174\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (7.2793841362,24.0379689706), test loss: 32.3100555897\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.9208111763,3.29932981666), test loss: 3.02959237248\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (19.9236164093,23.9369804876), test loss: 37.8863393784\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.11440777779,3.28864458094), test loss: 3.12905996889\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (13.083190918,23.8368030341), test loss: 32.9419347763\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.21247315407,3.27819701314), test loss: 3.06201814562\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (10.21504879,23.738527292), test loss: 34.195947361\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (3.04995989799,3.26783899852), test loss: 2.4138268277\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (9.86953735352,23.6412221024), test loss: 32.4356807947\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.21686935425,3.25766701751), test loss: 3.21982738376\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (6.12694644928,23.5452201453), test loss: 33.8440249443\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.05937290192,3.24753919515), test loss: 2.27266901135\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (2.29945707321,23.4503435401), test loss: 35.4837080479\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.71000099182,3.23747106028), test loss: 3.12058802545\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (11.1568746567,23.356619506), test loss: 33.0187221527\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (4.16042613983,3.22751273503), test loss: 2.39514834136\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.66076564789,23.263551749), test loss: 38.5756573677\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (4.30422258377,3.21768755883), test loss: 3.02608381659\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (17.3489723206,23.1713062835), test loss: 30.8338366985\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.53364336491,3.20796938924), test loss: 2.29437424541\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (7.77468299866,23.0803516648), test loss: 38.1054318666\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.78084945679,3.1984460708), test loss: 3.20335626304\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.83524036407,22.9909663618), test loss: 32.0754106522\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.536175966263,3.18899529293), test loss: 2.99416699111\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (7.34131002426,22.9017668179), test loss: 38.3875994444\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.54597520828,3.17969473041), test loss: 3.11332494915\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (6.55383205414,22.8142511431), test loss: 32.3902726173\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.783052325249,3.17039923888), test loss: 3.04948289245\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (4.78745412827,22.727562456), test loss: 34.4727902412\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (3.72198987007,3.16118293098), test loss: 2.35508188307\n",
      "\n",
      "MC # 2, Hype # hyp4, Fold # 8\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold8/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (360.829986572,inf), test loss: 201.001216507\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (277.46105957,inf), test loss: 337.35904541\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (36.2929077148,125.539246648), test loss: 42.7922762871\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.502005577087,64.6549653405), test loss: 3.03171694726\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (35.0126724243,87.2332504358), test loss: 33.9574588299\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.07127451897,33.8359518582), test loss: 2.89630064964\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (2.61057400703,73.9550037711), test loss: 36.9131351948\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.01273775101,23.5520867719), test loss: 3.25135168433\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (13.6361122131,67.3655924516), test loss: 35.3941922188\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.14842402935,18.4062510271), test loss: 3.1752040118\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (120.244628906,63.4102373336), test loss: 39.440088892\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.40326023102,15.3207856882), test loss: 2.81288245022\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (23.7760887146,60.6705072107), test loss: 41.2784762859\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.09322690964,13.2673337961), test loss: 3.20047987103\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (35.2179412842,58.7032834804), test loss: 41.1505283952\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.67523050308,11.7961210823), test loss: 2.81979201436\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (44.8216667175,57.1887814903), test loss: 40.4402720451\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.5588555336,10.6948820034), test loss: 3.20611090362\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (55.354850769,55.959043294), test loss: 37.8288038731\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.5665730238,9.83704450564), test loss: 2.71289868951\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (47.9699630737,54.9777269643), test loss: 39.6707626343\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (5.92151355743,9.14816924066), test loss: 3.1532464534\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (37.7970314026,54.1949131636), test loss: 36.1238314152\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.70681381226,8.58593423407), test loss: 2.65663031936\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (34.5066490173,53.5204531438), test loss: 37.652937603\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.69303369522,8.11651483014), test loss: 3.16697417498\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (83.5605010986,52.9361277697), test loss: 33.178192091\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.52451753616,7.72189912054), test loss: 2.52942250967\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (44.1677627563,52.4154062979), test loss: 35.9021696568\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (0.507467269897,7.38094780811), test loss: 3.02567815781\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (63.9697303772,51.9394188896), test loss: 32.4631264448\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.68912005424,7.08642913009), test loss: 3.01253464818\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (29.2387580872,51.4873598786), test loss: 36.7184223056\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.71102762222,6.82800341794), test loss: 3.01108929217\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (54.0512809753,51.094782812), test loss: 35.7227547646\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.6581184864,6.59756456693), test loss: 3.08796357214\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (25.0114192963,50.7498053676), test loss: 39.4567677498\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.88574194908,6.39350186087), test loss: 2.73320140243\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (40.7153015137,50.4238884815), test loss: 40.0916483402\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.11663734913,6.21081536621), test loss: 3.31583073139\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (34.1209869385,50.1114901008), test loss: 39.6973970413\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.08285808563,6.04762815193), test loss: 2.65654201508\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (46.7755279541,49.8203229844), test loss: 37.3297475815\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.28059005737,5.89808248861), test loss: 3.04497167766\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (50.6627922058,49.5292741861), test loss: 35.3679301739\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.8296122551,5.76201960637), test loss: 2.65988737345\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (58.2513999939,49.2386338475), test loss: 38.2059600353\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.54532730579,5.63709093337), test loss: 2.97879213095\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (3.5657684803,48.9694874749), test loss: 34.6944458723\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.46091747284,5.52109015526), test loss: 2.58069719374\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (14.9974594116,48.7221542647), test loss: 35.1107670784\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.784287035465,5.414428147), test loss: 3.22388163209\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (53.8985748291,48.4710390907), test loss: 30.6395921707\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.27380204201,5.31587225828), test loss: 2.80113849044\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (53.4741668701,48.2253786539), test loss: 33.7553732872\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.04558372498,5.22497498164), test loss: 3.08608267307\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (33.8076629639,47.9812847662), test loss: 32.0982777596\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.02442502975,5.13903808115), test loss: 2.97083263397\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (45.2824020386,47.7288032602), test loss: 35.6156763554\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (4.11014938354,5.05869809753), test loss: 2.68993881941\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (37.4314537048,47.4667274875), test loss: 37.4284762383\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.08843398094,4.98301936269), test loss: 3.05605017394\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (57.4986801147,47.2149816171), test loss: 36.1993673563\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.69291853905,4.91058731773), test loss: 2.46148840338\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (20.4590702057,46.9695307774), test loss: 36.616032505\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.72546482086,4.84280928845), test loss: 3.22853971124\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (47.8310546875,46.7112243573), test loss: 33.0749121428\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.27185416222,4.77846384727), test loss: 2.52977945954\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (39.2045249939,46.4454179591), test loss: 34.5186161041\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.87153851986,4.71736344557), test loss: 2.97894302011\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (22.7923774719,46.177445646), test loss: 30.7303028107\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.27908229828,4.65860959721), test loss: 2.52994729877\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (30.5315227509,45.8954401899), test loss: 32.8128961563\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.99133729935,4.60220330677), test loss: 2.81012725234\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (34.3726119995,45.6037898612), test loss: 27.249904871\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.66685533524,4.54800777795), test loss: 2.36574686766\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (46.81275177,45.311831446), test loss: 30.9415434361\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.86414408684,4.49492066316), test loss: 2.99208793938\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (22.6376724243,45.0176975604), test loss: 26.1260362387\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.7173730135,4.4441391094), test loss: 2.86195448488\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (7.98709630966,44.7077366955), test loss: 30.0068202734\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.61272120476,4.39509782933), test loss: 2.97176539451\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (7.14018440247,44.391591353), test loss: 28.3966189861\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.51303982735,4.34807407105), test loss: 2.88253081292\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (32.8895530701,44.0685564102), test loss: 31.8092374325\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.99056339264,4.30216031707), test loss: 2.51410208046\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (61.8048553467,43.7296037054), test loss: 31.6448822021\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.96844315529,4.25751786462), test loss: 3.04779064357\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (23.608127594,43.3782965562), test loss: 30.6281630039\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (4.63652515411,4.21425375319), test loss: 2.45799061358\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (72.3587493896,43.0206636975), test loss: 29.7470229149\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.67575335503,4.17149184856), test loss: 3.00956822336\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (23.8652458191,42.6577108622), test loss: 27.4087857246\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.60155928135,4.1304264719), test loss: 2.51470885277\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (24.9379558563,42.2793361531), test loss: 29.4977458954\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.25718075037,4.09044724569), test loss: 2.91638013124\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.6431388855,41.8950107686), test loss: 25.5144598961\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.32918834686,4.05188253628), test loss: 2.48853651583\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.5252170563,41.5035093629), test loss: 26.8676239014\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.95685660839,4.01448634981), test loss: 2.96247521341\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (64.818397522,41.1002612407), test loss: 22.2621221066\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.14507961273,3.97794349283), test loss: 2.7932999149\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (15.1649560928,40.6897941697), test loss: 25.5084862947\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (6.66912937164,3.94234731786), test loss: 3.15288841128\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (6.23533916473,40.2690454802), test loss: 22.1891675472\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.68072509766,3.90731884221), test loss: 2.89818761051\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (39.0147323608,39.8454347914), test loss: 26.4747035027\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.39376544952,3.87359105268), test loss: 2.78908863366\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.207868576,39.4174323546), test loss: 26.7322536945\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.69705796242,3.84068966952), test loss: 2.98847036809\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (11.8528823853,38.9958146945), test loss: 27.5139316559\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.84815597534,3.80894770118), test loss: 2.47225235105\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (11.6913604736,38.5788937756), test loss: 29.0189444065\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.742409348488,3.77815569909), test loss: 2.98557754755\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (27.6012496948,38.1652019465), test loss: 26.1097812891\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (4.39496898651,3.74815878812), test loss: 2.52552129924\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (8.43983650208,37.7609149342), test loss: 28.8823017359\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.90278291702,3.71859160284), test loss: 3.02103731483\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (15.9558515549,37.363258849), test loss: 24.7236019135\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.26650977135,3.68977180756), test loss: 2.60195574462\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (6.50802230835,36.9734343468), test loss: 28.5829427481\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.764971852303,3.66196277441), test loss: 3.19382326007\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (10.7697582245,36.5915624138), test loss: 23.1224291325\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.50404298306,3.63489498417), test loss: 2.41770576835\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (7.17337274551,36.2203190024), test loss: 28.3295559406\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.04027724266,3.60885106673), test loss: 3.08331949115\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (26.4143867493,35.8579563504), test loss: 23.1496809483\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.2308318615,3.58356083109), test loss: 2.96295489669\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (8.61333560944,35.5022116549), test loss: 28.125925684\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.53391546011,3.5588025635), test loss: 3.15340970457\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (15.6013479233,35.1571891123), test loss: 23.2738366604\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.201130867,3.53443963024), test loss: 2.99028548449\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (6.6412153244,34.8179185167), test loss: 30.9536774635\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.30861473083,3.5106330704), test loss: 2.81864226162\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (33.7481880188,34.4880093892), test loss: 26.2957055569\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.68722081184,3.48758616137), test loss: 3.15566673577\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (13.5536937714,34.1647470919), test loss: 29.8504433632\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.48607468605,3.4650545802), test loss: 2.56746032834\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (8.69409275055,33.8515559521), test loss: 28.8662730455\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.07922363281,3.44339663323), test loss: 2.90646206141\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (4.28422164917,33.5455208495), test loss: 30.548247385\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.92945671082,3.42228467025), test loss: 2.64421837926\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (19.4429607391,33.2460725465), test loss: 29.8806640148\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.76394438744,3.40154677728), test loss: 2.92943983078\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (6.27886819839,32.9545311894), test loss: 26.2259355068\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.949739217758,3.38102825659), test loss: 2.59064204097\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (24.4246520996,32.6682022244), test loss: 29.7718966484\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.13359487057,3.36095980681), test loss: 3.19976106584\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.98768424988,32.3879786114), test loss: 24.1373785973\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.24129509926,3.34150983516), test loss: 3.01050581038\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (6.35113620758,32.1141379955), test loss: 29.3143484592\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.43347454071,3.32245091762), test loss: 3.20165486336\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (3.45256233215,31.848130902), test loss: 23.6155032158\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.80315470695,3.30401862803), test loss: 2.95478397012\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (28.2097148895,31.5878304955), test loss: 31.3718941212\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.98055243492,3.28605587074), test loss: 2.89291618168\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (4.58325576782,31.3323308495), test loss: 27.3470874786\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.80212044716,3.26832183316), test loss: 2.99876262546\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (21.2376556396,31.0831618975), test loss: 30.7133998394\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.56773686409,3.25085382009), test loss: 2.5139634341\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (10.9862251282,30.8381681267), test loss: 30.9160676479\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.22555029392,3.23360786468), test loss: 3.00639573336\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (7.16863012314,30.5980495057), test loss: 29.3199778557\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.60900998116,3.2168633657), test loss: 2.57236294746\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (11.5268697739,30.3632980946), test loss: 31.4058879852\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.03353500366,3.20046407162), test loss: 3.00678347647\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (8.28981971741,30.1346297703), test loss: 26.8669258118\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.91897273064,3.18451486995), test loss: 2.50373168141\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.4725370407,29.9102221944), test loss: 31.545018363\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.326954722404,3.16899445069), test loss: 3.02598587275\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (17.4365978241,29.6906008481), test loss: 24.6030714989\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.78637504578,3.15365580489), test loss: 2.38234861493\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (12.4723968506,29.4751321486), test loss: 30.6469581604\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.9001595974,3.13848183839), test loss: 3.15243359208\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (11.3409290314,29.2631156391), test loss: 23.4729079008\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.28406178951,3.12345738736), test loss: 2.89583153129\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (17.8326225281,29.0554425939), test loss: 30.3140722752\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.59476089478,3.10876123332), test loss: 3.01891010404\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (8.75024223328,28.8517206463), test loss: 24.7677968979\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.58918452263,3.09456026611), test loss: 2.90202619582\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (4.43348073959,28.6529663941), test loss: 33.0794397831\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.08038377762,3.08059188699), test loss: 2.77936464846\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (16.1657867432,28.457760511), test loss: 27.9424761772\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.41191411018,3.06702735285), test loss: 3.04882780313\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (6.98820543289,28.2662698927), test loss: 30.7510357857\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.878570139408,3.05356060095), test loss: 2.52448539436\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (9.8374414444,28.0784044958), test loss: 30.5272980928\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.0682849884,3.04015563998), test loss: 2.92562571764\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (9.24178791046,27.8931162724), test loss: 29.7756639957\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.4094953537,3.02703656435), test loss: 2.63693056107\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (10.3225402832,27.7110191964), test loss: 32.0387593746\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.43955922127,3.01405297033), test loss: 3.03378705978\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (16.2951927185,27.5325815523), test loss: 26.5441785336\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.42336344719,3.00152926342), test loss: 2.51590959132\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (5.35419845581,27.358201052), test loss: 31.3356412411\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.335273951292,2.98921887512), test loss: 3.08553790003\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (6.76338815689,27.1863156756), test loss: 24.3900642395\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.432414412498,2.97722182492), test loss: 2.78245082498\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (8.07354927063,27.0176729875), test loss: 30.9015445471\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.77805233002,2.96531122188), test loss: 3.1830484286\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (6.13407611847,26.8519820571), test loss: 24.5931909561\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.4207239151,2.95334076981), test loss: 2.88724699318\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (11.2408838272,26.6883169732), test loss: 32.2322549343\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.12431430817,2.94164396657), test loss: 2.95167711377\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (20.6280441284,26.5272539338), test loss: 28.3162733555\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.83957386017,2.93016132759), test loss: 2.98320104182\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (11.3296728134,26.3691079565), test loss: 31.7802395821\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.4821472168,2.91906992492), test loss: 2.48248973787\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (6.67608642578,26.2148180643), test loss: 31.6549347401\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.02852749825,2.90811362131), test loss: 2.94739776254\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (12.5727386475,26.0624038471), test loss: 30.8752036333\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.55987715721,2.89740616478), test loss: 2.47726986259\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (16.4554977417,25.9128605171), test loss: 32.7549183846\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.0812292099,2.88674108667), test loss: 3.00298656225\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (11.5167751312,25.765725075), test loss: 26.9915742397\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.15932488441,2.87604202206), test loss: 2.47309554517\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.05060863495,25.6199950176), test loss: 32.0610338926\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.462206721306,2.86560180102), test loss: 3.08933353722\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (18.1413841248,25.4764195917), test loss: 24.746268034\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.94946706295,2.8553146751), test loss: 2.33512342274\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (5.34474849701,25.3357787222), test loss: 31.6822145462\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.60069274902,2.84537736688), test loss: 3.08313568234\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (6.0507440567,25.1982417742), test loss: 23.9976304412\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.87644743919,2.8355568771), test loss: 2.78784926236\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.45512962341,25.0622021357), test loss: 31.6253734112\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.43579149246,2.82595533601), test loss: 2.83243768513\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (13.1902685165,24.9286224131), test loss: 25.572329092\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.19982373714,2.81636278119), test loss: 2.84004534185\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (5.58544826508,24.7971468406), test loss: 33.80328269\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.612242221832,2.80668449769), test loss: 2.66923987865\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (8.7972202301,24.6665169586), test loss: 29.2327872276\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.70078706741,2.79731846425), test loss: 2.98675929308\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.40848350525,24.5378215412), test loss: 30.7585948467\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.43110656738,2.78803187299), test loss: 2.42937758863\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (8.31259155273,24.4120624593), test loss: 31.7591404438\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.665700316429,2.77905371507), test loss: 2.90054246485\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (7.51769447327,24.2886048418), test loss: 30.4595002651\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.89569497108,2.77020959786), test loss: 2.51382023692\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (8.10398483276,24.1663674233), test loss: 33.9613132477\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.93293273449,2.76153948597), test loss: 3.02665714622\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (12.4740581512,24.0461529177), test loss: 26.7736147404\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.77874982357,2.75284268839), test loss: 2.49223662615\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (13.1859951019,23.9280331051), test loss: 32.2708291054\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.811404109,2.7440960927), test loss: 3.05509584546\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (7.15080595016,23.810318669), test loss: 24.1453816414\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.07616353035,2.73558833858), test loss: 2.68574123383\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (3.68842959404,23.694289558), test loss: 31.5665531635\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.59175491333,2.72716878925), test loss: 3.11778853536\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (9.32324790955,23.5807345913), test loss: 24.5296473503\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.99165904522,2.719056677), test loss: 2.84472721219\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (9.1209154129,23.4692824247), test loss: 33.3648376942\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.87344378233,2.71099581255), test loss: 2.90384256542\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (9.76708030701,23.3588000115), test loss: 28.8050862789\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.55343580246,2.70309986326), test loss: 2.96164711714\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (4.06354236603,23.2500672483), test loss: 32.8141239166\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (4.98552894592,2.6952218463), test loss: 2.49379590154\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (22.1925849915,23.1430800002), test loss: 32.0940282345\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.16366648674,2.68725401087), test loss: 2.91459344029\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (8.7460603714,23.0364332795), test loss: 30.776157403\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.983679592609,2.67951397107), test loss: 2.45397626758\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (9.02373123169,22.9313420971), test loss: 33.6922132492\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.632423520088,2.67183037336), test loss: 2.96732368469\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (12.3499031067,22.8282535298), test loss: 27.3475156307\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.62940573692,2.66438649783), test loss: 2.48015750349\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (8.206199646,22.7269103316), test loss: 34.5260116577\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.52089738846,2.65708586954), test loss: 3.02849946618\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (7.69397306442,22.626665441), test loss: 25.2433012009\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.584480822086,2.64986124494), test loss: 2.39002847672\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.76711463928,22.5278595009), test loss: 33.055513823\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (5.95013427734,2.6426191436), test loss: 3.15967019498\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (2.98801016808,22.4302577865), test loss: 23.9154602528\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.26141524315,2.63536940963), test loss: 2.71519962251\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (11.2866306305,22.3332296292), test loss: 31.8009688854\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.986679136753,2.62828438196), test loss: 2.80406942964\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (9.01501369476,22.2373960427), test loss: 27.1680129051\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.75503659248,2.62126719675), test loss: 2.78576647937\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (10.5732526779,22.143441253), test loss: 34.3025559902\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.38544774055,2.61443704082), test loss: 2.61532156765\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (7.19131088257,22.0510293419), test loss: 30.3641928196\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.201064795256,2.60775058875), test loss: 2.98267585933\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (8.88094329834,21.9595176941), test loss: 31.6113064289\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (3.08289408684,2.60113116564), test loss: 2.3783591181\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (3.40656590462,21.869176323), test loss: 33.1638540506\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (3.86705613136,2.59440061796), test loss: 2.91724835932\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (9.26080417633,21.7800139901), test loss: 29.2782204151\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.793462574482,2.58779287341), test loss: 2.47926441431\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.24088287354,21.6911008766), test loss: 34.6594184637\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.763313174248,2.58126963908), test loss: 3.0322401464\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (6.39480781555,21.6033621588), test loss: 26.9854771614\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.2895553112,2.57484119912), test loss: 2.44280012846\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.85424900055,21.517292075), test loss: 32.9660436153\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.05608892441,2.56855266246), test loss: 3.03761863112\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (14.1041040421,21.4325804411), test loss: 24.9464880466\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.703329503536,2.56241388701), test loss: 2.65285536349\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (4.08993196487,21.34871159), test loss: 32.2004038811\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.528335511684,2.55628494168), test loss: 3.09871398658\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (14.8700847626,21.2658683429), test loss: 24.8870791435\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.90799975395,2.55008478825), test loss: 2.85693073571\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (6.64114665985,21.183630104), test loss: 33.3650354862\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.09034395218,2.54401827046), test loss: 2.90066283941\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (21.3542404175,21.1021387597), test loss: 29.3607204914\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.58574175835,2.5380021583), test loss: 2.96748244166\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (8.20956516266,21.0215043703), test loss: 33.1611341\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.76736009121,2.53207019967), test loss: 2.44194446504\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (7.3772611618,20.9423423188), test loss: 32.6953880548\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.072701931,2.52626896368), test loss: 2.89137205184\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (2.75268363953,20.8642928646), test loss: 32.9870506763\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.97520375252,2.5206030303), test loss: 2.52780922055\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (10.1277942657,20.7871817623), test loss: 34.3054941177\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.51695179939,2.5149218364), test loss: 2.97776922882\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (3.4555170536,20.7107271679), test loss: 27.8551187038\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.574445128441,2.50917752568), test loss: 2.50230657458\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (24.8355484009,20.6350274935), test loss: 32.8671993732\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.19648694992,2.50355957957), test loss: 2.93729875535\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (6.90694332123,20.5596639825), test loss: 26.1277610302\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.13807845116,2.49800118712), test loss: 2.63459180593\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (4.82027721405,20.4852842781), test loss: 33.1923692942\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.53188109398,2.49249485113), test loss: 3.04083741307\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (4.73743009567,20.4122646872), test loss: 23.7760217905\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.20823264122,2.48710979741), test loss: 2.68956031501\n",
      "\n",
      "MC # 2, Hype # hyp4, Fold # 9\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold9/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (263.128814697,inf), test loss: 192.927093506\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (337.560058594,inf), test loss: 366.907658386\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (42.5718841553,88.0460589724), test loss: 46.0901750565\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.58340680599,97.6878888154), test loss: 3.29612796903\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (62.0139007568,67.0169546494), test loss: 36.7233684063\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.73070549965,50.4713584711), test loss: 2.7882463932\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (27.6598453522,60.0158225705), test loss: 45.6575131655\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.0836250782,34.7215664327), test loss: 3.44029052854\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (22.3337745667,56.5915757985), test loss: 41.6406626225\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.37849855423,26.850565214), test loss: 3.4761054188\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (18.7829170227,54.4795044983), test loss: 47.7716759682\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.25971412659,22.1328066749), test loss: 3.35621683002\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (24.9799995422,53.0151425589), test loss: 45.2376219749\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (5.41414117813,18.9880824474), test loss: 3.64387318492\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (32.1544761658,51.9759923865), test loss: 45.0199520111\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.10014533997,16.7407588778), test loss: 2.76381110549\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (58.6724243164,51.106411907), test loss: 46.3408808708\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.91786026955,15.0542463172), test loss: 3.83571249843\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (92.8462753296,50.4928300116), test loss: 44.3589689255\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.94304800034,13.7424503003), test loss: 2.86369162202\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (31.9778022766,49.9371943232), test loss: 44.8083596706\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.21743226051,12.6923236689), test loss: 3.8119710207\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (45.6148376465,49.5340916884), test loss: 41.5542063236\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (8.7180480957,11.8367680765), test loss: 2.89201493263\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (30.3856735229,49.1578755444), test loss: 47.1985842228\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.90520083904,11.1233346627), test loss: 3.58055574894\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (43.2725524902,48.8270571626), test loss: 39.364168644\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.0014333725,10.5186269452), test loss: 3.12627147138\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (49.5965499878,48.5236155744), test loss: 45.4444229126\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.77724671364,10.0016809032), test loss: 3.21364119649\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (37.8734359741,48.2299268019), test loss: 35.8199448109\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (5.49807834625,9.55271455539), test loss: 2.78991329074\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (56.9283981323,47.9882614701), test loss: 44.0592809677\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.85931873322,9.1586777916), test loss: 3.37266230583\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (18.1844577789,47.779421817), test loss: 40.2436441898\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.709017753601,8.81200675366), test loss: 3.42248977423\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (77.7541503906,47.5770202981), test loss: 44.8492138147\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.87682986259,8.50561011775), test loss: 3.32627162039\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (26.9512710571,47.3775670011), test loss: 44.1957252502\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (0.493198484182,8.22957982233), test loss: 3.69124154449\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (49.8704185486,47.1890364947), test loss: 43.2446769714\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.11642384529,7.98266103802), test loss: 2.8983620882\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (41.5770111084,46.9894510843), test loss: 45.6816874981\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.91438269615,7.75817744436), test loss: 3.78679442406\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (110.937988281,46.8273327566), test loss: 43.2107022762\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.89852809906,7.55364474492), test loss: 2.8078055799\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (38.9805221558,46.6581381767), test loss: 44.3097248077\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.49986362457,7.36680950769), test loss: 3.73509225249\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (58.7704849243,46.5047588316), test loss: 40.0179867744\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.51472091675,7.1962897764), test loss: 2.92118836641\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (113.524238586,46.3558777259), test loss: 44.8954158783\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (6.4272236824,7.04041349877), test loss: 3.58558666706\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (61.6998977661,46.2002272489), test loss: 37.8614112854\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.36048793793,6.89505497732), test loss: 3.06960369349\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (56.550491333,46.0419573976), test loss: 43.8501451969\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (6.77330589294,6.76076839328), test loss: 3.20692478418\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (37.8433990479,45.8746948671), test loss: 35.4623924255\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.75517463684,6.63523788301), test loss: 2.80280714035\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (72.525428772,45.7201010881), test loss: 42.2198978424\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.21412992477,6.51793905121), test loss: 3.34037084579\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (35.3637275696,45.5756695192), test loss: 38.392525959\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.39807486534,6.40921024295), test loss: 3.27944294214\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (55.3730926514,45.4197673462), test loss: 42.4558718204\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (7.75153636932,6.3073911089), test loss: 3.28347216845\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (28.0386333466,45.2652205288), test loss: 41.2728475571\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.709574699402,6.21096335637), test loss: 3.66962845922\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (57.0704193115,45.103370271), test loss: 39.0892177105\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (4.90929222107,6.12088675874), test loss: 2.8266672492\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (29.5237350464,44.9289853499), test loss: 43.850180769\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.30316245556,6.03539140369), test loss: 3.67806820869\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (45.8792648315,44.7606195131), test loss: 40.9340433121\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.17205715179,5.95406519172), test loss: 2.77535696626\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (22.6492080688,44.5955601025), test loss: 42.5546044111\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (6.71284914017,5.87727441991), test loss: 3.69770114422\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (10.4369297028,44.4243844972), test loss: 36.6843442917\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.60893142223,5.80432356601), test loss: 2.87263363004\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (30.7489738464,44.2459919404), test loss: 42.3443413734\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.613717556,5.73542310876), test loss: 3.44137011468\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (14.1747303009,44.0605942087), test loss: 35.0449700356\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.08546996117,5.66906793461), test loss: 2.95647345781\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (67.8899993896,43.8516317044), test loss: 40.4030126333\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.10848867893,5.60549592265), test loss: 3.16219948232\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (22.4460258484,43.6263979946), test loss: 32.3122419834\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (7.46017313004,5.54444570942), test loss: 2.72005645037\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (18.8267154694,43.3957949719), test loss: 38.0993085384\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.56803894043,5.48536841177), test loss: 3.28386756182\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (15.7186050415,43.1637392673), test loss: 34.1800949574\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.75283956528,5.42881913805), test loss: 3.16122811735\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (31.5497703552,42.9174458587), test loss: 35.7175955057\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.14111804962,5.37420220775), test loss: 3.2075196743\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (42.9164390564,42.6614788118), test loss: 34.9069199085\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (5.4181303978,5.32117112906), test loss: 3.36567584276\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (23.6383552551,42.3904353278), test loss: 33.2862390518\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.825612306595,5.26976571024), test loss: 2.68206344843\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (32.982006073,42.1055173712), test loss: 39.7303534269\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.28768396378,5.21964503994), test loss: 3.53547433019\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (5.03079891205,41.8148151035), test loss: 33.9807525635\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.623790085316,5.16998695987), test loss: 2.601571545\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (16.4229621887,41.5159350582), test loss: 35.9732922077\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.88477516174,5.12198373449), test loss: 3.50296587348\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.9961700439,41.2047116711), test loss: 29.7337041855\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.8488240242,5.0753059504), test loss: 2.50736566186\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (34.1571884155,40.8823987239), test loss: 35.0321464539\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.68666982651,5.02961722234), test loss: 3.27191997766\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (12.2807559967,40.5522895564), test loss: 29.5103975534\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.80369925499,4.9845884643), test loss: 2.65953719914\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (11.3139429092,40.2067764404), test loss: 35.5923323154\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.972760558128,4.94008578414), test loss: 3.16123348176\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (7.71016693115,39.8603076747), test loss: 27.4733943462\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.60577869415,4.89606619441), test loss: 2.58588154316\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (15.0350780487,39.5047717057), test loss: 31.9379582167\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.634308815,4.85269261928), test loss: 3.10085395575\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (11.5538272858,39.1492588447), test loss: 29.2719599247\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.21639287472,4.81038426067), test loss: 2.95733626783\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (14.3859376907,38.7884993221), test loss: 31.2207754135\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.48701047897,4.76856165191), test loss: 3.09135312736\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (6.58289241791,38.4268398294), test loss: 29.0690360069\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.49100065231,4.72728652202), test loss: 3.22979775369\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (12.0977182388,38.0634804207), test loss: 28.0759123087\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.27742886543,4.6868810012), test loss: 2.57925750613\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (24.0343093872,37.701385243), test loss: 31.8492755175\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.23083543777,4.64691979736), test loss: 3.31358194053\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (11.5608100891,37.3444438593), test loss: 30.7339796782\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.64430141449,4.60725116056), test loss: 2.45532180071\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (12.0942020416,36.9897677037), test loss: 31.8610929489\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.93609786034,4.56835355704), test loss: 3.33620996475\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (6.38550710678,36.6400674987), test loss: 28.6358687401\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.10036885738,4.53046945822), test loss: 2.35941393673\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (7.50233459473,36.2949948745), test loss: 32.2876425982\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (5.88548517227,4.4933783982), test loss: 3.16010667086\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (10.9697742462,35.9573581642), test loss: 31.2984369278\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.14809346199,4.45692056046), test loss: 2.58906750977\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.2584037781,35.6221899417), test loss: 33.3608385086\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.639045596123,4.42102444909), test loss: 3.12476308346\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (19.0018959045,35.2968623521), test loss: 28.8411765575\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.93051958084,4.38578655879), test loss: 2.64589594007\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (21.5998687744,34.9763673069), test loss: 32.7393463135\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.06225633621,4.35122694716), test loss: 3.09284299314\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (16.3554611206,34.6628647344), test loss: 31.0349165916\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.96286582947,4.31753194168), test loss: 3.09344457984\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (7.85576438904,34.3554273853), test loss: 33.791016674\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.62292790413,4.28475611413), test loss: 3.19349464178\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.1217784882,34.0549820532), test loss: 29.057694459\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.68644797802,4.25257314242), test loss: 3.22879077643\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (3.83588242531,33.7575511545), test loss: 32.3915602684\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.647059857845,4.2212225839), test loss: 2.77464851886\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (6.75730419159,33.4670455252), test loss: 30.6090812683\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.70803570747,4.19056708891), test loss: 3.25153580904\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.07588195801,33.1843337173), test loss: 32.8114063263\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.51985883713,4.16018411015), test loss: 2.58627450466\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (16.8308982849,32.9051743547), test loss: 30.9582897425\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.7966106534,4.13059705923), test loss: 3.35400239229\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.9966135025,32.6322752048), test loss: 32.4475932121\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.27177977562,4.10181653576), test loss: 2.49760210216\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (7.12980890274,32.3653968353), test loss: 33.4714536667\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.30123257637,4.07363862678), test loss: 3.21379438639\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (10.5362358093,32.1041009209), test loss: 34.6184081078\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.21019554138,4.04620651227), test loss: 2.65171338916\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.61282539368,31.8482143319), test loss: 34.282715559\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.16318273544,4.01927040688), test loss: 3.20173522979\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (21.5457611084,31.5987639629), test loss: 29.2707391262\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.26984024048,3.99275061218), test loss: 2.71919347942\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (9.53706359863,31.3527926779), test loss: 33.9371162891\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.11824822426,3.96664280064), test loss: 3.06345643699\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (8.16179943085,31.1117044846), test loss: 26.7827811241\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.770315766335,3.94113574831), test loss: 2.6410443455\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (22.2444019318,30.87618163), test loss: 35.0651685238\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.60283756256,3.91647599549), test loss: 3.24368227124\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (7.51281690598,30.6458069485), test loss: 30.5649965763\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.69793689251,3.89224128872), test loss: 3.31030368805\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (6.21386432648,30.4190758131), test loss: 37.1490205765\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.83743667603,3.8685073037), test loss: 3.32180546522\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (3.3956155777,30.1975029411), test loss: 31.2575995922\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.41520786285,3.84518315222), test loss: 3.33024555445\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (17.9726638794,29.9807962977), test loss: 33.6083343506\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.11014890671,3.8220741814), test loss: 2.5795448035\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (17.7017364502,29.7664239788), test loss: 32.2355404377\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (3.02123093605,3.7995112711), test loss: 3.40815981328\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (9.86894226074,29.556889672), test loss: 33.5200736046\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.14112496376,3.77747285782), test loss: 2.55734001994\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.45169448853,29.3517783281), test loss: 33.9166130781\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.877771615982,3.7559919158), test loss: 3.29880526662\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (27.9333877563,29.1503700475), test loss: 35.4273910522\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.99171066284,3.73497855634), test loss: 2.71026940495\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (12.9621543884,28.9524277082), test loss: 35.0468159676\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.7717487812,3.71430457239), test loss: 3.22635647655\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (11.605178833,28.7588316558), test loss: 29.879480505\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.19333934784,3.69383670169), test loss: 2.71140557528\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (9.59922409058,28.5678665776), test loss: 34.3974435329\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (5.79881954193,3.6736590598), test loss: 3.13869790733\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (13.8223419189,28.3800954899), test loss: 27.1128869057\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.34235239029,3.65391424439), test loss: 2.55096870065\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (9.1481628418,28.1962396732), test loss: 36.1000962734\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.16761541367,3.63476852865), test loss: 3.27565487027\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (4.67846679688,28.0162447886), test loss: 31.5336585045\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.90588736534,3.61595261008), test loss: 3.33472594619\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (10.014084816,27.8382683568), test loss: 37.2989492893\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.0992629528,3.59742864696), test loss: 3.37548359782\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (6.31107568741,27.6640838465), test loss: 31.4350151062\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.66691350937,3.57911856128), test loss: 3.38385102153\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (11.2426118851,27.4933521015), test loss: 33.7520729542\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.4490146637,3.56104348442), test loss: 2.67315086126\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (10.4527845383,27.3241931511), test loss: 33.5955049753\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.17780399323,3.54334489538), test loss: 3.42611399889\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (13.9297990799,27.1583436095), test loss: 33.6931555271\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.31994605064,3.52601283946), test loss: 2.50995668173\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.88641643524,26.9956310115), test loss: 34.7308226824\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.11147117615,3.50904046955), test loss: 3.25198726952\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (6.48846721649,26.8351502805), test loss: 34.0617677689\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.61796402931,3.49242000006), test loss: 2.69094285369\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (17.0025577545,26.6778132216), test loss: 35.8455757618\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.46141576767,3.47603018192), test loss: 3.31318456233\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (6.18221473694,26.5234614684), test loss: 30.4371804714\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.1459312439,3.45965564963), test loss: 2.72846497595\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (3.17117834091,26.3708724464), test loss: 36.2519114971\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.254366338253,3.44361203043), test loss: 3.14570963681\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.26815700531,26.2202946086), test loss: 27.7314228535\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.32916808128,3.42790694171), test loss: 2.55972545445\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (6.46461629868,26.0727538895), test loss: 36.8173608065\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.77796471119,3.41258500399), test loss: 3.24026810378\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (16.5424728394,25.9277067502), test loss: 32.3011051178\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.09287881851,3.39751061502), test loss: 3.19562624097\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (3.81017637253,25.7842344375), test loss: 36.8288873672\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.09588265419,3.38261764895), test loss: 3.32962405682\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (2.11147785187,25.6439741182), test loss: 31.2290244102\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.15398550034,3.36782426776), test loss: 3.36165375933\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (6.50414323807,25.505539139), test loss: 33.9773775101\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.53334832191,3.35327846529), test loss: 2.82465889752\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (14.6810665131,25.368679555), test loss: 33.8758626938\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (3.02645516396,3.33902704076), test loss: 3.3699635148\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (13.5935163498,25.2337981589), test loss: 33.5046984196\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.7186653018,3.32497240803), test loss: 2.44462371767\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.71169424057,25.1013162622), test loss: 35.4642755747\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.09639883041,3.31117394137), test loss: 3.2727455616\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (6.93339157104,24.9706052469), test loss: 33.3578125\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.41711831093,3.29768396356), test loss: 2.63610537648\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (16.257062912,24.8423665617), test loss: 36.8139308929\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.43819177151,3.28434456728), test loss: 3.26994418502\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (8.80806732178,24.716438068), test loss: 30.6321176529\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.24870347977,3.27099335972), test loss: 2.71864519119\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (12.5512504578,24.5909916122), test loss: 38.103965807\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.68385505676,3.25789115297), test loss: 3.21241710782\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (4.25849103928,24.4675909095), test loss: 28.5553471565\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.848596096039,3.2450249059), test loss: 2.69163394272\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (7.73248720169,24.3462166816), test loss: 37.227316308\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (3.67639446259,3.23245175976), test loss: 3.23554134965\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (25.9891204834,24.226871479), test loss: 32.8307200432\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (3.18784594536,3.22008353432), test loss: 3.16054070592\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (7.63954734802,24.1088282148), test loss: 36.7083539724\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.806295394897,3.2078443459), test loss: 3.33047119379\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (6.43922615051,23.9929241694), test loss: 30.1206526279\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.789890110493,3.1956609847), test loss: 3.13410133421\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (9.18938064575,23.8780508707), test loss: 34.1498644352\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.67406129837,3.18364817028), test loss: 2.810980165\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.86704063416,23.7647236836), test loss: 35.18175807\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.06455683708,3.17177443402), test loss: 3.37413650453\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (4.08915948868,23.6527647576), test loss: 34.7900923729\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.784336745739,3.16021534866), test loss: 2.63668266982\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (7.50957393646,23.5430808945), test loss: 36.0535903692\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.430902063847,3.14880735041), test loss: 3.32671411037\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (4.65545558929,23.4341791247), test loss: 32.8098176003\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.417353510857,3.13759621748), test loss: 2.53837259412\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (7.82776165009,23.3271665537), test loss: 37.2098650694\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.699850320816,3.12648812224), test loss: 3.27645393461\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (13.86124897,23.222182689), test loss: 30.9369626045\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.8803422451,3.11535122833), test loss: 2.69359523505\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (16.4738025665,23.1172233346), test loss: 39.4490739346\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.05540847778,3.10442814215), test loss: 3.26384873241\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (11.5192546844,23.0139596929), test loss: 30.0126887321\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.83139610291,3.09369557954), test loss: 2.67617886961\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (4.4651389122,22.9124164311), test loss: 37.3118104458\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.76380872726,3.0831527978), test loss: 3.22636015713\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (8.47302532196,22.8119820781), test loss: 32.0474410295\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.86178803444,3.07282134119), test loss: 3.09787679613\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (7.42790365219,22.7129804018), test loss: 37.0632142067\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.21086061001,3.06256978833), test loss: 3.36226582229\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (6.21607446671,22.6154116425), test loss: 30.5764396906\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.1338789463,3.05233964525), test loss: 3.16866376102\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (4.38702535629,22.5186413313), test loss: 33.7207395554\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.2868168354,3.04220062625), test loss: 2.85726025403\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (9.23508930206,22.4229885128), test loss: 33.521603179\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.89118289948,3.03221124685), test loss: 3.32579800189\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (7.80872058868,22.3286399064), test loss: 35.4330045223\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.54062986374,3.02248616647), test loss: 2.63795769215\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (10.4267711639,22.2358200646), test loss: 35.6750990868\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.15131115913,3.01287861111), test loss: 3.36363877058\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (6.38662242889,22.1435495335), test loss: 33.8438430309\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.64804935455,3.00339822656), test loss: 2.48803446144\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (2.35403037071,22.0529331277), test loss: 36.6321289539\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.21891498566,2.99395854525), test loss: 3.19739373922\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (12.7807044983,21.9637916329), test loss: 32.2990975857\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.68602919579,2.98453846131), test loss: 2.71685374975\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (11.2003574371,21.8747122583), test loss: 38.6139925957\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (3.7183201313,2.97528668389), test loss: 3.23364788145\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (11.2694244385,21.7869312853), test loss: 31.8169478893\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.784285664558,2.96616542261), test loss: 2.79256518483\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (14.0123596191,21.7004056498), test loss: 37.6274104595\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.877999067307,2.95721936319), test loss: 3.22188248858\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (9.72301769257,21.6145724683), test loss: 33.5350809574\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.55874717236,2.94840743314), test loss: 3.16890933514\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (11.3416938782,21.5302440471), test loss: 37.4124955177\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.4770154953,2.93969147636), test loss: 3.31155849695\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (5.5089430809,21.4470573808), test loss: 29.7541232586\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (4.8917760849,2.93095830359), test loss: 3.07811464369\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (11.6272153854,21.3644441929), test loss: 34.5063098431\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.63573527336,2.92229674911), test loss: 2.86216926202\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (13.6743087769,21.2824333991), test loss: 33.4426273346\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.45875668526,2.91377488565), test loss: 3.2340777576\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (6.58248567581,21.2014903363), test loss: 36.2794525146\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.28569054604,2.90544248843), test loss: 2.69579155743\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (6.80851745605,21.1218936081), test loss: 33.1819684029\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.736499845982,2.89720099034), test loss: 3.30289776325\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (12.2007055283,21.042776203), test loss: 35.1099269867\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.536747694016,2.88905885692), test loss: 2.57711347789\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (7.47039365768,20.9650650348), test loss: 37.2706783295\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.15374612808,2.88094105676), test loss: 3.21107611358\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (12.1240978241,20.8883460669), test loss: 35.0771339893\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.33696985245,2.87288662529), test loss: 2.70787397027\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (8.35181808472,20.8114934823), test loss: 38.3021073341\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.411437213421,2.86491969268), test loss: 3.22356126755\n",
      "\n",
      "MC # 2, Hype # hyp4, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_2/fold10/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (317.909362793,inf), test loss: 168.087145233\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (310.756530762,inf), test loss: 375.707115173\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (40.5928726196,76.6546911507), test loss: 48.623786068\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.70018076897,132.408577573), test loss: 3.15645670295\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (39.0538864136,61.621478596), test loss: 35.4430920601\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.18239450455,67.79467038), test loss: 2.61888027191\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (33.0346794128,56.6183943915), test loss: 46.8452779293\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.22678685188,46.2470571393), test loss: 3.15287480354\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (23.1147766113,54.1894703301), test loss: 38.9918300629\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.99516248703,35.4782820787), test loss: 3.24982475936\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (54.7415657043,52.6408108963), test loss: 47.4002396584\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.66789281368,29.0224199284), test loss: 3.16508401036\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (36.0786209106,51.5528688293), test loss: 42.9329039097\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.801653444767,24.7195192182), test loss: 3.34001445919\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (13.9522600174,50.7725799148), test loss: 43.7169434071\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.70612502098,21.6452060756), test loss: 2.6110240221\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (33.0886192322,50.0979901722), test loss: 45.7224286079\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.28057956696,19.3387931113), test loss: 3.4119481802\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (40.3135986328,49.6031251993), test loss: 42.8934636593\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.55122756958,17.5447504651), test loss: 2.61665769219\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (54.3659591675,49.1526013096), test loss: 44.7608179092\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.16351652145,16.1071477066), test loss: 3.50719006658\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (40.8313064575,48.810464364), test loss: 38.9168512821\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.8470954895,14.934954075), test loss: 2.6870300442\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (32.5840835571,48.4781429739), test loss: 46.9428845406\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.87086606026,13.9577369784), test loss: 3.33759621382\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (83.8839797974,48.1902473454), test loss: 37.3981153965\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (7.41337919235,13.1300730791), test loss: 2.9020136416\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (44.167175293,47.9029654606), test loss: 46.2539718628\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.7232581377,12.4215106085), test loss: 3.08773215413\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (43.7523117065,47.6200020565), test loss: 33.9460880041\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.2812435627,11.8072221079), test loss: 2.60973759294\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (17.1777591705,47.3670033857), test loss: 44.998263979\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.02314639091,11.2680657884), test loss: 3.12973182201\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (20.4318504333,47.1525393742), test loss: 37.0698124886\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.73492383957,10.7936778945), test loss: 3.21237747073\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (64.3449020386,46.9309026878), test loss: 45.1899955273\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.63393306732,10.3732288041), test loss: 3.13135119379\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (49.9660263062,46.7071215108), test loss: 40.7231872559\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.31316947937,9.99724453873), test loss: 3.30017521381\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (22.652381897,46.4925046744), test loss: 41.3497771502\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (5.18394994736,9.65798762023), test loss: 2.6059171021\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (30.9988479614,46.2564814678), test loss: 43.6054133415\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.10218346119,9.35078875199), test loss: 3.38640015721\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (27.584733963,46.0398556123), test loss: 40.2770852566\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.02421426773,9.07208290009), test loss: 2.6216385901\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (42.3780059814,45.7964748604), test loss: 41.8596173286\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (5.83054113388,8.81889072095), test loss: 3.53731424809\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (15.5393896103,45.5618566102), test loss: 36.019303751\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.3050236702,8.5892281647), test loss: 2.71271606982\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (7.47805213928,45.309871582), test loss: 43.402893877\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.00138187408,8.37783952189), test loss: 3.35543305874\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (55.3002471924,45.0613973861), test loss: 34.4668264866\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (5.19952344894,8.18186461082), test loss: 2.88749958277\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (45.004776001,44.7945356221), test loss: 42.2305747271\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.81061279774,8.00062262981), test loss: 3.08335387707\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (41.0997657776,44.5154602), test loss: 30.6240056753\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.69165992737,7.83190925257), test loss: 2.62089487314\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (12.5245742798,44.2364151267), test loss: 40.645298481\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.06430399418,7.67343186279), test loss: 3.14163077474\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (10.9891366959,43.9603082798), test loss: 32.8321914673\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.85892260075,7.52598558832), test loss: 3.2085812211\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (34.2723083496,43.6673550264), test loss: 39.3663030624\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.39954566956,7.38834830335), test loss: 3.11316277683\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (40.8755264282,43.3615871223), test loss: 35.370072937\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.00488758087,7.25886896968), test loss: 3.28301219642\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (49.7081336975,43.0503918255), test loss: 35.5995930672\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.88009166718,7.13613770834), test loss: 2.60192728639\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (45.0763015747,42.7141861277), test loss: 38.2417742252\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.35048007965,7.01981527215), test loss: 3.37005271018\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (16.455116272,42.3768345912), test loss: 34.5833316803\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (4.46107196808,6.90940853402), test loss: 2.57282309532\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (38.7206306458,42.0240137721), test loss: 36.1234435081\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (6.11022377014,6.80385584219), test loss: 3.45426385403\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (15.3340835571,41.6666664352), test loss: 30.7414829016\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.30047273636,6.7039960397), test loss: 2.63098252416\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (1.29300951958,41.2915753622), test loss: 36.3654191971\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.05741548538,6.60854242284), test loss: 3.23856056631\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (32.1247138977,40.9123510063), test loss: 28.7362861633\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.55597877502,6.51641966622), test loss: 2.74979451001\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (26.6901855469,40.5176647788), test loss: 35.0420825958\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.30847930908,6.42820996506), test loss: 2.93300486803\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (17.0448513031,40.1107249237), test loss: 25.5624835014\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.70490193367,6.34311008558), test loss: 2.50548202693\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (21.1367111206,39.7031374844), test loss: 33.8878814459\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.82074904442,6.26039086226), test loss: 3.01103177965\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (13.966132164,39.2913213343), test loss: 27.0226382017\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.709400177002,6.18056832731), test loss: 3.11576561928\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (7.45522212982,38.8705854428), test loss: 32.264668417\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.90364980698,6.10378513994), test loss: 2.96779187322\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (15.2692451477,38.4485981773), test loss: 28.7970820844\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.11044931412,6.02947764518), test loss: 3.14457868636\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (16.1864280701,38.0293241205), test loss: 29.8060544014\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.03779911995,5.95705534783), test loss: 2.40087644458\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (55.0555114746,37.6061478069), test loss: 31.6185639381\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.7774951458,5.88665469822), test loss: 3.19347155392\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (4.73478221893,37.1879313144), test loss: 30.205317688\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.59825062752,5.81808742725), test loss: 2.37873782516\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (28.4800300598,36.7744295654), test loss: 31.2328963637\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.74594497681,5.75120805899), test loss: 3.22320654988\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (8.30959796906,36.3668573983), test loss: 29.0172291279\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.819763720036,5.6866188525), test loss: 2.50146273375\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (11.136926651,35.9651539067), test loss: 32.0817181587\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.9325466156,5.6239972001), test loss: 3.04053185582\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (4.85562419891,35.5750004305), test loss: 27.4549845219\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.69069421291,5.56294353046), test loss: 2.57381565571\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (16.1658649445,35.1911435796), test loss: 32.3961416662\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.339320063591,5.50382312444), test loss: 2.81068587303\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (15.5100030899,34.8146369666), test loss: 25.8091376781\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.84325146675,5.44621568582), test loss: 2.4361603111\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (18.9112548828,34.4502624276), test loss: 33.4849072695\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (5.56877660751,5.38996591433), test loss: 2.96520954967\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (4.86672544479,34.0935306934), test loss: 27.324733901\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.495456397533,5.33509707466), test loss: 3.15230470896\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (14.2827568054,33.743282308), test loss: 33.2999163091\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.28951346874,5.28214808176), test loss: 3.00744988322\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (8.21678638458,33.4040709312), test loss: 28.7335402608\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.61247158051,5.23106241993), test loss: 3.16149382889\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (3.79589366913,33.0757410681), test loss: 31.178044939\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.02695202827,5.18110141779), test loss: 2.36680392921\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (29.6323623657,32.7527649476), test loss: 30.9495148659\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.03679955006,5.13250911723), test loss: 3.1543946445\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (6.33719778061,32.4390825439), test loss: 31.826456356\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.94662857056,5.08518304863), test loss: 2.36899679005\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (29.6976242065,32.1332356282), test loss: 32.7165488839\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (5.48505449295,5.03880188159), test loss: 3.16061996818\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (9.37936401367,31.8331866282), test loss: 31.1440485477\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.800949633121,4.99386960424), test loss: 2.54846087694\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (16.4993209839,31.5408385314), test loss: 33.4064002872\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.7238432169,4.95034759805), test loss: 3.03013833165\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (3.68944501877,31.257708049), test loss: 28.7906009674\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.20532894135,4.90799585193), test loss: 2.58922480047\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (16.0693569183,30.9807396389), test loss: 33.7518360496\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.448734223843,4.86681029345), test loss: 2.83654467016\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (12.7628660202,30.7090622431), test loss: 27.0020759583\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.44188857079,4.82658232863), test loss: 2.47823031545\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (7.19438076019,30.44557425), test loss: 34.7271869421\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.41824054718,4.78724380784), test loss: 3.01455148757\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (3.71082782745,30.1868204799), test loss: 28.032317555\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (0.279967784882,4.74861071469), test loss: 3.20673889816\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (21.1788692474,29.9322082977), test loss: 34.8671485424\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.28046059608,4.71129051631), test loss: 3.07920623422\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (6.9287481308,29.6847533455), test loss: 29.4707878828\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.98739004135,4.67524147273), test loss: 3.19751324952\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (2.32564115524,29.4444886862), test loss: 32.2762006044\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.753332614899,4.63988095733), test loss: 2.4284881562\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (9.07434463501,29.2074190403), test loss: 31.3774410129\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.43651735783,4.60538302712), test loss: 3.17284269631\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (12.3436174393,28.9764745253), test loss: 32.6157602787\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (4.54494190216,4.57157792868), test loss: 2.38113895804\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (6.9568772316,28.7500462372), test loss: 34.1367755711\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (3.58314323425,4.53835749154), test loss: 3.13871669471\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (34.789276123,28.5276566901), test loss: 31.4829530716\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.55697762966,4.50603865047), test loss: 2.57933721542\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.0628957748,28.308962648), test loss: 34.8241361737\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.42508125305,4.47461403318), test loss: 3.08289032578\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (6.75497674942,28.0968878444), test loss: 29.0736620903\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.67874479294,4.44397601916), test loss: 2.61166607141\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.1929759979,27.8886579976), test loss: 35.3926478863\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.610668241978,4.41405768975), test loss: 2.83973070383\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (21.802772522,27.6840939846), test loss: 27.4992715836\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.8173968792,4.38471948557), test loss: 2.48591462076\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (12.2569742203,27.4844344495), test loss: 35.519609952\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.50497198105,4.35587334327), test loss: 3.02966447175\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (5.13004016876,27.2877557933), test loss: 28.5232766151\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.43804895878,4.32743013669), test loss: 3.22037007213\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (19.411239624,27.0937366279), test loss: 36.0611685216\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.21847677231,4.29984315216), test loss: 3.10035254806\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (10.4848537445,26.904677598), test loss: 29.945577538\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.91512060165,4.27312966775), test loss: 3.19458870292\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (6.12715482712,26.7204037848), test loss: 33.1491055012\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.946491003036,4.24681908411), test loss: 2.46154402196\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (9.03810596466,26.5379798687), test loss: 32.0342002869\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.26282978058,4.22105688567), test loss: 3.18747525066\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (11.4689855576,26.3598906737), test loss: 33.3905982971\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (5.09550857544,4.19566921351), test loss: 2.36619184762\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (6.7059211731,26.184631542), test loss: 35.8833447218\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (3.5042681694,4.17061030422), test loss: 3.11204910278\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (30.160369873,26.0117926935), test loss: 31.8940890312\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.05986714363,4.14611234068), test loss: 2.5769887507\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (10.0613002777,25.841497409), test loss: 36.4614505529\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.974893152714,4.12221492112), test loss: 3.13821799755\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (8.40933132172,25.675755768), test loss: 29.3266219139\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (4.04639625549,4.0988602841), test loss: 2.62861112207\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (8.22132587433,25.5126531127), test loss: 36.5741785645\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.03328406811,4.07594277079), test loss: 2.83534220755\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (15.9608211517,25.3523574464), test loss: 27.8949259758\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.38133716583,4.05340398991), test loss: 2.48475885391\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (8.54265594482,25.195091357), test loss: 36.3600198507\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.51494956017,4.03114854778), test loss: 3.01298562884\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (10.3605098724,25.0398893901), test loss: 28.7904154062\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.82563316822,4.00908880429), test loss: 3.23072770685\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (14.0359306335,24.8862796707), test loss: 36.9208134651\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.97991716862,3.98763862434), test loss: 3.10196308345\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (9.69567012787,24.7362687298), test loss: 30.3274969101\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.30487203598,3.96679843521), test loss: 3.22833967507\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (4.89763593674,24.589695473), test loss: 33.5590774298\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.95560777187,3.94622643366), test loss: 2.46510310471\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (6.81783294678,24.4443979089), test loss: 32.4911912918\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (3.4702963829,3.92602498614), test loss: 3.19050121009\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (9.32183456421,24.3022996445), test loss: 34.0105382442\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.34274661541,3.90600261863), test loss: 2.34950387627\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (6.71013975143,24.1621706108), test loss: 37.2274941444\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.76204657555,3.88624270684), test loss: 3.09989236295\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (17.3949203491,24.0235681845), test loss: 32.4785409927\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.41427028179,3.86681497017), test loss: 2.57376220673\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (10.3238019943,23.8868963026), test loss: 37.615519309\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.57837510109,3.84787154158), test loss: 3.15474810004\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (9.24974632263,23.75359435), test loss: 29.5326313972\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.32714366913,3.82929403181), test loss: 2.62475085407\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (6.07669878006,23.6223104692), test loss: 37.2199672103\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.48636579514,3.81105785656), test loss: 2.80811323673\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (15.3212509155,23.4931984966), test loss: 28.2324725628\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.07445430756,3.79306943602), test loss: 2.47494696826\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (7.97812318802,23.3662441841), test loss: 36.7653653502\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.4818533659,3.77524408752), test loss: 2.97652561963\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (11.6474246979,23.240797908), test loss: 28.8312463999\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.54517269135,3.75756636971), test loss: 3.23105565012\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (24.6084632874,23.1166662858), test loss: 37.3654450297\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.43101763725,3.74031363597), test loss: 3.09061627984\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (7.63121175766,22.9947930835), test loss: 30.7042785168\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.87632441521,3.72355264509), test loss: 3.23079524636\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (4.04967212677,22.8758535283), test loss: 33.8112879872\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.65484654903,3.70695853652), test loss: 2.45443366766\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (4.53159666061,22.7577935652), test loss: 32.8500682592\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (3.46878051758,3.69067513094), test loss: 3.17846395671\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (4.14228820801,22.6421946236), test loss: 34.4918902874\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.0277736187,3.67447594254), test loss: 2.34475302547\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (10.5732727051,22.5281081597), test loss: 38.0155740499\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.54544639587,3.65843487814), test loss: 3.10765267015\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (7.21652317047,22.4149058401), test loss: 32.9635592461\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.19405043125,3.64264810105), test loss: 2.56524323523\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (7.5866560936,22.3031657633), test loss: 38.399721539\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.57778918743,3.62723984946), test loss: 3.14499754608\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (9.93177604675,22.1940441999), test loss: 29.855126667\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.16682052612,3.6121007996), test loss: 2.6150840953\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (8.57314968109,22.0863426289), test loss: 37.7400262713\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.93042254448,3.59723627337), test loss: 2.77858432829\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (9.54262924194,21.9803446115), test loss: 28.4965971947\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.969100356102,3.58251298766), test loss: 2.46740202606\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (8.06713962555,21.8757735742), test loss: 36.9584947228\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.44778418541,3.56789448305), test loss: 2.94353921711\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (8.77878952026,21.772198151), test loss: 28.9325351238\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (5.31457853317,3.55343811899), test loss: 3.21559140682\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (21.4631462097,21.6695095852), test loss: 37.6754177094\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.79277670383,3.53917355958), test loss: 3.0708888948\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (4.93466997147,21.5684879213), test loss: 30.9639454842\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.54319953918,3.52534773671), test loss: 3.22296606302\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (6.78776741028,21.4698160802), test loss: 34.1098670125\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.42071485519,3.51164670484), test loss: 2.46307300031\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (4.78146743774,21.371609589), test loss: 33.1486495376\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.83902788162,3.49820038263), test loss: 3.16843745261\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (3.14602613449,21.2753365729), test loss: 34.5500023842\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (3.69863986969,3.48479817964), test loss: 2.31684781313\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (17.74061203,21.1803244891), test loss: 38.0399677515\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.86901283264,3.47146128562), test loss: 3.10104030669\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (4.57074308395,21.0855006731), test loss: 32.9680812836\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.923315763474,3.45833411172), test loss: 2.52047877908\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (10.1311798096,20.9919856445), test loss: 38.9950647473\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.907655000687,3.44550405581), test loss: 3.16029933691\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (11.6170158386,20.9004442208), test loss: 30.1883121967\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.791273832321,3.43286611065), test loss: 2.60898469687\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (12.5217285156,20.8100158051), test loss: 38.4211451769\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.84488511086,3.42045788455), test loss: 2.79599804431\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (7.34608602524,20.7208623843), test loss: 28.6887008667\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.543890595436,3.40811882151), test loss: 2.46050075367\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (7.19623851776,20.6327915287), test loss: 37.1732130766\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.36720752716,3.39586230431), test loss: 2.90790556669\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (8.19476604462,20.5455152675), test loss: 29.1252428293\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (5.6078748703,3.38372588027), test loss: 3.16076939404\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (20.5934791565,20.4588261378), test loss: 37.840066278\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.09389674664,3.37170126037), test loss: 3.0559713006\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (2.90074110031,20.3734739166), test loss: 31.1411247969\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.949855446815,3.36006431802), test loss: 3.21079111993\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (10.3893604279,20.2901591747), test loss: 34.5416682303\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.26977205276,3.34851795774), test loss: 2.46815144941\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (5.82055854797,20.2071177859), test loss: 33.3769581556\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.29681015015,3.33718464395), test loss: 3.15158275068\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (6.58382606506,20.125746319), test loss: 34.6842587948\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (4.29519271851,3.32585961164), test loss: 2.29187428951\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (15.6277360916,20.0452954854), test loss: 38.3381449461\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.981011986732,3.31456905944), test loss: 3.09273239225\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (7.86020421982,19.9650282094), test loss: 33.2515341759\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (4.31030845642,3.30349416345), test loss: 2.48800769076\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (9.2553768158,19.8856960127), test loss: 39.1741649151\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.931378602982,3.29257237831), test loss: 3.17073780298\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (7.27007675171,19.8080352999), test loss: 30.4796175003\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.560151457787,3.28184217335), test loss: 2.59609018266\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (7.38515281677,19.731328009), test loss: 38.9753247976\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.08134031296,3.27132979071), test loss: 2.81335645318\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (4.14344787598,19.6556918915), test loss: 28.9307351589\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.36462473869,3.26084922415), test loss: 2.44931422621\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.98515033722,19.5809369568), test loss: 37.4568441629\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.46273541451,3.25040624514), test loss: 2.88673938513\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (8.89759731293,19.50675273), test loss: 29.3695394039\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (3.91476249695,3.24009018737), test loss: 3.10575091541\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (8.07499790192,19.4329546969), test loss: 38.0313546896\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.05662322044,3.22983178274), test loss: 3.03149523437\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.50253725052,19.3602645602), test loss: 30.9668883562\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.58645248413,3.2199224195), test loss: 3.17832403779\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (9.60901832581,19.2892760493), test loss: 34.9246750116\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.68053388596,3.21007019621), test loss: 2.44332719147\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (9.21928882599,19.2184338571), test loss: 33.4154949665\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.9092707634,3.20039589259), test loss: 3.13340355754\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (6.42467403412,19.1488931274), test loss: 34.9833878517\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.06053256989,3.19071947715), test loss: 2.28013192713\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (8.04616165161,19.0800862031), test loss: 38.4505699635\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.40316343307,3.18103673521), test loss: 3.12577611804\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (12.2837200165,19.0113187384), test loss: 33.5394159317\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (4.5436797142,3.1715375684), test loss: 2.50195452347\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (9.11965560913,18.9431882932), test loss: 39.5243048668\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.12537407875,3.1621623028), test loss: 3.17844883204\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (7.17354106903,18.8764306345), test loss: 30.5806451797\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.620461583138,3.15293781118), test loss: 2.58445078135\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.46877193451,18.8103657812), test loss: 39.0000865221\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.08065891266,3.14390196423), test loss: 2.78592964709\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.14247226715,18.7452578411), test loss: 29.3200671196\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.83174967766,3.1348784813), test loss: 2.44053798318\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (11.3649101257,18.6807886461), test loss: 37.7890550375\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.61878859997,3.12587586675), test loss: 2.86667302698\n",
      "run time for single CV loop: 7103.30512977\n",
      "\n",
      "MC # 3, Hype # hyp5, Fold # 6\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold6/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (388.101776123,inf), test loss: 202.457165146\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (334.785583496,inf), test loss: 402.11394043\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (43.3316802979,68.2276291332), test loss: 43.5053686142\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.97241139412,45.2097363145), test loss: 3.23887948692\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (102.206207275,56.8152217188), test loss: 41.6910917759\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.64783430099,24.2186140388), test loss: 3.48700166047\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (33.6160011292,52.8462874365), test loss: 41.0660225391\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (7.70874738693,17.2280354809), test loss: 3.38004646599\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (49.5048446655,50.7781556108), test loss: 43.3867019892\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.56566524506,13.7309592769), test loss: 3.58600310683\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (15.8189582825,49.5362838446), test loss: 39.073216486\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.11908459663,11.6342664006), test loss: 2.84019838274\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (19.3200645447,48.6294213468), test loss: 46.1936942101\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (4.0584321022,10.2381497158), test loss: 3.75175420642\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (23.8031730652,47.9403524779), test loss: 39.1636525631\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.18866753578,9.24501724659), test loss: 2.7114459753\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (52.6302986145,47.3909464397), test loss: 44.3662046909\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (5.15463638306,8.49756437102), test loss: 3.51642779112\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (29.1384048462,46.8680666641), test loss: 35.6307323456\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.87316083908,7.91398713177), test loss: 2.86807119846\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (32.4180984497,46.4568267718), test loss: 42.7531066418\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.686353981495,7.44633411762), test loss: 3.06418535113\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (19.6216621399,46.0773976155), test loss: 35.3288680077\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.99920034409,7.06089228423), test loss: 2.76942881942\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (34.5685501099,45.7396819635), test loss: 40.4753561497\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.09455943108,6.73616846245), test loss: 3.27824441195\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (15.8832302094,45.4297090775), test loss: 38.2483920574\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.53489565849,6.46077127721), test loss: 3.41154521108\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (98.8146362305,45.1425833086), test loss: 38.8685217381\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (6.4094543457,6.22501234956), test loss: 2.91997961998\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (27.4769439697,44.8412170687), test loss: 43.3771710396\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.06896209717,6.0204785909), test loss: 3.46526905596\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (41.023147583,44.5648281153), test loss: 38.1488396168\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (0.781912863255,5.83920758094), test loss: 2.73765494823\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (99.8853378296,44.2781762487), test loss: 43.7965945959\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.93992471695,5.6787572652), test loss: 3.49612216651\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (84.5364227295,43.9875218683), test loss: 32.9533662319\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.69810628891,5.53364225291), test loss: 2.74910780638\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (26.5874938965,43.6814167873), test loss: 40.3012269974\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.25186872482,5.40306272275), test loss: 3.22461457849\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (32.7990493774,43.3744069398), test loss: 29.9073215485\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.5804964304,5.28240222514), test loss: 2.59628841281\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (30.5130996704,43.0689105362), test loss: 38.4875404119\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.0515422821,5.17261398187), test loss: 3.21464787126\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (33.8242340088,42.7601098444), test loss: 34.6827991247\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (6.46684265137,5.07131311299), test loss: 3.09681489468\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (32.8332023621,42.4319516897), test loss: 35.7227762699\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.41474688053,4.97679450039), test loss: 3.10662710071\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (20.5387039185,42.1051646367), test loss: 37.5206064701\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.977481126785,4.8886191976), test loss: 3.33633108139\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (16.8980636597,41.7693017678), test loss: 32.5721987724\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.94434034824,4.80639285746), test loss: 2.41116638184\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (30.9169216156,41.4159989942), test loss: 39.1228028297\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.43932509422,4.72810261272), test loss: 3.52456115782\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (49.8376083374,41.0612718052), test loss: 30.5338373184\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.99353265762,4.654437388), test loss: 2.45067037344\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (23.3379211426,40.7055714565), test loss: 36.6537120342\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.9301109314,4.58390586672), test loss: 3.15640980005\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (25.4754104614,40.3476838291), test loss: 26.2410159588\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.49242115021,4.51682760492), test loss: 2.42918337286\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (18.0450649261,39.9858556394), test loss: 36.2644137859\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.77633631229,4.45346341375), test loss: 3.12466702461\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (41.0135726929,39.6104954009), test loss: 25.5410202026\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (5.16492652893,4.39266352385), test loss: 2.37322443724\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (10.4048652649,39.2337380598), test loss: 34.4855741024\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (3.80730056763,4.33468161753), test loss: 3.16233730316\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (24.5362625122,38.8597639261), test loss: 29.6250226974\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.36390018463,4.27940814937), test loss: 3.15411389768\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (14.4699382782,38.4774497508), test loss: 27.4218738079\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.76043760777,4.22616521276), test loss: 2.57032026947\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (23.8249015808,38.1018078108), test loss: 34.9197135448\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.00639569759,4.17501993968), test loss: 3.32182946205\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (18.0614509583,37.7366056216), test loss: 28.7735100269\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.55086553097,4.12568016767), test loss: 2.34850344658\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (17.5864696503,37.3760600226), test loss: 34.0942943573\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.713646769524,4.0783000778), test loss: 3.12613998652\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (11.177772522,37.0219096642), test loss: 26.1071047306\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.19011330605,4.03326231859), test loss: 2.4437525928\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.4234733582,36.6665138171), test loss: 34.7463497639\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.610683619976,3.98992251388), test loss: 3.13020862639\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (19.0029716492,36.3190187973), test loss: 24.6149098396\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.91421484947,3.94856687223), test loss: 2.46871329844\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (20.2432441711,35.9822058883), test loss: 34.8283741951\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.01815485954,3.90888146297), test loss: 3.0445099324\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (12.252202034,35.6475712486), test loss: 28.9831962109\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.43789839745,3.87048299553), test loss: 3.15313183963\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (19.783575058,35.3251615887), test loss: 35.082101059\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.40399098396,3.83329092165), test loss: 3.12834143937\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (31.4617519379,35.0186661016), test loss: 29.2322196722\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.48512136936,3.79745993947), test loss: 3.18735618591\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (31.3557777405,34.7178656374), test loss: 29.4541929007\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (5.1797914505,3.76294101348), test loss: 2.475159958\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (23.6034297943,34.4261401955), test loss: 32.5680510283\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.09770345688,3.72958023754), test loss: 3.22795301676\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (6.7438583374,34.1369456841), test loss: 26.0840696335\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.08460354805,3.69729602331), test loss: 2.23952463269\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (6.45642280579,33.8589637058), test loss: 33.3618191242\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.86637187004,3.66643555037), test loss: 3.11151635647\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (25.9477272034,33.5928522446), test loss: 25.2087423801\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (0.82313066721,3.63654969889), test loss: 2.45018788576\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.4180488586,33.3261357245), test loss: 34.454022789\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.10360431671,3.60764254294), test loss: 2.99709204733\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (18.5424442291,33.0689838536), test loss: 22.9400593758\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.22039794922,3.57936783405), test loss: 2.48189279139\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (2.97032737732,32.8231621984), test loss: 36.024683094\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.944244623184,3.55189753894), test loss: 3.16762699783\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (8.26610565186,32.581341051), test loss: 29.6852035999\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.77824854851,3.52547799611), test loss: 3.15134361386\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (13.4326210022,32.3474287514), test loss: 28.5110445976\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.02565550804,3.49947291736), test loss: 2.63333574533\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (41.9423789978,32.1153679002), test loss: 32.4005716801\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.21038413048,3.47428642198), test loss: 3.26982175112\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (32.5995407104,31.8911648618), test loss: 28.5370271206\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.26189422607,3.45014841044), test loss: 2.33081983328\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (18.9571990967,31.6736688135), test loss: 32.0986752033\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.16936159134,3.42658022129), test loss: 3.09574437737\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (20.1043815613,31.4569164156), test loss: 29.4941432238\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.59655988216,3.40373884146), test loss: 2.45951562822\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (21.2085533142,31.246909102), test loss: 34.8619533062\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.65728843212,3.38141429174), test loss: 3.09328071922\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (10.0286493301,31.0426639167), test loss: 25.2453977585\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.0795648098,3.35948066915), test loss: 2.38753376901\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (28.6348266602,30.84472407), test loss: 35.4795121193\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.82113981247,3.33810741575), test loss: 3.09937100112\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (29.0456581116,30.649129285), test loss: 29.0760017395\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (4.05325269699,3.31715172464), test loss: 3.01321192384\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.8414764404,30.4568995447), test loss: 36.4296108723\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.2362806797,3.29670033627), test loss: 3.15342364609\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (14.5446338654,30.2677553347), test loss: 29.9182578087\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.34891414642,3.27698547303), test loss: 3.15800052583\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (9.17248344421,30.0847460491), test loss: 30.3456817627\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.13705062866,3.25770807551), test loss: 2.41560000181\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (9.57748126984,29.9022602955), test loss: 31.5359250546\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.92162525654,3.23898298866), test loss: 3.12207084\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (4.16770410538,29.7244772839), test loss: 28.5585886002\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.835668921471,3.22053675239), test loss: 2.27744258344\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (9.79685020447,29.5500816001), test loss: 32.6663242817\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.35519742966,3.20240740599), test loss: 2.98662848771\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (15.0996437073,29.380707271), test loss: 25.8299779892\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.06846821308,3.18444701023), test loss: 2.4120240435\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (12.4913911819,29.2130658254), test loss: 34.2233664989\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.50417590141,3.16693182919), test loss: 2.91013253182\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (34.2536849976,29.0480905167), test loss: 30.2612342834\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.27521824837,3.14984590648), test loss: 2.96365146935\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (2.7308588028,28.8838828624), test loss: 35.1705553055\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.45124471188,3.13323858934), test loss: 3.08751140237\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.1956672668,28.7252761341), test loss: 28.9500113726\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.15223646164,3.11697412725), test loss: 3.02437125146\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (40.4580879211,28.5686291604), test loss: 34.2265701056\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.31160736084,3.10111581012), test loss: 2.69588198364\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (18.7572555542,28.4118769313), test loss: 30.7401520729\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.71985125542,3.08532994542), test loss: 3.27863549739\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.3492889404,28.259151972), test loss: 29.539816618\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.52068912983,3.06988545317), test loss: 2.255489254\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (6.51083230972,28.1105359695), test loss: 32.1642082214\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.726238310337,3.05452085001), test loss: 3.01461733282\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (20.0064907074,27.9632863874), test loss: 26.8523726463\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.80470347404,3.03961506371), test loss: 2.41162266731\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (25.2721214294,27.8170431988), test loss: 35.3881266594\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (5.63274097443,3.02492264934), test loss: 2.94694938362\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (20.1654624939,27.6720547472), test loss: 25.6033203125\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.01184511185,3.01041309037), test loss: 2.41295185387\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (10.7307796478,27.5306643857), test loss: 35.4164569378\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.29728364944,2.99626499583), test loss: 3.0320803225\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (6.40895652771,27.3917044545), test loss: 31.6232736111\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.344747364521,2.98242499488), test loss: 3.13716216385\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (14.4519205093,27.2520906395), test loss: 36.7361778021\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.22178602219,2.968798684), test loss: 3.06695244312\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (17.1765174866,27.1159126586), test loss: 30.1661651134\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.26270544529,2.95531744133), test loss: 3.07729715109\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (22.322429657,26.9821475908), test loss: 31.6839748859\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (4.89147424698,2.94195710282), test loss: 2.40935754776\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (8.58027172089,26.8492043042), test loss: 34.8352016687\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.71227574348,2.92878148266), test loss: 3.116191113\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (10.2423601151,26.7183441513), test loss: 29.8150976181\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.37525296211,2.91586656447), test loss: 2.38316277564\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (16.8555755615,26.5872002849), test loss: 33.5380021095\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.58509421349,2.90302242892), test loss: 2.97529044896\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (11.1871519089,26.4593545586), test loss: 26.2506921768\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.67072916031,2.89048549693), test loss: 2.4128709197\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (8.10682868958,26.334184625), test loss: 35.1514865398\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.616862475872,2.87829016248), test loss: 2.80976685882\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.7496442795,26.2071425701), test loss: 30.4588363647\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.976174175739,2.86622151359), test loss: 2.90910592973\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (16.8690185547,26.0829348887), test loss: 35.474062252\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.49379515648,2.85423107003), test loss: 3.04439594597\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (7.72263717651,25.9612365078), test loss: 28.4548371792\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.3828690052,2.84231063062), test loss: 3.04333540499\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (16.7148094177,25.8404800958), test loss: 30.909574461\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.85208296776,2.83050397604), test loss: 2.54103599787\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (12.9913797379,25.7205041892), test loss: 31.0443150043\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.02347898483,2.81885309112), test loss: 3.15177826285\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (26.1370239258,25.6009815962), test loss: 30.408622551\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.2756652832,2.80738238713), test loss: 2.17310771346\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (19.0074386597,25.4838116998), test loss: 32.7518218994\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.28391432762,2.79616668522), test loss: 2.91171744168\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (10.8151607513,25.3683299767), test loss: 28.0211874008\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.13890278339,2.78519912687), test loss: 2.39143066108\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (5.31388282776,25.2514218723), test loss: 36.2447021961\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.78297197819,2.77430821009), test loss: 2.8497834444\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (20.8387336731,25.1378711443), test loss: 25.8770422935\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.24992716312,2.7634357332), test loss: 2.36379234791\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (10.7475070953,25.0257056404), test loss: 36.2351960182\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.418989479542,2.75261642587), test loss: 3.02243844867\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (4.38778400421,24.9137353865), test loss: 28.6600878716\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.61364769936,2.74196326008), test loss: 2.95799243748\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (20.2248516083,24.8030055347), test loss: 32.6348385811\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.61236965656,2.73142593523), test loss: 2.59516337812\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (5.3638458252,24.6920634792), test loss: 31.5581869602\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.64809203148,2.72103511447), test loss: 3.1139895916\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.27551984787,24.5830478003), test loss: 32.2465399742\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.25968241692,2.71085130117), test loss: 2.39217302799\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (12.625951767,24.4762501269), test loss: 33.1973408222\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.83599126339,2.70079657391), test loss: 2.97510837913\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (27.6654777527,24.3683399732), test loss: 32.8739888191\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.15835750103,2.69085982513), test loss: 2.46630559564\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (7.16612625122,24.2622929919), test loss: 35.3248250008\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.92972326279,2.68091531631), test loss: 2.92745946944\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.897652626,24.15787744), test loss: 27.823137331\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.63395738602,2.67103036537), test loss: 2.44963446409\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (3.42223834991,24.0536768555), test loss: 36.1113834381\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.675418794155,2.66138825682), test loss: 2.85861997008\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (17.203163147,23.9503367507), test loss: 30.8125637054\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.858813941479,2.65175030587), test loss: 2.92669496238\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (32.9246368408,23.8467327885), test loss: 36.6035560846\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.822908997536,2.64219760572), test loss: 3.01306111217\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (22.0597610474,23.7448129627), test loss: 29.8211195469\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.85524618626,2.63282466148), test loss: 3.00133807957\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (10.6641016006,23.6446381894), test loss: 32.8036240101\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.713569521904,2.62351151327), test loss: 2.42484768331\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (10.3265352249,23.5437280462), test loss: 30.8615246296\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.581185460091,2.61435891935), test loss: 3.04232197404\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (7.54620885849,23.4445411534), test loss: 31.5177605152\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.765357911587,2.60527717088), test loss: 2.20663717389\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.9637298584,23.3463469034), test loss: 34.3235147238\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.00801110268,2.59624967813), test loss: 2.9389566645\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (8.69055366516,23.2487639384), test loss: 29.6439729214\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.46152424812,2.58733783248), test loss: 2.44622997344\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (14.9711551666,23.1510737413), test loss: 36.4090310097\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.02931499481,2.57844542925), test loss: 2.86358426511\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (22.763053894,23.0544669722), test loss: 29.450481391\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.981424927711,2.56955878798), test loss: 2.47553713322\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (11.7573871613,22.9578480947), test loss: 36.8519510269\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.9296861887,2.56084855829), test loss: 3.02988933921\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (6.55711889267,22.863237374), test loss: 29.1199336767\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.00223660469,2.55218860017), test loss: 2.94610762\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (9.42395305634,22.7686053032), test loss: 34.5206816673\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.05509102345,2.54379230426), test loss: 2.67023635507\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (7.21366596222,22.6749699766), test loss: 32.1878742695\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.78056550026,2.53543450459), test loss: 3.09404231012\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (5.69809913635,22.5817442541), test loss: 33.6663016319\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.34963274002,2.52707006798), test loss: 2.36461333185\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (4.4395866394,22.4897509185), test loss: 34.1113630295\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.820734620094,2.51870123306), test loss: 2.8787024349\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (11.2493562698,22.3978156713), test loss: 33.9842424393\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.447540462017,2.51039382093), test loss: 2.54630777836\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (18.6500473022,22.3059809866), test loss: 36.5406134129\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.82079815865,2.50212573357), test loss: 2.91789475977\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (6.28436851501,22.2145548174), test loss: 30.0220279694\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.597006559372,2.49400022057), test loss: 2.52972344458\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.2986073494,22.1249436268), test loss: 37.4405752659\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.818764030933,2.48602277231), test loss: 2.87739246786\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (27.1310958862,22.0357339033), test loss: 35.4326444626\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.09147834778,2.47819283943), test loss: 3.17605285943\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (6.35822963715,21.9459582489), test loss: 39.4250211716\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.13629353046,2.47035041649), test loss: 3.01972446144\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (5.82985877991,21.8578605442), test loss: 30.1081760168\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.85742688179,2.46252158401), test loss: 2.99600388706\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (9.01238632202,21.7704628378), test loss: 35.1696686745\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.33042323589,2.45465843718), test loss: 2.44650534391\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (12.8201694489,21.6830113742), test loss: 33.5471633911\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (4.12294006348,2.44692406419), test loss: 3.01097725928\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.50922679901,21.5958151112), test loss: 32.4165818214\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (3.65228414536,2.43922884486), test loss: 2.32178658247\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (7.13501262665,21.5091156579), test loss: 36.545124054\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.90790307522,2.4316447537), test loss: 3.11579919159\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (8.36599731445,21.4235198868), test loss: 29.6693226814\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.43464422226,2.42414433973), test loss: 2.42610862553\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (6.25469112396,21.338636557), test loss: 43.7807984114\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.109554290771,2.41676984247), test loss: 2.98813138604\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (9.17386627197,21.2535052598), test loss: 27.4134309769\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.3050583601,2.40943050356), test loss: 2.57950786054\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (19.2658729553,21.1695757678), test loss: 38.3327617168\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.820300936699,2.40203456012), test loss: 3.03909270763\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (15.2061100006,21.0859654124), test loss: 29.8429758549\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (3.32358932495,2.39467178707), test loss: 2.98091209382\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (3.7299618721,21.0026796737), test loss: 34.031685257\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.54842424393,2.38743535976), test loss: 2.6305221796\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (12.7132873535,20.9198075466), test loss: 32.2126591206\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.51822400093,2.38025665003), test loss: 3.033769086\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (8.18365287781,20.8368733646), test loss: 33.7255674839\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.92911636829,2.37311796117), test loss: 2.31820463538\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.04752922058,20.7553157374), test loss: 34.4860013485\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.39898896217,2.36604606719), test loss: 2.93541372418\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (5.78409481049,20.6744164862), test loss: 35.484539938\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.731811225414,2.35906550406), test loss: 2.61415916085\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (5.23989534378,20.5928089556), test loss: 38.7735049248\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.366241842508,2.35212126339), test loss: 2.94325074553\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (6.79309511185,20.512611282), test loss: 29.2781184673\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (3.00746202469,2.34519795615), test loss: 2.40207734406\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (8.3315486908,20.4329423002), test loss: 39.9021151543\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.57887417078,2.33830889404), test loss: 3.03249851167\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (10.2071313858,20.3533009135), test loss: 32.181827879\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.94326162338,2.33152463129), test loss: 2.84269889295\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (8.65304279327,20.2740535907), test loss: 40.5750377178\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.766881942749,2.32472416319), test loss: 3.04433571696\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (16.5895347595,20.1952944506), test loss: 31.1814430714\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.30328083038,2.31798227332), test loss: 3.02410038412\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (6.54629611969,20.1169372746), test loss: 36.5335428715\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.19175601006,2.31126353054), test loss: 2.53980886936\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (7.58937358856,20.0392637741), test loss: 34.9041314125\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.05955576897,2.30465219955), test loss: 3.04270781577\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.97693729401,19.9615190047), test loss: 35.1993614674\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.18967795372,2.2981468051), test loss: 2.47093297541\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (11.7531089783,19.884898872), test loss: 36.5477588654\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.964435994625,2.29164278749), test loss: 2.91128793657\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.10587120056,19.8084611921), test loss: 33.3661637306\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.448402523994,2.28514384584), test loss: 2.63909649849\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (4.31790447235,19.7326100803), test loss: 38.8989340305\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.62629008293,2.27873277358), test loss: 2.80454507321\n",
      "\n",
      "MC # 3, Hype # hyp5, Fold # 7\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold7/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (314.739074707,inf), test loss: 170.686751938\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (317.116821289,inf), test loss: 380.41139679\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (23.7909450531,59.6411290293), test loss: 38.1999154091\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.90563154221,47.2234280438), test loss: 3.156606552\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (35.2522735596,53.0451938634), test loss: 32.7318792343\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.45416021347,25.244023608), test loss: 3.06736693382\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (19.2037277222,50.6675487213), test loss: 38.8727748156\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.40533590317,17.9135621253), test loss: 3.33539843857\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (30.4899978638,49.5449925033), test loss: 33.0096190453\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.55852842331,14.2552229088), test loss: 3.37315226793\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (29.764957428,48.7580494191), test loss: 41.2089448452\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.35261321068,12.0644462621), test loss: 2.99471040964\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (60.9419250488,48.1789139818), test loss: 36.1488277435\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.8351855278,10.6076510415), test loss: 3.3722384572\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (26.2910614014,47.7043789766), test loss: 42.7150871515\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.82155156136,9.55961905164), test loss: 2.84909325242\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (43.3954811096,47.2951824072), test loss: 35.6623943806\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.50009727478,8.77298346231), test loss: 3.50595613718\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (35.3738021851,46.8788141527), test loss: 38.6043012619\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.45494318008,8.15683462765), test loss: 2.75936481953\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (30.8315391541,46.5004445713), test loss: 34.3865029097\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (3.20391774178,7.65857171427), test loss: 3.36255348623\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (76.671295166,46.1972332049), test loss: 36.7341283321\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.69232606888,7.25271883152), test loss: 2.75096087754\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (51.6714706421,45.8566470735), test loss: 37.7255990982\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.23474168777,6.91372226555), test loss: 3.41803354025\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (23.2968788147,45.53369519), test loss: 34.8498577118\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.21486103535,6.62404784212), test loss: 2.87659969926\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (104.364929199,45.2152266269), test loss: 34.0536951542\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (7.06321716309,6.37699028016), test loss: 2.98460198045\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (68.7678527832,44.86400488), test loss: 28.2234715939\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.26780891418,6.15951628448), test loss: 3.06070569754\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (65.9282684326,44.545762986), test loss: 33.8484577656\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.51853609085,5.96816944969), test loss: 3.0619099319\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (13.0753488541,44.2030382372), test loss: 29.4025002003\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.66753578186,5.79783731848), test loss: 3.22492893338\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (27.1437530518,43.8838288607), test loss: 35.7535225153\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.51717281342,5.64656632711), test loss: 2.73783281446\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (28.9969959259,43.5354357391), test loss: 33.7602974892\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.75877523422,5.50954526464), test loss: 3.28068829179\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (62.4937820435,43.1809790874), test loss: 36.438345623\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (6.94575309753,5.38391126059), test loss: 2.62262290418\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (22.9314098358,42.8076513737), test loss: 31.4399952412\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.82895565033,5.26936565904), test loss: 3.22384675443\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (27.8289318085,42.4211443163), test loss: 30.1757139683\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.25566959381,5.16366658738), test loss: 2.47824364305\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (14.7192316055,42.0295815997), test loss: 30.6596497536\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.52207553387,5.06415767268), test loss: 3.10397393107\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (21.4108428955,41.6457159389), test loss: 29.3650563955\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.22234964371,4.97222489009), test loss: 2.56780516356\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (21.7840957642,41.2438849313), test loss: 31.7826985359\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.17722582817,4.88680797369), test loss: 3.05187821984\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (38.0151824951,40.8366968248), test loss: 25.4120027781\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.38187837601,4.80713885667), test loss: 2.52060484886\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (13.2116355896,40.4229266684), test loss: 27.7450104713\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.86428141594,4.73063849569), test loss: 2.86979158223\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (29.4166984558,39.9997230921), test loss: 22.7573638678\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.87649965286,4.65832317601), test loss: 2.86901404858\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (17.8518543243,39.573323418), test loss: 27.4981693268\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.09908628464,4.58985041706), test loss: 2.81595914662\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (19.1191444397,39.1524256971), test loss: 23.7351211548\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.21168923378,4.52363097762), test loss: 2.99105470777\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (34.7886505127,38.743539003), test loss: 28.6145800114\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.76834237576,4.46145045427), test loss: 2.50847988427\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (26.6422672272,38.3293348956), test loss: 27.4293746948\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.45911765099,4.40209624775), test loss: 3.17679835558\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (20.0794334412,37.9258976924), test loss: 29.6997483492\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (0.960330545902,4.345139614), test loss: 2.34170056581\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (61.9143447876,37.5336325719), test loss: 28.0958350182\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.33628320694,4.2913852181), test loss: 3.0148786366\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (41.7114181519,37.1374435854), test loss: 27.236725235\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.87262439728,4.23920225785), test loss: 2.56871725917\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (39.3717384338,36.7646585967), test loss: 28.4362730026\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.19268512726,4.18910995581), test loss: 2.9698708415\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (11.7009849548,36.395110351), test loss: 27.1066913128\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.13296771049,4.14095325866), test loss: 2.49765427411\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (20.1456108093,36.0422353675), test loss: 27.7744069815\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.58007144928,4.0954614255), test loss: 2.92227076292\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (29.6147041321,35.6957725062), test loss: 23.4712546349\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.72013068199,4.05173947249), test loss: 2.33756273091\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (27.0266647339,35.3632510708), test loss: 26.5888611794\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.51872587204,4.00970231109), test loss: 2.84432158768\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (29.505115509,35.0350915963), test loss: 22.5842251301\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.46522021294,3.96948776357), test loss: 2.91113684773\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (15.7635211945,34.7157057856), test loss: 27.5381066084\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.87903428078,3.9307180512), test loss: 2.83342983276\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (14.5465593338,34.4109432219), test loss: 23.3138353229\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.11056280136,3.89287406596), test loss: 3.01030088961\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (11.4737854004,34.1170361824), test loss: 28.8715121269\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.47758024931,3.85646053437), test loss: 2.38463564515\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (31.201128006,33.8270032818), test loss: 25.4785983086\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.08737778664,3.82177858406), test loss: 3.17249241769\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.9843845367,33.5483573807), test loss: 29.0173478842\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.57819318771,3.78874831284), test loss: 2.29573135972\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (11.348897934,33.2801369411), test loss: 26.3159420729\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.05721855164,3.75653389398), test loss: 2.93033007532\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.6183509827,33.0133757558), test loss: 28.4319871902\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.11746692657,3.72533358868), test loss: 2.50182008147\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (13.6128444672,32.7559321679), test loss: 28.1850901365\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.17594003677,3.69523956969), test loss: 3.00029797256\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (20.9554195404,32.5082897814), test loss: 27.0023727417\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.58047604561,3.66549649063), test loss: 2.60467956364\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (20.2531318665,32.2667145682), test loss: 26.7049041271\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.10705637932,3.6371316636), test loss: 2.80136688948\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (21.7899589539,32.0276538746), test loss: 23.887087059\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.87332844734,3.60961793597), test loss: 2.48836314976\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (14.9469051361,31.7977850784), test loss: 27.1305419207\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.710460186005,3.58308821584), test loss: 2.76903442442\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (47.5035972595,31.5759328949), test loss: 25.5946249723\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.25993967056,3.55763464484), test loss: 2.94470745772\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (43.0033416748,31.352601785), test loss: 27.8641560555\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.82077407837,3.53249879545), test loss: 2.60655050725\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (23.3587169647,31.1408144022), test loss: 23.3383875847\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.24240410328,3.50789782172), test loss: 3.00813544989\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.959186554,30.9307158104), test loss: 31.5243086338\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.03452730179,3.48388596889), test loss: 2.42589081675\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (19.3108444214,30.7268536472), test loss: 25.5942552924\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (4.86996078491,3.46085382131), test loss: 3.21006338596\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (26.0381011963,30.5265328138), test loss: 31.7908265591\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.77531695366,3.43841551458), test loss: 2.3444752723\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.5543069839,30.3327175123), test loss: 26.0014428139\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.64821815491,3.41659147371), test loss: 2.88425320089\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (29.360326767,30.1402832697), test loss: 31.2832291126\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.56257581711,3.39542766551), test loss: 2.50870783925\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (12.4174251556,29.9507504208), test loss: 27.779569149\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.54380750656,3.37468659476), test loss: 2.91118099689\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (10.2628498077,29.7687921104), test loss: 27.3544849157\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.39534068108,3.35409923785), test loss: 2.60718339086\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.0726928711,29.5897619676), test loss: 26.6566224098\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.390973031521,3.33403400219), test loss: 2.7139803648\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (40.192653656,29.4114488351), test loss: 25.4037951946\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.16167259216,3.31462761513), test loss: 2.77132572383\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (11.420211792,29.2382276064), test loss: 27.5684840679\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.11819148064,3.29595640137), test loss: 2.86290007234\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (10.3588314056,29.0703923684), test loss: 23.5343588352\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.857332110405,3.27758244055), test loss: 2.92946798801\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (13.87495327,28.9007686031), test loss: 28.4706819057\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.99514722824,3.25954302985), test loss: 2.50985122174\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (13.8984470367,28.7368327251), test loss: 26.434247005\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.0928645134,3.24190602949), test loss: 2.97779765427\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (24.0522384644,28.5766966526), test loss: 30.6218190193\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.26371312141,3.2242469893), test loss: 2.34686127156\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.8121118546,28.4184112953), test loss: 27.7695160866\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.26413464546,3.20721365166), test loss: 3.08716003895\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.9418544769,28.2600843016), test loss: 30.3131921768\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.02059626579,3.19047759176), test loss: 2.26632296145\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.2319383621,28.1072882207), test loss: 26.6202686787\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.464184939861,3.17424313631), test loss: 2.87475343943\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (44.8974838257,27.9575920849), test loss: 33.3990149498\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.70390033722,3.15851938792), test loss: 2.63811991811\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (38.1350326538,27.8069950222), test loss: 28.3939632416\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (4.63167715073,3.14284696225), test loss: 2.85237805247\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (17.9621429443,27.661456276), test loss: 26.8606323719\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.07929265499,3.12730510606), test loss: 2.59864604473\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (13.8726539612,27.5168716317), test loss: 27.8474863529\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.3725130558,3.11204499245), test loss: 2.65400855541\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (21.2635250092,27.3742908738), test loss: 25.5701207161\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (4.93031692505,3.09720804826), test loss: 2.88831324875\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (21.602640152,27.2337509905), test loss: 30.2078945398\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.75392985344,3.08264783909), test loss: 2.84620659649\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.1249418259,27.0963064671), test loss: 24.7749073505\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.75534999371,3.0683876039), test loss: 2.91867232472\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (30.4147987366,26.9596811764), test loss: 30.7562297344\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.01354074478,3.05448020634), test loss: 2.48659691364\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (11.2645778656,26.8242137073), test loss: 27.4261303425\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.6704416275,3.04075317555), test loss: 3.03421625197\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.55926322937,26.6932833709), test loss: 32.0595797062\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.98780298233,3.0269731902), test loss: 2.40417715162\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (10.3316488266,26.5626349692), test loss: 28.2467158079\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.418350815773,3.01347316343), test loss: 3.03710780144\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (36.5181808472,26.4323513618), test loss: 28.7995220184\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.85468685627,3.00028458561), test loss: 2.23775520027\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (9.89806556702,26.3045981845), test loss: 30.3000537872\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.63129043579,2.98751319764), test loss: 2.97335489094\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (9.1762342453,26.1805119411), test loss: 28.5786811829\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.876942157745,2.97490050459), test loss: 2.5413122043\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.909444809,26.0542346719), test loss: 30.4477296591\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.83751380444,2.96245025888), test loss: 2.82982540727\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (12.0871076584,25.9323023309), test loss: 25.7875950336\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.71043252945,2.95017156026), test loss: 2.50538990796\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (20.2808380127,25.8116763365), test loss: 29.2811920404\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.14460945129,2.93782154761), test loss: 2.75420360267\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (9.80043029785,25.6918647714), test loss: 25.5726467609\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.20033931732,2.92580892329), test loss: 2.81692520976\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (12.9896678925,25.5712922636), test loss: 31.042287302\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.88522911072,2.9139240563), test loss: 2.81838242263\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (11.4209880829,25.4548894786), test loss: 25.6359645367\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.423170655966,2.90234483858), test loss: 2.97508816421\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (42.7088851929,25.3395973564), test loss: 34.3577460289\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.28702521324,2.8910788835), test loss: 2.58719132543\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (30.530872345,25.2243476473), test loss: 28.3963514566\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (4.07424926758,2.87980597548), test loss: 3.05349939167\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (11.691570282,25.1113307788), test loss: 31.6199089527\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.01516366005,2.86853639456), test loss: 2.25247853249\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (13.1273841858,24.9992585914), test loss: 30.6275394917\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.60174846649,2.85744596143), test loss: 3.03167588711\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (22.1635818481,24.8875819968), test loss: 30.0949513435\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (5.16804504395,2.84657195373), test loss: 2.36858091652\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (18.3997535706,24.777391631), test loss: 31.7656746387\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.75231432915,2.83583597837), test loss: 2.9243286103\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (13.504486084,24.6687297466), test loss: 28.8206322193\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.50987565517,2.82527848347), test loss: 2.51405818462\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (30.7086982727,24.5609075907), test loss: 33.0660097599\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.64450645447,2.81493392745), test loss: 2.79123062342\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (7.08217906952,24.4535916111), test loss: 25.6023234367\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.85802197456,2.80468017062), test loss: 2.45683036745\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (5.7103395462,24.3493218209), test loss: 30.2592568398\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.922053933144,2.79429812531), test loss: 2.78975041956\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (9.17419433594,24.2444024106), test loss: 25.2050656796\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.438415080309,2.78408699741), test loss: 2.93244793713\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (27.5100078583,24.1398283325), test loss: 32.4193105221\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.45163583755,2.77402117886), test loss: 2.91171665788\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (8.92059135437,24.0366640098), test loss: 26.8145802498\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.22257268429,2.76424372738), test loss: 3.0795170486\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (7.04613113403,23.9361853152), test loss: 32.4513712406\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.862071633339,2.75455209298), test loss: 2.45561273247\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (8.80005931854,23.8338538555), test loss: 29.2732590199\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.41339433193,2.74496799395), test loss: 3.12770286798\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (9.82681846619,23.7347878959), test loss: 32.3978420734\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.24166846275,2.73545321978), test loss: 2.27627021074\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (17.5065937042,23.6360756399), test loss: 31.7893032551\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.15330219269,2.72587166782), test loss: 2.98659477532\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (7.90486955643,23.5377989964), test loss: 32.089666748\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.91632294655,2.7164731377), test loss: 2.43195066601\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (14.440618515,23.4387645726), test loss: 33.203965044\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.41496968269,2.7071341077), test loss: 3.0644335568\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (10.7284240723,23.3427406246), test loss: 29.22261796\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.458026766777,2.69797529749), test loss: 2.54509970695\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (30.1129665375,23.2471394369), test loss: 36.6684757233\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.58617162704,2.68901388943), test loss: 3.07429742813\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (21.6685733795,23.1520873574), test loss: 25.1970638275\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (3.44301652908,2.68001315435), test loss: 2.35001550615\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (8.67045307159,23.0579900839), test loss: 31.4093809605\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.12413644791,2.67094210001), test loss: 2.83815339208\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (12.0471248627,22.9645887947), test loss: 25.7323569775\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (3.58690905571,2.66199025767), test loss: 2.79695700854\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (19.9862689972,22.8711268874), test loss: 33.6550335407\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (4.74446964264,2.65313986964), test loss: 2.9206718117\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (16.1717720032,22.7787834964), test loss: 28.3653356075\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.85027885437,2.64439127904), test loss: 3.00672842264\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (16.0162315369,22.6873461623), test loss: 34.260644865\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.39308929443,2.63575727749), test loss: 2.55713126957\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (28.966342926,22.5966919771), test loss: 31.147630167\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.55985999107,2.62730345897), test loss: 3.22766773403\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (4.7484664917,22.5064921952), test loss: 33.4796210766\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.99851846695,2.61892008692), test loss: 2.33411981165\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (5.59002590179,22.418148943), test loss: 31.531803894\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.19558870792,2.61040511529), test loss: 2.96143410206\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (8.40721702576,22.329130027), test loss: 33.3760222435\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.486500740051,2.60203741157), test loss: 2.55374625474\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (20.9277839661,22.2404011768), test loss: 33.7281352639\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.973940730095,2.59375650603), test loss: 3.01982277632\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (5.11219978333,22.1524806445), test loss: 29.5186522722\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.46386742592,2.58568099424), test loss: 2.70036512166\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (5.71246671677,22.0665515917), test loss: 31.9515908241\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.17745757103,2.57767379836), test loss: 2.76437014639\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (6.60265922546,21.9793894217), test loss: 28.9779331684\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.14369547367,2.56976196643), test loss: 2.7803878054\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (7.85063838959,21.8944917281), test loss: 32.1738280296\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.79018497467,2.56186887935), test loss: 2.8496532172\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (17.6856994629,21.8098271093), test loss: 26.3529558182\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.10189425945,2.55393764473), test loss: 2.89290739894\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (7.17244291306,21.725220821), test loss: 32.2232123375\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.39936554432,2.54612320072), test loss: 2.58126398176\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (15.4852991104,21.6402080359), test loss: 28.6059904099\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.46359801292,2.53837322486), test loss: 2.95705218613\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (10.5127639771,21.5571444584), test loss: 35.9769882202\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.449106216431,2.53074353129), test loss: 2.4756850034\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (21.0369110107,21.4743183873), test loss: 32.6382082939\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.33754467964,2.52330078915), test loss: 3.2725141108\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (13.1237030029,21.3921637639), test loss: 33.4257683277\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.38528203964,2.51584735693), test loss: 2.41006556451\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (6.59832572937,21.3106486306), test loss: 33.3653219938\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.889921009541,2.50834266485), test loss: 3.00142626166\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (11.1812086105,21.2294767964), test loss: 32.4075941086\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (3.32580590248,2.50095459977), test loss: 2.55765437931\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (16.6747322083,21.1483689095), test loss: 33.7701203346\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (4.27255678177,2.49362366192), test loss: 2.91013641059\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (15.2379932404,21.0678614835), test loss: 31.1340910435\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.59823203087,2.48637465576), test loss: 2.64539839327\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (14.8268384933,20.9880885857), test loss: 32.6479614735\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.32325673103,2.47918982573), test loss: 2.77144590616\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (26.3133926392,20.9088508116), test loss: 32.5973405361\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.54277408123,2.47217157626), test loss: 2.96298871338\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (4.04903841019,20.8301815832), test loss: 33.8381219864\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (3.00816249847,2.46518179366), test loss: 2.91538094729\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.71007966995,20.7525733515), test loss: 27.3314389706\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.18014240265,2.45808035743), test loss: 2.89584655762\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (6.61595821381,20.6745627182), test loss: 34.0498809576\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.492677271366,2.45109579174), test loss: 2.65222325027\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (17.8127040863,20.5967277894), test loss: 29.0400228024\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.07456088066,2.44416597972), test loss: 3.04707096219\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (4.30160474777,20.5195891682), test loss: 35.0366154671\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.78788602352,2.4373917652), test loss: 2.46921575665\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (5.16017246246,20.4437259653), test loss: 32.166639185\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.47347652912,2.43066230139), test loss: 3.16292648911\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (5.52186727524,20.3672469375), test loss: 33.5907714844\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.01408219337,2.42401409576), test loss: 2.40266853273\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (5.70462036133,20.292259691), test loss: 34.3522696018\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.31068623066,2.41736182541), test loss: 3.06010604799\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (16.0837860107,20.2175748748), test loss: 31.7506386757\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.02863311768,2.41068017355), test loss: 2.61609871387\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (8.9288482666,20.1427596049), test loss: 35.5495290756\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.10597705841,2.40408840639), test loss: 2.95276534855\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (11.9082870483,20.0678715795), test loss: 29.0518743038\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.64017677307,2.39754366822), test loss: 2.60532204509\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (10.9968099594,19.9942345146), test loss: 34.023789072\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.492695868015,2.39107507265), test loss: 2.70808695406\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (21.192150116,19.9209638995), test loss: 31.6972451448\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.32869982719,2.38476581985), test loss: 3.14116222858\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (7.92049980164,19.8480773811), test loss: 35.3884411097\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.78006672859,2.37843434994), test loss: 2.95528572798\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.11509370804,19.775871137), test loss: 27.7204442978\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.718258738518,2.37204837042), test loss: 2.95712028444\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.11092853546,19.7037642828), test loss: 35.600204134\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.81458711624,2.36577091677), test loss: 2.68673390895\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (13.2859477997,19.6319065929), test loss: 30.9368537426\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.66746091843,2.3595180251), test loss: 3.05388622582\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (13.3804855347,19.5603703276), test loss: 37.1486055374\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.43408298492,2.35334439357), test loss: 2.33850399852\n",
      "\n",
      "MC # 3, Hype # hyp5, Fold # 8\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold8/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (401.578582764,inf), test loss: 226.807392502\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (232.772735596,inf), test loss: 290.871858978\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (54.3257675171,87.6640451117), test loss: 44.6962192059\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.83997917175,24.2660555884), test loss: 3.86484033763\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (23.2715950012,65.4719019146), test loss: 30.8961342096\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.53586435318,13.9185872334), test loss: 2.85287147164\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (40.3096313477,57.7287880913), test loss: 42.3105650902\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.84384298325,10.4110494395), test loss: 3.8572793901\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (25.8801498413,53.7290569857), test loss: 34.8758183002\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.02790164948,8.64796669897), test loss: 3.52444411516\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (18.9834899902,51.1672435758), test loss: 39.6986753941\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.16729712486,7.58900074572), test loss: 3.65301052928\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (50.9405975342,49.3460211207), test loss: 37.2953985691\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (4.46724414825,6.87088036572), test loss: 3.66690927148\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (25.491558075,47.871866836), test loss: 35.8764277458\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.55494618416,6.35195125704), test loss: 2.94205382168\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (42.8926086426,46.6428660021), test loss: 41.2181490898\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.01643061638,5.95778184434), test loss: 3.70530304611\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (24.4174118042,45.6025010614), test loss: 34.6731652737\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.03759860992,5.64097646382), test loss: 2.60318753719\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (69.0848464966,44.6997616338), test loss: 38.3597671986\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.58583927155,5.3856044757), test loss: 3.82452530265\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (36.063999176,43.8379849264), test loss: 30.929642725\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (0.486634910107,5.1746930935), test loss: 2.52253606915\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (23.1540374756,43.033211165), test loss: 38.6601474524\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.18098783493,4.99411117067), test loss: 3.49823969603\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (3.97407341003,42.2182495863), test loss: 29.0861999035\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.40522289276,4.83676460656), test loss: 2.84074754715\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (18.294052124,41.3793890013), test loss: 37.5268619299\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.96826136112,4.69806532184), test loss: 3.51438602507\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (28.0536079407,40.5717170039), test loss: 26.4405516386\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.03104972839,4.57157992184), test loss: 2.68631700724\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (42.0089874268,39.8008837035), test loss: 34.2103822947\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.7161898613,4.45909550598), test loss: 3.44917742014\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (25.4571475983,39.041878415), test loss: 23.4462198257\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.23333609104,4.35831852358), test loss: 2.50793687552\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (25.3932342529,38.3227172242), test loss: 33.9798188686\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.64630627632,4.26664093485), test loss: 3.38761703968\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (22.2273445129,37.6169035549), test loss: 29.4436282158\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.12304365635,4.18185529025), test loss: 3.37897796035\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (22.201915741,36.9450799169), test loss: 32.2049719572\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.03499746323,4.10401681217), test loss: 3.37438612282\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (19.011631012,36.3193728243), test loss: 30.141414547\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.46600270271,4.03004485668), test loss: 3.38579705954\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (19.8149604797,35.7263546375), test loss: 27.9979119301\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.32939481735,3.96230387224), test loss: 2.72814981937\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (10.606174469,35.1549227398), test loss: 34.573080349\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.78637349606,3.90009019868), test loss: 3.5209363699\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (21.4374389648,34.6214695557), test loss: 29.081175065\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.797257900238,3.84232006919), test loss: 2.55991722345\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (27.7655487061,34.1049699574), test loss: 33.7300187111\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.734945058823,3.7878015477), test loss: 3.44175742865\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (13.7635231018,33.6138550578), test loss: 27.1310934544\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.93231940269,3.736779611), test loss: 2.453098014\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (13.5161514282,33.1594333932), test loss: 34.6293074965\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.50480937958,3.68741119569), test loss: 3.38958022296\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (15.9253864288,32.7261142913), test loss: 27.5009257793\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.40089905262,3.64108633654), test loss: 2.70078854263\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (18.1963424683,32.307604865), test loss: 35.5935727596\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.67870128155,3.59807402679), test loss: 3.37626445293\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (5.82231664658,31.915081885), test loss: 28.3295152664\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.11580741405,3.55758627379), test loss: 2.8466768384\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (29.9998054504,31.5353721453), test loss: 31.4316290855\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.10136473179,3.51888406172), test loss: 3.21408416033\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (25.4282035828,31.1703626299), test loss: 24.2123535156\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.71510982513,3.48215849235), test loss: 2.5779964596\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (5.81571960449,30.8289439016), test loss: 33.4988842249\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.19723296165,3.44610472449), test loss: 3.33594885767\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (16.0302009583,30.5017524415), test loss: 29.8941003799\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.29181468487,3.41173195339), test loss: 3.34396786392\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (31.0620498657,30.182898126), test loss: 34.5155322552\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.834201395512,3.37943906106), test loss: 3.35624295771\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (6.73245763779,29.8799762949), test loss: 31.0562650204\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.38306593895,3.34882314318), test loss: 3.36743188351\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (39.9977798462,29.5872764151), test loss: 28.293988657\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.9756565094,3.31938547078), test loss: 2.72548573315\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (22.3443756104,29.3016559678), test loss: 32.5683639526\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.04690742493,3.29090066868), test loss: 3.45498981923\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (7.03438663483,29.0318500792), test loss: 28.9652968168\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.73042416573,3.26272944712), test loss: 2.42956786752\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (11.0816574097,28.7722344681), test loss: 34.449825263\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.94878196716,3.23569048892), test loss: 3.35842338502\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (13.414358139,28.5163928318), test loss: 28.0904226303\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.604619562626,3.20988627238), test loss: 2.33324973583\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (13.6065187454,28.2727421403), test loss: 34.7797292709\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.45921802521,3.18545270174), test loss: 3.29674270153\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (6.69692516327,28.0348746233), test loss: 28.8877277851\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.11702346802,3.1618156769), test loss: 2.65649126619\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (17.0129699707,27.8024561479), test loss: 32.9712013721\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.838998794556,3.13860906953), test loss: 3.28777441084\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (24.8650588989,27.5807212657), test loss: 27.8916231632\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.90128040314,3.11577273574), test loss: 2.74768829644\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (34.1226959229,27.3657152737), test loss: 31.5164134979\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.54943990707,3.093466047), test loss: 3.11606719792\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (5.14656257629,27.1507775453), test loss: 25.0844807148\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.19592213631,3.07203439977), test loss: 2.63291603923\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.1251068115,26.9467584802), test loss: 33.9726807833\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.980963170528,3.05171725493), test loss: 3.22302903235\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (53.3526496887,26.7475654793), test loss: 30.1337249517\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.03115916252,3.0320527529), test loss: 3.28956442028\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (9.62866020203,26.5493089572), test loss: 33.0121900558\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.68182253838,3.01260489044), test loss: 3.25426996648\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (15.9153499603,26.3604942979), test loss: 29.9618287325\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (5.93281555176,2.9934997608), test loss: 3.29447576702\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (5.35399627686,26.1760819396), test loss: 29.2010951042\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.355934381485,2.97448497876), test loss: 2.63824543506\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (7.02438688278,25.9908977759), test loss: 34.2151130199\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.17253160477,2.95631713174), test loss: 3.32881352156\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (10.1793861389,25.8147877441), test loss: 30.0868178844\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.6060128212,2.93896207723), test loss: 2.30615782887\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (48.766708374,25.6431168513), test loss: 35.7467159986\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.41655063629,2.92219187297), test loss: 3.32637704015\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (22.3536682129,25.4699427797), test loss: 27.1099966049\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.23057365417,2.90544470683), test loss: 2.27642868161\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (9.27604866028,25.3041533001), test loss: 34.2085254669\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.15046811104,2.88895888886), test loss: 3.23912346065\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (12.9755344391,25.1421357123), test loss: 27.9815838337\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.321797788143,2.87244845748), test loss: 2.64434248805\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.5033721924,24.9785870237), test loss: 34.5309481144\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.511274337769,2.85663784259), test loss: 3.13551134467\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (10.9880886078,24.8228829046), test loss: 28.3056965828\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.21378421783,2.84163663848), test loss: 2.69887232184\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (23.6233215332,24.6709706169), test loss: 32.199341011\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.879493892193,2.82687287469), test loss: 3.03765048683\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (21.0767726898,24.5173878743), test loss: 24.8913822174\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.09341168404,2.81230459304), test loss: 2.72663532197\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (8.26570415497,24.3689794192), test loss: 33.4250293732\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.796185016632,2.79774506653), test loss: 3.17079072595\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (15.2952013016,24.2241397036), test loss: 28.6630712509\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (4.92104053497,2.78337030866), test loss: 3.29619606733\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (6.69925069809,24.0771723849), test loss: 34.4485274076\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.67488193512,2.76931822676), test loss: 3.16020643264\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (22.4505062103,23.9375187701), test loss: 33.5999071121\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.05295944214,2.75597704283), test loss: 3.15384431481\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (7.40464544296,23.7997093897), test loss: 30.6490968227\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.806319475174,2.74284150469), test loss: 2.6168923974\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (4.78468894958,23.6610806648), test loss: 31.9350327969\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.694205641747,2.72988884678), test loss: 3.31889564544\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (18.7120819092,23.526955953), test loss: 29.7786867142\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.32390379906,2.71692512948), test loss: 2.3167052865\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (14.0632286072,23.3950806926), test loss: 33.755155158\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (4.17862653732,2.70416785936), test loss: 3.17323464453\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (20.1687545776,23.2618523542), test loss: 27.9105127811\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.50234341621,2.69157095135), test loss: 2.27365242392\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (21.5836791992,23.1345109728), test loss: 35.4401276112\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.08903026581,2.67959344423), test loss: 3.1560058713\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (27.8770446777,23.0087509947), test loss: 28.0776873589\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.3249630928,2.66783362193), test loss: 2.61916067898\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (14.4245796204,22.8817922308), test loss: 32.939328146\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.424524247646,2.65610413505), test loss: 3.11114342809\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (29.13463974,22.7591377149), test loss: 28.0994635582\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.94821715355,2.644447297), test loss: 2.71189987957\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.2810621262,22.6373367645), test loss: 33.478330946\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.539553403854,2.63291207485), test loss: 2.91391809732\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (18.51171875,22.5150442942), test loss: 25.4772073269\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.87613868713,2.62153466761), test loss: 2.89699361324\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (6.82327842712,22.3970410858), test loss: 34.3555053234\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.08019781113,2.61063628696), test loss: 3.07855878472\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.8599119186,22.2811174107), test loss: 27.8879007816\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.25763893127,2.5999602627), test loss: 3.05967088342\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (19.1315307617,22.1640164214), test loss: 34.1530389547\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (3.41587018967,2.58937385033), test loss: 3.14948458672\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (12.0299224854,22.0504397593), test loss: 29.6297150612\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.472140580416,2.57870417497), test loss: 3.1581053257\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (6.21637439728,21.9374717443), test loss: 31.6706605434\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.08273291588,2.56823437708), test loss: 2.48197574466\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (30.9500522614,21.8247449678), test loss: 31.503088212\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.81530714035,2.55787463923), test loss: 3.19922651649\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (2.89073419571,21.7142279159), test loss: 30.8161606789\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.86130154133,2.54789801471), test loss: 2.31478042603\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (9.34063148499,21.6063694707), test loss: 34.5922652483\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.977503418922,2.53809123612), test loss: 3.18126017302\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (4.3603143692,21.4971016294), test loss: 28.0215147018\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.953828215599,2.52840014705), test loss: 2.34519084096\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (11.6561965942,21.39116142), test loss: 34.845111227\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.995266318321,2.51859717692), test loss: 3.16248879433\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (9.16713237762,21.2854838869), test loss: 28.1078482151\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.876059830189,2.50898342272), test loss: 2.55197352171\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (8.10720443726,21.1800594922), test loss: 33.6307948112\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.96585655212,2.4995062175), test loss: 2.9995320797\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (7.08796405792,21.0763616493), test loss: 26.7232176304\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.071362257,2.49028529991), test loss: 2.64823637307\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (5.45311546326,20.9752213373), test loss: 34.6425719261\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.39455902576,2.48123596803), test loss: 3.00933266878\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (21.3179893494,20.8731391426), test loss: 29.3356430054\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.17113482952,2.47230044551), test loss: 2.98088594675\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (12.3649168015,20.7734457513), test loss: 34.3286221504\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.54865288734,2.46327984623), test loss: 3.05342359543\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (13.0726413727,20.6740119429), test loss: 27.6592919827\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.56876301765,2.45438583877), test loss: 3.01247128546\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.97485351562,20.574546886), test loss: 35.04159832\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.8877453804,2.44558317037), test loss: 3.10868168175\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (9.96344661713,20.4766268303), test loss: 29.1367625713\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.12442016602,2.43701012683), test loss: 3.10855559111\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (7.03907585144,20.3809880897), test loss: 31.8593255281\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.83506077528,2.4286096233), test loss: 2.5870290637\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (4.90863609314,20.2845374781), test loss: 31.447922492\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.46820020676,2.42031397393), test loss: 3.19979700446\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (4.81888580322,20.1902858544), test loss: 30.8476060867\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (3.68162488937,2.41198067269), test loss: 2.34856417179\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (16.5682640076,20.0963710864), test loss: 34.2890152454\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.88696551323,2.40367080786), test loss: 3.10789963901\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (28.5957698822,20.002412333), test loss: 29.1911460876\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.54126310349,2.39549486586), test loss: 2.41140561402\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (11.5279483795,19.9093724452), test loss: 37.6004732609\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.6619591713,2.38751359411), test loss: 3.182652843\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.59893608093,19.8185346984), test loss: 27.5945626259\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.23702561855,2.37967909533), test loss: 2.60271467268\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (5.95693302155,19.7268442637), test loss: 34.2495997906\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.2754073143,2.37194704589), test loss: 3.00180521011\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (5.33592700958,19.637115242), test loss: 26.6455414772\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.27852320671,2.36413595823), test loss: 2.64862315357\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (7.6932926178,19.5476949125), test loss: 34.0828950882\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.42038667202,2.35637697772), test loss: 2.85510344505\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (7.94907236099,19.4580899612), test loss: 28.5169648647\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.74001777172,2.3487445868), test loss: 2.94959776103\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (21.6323814392,19.3695392211), test loss: 34.7670512199\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.32897269726,2.34125810442), test loss: 3.04540362656\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (8.10113143921,19.2823520193), test loss: 27.9231965065\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.853909134865,2.33390716433), test loss: 3.02386543453\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (9.47086143494,19.1947111903), test loss: 36.3551479816\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.83062314987,2.32667225014), test loss: 3.11044372469\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (3.88321375847,19.1086105644), test loss: 29.1881263733\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (4.68219947815,2.31939162424), test loss: 3.14698135853\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (6.81628894806,19.0230276168), test loss: 32.0807044983\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.03267335892,2.3120567987), test loss: 2.57769780159\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (16.065612793,18.9373628046), test loss: 31.8575135231\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.93124628067,2.30490255132), test loss: 3.1734531045\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (4.87282466888,18.8523612892), test loss: 31.8352301121\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.71254992485,2.29777867351), test loss: 2.397382164\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (5.65424203873,18.7687219373), test loss: 34.6333831787\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.885350465775,2.29084391795), test loss: 3.14314774275\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (10.9845409393,18.68481718), test loss: 30.3808885574\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.06256890297,2.28403094519), test loss: 2.52976770103\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.55319881439,18.6019913404), test loss: 35.9953089714\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.43941402435,2.27715080749), test loss: 3.16502631009\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (10.6950187683,18.5201646764), test loss: 27.7347843647\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.92314553261,2.27024576357), test loss: 2.70306466967\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (3.35353422165,18.4379339803), test loss: 35.6037036896\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.14261031151,2.26346496626), test loss: 2.98605411649\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (7.79067420959,18.3565170372), test loss: 26.9691616535\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.67346167564,2.25674569515), test loss: 2.66786832064\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (2.08359837532,18.2761575168), test loss: 35.2731463909\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.389419943094,2.25016683325), test loss: 2.92331508249\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (2.69494819641,18.1956665088), test loss: 30.0462282181\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (0.226398199797,2.24370509673), test loss: 3.04394189417\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (2.80676651001,18.1161290903), test loss: 34.9975010872\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.46023392677,2.23723381297), test loss: 3.01820484996\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (5.12822389603,18.0375530728), test loss: 28.0318989754\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.80983638763,2.23066976024), test loss: 2.99337827265\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (6.58557033539,17.9585846852), test loss: 36.8513104439\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.29587197304,2.22422705685), test loss: 3.08422207236\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (11.004983902,17.8802987025), test loss: 29.8089605331\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.56861901283,2.21785609455), test loss: 3.05357449949\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (2.98942351341,17.8028657625), test loss: 33.2435107708\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.127173393965,2.21158927817), test loss: 2.60045933723\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.33585262299,17.7257176092), test loss: 31.6258732319\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.05791413784,2.20547719033), test loss: 3.15883880854\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (6.37275075912,17.6492410312), test loss: 32.0209895611\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.00709962845,2.19934379788), test loss: 2.37787899971\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (3.36651086807,17.5737404705), test loss: 35.4819136143\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.705967783928,2.19309971529), test loss: 3.09686521888\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (9.48552417755,17.4979774615), test loss: 32.0867321968\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.08269882202,2.18697615396), test loss: 2.60101793259\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (18.1932525635,17.4227935931), test loss: 43.4900739193\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.86317026615,2.18092198377), test loss: 3.35050017238\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (9.32922458649,17.3481657807), test loss: 27.9118047714\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.864694833755,2.17494083635), test loss: 2.63554294258\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (10.0599002838,17.2740074803), test loss: 36.4991371632\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.657572090626,2.16911998593), test loss: 2.9935395807\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (6.15288352966,17.2003905544), test loss: 26.9906430721\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.773314476013,2.16327224848), test loss: 2.62747142017\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (16.2514858246,17.1278244466), test loss: 35.5905221462\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.44778966904,2.15733612729), test loss: 2.82739289105\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (22.5671234131,17.0550085196), test loss: 31.203404808\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.37837743759,2.15150415609), test loss: 3.00754689276\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (2.58393955231,16.9824831775), test loss: 42.7036860466\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.460345685482,2.14573499611), test loss: 3.23803449124\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (6.25815677643,16.9107981983), test loss: 28.3188864231\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.19774341583,2.14004468612), test loss: 3.00859805048\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (3.48575282097,16.8395131883), test loss: 37.3860673428\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.54099178314,2.13450543217), test loss: 3.01161943823\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (8.95709991455,16.768843174), test loss: 29.8003229856\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.02799582481,2.12893380392), test loss: 3.05171634257\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.21673679352,16.6989697788), test loss: 33.9207024574\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.16168951988,2.12325135402), test loss: 2.61845013201\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (3.59415149689,16.6288532199), test loss: 33.4740838528\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.475011229515,2.11766933366), test loss: 3.21586555541\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (2.23673176765,16.5590923026), test loss: 37.8614551544\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.51874470711,2.11218836185), test loss: 2.45431812108\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (7.17521762848,16.4902114997), test loss: 36.5942710876\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.746664345264,2.1067381011), test loss: 3.10973235369\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (2.98646306992,16.4216107402), test loss: 32.0738771915\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.836357712746,2.10144559562), test loss: 2.66716026962\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (14.0002241135,16.3538105258), test loss: 37.0708300114\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.849582731724,2.09612081936), test loss: 3.13437547684\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (14.1312456131,16.2865942457), test loss: 29.3734273911\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.7332829237,2.09071604546), test loss: 2.71701433659\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (9.46465587616,16.2192291996), test loss: 38.0537266731\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (4.33662414551,2.0854129606), test loss: 2.9981690526\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (8.20379829407,16.1522836046), test loss: 29.4605153561\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.791206002235,2.08013804278), test loss: 2.64234450758\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (5.48031663895,16.0860318247), test loss: 37.7959924221\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.03594660759,2.07492049736), test loss: 2.84376181662\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (5.95571470261,16.0201829476), test loss: 31.4595975876\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.488811969757,2.06984538549), test loss: 3.18983012438\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (4.34198474884,15.9549298514), test loss: 36.7248795509\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.64395648241,2.06474231558), test loss: 3.036324431\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (1.82354140282,15.8901891067), test loss: 28.9296995163\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.589107871056,2.05958096461), test loss: 3.09343072474\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.43429756165,15.8255581009), test loss: 38.6706673145\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.89836335182,2.05451922438), test loss: 2.88693281412\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (4.79041862488,15.7612502353), test loss: 35.0373370647\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.589177131653,2.04945126057), test loss: 3.15007404983\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (4.30440330505,15.6975918557), test loss: 35.4545125008\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.84311628342,2.04446953017), test loss: 2.60768778026\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.25518417358,15.6344108704), test loss: 32.5977347851\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.21344327927,2.03961117693), test loss: 3.15833638906\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (5.59288024902,15.5718086406), test loss: 33.8599124908\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.50104928017,2.03473645648), test loss: 2.42983883023\n",
      "\n",
      "MC # 3, Hype # hyp5, Fold # 9\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold9/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (379.430847168,inf), test loss: 174.45401268\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (301.911621094,inf), test loss: 382.798722839\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (71.092880249,65.2393285713), test loss: 47.2569626808\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.77027368546,44.4513654783), test loss: 3.45741368532\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (95.0405349731,55.7051484356), test loss: 40.4284490108\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.00879573822,23.7843560902), test loss: 3.6843151927\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (37.6125335693,52.5821437232), test loss: 46.7555868626\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (6.89408493042,16.9027604458), test loss: 3.46440865993\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (66.3333053589,50.9097133899), test loss: 43.826068306\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.86997961998,13.4593536431), test loss: 3.84744066596\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (13.5747041702,49.9475350519), test loss: 45.1235441208\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.72771978378,11.4009051895), test loss: 2.73970661759\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (16.9609375,49.2792960971), test loss: 46.3279910088\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.76591253281,10.0301773729), test loss: 3.9842312336\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (14.9251785278,48.7849823617), test loss: 42.2148596287\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.35326838493,9.05662305744), test loss: 2.76636870205\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (42.5337295532,48.4003080275), test loss: 47.6025680065\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.90552258492,8.32408285701), test loss: 3.64558118582\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (100.911735535,48.0488914981), test loss: 40.4359726906\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.23149967194,7.75745514283), test loss: 3.14340547919\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (147.678070068,47.7402627565), test loss: 47.0683442116\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.62550640106,7.3046098354), test loss: 3.39964917004\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (28.4999580383,47.4672361742), test loss: 35.8230354786\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.81882858276,6.93625162387), test loss: 2.91468388438\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (11.1411705017,47.2200262742), test loss: 45.7059058189\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.36606025696,6.62796964609), test loss: 3.55221654177\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (28.7347927094,47.0345364762), test loss: 40.256089592\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.80486106873,6.37010489386), test loss: 3.62221392393\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (52.6130828857,46.8649354069), test loss: 41.6114737034\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (6.85605049133,6.15050767589), test loss: 2.97453915477\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (36.0807647705,46.6962536922), test loss: 46.8514266014\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.57755446434,5.96167772916), test loss: 3.88317400217\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (39.1032028198,46.55113404), test loss: 44.7672022343\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.26229906082,5.79557056147), test loss: 2.91678735018\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (52.5581741333,46.3980880855), test loss: 45.3606291771\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.70041322708,5.65048274508), test loss: 3.76345351338\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (33.8747558594,46.2437500679), test loss: 39.4761654377\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.40856480598,5.52106477305), test loss: 3.04653624892\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (34.7928161621,46.0866793768), test loss: 46.9897706509\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.69591164589,5.40606639069), test loss: 3.64425945282\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (40.5390968323,45.940062967), test loss: 38.4294640064\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.30547475815,5.30124464555), test loss: 3.06312459111\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (12.7741565704,45.8162787686), test loss: 44.6705964088\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.22183299065,5.20824265085), test loss: 3.49883770347\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (34.7593078613,45.7021094345), test loss: 37.7620802879\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.30930089951,5.12406448665), test loss: 3.35411709547\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (34.1375465393,45.5799259018), test loss: 44.7153973103\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.88076233864,5.04553675067), test loss: 3.30076550841\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (32.4419174194,45.4635692756), test loss: 41.4603927135\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (8.49607086182,4.97404900563), test loss: 3.68200878501\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (65.107131958,45.3425993258), test loss: 42.8092877388\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.39002370834,4.90792537423), test loss: 2.94237437248\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (60.2479782104,45.214948404), test loss: 44.2946469307\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (4.082447052,4.84705779888), test loss: 3.77154640555\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (31.0831947327,45.0775585971), test loss: 38.9403733253\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (6.18198347092,4.78844134941), test loss: 2.88977608681\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (64.8669509888,44.9532661303), test loss: 44.816970253\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.65838932991,4.73074570471), test loss: 3.54322297573\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (13.9924583435,44.8330498455), test loss: 37.0643335819\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.81584024429,4.67697171275), test loss: 2.88436239958\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (34.5235099792,44.7199718318), test loss: 44.1078068256\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.10086631775,4.62730482482), test loss: 3.3679151535\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (24.5114192963,44.5977765362), test loss: 32.600754118\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.92801523209,4.58051057876), test loss: 2.8038304776\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (51.3814544678,44.4712135486), test loss: 43.393609333\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.75326395035,4.5361897761), test loss: 3.28008794188\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (27.1320323944,44.3357831022), test loss: 37.7677195549\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.21094512939,4.49367397185), test loss: 3.56262645721\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (16.8049583435,44.192515996), test loss: 38.7431390524\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.78014063835,4.4538913798), test loss: 2.80099111199\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (31.0908298492,44.037628533), test loss: 42.773904705\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.50260162354,4.41549288767), test loss: 3.69882302284\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (30.8785305023,43.8911012823), test loss: 39.5057422638\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.00408148766,4.37867419994), test loss: 2.74260564446\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (44.8267593384,43.730211443), test loss: 41.0127723217\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (5.10795307159,4.34322995191), test loss: 3.62094333768\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (93.6343078613,43.579823137), test loss: 35.4196368217\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (5.70645856857,4.30979357224), test loss: 2.70333609581\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (14.6238746643,43.4119955668), test loss: 43.728676939\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.84584856033,4.27686561041), test loss: 3.42524965405\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (46.6003265381,43.2439950122), test loss: 31.2998432636\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.01207971573,4.24552587578), test loss: 2.74471071959\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (36.50989151,43.0646146978), test loss: 39.4313213348\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.22217273712,4.21407343064), test loss: 3.11416943669\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (20.7689933777,42.874080381), test loss: 33.8170381546\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.97446227074,4.18411826773), test loss: 3.3044052422\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (32.7527198792,42.6702939546), test loss: 39.9139402866\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.06310367584,4.15392661234), test loss: 3.05870216191\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (27.1003513336,42.4740439256), test loss: 37.5252252102\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.67118644714,4.12443619992), test loss: 3.37836673856\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (74.6863861084,42.2592986934), test loss: 35.5701598167\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.74784994125,4.09520235653), test loss: 2.54949992895\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (12.7769374847,42.0455864776), test loss: 39.0398133278\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.905507445335,4.06658667736), test loss: 3.49301493168\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (20.7793312073,41.8175278749), test loss: 31.8839012623\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.519951343536,4.03790807552), test loss: 2.51384368837\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (57.8113555908,41.5884658666), test loss: 38.9611644268\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.32658004761,4.00985902125), test loss: 3.29231034517\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (44.7142028809,41.3451509653), test loss: 30.3028038502\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.67870616913,3.98113159498), test loss: 2.73430223465\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (25.8105888367,41.0882536871), test loss: 36.4382861137\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.796349525452,3.95294790446), test loss: 3.08498096168\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (22.3617515564,40.822529028), test loss: 31.9385852337\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.1271340847,3.92481014526), test loss: 3.12606320977\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (19.4881420135,40.5609826165), test loss: 34.9177258015\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (7.84996700287,3.89716676725), test loss: 3.02095751762\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (33.6370925903,40.293247978), test loss: 31.0038050175\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.54976773262,3.86927346633), test loss: 3.40852566361\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.7101154327,40.0254921426), test loss: 30.1903891087\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.70242881775,3.84227093885), test loss: 2.481094414\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (32.2727813721,39.7556697757), test loss: 34.2504785538\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.00694131851,3.81542986784), test loss: 3.57187238336\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (12.9926118851,39.4867513436), test loss: 30.4388604164\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.11407124996,3.7895442678), test loss: 2.38929146826\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (13.5574226379,39.2207386787), test loss: 35.6463941574\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.55526888371,3.76398293634), test loss: 3.3075779438\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (39.0225868225,38.9533639836), test loss: 28.1987272739\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.62675023079,3.73897202676), test loss: 2.58013689518\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (6.02199363708,38.684870312), test loss: 35.9772764683\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.39639925957,3.71403568325), test loss: 3.10563171506\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (15.7830247879,38.4249056358), test loss: 25.1773722649\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.67038607597,3.68984089071), test loss: 2.56758256257\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (15.3154010773,38.1678747433), test loss: 35.0373274684\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.40813159943,3.66564407995), test loss: 3.05396558046\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (18.2884025574,37.915258732), test loss: 29.610842514\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.21078801155,3.64222747716), test loss: 3.30097568631\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (24.0433940887,37.6660922242), test loss: 34.2029121876\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (4.79636859894,3.61940849021), test loss: 2.86037413776\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (8.74295520782,37.4207307617), test loss: 34.0806538105\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.85675537586,3.59710008451), test loss: 3.53589900136\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (8.00871944427,37.1810970103), test loss: 30.3804695129\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.10079312325,3.57536901293), test loss: 2.43851705492\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (23.0594863892,36.9467716203), test loss: 34.9258779526\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.446480572224,3.55435301981), test loss: 3.38278318644\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (29.3576202393,36.7107322904), test loss: 30.0033827782\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.20762777328,3.53345216205), test loss: 2.51815338135\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (19.3952274323,36.4796860902), test loss: 34.7650570869\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.795124053955,3.51294530836), test loss: 3.21607647836\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (6.9185090065,36.2553533968), test loss: 27.4722966671\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (0.794466078281,3.49252735693), test loss: 2.72397622168\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (7.53474330902,36.0354405025), test loss: 34.4388439178\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.33622670174,3.47292727256), test loss: 3.02988668084\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (17.0488796234,35.8187244693), test loss: 29.7417453289\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.045535326,3.45376157731), test loss: 3.26264398098\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (36.6945648193,35.605769843), test loss: 35.5338340759\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.32223749161,3.43479304181), test loss: 3.08312320113\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (9.37412071228,35.3967815101), test loss: 30.345342207\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.81653594971,3.41645390198), test loss: 3.47221914828\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (21.0210113525,35.193833459), test loss: 30.1403617859\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.4214951992,3.39859953106), test loss: 2.36317551881\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (31.1553421021,34.9896427697), test loss: 33.0572839499\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (4.64250421524,3.38099424712), test loss: 3.54982643723\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (3.54190587997,34.7877622119), test loss: 28.356842804\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.816930174828,3.36344077295), test loss: 2.23773117065\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (11.1537637711,34.5935085283), test loss: 35.4176218629\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.85410499573,3.34632315405), test loss: 3.29155988395\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (17.1152763367,34.401211185), test loss: 27.6208603382\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.91709375381,3.32937562534), test loss: 2.5876429081\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (56.6138725281,34.2134843337), test loss: 34.2290421247\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.75330901146,3.31293208053), test loss: 3.09476011992\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (14.4536533356,34.0257148667), test loss: 24.6932443142\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.80911767483,3.29657637118), test loss: 2.52208171785\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (29.6059188843,33.8427848078), test loss: 34.7044958591\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.03939342499,3.28076224295), test loss: 3.07252941132\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (9.95057296753,33.663720095), test loss: 28.9278794527\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.879172921181,3.26531978017), test loss: 3.36250959337\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (25.0463657379,33.4842194433), test loss: 28.5952528477\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.52864742279,3.2500804333), test loss: 2.58056010902\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (27.0038642883,33.3071141351), test loss: 32.5318780422\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.6788687706,3.23482692241), test loss: 3.4653611064\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (34.8648223877,33.1359128047), test loss: 30.3970310926\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.79281616211,3.21990304008), test loss: 2.31067359298\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (10.1945972443,32.9648821673), test loss: 32.7478226185\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.23060369492,3.20515644422), test loss: 3.28374111652\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (35.6667785645,32.7982311935), test loss: 28.5693937302\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.9309990406,3.19077109169), test loss: 2.49046791643\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (6.16059970856,32.6303540979), test loss: 34.8381988049\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.17375600338,3.17640011453), test loss: 3.14539006352\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (9.13988494873,32.4678986368), test loss: 26.0720733166\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.26677143574,3.16251543306), test loss: 2.6309096992\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (19.2947616577,32.308158952), test loss: 34.2009884357\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.855270266533,3.14883853211), test loss: 3.02090966552\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (10.9340791702,32.1475675789), test loss: 28.8335423946\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.73942101002,3.13544863711), test loss: 3.13141690195\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (11.33568573,31.9890452044), test loss: 34.8806972504\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.76576185226,3.12198440401), test loss: 2.97029903233\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (17.4761676788,31.8352741864), test loss: 29.7994023323\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.67512130737,3.10880727347), test loss: 3.37089189142\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.8405599594,31.682002679), test loss: 30.7502662659\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.622461378574,3.09575965963), test loss: 2.33861466348\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (21.9774894714,31.5313778016), test loss: 33.6659343719\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.1480948925,3.08290477144), test loss: 3.31772560179\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (13.905954361,31.3796118449), test loss: 28.1376133919\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.15885698795,3.07011926351), test loss: 2.31612327695\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (29.8803062439,31.2330167393), test loss: 35.3563970089\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.17316365242,3.05780455156), test loss: 3.24890117645\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (10.3825702667,31.0878739012), test loss: 27.0038457394\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.548036456108,3.04554751425), test loss: 2.60261056125\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (12.6696538925,30.9425237831), test loss: 34.2574899673\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.669538080692,3.03357405583), test loss: 3.0391874969\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (9.18597412109,30.7994000629), test loss: 24.9046669006\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.400504082441,3.02161317883), test loss: 2.78637200296\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (8.60890388489,30.6588331174), test loss: 35.1779991388\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.29201078415,3.00977145835), test loss: 2.9856654346\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (16.7817268372,30.5201997926), test loss: 28.0822642803\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.96631789207,2.99805593034), test loss: 3.20037751794\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (15.0880823135,30.3819796556), test loss: 29.8425138474\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.14452385902,2.98652194161), test loss: 2.46299341023\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (45.8966140747,30.2447403945), test loss: 31.2291767359\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.08188414574,2.97498636485), test loss: 3.39721711278\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (10.6840200424,30.1093700049), test loss: 30.978948307\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.815898060799,2.96378628764), test loss: 2.32283095717\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (10.8397006989,29.9766148316), test loss: 32.6928964615\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.9157333374,2.95279590004), test loss: 3.11856718212\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (14.4541435242,29.8443159845), test loss: 28.4655434132\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.55156135559,2.9419782532), test loss: 2.53367862403\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (3.85763287544,29.7127933427), test loss: 35.824970746\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.636417865753,2.93114685978), test loss: 3.03243728578\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (14.6471138,29.5833421649), test loss: 25.2358716965\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (3.40944862366,2.92043274024), test loss: 2.50471335351\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (9.1990814209,29.4557754422), test loss: 33.9426612377\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.60911560059,2.90968341781), test loss: 2.898425138\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (18.2937984467,29.3287594286), test loss: 27.7568570137\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.05873215199,2.8991673567), test loss: 3.09398673177\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (25.7675476074,29.2024705867), test loss: 35.829840827\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.65895938873,2.88873101178), test loss: 2.90264510512\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (3.95649075508,29.0772497416), test loss: 31.8913030624\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.4869120121,2.87850547521), test loss: 3.31886856556\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (6.8927025795,28.9543763966), test loss: 31.5461166382\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.540412425995,2.8683860631), test loss: 2.34718048871\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (23.0473594666,28.8333558694), test loss: 36.9157109737\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.259208440781,2.8584947851), test loss: 3.35471598208\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (10.5102558136,28.7109255677), test loss: 30.6245049\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (3.24200987816,2.84861043582), test loss: 2.46776221395\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (16.0181179047,28.5907899615), test loss: 34.911261344\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.76989293098,2.83876733133), test loss: 3.07623587102\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (7.3808259964,28.4722183879), test loss: 27.00777812\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.40510821342,2.82891697058), test loss: 2.52684541941\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.15226554871,28.3543356687), test loss: 34.2938131809\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.736139535904,2.81928244512), test loss: 2.90602997243\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (18.3340301514,28.2373137493), test loss: 29.0395252228\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.40111231804,2.80970516495), test loss: 2.94453509748\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (15.9837646484,28.1210102572), test loss: 34.7573938847\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.25582146645,2.8001876034), test loss: 2.92979876399\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (15.0120372772,28.0064442392), test loss: 28.6866119862\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.44362926483,2.79087802056), test loss: 3.23009704202\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (19.680305481,27.8937093827), test loss: 32.2740534306\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.27811169624,2.78174927149), test loss: 2.55374640822\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (16.4247226715,27.7796266885), test loss: 32.1538065434\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.7611951828,2.77267241688), test loss: 3.29469491839\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (3.78287482262,27.6674174831), test loss: 31.7825853109\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.394027411938,2.7635060589), test loss: 2.2928018406\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (15.0240163803,27.557171708), test loss: 34.6183661461\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (3.27833986282,2.75450847507), test loss: 3.05453836322\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (11.640411377,27.446902049), test loss: 27.3707906246\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.34948444366,2.74554991851), test loss: 2.48319068253\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (18.5695552826,27.3376362493), test loss: 35.100894022\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.45523619652,2.73671444819), test loss: 2.93464161158\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (22.5354804993,27.2286454294), test loss: 25.330079031\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.54466462135,2.72790698202), test loss: 2.44818207026\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (17.750995636,27.1212742255), test loss: 35.3246944904\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.798318386078,2.71924797172), test loss: 2.84638390392\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (7.81316757202,27.0153340303), test loss: 27.2984516621\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.846447944641,2.71074572781), test loss: 2.96783802509\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (18.1518287659,26.9086433876), test loss: 33.4804690838\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.0050214529,2.70232723629), test loss: 2.6520589143\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (18.2252292633,26.8035838724), test loss: 32.4137308598\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.01347076893,2.69389634329), test loss: 3.21574738622\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (26.6183242798,26.6998500273), test loss: 32.5310500145\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.730887293816,2.685525457), test loss: 2.37954739332\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (9.56972694397,26.5963126968), test loss: 34.4634464741\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.36721777916,2.67721131388), test loss: 3.08444615602\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (11.7830820084,26.4938055449), test loss: 31.3686939716\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (3.23772096634,2.66899534104), test loss: 2.45019297302\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (3.43296313286,26.3908763675), test loss: 34.9832892418\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.492849290371,2.66078713582), test loss: 2.96657159925\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (4.89308214188,26.2898743215), test loss: 26.9984647751\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.95721697807,2.65278071112), test loss: 2.59293621555\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (13.3784132004,26.1898396279), test loss: 35.1390525818\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.532117724419,2.6448261106), test loss: 2.81936625242\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (25.1371803284,26.0894901976), test loss: 28.1983541489\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.736111998558,2.63698058591), test loss: 2.9264725104\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (5.04451131821,25.9904090366), test loss: 35.8151232243\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.841483831406,2.62909684298), test loss: 2.861568892\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (14.9682044983,25.8924939273), test loss: 29.9575402737\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.46557378769,2.62129464592), test loss: 3.17679077387\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.3464384079,25.7947945546), test loss: 33.9009176254\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.703000366688,2.61360950849), test loss: 2.35470438749\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (16.6723194122,25.6975408374), test loss: 33.2255951643\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.789652764797,2.60591760915), test loss: 3.21977179646\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (9.79004669189,25.6002457538), test loss: 30.5050057411\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.456144124269,2.5982408015), test loss: 2.23996406198\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (18.2303009033,25.5047206402), test loss: 35.6554686546\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.72699010372,2.59073613572), test loss: 3.03589497805\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.11172914505,25.4097945904), test loss: 28.7155932903\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.338694155216,2.58327119893), test loss: 2.57122630477\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (9.81021595001,25.3148444536), test loss: 37.1383822441\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.41090542078,2.57597348049), test loss: 2.94867582023\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (5.77870225906,25.2209172576), test loss: 25.7180149078\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.947467029095,2.56866425453), test loss: 2.50635656416\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (7.41763687134,25.1278411277), test loss: 35.6590069294\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.93164181709,2.56132247791), test loss: 2.91329237819\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (11.3320426941,25.0354922745), test loss: 27.7178903103\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.33051633835,2.5540769911), test loss: 3.01001405418\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (10.612991333,24.9430089285), test loss: 32.7582694292\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.28744578362,2.54686617662), test loss: 2.618033427\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (26.5776634216,24.8509827434), test loss: 32.4355556488\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.33265805244,2.53969806345), test loss: 3.17914901972\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (10.9049320221,24.7593635655), test loss: 33.6506173611\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.56334638596,2.53266429443), test loss: 2.3003060922\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (6.48380565643,24.6691906567), test loss: 33.8937654734\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.5085682869,2.52567678793), test loss: 3.0039601624\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (9.88495826721,24.5790281574), test loss: 31.4649521828\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.57262396812,2.51877794426), test loss: 2.58087243736\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (6.15763378143,24.4895441974), test loss: 36.3603259802\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.36736559868,2.51191133771), test loss: 2.9532433778\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.36350393295,24.400777212), test loss: 28.0970723629\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.54548931122,2.50505123863), test loss: 2.52335508168\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (6.13784503937,24.3125197484), test loss: 36.1789531231\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.43751549721,2.49819370505), test loss: 2.84027791321\n",
      "\n",
      "MC # 3, Hype # hyp5, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold10/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (392.05947876,inf), test loss: 232.404336166\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (295.86328125,inf), test loss: 356.023750305\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (49.2222290039,111.231531708), test loss: 44.6587950706\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.406621843576,47.0942194839), test loss: 3.76148699522\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (21.0530414581,79.343765976), test loss: 42.7691807747\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.01263737679,25.2050426488), test loss: 3.71477409601\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (203.985595703,68.8444687904), test loss: 42.2739948988\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (8.1875038147,17.9029475686), test loss: 3.75518149436\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (33.2550849915,63.3895274661), test loss: 43.8026987553\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.31509697437,14.2433157134), test loss: 3.84037217498\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (24.066450119,60.1827911753), test loss: 40.0737364054\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.64152348042,12.0574995533), test loss: 3.20804943144\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (18.2378044128,57.9502885447), test loss: 47.1859096527\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.601187586784,10.5972645765), test loss: 3.98344947994\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (42.4497146606,56.3717582066), test loss: 42.7986397266\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.60777938366,9.5525467937), test loss: 2.98045452833\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (100.662078857,55.1067975808), test loss: 46.3801811218\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.38150429726,8.76953163015), test loss: 3.89454924464\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (42.3345603943,54.0590464381), test loss: 39.9486104012\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.80623698235,8.15905618232), test loss: 3.28550507426\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (13.9478034973,53.2796856318), test loss: 46.4233077049\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.30316662788,7.67071720536), test loss: 3.73458108902\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (50.2194366455,52.5722539514), test loss: 38.4392815113\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.57644033432,7.26906373588), test loss: 2.94802953601\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (89.4914169312,52.0022801231), test loss: 43.1735496521\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (7.56892061234,6.9379065088), test loss: 3.76410441101\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (62.1246337891,51.4754817971), test loss: 34.8478562355\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.57430076599,6.65637115697), test loss: 3.012181288\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (25.9382171631,51.0145974478), test loss: 41.0657585621\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.16722488403,6.41349928545), test loss: 3.58966522813\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (24.1280479431,50.591248992), test loss: 41.8752729416\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.02134633064,6.20388710071), test loss: 3.6393520236\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (25.2025642395,50.1706842541), test loss: 37.4879720688\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.20587825775,6.01848453168), test loss: 3.12878136635\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (38.7955703735,49.7946686405), test loss: 46.9560912609\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.16991281509,5.85483439671), test loss: 3.79434185326\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (31.7007751465,49.4171005947), test loss: 39.0288374424\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.28350782394,5.70746640418), test loss: 2.84061505198\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (35.1738815308,49.0149584273), test loss: 43.0254869461\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.82409477234,5.57562210418), test loss: 3.84271842241\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (20.7361984253,48.5952606183), test loss: 35.590131712\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.48173356056,5.45605083527), test loss: 2.90332860053\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (54.8358230591,48.1892081081), test loss: 43.1148557663\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.69141721725,5.34705550711), test loss: 3.6121557951\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (16.888792038,47.7822426254), test loss: 34.5106821775\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.13154935837,5.24584926786), test loss: 3.10731528103\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (51.9898033142,47.3617364837), test loss: 38.9720835686\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (6.58386898041,5.15228343781), test loss: 3.2566755414\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (27.6331386566,46.93596503), test loss: 30.1897583485\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.63414692879,5.06484592288), test loss: 2.72611929178\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (8.67915821075,46.5156247908), test loss: 37.2456673145\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.67475032806,4.98227485077), test loss: 3.45521874428\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (18.5552558899,46.0967181334), test loss: 35.1758259773\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.35279560089,4.90516740983), test loss: 3.39434309602\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (67.9204711914,45.6682333045), test loss: 35.8866307497\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.88214278221,4.83269712044), test loss: 3.07301326394\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (59.4304389954,45.2302837466), test loss: 38.4589336395\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.90607190132,4.76446956296), test loss: 3.51019034386\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (28.7378349304,44.785166151), test loss: 32.466528368\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.07894802094,4.69820787863), test loss: 2.79073858559\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (22.2546024323,44.3236164783), test loss: 37.9626793861\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.47410750389,4.635355354), test loss: 3.62661779821\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (28.3245582581,43.8578443246), test loss: 28.2860290527\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (0.975789546967,4.57450247621), test loss: 2.6353125453\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (33.93775177,43.3953676839), test loss: 37.9492417812\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (6.73280096054,4.51563565209), test loss: 3.4926772058\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (10.4623765945,42.9354552012), test loss: 28.1954200268\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.36796283722,4.45906112674), test loss: 2.72094695568\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (44.6175765991,42.4758972047), test loss: 35.0678843498\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.23859834671,4.40499877676), test loss: 3.42402284741\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (15.5693092346,42.0174512203), test loss: 24.1840738058\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.14264154434,4.35324671469), test loss: 2.48465675414\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (30.0655593872,41.5668164282), test loss: 33.3046996593\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.52423715591,4.30283811138), test loss: 3.32047500014\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (13.954659462,41.1139452965), test loss: 31.3366036892\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.47193789482,4.25417059807), test loss: 3.43034673929\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (22.4400157928,40.6687541814), test loss: 33.635412097\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.82960653305,4.20725493092), test loss: 3.36663341522\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (26.7550621033,40.2440178252), test loss: 31.8458884954\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.65816295147,4.16180274288), test loss: 3.42243892252\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (13.7837762833,39.8223960222), test loss: 29.9493287325\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.60985302925,4.11765892589), test loss: 2.57935875952\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (25.073387146,39.415067507), test loss: 33.5996313334\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.12398767471,4.07592354113), test loss: 3.66559060812\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (31.6071014404,39.0146700003), test loss: 27.7777843475\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.45522499084,4.0353471429), test loss: 2.45075418651\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (11.8088512421,38.6271866809), test loss: 35.2368960977\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.853219866753,3.99628785912), test loss: 3.46380255818\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (73.444519043,38.2487240978), test loss: 27.8692711353\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (5.334233284,3.95871388333), test loss: 2.74874902219\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (56.5300216675,37.8737905194), test loss: 34.5236119986\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.94162011147,3.92191798219), test loss: 3.38114567101\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (9.61424827576,37.5161919125), test loss: 24.6383247375\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.87631082535,3.88629202555), test loss: 2.54509910643\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (10.0500497818,37.166407528), test loss: 33.1774228573\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.91468763351,3.85179760374), test loss: 3.33521325588\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (36.3013000488,36.827231876), test loss: 30.6152523994\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.47242641449,3.81874520687), test loss: 3.32256144881\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (16.7012786865,36.4928527607), test loss: 33.6362306595\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.59245800972,3.78656284182), test loss: 3.38546434492\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (8.11287307739,36.171005803), test loss: 31.7489705563\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (4.98719406128,3.75568687997), test loss: 3.49098303616\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (33.4155807495,35.8611529848), test loss: 29.4470463276\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.05324029922,3.72568247398), test loss: 2.59063048661\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (23.7036437988,35.5514929847), test loss: 32.7495365858\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.86982059479,3.6962954433), test loss: 3.66608030796\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (11.9565753937,35.2530270608), test loss: 29.4806510925\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.56122756004,3.66772072362), test loss: 2.53929857314\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.5210418701,34.9655959885), test loss: 33.2442706347\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.80083322525,3.63960854648), test loss: 3.39359279871\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.7006168365,34.6839030206), test loss: 28.2569599152\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.5967258215,3.61261906969), test loss: 2.68950566351\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (16.2187576294,34.4057994507), test loss: 35.084693861\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.672832548618,3.58626657818), test loss: 3.37329969406\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (11.9203701019,34.1397009122), test loss: 25.7310593605\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.91628909111,3.56106090922), test loss: 2.58049833179\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.4972906113,33.8800195827), test loss: 33.1647943497\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.00613331795,3.53634409392), test loss: 3.29564919472\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.1882457733,33.6216844528), test loss: 30.5871939182\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.898041605949,3.51216961029), test loss: 3.20408657491\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.0733213425,33.3712716773), test loss: 33.5895287275\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.02121043205,3.48839201279), test loss: 3.3544908464\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (10.4859714508,33.1293677979), test loss: 32.4127699375\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.01534867287,3.46504807851), test loss: 3.45948501825\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (10.5042667389,32.8923300781), test loss: 29.2826794624\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.39263105392,3.44241617517), test loss: 2.72699820101\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (24.9943237305,32.6584048113), test loss: 33.2686838388\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.00618624687,3.42033377963), test loss: 3.6183639437\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (40.4660835266,32.4301064925), test loss: 29.6394263268\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.2266869545,3.39904403128), test loss: 2.39924385548\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (13.833360672,32.2077570446), test loss: 33.2769583702\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.565872967243,3.37809163311), test loss: 3.32316128612\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (28.8431854248,31.9866751778), test loss: 30.4903002739\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.27260828018,3.35767539702), test loss: 2.63772651404\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (11.5597352982,31.7701673676), test loss: 34.3708009243\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.01846349239,3.33747713131), test loss: 3.2430649519\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (16.8552455902,31.5610120836), test loss: 26.8662270784\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.51978731155,3.31769917547), test loss: 2.6070899263\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (7.26162099838,31.3547954458), test loss: 34.4801041126\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.75094735622,3.29822359249), test loss: 3.16303375661\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (11.4309921265,31.1512821354), test loss: 30.978217268\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (5.26285600662,3.27942032359), test loss: 3.23434084952\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.1378898621,30.9518931921), test loss: 34.4895563602\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.887952685356,3.2609323216), test loss: 3.29965021312\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (4.74319553375,30.7571655575), test loss: 30.9091126919\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.30965483189,3.24283886453), test loss: 3.37670031488\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (40.2448997498,30.5647787421), test loss: 29.0506859779\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.62232375145,3.2252091579), test loss: 2.6784933269\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.123292923,30.3747557036), test loss: 34.3349442482\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.00496101379,3.20775429401), test loss: 3.52782063484\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (11.9403190613,30.1918954762), test loss: 29.1512623072\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.767984628677,3.19052012969), test loss: 2.34351096451\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (12.2325716019,30.0096412054), test loss: 33.2758101463\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.6182205677,3.17363077802), test loss: 3.29498828351\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (32.9575614929,29.8307583061), test loss: 29.8100775242\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.18170654774,3.15728620168), test loss: 2.54366394281\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (16.2086925507,29.6535372161), test loss: 36.5548666477\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.27148103714,3.14109671617), test loss: 3.29560193121\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (5.78070068359,29.4806905455), test loss: 27.5740460396\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.852444767952,3.12531597425), test loss: 2.62647884786\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (37.5158081055,29.3110029999), test loss: 34.8528895617\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (3.53086924553,3.10989927652), test loss: 3.18898860514\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (33.6601867676,29.1407832728), test loss: 30.7143726826\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.62344694138,3.0945580137), test loss: 3.09704098701\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (6.03952217102,28.9755879204), test loss: 33.2347968102\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.89693140984,3.07929468177), test loss: 3.25668263137\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (6.23083782196,28.8124459176), test loss: 31.0194801807\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.78224849701,3.06438924791), test loss: 3.1209808588\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (14.5440788269,28.6513486959), test loss: 29.1258816719\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.77558970451,3.04986546996), test loss: 2.65774073303\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (28.8366470337,28.4909669451), test loss: 34.2942510128\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.31603550911,3.03542555582), test loss: 3.38941578865\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (16.8194065094,28.3343660458), test loss: 30.4175347805\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.33290815353,3.02141047647), test loss: 2.24746063352\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (18.5813522339,28.1812029631), test loss: 34.2986236572\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.19763326645,3.00759521838), test loss: 3.23937534392\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (9.27909660339,28.0267854047), test loss: 29.4340382099\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (3.00974154472,2.99394118672), test loss: 2.48989741504\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (9.69777679443,27.8767322545), test loss: 35.7110549927\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.75955057144,2.98042179729), test loss: 3.30546872616\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.52655410767,27.7294630891), test loss: 28.6338681698\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.63356304169,2.96694678583), test loss: 2.58740309477\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (20.4885311127,27.5832024662), test loss: 33.0347053051\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.20924091339,2.95392290643), test loss: 3.05280939937\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (8.31872940063,27.4369604555), test loss: 25.4466514111\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.89134049416,2.94093564807), test loss: 2.41099473089\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (3.84445047379,27.2951595474), test loss: 34.2710280418\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.44887328148,2.92839824051), test loss: 3.20876673162\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (13.2767782211,27.1551875724), test loss: 30.5409021854\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.64065468311,2.91596133562), test loss: 3.11488096118\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.26371192932,27.014573833), test loss: 30.0932616711\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.787479579449,2.90371630779), test loss: 2.69233086556\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (16.3019447327,26.8776841878), test loss: 34.5864664555\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.24368751049,2.89154661411), test loss: 3.39179641157\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (5.91256713867,26.742074286), test loss: 32.1556974888\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.793667018414,2.87942463929), test loss: 2.49139695168\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (15.5365419388,26.6078806085), test loss: 35.0696012497\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.15118932724,2.8676054521), test loss: 3.30671278238\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (8.19088363647,26.4736933431), test loss: 30.121345377\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.08954000473,2.85587006709), test loss: 2.52300630361\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (10.1568126678,26.3418351693), test loss: 36.0382981539\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.655503630638,2.84442417437), test loss: 3.32192678452\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (3.07027435303,26.2115746072), test loss: 29.3541151047\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.434767842293,2.83306802837), test loss: 2.67137350738\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (20.9079494476,26.0819038183), test loss: 34.325268507\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.851983070374,2.82196772698), test loss: 3.0551025331\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (14.9727430344,25.9547137819), test loss: 25.9721408367\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.959542870522,2.81088794861), test loss: 2.38797914684\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (11.877412796,25.8291060384), test loss: 33.1488581181\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.8728043437,2.79986954306), test loss: 3.11824417114\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (4.83310937881,25.7042922795), test loss: 33.3169129133\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.923686146736,2.78897956382), test loss: 3.09992847443\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (12.8654069901,25.5805105026), test loss: 34.7871396065\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (3.91023206711,2.77834572198), test loss: 3.1828751564\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (7.15271520615,25.4582031542), test loss: 35.4403934002\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.804890573025,2.76778721123), test loss: 3.23379835486\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (7.85928821564,25.337656762), test loss: 33.2456196308\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.14464592934,2.75740406535), test loss: 2.53573695421\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (34.3949737549,25.2181204069), test loss: 35.3870423794\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.71996235847,2.74723669112), test loss: 3.30577216446\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (10.785030365,25.0995978836), test loss: 28.549190712\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.16043543816,2.73712780976), test loss: 2.27251881808\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (11.3877401352,24.983123012), test loss: 35.5994495153\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.796566665173,2.72698941991), test loss: 3.19738387465\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (5.44141054153,24.8667948268), test loss: 29.5195665359\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.12909948826,2.71708670393), test loss: 2.64197264463\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (29.7177619934,24.751402413), test loss: 35.0107655525\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.73861443996,2.70731576512), test loss: 3.06938831806\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (9.13301372528,24.6364357315), test loss: 27.8113637447\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.691912055016,2.69759192089), test loss: 2.42333836555\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (5.34073734283,24.5230751988), test loss: 33.7122899532\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.599528551102,2.68801225422), test loss: 3.04175029695\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (20.0520877838,24.4114438092), test loss: 34.2926067352\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.05164849758,2.67863173775), test loss: 3.25574192405\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (9.71546363831,24.2992225389), test loss: 35.2643355131\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.60742998123,2.66929465355), test loss: 3.19546464756\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (6.08228683472,24.1892896191), test loss: 33.4023741722\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.7418012619,2.65989414022), test loss: 3.26278232038\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (6.74605178833,24.0800292774), test loss: 34.304726553\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.93020057678,2.65069064587), test loss: 2.53342377841\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (11.2670183182,23.9712742536), test loss: 36.7729686737\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.04784965515,2.64162741236), test loss: 3.34322845638\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (8.95511817932,23.8631533564), test loss: 31.2733179092\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.694404959679,2.63257905918), test loss: 2.39206287265\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (9.38395881653,23.7566814104), test loss: 36.1171804905\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.71438097954,2.62372205693), test loss: 3.22103897333\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (9.84933662415,23.6516709558), test loss: 29.5277664185\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.982717752457,2.61494436136), test loss: 2.63684309721\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (11.5690326691,23.5463561387), test loss: 37.5852439404\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.19798445702,2.60631009522), test loss: 3.26954795718\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (9.82197380066,23.442942775), test loss: 28.6658343792\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.99454379082,2.59766215079), test loss: 2.57015464008\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (5.23555660248,23.3406630052), test loss: 34.418863678\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.949515521526,2.58901921743), test loss: 3.04369866103\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (24.9759674072,23.2385279918), test loss: 32.4233374596\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.97424805164,2.58061422752), test loss: 3.00894052386\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (13.7459821701,23.1362432349), test loss: 35.0905582905\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.70714330673,2.57219401125), test loss: 3.13599807024\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (3.91667056084,23.035841241), test loss: 33.9358942986\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.66412448883,2.56397371758), test loss: 3.2957939446\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (5.68603420258,22.9363037882), test loss: 33.231317997\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.72638821602,2.55579414775), test loss: 2.7627628386\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (14.0651378632,22.8366340433), test loss: 35.3789525509\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.926698863506,2.54773593635), test loss: 3.38369420469\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (16.8039875031,22.7386643594), test loss: 33.1871948719\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.974200189114,2.53966739365), test loss: 2.46461008191\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (4.21857357025,22.6409918486), test loss: 35.5286559105\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.615132451057,2.5315918324), test loss: 3.17600065172\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (10.098197937,22.5440176789), test loss: 31.5591266155\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.37338399887,2.52370763414), test loss: 2.64027560651\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (5.33400201797,22.4470392604), test loss: 39.1393463612\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.2258951664,2.51580967066), test loss: 3.23399288952\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (4.15996694565,22.3513908367), test loss: 29.5204398394\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.882630884647,2.50807268564), test loss: 2.60421020985\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (6.08650970459,22.2564645682), test loss: 35.305568409\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.509879469872,2.50035822573), test loss: 3.03875364065\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (8.83461475372,22.1620578362), test loss: 33.1131434917\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.758718907833,2.49282406747), test loss: 2.89509099722\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (10.4561157227,22.0689951886), test loss: 35.7069647312\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.45760822296,2.48528277628), test loss: 3.14180413783\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (6.42583274841,21.9764159877), test loss: 33.455369854\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.444360107183,2.4777144338), test loss: 3.2809377104\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (12.1716213226,21.8844052177), test loss: 33.3425712824\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.15663743019,2.47028820598), test loss: 2.73578920513\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (6.94370985031,21.7927675614), test loss: 36.1794233322\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.29814624786,2.46294166989), test loss: 3.42072401643\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (2.53377056122,21.702003268), test loss: 34.228506422\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.37726974487,2.4556372486), test loss: 2.37217411846\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (9.32309150696,21.6119090762), test loss: 36.9587008476\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.81549549103,2.44841829547), test loss: 3.16361721158\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (8.21162223816,21.522467934), test loss: 33.323295784\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.70149075985,2.44133692201), test loss: 2.63721719086\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (8.08513069153,21.433793682), test loss: 38.3545610905\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.63854813576,2.4342690377), test loss: 3.22334008515\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (11.453125,21.3457144834), test loss: 31.5385656834\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.06956803799,2.42711378611), test loss: 2.74432301521\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (16.4066543579,21.258034921), test loss: 35.835200429\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.77786171436,2.42014621579), test loss: 2.99418027997\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (9.94440937042,21.1704233858), test loss: 33.5551156998\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.07749629021,2.41319076765), test loss: 3.01531582177\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (7.18985748291,21.0835112396), test loss: 36.1626889706\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.11332345009,2.40630169162), test loss: 3.16441837549\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (8.46083927155,20.9973954625), test loss: 33.587189579\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.75831544399,2.39947058751), test loss: 3.07073598802\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (12.5252389908,20.9123279616), test loss: 35.5502296925\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.135260850191,2.39275434341), test loss: 3.00265016854\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.21001577377,20.8272458634), test loss: 37.326597023\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.35861825943,2.38607359034), test loss: 3.4063369751\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (2.96338248253,20.7431885618), test loss: 33.0458955765\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (4.53963756561,2.37936048), test loss: 2.386993891\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (20.4065704346,20.6599331551), test loss: 37.9187243462\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.54078435898,2.37268467914), test loss: 3.18689654768\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (7.1194934845,20.5765780055), test loss: 34.0661841393\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.805208921432,2.36613011245), test loss: 2.61906591952\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.01424980164,20.4938886971), test loss: 39.3064908981\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.577406585217,2.3595751175), test loss: 3.24067073464\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (9.14271736145,20.4122577669), test loss: 31.7957734108\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.518000125885,2.35313063822), test loss: 2.68845028281\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (6.39394140244,20.3312953155), test loss: 35.786262989\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.919657230377,2.3467352914), test loss: 2.94234708846\n",
      "run time for single CV loop: 7103.79277706\n",
      "\n",
      "MC # 3, Hype # hyp4, Fold # 6\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold6/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (388.101776123,inf), test loss: 204.107067108\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (334.785583496,inf), test loss: 405.0568573\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (43.5165710449,84.4798367596), test loss: 43.017156744\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.64445006847,71.9946121246), test loss: 3.09473471045\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (102.60836792,65.0394887261), test loss: 42.0603992462\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.63029432297,37.566213549), test loss: 3.17019087225\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (33.4279327393,58.4091481123), test loss: 41.2585352898\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (7.69103240967,26.0844754674), test loss: 3.24439003468\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (50.3222618103,55.0436099875), test loss: 43.9397672415\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.25344944,20.3399100849), test loss: 3.4230232954\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (16.3899841309,53.0434714035), test loss: 39.474887085\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.20615291595,16.8955016948), test loss: 2.69564521611\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (19.7590179443,51.6221972027), test loss: 46.4751363754\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.87162470818,14.6046280357), test loss: 3.59674770832\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (23.6069641113,50.5693245772), test loss: 39.7184361458\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.16355991364,12.9758126767), test loss: 2.59270977974\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (53.5461006165,49.7590641534), test loss: 44.6496635437\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (5.05795907974,11.7541808382), test loss: 3.44830174744\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (30.3558940887,49.0513693732), test loss: 36.4859523296\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.95271849632,10.8035922752), test loss: 2.72501680851\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (33.6703948975,48.5035049514), test loss: 43.1932666779\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.82407283783,10.0443639083), test loss: 3.05075336695\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (20.484790802,48.0233541726), test loss: 36.3279759884\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (5.00938940048,9.42312131027), test loss: 2.68874306083\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (35.473400116,47.611622062), test loss: 40.865506506\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.14406061172,8.9035387558), test loss: 3.26217106581\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (16.408531189,47.2476165402), test loss: 39.3887622356\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.58106732368,8.46410387338), test loss: 3.34662271738\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (103.849830627,46.9212586501), test loss: 39.7921933651\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (6.65706825256,8.08853652457), test loss: 2.86581044793\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (28.5144958496,46.5952957481), test loss: 44.4829554558\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.58001112938,7.76389633114), test loss: 3.43348740637\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (44.6127357483,46.3085745682), test loss: 39.5288222551\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (0.622440993786,7.47855919741), test loss: 2.7413371563\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (101.305603027,46.0226634892), test loss: 44.8563408613\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.66358470917,7.22728824635), test loss: 3.51233669519\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (85.7085571289,45.7431663959), test loss: 34.7215488434\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.46875977516,7.00258538591), test loss: 2.72930419743\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (26.9382019043,45.4735700804), test loss: 41.6793110371\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.35503053665,6.80162907922), test loss: 3.30810262561\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (41.0199317932,45.2203946631), test loss: 32.5274586678\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.02217936516,6.6188961325), test loss: 2.62106566131\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (32.3020324707,44.9781915426), test loss: 39.890883708\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.94057750702,6.45433277494), test loss: 3.22677841783\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (37.2311096191,44.7413550735), test loss: 37.4138855934\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (6.94830226898,6.30440737815), test loss: 3.11083140373\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (32.4232749939,44.4907066834), test loss: 37.519017148\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.07633936405,6.16654185128), test loss: 3.15944753885\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (21.5112533569,44.2509623786), test loss: 40.4427889824\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.14517831802,6.03973819188), test loss: 3.44979829788\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (20.6963806152,44.0065177029), test loss: 35.9326692581\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.91446864605,5.92284157432), test loss: 2.49553112984\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (32.5450210571,43.7503941014), test loss: 42.5141037464\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.8422883749,5.8134569659), test loss: 3.57670983374\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (57.1938323975,43.4967136315), test loss: 34.3891159058\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.19296336174,5.71167426555), test loss: 2.56316402555\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (30.4038658142,43.2443256451), test loss: 38.987125802\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.98905467987,5.61555085573), test loss: 3.28942496777\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (29.3794059753,42.9921059267), test loss: 30.2083914518\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.98389267921,5.52539102708), test loss: 2.54254299402\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (18.9126186371,42.7378135854), test loss: 37.9766252756\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.46016418934,5.44091262252), test loss: 3.10265708566\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (61.77394104,42.4699254781), test loss: 29.9017478704\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (6.99911308289,5.36077455179), test loss: 2.49343671799\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (11.069144249,42.1976026601), test loss: 35.8483840466\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.30146789551,5.28460399028), test loss: 3.13848909736\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (38.674156189,41.9215088562), test loss: 33.2975396633\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.97958040237,5.21202106673), test loss: 3.23449414968\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (17.2019996643,41.6294679441), test loss: 30.3361543417\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.2019276619,5.14258927575), test loss: 2.54555609822\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (35.1703872681,41.3355844672), test loss: 38.7017707348\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.3000600338,5.07595692542), test loss: 3.31757459641\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (33.5050125122,41.040597869), test loss: 31.3711508751\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.91691172123,5.01134351504), test loss: 2.36873146594\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (15.0580358505,40.7381418742), test loss: 36.9073877335\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (0.941108345985,4.94933118617), test loss: 3.25960898995\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.7492074966,40.4337306405), test loss: 27.4342791557\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.41684150696,4.88979442614), test loss: 2.39912689626\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (24.343454361,40.1181178066), test loss: 34.7796578884\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.21218162775,4.83228713779), test loss: 3.11778910756\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (20.6944122314,39.7975729828), test loss: 26.0145289421\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.35022974014,4.77677338452), test loss: 2.39954407811\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (20.4676055908,39.4744577103), test loss: 32.8282091618\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.00979948044,4.72288911157), test loss: 2.87549366057\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (23.094543457,39.1426420504), test loss: 28.9576754093\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.80000066757,4.67061995943), test loss: 3.00474500358\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (22.8940639496,38.8110500466), test loss: 32.3899983168\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.56575846672,4.61965198586), test loss: 2.9502391845\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (34.8617248535,38.4850667003), test loss: 29.8948911905\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.59633731842,4.56999389881), test loss: 3.10357771814\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (31.7848167419,38.1554512182), test loss: 28.7006169796\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (5.33495855331,4.52193974499), test loss: 2.34103744626\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (27.846031189,37.8309040721), test loss: 32.2960617065\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.03081989288,4.4754498436), test loss: 3.19533979297\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (7.11467266083,37.5021803837), test loss: 24.9646331787\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.0891263485,4.4301601004), test loss: 2.17693463862\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (8.51840591431,37.1806165215), test loss: 31.880506134\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.8416980505,4.38658529192), test loss: 3.05350488424\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (22.0990600586,36.8667612941), test loss: 24.3143259048\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.19624829292,4.3442205291), test loss: 2.35034140646\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (12.3057832718,36.5512151175), test loss: 32.2903202534\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (3.23904418945,4.30321899092), test loss: 2.88064073026\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (15.5035133362,36.2440108121), test loss: 22.5701530457\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.37821507454,4.26317793544), test loss: 2.38281592727\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (2.12534952164,35.9480876549), test loss: 33.3545379162\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.12779688835,4.22420860294), test loss: 3.10331599116\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (7.98928785324,35.6561422259), test loss: 28.2529151917\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.23633575439,4.18668979215), test loss: 3.08895402253\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (13.6420688629,35.3735063661), test loss: 27.3621921539\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.61690151691,4.15004240161), test loss: 2.59636184573\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (42.3579559326,35.0936951102), test loss: 31.9113452911\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.09484374523,4.11459512829), test loss: 3.23232835531\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (34.1198730469,34.8231429902), test loss: 27.8325832367\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.50419020653,4.08058407395), test loss: 2.28261071742\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (21.4590740204,34.5606809789), test loss: 31.0432940722\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.3329205513,4.04748533889), test loss: 3.10152584016\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (20.3704986572,34.3011317094), test loss: 27.7938437939\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.58358085155,4.01546178851), test loss: 2.42202402949\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (21.8127765656,34.0510428263), test loss: 33.6129765987\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.89469587803,3.9843312965), test loss: 3.0882301271\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (12.1325721741,33.8087729085), test loss: 24.9655569077\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.94560742378,3.95387648091), test loss: 2.36204852164\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (29.109205246,33.5748860965), test loss: 34.2272528648\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.72955405712,3.92435390271), test loss: 3.14286557138\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (29.5384941101,33.3458205416), test loss: 28.5596412897\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (4.03489112854,3.8956319175), test loss: 3.05078359842\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (14.1349086761,33.1220359852), test loss: 34.9649186134\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.41145253181,3.86774207114), test loss: 3.18016301394\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (14.4077959061,32.9031802041), test loss: 28.9708645344\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.11354231834,3.840864269), test loss: 3.16642231941\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (9.47872829437,32.6929624074), test loss: 30.4820422173\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.43152761459,3.81476069195), test loss: 2.45143986046\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (8.9499912262,32.485690663), test loss: 30.9640169859\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.0540471077,3.78944736935), test loss: 3.19199457318\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (4.47858142853,32.2851071937), test loss: 27.8730877876\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.756741821766,3.76471067655), test loss: 2.32795560956\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (8.68645763397,32.0898605126), test loss: 31.7300340176\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.87185907364,3.74052257479), test loss: 3.08940540552\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (14.5724983215,31.9019435523), test loss: 25.5751870155\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.20965409279,3.71679056649), test loss: 2.46521103978\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (12.9115695953,31.7174969455), test loss: 33.460124588\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.64481830597,3.69372767073), test loss: 3.03344296217\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (38.5356559753,31.5375787152), test loss: 30.1852779388\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.30468964577,3.67135922688), test loss: 3.04533551931\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (4.59147977829,31.3600630398), test loss: 34.7776077747\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.38693380356,3.64968892163), test loss: 3.22937033176\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (20.996925354,31.1899521843), test loss: 27.9889951229\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.22870445251,3.62863252647), test loss: 3.12377947569\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (53.3053665161,31.0237026316), test loss: 30.6889215469\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (4.17060947418,3.60818288724), test loss: 2.61460969746\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (22.8429145813,30.8584926192), test loss: 30.6827978134\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.8850518465,3.58799064305), test loss: 3.37415740192\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.9239406586,30.6988120662), test loss: 29.3286327124\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.68835830688,3.56831610428), test loss: 2.31981668472\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (7.50454187393,30.5453487936), test loss: 31.471331358\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.667982459068,3.54887287704), test loss: 3.11793289483\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (17.9803886414,30.3945129383), test loss: 26.8835638046\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.75941467285,3.53008545834), test loss: 2.51387494802\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (25.1098384857,30.2465982056), test loss: 34.4662978172\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (6.27002334595,3.51174263639), test loss: 3.09873582721\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (31.1321544647,30.1005805268), test loss: 25.5459797382\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (2.34791040421,3.49374374515), test loss: 2.43099476248\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (11.5048351288,29.9592855576), test loss: 35.2675179958\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.35358655453,3.47631973893), test loss: 3.21604336798\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (8.1904706955,29.8221885397), test loss: 29.7904465914\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.622967600822,3.45938057695), test loss: 3.23124512434\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (17.7983856201,29.6848052632), test loss: 36.4186489105\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.49636650085,3.44271094292), test loss: 3.25261138976\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (19.6195755005,29.5519487179), test loss: 30.0386902332\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.70527172089,3.42631987124), test loss: 3.19654845595\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (22.1560649872,29.423736005), test loss: 30.8028920174\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (5.30148267746,3.41017634678), test loss: 2.49181781411\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (16.5036888123,29.2974800553), test loss: 33.1205157995\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.27202582359,3.39440552987), test loss: 3.18640426099\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (12.5939426422,29.1738649286), test loss: 27.9593186855\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.60286080837,3.37907871016), test loss: 2.4093760252\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (16.3049964905,29.0508592128), test loss: 32.5672235727\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.96247196198,3.36393635334), test loss: 3.12402005196\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (11.6723327637,28.9318206496), test loss: 26.3136099815\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.09334611893,3.34927733211), test loss: 2.51428695023\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.30272865295,28.8166690653), test loss: 34.011493659\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.6080468297,3.33503631825), test loss: 3.02264596075\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (15.340587616,28.6998966403), test loss: 31.1172362804\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.924402475357,3.32094039771), test loss: 3.13186097741\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (23.0209293365,28.5870859992), test loss: 35.4644237518\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.54955530167,3.30707220702), test loss: 3.26815122366\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (6.38863277435,28.4781429165), test loss: 28.7083412647\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.46053767204,3.29340806138), test loss: 3.23688318133\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (16.8263702393,28.3711358789), test loss: 30.0543561459\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.79752588272,3.2799588235), test loss: 2.64664165378\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (14.8031044006,28.2655037847), test loss: 31.0077985287\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.41556584835,3.26686562219), test loss: 3.33117027879\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (34.1686630249,28.1606838372), test loss: 29.2920907021\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.86750257015,3.25396685597), test loss: 2.25448206961\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (24.4320144653,28.0587431673), test loss: 31.5089622021\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.87454509735,3.24145098023), test loss: 3.07737045884\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (10.9865036011,27.9593171456), test loss: 28.4761950493\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.04926025867,3.22925474866), test loss: 2.52673509121\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (6.68715333939,27.8589903969), test loss: 34.8342952728\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.07785177231,3.21718071232), test loss: 3.06055253148\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (22.3890190125,27.7620081397), test loss: 25.7355583072\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.71708846092,3.20524300941), test loss: 2.44602864683\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (16.7349891663,27.6682885491), test loss: 35.381318903\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.825752794743,3.19348083879), test loss: 3.23547557592\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (8.16659927368,27.575444531), test loss: 29.3060124636\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (3.40920758247,3.18192625122), test loss: 3.17147480249\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (28.902223587,27.4842411643), test loss: 31.4020049095\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.54962849617,3.17058037305), test loss: 2.74771695137\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (7.29620075226,27.3925956049), test loss: 30.9336692333\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.78347468376,3.15940347021), test loss: 3.27475278378\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (7.75100421906,27.3038781022), test loss: 30.6551345348\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.13245368004,3.14857834324), test loss: 2.49369829893\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (20.3759250641,27.2180913409), test loss: 31.9902467251\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.818471312523,3.13797521321), test loss: 3.13314794004\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (33.2585754395,27.1306613685), test loss: 29.2660115242\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.38282716274,3.1274983066), test loss: 2.43306415975\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (10.5810556412,27.045580593), test loss: 33.4114342213\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (3.24865007401,3.11713526632), test loss: 3.13425055742\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (14.3427715302,26.9638009067), test loss: 25.9212338448\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.78973519802,3.10685535368), test loss: 2.51692573428\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (3.20739388466,26.8824497077), test loss: 34.4025062323\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.03162324429,3.09683558955), test loss: 3.10907070339\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (17.2356357574,26.8024938101), test loss: 30.7030600548\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.976902723312,3.08686354604), test loss: 3.17034148574\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (46.6997413635,26.72226213), test loss: 35.383921361\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.14399778843,3.07707091436), test loss: 3.23346803188\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (40.0223274231,26.6441091926), test loss: 29.4115605593\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.53841543198,3.06759608627), test loss: 3.2200437367\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (14.8821296692,26.5681439819), test loss: 31.0973407269\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.945138990879,3.05823724635), test loss: 2.52600210309\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (18.0503158569,26.4911729585), test loss: 30.1196244717\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.862940788269,3.04905574781), test loss: 3.29369364977\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (14.7790336609,26.4165458362), test loss: 29.3472646236\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.649924278259,3.03999720907), test loss: 2.27973888516\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (8.01485919952,26.3433882263), test loss: 32.1319621325\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.32465195656,3.03093307378), test loss: 3.13251817375\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (19.8684120178,26.2720120647), test loss: 26.1725538254\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.77425885201,3.02206741757), test loss: 2.44929890037\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (21.4739055634,26.2006861294), test loss: 34.2098316193\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (4.03906154633,3.01329188794), test loss: 3.08601271808\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (44.4978561401,26.1302451099), test loss: 25.73218081\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.78702926636,3.0045940035), test loss: 2.42897280455\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (11.2417182922,26.0597399362), test loss: 35.3189337254\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.7455290556,2.99617373927), test loss: 3.23181204498\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (7.58101701736,25.9922896647), test loss: 28.7679009676\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.24821019173,2.9878837477), test loss: 3.16933342814\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (12.402551651,25.9242327983), test loss: 30.5490411282\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.37658548355,2.97978138521), test loss: 2.71071861684\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (7.39943885803,25.8576225561), test loss: 30.9738788128\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.74174928665,2.97174006852), test loss: 3.29570534825\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (10.9741687775,25.7917505304), test loss: 30.1674223423\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.12047147751,2.96373017556), test loss: 2.31545801908\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (8.53585624695,25.7279110572), test loss: 32.1986106396\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.60612177849,2.95573990722), test loss: 3.04668777287\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (13.2365570068,25.6642388175), test loss: 28.4358579636\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.430085480213,2.94788905994), test loss: 2.4359546721\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (19.210767746,25.600965233), test loss: 33.7218907595\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.9404848814,2.94017943817), test loss: 3.12421757579\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (7.13083314896,25.5380046065), test loss: 26.6520708561\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.06656801701,2.93263642634), test loss: 2.52224473506\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (11.5763206482,25.4772397591), test loss: 35.0256302357\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.19217562675,2.92525780207), test loss: 3.10147555172\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (50.0758094788,25.4170595821), test loss: 29.6768624306\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (3.59413480759,2.91801360067), test loss: 3.12347204089\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (13.4114017487,25.3558286657), test loss: 36.2299494267\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.19751524925,2.91075367943), test loss: 3.20634429753\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (9.5220041275,25.2964853771), test loss: 29.2323511124\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.667614996433,2.90359040065), test loss: 3.19335609972\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (15.5038385391,25.2389941141), test loss: 31.3290765047\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.95269274712,2.89639732037), test loss: 2.46097058356\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (21.195520401,25.1817597461), test loss: 32.1626792908\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (5.84296512604,2.88941318707), test loss: 3.19368816614\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (14.1347570419,25.1246872922), test loss: 28.2248559952\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (5.43360328674,2.88249034176), test loss: 2.27639910579\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (20.1343860626,25.0676256419), test loss: 32.5345610619\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.02129101753,2.8756312423), test loss: 3.11849750727\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (9.20996570587,25.0123141809), test loss: 26.4630620956\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.27959704399,2.86896087278), test loss: 2.44494773149\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (9.97369766235,24.9580362896), test loss: 34.7305230141\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.582703232765,2.86242372747), test loss: 3.03076697439\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (15.4656906128,24.9024103331), test loss: 24.4889970779\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.55361521244,2.85591058124), test loss: 2.54767804742\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (24.0085487366,24.8486236949), test loss: 35.570430398\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.41860103607,2.84942267359), test loss: 3.18421678543\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (25.279712677,24.7964346043), test loss: 28.8731778622\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (4.77513599396,2.84295451494), test loss: 3.10396938473\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (10.2333869934,24.7441996561), test loss: 29.4391993046\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.10982251167,2.83659444552), test loss: 2.63265226483\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (25.0815162659,24.6925742953), test loss: 30.8355624914\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.57462787628,2.83036170859), test loss: 3.22064628005\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (13.0217895508,24.6403409471), test loss: 29.3917141914\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.31793141365,2.82412283375), test loss: 2.25679069161\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (10.1302280426,24.5898328576), test loss: 31.3103310108\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (3.35486221313,2.81807664668), test loss: 3.06362212002\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (15.8240947723,24.5406161778), test loss: 30.7122370243\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.12429654598,2.81214826594), test loss: 2.5096788615\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (9.95835018158,24.4895007343), test loss: 33.931415844\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.404508173466,2.80621428896), test loss: 3.08418785334\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (11.0746936798,24.4403255008), test loss: 25.7511013985\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.86638426781,2.80032871217), test loss: 2.376606673\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (11.1279382706,24.3925936087), test loss: 35.0096266031\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.897050082684,2.79446758388), test loss: 3.1344674468\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (19.3498249054,24.3449308681), test loss: 29.3903031111\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.92671298981,2.78866642939), test loss: 2.97314478159\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (14.4667482376,24.2975612304), test loss: 36.0237484932\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.29407453537,2.78296449138), test loss: 3.19268699288\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (31.7837543488,24.2499294723), test loss: 29.5774306774\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.92084169388,2.77728322136), test loss: 3.15751867294\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (20.6526470184,24.2034681639), test loss: 31.4043554783\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.2565882206,2.77174904987), test loss: 2.48783694357\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (13.2603721619,24.1580576157), test loss: 31.2734130621\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.61543512344,2.76633842187), test loss: 3.15062417686\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (14.9637527466,24.1113495482), test loss: 28.3580462933\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.30277609825,2.76093739527), test loss: 2.26086227298\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (32.4699401855,24.0661969482), test loss: 32.0463005781\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.04936695099,2.75555600724), test loss: 3.04401195943\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (22.0470943451,24.0221696219), test loss: 26.2986515522\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.3288424015,2.75019174379), test loss: 2.42614548504\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (12.6648254395,23.9782047161), test loss: 33.5837377071\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (3.13323831558,2.74488967532), test loss: 2.94454574734\n",
      "\n",
      "MC # 3, Hype # hyp4, Fold # 7\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold7/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (314.739074707,inf), test loss: 171.847942352\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (317.116821289,inf), test loss: 381.392504883\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (24.5185909271,69.384014616), test loss: 39.1698842049\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.08060359955,78.9572597335), test loss: 2.88766415119\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (36.6826438904,58.6334486413), test loss: 34.1747137308\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.92426419258,40.9207030248), test loss: 2.69855570793\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (19.340877533,54.9357042786), test loss: 40.1446326017\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.38465440273,28.215916628), test loss: 2.99331745505\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (31.4940643311,53.1663340015), test loss: 34.5383732796\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.17209291458,21.8668309292), test loss: 3.00985680223\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (30.7167205811,51.9932283342), test loss: 42.8272816658\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.3067035675,18.0607523744), test loss: 2.6506100297\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (65.1493301392,51.161365123), test loss: 37.449516058\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.2396440506,15.5270458654), test loss: 3.05808371305\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (27.1634750366,50.5087752395), test loss: 44.4478140354\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.3737514019,13.7124697598), test loss: 2.4528922081\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (45.6605987549,49.9742109902), test loss: 36.8077604294\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.73748660088,12.3524963781), test loss: 3.19696919918\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (37.2437515259,49.5040582923), test loss: 40.8106460571\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.06234836578,11.2954053479), test loss: 2.43248139322\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (32.2931747437,49.104667287), test loss: 35.6731982231\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.61858439445,10.4472742687), test loss: 3.07402393818\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (81.6365890503,48.8042661338), test loss: 39.1965132713\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.44968950748,9.7560977395), test loss: 2.52293955684\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (57.300365448,48.4789704012), test loss: 39.5988889217\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.07581472397,9.17977039678), test loss: 3.04123814106\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (23.940164566,48.1798113618), test loss: 37.1193036556\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.937848687172,8.68968779326), test loss: 2.61843828261\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (111.950881958,47.8987341224), test loss: 36.2398192406\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (5.70399713516,8.27144240373), test loss: 2.74633307457\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (77.5153045654,47.5984392818), test loss: 30.5172114372\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.07672023773,7.90684897523), test loss: 2.75277245343\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (70.6384887695,47.3433155523), test loss: 36.4261374474\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.17639398575,7.58738404759), test loss: 2.8667316854\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (13.845790863,47.0726469161), test loss: 32.3136857033\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.80984044075,7.30520674069), test loss: 2.98928661942\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (28.9994163513,46.8378823652), test loss: 39.4507299185\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.28033161163,7.05515300737), test loss: 2.55182494223\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (30.9799346924,46.581663803), test loss: 36.981696701\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.59305810928,6.83080072334), test loss: 3.0921595782\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (68.2569122314,46.3285412509), test loss: 40.1190135479\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (5.95783805847,6.6277492994), test loss: 2.40494327843\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (26.1685142517,46.066776571), test loss: 34.4710726261\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.14487457275,6.44427042899), test loss: 3.09173100591\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (32.2143554688,45.7989869333), test loss: 34.2423701286\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.21703481674,6.27679360254), test loss: 2.32466678917\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (16.8395347595,45.5302527041), test loss: 33.9711146355\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.45208156109,6.12192418072), test loss: 3.02926151156\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (29.3349227905,45.2784819724), test loss: 34.0473195076\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.1695086956,5.98020679483), test loss: 2.46959353387\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (28.5295257568,45.0116671327), test loss: 35.9879915476\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.32665395737,5.84972595726), test loss: 2.88243744969\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (50.4187774658,44.7391838004), test loss: 30.1038310528\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (3.26740121841,5.72925015326), test loss: 2.4461127013\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (14.7903289795,44.4572430853), test loss: 31.6688873768\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.04439330101,5.61575090346), test loss: 2.74612957835\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (35.8893127441,44.1639156112), test loss: 25.9965780735\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.65191626549,5.50977071199), test loss: 2.69300411642\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (27.9787368774,43.8574016192), test loss: 31.4309786081\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.88959503174,5.40952515663), test loss: 2.7134409517\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.5346221924,43.5385882621), test loss: 27.6155391455\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.01741838455,5.31169408376), test loss: 2.90912551582\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (51.4068870544,43.2241718449), test loss: 33.213451004\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.63512814045,5.2195830247), test loss: 2.41087050438\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (42.5840377808,42.8868391163), test loss: 30.9519874096\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.50817608833,5.13226827739), test loss: 3.05674552917\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (22.1169242859,42.5459227819), test loss: 32.9641793251\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (0.727627754211,5.04916098123), test loss: 2.21244393587\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (84.7968978882,42.1984516525), test loss: 29.3111026049\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.78464031219,4.97086832485), test loss: 2.94930856824\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (46.9074249268,41.8306274497), test loss: 29.1184153557\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.39418745041,4.89554286183), test loss: 2.38003727794\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (47.1182327271,41.471001849), test loss: 29.7192713499\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.05705690384,4.82354142509), test loss: 2.93054981232\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (10.6510257721,41.1000125573), test loss: 27.3326686144\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.25871992111,4.75474334914), test loss: 2.41004487425\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (22.6239719391,40.734293289), test loss: 28.1888759613\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.42915344238,4.68942551352), test loss: 2.73190349936\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (23.0043334961,40.3596268312), test loss: 23.1131915569\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.50936746597,4.62685169678), test loss: 2.25703844205\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (34.5949859619,39.9875353094), test loss: 26.1246165514\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.84212517738,4.56679711633), test loss: 2.71468259543\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (32.012260437,39.6106279577), test loss: 21.4690654278\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.04015541077,4.50950414848), test loss: 2.79678910822\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (15.381986618,39.234668211), test loss: 27.0382221937\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.94084024429,4.45440985304), test loss: 2.73748283535\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (14.6710233688,38.8673036174), test loss: 23.0973419189\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.04166078568,4.40105019406), test loss: 2.9601509437\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (12.2373752594,38.5078764637), test loss: 28.1773655415\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.506537079811,4.34984994071), test loss: 2.33838179708\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (29.740026474,38.1503429808), test loss: 25.6664467573\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.18155431747,4.3010289197), test loss: 3.11596459448\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (21.1354866028,37.8016422771), test loss: 28.5297283649\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.49966812134,4.25449064102), test loss: 2.22890624702\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (10.7936658859,37.4619690011), test loss: 26.3510943651\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.12438011169,4.20945959194), test loss: 2.89268519878\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (18.7358627319,37.1250380516), test loss: 27.1421680927\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.04343485832,4.16611407958), test loss: 2.44613282084\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (14.7919540405,36.7977045407), test loss: 27.9082016706\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.0910551548,4.12442902942), test loss: 2.97561030388\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (18.7330245972,36.4816410866), test loss: 26.2866003036\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.44866669178,4.08370009714), test loss: 2.58294234574\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (20.6143188477,36.1752609308), test loss: 26.2601744175\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.07774996758,4.04481085666), test loss: 2.72566199303\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (22.0752182007,35.8729612851), test loss: 23.0523199558\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.78179442883,4.00727738379), test loss: 2.49085091352\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (17.257522583,35.5823166739), test loss: 26.4541172147\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.598743200302,3.97124443454), test loss: 2.74226849973\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (55.42710495,35.3019243704), test loss: 23.0889830828\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.01210403442,3.93675028797), test loss: 2.92686862797\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (40.869972229,35.0225087363), test loss: 27.3844450951\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (4.64893436432,3.90303867038), test loss: 2.56288516074\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (26.3879470825,34.7575719893), test loss: 23.0686611652\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.10896039009,3.87023532596), test loss: 3.03138952851\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (12.3800487518,34.49764413), test loss: 30.6703376293\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.00443172455,3.83845844487), test loss: 2.43614069223\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (17.5646896362,34.2469102933), test loss: 25.6603997946\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (4.68422937393,3.80797862408), test loss: 3.1879986912\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (27.1091938019,34.0023221902), test loss: 30.5599364281\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.65256094933,3.77846084557), test loss: 2.35146613121\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (15.2980651855,33.7666281736), test loss: 25.5456749439\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.23481488228,3.7498882094), test loss: 2.90669384897\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (31.4535675049,33.5348896482), test loss: 30.0593957424\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.52847146988,3.72232974292), test loss: 2.53897522986\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (11.8787336349,33.3083873743), test loss: 27.0948595047\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.46905016899,3.69550828742), test loss: 2.90929062366\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (11.3152618408,33.092334439), test loss: 27.3406399727\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.49888324738,3.66913761445), test loss: 2.60316370726\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.2369480133,32.8819266827), test loss: 25.8385107994\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.412980884314,3.64355504109), test loss: 2.69006584287\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (40.2658538818,32.6746694384), test loss: 25.2268920898\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.22236895561,3.61889539182), test loss: 2.78826438636\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.2805347443,32.4745965685), test loss: 27.144396019\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.97128272057,3.5952284002), test loss: 2.87296938896\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (12.2849187851,32.2817878065), test loss: 23.9550944805\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.971990942955,3.57211555722), test loss: 2.99639211595\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (16.8361721039,32.0897908663), test loss: 27.8279084444\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.83533465862,3.54956343362), test loss: 2.52731871307\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (14.1471309662,31.9043413896), test loss: 26.2597243786\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (3.11933898926,3.5276604887), test loss: 3.02583282888\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (25.360956192,31.7256363298), test loss: 30.9631110668\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.22708940506,3.50593077268), test loss: 2.37044342756\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (17.7086982727,31.5508022021), test loss: 26.7578875542\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.27498888969,3.48501305981), test loss: 3.12924440205\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.3599662781,31.3773881756), test loss: 29.6340128422\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.9944665432,3.46457031789), test loss: 2.3150831297\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.0405902863,31.2109645087), test loss: 25.500078392\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.455470979214,3.44485275141), test loss: 2.91830329895\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (52.5244560242,31.0498990965), test loss: 30.5679238319\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.75427770615,3.42579924657), test loss: 2.63166276515\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (43.7758865356,30.8880608643), test loss: 26.9834049225\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (4.95334672928,3.40696550655), test loss: 2.85616019368\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (22.247127533,30.7339719053), test loss: 27.1105988979\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.964654684067,3.38838751456), test loss: 2.62911671102\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (13.7440719604,30.5820014068), test loss: 26.1114356041\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.21193957329,3.37024829197), test loss: 2.66418710947\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (18.5136356354,30.4338632337), test loss: 25.4647675991\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (4.83887386322,3.35268356902), test loss: 2.97554190755\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (27.3303070068,30.2888353945), test loss: 28.5488009453\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.74555373192,3.33553819029), test loss: 2.93087922335\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (10.4824256897,30.1482584559), test loss: 24.4339399338\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.42082881927,3.31881122935), test loss: 3.02170925438\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (32.2973709106,30.0091894693), test loss: 29.2405148029\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.07886338234,3.30256018577), test loss: 2.52115829885\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (13.9292564392,29.872021068), test loss: 26.4672379494\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.32171440125,3.2866037718), test loss: 3.09934606552\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (9.07932186127,29.7410271663), test loss: 31.3712226391\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.0988574028,3.27072493785), test loss: 2.43617804945\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.1275348663,29.6120960323), test loss: 26.6509889364\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.423346757889,3.25522019176), test loss: 3.07801180184\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (42.4417915344,29.4839875301), test loss: 27.9859485626\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.95012521744,3.24013823079), test loss: 2.25811659992\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (11.9357061386,29.3597681311), test loss: 27.7105644226\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.49577152729,3.22560208738), test loss: 2.99252429605\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (13.1026363373,29.2396739615), test loss: 28.9995486259\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.00889158249,3.21132002208), test loss: 2.61005481333\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (16.6046981812,29.1185403609), test loss: 27.6343637466\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.7435721159,3.19725286384), test loss: 2.86066803783\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (13.1954536438,29.0013285857), test loss: 25.4531726122\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.76658797264,3.1834984542), test loss: 2.53380785286\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (25.5796356201,28.8877607579), test loss: 26.8547067404\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.08264255524,3.16971727086), test loss: 2.7870002836\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (14.7603225708,28.7756943931), test loss: 25.0362685919\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.28145456314,3.15639675119), test loss: 2.93578484356\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (12.5327835083,28.6633969126), test loss: 28.9314287663\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.00226211548,3.14326017818), test loss: 2.88482685089\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (15.7319984436,28.5557244746), test loss: 24.3388776302\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.502502441406,3.13056939738), test loss: 3.0547804594\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (51.7277908325,28.4507616204), test loss: 29.8866556406\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.42092633247,3.11823477094), test loss: 2.55593320131\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (44.1520767212,28.3444492357), test loss: 26.7460426569\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (4.84610462189,3.10597090871), test loss: 3.1131642729\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (16.7528572083,28.2427217259), test loss: 30.2318428516\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.937648415565,3.09376430109), test loss: 2.26809726506\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (13.8380098343,28.1420319218), test loss: 28.0029768467\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.36036801338,3.08180551305), test loss: 3.06591414809\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (20.682970047,28.0430096518), test loss: 28.4775105476\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (5.08483982086,3.07014693881), test loss: 2.40431834757\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (26.6051139832,27.9456784432), test loss: 28.2562176704\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.89178991318,3.05871786067), test loss: 2.97911168039\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (11.7617530823,27.850936486), test loss: 27.9423048019\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.32129943371,3.04751860397), test loss: 2.56142307222\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (32.7538566589,27.7565847639), test loss: 28.4506996155\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.71026349068,3.03659686769), test loss: 2.79712682366\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (11.3002891541,27.6629526973), test loss: 24.8375654221\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.30138230324,3.02582995014), test loss: 2.48423285186\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (8.74493980408,27.5735720923), test loss: 27.5101183176\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.04528701305,3.01503081322), test loss: 2.81413712502\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (10.9635601044,27.4848458908), test loss: 24.2103275061\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.443024516106,3.00444793594), test loss: 2.96321226358\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (37.5695266724,27.3960803258), test loss: 29.2808305025\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.57978439331,2.99409761616), test loss: 2.90343345702\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (11.5931816101,27.3098374884), test loss: 24.5370530605\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.28706634045,2.9841043668), test loss: 3.05658926368\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (13.8435344696,27.2262647514), test loss: 29.495503521\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.04122567177,2.9742561909), test loss: 2.44278357029\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (16.0242042542,27.1411042393), test loss: 26.7783298969\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.40774130821,2.96450136361), test loss: 3.17398227155\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (12.6766586304,27.0587010275), test loss: 30.370177269\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.41223263741,2.95493375084), test loss: 2.27512049675\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (25.6487007141,26.9785190885), test loss: 27.1220838547\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.09538435936,2.94528199097), test loss: 2.97225868106\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (14.3940067291,26.8990195737), test loss: 29.1393989086\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.02692055702,2.93593024402), test loss: 2.40440364033\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (14.6032180786,26.8186896026), test loss: 28.5940726757\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.84275889397,2.92665075435), test loss: 3.03803222477\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (15.7263660431,26.7417948256), test loss: 27.8808293819\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.622478067875,2.91767958931), test loss: 2.53891912997\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (39.4268493652,26.6662610718), test loss: 29.4198834658\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.93072319031,2.9089294617), test loss: 2.93422648311\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (42.9829139709,26.5897168821), test loss: 24.2863471508\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (4.62194490433,2.90021318813), test loss: 2.36553756297\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (16.3635005951,26.5160922129), test loss: 27.7051020145\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.31718623638,2.891471428), test loss: 2.8262083441\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (13.8575124741,26.4429874741), test loss: 24.3214274883\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (3.48193216324,2.88290124519), test loss: 2.86340662539\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (22.009765625,26.3706805752), test loss: 29.7970417023\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (5.21307468414,2.87450381784), test loss: 2.90102167726\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (25.1671962738,26.2993712972), test loss: 24.6340964079\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.28914690018,2.86625614652), test loss: 3.04082019031\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (13.2219142914,26.2298544201), test loss: 30.5119548798\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.32193589211,2.85814328212), test loss: 2.5098595351\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (33.1289329529,26.1602416612), test loss: 26.9116802692\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.7351629734,2.85022491848), test loss: 3.21592097282\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (9.26618766785,26.0909652043), test loss: 30.9628706932\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.36054086685,2.84239538596), test loss: 2.30581653118\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (10.5778980255,26.0248210318), test loss: 26.5368901253\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.40724396706,2.83449321227), test loss: 2.92460726202\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (10.9733066559,25.958780172), test loss: 30.1055738926\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (0.441281944513,2.8267329314), test loss: 2.46671469957\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (33.6112518311,25.8923250966), test loss: 27.962017262\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.17980408669,2.81911390834), test loss: 2.94865359664\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (7.97344255447,25.827716203), test loss: 27.8933585644\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.75028514862,2.81175289901), test loss: 2.59649426639\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (14.7433261871,25.7650939306), test loss: 26.9279034615\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.45284295082,2.80448365756), test loss: 2.72903540134\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (15.0092601776,25.7007605946), test loss: 26.4756473541\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.13529646397,2.79725658008), test loss: 2.7805716604\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (11.8534507751,25.6385501668), test loss: 27.752887845\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.23002386093,2.79015218369), test loss: 2.82275697887\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (32.7627601624,25.577876947), test loss: 24.4550900459\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.23240733147,2.78295283412), test loss: 2.92260919511\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (14.3001441956,25.5174633836), test loss: 27.9630952358\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.68723905087,2.77595977726), test loss: 2.51663121581\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (19.6869010925,25.4560435463), test loss: 25.5393080473\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.87754130363,2.76899729703), test loss: 2.96245144606\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (15.8131694794,25.3973427852), test loss: 31.4019100666\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.758297622204,2.76225880832), test loss: 2.42465390712\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (34.1979522705,25.3392239938), test loss: 27.7221046448\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.9142973423,2.75566984509), test loss: 3.24399841428\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (35.0751152039,25.2805065611), test loss: 30.3966950893\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (3.65580034256,2.74909556806), test loss: 2.36715694964\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (16.9214992523,25.2239200652), test loss: 26.9003849268\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.25271272659,2.74248240892), test loss: 2.94072994292\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (14.1672639847,25.1674708504), test loss: 29.7225922108\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (3.51976370811,2.73598423671), test loss: 2.48309083283\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (23.2304039001,25.1114676278), test loss: 27.4499245644\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (5.32358407974,2.72959390513), test loss: 2.85187717676\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (24.4143600464,25.0559948728), test loss: 27.5066542149\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.18934249878,2.72330380605), test loss: 2.58480274677\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (13.2344684601,25.0019848578), test loss: 26.351499033\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.44324243069,2.71710166412), test loss: 2.69607903957\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (33.3461837769,24.9475744344), test loss: 27.1242239237\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.77132678032,2.71104688289), test loss: 2.81349996328\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (9.16092681885,24.8934368578), test loss: 28.7578991413\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.4233789444,2.70504756165), test loss: 2.89032153487\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (11.296497345,24.8416546712), test loss: 25.1692655563\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.6091016531,2.69896257651), test loss: 2.95507866591\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (7.71834182739,24.7897443503), test loss: 29.1863051414\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.423768341541,2.69297817451), test loss: 2.55616280735\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (35.6092987061,24.7373466151), test loss: 26.0810404301\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.54492402077,2.68708704674), test loss: 3.08126532435\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.65053129196,24.686400549), test loss: 31.3735752583\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.29602956772,2.68139562861), test loss: 2.38039078712\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (15.058385849,24.6369106715), test loss: 27.4256525278\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.82156348228,2.67575899378), test loss: 3.16898317039\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (13.8820199966,24.5858399978), test loss: 29.814555788\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.944733262062,2.67014767708), test loss: 2.29556953609\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (10.6052808762,24.5364611351), test loss: 26.9655880451\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.92502856255,2.66461868308), test loss: 2.96469489038\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (36.6066322327,24.4882018794), test loss: 28.8806665421\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.38059139252,2.65900016724), test loss: 2.50745156705\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (18.6208362579,24.4400312135), test loss: 27.9828464389\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.34266984463,2.6535288826), test loss: 2.8632771939\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (19.0327968597,24.3908288669), test loss: 26.4771791935\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.14003324509,2.64807200775), test loss: 2.50600973815\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (16.3253746033,24.3438558695), test loss: 26.9087814331\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.831275224686,2.64278106954), test loss: 2.65559867918\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (43.3236274719,24.2972234061), test loss: 27.3987435341\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (3.27320861816,2.63761007652), test loss: 3.01854181141\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (27.6023731232,24.2499458137), test loss: 29.3755035996\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.95242595673,2.63242650218), test loss: 2.90104292333\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (17.345703125,24.2045271265), test loss: 25.0074338436\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.17864620686,2.62721671013), test loss: 2.96694405973\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (12.4061279297,24.1589468856), test loss: 29.9221078396\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (3.36475992203,2.6220807155), test loss: 2.51809313297\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (23.3104515076,24.1136899358), test loss: 27.5264169693\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (5.1569442749,2.61701825325), test loss: 3.1371884495\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (22.2126693726,24.0686598463), test loss: 30.9244725227\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (3.12802410126,2.61202564238), test loss: 2.23911520094\n",
      "\n",
      "MC # 3, Hype # hyp4, Fold # 8\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold8/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (401.578582764,inf), test loss: 227.308965302\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (232.772735596,inf), test loss: 293.080935669\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (51.8620300293,119.153868523), test loss: 46.9560413361\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.66081047058,43.6438478472), test loss: 3.61672519445\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (22.4694271088,81.4636101933), test loss: 30.7425188541\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.3632311821,23.6251283525), test loss: 2.70062860847\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (40.9589538574,68.3350331548), test loss: 42.1991621971\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.55292272568,16.8853327417), test loss: 3.67162117362\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (27.4192237854,61.6766973445), test loss: 34.8843744278\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (3.07610797882,13.5047521646), test loss: 3.30663231015\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (18.7308139801,57.5568772558), test loss: 39.8810487747\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.51933503151,11.4720991209), test loss: 3.42682440877\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (51.9860267639,54.7328517799), test loss: 37.282927227\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (4.57302618027,10.1041373657), test loss: 3.48219195604\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (27.6389141083,52.5772826652), test loss: 36.4978691101\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.65905272961,9.12159123192), test loss: 2.80596629083\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (41.6145210266,50.8703094073), test loss: 41.6481323719\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.02926945686,8.37983741017), test loss: 3.51744039357\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (24.0130805969,49.4887857677), test loss: 35.8660817623\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.2861571312,7.79348800028), test loss: 2.57980226278\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (72.4950332642,48.345951728), test loss: 38.8526757717\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (7.28174829483,7.32354253503), test loss: 3.63028694987\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (38.4761047363,47.3177857568), test loss: 32.3525412083\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (0.644523143768,6.93655311859), test loss: 2.4753813386\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (24.3851509094,46.4038832616), test loss: 39.287370801\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.93556642532,6.60958485039), test loss: 3.41417177022\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (7.12305307388,45.5309607084), test loss: 30.4462939739\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.11848640442,6.3289202858), test loss: 2.64957506955\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (24.3851585388,44.6816171685), test loss: 38.6979943275\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.4096930027,6.08552642405), test loss: 3.44109648466\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (38.6909370422,43.8444202321), test loss: 27.8991908789\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.12821865082,5.86899784537), test loss: 2.60719414651\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (47.9471359253,43.0544244772), test loss: 35.4586219788\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.41742682457,5.67771667219), test loss: 3.31493324041\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (25.6852664948,42.2764200086), test loss: 23.9718792439\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (0.996055603027,5.50678092351), test loss: 2.40994279683\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (31.2083835602,41.5372803219), test loss: 34.1520452976\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.69701385498,5.35229072438), test loss: 3.25622966886\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (21.7782173157,40.8009587449), test loss: 28.2885488987\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.12367200851,5.2110720708), test loss: 3.16670795083\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (23.0311393738,40.0879071579), test loss: 31.7488113403\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.27416014671,5.08214792984), test loss: 3.20087338686\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (18.6245956421,39.4070195709), test loss: 29.4384072781\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.02429914474,4.96152803146), test loss: 3.24884956628\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (18.5310211182,38.7509980758), test loss: 27.7663333893\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.41114974022,4.8512192075), test loss: 2.62688593268\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (12.6758060455,38.1054784405), test loss: 33.4097660065\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.15679311752,4.74966226014), test loss: 3.3787321955\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (22.7324295044,37.4914520793), test loss: 27.6885037661\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.95205527544,4.65549774804), test loss: 2.46233919561\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (25.9375572205,36.8869662573), test loss: 32.4174982548\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.672352910042,4.56744310117), test loss: 3.35452350974\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (14.0093946457,36.3058366407), test loss: 25.678788805\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.69759178162,4.48551409715), test loss: 2.36953168213\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (11.140250206,35.7589752847), test loss: 33.1676020861\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.04988574982,4.40757722757), test loss: 3.33681158125\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (14.6469631195,35.2358557555), test loss: 26.2486910343\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.24081730843,4.33487697672), test loss: 2.63703598976\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (19.7305488586,34.7304710583), test loss: 33.3492306232\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.83910679817,4.26733919317), test loss: 3.29622826576\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (7.67597007751,34.255125191), test loss: 27.2011746407\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.1359359026,4.20412196708), test loss: 2.79061574042\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (28.2526855469,33.7972673723), test loss: 30.5178348064\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.14734280109,4.14436595793), test loss: 3.14436606467\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (25.4177436829,33.360536791), test loss: 24.1756449223\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.72428417206,4.08818012237), test loss: 2.56948859394\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (5.07076644897,32.9529582441), test loss: 32.5553851843\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.91328573227,4.03410146452), test loss: 3.31066652238\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (16.4528808594,32.5657261412), test loss: 29.342057991\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.21328747272,3.98295800372), test loss: 3.34319158942\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (31.0838470459,32.1934178764), test loss: 33.334432888\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.93793463707,3.93505600236), test loss: 3.34779455662\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (8.74400901794,31.8424633475), test loss: 29.9590644836\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.46180057526,3.88995264415), test loss: 3.37867788076\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (39.371799469,31.5067925818), test loss: 28.1513072968\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.99748802185,3.8469908291), test loss: 2.75535755008\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (24.1560726166,31.183512489), test loss: 32.1831606865\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.94872999191,3.80596095913), test loss: 3.4605825007\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (8.72225952148,30.8809026925), test loss: 28.9201751947\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.0053153038,3.76616658182), test loss: 2.47448257506\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (12.6709213257,30.5923272546), test loss: 34.5145339012\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.18332242966,3.72821242767), test loss: 3.39133012593\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (16.4148025513,30.3126538021), test loss: 27.9802811623\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.584300637245,3.69222185283), test loss: 2.44266014546\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (12.247587204,30.0482000123), test loss: 34.1531347275\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.21504473686,3.6583171429), test loss: 3.33182498217\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (7.25541257858,29.7934435485), test loss: 29.1757447243\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.53144645691,3.62578556379), test loss: 2.7420108676\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (19.366230011,29.547219334), test loss: 32.1863676071\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.1604694128,3.59425289139), test loss: 3.27087012529\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (25.2531585693,29.3154182149), test loss: 28.7262813091\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.74125432968,3.56364964314), test loss: 2.83224122822\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (41.4378433228,29.0930420256), test loss: 32.1981130123\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (4.03610181808,3.53401405061), test loss: 3.13921845853\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (6.5849571228,28.874043503), test loss: 25.8104262352\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.29667115211,3.5057102367), test loss: 2.75219900459\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (16.8363819122,28.6677601869), test loss: 33.4874471664\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.09991312027,3.47899749648), test loss: 3.29662051648\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (56.2974510193,28.4691234771), test loss: 30.2622022629\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.97089874744,3.45325505061), test loss: 3.3674300611\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.4443817139,28.272704124), test loss: 32.7090367317\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.86466670036,3.42810116966), test loss: 3.33199039102\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (18.4581813812,28.0889092074), test loss: 30.3013637066\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (6.24367332458,3.4036593811), test loss: 3.38174618483\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (5.06436252594,27.91110202), test loss: 29.8091736317\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.295982033014,3.37954254758), test loss: 2.6978525728\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (8.18567943573,27.7350391151), test loss: 34.4203191757\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.35554027557,3.35663345858), test loss: 3.42005620599\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (10.7183599472,27.5689096286), test loss: 29.399827528\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.77636408806,3.33487430075), test loss: 2.41355698854\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (54.4078178406,27.4094430222), test loss: 34.10457021\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.76844620705,3.31389497849), test loss: 3.32526063025\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (28.4062099457,27.2485614498), test loss: 27.3246342659\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.89027070999,3.2932010343), test loss: 2.40839400142\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (9.86421012878,27.0978649499), test loss: 33.9497741938\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (3.46884346008,3.27302445219), test loss: 3.30674838424\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.0556735992,26.9521002591), test loss: 28.0358435869\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.228858977556,3.25294006686), test loss: 2.73282429278\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (15.9402837753,26.8067675669), test loss: 34.5069216728\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.64416706562,3.23387576886), test loss: 3.18540162444\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (13.8158273697,26.6693784865), test loss: 28.7122177124\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.5229434967,3.21586991683), test loss: 2.8052701354\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (30.1397247314,26.5375260295), test loss: 30.7821799994\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.98423063755,3.19820561474), test loss: 3.07160604894\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (28.1157093048,26.4033493386), test loss: 26.3793822289\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.58967709541,3.18092217267), test loss: 2.87727180719\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (13.1332159042,26.276894056), test loss: 32.8396129608\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.79723072052,3.16381798896), test loss: 3.25263666809\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (19.4716682434,26.1548390965), test loss: 29.2812810898\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (6.35943508148,3.14700892718), test loss: 3.35521930158\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (7.11825704575,26.0318509558), test loss: 34.5467905283\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.77340698242,3.13070746678), test loss: 3.26961567253\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (35.9575538635,25.9167127885), test loss: 31.0442135334\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.03767347336,3.11535243906), test loss: 3.30640998483\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (9.03982543945,25.8041532052), test loss: 29.1507481098\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.05581510067,3.10029044155), test loss: 2.61544608474\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (5.47335577011,25.6898499765), test loss: 31.8884953022\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.736513078213,3.08550643133), test loss: 3.38241962343\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (26.6632137299,25.5826582728), test loss: 28.8971551657\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.45071017742,3.0708505362), test loss: 2.39815399349\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (15.2109966278,25.4779971564), test loss: 34.0822219133\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (5.11281776428,3.05645943423), test loss: 3.26329368353\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (20.5910129547,25.3725874085), test loss: 27.6406785011\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.47841072083,3.04235548741), test loss: 2.39660860747\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (20.6630897522,25.2736668995), test loss: 34.6492472172\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.5978705883,3.0290964435), test loss: 3.24190396667\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (33.1962165833,25.1769712384), test loss: 29.5132899284\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (4.09602546692,3.0160980119), test loss: 2.7297125712\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (32.1223487854,25.0780066791), test loss: 31.5913446903\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.666448056698,3.00317516799), test loss: 3.13871178925\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (44.677154541,24.9853575349), test loss: 28.5665197849\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.85159015656,2.99046771238), test loss: 2.78522215486\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (11.5780467987,24.8935447379), test loss: 32.4626990795\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.715789556503,2.97786350935), test loss: 2.99742408693\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (23.4336166382,24.801837553), test loss: 25.8652950287\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.11686515808,2.96556137661), test loss: 3.00331711769\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (10.7013835907,24.7150351439), test loss: 33.6443722248\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.01442337036,2.953914351), test loss: 3.1814117074\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (20.6770248413,24.6306685259), test loss: 29.4173690796\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.83829545975,2.94253829899), test loss: 3.27717925906\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (22.8432941437,24.5437771005), test loss: 32.764726758\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (4.00063705444,2.93124005997), test loss: 3.23061813861\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (22.2774887085,24.4622602088), test loss: 29.9063506603\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.801278233528,2.91996972274), test loss: 3.28983807564\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (11.2852020264,24.3809672763), test loss: 30.2448824883\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.99848139286,2.90885189675), test loss: 2.54378929436\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (46.7117080688,24.3004353354), test loss: 33.145701766\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.48995471001,2.89800131529), test loss: 3.32528005242\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (3.13445305824,24.2221062465), test loss: 29.6463556767\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.15530920029,2.88767419745), test loss: 2.37471259236\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (10.2072172165,24.1467723536), test loss: 33.274754715\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.888651371002,2.87755698751), test loss: 3.2276556462\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (4.57081365585,24.0684531066), test loss: 27.560021615\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.889915704727,2.86751862208), test loss: 2.43925696462\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (14.9251995087,23.9948578059), test loss: 33.6544549465\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.25291240215,2.85748364814), test loss: 3.23267492652\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (18.5956420898,23.9210988516), test loss: 27.4600095749\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.59148705006,2.84757378438), test loss: 2.67348059267\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (7.84582138062,23.8476234837), test loss: 33.1025285721\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.84041619301,2.83792517305), test loss: 3.0682326898\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (11.6495113373,23.776380324), test loss: 27.2291548252\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.89691638947,2.82864172459), test loss: 2.7103588596\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (9.42700386047,23.707851287), test loss: 30.5283988476\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.74610996246,2.81958721442), test loss: 2.98035619929\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (52.7615242004,23.637704582), test loss: 29.2311779499\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.90784776211,2.81059383265), test loss: 3.09290511012\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (15.0019645691,23.5702466597), test loss: 32.5420865059\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.78484535217,2.80160389286), test loss: 3.14072566628\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (14.7803344727,23.5036961003), test loss: 29.0490935326\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.49373412132,2.79268380537), test loss: 3.22061896324\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (12.0203475952,23.4372097985), test loss: 33.8693142056\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.99249720573,2.78398084553), test loss: 3.16945585907\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (16.7940254211,23.3729751652), test loss: 30.0450146437\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.4294052124,2.77560226198), test loss: 3.22414433211\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (9.04907512665,23.3110011773), test loss: 29.1684263945\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.1006295681,2.76743312933), test loss: 2.57817192972\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (9.89301490784,23.2476065036), test loss: 31.7025334954\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.33285498619,2.75930747123), test loss: 3.31960374713\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (5.1403632164,23.186558557), test loss: 29.137378788\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (3.08094620705,2.75120445578), test loss: 2.37275035381\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (23.8644542694,23.1267165194), test loss: 33.3781801224\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.9376437664,2.74308588818), test loss: 3.16702174544\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (40.4876556396,23.0667206155), test loss: 27.5756440639\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.66289377213,2.73519307326), test loss: 2.43915230632\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (13.687289238,23.007953107), test loss: 34.4592362881\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.71648216248,2.72757257004), test loss: 3.19119989276\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (15.9723243713,22.9517467333), test loss: 28.5719877243\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.35670483112,2.72013854863), test loss: 2.68268571198\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (9.37833118439,22.8938825514), test loss: 31.4643406391\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.28767609596,2.71274734913), test loss: 3.04983008206\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (8.81247329712,22.8383309708), test loss: 27.1948990345\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.71568071842,2.70533557467), test loss: 2.68233953267\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (13.4251003265,22.7838940685), test loss: 31.9029733658\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.87986290455,2.69791591381), test loss: 2.93325195313\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (16.9154815674,22.7291582151), test loss: 28.7608027458\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.96062660217,2.69071417843), test loss: 3.09352780879\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (40.5136604309,22.675911794), test loss: 32.2235741854\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.36401820183,2.68371363125), test loss: 3.07777175009\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (21.7103996277,22.6242335376), test loss: 28.978316021\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.16522479057,2.6768741551), test loss: 3.18803819418\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (22.8868217468,22.5712355796), test loss: 33.2007779121\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.73150610924,2.67009245703), test loss: 3.14252065122\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (11.0094966888,22.5201813682), test loss: 29.7717819929\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (5.91537952423,2.66334015472), test loss: 3.22485843301\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.55224895477,22.470275324), test loss: 29.8306865931\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.83503365517,2.65644978476), test loss: 2.5455922097\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (23.7533378601,22.4202156649), test loss: 32.617434144\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (4.89948225021,2.64983074747), test loss: 3.28257613778\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (9.08486366272,22.3710026157), test loss: 29.3602552891\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.829005658627,2.64329033293), test loss: 2.34435834736\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (21.5127391815,22.3236169486), test loss: 32.4629635334\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.759060263634,2.63696197217), test loss: 3.16150371432\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (23.4566001892,22.2747905372), test loss: 28.6479435921\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (3.39840483665,2.63072985485), test loss: 2.48537218273\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (13.2928152084,22.2275270595), test loss: 33.6187618256\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.13130140305,2.62445632769), test loss: 3.18221949935\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (22.4496173859,22.1817230709), test loss: 27.2920003176\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (3.64840078354,2.61809148965), test loss: 2.6939445883\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (5.66365671158,22.1352166344), test loss: 32.8995748043\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.67758357525,2.61193956101), test loss: 3.02618181556\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (14.6485815048,22.0898664756), test loss: 26.6222688675\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.88704800606,2.60588237072), test loss: 2.64501438141\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (8.84536170959,22.0459451913), test loss: 30.9329059839\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.463926792145,2.59999748976), test loss: 2.9544795841\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (9.48800849915,22.0007084606), test loss: 31.3310067177\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (0.271557748318,2.59422106238), test loss: 3.17823653221\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (8.80480575562,21.956885331), test loss: 31.4081010461\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (3.19541168213,2.58845508252), test loss: 3.0626886636\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (6.83802270889,21.9143596797), test loss: 28.647791028\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.79071855545,2.58254165744), test loss: 3.17476590872\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (10.6447753906,21.8711648969), test loss: 33.7995733261\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.65283024311,2.57681501262), test loss: 3.09059503675\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (11.2759580612,21.8289848954), test loss: 29.8970930338\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.5852022171,2.57119130654), test loss: 3.1645537883\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (8.00089263916,21.7880977391), test loss: 29.6229012012\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.222261935472,2.56571478221), test loss: 2.56465208083\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (12.5812644958,21.7460709628), test loss: 31.2781824112\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.07822287083,2.56037202624), test loss: 3.29045921117\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (6.34066534042,21.7051719328), test loss: 29.1338799477\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.49865198135,2.55504036515), test loss: 2.36911135465\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.76281023026,21.6655882422), test loss: 32.6601163387\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.21971857548,2.54952861161), test loss: 3.1332798481\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (19.8984603882,21.6254632296), test loss: 28.3898415089\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.16380167007,2.54420025066), test loss: 2.48483279347\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (31.0142440796,21.5861656947), test loss: 34.7074743748\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.55340766907,2.53898630364), test loss: 3.15905537009\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (17.2430801392,21.5477921986), test loss: 27.6733068228\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.87611699104,2.53387842019), test loss: 2.6515512988\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (28.9394931793,21.5086203391), test loss: 31.7877709389\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.58923435211,2.52890272518), test loss: 3.04458262324\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (6.91875267029,21.4700134093), test loss: 26.6485501289\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.745563983917,2.52391824848), test loss: 2.64146592468\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (33.4714012146,21.4333277706), test loss: 31.2659895182\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.32896542549,2.51879974057), test loss: 2.8609617278\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (51.8325233459,21.3959578001), test loss: 30.254244256\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.80635595322,2.51384562823), test loss: 3.12151343524\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (7.52770090103,21.3584757653), test loss: 33.1120925903\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.867518782616,2.50895353504), test loss: 3.05127702355\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (9.66822624207,21.3225231747), test loss: 28.5184080362\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.25864005089,2.5041929522), test loss: 3.15488592684\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (4.66812610626,21.2856751321), test loss: 32.7455513716\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.16747713089,2.49956253586), test loss: 2.95879244208\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (14.9056758881,21.2495429872), test loss: 29.5190692663\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.2646021843,2.49490774243), test loss: 3.16620031893\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (19.757068634,21.2151107938), test loss: 29.7989442348\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.4298094511,2.49010785569), test loss: 2.54566396177\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (10.1507148743,21.1797769043), test loss: 32.3290415764\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.636063098907,2.48546306378), test loss: 3.26719715595\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (3.49450588226,21.1443601466), test loss: 30.2981552601\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.57322001457,2.48089700772), test loss: 2.3545569092\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (12.59694767,21.1106180678), test loss: 32.3714051723\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.81741666794,2.47644047088), test loss: 3.14086273909\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (8.4362487793,21.0758002423), test loss: 28.7690594196\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.985193192959,2.47209368133), test loss: 2.53678960204\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (21.1660270691,21.0418872731), test loss: 32.9578261852\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.159948349,2.46773834299), test loss: 3.13936075419\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (22.6061859131,21.0093908136), test loss: 27.3314713955\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.10461735725,2.4632512958), test loss: 2.66497705877\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (28.1156864166,20.9761594814), test loss: 33.2487556458\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (3.42199611664,2.45891400958), test loss: 3.01239281893\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (10.6446237564,20.9424592909), test loss: 26.4420641422\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.697070717812,2.45459629204), test loss: 2.5883966431\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.16009902954,20.9104825495), test loss: 31.1211791873\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.36467123032,2.45041653238), test loss: 2.85238296241\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (9.31018161774,20.877561739), test loss: 31.1222348213\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.378289699554,2.44632323016), test loss: 3.18690462708\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (7.86007785797,20.8454429857), test loss: 31.4445079327\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.85020917654,2.44223639396), test loss: 3.04354370832\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (5.29622745514,20.8145833667), test loss: 28.3950880051\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.980965018272,2.43802145289), test loss: 3.18157355189\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (11.6597127914,20.7831811551), test loss: 33.4131043434\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.00314569473,2.43395398177), test loss: 2.78655808568\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (17.3228988647,20.751243302), test loss: 30.6936779737\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.812060475349,2.42987599421), test loss: 3.13284506798\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (17.1142082214,20.7209342175), test loss: 30.0076746941\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (3.2398557663,2.42597046251), test loss: 2.53981601447\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (12.135515213,20.6896342433), test loss: 31.0963643074\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.41166675091,2.422101479), test loss: 3.26329458356\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (10.3468971252,20.6591457188), test loss: 29.5138219833\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.647960186,2.41826943304), test loss: 2.41203962266\n",
      "\n",
      "MC # 3, Hype # hyp4, Fold # 9\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold9/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (379.430847168,inf), test loss: 175.488587952\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (301.911621094,inf), test loss: 385.678216553\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (71.4228973389,77.5400029306), test loss: 47.169208622\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.20468044281,75.8406507621), test loss: 3.22677813768\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (94.5806121826,61.8523216004), test loss: 40.7291023254\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.37837743759,39.4288487073), test loss: 3.35001730323\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (37.1617546082,56.6521429237), test loss: 46.798753643\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (7.20911502838,27.3052868334), test loss: 3.29685781002\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (65.2837524414,53.9461238348), test loss: 43.9197685242\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.27184391022,21.2430645919), test loss: 3.66198288798\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (13.6435823441,52.368707394), test loss: 45.2492252827\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.57101726532,17.6137595676), test loss: 2.61258141994\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (17.3236198425,51.2928274179), test loss: 46.3127826691\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (3.38289093971,15.1966921239), test loss: 3.80401232243\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (14.6214761734,50.5117332122), test loss: 42.4648301125\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.48327064514,13.4765243773), test loss: 2.68665653765\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (42.8143234253,49.9154513394), test loss: 47.5583925247\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.84009552002,12.1852562301), test loss: 3.56763632894\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (102.129341125,49.4056414729), test loss: 40.8438416481\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.04903078079,11.1839391947), test loss: 2.97688673139\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (146.877990723,48.9778675998), test loss: 47.1510258675\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.49779844284,10.3831554633), test loss: 3.30485567451\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (28.9537315369,48.6139732443), test loss: 36.2287156582\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (4.55398750305,9.72970503063), test loss: 2.79308575988\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (10.9168338776,48.2953935049), test loss: 45.9634810448\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (4.19529819489,9.18413562503), test loss: 3.44143935442\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (28.6870956421,48.0537769833), test loss: 40.6509518623\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (4.14024829865,8.72541860029), test loss: 3.51405891776\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (53.9752960205,47.8390266005), test loss: 42.0284683704\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (6.45286512375,8.33375765237), test loss: 2.87086703181\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (35.6948280334,47.6339818442), test loss: 47.2073438168\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.91620588303,7.99607282637), test loss: 3.77791469097\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (39.0050811768,47.459821938), test loss: 45.3214029312\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.14809608459,7.7000614932), test loss: 2.80504533052\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (53.7253913879,47.2839810499), test loss: 45.6235497475\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.33780837059,7.44038522378), test loss: 3.69571002126\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (33.8731231689,47.111744313), test loss: 40.1405594349\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.95596218109,7.20948890922), test loss: 2.92820712328\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (35.7926940918,46.9417753526), test loss: 47.4249385834\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.7470035553,7.00389495794), test loss: 3.59298767447\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (41.2291870117,46.7858518572), test loss: 39.1856978893\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.28197860718,6.81776858385), test loss: 2.98507594168\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (12.6796875,46.6565105266), test loss: 45.1508067131\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.31887769699,6.65135847669), test loss: 3.41239616871\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (35.2177619934,46.5398923684), test loss: 38.3922106743\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.42411231995,6.50152170593), test loss: 3.32598760128\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (34.9878196716,46.417999188), test loss: 45.3315990686\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.88608360291,6.36403737397), test loss: 3.26572258472\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (33.0843200684,46.305396234), test loss: 42.0699777603\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (8.5689163208,6.23888179426), test loss: 3.70151457787\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (67.6425628662,46.1910145993), test loss: 43.7424969196\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.48031878471,6.12361987643), test loss: 2.90185649991\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (61.0161819458,46.0716095757), test loss: 44.8202125549\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (4.35295963287,6.0180149881), test loss: 3.78996111751\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (32.8358306885,45.9428058321), test loss: 39.8742895603\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (6.44558143616,5.91982136418), test loss: 2.95489292741\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (68.260635376,45.8272810355), test loss: 45.2573171616\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.86973309517,5.82815889788), test loss: 3.67655786276\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (14.3697376251,45.7190459582), test loss: 38.1681257248\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (4.2687997818,5.74357354467), test loss: 2.966729182\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (34.2283630371,45.6218300606), test loss: 44.9299465656\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.19797325134,5.66543772469), test loss: 3.43337813616\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (26.7887611389,45.5191668631), test loss: 33.7210226059\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.1413629055,5.59243422594), test loss: 2.89950385988\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (51.9366760254,45.416884906), test loss: 44.5733273983\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.09360647202,5.52369288816), test loss: 3.37583097219\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (28.2628440857,45.31031447), test loss: 38.9137046814\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.64610433578,5.45839055327), test loss: 3.66543716192\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (18.964345932,45.1999898935), test loss: 40.5626319408\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.80375778675,5.39737763781), test loss: 2.8967980504\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (33.3926429749,45.0816618607), test loss: 44.2693592548\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.38480234146,5.33903238523), test loss: 3.81617183685\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (35.1568412781,44.9740924408), test loss: 41.5157653332\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.16105127335,5.28352975157), test loss: 2.8407114327\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (49.2783660889,44.8508014217), test loss: 42.0609899521\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (5.64082431793,5.23063648558), test loss: 3.76517681479\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (100.519668579,44.7414868658), test loss: 37.4199667454\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (6.6885471344,5.18094881112), test loss: 2.81574831605\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.0386075974,44.6179107141), test loss: 45.4702789783\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.17760396004,5.13290602631), test loss: 3.5378341645\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (51.0769920349,44.4973294697), test loss: 33.6254597664\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.43233394623,5.08725787504), test loss: 2.86971621513\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (38.5238189697,44.3681290249), test loss: 41.2592466354\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.66697883606,5.04238518854), test loss: 3.22554082274\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (22.496421814,44.2316882396), test loss: 35.5310287237\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.23916339874,5.00014783794), test loss: 3.43902780414\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (33.8772201538,44.084496474), test loss: 42.3538487434\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.81533432007,4.95871720587), test loss: 3.18205482364\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (28.5561904907,43.9493107771), test loss: 39.8353413105\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.75521206856,4.91891223863), test loss: 3.50418113768\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (81.6187744141,43.7976288829), test loss: 39.1699436665\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.59725379944,4.88013568098), test loss: 2.74268196225\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (15.1622190475,43.6523695141), test loss: 41.1335174084\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.938999712467,4.84286003343), test loss: 3.67371996641\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (19.9945735931,43.4954383011), test loss: 35.3271310806\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.439220666885,4.80613990025), test loss: 2.72540620565\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (64.6376571655,43.3403263096), test loss: 41.3109051704\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.99587535858,4.77055534707), test loss: 3.44296004772\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (58.9487190247,43.1742592709), test loss: 33.4054536819\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (5.6285610199,4.73484485409), test loss: 2.87566860914\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (28.45262146,42.998319251), test loss: 39.3539704323\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.5153465271,4.70025283746), test loss: 3.16799108982\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (28.0580673218,42.8139011276), test loss: 33.7906458855\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.58135318756,4.66616350708), test loss: 3.09632034302\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (22.6403770447,42.6326832899), test loss: 37.1666303635\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (7.93652629852,4.63274265594), test loss: 3.09631350636\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (44.1204147339,42.440228445), test loss: 33.4771007061\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.76759576797,4.59922980602), test loss: 3.44081857204\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (15.1881551743,42.24562932), test loss: 33.7884707451\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.91177582741,4.56664950744), test loss: 2.58811414838\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (26.5052127838,42.0435437409), test loss: 36.9277666092\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.25461053848,4.53422370861), test loss: 3.59550842047\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (15.6663751602,41.8357363739), test loss: 33.5029232025\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.20501589775,4.50270581242), test loss: 2.48224608302\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (16.7330970764,41.6227739085), test loss: 36.745816803\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.47185301781,4.4711565736), test loss: 3.31327927113\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (38.4222602844,41.3993964298), test loss: 29.6345925331\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.44935798645,4.43995758543), test loss: 2.61459422708\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.9803409576,41.1677203702), test loss: 36.6304675579\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.43075847626,4.40878448954), test loss: 3.03863995671\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (17.4591464996,40.9362651314), test loss: 25.9015966892\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.10064077377,4.37805912614), test loss: 2.53800649643\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (9.24465656281,40.6987462626), test loss: 34.5399670124\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (4.04337596893,4.34715832161), test loss: 2.98728089929\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (21.7465171814,40.4599231017), test loss: 28.7167780399\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.23491835594,4.31686499132), test loss: 3.18516139984\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (33.0479888916,40.2175270759), test loss: 34.1473302364\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (5.29428052902,4.28700705652), test loss: 2.78899109364\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (9.98075008392,39.9718966143), test loss: 34.0157248974\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.33145284653,4.25758452558), test loss: 3.46218398809\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (13.2897968292,39.7264951358), test loss: 30.5015740633\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.85277581215,4.22844393052), test loss: 2.40434241593\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (19.503742218,39.4795754596), test loss: 33.466271925\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.589182734489,4.1998862574), test loss: 3.32029365301\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (38.5806274414,39.2282844753), test loss: 28.3406970501\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.39954686165,4.17152102283), test loss: 2.44368812442\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (22.5041275024,38.9778875171), test loss: 33.7306221008\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.0096758604,4.14359654814), test loss: 3.1945189029\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (7.70563602448,38.7316447812), test loss: 26.3204161644\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (0.830924868584,4.11576325499), test loss: 2.64003040493\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (8.95685863495,38.4885047461), test loss: 32.8790640116\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.40286457539,4.08884977575), test loss: 3.01823391318\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.8529310226,38.2475830038), test loss: 28.2188137531\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.7679605484,4.06249248116), test loss: 3.17536886334\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (35.1175384521,38.0094627463), test loss: 33.2304067731\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.92156791687,4.03649242402), test loss: 3.04649206996\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (10.7593517303,37.7751266005), test loss: 29.0785638332\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (3.35115671158,4.01124997861), test loss: 3.45755739212\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (24.5037307739,37.5460765565), test loss: 29.3305969715\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.59441256523,3.98658516687), test loss: 2.33096854687\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (31.3409061432,37.3164990792), test loss: 31.7528890848\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (5.14640235901,3.96239045981), test loss: 3.57068404555\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (2.98016285896,37.0896266121), test loss: 27.5290556431\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.92149245739,3.93839133678), test loss: 2.24281978309\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.8165950775,36.8714945941), test loss: 34.3267147303\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.13049602509,3.91499621441), test loss: 3.36869937778\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.7387075424,36.6565213001), test loss: 26.8981448174\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.67093873024,3.89194309141), test loss: 2.59462177753\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (52.9374389648,36.4468268935), test loss: 33.3960699558\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.65076303482,3.8695942383), test loss: 3.20142020583\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (17.12890625,36.2383118804), test loss: 23.9928552151\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.77968811989,3.84751154385), test loss: 2.54145866632\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (30.7029819489,36.0359219706), test loss: 33.4371449947\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.93668746948,3.82613543278), test loss: 3.17238833308\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (11.6573553085,35.8381902986), test loss: 28.3393732786\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.16063308716,3.8052914282), test loss: 3.42504507154\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (23.3676013947,35.6416185808), test loss: 27.7200273514\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.53952407837,3.78482622883), test loss: 2.61783130467\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (29.511724472,35.4482331882), test loss: 32.298571825\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.78308260441,3.76453001822), test loss: 3.55984494686\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (28.4839305878,35.2624349627), test loss: 29.946101594\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (2.19059848785,3.74475350796), test loss: 2.32338442504\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (12.0348510742,35.0784135336), test loss: 32.0632401943\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.44711661339,3.72527655947), test loss: 3.41091135144\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (37.3934555054,34.8999305483), test loss: 28.2036193371\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (3.0763001442,3.70638869062), test loss: 2.53308033049\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (7.0883769989,34.7216397535), test loss: 33.972262764\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.20048177242,3.68765686595), test loss: 3.29440143406\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (10.704252243,34.5498723103), test loss: 25.6496216774\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.43696331978,3.66962878688), test loss: 2.66037011445\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (21.3855552673,34.3818579331), test loss: 33.5789038658\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.934186279774,3.65192546612), test loss: 3.16009783149\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (11.3414707184,34.2144608652), test loss: 28.5704183102\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.84233164787,3.63461382106), test loss: 3.21848320961\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (15.194893837,34.0495896671), test loss: 34.2500068665\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.50846576691,3.61738285781), test loss: 3.09097294658\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (17.6120643616,33.8914408255), test loss: 29.6426437378\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.50382781029,3.60057833202), test loss: 3.5022964865\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (10.2872676849,33.7348573287), test loss: 30.1854026794\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.706142425537,3.58400701077), test loss: 2.38125138283\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (22.565284729,33.5819997522), test loss: 33.3390569687\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.36339569092,3.56782693628), test loss: 3.48112622499\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (17.7417049408,33.4290233056), test loss: 27.3915855169\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.57984912395,3.55180213268), test loss: 2.36908841431\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (31.7400817871,33.2822055403), test loss: 34.7052071333\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.43162059784,3.5364147069), test loss: 3.43905246258\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (11.8529205322,33.1379008339), test loss: 26.8893067837\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.532083213329,3.52118818496), test loss: 2.68295811713\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (16.6678695679,32.9943643416), test loss: 33.3523021221\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.970549345016,3.50635051637), test loss: 3.19430710077\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (13.6651582718,32.8532913044), test loss: 24.729929018\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.40265339613,3.49163741255), test loss: 2.8747879982\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.40378761292,32.7160441399), test loss: 34.5719028115\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.29663133621,3.47711875427), test loss: 3.14390583038\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (17.5134048462,32.5818548), test loss: 28.5172055721\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.16493940353,3.46281976443), test loss: 3.38641185164\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (16.3847732544,32.4493155943), test loss: 28.9102963924\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.92805767059,3.44887345282), test loss: 2.49468441755\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (54.5056991577,32.3182740296), test loss: 31.3022531509\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.30586481094,3.43499644647), test loss: 3.5782219559\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (10.9552574158,32.1895122753), test loss: 30.2186587811\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.688885033131,3.42158803092), test loss: 2.37781459987\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (14.1767673492,32.0643835278), test loss: 32.756030345\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.97305989265,3.40848179313), test loss: 3.33460661471\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (15.6852865219,31.940383854), test loss: 28.2318957329\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.94152522087,3.39561659768), test loss: 2.60512533784\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (4.11066913605,31.8174647854), test loss: 34.9136828184\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.667739033699,3.38279154601), test loss: 3.23249807805\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (16.9825572968,31.6974162739), test loss: 24.9043579578\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (3.87126874924,3.37020097001), test loss: 2.60822878182\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (12.6838302612,31.5801874115), test loss: 33.4639315844\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.44167613983,3.35761885634), test loss: 3.08335069418\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (16.8878898621,31.4642592752), test loss: 28.7321861744\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.05546307564,3.34538806287), test loss: 3.34683544487\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (24.3324317932,31.3496349836), test loss: 34.9479618073\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.64041006565,3.33331179604), test loss: 3.07591970563\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (4.01261520386,31.2366923141), test loss: 31.3206643105\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.34912323952,3.32154329778), test loss: 3.53552939892\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (8.88317775726,31.1267768301), test loss: 29.556717968\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.727695226669,3.30999503023), test loss: 2.37629748136\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (28.9310264587,31.0195250762), test loss: 34.1983266354\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.358824789524,3.29875291253), test loss: 3.47645761371\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (16.2252540588,30.9105346771), test loss: 28.2747275352\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (3.4570941925,3.28751054758), test loss: 2.49700206816\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (17.404794693,30.8037747797), test loss: 33.6971335649\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.86287581921,3.27638890077), test loss: 3.29562811553\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (10.2696037292,30.7002742731), test loss: 26.3054338455\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.86450684071,3.26529637141), test loss: 2.61345238686\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (7.89392662048,30.5979715469), test loss: 33.1223792553\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.07326865196,3.25453535585), test loss: 3.11207226217\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (17.5026092529,30.4967700068), test loss: 29.7696670055\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.85988330841,3.24390973078), test loss: 3.15998072624\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (26.7398815155,30.3969547097), test loss: 33.6041630507\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.39824676514,3.23338482633), test loss: 3.09540891051\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (18.1942863464,30.2989927686), test loss: 28.8953296661\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.61384296417,3.22316970535), test loss: 3.50300212651\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (18.220123291,30.2037561766), test loss: 29.8611238003\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.31516885757,3.21318208078), test loss: 2.57644680142\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (23.2777576447,30.1071835289), test loss: 31.7968431473\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (4.16698122025,3.20326239822), test loss: 3.55714583099\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (5.41557073593,30.0117004661), test loss: 29.6960294127\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.54961335659,3.1933015095), test loss: 2.34404623806\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (18.3372650146,29.9199969015), test loss: 33.8724405527\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (4.97503614426,3.18356554026), test loss: 3.30775856376\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (15.0631465912,29.8285893148), test loss: 26.5013412952\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.19115972519,3.17388728695), test loss: 2.53008703142\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (21.9120178223,29.7384515933), test loss: 34.0300573349\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.3708717823,3.16442769352), test loss: 3.21064638197\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (22.2259368896,29.6489088521), test loss: 24.1201588631\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.55974602699,3.15501187943), test loss: 2.51866359711\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (29.3732471466,29.5614600459), test loss: 33.5184362411\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.734672129154,3.14586486581), test loss: 3.02981599569\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (13.8346662521,29.4757836263), test loss: 28.0627254248\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.21938800812,3.13693038528), test loss: 3.29633117914\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (28.2650775909,29.3892249377), test loss: 30.8633156776\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.45825636387,3.12804015431), test loss: 2.71427338123\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (24.7694225311,29.3036946652), test loss: 32.1456580639\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.42814397812,3.11914424705), test loss: 3.50133220255\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (47.9716949463,29.2213794576), test loss: 29.6804083824\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.72234094143,3.11042428796), test loss: 2.36363002956\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (15.4172840118,29.1389855556), test loss: 32.9092957973\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.79969596863,3.10172804507), test loss: 3.33370475769\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (24.2701931,29.0583533342), test loss: 28.8433012962\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.79540395737,3.09325578654), test loss: 2.45328546762\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (5.10239315033,28.9768035763), test loss: 33.2984059572\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.47911080718,3.08474681124), test loss: 3.19769666791\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (9.11668777466,28.898097016), test loss: 25.8048426628\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.53941226006,3.07655534974), test loss: 2.67167388052\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (25.8598175049,28.8206996459), test loss: 33.3060557842\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.695725858212,3.0684567784), test loss: 3.03356164098\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (32.2326469421,28.7425926492), test loss: 28.2335848331\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.975236296654,3.06045566786), test loss: 3.2035802573\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (9.00988101959,28.6652255446), test loss: 33.8966995716\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.11177372932,3.05243621494), test loss: 3.02523604631\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (22.7286586761,28.5906763505), test loss: 29.4048051596\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.82116770744,3.04454552815), test loss: 3.46744550169\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (4.47612094879,28.5160933671), test loss: 30.3340248108\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.85347443819,3.0367325386), test loss: 2.30630597174\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (20.9217739105,28.4428235775), test loss: 32.1789394855\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.30713021755,3.02901952406), test loss: 3.48120839\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (19.5028781891,28.3686030904), test loss: 27.7377490997\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.0940015316,3.02129898794), test loss: 2.21229891777\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (18.4845314026,28.2971978701), test loss: 34.4022337437\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.65922653675,3.01387671854), test loss: 3.29892663956\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (10.487411499,28.2266990822), test loss: 26.6328945637\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.40604147315,3.00650157967), test loss: 2.59614474475\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (13.2402019501,28.155575952), test loss: 33.7671257019\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.551919579506,2.99926522401), test loss: 3.11979304701\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (9.26347541809,28.0855266337), test loss: 23.7991268158\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.432301014662,2.9920459674), test loss: 2.53981123865\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (11.1196966171,28.0169984182), test loss: 33.6623019695\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.57024240494,2.98482788309), test loss: 3.07570705414\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (16.9646759033,27.9496889767), test loss: 28.04947052\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.96811389923,2.9777094249), test loss: 3.31811653674\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (16.4409160614,27.8823216387), test loss: 28.3985728741\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.53595244884,2.97068200566), test loss: 2.53863442838\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (45.2343826294,27.8152265044), test loss: 31.4660357714\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.45610404015,2.96366625855), test loss: 3.47572000325\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (13.9973964691,27.7491079921), test loss: 30.0363864899\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.57242822647,2.95687275812), test loss: 2.24521360546\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (10.9534225464,27.6845398845), test loss: 32.4524715185\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (4.20523738861,2.95017879905), test loss: 3.26721291542\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (14.0397768021,27.6200810017), test loss: 28.0759008884\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.69475865364,2.94358422973), test loss: 2.49882228374\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (6.42477703094,27.5557715373), test loss: 33.725620842\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.30993449688,2.93697020849), test loss: 3.17856179178\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (13.5929355621,27.492459535), test loss: 25.3364887714\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.96675395966,2.93041505805), test loss: 2.57661727965\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (9.59220504761,27.4305253978), test loss: 33.3014777184\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.29966163635,2.92381898001), test loss: 2.99548915327\n",
      "\n",
      "MC # 3, Hype # hyp4, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_3/fold10/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (392.05947876,inf), test loss: 233.536161804\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (295.86328125,inf), test loss: 359.316000366\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (60.5556869507,136.104724575), test loss: 50.4130159855\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.525436520576,77.5205155692), test loss: 3.62504628301\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (20.3384494781,92.2141163149), test loss: 43.0021298409\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.20370674133,40.3997689477), test loss: 3.39067242742\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (203.618423462,77.3298010314), test loss: 42.1850757003\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (8.40901851654,28.0176925258), test loss: 3.62846327424\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (32.5448493958,69.6820458131), test loss: 43.8244198799\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.32387661934,21.821846751), test loss: 3.62107833922\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (23.4188957214,65.1556504629), test loss: 40.0713255405\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.6991686821,18.1142798086), test loss: 3.07318972051\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (17.9862499237,62.046243686), test loss: 47.1199706078\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.582162678242,15.6408342314), test loss: 3.83965655565\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (42.4896850586,59.8455404921), test loss: 42.8216241837\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.58537042141,13.8733000177), test loss: 2.88354783952\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (100.571609497,58.1174164555), test loss: 46.1941449165\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.39675807953,12.5490091101), test loss: 3.8093881309\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (42.2242088318,56.7140748669), test loss: 40.1349491596\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.90123486519,11.5178959868), test loss: 3.11709122658\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (13.4834003448,55.6533686835), test loss: 46.2642413139\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.26149368286,10.6934394095), test loss: 3.7383654356\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (50.5203170776,54.7192405852), test loss: 38.7482121944\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (3.31549692154,10.0176833369), test loss: 2.86736211777\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (90.1312637329,53.9637816615), test loss: 43.1571692944\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (8.05277633667,9.45816453608), test loss: 3.6273050189\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (63.0413360596,53.283958432), test loss: 35.1108258724\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.6776766777,8.98364137379), test loss: 2.9666585505\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (25.4672870636,52.6959319216), test loss: 41.1311706543\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.41706609726,8.57586595434), test loss: 3.57437989712\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (23.8581371307,52.1440810709), test loss: 41.8139674664\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (2.01007437706,8.22374118804), test loss: 3.59016950428\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (25.121131897,51.5842321786), test loss: 37.3492004395\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.38739347458,7.91447251717), test loss: 3.04098933935\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (37.7103309631,51.0831697456), test loss: 46.6265069962\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.01600074768,7.64158261466), test loss: 3.76506942809\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (31.3493728638,50.6156206372), test loss: 39.687308836\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.50197887421,7.39761998398), test loss: 2.82447709739\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (37.6419334412,50.1918554977), test loss: 43.4693598747\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.68097925186,7.18027593341), test loss: 3.83786348104\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (21.2757205963,49.7687981971), test loss: 36.9315283775\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.51051676273,6.9844537729), test loss: 2.94879315794\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (51.35181427,49.3714875971), test loss: 43.7520913601\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (3.99289369583,6.80720266996), test loss: 3.66108137965\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (17.4846763611,48.9856165522), test loss: 35.9856657982\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (3.40520215034,6.64458818122), test loss: 3.07594662309\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (54.3390350342,48.5962467579), test loss: 40.2217542648\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (6.86216592789,6.49586743709), test loss: 3.30159701705\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (30.0300540924,48.211773638), test loss: 32.0324215412\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.77463841438,6.35851113679), test loss: 2.71129853427\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (7.18178796768,47.8411712861), test loss: 38.7201738358\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.86654925346,6.23081723829), test loss: 3.53503249288\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (20.7182483673,47.4820135017), test loss: 37.1105091095\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.40607357025,6.11285970738), test loss: 3.38092266917\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (71.1508483887,47.1206311601), test loss: 38.1421012521\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.7127263546,6.00326254074), test loss: 3.1259980917\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (62.3724517822,46.7551061055), test loss: 40.7151432514\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.22638654709,5.90133856224), test loss: 3.54141036272\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (28.5691051483,46.3875637293), test loss: 34.9736124754\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.59885859489,5.80427137626), test loss: 2.85074903369\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (28.3398628235,46.0072317334), test loss: 39.5347728252\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.7339515686,5.71347512858), test loss: 3.68485186398\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (38.1294021606,45.6225524452), test loss: 30.0971580505\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (0.904928565025,5.62701206587), test loss: 2.69618794918\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (42.0869522095,45.2396694972), test loss: 39.262480545\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (6.83294439316,5.54453850016), test loss: 3.59296407104\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (13.7406482697,44.8563536324), test loss: 30.5509422541\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.73083376884,5.46628740434), test loss: 2.7595836252\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (58.3020095825,44.4674030117), test loss: 35.6806678772\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (5.13667488098,5.39186660218), test loss: 3.34255952835\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (13.512799263,44.0678501727), test loss: 26.5281774998\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.0066037178,5.32092928435), test loss: 2.54224658012\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (36.9142799377,43.666318145), test loss: 33.0600754976\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (4.37623786926,5.2520829314), test loss: 3.26600213647\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (18.4731731415,43.2503974283), test loss: 31.7239611149\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.15503644943,5.18579274389), test loss: 3.32550314069\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (21.2513237,42.8310114021), test loss: 33.2535365582\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.46401047707,5.12198696664), test loss: 3.23741748035\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (29.2438735962,42.4207502219), test loss: 32.2775880814\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.99496543407,5.06002852403), test loss: 3.30717930347\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (16.9907073975,42.0014230813), test loss: 29.9291500092\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.33711528778,4.99975574594), test loss: 2.5520934999\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (23.4640598297,41.5881952526), test loss: 33.5323535085\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.06292176247,4.94216968791), test loss: 3.57482361794\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (28.6185188293,41.1724678129), test loss: 26.7545905113\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.01383566856,4.88601564905), test loss: 2.4373377651\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (14.9807090759,40.7636202515), test loss: 33.957587719\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.859390795231,4.83147035368), test loss: 3.40475962162\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (78.8293838501,40.3568579503), test loss: 26.657829237\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (5.64591884613,4.77883618658), test loss: 2.70662463903\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (60.676071167,39.9504514392), test loss: 33.407175684\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.77074766159,4.72728876421), test loss: 3.25162983835\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (11.6279821396,39.559872085), test loss: 24.5493870258\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.8626408577,4.67722316384), test loss: 2.46326749325\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (9.05824279785,39.1754066498), test loss: 31.5541896343\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (4.65078544617,4.62865734295), test loss: 3.23844607472\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (37.0661506653,38.8017844469), test loss: 29.4058138609\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.32012319565,4.58192941739), test loss: 3.13268272579\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (18.315738678,38.4335519965), test loss: 31.8390546322\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.95470023155,4.53646792547), test loss: 3.22691457272\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (8.42506217957,38.0781103469), test loss: 31.0000453472\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (6.16994047165,4.49261703238), test loss: 3.3805124402\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (27.1485919952,37.7346205903), test loss: 28.7427865267\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.38529038429,4.45001864261), test loss: 2.52682312131\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (28.3603630066,37.3936952369), test loss: 32.035542202\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.72027683258,4.40860226185), test loss: 3.56895972192\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.4706850052,37.0649986071), test loss: 29.219251132\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.54264354706,4.36838049686), test loss: 2.51030799896\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (17.1558227539,36.7492419613), test loss: 32.4670799732\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.90851783752,4.32906639185), test loss: 3.38187257946\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (17.3491020203,36.4412778124), test loss: 28.0764615536\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.609608531,4.29128033476), test loss: 2.67002028823\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (19.8579044342,36.1394221154), test loss: 34.4632523298\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.728343963623,4.25455341712), test loss: 3.33035809696\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (11.2378444672,35.8507282441), test loss: 25.8588400841\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.87218362093,4.21936088396), test loss: 2.59149328172\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.5848369598,35.5707324543), test loss: 32.4165331364\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.3996989727,4.18502676698), test loss: 3.25711797476\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.497625351,35.2944763492), test loss: 30.1092473507\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.41472256184,4.15171937196), test loss: 3.16715508103\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (15.6029338837,35.0277343944), test loss: 32.8004808903\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.28074407578,4.11916229652), test loss: 3.31752027571\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (15.602563858,34.7719841741), test loss: 31.7851544857\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (3.19400691986,4.08738568162), test loss: 3.43987978995\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (12.1073551178,34.522732023), test loss: 29.2759533882\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.664716959,4.05665053556), test loss: 2.73717227876\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (27.7097759247,34.2787394357), test loss: 33.1951648474\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.8339548111,4.02678959774), test loss: 3.57190431952\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (42.4573516846,34.0414562433), test loss: 29.8770666599\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.26854228973,3.99807168227), test loss: 2.44937954843\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (14.8571376801,33.8118318491), test loss: 32.5996380091\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.54892706871,3.96992071403), test loss: 3.33903580308\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (30.4146671295,33.5849010526), test loss: 29.9464031458\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.13909435272,3.94262856407), test loss: 2.71098270565\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (9.56583404541,33.3633313536), test loss: 33.3600082874\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.10531592369,3.91581580648), test loss: 3.26312249154\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (18.5787410736,33.1514369803), test loss: 27.0343446255\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.84421610832,3.88967775192), test loss: 2.67368626297\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (11.3509731293,32.9439914483), test loss: 33.5581986427\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.91357207298,3.86406978944), test loss: 3.25296323299\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (12.2311058044,32.740556636), test loss: 30.2149636745\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (6.24161243439,3.83940623171), test loss: 3.17192143202\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (11.5818033218,32.5422666232), test loss: 33.4737721443\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (0.869717955589,3.81527247611), test loss: 3.36727366745\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (6.36964130402,32.3505743519), test loss: 30.3816349983\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.51998591423,3.79177254656), test loss: 3.43392059654\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (43.4375419617,32.1619797432), test loss: 28.8187383056\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (4.12401199341,3.76894578654), test loss: 2.73764010072\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (19.8117923737,31.9763665075), test loss: 33.5752576351\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.54005670547,3.74651579478), test loss: 3.54867140651\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (14.1909351349,31.800103882), test loss: 29.0035605431\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.974090218544,3.72449727601), test loss: 2.40904785842\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (15.1567544937,31.6256534123), test loss: 32.7771245718\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.85931885242,3.70296082391), test loss: 3.36029768884\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (37.9221801758,31.4555956593), test loss: 29.5935986519\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.41306376457,3.68221442557), test loss: 2.67459073812\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (26.0174293518,31.2879647164), test loss: 34.8727991581\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.50971508026,3.66175834448), test loss: 3.3294393152\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.12900257111,31.1262031071), test loss: 27.2512394667\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.04572105408,3.64195175967), test loss: 2.77879101932\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (49.7836685181,30.968697326), test loss: 32.2101639271\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (3.93481826782,3.62265598645), test loss: 3.19774442613\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (45.486869812,30.8108224709), test loss: 30.9164349079\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.97628688812,3.60360358918), test loss: 3.22743380666\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (7.29506444931,30.6594195789), test loss: 33.2199741125\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.23892378807,3.58477943664), test loss: 3.36007683873\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (5.4315237999,30.5116327762), test loss: 30.4249655008\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.44400787354,3.56643907702), test loss: 3.22864364088\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (19.06965065,30.3667060041), test loss: 28.4188908815\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.83809614182,3.54865142077), test loss: 2.78058305681\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (35.9004364014,30.2226079753), test loss: 34.2493571758\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.52264451981,3.53107700987), test loss: 3.47499154806\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (18.8799438477,30.0835424843), test loss: 29.6556882381\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.45339488983,3.51411360764), test loss: 2.36817560494\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (20.1274795532,29.9488460602), test loss: 33.0537685871\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.45688056946,3.49747704091), test loss: 3.31426439881\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (11.7113237381,29.8126989506), test loss: 28.3121303558\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.95433664322,3.4810795541), test loss: 2.61215944886\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (10.4449748993,29.6812558039), test loss: 34.7312534571\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (4.468542099,3.46494648582), test loss: 3.40388710201\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.05969429016,29.554243906), test loss: 28.1254105091\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.89592218399,3.44894090029), test loss: 2.71634007394\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (23.9838809967,29.4288704652), test loss: 32.1536745787\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.43545913696,3.4335093303), test loss: 3.16854040623\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (11.3512706757,29.3031124027), test loss: 25.0822822571\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.67728328705,3.41819933632), test loss: 2.53107547909\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (4.34534168243,29.1830370243), test loss: 34.3268801212\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.78275203705,3.40349757263), test loss: 3.32817060202\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (14.1526432037,29.0652459589), test loss: 29.6508570194\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.68673861027,3.38897937651), test loss: 3.25045029968\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (13.5515899658,28.9468813084), test loss: 28.8896202445\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.80106729269,3.37471723189), test loss: 2.76834593266\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (16.3771438599,28.8321015851), test loss: 33.190073967\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.6727232933,3.36061558405), test loss: 3.45934601128\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (7.96224021912,28.7206806501), test loss: 30.7988027096\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.00120425224,3.34664778434), test loss: 2.55554501861\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (20.9985370636,28.6111808497), test loss: 34.038588953\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.38841199875,3.33306999276), test loss: 3.39248282015\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (10.5835313797,28.5019166554), test loss: 27.962505722\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.46086871624,3.3196816815), test loss: 2.55094444901\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (12.6834506989,28.3949197493), test loss: 34.9909463763\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.822586536407,3.30670399102), test loss: 3.43052854538\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (6.09616374969,28.2908042963), test loss: 28.0960186005\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.539710283279,3.29389134279), test loss: 2.80540089011\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (25.1413841248,28.1866174884), test loss: 32.8740714788\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.887258887291,3.28137565782), test loss: 3.14462444186\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (21.8214969635,28.0844737571), test loss: 25.0382663965\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.2219465971,3.26893248155), test loss: 2.48058101088\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (12.5978078842,27.985809256), test loss: 32.9542272568\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.04618775845,3.25665592886), test loss: 3.23425084949\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (8.3846206665,27.8883081513), test loss: 30.8023765087\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.33317947388,3.24453309304), test loss: 3.22520962358\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (14.0296936035,27.7916268816), test loss: 33.783447957\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (4.89415550232,3.23278002549), test loss: 3.26198779643\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (7.66586065292,27.6962407018), test loss: 31.8743189812\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.934772610664,3.22113945515), test loss: 3.28816569149\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (7.04883432388,27.6036622811), test loss: 31.145627737\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.5443520546,3.20977656883), test loss: 2.61726299226\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (46.0793991089,27.511505827), test loss: 33.3164838791\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (3.47905826569,3.19863523262), test loss: 3.37825346589\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (21.7742977142,27.4198547582), test loss: 26.7846081257\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.49574184418,3.18760010938), test loss: 2.34447202086\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (17.4456253052,27.3322299521), test loss: 34.2609977841\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.966642260551,3.17659197687), test loss: 3.26328379214\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (6.28080606461,27.2444940469), test loss: 27.8373355389\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.67437744141,3.16581430598), test loss: 2.71152828634\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (47.7758979797,27.1585691695), test loss: 33.5666621208\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.95942568779,3.15533727936), test loss: 3.20287334323\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (14.0080947876,27.0721361243), test loss: 25.4160925865\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.12396717072,3.14487242828), test loss: 2.47279822826\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (7.57110023499,26.9885686779), test loss: 32.755070591\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.728653311729,3.13470974001), test loss: 3.13583702445\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (35.0124282837,26.9069008613), test loss: 30.5848533154\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.61490130424,3.12475520118), test loss: 3.16018567085\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (14.0519542694,26.823459564), test loss: 33.4879775524\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (3.09248161316,3.11485661433), test loss: 3.267776151\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (7.11065006256,26.7434305344), test loss: 31.0616838694\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.10013628006,3.10494663625), test loss: 3.3096565783\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (9.98683547974,26.6647294813), test loss: 31.9767763495\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.95199012756,3.09527006232), test loss: 2.55971756577\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (19.3753356934,26.5868423481), test loss: 33.6991938353\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.60140132904,3.08578803991), test loss: 3.38081707358\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (21.0899753571,26.5084071466), test loss: 29.4154741049\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.818052053452,3.07633623407), test loss: 2.43848555386\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (14.8852701187,26.4325736197), test loss: 34.0729528427\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.49708902836,3.06719111869), test loss: 3.26499485373\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (16.2588844299,26.358760884), test loss: 27.868903327\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.31959426403,3.05815998631), test loss: 2.66646075398\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (15.0034303665,26.2832206994), test loss: 34.5676335812\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.18212795258,3.0492243994), test loss: 3.30887156427\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (9.27310276031,26.2099965172), test loss: 25.7830720425\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.81355476379,3.04035569045), test loss: 2.5803808108\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (7.65950965881,26.1389922985), test loss: 32.7984628201\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.73590362072,3.03147701922), test loss: 3.11162427664\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (27.7904243469,26.0683341449), test loss: 29.7583618164\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.37568330765,3.02289135953), test loss: 3.066306144\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (28.2785606384,25.9966313161), test loss: 33.6304093838\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.56256866455,3.01430396194), test loss: 3.19543628991\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (5.09633207321,25.9276488919), test loss: 31.3165623188\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.03454375267,3.00601740781), test loss: 3.31349387169\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (9.73770713806,25.8598941627), test loss: 30.380091095\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.02871012688,2.99780068716), test loss: 2.73656069785\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (31.4174461365,25.7912306207), test loss: 32.5759706497\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.40356719494,2.98968648609), test loss: 3.43745213449\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (16.9336738586,25.7240163335), test loss: 30.3295480728\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.46432721615,2.98159880302), test loss: 2.46956818402\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (7.08663368225,25.6586187043), test loss: 32.2159475327\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.3390185833,2.97353306721), test loss: 3.18450774252\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (21.6770935059,25.5940829475), test loss: 28.5346896172\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.2000784874,2.96568745936), test loss: 2.62121964097\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (10.2701063156,25.5290050078), test loss: 34.0684281349\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.81006145477,2.95786520424), test loss: 3.22426024079\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (10.6492195129,25.4649512634), test loss: 26.1273710728\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.14589679241,2.95025768505), test loss: 2.57910936326\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (13.8058376312,25.4026194415), test loss: 32.7881076813\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.04878544807,2.94272040293), test loss: 3.12616263032\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (15.2309093475,25.339479167), test loss: 29.8337701797\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.01978731155,2.93532526313), test loss: 2.94533855319\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (24.0022163391,25.2775741147), test loss: 33.1095537663\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.3551068306,2.92794931084), test loss: 3.19303002357\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (6.43575000763,25.2174259868), test loss: 30.32024405\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.342024505138,2.92058954408), test loss: 3.2908241719\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (13.2246427536,25.1577814767), test loss: 29.8977362156\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.06217074394,2.91334378172), test loss: 2.65363784134\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (19.6595993042,25.098243658), test loss: 32.8217706323\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (3.62877488136,2.9062837125), test loss: 3.42668595612\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (3.70253753662,25.0389276472), test loss: 29.9224932909\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.45485055447,2.89920922066), test loss: 2.34815891832\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (14.8481245041,24.9813658143), test loss: 32.7047002316\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (3.9504904747,2.89231674566), test loss: 3.17669917047\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (17.4753723145,24.9235726522), test loss: 29.6415496349\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.52496099472,2.8855258642), test loss: 2.58605758846\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (26.4529399872,24.8661554968), test loss: 33.2889462471\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (4.06286048889,2.87878136715), test loss: 3.13417924643\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (19.2643165588,24.8108833901), test loss: 27.8578788757\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.49864113331,2.8719672751), test loss: 2.67368685305\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (39.5569038391,24.7554266883), test loss: 32.9338378429\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (3.12255740166,2.86532312353), test loss: 3.09687087834\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (15.7295799255,24.7001649882), test loss: 30.0889790058\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.35843789577,2.85880479162), test loss: 3.0118316099\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (9.59474086761,24.6445572284), test loss: 33.7456224918\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.5702047348,2.85227999412), test loss: 3.17118697464\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (9.43261909485,24.5907816299), test loss: 29.6495803356\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.09417009354,2.84592527514), test loss: 3.04943635315\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (18.6164321899,24.5380665944), test loss: 29.1477989674\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.393312811852,2.8396663888), test loss: 2.73458548188\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (5.97115945816,24.4838447851), test loss: 33.207564497\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.4008846283,2.83343508419), test loss: 3.37818282545\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (9.68626308441,24.4318572448), test loss: 29.4544610023\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (6.3655796051,2.82720613208), test loss: 2.29934395552\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (30.0361175537,24.3806538798), test loss: 33.8024997711\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.44423556328,2.82099369594), test loss: 3.18084344566\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (14.2424783707,24.3293324541), test loss: 28.9205449104\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.29987323284,2.81494624406), test loss: 2.53279880881\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (4.80629825592,24.2775101713), test loss: 34.4694484711\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.595547616482,2.80888827393), test loss: 3.21705180407\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (25.859664917,24.2277046871), test loss: 27.5600231647\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.814608573914,2.80302184108), test loss: 2.63668081611\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (10.0300683975,24.178496797), test loss: 31.7962986231\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.823463320732,2.79720548142), test loss: 2.96848042905\n",
      "run time for single CV loop: 7096.97836304\n",
      "\n",
      "MC # 4, Hype # hyp5, Fold # 6\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold6/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (364.713348389,inf), test loss: 198.388111877\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (340.892028809,inf), test loss: 406.999243164\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (13.8128786087,135.727575455), test loss: 46.8107217789\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.2756831646,163.782400684), test loss: 3.37901688218\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (30.2512283325,91.9609295163), test loss: 37.0097532272\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.14206600189,83.4740100917), test loss: 2.79964652658\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (108.934188843,77.3423254264), test loss: 45.6454890251\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.08535051346,56.6776083311), test loss: 3.68907711506\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (32.4771118164,70.0669367611), test loss: 39.7265298128\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.16891765594,43.2801299442), test loss: 3.60898697674\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (7.46487808228,65.6244809576), test loss: 43.7857013702\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.52964448929,35.2380656717), test loss: 3.4459785521\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (40.7530479431,62.6298032961), test loss: 41.3605735302\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.35447573662,29.8700802269), test loss: 3.65031396449\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (46.0323677063,60.4425057834), test loss: 41.8383804321\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.53163576126,26.0364112526), test loss: 2.74043641388\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (27.3699760437,58.7161722543), test loss: 44.8037873745\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.07204055786,23.1573528736), test loss: 3.59961491227\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (20.8499069214,57.3714574042), test loss: 42.6794171333\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.83559703827,20.9123874297), test loss: 2.61712193489\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (19.6896362305,56.3438689425), test loss: 44.4735348701\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.53271281719,19.1195291106), test loss: 3.82372049093\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (24.3277282715,55.4525155165), test loss: 41.3389420033\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.94979524612,17.6511876266), test loss: 2.59860929847\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (36.1759605408,54.6812771881), test loss: 43.8636250019\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.854352235794,16.4254333061), test loss: 3.5720698297\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (42.0993499756,53.9948111969), test loss: 38.4242523193\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.70308303833,15.3873665046), test loss: 2.82385677695\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (36.3349609375,53.3599493138), test loss: 44.5290916443\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.7521135807,14.4953812759), test loss: 3.45873163342\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (57.8141174316,52.7940238525), test loss: 36.4526732683\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (5.06454849243,13.7196801644), test loss: 2.75555996895\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (40.9195480347,52.3127454137), test loss: 44.4242890358\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (6.72795820236,13.043777085), test loss: 3.43107885718\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (24.3287906647,51.8497588001), test loss: 35.130019474\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.65113592148,12.4462789648), test loss: 2.57975633144\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (58.9026145935,51.4032635453), test loss: 42.8475844383\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.792846798897,11.9136533566), test loss: 3.2719805181\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (70.6069717407,50.9742669972), test loss: 37.7616462708\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.07217121124,11.4360859793), test loss: 3.35254607797\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (29.6511611938,50.5523003503), test loss: 40.4293546677\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (7.13319444656,11.0056237736), test loss: 3.30631332397\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (28.9071483612,50.1512830516), test loss: 36.9237411022\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.36187016964,10.6131772322), test loss: 3.32323354483\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (97.5579376221,49.793103338), test loss: 37.1416818619\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.79071140289,10.2576840345), test loss: 2.65783234835\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (95.118057251,49.4327270719), test loss: 39.2986023903\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.31935429573,9.93280539366), test loss: 3.29897134155\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (14.6369152069,49.064772246), test loss: 38.0354194164\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.808853983879,9.63328288825), test loss: 2.44989486039\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (140.436798096,48.6975891918), test loss: 41.1657913685\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.60879230499,9.35669248468), test loss: 3.57877012491\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (34.4734344482,48.3308196262), test loss: 37.4492655754\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (4.2029914856,9.10034604439), test loss: 2.3987850666\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (22.0411529541,47.9624033307), test loss: 40.1230798244\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.02099752426,8.86144348679), test loss: 3.46174939275\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (28.9761009216,47.6071061159), test loss: 33.6584983349\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.95937347412,8.63945607254), test loss: 2.51803917587\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (18.2911911011,47.2494626981), test loss: 39.100878191\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.29513502121,8.43298866336), test loss: 3.41170598865\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (12.2115449905,46.8800429185), test loss: 31.409458971\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.797381997108,8.23882827857), test loss: 2.64917358756\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (20.9875869751,46.4757188421), test loss: 39.3064576387\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.94567489624,8.05626034956), test loss: 3.20299982429\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (59.2860565186,46.0730386123), test loss: 29.6232470989\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.65290737152,7.88416374757), test loss: 2.57914544344\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (8.02220726013,45.6576712364), test loss: 36.5484429836\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.41609501839,7.72139529742), test loss: 3.28940486312\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (9.956823349,45.2492750113), test loss: 32.3663061142\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.96269917488,7.5680196792), test loss: 3.19443251267\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (37.8641319275,44.8384594202), test loss: 34.8628437996\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.48237943649,7.4236350771), test loss: 3.35752310753\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (25.612985611,44.4198014768), test loss: 30.1318467379\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.23801469803,7.28639757125), test loss: 3.32950231433\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (84.8765563965,43.9870486867), test loss: 35.3862709045\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.12255072594,7.15550986136), test loss: 2.96018825769\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (39.8007583618,43.5653073437), test loss: 30.6503192186\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.36208176613,7.03110711369), test loss: 3.32268146873\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (20.2728748322,43.1424128941), test loss: 30.0350211143\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.14702630043,6.91253443574), test loss: 2.42141394764\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (17.8058815002,42.7262898479), test loss: 34.1768257856\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.03147292137,6.79949301955), test loss: 3.5009559691\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (49.7167358398,42.3144114088), test loss: 30.1688044548\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.60449934006,6.69248611209), test loss: 2.38557541072\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (20.993473053,41.9050695933), test loss: 33.6938758373\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.27130866051,6.58990259685), test loss: 3.44879310131\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (54.0617866516,41.4893176852), test loss: 26.8451051712\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.3886475563,6.49138325262), test loss: 2.47216998041\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (81.4511108398,41.0928686712), test loss: 33.4335288048\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (3.80071496964,6.39698622866), test loss: 3.35448513031\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (23.0222148895,40.7023514878), test loss: 28.7487371445\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (6.04196739197,6.3063114227), test loss: 2.73702189028\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (6.78000450134,40.320280279), test loss: 34.3302309036\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.05884695053,6.21941775938), test loss: 3.388856107\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (35.0377960205,39.94876266), test loss: 28.9055259228\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.26811790466,6.13668509007), test loss: 2.74070672095\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (33.5682411194,39.5860066349), test loss: 34.0821968079\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.13576173782,6.05705516776), test loss: 3.28908953071\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (19.8738517761,39.2197869403), test loss: 27.2659787178\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.72978878021,5.97982597845), test loss: 2.93663569391\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (17.0814437866,38.8730474874), test loss: 34.5206089973\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.14311289787,5.90567359927), test loss: 3.42110157609\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (5.14204597473,38.5356309804), test loss: 29.8700226307\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.374658226967,5.83396277142), test loss: 3.63289575875\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (9.65367126465,38.2044395785), test loss: 34.2185959816\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.63184833527,5.76509872374), test loss: 3.45735014379\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (11.390748024,37.8825901585), test loss: 29.742655468\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.975066185,5.69913961553), test loss: 3.50303340554\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (65.3942489624,37.5698853882), test loss: 30.9945195198\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.80926179886,5.63536213257), test loss: 2.66207405925\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (18.4288520813,37.2567977183), test loss: 32.3598693848\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.851165890694,5.5730670583), test loss: 3.51742097437\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (26.5967330933,36.9585653796), test loss: 30.7665960312\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (6.54259586334,5.51295143236), test loss: 2.50996063054\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (7.00015830994,36.6692230604), test loss: 32.3758783817\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.76615869999,5.4547386272), test loss: 3.49999682605\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (6.97934246063,36.3834194555), test loss: 29.318859911\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.688387751579,5.39831490584), test loss: 2.34529271722\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (11.8899364471,36.1058301947), test loss: 31.7874709368\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.72504997253,5.34410551945), test loss: 3.35122332573\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (43.6777458191,35.8340824767), test loss: 32.2518903255\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (4.90770053864,5.29143741675), test loss: 2.71856558174\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (24.9689826965,35.5642213361), test loss: 30.9222946644\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.2787168026,5.23985318654), test loss: 3.29326956868\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (8.19916629791,35.3056235969), test loss: 28.5177547455\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.93029642105,5.18966639415), test loss: 2.70387042165\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (10.8768167496,35.0542009785), test loss: 34.1024051785\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.3885602951,5.14117812872), test loss: 3.1864417851\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (9.2449092865,34.8041426316), test loss: 27.3903582335\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.20998764038,5.09391715686), test loss: 2.54189218879\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (5.43657112122,34.5619686197), test loss: 34.2831724644\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.0721322298,5.04829438741), test loss: 3.34877044261\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.151017189,34.3228470684), test loss: 30.9252681971\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.79208922386,5.00383526327), test loss: 3.43649744689\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (24.9281311035,34.0875803988), test loss: 34.0482966423\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.77801132202,4.96036998194), test loss: 3.37509764135\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (13.0603027344,33.8610650578), test loss: 29.3369431496\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.65768027306,4.9178289242), test loss: 3.33737216294\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (25.1132507324,33.6394291003), test loss: 29.7552757978\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.2391140461,4.87665626852), test loss: 2.60087739676\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (17.2423458099,33.4188839809), test loss: 32.2484895945\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.517619133,4.83635810312), test loss: 3.44143584967\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.9094657898,33.2050464428), test loss: 31.322410202\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.47330284119,4.79736826215), test loss: 2.46335085332\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (20.7005558014,32.9929318582), test loss: 31.2735165954\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.00045442581,4.75924522477), test loss: 3.47769079804\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (6.14684152603,32.7841986619), test loss: 31.8220703125\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.12183749676,4.72185895461), test loss: 2.43868809938\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (11.390296936,32.5829219037), test loss: 30.7161883831\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.07183969021,4.68515169362), test loss: 3.34980740547\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (34.7927932739,32.384732547), test loss: 30.1822080135\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.92353248596,4.64958909059), test loss: 2.58599711061\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (17.9575710297,32.1876851177), test loss: 33.6123525143\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.46071350574,4.61467175503), test loss: 3.34475213289\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (6.97576618195,31.9963297894), test loss: 29.342934227\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.63114404678,4.58078367577), test loss: 2.62957500815\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (13.3978710175,31.8055062014), test loss: 33.6898130417\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.83187675476,4.54765666192), test loss: 3.05231029391\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (11.672290802,31.6185922649), test loss: 27.9186335087\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.03856444359,4.51505598651), test loss: 2.50669250637\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (13.5609483719,31.4372487161), test loss: 32.840871644\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.84661149979,4.48295153183), test loss: 3.11827174425\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (11.0102329254,31.2577551687), test loss: 30.5123569965\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.2905575037,4.45179948612), test loss: 3.23231488466\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (22.0063934326,31.0797864522), test loss: 33.6378181458\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.910872161388,4.4212008043), test loss: 3.24596458972\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (14.4831352234,30.9063853051), test loss: 28.7432462215\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.86032986641,4.39137337816), test loss: 3.25100771487\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (16.7525920868,30.7332533104), test loss: 29.1313860893\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.38374590874,4.36223550808), test loss: 2.56272798628\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.008764267,30.5633680764), test loss: 29.218844986\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.18757224083,4.33352569615), test loss: 3.28069208562\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (37.2526817322,30.398163604), test loss: 31.9064557552\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.39512753487,4.30517436864), test loss: 2.40852861851\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (24.1185722351,30.2343993664), test loss: 30.6799189091\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.811286449432,4.27760876515), test loss: 3.40552604496\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (7.98809337616,30.0719722959), test loss: 32.589761591\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.09949064255,4.25049271215), test loss: 2.2508410722\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (13.4049854279,29.9134551015), test loss: 31.301833725\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.702269136906,4.22402665507), test loss: 3.29396252632\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (5.4066286087,29.7544270023), test loss: 31.484949398\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.457046926022,4.19812693283), test loss: 2.4620946005\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (9.67212200165,29.5987346692), test loss: 32.015964818\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.73193740845,4.17256582788), test loss: 3.30610696673\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (6.92268753052,29.4460319909), test loss: 29.0340665817\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.5115749836,4.14724325363), test loss: 2.56213402897\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (25.1192245483,29.2952208386), test loss: 34.2541182995\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.98793172836,4.12262137068), test loss: 3.28746436238\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (23.2848968506,29.1456341603), test loss: 29.5931408882\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.99252533913,4.09835272562), test loss: 2.53658041507\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (10.616607666,28.9989028854), test loss: 33.2033563733\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.637444555759,4.07461462909), test loss: 3.06103052571\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (8.8594121933,28.8519485188), test loss: 31.658197403\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.58165431023,4.05133940053), test loss: 3.10005132556\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (8.89752483368,28.7081604528), test loss: 33.1570822954\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.94540381432,4.02833113646), test loss: 3.15705319494\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (6.72193050385,28.5667501898), test loss: 28.6015984297\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.60652351379,4.00556834319), test loss: 3.2070729658\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (24.7757606506,28.4267199816), test loss: 34.8901536703\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (5.09720802307,3.98341823818), test loss: 3.06653915048\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (11.077038765,28.2879182349), test loss: 28.8379342198\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.79164886475,3.96156200622), test loss: 3.19598898888\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (16.3748512268,28.1519277454), test loss: 31.8294667244\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.912135064602,3.94017944372), test loss: 2.38830010891\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (11.1813211441,28.0155174396), test loss: 31.231653738\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (3.64509248734,3.91915353035), test loss: 3.318969208\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (5.13808059692,27.8820369987), test loss: 32.451780653\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.09553384781,3.89828593491), test loss: 2.2287642777\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (29.9226665497,27.7507774068), test loss: 33.1455325127\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.806013047695,3.87775550392), test loss: 3.28452492952\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (16.470659256,27.6200539801), test loss: 29.8298657417\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.12722802162,3.85761566119), test loss: 2.22524082065\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (26.1802062988,27.490974983), test loss: 31.8376038074\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.301861763,3.83786535319), test loss: 3.15763429999\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (9.48303222656,27.364069143), test loss: 30.110228157\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.673546135426,3.81843149921), test loss: 2.51029514074\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (20.7678413391,27.2369137932), test loss: 33.8375746727\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.678367674351,3.79927894541), test loss: 3.16386564374\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (3.83723855019,27.1123612534), test loss: 29.6417444229\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (3.6190867424,3.78030051356), test loss: 2.56681131721\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.3236064911,26.9888007052), test loss: 34.4459644079\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.663825035095,3.76162081963), test loss: 2.94649770856\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (11.7871608734,26.8663166935), test loss: 28.9745078087\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.06009292603,3.74322721263), test loss: 2.59058909416\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (11.9857158661,26.7450866626), test loss: 34.7415373802\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.21746230125,3.72519469434), test loss: 3.1268835485\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (4.57649040222,26.6261393953), test loss: 28.8782910347\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.809372007847,3.70744340869), test loss: 3.20160805285\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.12004041672,26.5064125294), test loss: 35.3392567635\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.81445264816,3.689955639), test loss: 3.16756435335\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (16.3723602295,26.3895704896), test loss: 28.3457659006\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.97625041008,3.67252938328), test loss: 3.18020643294\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.59713840485,26.2730418458), test loss: 33.6640465736\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.204449296,3.65550823121), test loss: 2.49586848766\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (8.9197883606,26.1577044095), test loss: 30.5478285313\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.29447424412,3.63861073321), test loss: 3.21405926645\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (4.10987091064,26.0434751238), test loss: 34.4405800343\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.01257801056,3.62213846756), test loss: 2.45284604132\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (22.2656383514,25.9314027433), test loss: 31.2834991932\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.80762374401,3.60586916228), test loss: 3.29031366706\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (13.5145540237,25.818381896), test loss: 31.941969347\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.81747925282,3.5897740158), test loss: 2.20772413313\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (6.75330448151,25.7079912335), test loss: 32.2764103413\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.453926533461,3.57377641778), test loss: 3.14242554307\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (10.2741374969,25.5980844627), test loss: 32.941248703\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.49002504349,3.55813670548), test loss: 2.40215781927\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (23.1081428528,25.4886887816), test loss: 31.8945590496\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.58026063442,3.5425468561), test loss: 3.0512271136\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (15.6824645996,25.3804653967), test loss: 30.8148268223\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.10463833809,3.52733650473), test loss: 2.5933473587\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (10.0510616302,25.2737433169), test loss: 38.0752873898\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.766313314438,3.51229779247), test loss: 2.96712252945\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (14.6218605042,25.166702617), test loss: 28.2461454868\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (3.29878616333,3.49742279213), test loss: 2.40361284614\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (20.1454963684,25.061689169), test loss: 35.0133206606\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.49387252331,3.48263060565), test loss: 3.08030918986\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (13.6097421646,24.9571747968), test loss: 29.9066994429\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (3.49998617172,3.46811215684), test loss: 3.07026984692\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (9.18839454651,24.8529503994), test loss: 36.1792173862\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.79906511307,3.45369059338), test loss: 3.13861230761\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (13.9189453125,24.7500620098), test loss: 28.6795780182\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.01775336266,3.43958364116), test loss: 3.04299849272\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (23.3850288391,24.6484355641), test loss: 38.7353120327\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.45218873024,3.4256563787), test loss: 2.89852452278\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (5.55345153809,24.5466206375), test loss: 31.0370151997\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.13988637924,3.41180545244), test loss: 3.20007066727\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (11.5950069427,24.4465615187), test loss: 35.3905370712\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.24549269676,3.39810430936), test loss: 2.48777244687\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (3.51495909691,24.3472331135), test loss: 30.5836800337\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.487901896238,3.38459035408), test loss: 3.24656238854\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (6.64077091217,24.247986147), test loss: 35.7751132011\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.99082958698,3.37122656149), test loss: 2.34223766625\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (13.6599407196,24.1499647902), test loss: 32.5552910686\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.0634560585,3.35806147982), test loss: 3.21312330961\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (29.5987052917,24.0530270908), test loss: 37.5985530853\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.41181600094,3.34513777145), test loss: 2.70536212325\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (20.3518543243,23.9564471182), test loss: 33.6102956772\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.956215679646,3.33224088928), test loss: 3.17627781034\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (18.6999282837,23.8609393146), test loss: 31.8262670517\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (4.60687065125,3.31943826921), test loss: 2.55458869934\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (11.2594137192,23.7661824869), test loss: 35.2459289074\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.4710367918,3.30685571378), test loss: 2.88841461539\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (2.73924040794,23.6713869263), test loss: 30.2553691864\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.437969326973,3.29434382795), test loss: 2.44206421375\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.64115285873,23.5772968786), test loss: 35.4723703384\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.03858733177,3.28202274764), test loss: 2.95913123488\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (28.7406272888,23.4842701221), test loss: 31.459084034\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.44830489159,3.2699252573), test loss: 2.956684497\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (11.6054010391,23.3917806915), test loss: 35.9046901703\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.678731322289,3.25785950158), test loss: 3.1666759342\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (4.4756975174,23.3002830368), test loss: 28.8449029922\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.754015564919,3.24581926925), test loss: 2.98081306517\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.43410205841,23.2094687545), test loss: 33.4955080032\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.925711631775,3.23406769011), test loss: 2.6032800436\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (11.3159952164,23.1184640937), test loss: 32.3122055054\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.28335213661,3.22235244783), test loss: 3.13974137604\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (7.15290594101,23.0284628661), test loss: 35.4433186531\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.611428380013,3.21079169377), test loss: 2.42257773131\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.39047622681,22.9388447742), test loss: 30.7635969877\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.673280179501,3.1994461625), test loss: 3.23598781228\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (11.446138382,22.8504485773), test loss: 34.8734249592\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (3.7096092701,3.18814939237), test loss: 2.31440434307\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (5.7641658783,22.7627454619), test loss: 32.9406534672\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.57950365543,3.17686392574), test loss: 3.23514668047\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (6.33777809143,22.6753682544), test loss: 33.9193489552\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.91134011745,3.1658389228), test loss: 2.4135112077\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (10.6124486923,22.5881368495), test loss: 34.4980216265\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.54936027527,3.1548232431), test loss: 3.24405171871\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (9.41593647003,22.5016640844), test loss: 31.1821252346\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.67337608337,3.14396411922), test loss: 2.46340666711\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (9.31007957458,22.4157099494), test loss: 37.2677684784\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.46497535706,3.13329609695), test loss: 3.13794730604\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (12.9841213226,22.3306305964), test loss: 32.1294772148\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.957875490189,3.12261546465), test loss: 2.58061108291\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.38744926453,22.2463195343), test loss: 36.3431312084\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.869848608971,3.11199291238), test loss: 2.94370529801\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (11.8162708282,22.1621688661), test loss: 33.1851768255\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (4.39661979675,3.10158930398), test loss: 2.92529139817\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (5.2969083786,22.0781166898), test loss: 39.1987985611\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.5122718811,3.09119656305), test loss: 3.20097431168\n",
      "\n",
      "MC # 4, Hype # hyp5, Fold # 7\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold7/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (173.541931152,inf), test loss: 157.973293114\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (397.783599854,inf), test loss: 411.931011963\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (41.4577789307,69.319493876), test loss: 48.4730303764\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.64391803741,59.0972576166), test loss: 3.64107866883\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (38.6373786926,57.4291314037), test loss: 37.9226227283\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.2076883316,31.1233523768), test loss: 3.05724305511\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (171.179901123,53.4801459912), test loss: 46.7708756447\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.30257368088,21.7864675255), test loss: 3.67647389174\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (54.9516220093,51.407185475), test loss: 42.2679554462\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.94173049927,17.1135184521), test loss: 3.85103299618\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (32.387096405,50.1868792239), test loss: 48.4956008911\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.2892203331,14.3164969299), test loss: 3.68815707564\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (31.6976470947,49.2592441351), test loss: 45.9258475304\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.60998749733,12.4231084825), test loss: 3.67891395688\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (65.9751739502,48.5723974299), test loss: 45.2639985561\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.08633947372,11.0586106352), test loss: 2.87996417284\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (33.6498184204,47.9745662555), test loss: 46.4922688961\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.67051506042,10.0345172317), test loss: 3.64708762169\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (20.7465057373,47.4566798593), test loss: 40.7602797985\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.19431495667,9.23717381052), test loss: 3.09834489226\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (90.8113250732,47.0696906792), test loss: 47.0842027187\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.2276468277,8.59820397158), test loss: 3.59039486051\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (27.1402397156,46.660862295), test loss: 39.0832797527\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.61587619781,8.0724467223), test loss: 3.05949405432\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (34.4387245178,46.3408898117), test loss: 46.641543746\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.977286219597,7.63819359102), test loss: 3.61770731807\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (19.3855018616,46.0060347554), test loss: 36.243703413\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.819944262505,7.26969779951), test loss: 2.82074620724\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (46.4078598022,45.6980353967), test loss: 42.8771653175\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (4.09802484512,6.9524998749), test loss: 3.37204419971\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (123.551651001,45.3825885019), test loss: 38.8773482084\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (6.33889532089,6.67720175592), test loss: 3.45891390443\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (102.674438477,45.0568344445), test loss: 44.6527882099\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (7.38839817047,6.43447402031), test loss: 3.37922984064\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (38.380531311,44.7602535107), test loss: 40.288276577\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.85940694809,6.21948999009), test loss: 3.53356079459\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (20.0737934113,44.4383764079), test loss: 40.4316384315\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (4.58783769608,6.02679606272), test loss: 2.77137338519\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (81.7926559448,44.1538491937), test loss: 43.2204068661\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.66246342659,5.85537428095), test loss: 3.73093452454\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (61.1648788452,43.8405552396), test loss: 38.4365414619\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.09784579277,5.70034735785), test loss: 2.95873156786\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (27.3248348236,43.5203134227), test loss: 42.6328634262\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (6.8561425209,5.55887646092), test loss: 3.52767964005\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (69.7345275879,43.2006753209), test loss: 35.336144352\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.46201229095,5.42893888548), test loss: 2.89705728889\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (141.940246582,42.8536544215), test loss: 42.2703636169\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.05525803566,5.30845046266), test loss: 3.09264349341\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (36.4686737061,42.5083370824), test loss: 31.3228248119\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.83573174477,5.19708649097), test loss: 2.67580144107\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (33.9176979065,42.1540968264), test loss: 39.619720459\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.77660942078,5.09216275903), test loss: 3.35319547057\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (21.4132766724,41.8072630641), test loss: 33.7306078911\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.815369486809,4.99540983132), test loss: 3.34651978016\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (15.7231464386,41.439284984), test loss: 37.8183797359\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.0794916153,4.90493905545), test loss: 3.37729282379\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (47.5863571167,41.066651455), test loss: 35.3208714485\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.73348975182,4.82013220735), test loss: 3.46959133148\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (40.4604797363,40.6921389778), test loss: 34.3097343445\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.93079388142,4.73972977821), test loss: 2.69366219342\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (26.5977249146,40.3010158983), test loss: 36.8279277325\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.93481016159,4.66360214239), test loss: 3.52600345612\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (20.229259491,39.9097081366), test loss: 34.3656005859\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.35586023331,4.59190965761), test loss: 2.6198728621\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (19.4750080109,39.5230688013), test loss: 37.8393097162\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.01249027252,4.52305888925), test loss: 3.40858439803\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (14.4085044861,39.1403590346), test loss: 32.1195580959\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.28103542328,4.45832623822), test loss: 2.83224030733\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (37.2260475159,38.7546408159), test loss: 39.4439231396\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.16823625565,4.39713001554), test loss: 3.38388746977\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (11.3890743256,38.3759441426), test loss: 28.7244515419\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.99838721752,4.33960352762), test loss: 2.80722194016\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (14.8870124817,38.0065374843), test loss: 36.1479468822\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (0.494082331657,4.28439007352), test loss: 3.18485624194\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (25.3730945587,37.6377345544), test loss: 32.1541780949\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.29673492908,4.23184094215), test loss: 3.35211396813\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (9.50963401794,37.2758184573), test loss: 36.1351694584\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.31351089478,4.18141823401), test loss: 3.29728633761\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (7.42896175385,36.9288284569), test loss: 32.0545866013\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.45327734947,4.13283109672), test loss: 3.46007146686\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (14.8112955093,36.594288519), test loss: 32.3603724957\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.519677519798,4.08651164307), test loss: 2.84942950308\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (13.1365127563,36.2646846028), test loss: 34.9032347679\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (4.06958198547,4.04282271449), test loss: 3.49603253305\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (33.4812355042,35.9436481415), test loss: 33.7459268093\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.67509901524,4.0009694324), test loss: 2.68335365206\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (25.3594779968,35.6340367547), test loss: 34.2842285633\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.37552022934,3.96074679335), test loss: 3.29108894467\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (16.2056999207,35.3256944786), test loss: 33.7766005993\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.83932721615,3.92203946218), test loss: 2.72742923498\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (19.7362442017,35.0263455321), test loss: 37.1021051407\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.12987136841,3.88441659813), test loss: 3.30910582244\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (25.4090633392,34.7417318863), test loss: 31.349634099\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (5.85391712189,3.84821299806), test loss: 2.85587153435\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (16.5097427368,34.466063374), test loss: 36.1239915609\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.09130835533,3.81305225236), test loss: 3.20390469432\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (35.688835144,34.1959394028), test loss: 25.8634832382\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.29466938972,3.7797046729), test loss: 2.66008591056\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (31.6082286835,33.930775537), test loss: 36.7896839619\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.32069158554,3.74747692098), test loss: 3.2644698143\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (20.0541877747,33.6759509677), test loss: 32.0772346497\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.97875118256,3.71626927505), test loss: 3.48551149964\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (28.2364349365,33.4237497686), test loss: 34.9278978348\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.709897398949,3.68619908924), test loss: 2.81284935474\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (10.896686554,33.1770689757), test loss: 34.339117384\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.3008954525,3.65673895297), test loss: 3.34635244012\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (71.6069793701,32.9424906977), test loss: 34.6965466499\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.6758146286,3.62816754133), test loss: 2.5038890481\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (9.29419136047,32.711249327), test loss: 34.3310697556\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.737222909927,3.60007058385), test loss: 3.18486319184\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (31.4025859833,32.4873931024), test loss: 33.2707374573\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.77850914001,3.57357627642), test loss: 2.78931256533\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (19.0638008118,32.2646574479), test loss: 35.5574584961\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.990107297897,3.54753735579), test loss: 3.16018027067\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (10.5256185532,32.0503042462), test loss: 31.9854650497\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.18977069855,3.52237437034), test loss: 2.74788521975\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (29.4312515259,31.8388017186), test loss: 36.6093421459\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.06776475906,3.49786331012), test loss: 3.10858038068\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (40.1830596924,31.6308591697), test loss: 29.4584827185\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (5.18271970749,3.47391786706), test loss: 2.67169196308\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (26.503068924,31.4324633923), test loss: 36.6180121899\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.6260869503,3.45042513879), test loss: 3.16173873246\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (12.0960626602,31.234121463), test loss: 31.4750439644\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.40407156944,3.42725299834), test loss: 3.4001519084\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (50.0885505676,31.043090654), test loss: 38.1648626804\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.81992220879,3.40524224448), test loss: 3.23234773278\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (11.2754869461,30.8513475093), test loss: 33.0429924488\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.22200727463,3.38355791229), test loss: 3.16072960198\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (10.0863056183,30.6661992744), test loss: 33.7361405373\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.48778557777,3.36259320613), test loss: 2.39739063978\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (44.2636947632,30.485930619), test loss: 37.402155304\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.6761713028,3.34218943176), test loss: 3.33588423729\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (36.2516441345,30.3039882185), test loss: 33.1920926094\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (4.18166637421,3.32196718209), test loss: 2.58442941904\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (9.32841205597,30.1302369709), test loss: 36.0414953709\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.80469346046,3.30215403349), test loss: 3.20125009269\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (10.6182041168,29.9581602366), test loss: 32.3679374695\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.37079453468,3.28269111174), test loss: 2.75836904645\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (20.0307445526,29.7905470385), test loss: 37.1775893688\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.02295565605,3.2637970384), test loss: 2.91879157126\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (32.7177619934,29.6220388472), test loss: 29.2547776222\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.53859877586,3.24530196258), test loss: 2.58063095808\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (17.1783542633,29.4577235381), test loss: 35.8623474598\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.00179290771,3.2274169661), test loss: 3.07644971609\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (4.85786151886,29.2983583026), test loss: 33.4362131119\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.495197683573,3.2099074178), test loss: 3.22782599628\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (18.4744110107,29.137578264), test loss: 38.2919708729\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.35740530491,3.19264786921), test loss: 3.06033322811\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (15.1371221542,28.9808255756), test loss: 31.1368600607\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.11283540726,3.17568879311), test loss: 3.16796143651\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (19.4292850494,28.8278122168), test loss: 34.2500652313\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.98861694336,3.15870035151), test loss: 2.4567198962\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.3239421844,28.6767272472), test loss: 33.0422899485\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.32276773453,3.14237291308), test loss: 3.17393950224\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (9.29199409485,28.5250763741), test loss: 34.9343327999\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.43433010578,3.1263101), test loss: 2.47624830902\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (24.5885868073,28.3778587148), test loss: 34.9124540329\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.44052457809,3.11069229182), test loss: 3.17862735391\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.26874923706,28.2344152692), test loss: 32.4703855038\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.722137212753,3.0953793121), test loss: 2.68161730021\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (13.6767578125,28.0893753048), test loss: 36.9719727516\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.42498040199,3.08027460137), test loss: 2.86165485084\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (12.930975914,27.9480113995), test loss: 29.67059021\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.9253821373,3.06538188449), test loss: 2.51872426867\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (9.36443328857,27.8093657504), test loss: 35.8484877586\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.26991653442,3.05040140034), test loss: 2.929968898\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (28.0512809753,27.6728183534), test loss: 31.0044023514\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.41469764709,3.0359814472), test loss: 3.08130326867\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (40.2373886108,27.5357619386), test loss: 36.5396193981\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.26485228539,3.02180606214), test loss: 3.08821939826\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (15.2680339813,27.4012099722), test loss: 30.5627007008\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.33504951,3.00798814166), test loss: 3.09735976458\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (17.9422607422,27.270130933), test loss: 34.6880222797\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.82912683487,2.99432423684), test loss: 2.56373401284\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (10.8654441833,27.1378032893), test loss: 32.5187871218\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.26220750809,2.98091090706), test loss: 3.19529573321\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (3.62472605705,27.0087638229), test loss: 35.0337294579\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.03010749817,2.96762025924), test loss: 2.47364045978\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (7.00728130341,26.8821838501), test loss: 32.7500783205\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.29411172867,2.95435559881), test loss: 2.9244161129\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (11.9613847733,26.7566706072), test loss: 34.7589207172\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.22259402275,2.94140651315), test loss: 2.70433450043\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.0840263367,26.6312632508), test loss: 37.5603796482\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.9031598568,2.92877277908), test loss: 3.0504190594\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (7.43208646774,26.5074784892), test loss: 30.8767744064\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.701296329498,2.91630751568), test loss: 2.73622866869\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (10.3013458252,26.3869434702), test loss: 35.639880228\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.374333679676,2.90406439796), test loss: 2.79952433705\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (9.93899726868,26.2645571162), test loss: 28.5978165627\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.2302069664,2.8920565389), test loss: 2.84461944103\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (10.0267982483,26.1451453068), test loss: 35.6419564962\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.30561470985,2.88007886462), test loss: 2.95912555754\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (10.761259079,26.0283700562), test loss: 30.5866128206\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (3.25816607475,2.86821972605), test loss: 3.14937815666\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (5.44573688507,25.9124873324), test loss: 33.5579581738\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.65780287981,2.85648216358), test loss: 2.56171661615\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (7.57996940613,25.7965427901), test loss: 32.3030599594\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.55987453461,2.84506289597), test loss: 3.06667346358\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (15.8431167603,25.6812024266), test loss: 36.594819355\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.29663288593,2.8337561577), test loss: 2.38185713887\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (3.66942834854,25.5686330907), test loss: 33.0953891277\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.14956057072,2.82260388628), test loss: 2.92680452466\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (11.3078508377,25.4559307335), test loss: 37.1683183193\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.261046886444,2.81174069822), test loss: 2.75756915808\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (6.1220035553,25.3449006079), test loss: 34.0681391478\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.04398322105,2.80089467878), test loss: 2.85631479621\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (34.9010620117,25.2360309719), test loss: 34.2437987804\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.982188344,2.7900998876), test loss: 2.8060331881\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (16.5326061249,25.1273919415), test loss: 36.6065020561\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (3.69652938843,2.77943648589), test loss: 2.81570515335\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (21.2593097687,25.0197668666), test loss: 28.463658762\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.68149733543,2.76907042676), test loss: 2.61137949824\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (10.6193418503,24.9117151225), test loss: 35.9282609463\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.720923304558,2.75872056969), test loss: 2.85021817982\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (11.9526910782,24.8057929111), test loss: 30.5306789875\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.843999743462,2.74851869262), test loss: 3.03909794092\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (5.57171201706,24.7006622017), test loss: 39.1232703209\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.49087452888,2.73855086499), test loss: 3.0119735986\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (7.3516740799,24.5967737263), test loss: 32.3839605808\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.12449002266,2.72866145972), test loss: 3.0563754797\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (12.1830348969,24.4948013064), test loss: 36.8032116413\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.4337849617,2.71876536305), test loss: 2.44451280236\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (7.83147621155,24.392223725), test loss: 34.2963214397\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.29146814346,2.70896504703), test loss: 3.03303188682\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (36.2093696594,24.2911837014), test loss: 34.34340868\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.02643084526,2.69939417084), test loss: 2.56908093393\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (13.9788694382,24.1895803694), test loss: 35.4809137344\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.66760206223,2.68988535628), test loss: 2.95323623717\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.86269950867,24.0899353954), test loss: 32.8026101828\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.956786513329,2.68050355653), test loss: 2.72172901332\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (32.4181060791,23.9915661566), test loss: 42.4332311392\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.9627584219,2.67134454984), test loss: 3.05323289335\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (7.67438316345,23.8924199032), test loss: 29.9684505463\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.69990777969,2.66219649844), test loss: 2.5954405427\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (4.50252056122,23.7958520027), test loss: 36.6587532282\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.92531192303,2.65304991095), test loss: 2.91767521203\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (26.7284507751,23.6997274195), test loss: 29.7919426441\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.82543098927,2.64401075547), test loss: 2.87929417193\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (12.6146965027,23.6036831028), test loss: 39.3699141979\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.00116634369,2.63511285814), test loss: 2.92762545049\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (16.1424636841,23.5075746352), test loss: 30.6653462172\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.06874847412,2.62628440082), test loss: 2.9679011941\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.7118368149,23.4129485203), test loss: 36.4614199638\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.06196594238,2.61761583877), test loss: 2.47231269479\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (17.2768669128,23.3199601026), test loss: 34.5635246754\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.64732885361,2.60909846466), test loss: 3.10644526482\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (9.0573387146,23.2257556788), test loss: 38.1930468559\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.96787846088,2.6006072129), test loss: 2.68009853065\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (4.8892288208,23.1336372186), test loss: 35.7449763775\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (3.12302923203,2.59217904315), test loss: 3.0036013186\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (11.0330696106,23.0427000259), test loss: 32.9728691101\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.91214859486,2.58372071302), test loss: 2.71980277896\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (4.28954029083,22.9513872368), test loss: 37.9013280869\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.15009641647,2.57545033279), test loss: 2.71096781194\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.82031726837,22.8596962838), test loss: 30.4796172142\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.24928295612,2.56726213036), test loss: 2.5221583575\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (4.17186546326,22.7692833515), test loss: 37.2833935499\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.78624892235,2.55916042954), test loss: 2.88609542251\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (6.64168024063,22.6808459267), test loss: 30.1367729664\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.16033649445,2.55118946904), test loss: 2.85746975541\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (5.18240737915,22.5913401715), test loss: 38.6127358437\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.05274510384,2.54334995454), test loss: 2.94838304818\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (12.8377990723,22.503502092), test loss: 30.9593269825\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.57740426064,2.53547618676), test loss: 2.93615632355\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (8.42135906219,22.416120704), test loss: 37.2957260132\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.11433458328,2.52754754789), test loss: 2.64224487171\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (14.8062353134,22.3291029626), test loss: 32.4454303026\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.933762013912,2.51982043865), test loss: 3.07826964259\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (17.469203949,22.2419351699), test loss: 38.0479967117\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.20599222183,2.51219281008), test loss: 2.63373004198\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (15.4453334808,22.1553748504), test loss: 34.8955715656\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.34598052502,2.50463936897), test loss: 2.90630849004\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.58783149719,22.0701522357), test loss: 34.9440137863\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.95857334137,2.49713786577), test loss: 2.77860434651\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (14.2338285446,21.9845275497), test loss: 40.445779705\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.706081390381,2.48976917763), test loss: 2.9844508484\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (3.9564344883,21.9003480663), test loss: 31.2986705303\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.744449317455,2.48238993925), test loss: 2.61497690082\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (3.0413184166,21.8166388857), test loss: 37.6551296234\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.598722934723,2.47497147695), test loss: 2.7005125314\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (11.2845954895,21.7331985915), test loss: 31.4958705425\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (3.62766838074,2.4677357825), test loss: 2.99293974191\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (7.49731206894,21.6496163066), test loss: 40.730974865\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.4800760746,2.46054482811), test loss: 3.15662012696\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (6.97143268585,21.566710524), test loss: 30.2483870029\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.817165374756,2.45342831249), test loss: 2.98698523045\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (2.903028965,21.4847462586), test loss: 36.5785217762\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.403793722391,2.44635126222), test loss: 2.71655422151\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (19.7052516937,21.4025287388), test loss: 34.4949629784\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.77939581871,2.43941749295), test loss: 3.11221252084\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (8.77484893799,21.3215844823), test loss: 38.1808250427\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.663581669331,2.43247064094), test loss: 2.66641915143\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (10.5749473572,21.2415010228), test loss: 35.1713310003\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.780095934868,2.42552965355), test loss: 2.98298294246\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (3.98815965652,21.161288599), test loss: 36.4295274258\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.97988307476,2.41864736119), test loss: 2.67629751861\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (7.0989317894,21.0810488621), test loss: 37.0185890675\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.04570555687,2.41187940631), test loss: 2.88485493064\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (7.85333395004,21.0010976916), test loss: 35.7357095718\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.849507689476,2.40512589199), test loss: 2.78028159142\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (6.04894542694,20.922258991), test loss: 38.4888056278\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.14857888222,2.39841339544), test loss: 2.72812773734\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (5.73002243042,20.8435422495), test loss: 31.0654280186\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.664500892162,2.39188002187), test loss: 2.73219280243\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (7.90166759491,20.765642749), test loss: 38.119068718\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.19661164284,2.38532342079), test loss: 2.97773864269\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (8.06372833252,20.6882803789), test loss: 31.0630356789\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.893061161041,2.378724525), test loss: 3.12569306493\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (12.7374839783,20.6112283082), test loss: 39.103231144\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (3.51184415817,2.37222991113), test loss: 2.74033331871\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.9289188385,20.5343207646), test loss: 33.0138171077\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.26504588127,2.3658327875), test loss: 2.99606694877\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.10036277771,20.4574222518), test loss: 38.3967509747\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.95862364769,2.35944149318), test loss: 2.50011951327\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.86542129517,20.3812095888), test loss: 36.3658148766\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.516192913055,2.3530603121), test loss: 2.94584052563\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (8.51329135895,20.305722488), test loss: 37.5698679924\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.27063274384,2.34685120762), test loss: 2.87114838958\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (4.39235448837,20.2307051113), test loss: 36.995946002\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.715451955795,2.34064835046), test loss: 3.01027317047\n",
      "\n",
      "MC # 4, Hype # hyp5, Fold # 8\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold8/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (301.543243408,inf), test loss: 190.679405594\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (330.821533203,inf), test loss: 372.265040588\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (32.6588554382,100.242828633), test loss: 43.3948620319\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.6449264884,48.2709980072), test loss: 3.6382645607\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (24.9575767517,73.8421108193), test loss: 40.8492276192\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.08320689201,25.6923472655), test loss: 3.57127862573\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (48.7033805847,64.9772633203), test loss: 42.1601007938\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.73807835579,18.1440093208), test loss: 3.80798488855\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (19.370098114,60.5686288044), test loss: 41.8071177721\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.19383883476,14.3525369914), test loss: 3.76302808821\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (82.5903015137,57.9079194252), test loss: 40.1539081097\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (4.41033267975,12.0755407333), test loss: 2.96862774193\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (24.2109260559,56.0355280525), test loss: 48.1336150169\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.89429795742,10.5571219108), test loss: 3.73290885389\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (20.1550941467,54.6671911731), test loss: 42.1739415884\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.41991043091,9.46729802681), test loss: 2.96356308162\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (39.9863471985,53.5959127961), test loss: 45.8819089413\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.37781190872,8.64905826835), test loss: 3.65594003201\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (46.4318656921,52.7193094054), test loss: 37.9380443573\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.70314264297,8.00882884564), test loss: 2.83655177951\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (72.1775970459,52.0051698133), test loss: 43.6141722679\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.22097158432,7.49348010476), test loss: 3.68069342971\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (28.6591320038,51.4394984876), test loss: 35.793057394\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.05621099472,7.06932831284), test loss: 2.81360217035\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (127.513259888,50.9481504635), test loss: 42.5452446461\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (5.00617599487,6.71560892669), test loss: 3.60358666778\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (24.3039360046,50.4893607484), test loss: 33.0528389931\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.79018473625,6.41644107212), test loss: 2.59598784149\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (43.9615020752,50.0725907474), test loss: 40.7020097733\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.29149460793,6.15735359163), test loss: 3.4609372288\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (25.5709571838,49.6748570128), test loss: 38.715898037\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.01885938644,5.93215573252), test loss: 3.45075801164\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (39.1196060181,49.2972220194), test loss: 42.4235889435\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.213793993,5.73263490079), test loss: 3.14656723142\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (31.1097335815,48.94905244), test loss: 41.4025120258\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.69140577316,5.55476130501), test loss: 3.49162517488\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.5638656616,48.6467270249), test loss: 39.3843337059\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.49457263947,5.39511557769), test loss: 2.70368203223\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (46.7725944519,48.3425443153), test loss: 43.8853320599\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.39669084549,5.25172695376), test loss: 3.63588163257\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (40.1593933105,48.0426382532), test loss: 36.5982115746\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.12146711349,5.12325100746), test loss: 2.60709433258\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (38.598033905,47.7396088908), test loss: 41.7086914301\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.06570541859,5.00498821612), test loss: 3.43149507493\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (39.2431983948,47.4284384103), test loss: 33.6128472805\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.1563962698,4.89705126946), test loss: 2.69305071235\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (40.3849716187,47.1130112405), test loss: 41.2763194084\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.93710565567,4.79695496533), test loss: 3.40826550573\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (41.4949188232,46.8046757854), test loss: 32.5059436798\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (6.21341657639,4.70385337684), test loss: 2.67900383472\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (14.6522159576,46.5107676973), test loss: 38.1732504845\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.88482666016,4.61725141936), test loss: 3.50049104095\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (10.7347736359,46.2035576165), test loss: 34.1152696609\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.00891637802,4.53690716681), test loss: 3.34528077543\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (67.7148132324,45.8924293605), test loss: 36.3573294163\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.67364215851,4.46310165724), test loss: 3.51647673845\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (32.9256095886,45.5693008087), test loss: 36.2475405693\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.640479147434,4.39338431289), test loss: 3.50149655938\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (23.9765529633,45.2295207747), test loss: 32.2201804161\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.58463871479,4.32815793235), test loss: 2.69920615852\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (19.9275474548,44.8786672727), test loss: 40.5478490353\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.24126529694,4.26608301821), test loss: 3.52789360285\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (33.5320281982,44.52040104), test loss: 32.4818408012\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (5.56654644012,4.20696379914), test loss: 2.45558261275\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (32.1975288391,44.1440218297), test loss: 37.887739563\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (6.08827161789,4.15109651774), test loss: 3.55781361461\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (11.6809082031,43.7486666424), test loss: 27.6756587505\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.63453960419,4.09817928608), test loss: 2.56874748021\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (81.4630355835,43.3484837895), test loss: 36.9983259439\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.29328680038,4.04854514398), test loss: 3.55785923302\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (31.7405357361,42.9445206654), test loss: 25.7635584831\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.26688230038,4.00111898658), test loss: 2.81584684849\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (15.995470047,42.529899301), test loss: 33.5632540703\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.42648506165,3.95601469716), test loss: 3.29884869456\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (17.8679275513,42.1140180161), test loss: 24.9036656857\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.27006292343,3.91244458576), test loss: 2.96959604621\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (6.23727893829,41.7069850136), test loss: 33.6010068893\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.873737335205,3.87038977586), test loss: 3.38761614561\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.8044052124,41.3090568903), test loss: 29.6243632078\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.9788441658,3.83027631798), test loss: 3.51841408312\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (59.7037506104,40.9104718562), test loss: 26.5428268433\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.25537371635,3.79225815924), test loss: 2.74719018936\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (15.2126569748,40.5148367162), test loss: 32.6793323517\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.56789660454,3.75615871551), test loss: 3.41139104664\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (34.014793396,40.1320387657), test loss: 28.6459953308\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.1376799345,3.72192744644), test loss: 2.61672186553\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (26.7079544067,39.7470543442), test loss: 33.985051918\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.82605838776,3.68901209253), test loss: 3.48377964795\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (26.4943237305,39.3699539204), test loss: 26.1530080318\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.3331143856,3.65696425624), test loss: 2.38486123383\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (6.8934340477,39.0067379163), test loss: 34.6716802597\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.77071046829,3.62573092024), test loss: 3.32753611207\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (9.39462184906,38.6539564936), test loss: 26.3080064297\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.11422622204,3.59567056742), test loss: 2.78260357082\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (46.6661987305,38.3055954671), test loss: 34.8477502823\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.18495750427,3.56699090143), test loss: 3.55385066867\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.7531986237,37.9658401092), test loss: 23.5325081348\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.15352344513,3.53958572678), test loss: 2.69883764386\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (34.2350883484,37.6390411754), test loss: 32.8758399963\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.32556080818,3.51322712831), test loss: 3.46308310628\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (39.1044235229,37.3144177524), test loss: 29.2097093105\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.17701387405,3.48753300506), test loss: 3.25624041557\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (9.75458335876,36.9968413771), test loss: 32.8519999981\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.62829113007,3.46232703305), test loss: 3.4881875962\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (8.40532493591,36.6928526516), test loss: 31.1843878746\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.46887886524,3.43753713793), test loss: 3.47881316692\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (5.92635822296,36.3963434106), test loss: 27.6648383379\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.343341141939,3.41353522253), test loss: 2.63690236509\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (33.4944419861,36.102551501), test loss: 32.4313022137\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.98335027695,3.3904058087), test loss: 3.53360339999\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (3.67631745338,35.8185000723), test loss: 27.8587691069\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.98253846169,3.36822146776), test loss: 2.4771115467\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (16.968214035,35.544019337), test loss: 33.143366313\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.52324724197,3.34672460847), test loss: 3.28553828597\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (20.1912574768,35.2718027233), test loss: 27.3283772945\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.1468629837,3.32561519879), test loss: 2.61122775376\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (15.4600286484,35.0057770246), test loss: 33.8637500763\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.11618900299,3.30478693198), test loss: 3.3413785547\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (6.70235824585,34.7497767818), test loss: 25.6277829885\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (4.700963974,3.28417574448), test loss: 2.64664921314\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.3225688934,34.4987032275), test loss: 32.0790110111\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.20646452904,3.26411619447), test loss: 3.31135981977\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (37.2669525146,34.2490892752), test loss: 30.2379225016\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.54587829113,3.24464351183), test loss: 3.2465102464\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (5.0295715332,34.0079501096), test loss: 32.315783143\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.6076104641,3.22595772947), test loss: 3.39878106117\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (18.2822093964,33.7734803664), test loss: 28.910819912\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.501828193665,3.2076926513), test loss: 3.39121885896\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (10.0884160995,33.5398037867), test loss: 27.3255980015\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.29999923706,3.18978762028), test loss: 2.78810746223\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (18.0357284546,33.3122068867), test loss: 31.2690310478\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.24140357971,3.17203043326), test loss: 3.40179120898\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (9.43896484375,33.0920016999), test loss: 29.5706908226\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.31257104874,3.15428092193), test loss: 2.49442528784\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (31.3202857971,32.8749302266), test loss: 33.3849186182\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.57688593864,3.13716285212), test loss: 3.37728168964\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (9.55711364746,32.6576425466), test loss: 25.9772924423\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.81313371658,3.12033027465), test loss: 2.30076000541\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (22.7309150696,32.4489052327), test loss: 34.6746801853\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.20219683647,3.1041201776), test loss: 3.27848474383\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (13.398481369,32.2441745164), test loss: 26.0554035187\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.92131984234,3.08833128093), test loss: 2.63657018393\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.9508810043,32.0399748949), test loss: 33.7468176126\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.27692139149,3.07271540797), test loss: 3.26948367208\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (10.4796667099,31.8406005912), test loss: 23.4569127083\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.698260009289,3.05724346232), test loss: 2.41281128079\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (7.26976537704,31.6470272825), test loss: 33.5127402067\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.896572887897,3.04173698313), test loss: 3.26903014183\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (19.1176738739,31.4550125792), test loss: 28.6826858521\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.58779859543,3.02669749666), test loss: 3.20643702745\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (10.8513326645,31.2636336212), test loss: 34.005565846\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.29864573479,3.01193857011), test loss: 3.34809006751\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (30.3548583984,31.0787889375), test loss: 29.9218398571\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.46921277046,2.99769034578), test loss: 3.32973609865\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (5.01528644562,30.8967576729), test loss: 30.1675930023\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.02411425114,2.98371703465), test loss: 2.57551891506\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (16.0098247528,30.7151650767), test loss: 31.4278749943\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.75105500221,2.96992518036), test loss: 3.39674042463\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (6.16818904877,30.5372519318), test loss: 29.9349543571\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.07827115059,2.95621615216), test loss: 2.42964656353\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (10.6739368439,30.3641080123), test loss: 32.2663105011\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.2631611824,2.9424473259), test loss: 3.20024951696\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (18.1271800995,30.1920397216), test loss: 29.3361552715\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.08495473862,2.92905845277), test loss: 2.54788809419\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (4.450922966,30.0200043417), test loss: 33.7894773722\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.25200164318,2.91591655292), test loss: 3.21829341501\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (25.2746505737,29.8531005973), test loss: 27.5460024834\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.753458976746,2.90314868653), test loss: 2.59505580962\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (9.7173576355,29.689547104), test loss: 32.7273688078\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.50283998251,2.89068014808), test loss: 3.13579729497\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.3009376526,29.525510105), test loss: 29.9379214764\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.05080866814,2.87832004323), test loss: 3.09695762396\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (9.7324256897,29.3647798984), test loss: 33.7856347561\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.27293205261,2.86603626011), test loss: 3.29173983335\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (17.7962265015,29.2077584811), test loss: 28.3769151449\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.58117055893,2.85367135804), test loss: 3.09168139398\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (9.47434806824,29.050840569), test loss: 28.9255043507\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.40141010284,2.8416104515), test loss: 2.6517057687\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (5.08400630951,28.8948730184), test loss: 31.9275147438\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.02289748192,2.82975363828), test loss: 3.29070245028\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (12.7029342651,28.7427491222), test loss: 31.4204549789\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.07293105125,2.81823084422), test loss: 2.49212297648\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (17.7839546204,28.594171041), test loss: 34.8104596615\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.743102014065,2.80696673195), test loss: 3.25056184828\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (10.6355628967,28.4444065085), test loss: 29.8050440788\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.55039644241,2.79576815731), test loss: 2.52205286622\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (7.05281639099,28.2979647097), test loss: 35.1975768805\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.87927532196,2.78459939601), test loss: 3.29479157627\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (22.4997463226,28.1544109117), test loss: 25.9660205364\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.00451600552,2.77338158038), test loss: 2.55945703387\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (23.7214241028,28.0108846427), test loss: 33.9547996283\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.641700148582,2.76244544944), test loss: 3.18505297899\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (8.15303993225,27.8679965494), test loss: 23.2139931917\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.45785677433,2.75165178762), test loss: 2.39799539\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (15.7251529694,27.7282559649), test loss: 34.1733878613\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.33601152897,2.74114594684), test loss: 3.16575679183\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (12.5075798035,27.5915101179), test loss: 28.5523512602\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.558104872704,2.73085217764), test loss: 3.18963934779\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (26.0182914734,27.4541701253), test loss: 34.0523368835\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.11441516876,2.72058807217), test loss: 2.94018329531\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (7.9540438652,27.3198067154), test loss: 29.3424369812\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.04083991051,2.71037716188), test loss: 3.20572498143\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (18.4125328064,27.187401996), test loss: 33.2323207378\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.8985325098,2.70015987994), test loss: 2.57068907917\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (13.8453998566,27.0550960776), test loss: 32.5233124256\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.779360175133,2.69013306305), test loss: 3.3098534584\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (31.2075061798,26.9234357636), test loss: 30.1811454773\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.50917339325,2.68023766381), test loss: 2.44882738292\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (12.0750246048,26.7940044012), test loss: 33.6732198238\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.49499356747,2.67056123965), test loss: 3.16428244412\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (7.47373771667,26.6674678617), test loss: 32.4144918919\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.335914671421,2.6611012275), test loss: 2.75866713524\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (19.8341770172,26.5406563424), test loss: 34.7212394714\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.07512998581,2.65166623839), test loss: 3.2140654996\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (11.8938512802,26.4161981791), test loss: 25.905522871\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (4.48291873932,2.64225465446), test loss: 2.6399622947\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.98272705078,26.293335067), test loss: 34.7913825512\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.55296945572,2.63283231199), test loss: 3.12437440157\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (14.990647316,26.1706245788), test loss: 29.6604218483\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.76109480858,2.62359476522), test loss: 2.99829297066\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.5086364746,26.0481627228), test loss: 35.4804689169\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.94096064568,2.61444287277), test loss: 3.24547495097\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (10.882188797,25.9281459397), test loss: 28.2907483101\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.07620406151,2.60547161406), test loss: 3.15543856025\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (12.0689105988,25.810247373), test loss: 33.590300703\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.260125428438,2.59672047366), test loss: 2.85746924281\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (5.06414365768,25.6917550687), test loss: 33.0674672842\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.43942785263,2.58797330678), test loss: 3.33203225434\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (6.80928707123,25.5759142719), test loss: 31.5785130024\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (3.61098575592,2.57922550164), test loss: 2.37702602744\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (4.96491479874,25.4614993133), test loss: 35.053361845\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.5361353159,2.57050176261), test loss: 3.17948686779\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (21.876947403,25.3468254782), test loss: 30.1859188557\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.830676138401,2.56192674237), test loss: 2.51408709139\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (16.8304214478,25.2324452741), test loss: 36.7079142094\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.26690578461,2.55339982711), test loss: 3.26022486687\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (8.61694717407,25.1201390534), test loss: 26.7701221943\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.03512477875,2.54506197491), test loss: 2.66709278226\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (11.9275598526,25.0097898166), test loss: 37.922939539\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.205350548029,2.5369209298), test loss: 3.28495768607\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (17.9171638489,24.8988571782), test loss: 26.3394714832\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (3.15922212601,2.52876680861), test loss: 2.84498957992\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (6.77538013458,24.7901837139), test loss: 35.0401890993\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.54211091995,2.52056028168), test loss: 3.19055006802\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (10.3508586884,24.6825298321), test loss: 27.9098017454\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.41254878044,2.51245040693), test loss: 3.07020095587\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (22.3502960205,24.5747085726), test loss: 32.0076299191\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (3.13614940643,2.50444670487), test loss: 2.74811837077\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (11.1927871704,24.4672896122), test loss: 32.0728010416\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.85456037521,2.49648120501), test loss: 3.17571586072\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.24985980988,24.3618430857), test loss: 34.1840780735\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.13798308372,2.4886883806), test loss: 2.61833695173\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (31.1713104248,24.2577270806), test loss: 35.237051034\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.24058866501,2.4810640889), test loss: 3.37360735536\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.86632156372,24.1532518231), test loss: 30.2497751713\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.3931440413,2.47338327283), test loss: 2.33382351547\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (3.79481244087,24.0507689377), test loss: 35.2513967037\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (3.08644008636,2.46571524982), test loss: 3.12354657352\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (33.0224151611,23.9492035352), test loss: 28.4579001427\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.72288990021,2.45811996355), test loss: 2.62489388287\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (5.91571426392,23.8472663065), test loss: 37.1275187492\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.862706422806,2.4505738203), test loss: 3.24863233864\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (10.9154624939,23.7458915666), test loss: 25.6345594406\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.28015971184,2.44313411486), test loss: 2.5906791091\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (5.44991636276,23.6461006432), test loss: 35.96725595\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.521355092525,2.43578394328), test loss: 3.21276535392\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (26.6528320312,23.5474673673), test loss: 29.6371680021\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.17950057983,2.42862958078), test loss: 3.12399373651\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (5.01679563522,23.448977376), test loss: 36.4266921997\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.606698274612,2.42142402677), test loss: 3.32408977374\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (3.19038820267,23.3518936764), test loss: 29.3445014954\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.40504682064,2.41418915999), test loss: 3.19178885221\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (20.1221446991,23.2554223153), test loss: 32.8833021879\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.82394456863,2.40705174598), test loss: 2.70889623016\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (15.4698057175,23.1586885328), test loss: 32.9006155014\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.30596041679,2.39995302908), test loss: 3.28378148675\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (9.75981712341,23.0625441954), test loss: 34.4284163475\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.7402715683,2.39294171216), test loss: 2.5146794498\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (4.71269226074,22.9678935734), test loss: 35.6929665565\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.564903855324,2.38602912909), test loss: 3.15364224017\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (19.4846839905,22.8741429559), test loss: 32.539721632\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.589370727539,2.37926378908), test loss: 2.6973741293\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (6.88554859161,22.7807273), test loss: 36.8944942474\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.49259316921,2.37248645119), test loss: 3.2681793347\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (7.19192123413,22.6883852721), test loss: 26.9430262327\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.1943230629,2.36562743121), test loss: 2.62880374193\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (7.40615081787,22.5964471898), test loss: 35.0223333359\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.15011310577,2.35888321908), test loss: 3.09940688312\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (22.9545669556,22.5046077048), test loss: 30.5395957708\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.40829002857,2.35219157172), test loss: 2.99307652414\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (6.97093868256,22.4131378759), test loss: 36.4992486954\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.14147639275,2.34555537577), test loss: 3.28694953024\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (6.8532037735,22.3230518492), test loss: 29.4178689957\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.639224946499,2.33901627131), test loss: 3.12194859385\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (8.08241462708,22.233472312), test loss: 33.0149249554\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.04949903488,2.33261102806), test loss: 2.86749720722\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (12.8263292313,22.1447501771), test loss: 32.9938886881\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.89298868179,2.32619427222), test loss: 3.29410244823\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (11.1377487183,22.0568569206), test loss: 34.328764677\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.18522143364,2.31968217472), test loss: 2.62623029649\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (12.1249904633,21.9691727031), test loss: 35.5169469357\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.85573005676,2.31329022212), test loss: 3.27064056098\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (9.93332004547,21.8814259501), test loss: 29.921895647\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.21049010754,2.30693060644), test loss: 2.40814975351\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (13.6860952377,21.7943913027), test loss: 37.4333148479\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.14550673962,2.300628303), test loss: 3.20242044628\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.84367895126,21.7083899259), test loss: 28.724209404\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.00008821487,2.29441817735), test loss: 2.59674267769\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (7.14239788055,21.622942782), test loss: 36.138462162\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.50081014633,2.2883179332), test loss: 3.12547940314\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (5.7893781662,21.538274316), test loss: 25.351390934\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.18657243252,2.28217595262), test loss: 2.45925634801\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (8.65678405762,21.4542452634), test loss: 36.6348584652\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.697871923447,2.27598709937), test loss: 3.20633901954\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (18.2643814087,21.3705306064), test loss: 29.4632582664\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.820684075356,2.26988200264), test loss: 3.01691464782\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (6.08805274963,21.2869318371), test loss: 38.5569542885\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.30071353912,2.26382648599), test loss: 3.35982091129\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (8.26991271973,21.2037658584), test loss: 32.0214422703\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.34602594376,2.25782110508), test loss: 3.21575563252\n",
      "\n",
      "MC # 4, Hype # hyp5, Fold # 9\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold9/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (364.444488525,inf), test loss: 212.251405716\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (268.142089844,inf), test loss: 324.214910889\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (52.9666976929,106.655399653), test loss: 46.6285742044\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.597322702408,46.6175942725), test loss: 3.66072994769\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (73.0971908569,77.3364748569), test loss: 41.8236673355\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.72485876083,24.9156591586), test loss: 3.68745445013\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (97.9811096191,67.612135383), test loss: 43.9828939915\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.92569088936,17.6761238559), test loss: 3.69440510273\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (54.8991317749,62.5551647031), test loss: 43.2523468018\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (5.40130329132,14.0465629581), test loss: 3.90609366894\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (90.9855957031,59.6166236101), test loss: 43.4227074146\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (7.5881485939,11.8787959574), test loss: 3.03434719145\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (51.7210960388,57.5813754905), test loss: 46.4942769051\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.35054948926,10.4304717711), test loss: 3.89161052704\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (42.7466621399,56.0798503692), test loss: 44.4462254524\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.43408465385,9.39683148016), test loss: 2.84126448035\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (31.3369541168,54.9459252801), test loss: 44.1895629406\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.61526203156,8.61951088868), test loss: 3.69009731412\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (42.8593902588,54.0216309741), test loss: 41.1226515293\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.96205949783,8.01369082362), test loss: 3.19872538447\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (30.5048675537,53.2135783865), test loss: 47.2344501495\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.36259460449,7.52842433568), test loss: 3.6637932092\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (25.0667095184,52.5579555827), test loss: 38.5654561043\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.12778449059,7.12853588163), test loss: 2.86670967937\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (45.0531539917,52.0235405833), test loss: 44.0461146832\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.52882933617,6.79556931659), test loss: 3.71379702091\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (42.7236862183,51.5507986013), test loss: 39.2385668755\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (8.42872619629,6.51465769759), test loss: 3.44998977482\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (21.5156154633,51.1012888957), test loss: 44.9938880444\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.8234539032,6.27298646969), test loss: 3.51943382025\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (43.7050628662,50.706500212), test loss: 43.508932972\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.27923810482,6.06227018538), test loss: 3.74596549273\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (99.2829360962,50.3244491), test loss: 42.6666497231\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.87317562103,5.877757741), test loss: 2.82982291877\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (36.4790458679,49.9455559478), test loss: 43.7830327988\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.48224115372,5.71443457276), test loss: 3.79968720675\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (33.7883834839,49.6041802802), test loss: 41.6335181713\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (6.63475561142,5.56870871922), test loss: 2.82765164971\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (119.093109131,49.2717090944), test loss: 43.5969087601\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.27340364456,5.43648917182), test loss: 3.65230549574\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (29.5805702209,48.9604556229), test loss: 37.933790493\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.30107116699,5.31963935939), test loss: 2.85137703419\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (20.6214141846,48.6277887012), test loss: 45.0605027199\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.71535503864,5.21632881509), test loss: 3.67123650908\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (24.0632896423,48.3046038129), test loss: 33.7223816395\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.918753266335,5.12130858869), test loss: 2.69354436398\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (26.6585769653,47.9784715653), test loss: 41.1348545074\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.65260910988,5.03164279016), test loss: 3.38911170065\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (37.2206420898,47.6425146148), test loss: 36.5550949574\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.79035139084,4.94929449361), test loss: 3.61797136664\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (29.3266220093,47.292973191), test loss: 41.2394719124\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.14596748352,4.87240264091), test loss: 3.45069692135\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (34.934387207,46.9515277068), test loss: 37.6172410488\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.71085107327,4.80021968928), test loss: 3.42611566484\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (14.1463003159,46.6087676401), test loss: 36.634489727\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.21209096909,4.73167659103), test loss: 2.73313889503\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (27.441576004,46.2606179419), test loss: 38.6183028698\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.29264497757,4.6671585273), test loss: 3.69943878651\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (40.4646377563,45.8928136463), test loss: 33.6178545952\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (6.00632286072,4.60606836402), test loss: 2.54426257014\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (17.23853302,45.5213804587), test loss: 40.4866773129\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.28096747398,4.54757259179), test loss: 3.54543645382\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (16.3245582581,45.1409667243), test loss: 31.0093571186\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.19374227524,4.4925256522), test loss: 2.85034770966\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (69.8146286011,44.7414210544), test loss: 37.8661952019\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.38744163513,4.43926719867), test loss: 3.21931110024\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (27.9384002686,44.3366010713), test loss: 26.9335280895\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.49222254753,4.38872885046), test loss: 2.58822729886\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (21.1780490875,43.9356308445), test loss: 36.7010658741\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.955694198608,4.3392580513), test loss: 3.40123477578\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (16.1870574951,43.5336931166), test loss: 30.9264629841\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.934945642948,4.29206028278), test loss: 3.43899686337\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (15.3181304932,43.1293541047), test loss: 30.1356995583\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.69055557251,4.24706567676), test loss: 2.83211734891\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (12.886715889,42.7262391687), test loss: 34.5210295677\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.48415350914,4.20408586237), test loss: 3.53037686944\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (42.4044113159,42.3257132284), test loss: 32.2435897827\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (5.2401008606,4.16208272211), test loss: 2.77714295387\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (31.9330444336,41.9213252085), test loss: 35.8884472847\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.689732611179,4.12161021852), test loss: 3.54703696668\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (60.4810256958,41.5216971629), test loss: 28.9044435978\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.04592585564,4.08271876939), test loss: 2.85497317016\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (30.9556026459,41.1384081481), test loss: 37.2907427311\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.11796712875,4.04493460341), test loss: 3.54531069696\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (55.5366325378,40.7587689499), test loss: 27.6516704559\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.76006269455,4.00796365988), test loss: 2.70622781962\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (44.8192672729,40.3916223119), test loss: 34.569802022\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.57998085022,3.97297324353), test loss: 3.40994902253\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (11.8751602173,40.0269161517), test loss: 30.8930173874\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.699391424656,3.93893220803), test loss: 3.15490694642\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (31.3985481262,39.6748488946), test loss: 34.8279257298\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.2781264782,3.90638828336), test loss: 3.47574483156\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (18.0114402771,39.3319349115), test loss: 30.4658944607\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.78038787842,3.87468010591), test loss: 3.55865886509\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (21.4340820312,38.991167202), test loss: 28.5468873501\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.15598094463,3.84390042473), test loss: 2.78388844132\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (2.57812142372,38.6577064464), test loss: 35.1509674311\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.415418058634,3.81398240073), test loss: 3.69722086489\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (8.18896389008,38.3390260977), test loss: 31.4246391773\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.18751239777,3.78463754515), test loss: 2.56375924945\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (11.1670360565,38.0288253102), test loss: 34.9865712643\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.5515409708,3.75631735706), test loss: 3.57974284887\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (18.2354660034,37.7241925652), test loss: 30.0908829689\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (6.01178693771,3.72925120351), test loss: 2.78998344094\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (15.1217947006,37.4265592163), test loss: 37.3912895679\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.93443346024,3.70288125507), test loss: 3.43620328009\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (20.846622467,37.1387342157), test loss: 29.2625424385\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.796600699425,3.67728684648), test loss: 2.84390065074\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (61.4646072388,36.8556362152), test loss: 34.2992378712\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.23507356644,3.65245702326), test loss: 3.42045160234\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (11.3577642441,36.5745717217), test loss: 30.4861161232\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.73294758797,3.62827373728), test loss: 3.24891952127\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (26.0719985962,36.3059454169), test loss: 34.1526492596\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (6.87541151047,3.60469703947), test loss: 3.47087358832\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (48.9948616028,36.0436017186), test loss: 30.5534521818\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.60111474991,3.58122173296), test loss: 3.52659267634\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.1615581512,35.7861221688), test loss: 30.6395071983\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.986566782,3.55871854806), test loss: 2.76620205939\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (15.4854679108,35.531284381), test loss: 35.0168567181\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.89131879807,3.53674186936), test loss: 3.64864401817\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (18.5026187897,35.2866786794), test loss: 31.1615749359\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.50278246403,3.51568918847), test loss: 2.58092414737\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (15.9316749573,35.0480441157), test loss: 34.7322284937\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.741329073906,3.49489464318), test loss: 3.46902678311\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (15.2719421387,34.8078169059), test loss: 30.1010499239\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.36617124081,3.47464738126), test loss: 2.70449359417\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (17.7692127228,34.5738344106), test loss: 36.16731112\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.5563056469,3.4547878637), test loss: 3.4605393827\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (7.50808811188,34.3474809349), test loss: 26.034992075\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.18275868893,3.43525063362), test loss: 2.61304762065\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (22.5479698181,34.1256241167), test loss: 35.706959343\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.8170478344,3.41586241154), test loss: 3.38667422235\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (24.0828552246,33.9072548916), test loss: 29.2864473581\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.917602479458,3.39735555563), test loss: 3.31929955781\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (18.5933761597,33.6919108709), test loss: 36.6243549824\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.85037899017,3.37911556652), test loss: 3.41509149224\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (14.1899833679,33.481909747), test loss: 31.2673468828\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.90757501125,3.36128753141), test loss: 3.46003164053\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (39.7361297607,33.2772804659), test loss: 31.0886022091\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (4.24303340912,3.34403260467), test loss: 2.63719790578\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (13.6681213379,33.0696742557), test loss: 33.2722588778\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.73373174667,3.32685491056), test loss: 3.60464694798\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.7047586441,32.868807412), test loss: 31.2048631191\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.60070848465,3.31008577372), test loss: 2.56229719669\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (10.13032341,32.6731384758), test loss: 37.1820935488\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.10108160973,3.29326753413), test loss: 3.4837675631\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (14.7294921875,32.4796045714), test loss: 29.0069081306\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.456840515137,3.27693180285), test loss: 2.70704987198\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (9.49962520599,32.2879803237), test loss: 37.2682302475\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.2342414856,3.26098258238), test loss: 3.4959480077\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (9.90802383423,32.0999297488), test loss: 25.0439737558\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.16750001907,3.24546735805), test loss: 2.4588544935\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (12.4450035095,31.916662207), test loss: 35.4294727325\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.00552606583,3.2301627951), test loss: 3.35929832757\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (14.4834051132,31.732532723), test loss: 29.2000457764\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.35916256905,3.21513643397), test loss: 3.35313488841\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (25.0732250214,31.5524398284), test loss: 35.4431697845\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.01542568207,3.20040121615), test loss: 3.41667681634\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (13.8293781281,31.3767803089), test loss: 30.3417427778\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.46709227562,3.18572966582), test loss: 3.34841243029\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (45.364315033,31.2025595894), test loss: 31.9967394829\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (4.14574718475,3.17122397586), test loss: 2.62217338383\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (18.8177528381,31.0306705296), test loss: 34.8364963531\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.50009191036,3.15713742225), test loss: 3.47462385893\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (5.3041305542,30.8596061929), test loss: 30.2469816208\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.517422497272,3.14313649038), test loss: 2.3876742959\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (21.7030124664,30.6931529176), test loss: 39.1655499935\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.69063901901,3.12959842857), test loss: 3.54774634242\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (12.1756410599,30.5293442128), test loss: 28.6092624187\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.928078234196,3.11626625089), test loss: 2.70816398859\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.9444713593,30.3642281925), test loss: 36.370950079\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.12631654739,3.10307889259), test loss: 3.23646276295\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (4.07142066956,30.2028079222), test loss: 25.0946273327\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.55072784424,3.09003513587), test loss: 2.41309503317\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (5.70683240891,30.0446932667), test loss: 36.8930674314\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.607557594776,3.07698475855), test loss: 3.35510512888\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (4.1477355957,29.8883871523), test loss: 30.3405332565\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.29314720631,3.06427093917), test loss: 3.27145885825\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (26.6528377533,29.7331682145), test loss: 31.2806722164\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (5.41221380234,3.05184760245), test loss: 2.84203683734\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (23.060251236,29.5797035437), test loss: 34.4802660704\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.77090132236,3.03950760438), test loss: 3.3901199609\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (9.09276294708,29.4292758522), test loss: 33.3013281345\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.631323039532,3.02741911911), test loss: 2.59754210562\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (55.484992981,29.280244944), test loss: 37.1952092409\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.99325990677,3.015604499), test loss: 3.47003496289\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (3.68200516701,29.131179509), test loss: 30.5725552082\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.60053467751,3.00385413066), test loss: 2.56056354493\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (15.0805454254,28.9859114397), test loss: 37.7333941936\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.68124556541,2.99222821918), test loss: 3.41068603992\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (22.7408561707,28.8425233112), test loss: 28.3512501717\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.64271521568,2.98057193542), test loss: 2.68027774096\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (14.7392463684,28.6994550832), test loss: 36.3447366953\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.62992691994,2.96923035566), test loss: 3.11936875284\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (22.9961986542,28.5572310873), test loss: 30.563742137\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.53324794769,2.95796851738), test loss: 3.00865087509\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (12.8116092682,28.4181625831), test loss: 35.8287528515\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.0360326767,2.9470574063), test loss: 3.29098947942\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (6.0271115303,28.2814704941), test loss: 30.2231862545\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.643023312092,2.93619904941), test loss: 3.20576196015\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (24.288356781,28.1436187818), test loss: 32.4635243893\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.37118911743,2.92552855663), test loss: 2.73849921823\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (16.5984687805,28.008713819), test loss: 33.2112400532\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.56054723263,2.91495039325), test loss: 3.39714232683\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (11.2798538208,27.8754301719), test loss: 32.0499995232\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.24121654034,2.90440864574), test loss: 2.45374337435\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (23.0761528015,27.7434195893), test loss: 36.3274014711\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.18543100357,2.89391200334), test loss: 3.29490487874\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (34.4956436157,27.6125104069), test loss: 32.9040774822\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.47995066643,2.88374255073), test loss: 2.62026120722\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (16.2148780823,27.4820188447), test loss: 38.5768788815\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.04330539703,2.87357927982), test loss: 3.28112052381\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (11.0896158218,27.3540797099), test loss: 28.9953832626\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.60080707073,2.86358275681), test loss: 2.63938574195\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (36.2440567017,27.2283567986), test loss: 36.208373642\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (4.04175233841,2.85386479067), test loss: 3.1884329915\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (7.65322160721,27.1007874608), test loss: 30.5446252346\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.41477370262,2.8441425678), test loss: 3.07096738219\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.6301469803,26.9765718492), test loss: 36.2997023582\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.35369920731,2.83452754313), test loss: 3.26838669479\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (6.0522480011,26.8537579171), test loss: 30.9914849758\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.12669277191,2.82482257324), test loss: 3.29454512894\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (14.5130214691,26.7315201722), test loss: 33.3711431026\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.402376383543,2.81536532073), test loss: 2.77080114484\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (7.14596176147,26.6097465329), test loss: 33.1750298023\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.80995512009,2.80600371389), test loss: 3.48800260127\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (16.0985641479,26.4895712076), test loss: 34.5344341278\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.6591643095,2.79682600896), test loss: 2.54475322068\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (10.4022846222,26.3711547549), test loss: 37.131033349\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.31836915016,2.78770119113), test loss: 3.29988462627\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (14.0329494476,26.2526625883), test loss: 31.8464436054\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.13258004189,2.77875059309), test loss: 2.67278096378\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (11.2726659775,26.1361610764), test loss: 38.5084237576\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.90864992142,2.76989698009), test loss: 3.30258713216\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (11.0056934357,26.0209065389), test loss: 27.255933857\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.02250981331,2.76098382272), test loss: 2.53325996697\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (22.9007949829,25.9062086133), test loss: 37.0587506771\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (3.23144459724,2.75220423746), test loss: 3.13935461938\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (7.62818336487,25.7920755867), test loss: 30.5511092186\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.3292028904,2.74356622998), test loss: 3.00146351159\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (3.89915156364,25.6784499289), test loss: 37.686951828\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.18990111351,2.73490423524), test loss: 3.24884686172\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (18.6536979675,25.5668036277), test loss: 31.4297967672\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.37416386604,2.72645428659), test loss: 3.27995961905\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (11.0099067688,25.4563402994), test loss: 35.0701990128\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.57113826275,2.71813404456), test loss: 2.62862163633\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (8.66282272339,25.3449977638), test loss: 33.8919247389\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.983596205711,2.70986517693), test loss: 3.4021143496\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (7.61252212524,25.2360221528), test loss: 34.9731812477\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.82507681847,2.70164957175), test loss: 2.51878268868\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (16.3133831024,25.1279779381), test loss: 39.4538533688\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (3.06101918221,2.69339922205), test loss: 3.37995669842\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (3.08569192886,25.0203436573), test loss: 31.3949570656\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.01461315155,2.68528037034), test loss: 2.63922419548\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (15.5765485764,24.9132851548), test loss: 40.3743084192\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (3.47692918777,2.67729934282), test loss: 3.27707188725\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (21.543762207,24.8069769512), test loss: 27.8123583317\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.14579892159,2.66932051065), test loss: 2.40055256784\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (5.94000482559,24.7019534022), test loss: 38.9202337742\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (0.928705990314,2.66147184588), test loss: 3.17879170179\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (46.8526382446,24.5980697152), test loss: 34.1733775616\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.49778056145,2.65379314535), test loss: 3.61263850331\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (2.29312968254,24.4935902569), test loss: 38.5978531361\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.776800096035,2.64611298882), test loss: 3.27389098108\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (10.2696390152,24.3910561306), test loss: 31.2429855108\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.6814661026,2.63845222818), test loss: 3.24259861708\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (14.512298584,24.2893942154), test loss: 35.1014032364\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.30102539062,2.63078608813), test loss: 2.61375720203\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (11.39656353,24.1876638881), test loss: 37.1044897079\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.23182547092,2.62327990637), test loss: 3.38003512621\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (17.7919311523,24.0863910379), test loss: 32.7428254604\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.20401835442,2.6157601071), test loss: 2.39974220395\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (7.36807441711,23.986379184), test loss: 42.2692978859\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.45378017426,2.60842271482), test loss: 3.47867382616\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (1.63756465912,23.887529561), test loss: 30.7011788368\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.550401329994,2.60110970752), test loss: 2.66363722682\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (24.0088329315,23.7884309329), test loss: 40.2535716057\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.16957914829,2.59392084913), test loss: 3.18731796443\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (5.21978187561,23.6908750057), test loss: 26.5652989864\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.22872054577,2.58675166108), test loss: 2.4122802794\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.54085254669,23.5937366376), test loss: 37.6729465008\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.01106870174,2.57956473924), test loss: 3.20622315705\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (13.4295711517,23.4972335469), test loss: 30.6747303963\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.658398747444,2.57243427042), test loss: 3.1323277235\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (21.6696586609,23.4012653523), test loss: 35.51602211\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.15840113163,2.56542332192), test loss: 2.81038344502\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (11.4986476898,23.3054589692), test loss: 35.2377522469\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.5616158247,2.55842219078), test loss: 3.27884691358\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (9.64957809448,23.2108013282), test loss: 36.2262129784\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.77806973457,2.55150832447), test loss: 2.60563021898\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (22.2196331024,23.1173097124), test loss: 38.7578181028\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.71151256561,2.54472372369), test loss: 3.31384794116\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (6.36660289764,23.0229753619), test loss: 34.3163136482\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.78722858429,2.53798236368), test loss: 2.64422842562\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.22279644012,22.9304100267), test loss: 42.2023531914\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.31034874916,2.5312337299), test loss: 3.38493947089\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (7.14598464966,22.8385249716), test loss: 30.4951581001\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.34517359734,2.52444342227), test loss: 2.70724387467\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (11.766957283,22.7468517612), test loss: 38.2017601967\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.60184729099,2.51780011797), test loss: 3.00123740137\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (5.72660636902,22.6553865947), test loss: 32.7901251316\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.3311548233,2.51116085122), test loss: 3.08561856151\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (9.45359134674,22.5647855507), test loss: 39.4014358997\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.02809453011,2.50464094283), test loss: 3.21661301851\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (8.2443523407,22.4749593783), test loss: 31.5265129209\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.741480588913,2.49812289019), test loss: 3.09828161597\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (17.7201099396,22.3854034194), test loss: 35.4473324776\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.3438410759,2.49174406128), test loss: 2.77244459987\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (5.86930942535,22.296757645), test loss: 34.5160691977\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.31382954121,2.48538640797), test loss: 3.31834906936\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (5.48353481293,22.2086757394), test loss: 35.4902532578\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.47886836529,2.47897390758), test loss: 2.50375179648\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (3.88530445099,22.1211196242), test loss: 39.7315010071\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (2.29925608635,2.47266794437), test loss: 3.23679255843\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.82595348358,22.0338759434), test loss: 36.9390573025\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.33367109299,2.46641762007), test loss: 2.67737929523\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (3.37808895111,21.946981057), test loss: 41.8209561348\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.59446668625,2.46013449331), test loss: 3.22986707687\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (7.88294506073,21.8609415044), test loss: 30.1071099281\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.98239803314,2.45396772083), test loss: 2.64544865787\n",
      "\n",
      "MC # 4, Hype # hyp5, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold10/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (301.839416504,inf), test loss: 195.062639236\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (344.878051758,inf), test loss: 386.384529114\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (39.9197692871,80.3358878298), test loss: 43.3071467876\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.24847126007,78.2303938583), test loss: 3.26302135587\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (80.0769042969,64.325580194), test loss: 36.3237867355\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.47316884995,40.7463267184), test loss: 3.5504145354\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (33.5999298096,58.9221087058), test loss: 39.1861633301\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (6.97229814529,28.2588146494), test loss: 3.44976620674\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (39.4736328125,56.1212823715), test loss: 38.8817728996\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.87282180786,22.0071465891), test loss: 3.49865138829\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (35.8961715698,54.5012402645), test loss: 43.0146691322\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.33560180664,18.2635781824), test loss: 3.10140776634\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (53.3215103149,53.3270552734), test loss: 43.9714565277\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.5848596096,15.7684498271), test loss: 3.59264448881\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (112.51309967,52.4910734385), test loss: 43.1014222145\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (5.04578971863,13.9908422265), test loss: 3.15407469869\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (80.9800720215,51.8287231928), test loss: 42.6450557709\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (6.77728652954,12.6543017861), test loss: 3.27065578699\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (31.6207008362,51.2565463345), test loss: 39.248988533\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.49394798279,11.6148582587), test loss: 3.42262563109\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (122.315872192,50.7862984485), test loss: 42.4796250343\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (8.92908287048,10.7833330572), test loss: 3.0219291687\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (29.983379364,50.3932266085), test loss: 35.9829818487\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (7.2606139183,10.1026914742), test loss: 3.01761704981\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (47.6081733704,50.05011149), test loss: 40.0239178181\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.71294355392,9.53226585411), test loss: 3.40052585006\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (37.9005813599,49.7778909837), test loss: 34.572888279\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.26075649261,9.05152051353), test loss: 3.24500462413\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (35.491191864,49.5027626104), test loss: 40.302852726\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.29944849014,8.63929749138), test loss: 3.07060292661\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (102.46219635,49.2649816902), test loss: 39.6585801125\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.87503862381,8.28373222719), test loss: 3.26415332854\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (59.3262290955,49.0294160339), test loss: 42.092604208\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.27073669434,7.97035908178), test loss: 3.15685815811\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (28.8308696747,48.7890969421), test loss: 41.6849466801\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.38995313644,7.69367992804), test loss: 3.24575888813\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (121.422660828,48.5641042455), test loss: 38.3654198647\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (6.4112830162,7.44736688815), test loss: 3.33949030936\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (34.5375900269,48.3533500976), test loss: 41.5256862164\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (7.95985412598,7.22652769518), test loss: 3.26261008978\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (31.9301834106,48.1471431468), test loss: 36.2862794876\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.35518455505,7.02534177769), test loss: 3.09649358988\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (30.2218360901,47.965662663), test loss: 40.4216387749\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.55148088932,6.84417459217), test loss: 3.34282013774\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (32.4155693054,47.7682226435), test loss: 33.3907633305\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.74239253998,6.6792032202), test loss: 3.12991145849\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (42.0410079956,47.5811841101), test loss: 36.5474678993\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.09735870361,6.52944558309), test loss: 3.26925858855\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (26.4564380646,47.3846200235), test loss: 35.578873682\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.13127732277,6.39074274721), test loss: 3.30558650196\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (22.6565227509,47.1738545995), test loss: 38.8863865376\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.28150463104,6.26276389168), test loss: 2.95931476355\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (34.0309753418,46.9628018527), test loss: 39.8620589256\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.96564304829,6.14412861885), test loss: 3.30704953969\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (34.8871307373,46.7524044926), test loss: 38.9493576527\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.49411010742,6.03383649896), test loss: 3.0301741451\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (49.4435310364,46.5353752145), test loss: 38.185960269\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (5.89753627777,5.93009652995), test loss: 3.29326363206\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (19.2517356873,46.2804529825), test loss: 32.9715432644\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (3.89437389374,5.83611709481), test loss: 3.00997109711\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (75.6108322144,45.9811988962), test loss: 35.781912446\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.37829351425,5.74459807735), test loss: 3.02784158587\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (31.5003986359,45.6644944488), test loss: 29.1540650845\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.01442742348,5.65633670171), test loss: 2.75673935413\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (27.0446281433,45.3336793138), test loss: 32.4469343662\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.20624160767,5.57100823965), test loss: 2.98904951811\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (86.7908477783,44.9841850644), test loss: 27.0166291714\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.06878209114,5.48931750885), test loss: 3.02789314985\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (19.0739173889,44.6177725951), test loss: 31.2823395014\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.691946864128,5.41108588189), test loss: 2.73381789029\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (19.6351089478,44.2501999759), test loss: 30.7181185246\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.81458830833,5.33647796926), test loss: 3.04504254758\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (27.5109367371,43.8747237742), test loss: 32.1420861721\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (4.68350982666,5.26393220964), test loss: 2.7160623014\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (21.2570838928,43.4958466521), test loss: 32.6997056484\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.41191411018,5.19500235483), test loss: 3.23750290275\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (64.404586792,43.1049890693), test loss: 28.9247941971\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.83637595177,5.12867969469), test loss: 2.82373096049\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (13.7901115417,42.7109651714), test loss: 30.6565262794\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.70947730541,5.06574947494), test loss: 3.06786983311\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (27.6251544952,42.3154546253), test loss: 27.6341458797\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.62923765182,5.00488980888), test loss: 2.96251101494\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (72.5671386719,41.9144761979), test loss: 28.6341342926\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.21250772476,4.94607502647), test loss: 2.83865120411\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (6.68687057495,41.5111512078), test loss: 25.5651651859\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.42222189903,4.88929621318), test loss: 3.03826141655\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (16.4212760925,41.1204857977), test loss: 27.7196802378\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.39890098572,4.83486908205), test loss: 3.00512778163\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (21.0284824371,40.7367367791), test loss: 25.6401520252\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.65549850464,4.781354037), test loss: 3.17958595306\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (31.6515426636,40.3616507422), test loss: 28.8951922417\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.62584495544,4.73037968127), test loss: 2.68260421753\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (22.923948288,39.9889930103), test loss: 28.6774994373\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (5.03903055191,4.68111618063), test loss: 3.24392648637\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (12.4505615234,39.6266377839), test loss: 30.6448601723\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.54746055603,4.63397628483), test loss: 2.70664509833\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (21.040348053,39.2733138898), test loss: 29.0812927246\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.92155957222,4.58850317179), test loss: 3.02349970937\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (58.2440643311,38.9260687499), test loss: 28.910670805\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (5.15252733231,4.54444139925), test loss: 2.82114850283\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (74.0826950073,38.5836913388), test loss: 29.6271330357\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.42505836487,4.50158644585), test loss: 2.77246163487\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (11.1570930481,38.2506636366), test loss: 25.6275228024\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.64382076263,4.46039585508), test loss: 2.65737097859\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (17.3388710022,37.9296297096), test loss: 28.5009984493\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.11583089828,4.41969489809), test loss: 2.98246190548\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (23.9658355713,37.6168392585), test loss: 23.4069235325\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.26087975502,4.38065256183), test loss: 3.09219451398\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (8.3405752182,37.3075452952), test loss: 27.6167788744\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.22781085968,4.34278755499), test loss: 2.82913090587\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (10.4659891129,37.0088912947), test loss: 27.0820851088\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.956865429878,4.30638375926), test loss: 3.10857343674\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (14.0467214584,36.719215943), test loss: 30.1720872402\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.22934269905,4.27123853009), test loss: 2.73745059073\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (50.6011772156,36.4368525236), test loss: 31.492703867\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (4.61818981171,4.23701419244), test loss: 3.07782261372\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (62.049030304,36.1571303785), test loss: 29.9395067453\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.13297224045,4.20343544214), test loss: 2.82008548826\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (7.30512809753,35.887239745), test loss: 28.8114627838\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.54259872437,4.17103080778), test loss: 2.9756502986\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (11.8406085968,35.6273546877), test loss: 27.4408488512\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.24291563034,4.13882784005), test loss: 2.75625587106\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (6.61898374557,35.3736395852), test loss: 28.0165116906\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.420730412006,4.10789178026), test loss: 2.94292005301\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (8.16038131714,35.121822014), test loss: 27.528041029\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (4.09962368011,4.07778627385), test loss: 3.06244327426\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.5100498199,34.8779229703), test loss: 27.8671864271\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.559918046,4.04875734575), test loss: 3.01222652495\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.4416656494,34.6399394511), test loss: 25.5385179877\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.95971310139,4.02062070401), test loss: 3.16258363426\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (53.5026931763,34.4066325487), test loss: 30.5184565544\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.76227378845,3.9931388956), test loss: 2.62530984581\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (24.1498031616,34.1734971668), test loss: 29.6301057935\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.50433278084,3.96602704938), test loss: 3.1627347663\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (7.10778665543,33.9473329831), test loss: 31.0430294991\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.36034345627,3.93973619606), test loss: 2.67393503487\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (17.7326431274,33.7289590761), test loss: 28.5696121931\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.59499454498,3.91357883221), test loss: 2.98701028526\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (6.39165115356,33.5146624109), test loss: 28.5831371307\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (0.32220095396,3.88828532387), test loss: 2.76448766291\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (12.010225296,33.3018992818), test loss: 29.0725571632\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.3974647522,3.86366255779), test loss: 2.86773506403\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (21.7885437012,33.0959153925), test loss: 26.4816079617\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.60816240311,3.83983787621), test loss: 2.59913911521\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.7216300964,32.8945646319), test loss: 29.5699415088\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.993151664734,3.81657743212), test loss: 2.95835770369\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (19.0517807007,32.6974467131), test loss: 25.9152736664\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.09868896008,3.79382622869), test loss: 3.11698430926\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (17.1382312775,32.5001536978), test loss: 30.2878123522\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.71678602695,3.77136390852), test loss: 2.96638081074\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (10.0769100189,32.3084752937), test loss: 27.3804186583\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.513627767563,3.74952530361), test loss: 3.05538028479\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (11.0197811127,32.1225152238), test loss: 30.4882535934\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (4.09703350067,3.72779032736), test loss: 2.68509974182\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (13.4047002792,31.9389261941), test loss: 29.5339113474\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.384954959154,3.70663057019), test loss: 3.09861334562\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (26.0481586456,31.7551488662), test loss: 28.8520359516\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.89789879322,3.6859760055), test loss: 2.68796903789\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (27.7947311401,31.5755885702), test loss: 30.4656108856\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.84388637543,3.66586648308), test loss: 2.94798961282\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (9.69472694397,31.3994071591), test loss: 28.4089226723\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.66329407692,3.64616833897), test loss: 2.7666792497\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (22.9652023315,31.2266488221), test loss: 32.2917516232\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.542911171913,3.62684724411), test loss: 2.93824720681\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (16.9187812805,31.0531245819), test loss: 30.6289643288\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.78222227097,3.60776394338), test loss: 3.03663221002\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (11.8133487701,30.8846438008), test loss: 29.1102018148\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.2405154705,3.5891674013), test loss: 2.99903208017\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (7.52270317078,30.7210862874), test loss: 25.3649911046\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.74097323418,3.57054456559), test loss: 3.12624413073\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (13.3319797516,30.5599597393), test loss: 29.4164320707\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.73935687542,3.55240732919), test loss: 2.58950438499\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (40.7909965515,30.3994747235), test loss: 29.2596667051\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.17120242119,3.53462855237), test loss: 3.11241171956\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (18.1423339844,30.2417326758), test loss: 33.1845232487\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.43626832962,3.51729030865), test loss: 2.64884116948\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (7.60696220398,30.0870493053), test loss: 28.23795439\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.779779553413,3.50032418863), test loss: 2.90161583871\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (18.0025901794,29.9352576172), test loss: 33.3262766838\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.607127070427,3.48367690922), test loss: 2.83929552436\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (13.3689603806,29.7819591368), test loss: 32.6274156392\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (3.31299638748,3.46724122298), test loss: 2.87485202551\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (18.6808433533,29.6321573139), test loss: 26.6040860653\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.08829307556,3.45110323175), test loss: 2.6002728194\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (3.54979681969,29.4851320349), test loss: 30.5719043732\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.18598806858,3.43489508288), test loss: 2.90837571621\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (11.8774452209,29.3398910631), test loss: 24.0063149929\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.94143223763,3.41905978191), test loss: 3.05432598889\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (30.5601272583,29.1945096325), test loss: 29.6855580211\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.36395359039,3.40347672326), test loss: 3.00339682996\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.10338401794,29.0519221776), test loss: 28.8610334158\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.10878574848,3.3882648717), test loss: 2.96212592274\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (5.51706886292,28.9118550981), test loss: 32.3061047077\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.834893882275,3.37337110346), test loss: 2.68630730808\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (9.1694984436,28.7746278272), test loss: 34.3261533976\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.5847427845,3.35874410473), test loss: 3.19455626607\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (12.7288475037,28.6368196659), test loss: 30.8210307598\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.93542027473,3.34428122036), test loss: 2.696781075\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (18.6684436798,28.5026176696), test loss: 31.367363286\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.05215787888,3.33004098952), test loss: 3.01166149378\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (5.49900007248,28.3707768427), test loss: 28.4292173147\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.25218236446,3.31573876463), test loss: 2.72025538236\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (14.6910905838,28.24054241), test loss: 31.2871654987\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.68187856674,3.301775107), test loss: 2.88408459723\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (20.5904254913,28.1099583018), test loss: 28.358033371\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.5383207798,3.28805316974), test loss: 2.98621309698\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (7.85220623016,27.9810287861), test loss: 32.0398066759\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.22977161407,3.27458843878), test loss: 2.9697465688\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (5.73780632019,27.8537138701), test loss: 26.6919167042\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.708151102066,3.2614032517), test loss: 3.0893873632\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (7.2906908989,27.7281241694), test loss: 33.1612396002\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.701376080513,3.24841729891), test loss: 2.70235702395\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (13.9680728912,27.6015101643), test loss: 30.5297167063\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.78169691563,3.23551238436), test loss: 3.0990480721\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (11.6263360977,27.4776466165), test loss: 32.1886258125\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.949135541916,3.22275090813), test loss: 2.62554911226\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (6.55195045471,27.3559272133), test loss: 29.8807923973\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.44044327736,3.20994764636), test loss: 2.91386064887\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (17.151424408,27.2358375907), test loss: 31.0106593609\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (4.53644180298,3.19751237858), test loss: 2.78789984584\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (8.86850547791,27.1156168188), test loss: 32.335328871\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.9586057663,3.18516780938), test loss: 2.93204233348\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (5.02541065216,26.9975872645), test loss: 28.7629581928\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.71061193943,3.17308982039), test loss: 2.66043278873\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.04860687256,26.8814821858), test loss: 32.2069829464\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.54272270203,3.16122888551), test loss: 2.90773654282\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (19.7294788361,26.7672232016), test loss: 26.2607226372\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.51397037506,3.14956103025), test loss: 3.06366935372\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (15.5927562714,26.6517471571), test loss: 31.6417134523\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.62115085125,3.13795247428), test loss: 3.0042344451\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (5.49484920502,26.5388308889), test loss: 26.7724427283\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.1421816349,3.12652308183), test loss: 2.95653757304\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (3.20501613617,26.4272687223), test loss: 32.2727617264\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.717732906342,3.11503022193), test loss: 2.65347639322\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (18.348903656,26.3165469847), test loss: 32.363949573\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (5.88617372513,3.10389054337), test loss: 3.12661619037\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (13.9368095398,26.2048289734), test loss: 30.7803514481\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (4.96793889999,3.09276533126), test loss: 2.7086003989\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (3.82601094246,26.0944659461), test loss: 33.557259655\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.44705677032,3.08176197031), test loss: 3.04174537659\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (10.4331483841,25.9857299187), test loss: 29.2155842304\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.36079883575,3.07096245054), test loss: 2.70401613414\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (25.549331665,25.8787566053), test loss: 33.2306762695\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.35345983505,3.06034808098), test loss: 2.83636953235\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (9.83908653259,25.770727534), test loss: 29.384314847\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.31682884693,3.04978918291), test loss: 3.03506784439\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (13.1296663284,25.6656327712), test loss: 33.3025992274\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.06994819641,3.03939821059), test loss: 3.01463132799\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (17.3409919739,25.5621907724), test loss: 26.7448373079\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (3.01976633072,3.02896434924), test loss: 3.07026112378\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (14.5239830017,25.4596711879), test loss: 31.1061460316\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (5.2338051796,3.01876949933), test loss: 2.61864888668\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (18.2086048126,25.3567823515), test loss: 31.1557312727\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (4.96179199219,3.00861911419), test loss: 3.12451409996\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (3.46739912033,25.2547927259), test loss: 34.9909916878\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.09218287468,2.99858018045), test loss: 2.61268532276\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (13.3786582947,25.1543238782), test loss: 31.8493782759\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.22760105133,2.98875716567), test loss: 2.97261424363\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (29.2452659607,25.0551464304), test loss: 31.2308611393\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.24616622925,2.9791045251), test loss: 2.75216644406\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.18155050278,24.9543889431), test loss: 34.584481135\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.98157978058,2.96950548716), test loss: 2.98137979209\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (12.2388830185,24.855548476), test loss: 28.071626997\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.957277476788,2.95999117924), test loss: 2.66235325336\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (19.7419967651,24.7577042646), test loss: 34.1172347307\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (5.06521606445,2.95044438032), test loss: 2.88517014831\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (11.4880065918,24.660374111), test loss: 26.0416395426\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.97629654408,2.94102107113), test loss: 2.98684159517\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (14.1561832428,24.5628414805), test loss: 32.9065873861\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.57215452194,2.9316645022), test loss: 3.06802033484\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (6.08845090866,24.4665021574), test loss: 29.3208189249\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.849908232689,2.92243134444), test loss: 2.9538344413\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (6.93304347992,24.3715838503), test loss: 33.8557351589\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.616859793663,2.91337841089), test loss: 2.66158552468\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (14.8097362518,24.2784365197), test loss: 33.2738578498\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.831597745419,2.90449224002), test loss: 3.13421934992\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (13.3690261841,24.1844266714), test loss: 32.4854422569\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.54359769821,2.89564871081), test loss: 2.75404304862\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (2.74423146248,24.092293541), test loss: 34.6606489897\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.997057378292,2.88685719149), test loss: 3.13690439165\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (21.9868164062,24.0010496051), test loss: 30.4754888535\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (5.47124052048,2.87805532737), test loss: 2.73686841279\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (9.00738620758,23.9101719873), test loss: 33.6274794579\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.488062679768,2.86937980468), test loss: 2.81367947757\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (11.1950740814,23.818811115), test loss: 29.9170300484\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.72703826427,2.86078018307), test loss: 3.06823587418\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (7.35112571716,23.7279605562), test loss: 36.5085303545\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.967115163803,2.85226163079), test loss: 3.04514368176\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (9.26620101929,23.637954955), test loss: 26.6102614403\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.988660335541,2.84390790522), test loss: 2.95596193224\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (9.80701065063,23.5489917528), test loss: 32.2845662832\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.931524991989,2.83565861939), test loss: 2.69026666284\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (12.3522005081,23.4591312541), test loss: 32.2091359615\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.09228014946,2.82742382327), test loss: 3.10847438276\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (3.78534460068,23.3709710603), test loss: 33.3809285641\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.841473340988,2.81922563634), test loss: 2.62941848785\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (5.58486557007,23.28381048), test loss: 35.1819199562\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (4.31584787369,2.81107005786), test loss: 3.06162336469\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (5.15802717209,23.197177874), test loss: 32.6346330643\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.6109547019,2.80295898664), test loss: 2.73303561658\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (10.9997634888,23.1105897505), test loss: 36.0822101355\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.46648836136,2.79495180393), test loss: 3.04890637994\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (7.12035226822,23.0247716448), test loss: 32.3755598783\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.508222460747,2.78698704216), test loss: 2.74462441206\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (10.5050907135,22.9400476608), test loss: 34.5586325467\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.969241976738,2.77917034197), test loss: 2.82250688225\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.61536741257,22.8561043544), test loss: 27.4218906164\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.56295388937,2.7714547352), test loss: 3.06177670062\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (2.100887537,22.771420746), test loss: 34.9107818604\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.24591550231,2.763776099), test loss: 3.08974734098\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (6.71489715576,22.688148617), test loss: 27.600980854\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.211339741945,2.75613379637), test loss: 2.96073984802\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (7.43561840057,22.605134898), test loss: 36.1215054035\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (4.14687204361,2.748522019), test loss: 2.76536034346\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.61429381371,22.5223064835), test loss: 34.4217667937\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.15442168713,2.7409473082), test loss: 3.1432412535\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (11.6595897675,22.4392337289), test loss: 33.5428905964\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.87926018238,2.73343186908), test loss: 2.86041689515\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (9.86493492126,22.3566594801), test loss: 36.0221594572\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.695313870907,2.72593385647), test loss: 3.13565283418\n",
      "run time for single CV loop: 7098.27013803\n",
      "\n",
      "MC # 4, Hype # hyp4, Fold # 6\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold6/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (364.713348389,inf), test loss: 198.877184296\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (340.892028809,inf), test loss: 408.759342957\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (13.0125350952,158.96480242), test loss: 64.5400511503\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.27962446213,231.866523545), test loss: 3.9947010994\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (29.3990917206,105.365163656), test loss: 36.8635989189\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.05447912216,117.421887565), test loss: 2.5578196615\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (107.896850586,86.0899724771), test loss: 45.3711199284\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.94293284416,79.2573119733), test loss: 3.41477259994\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (31.7517929077,76.5074981776), test loss: 39.6963498592\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.82889199257,60.1837523803), test loss: 3.29852715731\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (7.87282180786,70.6976733476), test loss: 43.772677803\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.45618677139,48.7410915573), test loss: 3.2238771081\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (40.2676544189,66.7982156322), test loss: 41.0968563557\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.6058191061,41.1114475658), test loss: 3.41951139867\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (46.0375442505,63.968442113), test loss: 41.7778869152\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.76937568188,35.6670619861), test loss: 2.5610607475\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (27.706823349,61.7675821195), test loss: 44.6894115925\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.1025185585,31.5822484062), test loss: 3.44692257643\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (21.7810287476,60.0581218112), test loss: 42.7222695351\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.11754322052,28.4024394667), test loss: 2.53352171183\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (19.8303546906,58.7432709695), test loss: 44.4295553207\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.34434664249,25.8639396514), test loss: 3.64037238359\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (24.7089996338,57.6254762232), test loss: 41.5984004974\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.74454164505,23.7868240216), test loss: 2.55721598566\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (36.4864234924,56.6751254634), test loss: 43.8334414005\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.80732935667,22.0553913571), test loss: 3.53729455471\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (42.772857666,55.8461461479), test loss: 38.9260307789\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.72998285294,20.5915399357), test loss: 2.71326439381\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (36.1995124817,55.1032911112), test loss: 44.9102646828\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.12488269806,19.3359452638), test loss: 3.44498336017\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (60.5177764893,54.4575240954), test loss: 37.3005005836\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (5.586186409,18.2454512701), test loss: 2.64788498282\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (43.5943374634,53.9202232342), test loss: 44.9220806122\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (7.02323722839,17.2940246802), test loss: 3.26963707805\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (24.6031608582,53.4216110503), test loss: 36.1266595364\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.75110530853,16.4537517255), test loss: 2.55664069057\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (60.0580596924,52.9594882687), test loss: 43.9964875221\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.852265000343,15.7057393675), test loss: 3.19325577021\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (70.3009872437,52.5299312482), test loss: 38.9183369637\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.51701843739,15.0365512637), test loss: 3.24923427701\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (31.06665802,52.119782884), test loss: 41.8338125467\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (7.45027160645,14.43377621), test loss: 3.25927399993\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (28.6746940613,51.740328826), test loss: 38.5551659107\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.18457508087,13.8862167995), test loss: 3.2392447412\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (104.665603638,51.4157698766), test loss: 39.0302360535\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.66790771484,13.3903881161), test loss: 2.61480004191\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (101.33581543,51.0990594948), test loss: 41.3316080093\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (4.24071931839,12.9379858351), test loss: 3.25285133123\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (16.1576213837,50.783940192), test loss: 40.4981425762\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.998184680939,12.5218394665), test loss: 2.44383834898\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (147.215515137,50.4808420051), test loss: 43.4940336227\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.62230491638,12.1390632101), test loss: 3.56491908133\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (35.321937561,50.1869887299), test loss: 40.4682878971\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (4.31062030792,11.7850042283), test loss: 2.44988158047\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (25.096786499,49.9000426609), test loss: 42.7438304901\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.49251389503,11.4564308329), test loss: 3.54216375947\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (29.2645244598,49.6369057091), test loss: 37.3663748264\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.73164153099,11.1515068957), test loss: 2.56178249121\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (20.3227539062,49.3822658168), test loss: 41.9767064095\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.08989441395,10.8682094809), test loss: 3.36134953499\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (16.784488678,49.1248667614), test loss: 35.1812632561\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.0183403492,10.6024098427), test loss: 2.65680539012\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (26.6999168396,48.8553039494), test loss: 43.5183390141\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.17097592354,10.3534368797), test loss: 3.18782401681\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (73.3296432495,48.6034801807), test loss: 34.3503184319\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.07634925842,10.119140742), test loss: 2.51114251018\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (13.5586833954,48.341049259), test loss: 41.2059604645\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.2596154213,9.89793126692), test loss: 3.26351879239\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (12.7472400665,48.0928417983), test loss: 36.712528038\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (3.26052451134,9.68964022216), test loss: 2.98550810814\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (57.2117233276,47.8459867861), test loss: 38.9618456364\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (5.09541893005,9.49361995405), test loss: 3.15887212753\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (26.7518920898,47.5890800342), test loss: 34.4012709618\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.46639800072,9.3071277089), test loss: 3.1670219928\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (103.13495636,47.3182414034), test loss: 40.0997703075\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.41208457947,9.13017191814), test loss: 2.85518009067\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (53.3114509583,47.0464124598), test loss: 35.4698108912\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.89617192745,8.9632443322), test loss: 3.19191223979\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (33.3148498535,46.75822127), test loss: 34.4871711254\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.51006698608,8.80605812098), test loss: 2.37886719406\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (29.8052864075,46.4697898056), test loss: 38.6690700054\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.18103694916,8.65680874141), test loss: 3.43781149387\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (51.6884918213,46.1748327645), test loss: 34.5560239792\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (4.99161815643,8.51511905174), test loss: 2.33756935894\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (51.5283546448,45.8714161452), test loss: 36.7279535532\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.46433949471,8.37887544576), test loss: 3.3708704412\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (70.5441055298,45.5487596482), test loss: 29.0281344891\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.32802414894,8.24794709468), test loss: 2.38091644645\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (128.078613281,45.2349548259), test loss: 36.0092864513\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (5.44926929474,8.12218730336), test loss: 3.23471164703\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (31.5310249329,44.9124076043), test loss: 29.9832039833\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (5.66519927979,8.00108018257), test loss: 2.47805006206\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (12.5015640259,44.587835715), test loss: 35.9779690266\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.7779635191,7.88466019108), test loss: 3.24997802377\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (34.1014518738,44.258606683), test loss: 29.0830365419\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (3.18518304825,7.77293467087), test loss: 2.51571156085\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (29.050567627,43.9223114561), test loss: 34.6142635345\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.35580325127,7.66477326688), test loss: 3.01443555355\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (23.4188690186,43.5702471452), test loss: 25.9424796581\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.77431321144,7.55988926536), test loss: 2.64878509939\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (29.961555481,43.2249240232), test loss: 34.1555206299\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.33854198456,7.45850078173), test loss: 3.15144134164\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (12.4680871964,42.8781149565), test loss: 28.203654623\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.667669773102,7.36027197817), test loss: 3.31058858037\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (8.98991107941,42.5270703008), test loss: 32.5788673878\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.28623735905,7.26573888991), test loss: 3.12972127795\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (12.758058548,42.1752788577), test loss: 28.579536581\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.47676992416,7.17452980943), test loss: 3.24606180191\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (72.5134735107,41.8221079838), test loss: 28.5058259487\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.99983739853,7.08600756547), test loss: 2.38638173491\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (22.1579189301,41.4644776145), test loss: 32.2403894424\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.32656908035,6.99963466208), test loss: 3.36820480824\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (31.9167881012,41.1169201518), test loss: 29.8153261185\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (6.70311737061,6.9159188509), test loss: 2.35871968269\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (8.16976165771,40.7761824201), test loss: 31.5191901207\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.80430078506,6.8347440559), test loss: 3.42059357762\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (10.1504583359,40.4380554031), test loss: 28.1604730129\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (0.775037169456,6.75614500137), test loss: 2.23906735182\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (12.0528240204,40.1068376674), test loss: 31.1971467495\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.77424049377,6.68032196975), test loss: 3.24651205838\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (42.7292442322,39.7799516919), test loss: 29.9592569828\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (5.5552444458,6.60674903518), test loss: 2.560600245\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (26.728729248,39.4567789493), test loss: 30.8685512543\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.32641410828,6.53500720131), test loss: 3.22535071671\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (8.00263786316,39.1452609212), test loss: 27.9757681847\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.88396203518,6.46532010064), test loss: 2.61975788474\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (9.81544303894,38.8425856534), test loss: 34.1421854138\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.54975914955,6.39798775267), test loss: 3.17146114111\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (6.75740337372,38.5434804868), test loss: 26.774363637\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.18996322155,6.33262560238), test loss: 2.51192271411\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (5.74624681473,38.2538992574), test loss: 34.1793858051\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.0125617981,6.26952742652), test loss: 3.37009982765\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (14.752325058,37.9684217974), test loss: 30.0998312473\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.97018551826,6.2081185521), test loss: 3.42702884078\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (21.1361808777,37.689248086), test loss: 34.1508330345\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.87869262695,6.14834281422), test loss: 3.40472767949\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (14.1753940582,37.4206019609), test loss: 29.0019838095\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.97455072403,6.09004447919), test loss: 3.36234351695\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (22.052936554,37.1591743444), test loss: 29.6509761333\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.44466722012,6.03362782629), test loss: 2.61179694235\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (13.8321828842,36.9012630034), test loss: 31.8586693287\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.44355297089,5.97867546018), test loss: 3.46372028589\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.9742031097,36.6517932009), test loss: 31.2997887135\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.47376561165,5.92555823893), test loss: 2.50817595273\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (15.7981262207,36.4059047899), test loss: 31.3253203392\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.86763215065,5.87367443255), test loss: 3.50919144154\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (5.51806545258,36.1653675535), test loss: 31.8419842243\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.35411250591,5.82302410675), test loss: 2.46652725339\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (8.91426086426,35.9340017583), test loss: 31.1284968853\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.27438735962,5.77348072573), test loss: 3.38202698529\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (38.2068977356,35.7080010706), test loss: 29.9988839149\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.95066928864,5.72548983914), test loss: 2.60476082563\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (25.121465683,35.484999395), test loss: 32.7486664772\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.63133955002,5.6785830178), test loss: 3.35209031403\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (9.86983108521,35.2688949346), test loss: 29.7910820484\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.62120246887,5.63311438432), test loss: 2.6658390522\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (12.2824172974,35.0552496343), test loss: 34.1725940704\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.82749223709,5.58867269896), test loss: 3.10280398726\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (10.8693666458,34.8464760931), test loss: 28.524480772\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.36329102516,5.54516580142), test loss: 2.56940845549\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.4504518509,34.6454525673), test loss: 33.9435741425\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.94595587254,5.50247006871), test loss: 3.22977742851\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (11.6211738586,34.448096734), test loss: 31.3466773987\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.32411384583,5.46105565148), test loss: 3.38082957268\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (19.4623718262,34.2535990536), test loss: 34.2957941532\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.777426421642,5.42051909353), test loss: 3.39497413635\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (14.0371627808,34.0652526785), test loss: 29.4952573776\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.87900280952,5.38110637342), test loss: 3.39277223647\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (22.5493011475,33.8786180145), test loss: 29.2721704006\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.69667339325,5.34257025945), test loss: 2.61957594752\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (13.1629638672,33.6956344427), test loss: 29.8025148392\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.53521823883,5.30477436102), test loss: 3.37982005179\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (37.3939628601,33.5192962737), test loss: 31.7413630962\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.43734908104,5.26759702376), test loss: 2.44244253784\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (25.294380188,33.3459284437), test loss: 31.9134129524\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.00625443459,5.23145988931), test loss: 3.47954467535\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (7.60737085342,33.1752705315), test loss: 31.5651170492\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.38121509552,5.19606856451), test loss: 2.3260965839\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (14.4761819839,33.0095633892), test loss: 31.48054142\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.781963944435,5.16156738416), test loss: 3.35545455217\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (7.87479877472,32.8444491245), test loss: 30.4306473255\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.960133492947,5.12778601176), test loss: 2.51891461015\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (8.7265625,32.6830645942), test loss: 32.2697402954\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.12857365608,5.09460482267), test loss: 3.37910287678\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (9.58880233765,32.5264358011), test loss: 28.8045471668\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.94949269295,5.06186259413), test loss: 2.6109489888\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (30.1501026154,32.3729536557), test loss: 34.45314188\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.69191408157,5.03004602532), test loss: 3.33880268931\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (32.6898040771,32.2218234899), test loss: 29.2602365971\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.19155597687,4.99881283896), test loss: 2.62902177572\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (15.706949234,32.0743124626), test loss: 33.6418604851\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.740258455276,4.96832748904), test loss: 3.2064549014\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (11.5095825195,31.9269474098), test loss: 32.8164557457\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.17023682594,4.93835896772), test loss: 3.32971086502\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (9.29193687439,31.7833505059), test loss: 33.9671296358\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.7854783535,4.90891852352), test loss: 3.33797446489\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (5.8036699295,31.6433241834), test loss: 30.061435771\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.79761648178,4.87984921103), test loss: 3.37052448988\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (26.2115974426,31.5057632494), test loss: 35.4188031197\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (5.64554023743,4.85158284066), test loss: 3.23548647165\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (12.9216938019,31.3703178545), test loss: 29.7667096972\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.63095414639,4.8238138416), test loss: 3.34728799462\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (26.991394043,31.2383472741), test loss: 31.1484039545\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.17214882374,4.79668502112), test loss: 2.44259747565\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (14.1329870224,31.1060738815), test loss: 31.6577591181\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (3.68466663361,4.76993615924), test loss: 3.48805697858\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (7.79498624802,30.9769594953), test loss: 31.4276927948\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.16259121895,4.74354881106), test loss: 2.28767506778\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (29.3642539978,30.8512506646), test loss: 33.9191622734\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.890084803104,4.71759135811), test loss: 3.36109429002\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (22.5401725769,30.7269288929), test loss: 28.4269234419\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.66503620148,4.6922049132), test loss: 2.27099136114\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (32.4374656677,30.6050916475), test loss: 31.9031576633\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.59974324703,4.66739548158), test loss: 3.23262338638\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (11.4963159561,30.4857033777), test loss: 30.9300508022\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (0.929543197155,4.64302331691), test loss: 2.60882740617\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (56.1180648804,30.3662414354), test loss: 33.0822418451\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.26837277412,4.61893798692), test loss: 3.27715116143\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (4.88930130005,30.2496517238), test loss: 29.323538065\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (3.23485469818,4.59519628646), test loss: 2.65423190594\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (10.7493829727,30.1352698669), test loss: 34.1546321869\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.477579623461,4.57183147156), test loss: 3.1049433887\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (13.0628499985,30.0224055635), test loss: 28.1040799141\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.07275390625,4.54889206769), test loss: 2.64199527204\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (13.5840816498,29.9118747696), test loss: 34.6749938488\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.10266268253,4.52649712789), test loss: 3.28853557706\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (5.62746524811,29.8036974707), test loss: 29.7878577232\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.863671958447,4.50446694798), test loss: 3.46253929734\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.40818452835,29.6942430635), test loss: 35.1257153988\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.16416025162,4.48267530176), test loss: 3.36224061847\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (20.5856838226,29.588989814), test loss: 28.9891483307\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.08146762848,4.46112955006), test loss: 3.35344451666\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (11.4754047394,29.4844522464), test loss: 30.7540216446\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.32079565525,4.43999246944), test loss: 2.49349489957\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (14.3557634354,29.3816682342), test loss: 32.3294650078\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.70926511288,4.41911991483), test loss: 3.35593377948\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (5.66275405884,29.2807922992), test loss: 32.0561524391\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.41861844063,4.39884784467), test loss: 2.45938790888\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (27.5198249817,29.1821314542), test loss: 31.4794069767\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.558842659,4.37883954225), test loss: 3.38209325671\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (29.2924880981,29.081664841), test loss: 30.4184393406\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.47315073013,4.35896342148), test loss: 2.27371971682\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.5909194946,28.9854669382), test loss: 31.4211259365\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.834337353706,4.33937225328), test loss: 3.19316087663\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (12.9427747726,28.8900340381), test loss: 30.6328529358\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.66733515263,4.3201274373), test loss: 2.50744617581\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (34.3031806946,28.7956004056), test loss: 31.0135664701\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.97785127163,4.30105855504), test loss: 3.1141554147\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (23.9615516663,28.7032226578), test loss: 29.245876646\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (3.03894758224,4.28255922905), test loss: 2.61344027817\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (11.2610874176,28.6121717002), test loss: 35.5018813133\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.22097420692,4.26425252498), test loss: 3.05293703526\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (33.7250823975,28.5198609411), test loss: 28.1506211758\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (4.24742794037,4.24611453525), test loss: 2.50328184366\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (39.1837921143,28.4313395681), test loss: 34.1366244674\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.4825193882,4.22817777135), test loss: 3.25520391017\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (19.9105987549,28.3435778431), test loss: 30.4359771729\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (4.52616214752,4.21051413123), test loss: 3.31661769152\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (13.0120220184,28.256119783), test loss: 34.8746803522\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.56046581268,4.19304528892), test loss: 3.27746851444\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (26.4451980591,28.1710586596), test loss: 28.9917018175\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.36282777786,4.17607553701), test loss: 3.24255747497\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (40.7212677002,28.0872111903), test loss: 31.6634056807\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.34655165672,4.15929539156), test loss: 2.67675822079\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (8.73014354706,28.0015814958), test loss: 31.3363343239\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (2.03002429008,4.14256300022), test loss: 3.35885227323\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (13.2661399841,27.9198420464), test loss: 32.0999694347\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.64917850494,4.12612360731), test loss: 2.46780472398\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (3.64692831039,27.8390595), test loss: 31.6645729184\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.400305151939,4.10984979406), test loss: 3.33833120167\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (7.34134864807,27.758178453), test loss: 32.5198407173\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.97778630257,4.09381540699), test loss: 2.34727820754\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (18.0083503723,27.6792865844), test loss: 31.3370973349\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.20723128319,4.07817014461), test loss: 3.23143692613\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (57.6675796509,27.6013715468), test loss: 32.33726511\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.76747608185,4.0627213404), test loss: 2.55018991232\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (25.393081665,27.5226347005), test loss: 31.1968064308\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.936686933041,4.04730829727), test loss: 3.20032336414\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (25.0963592529,27.4467622163), test loss: 28.8888958931\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (5.52494668961,4.03210868584), test loss: 2.51947237253\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (16.3228874207,27.3720985944), test loss: 33.6264219522\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.8978985548,4.01713677753), test loss: 2.95290839076\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (5.00496864319,27.2969314975), test loss: 28.2456804752\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.794078588486,4.0022945476), test loss: 2.4980876565\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (4.56823253632,27.223347771), test loss: 33.2976216793\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.28781509399,3.98782180444), test loss: 3.0401139617\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (48.0784835815,27.1503770125), test loss: 31.5212601662\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (3.48074150085,3.97349774931), test loss: 3.14848226011\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (22.8966178894,27.0774547867), test loss: 33.4024647713\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.29662978649,3.95926855438), test loss: 3.22128800452\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.76721096039,27.0068336042), test loss: 29.0225500584\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.16783106327,3.94513005523), test loss: 3.15658876896\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (9.90664196014,26.9374227165), test loss: 29.4056799412\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.21096479893,3.93130267059), test loss: 2.52452435493\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (13.7513484955,26.8671325403), test loss: 30.2537076473\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.50583004951,3.91755429534), test loss: 3.20658080578\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (10.2983322144,26.798774206), test loss: 31.5576172829\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.871119201183,3.90411302414), test loss: 2.37252741605\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (8.8717508316,26.7300305697), test loss: 30.7882987499\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.669054925442,3.89079361149), test loss: 3.33795491159\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (20.138917923,26.662379815), test loss: 31.626061058\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.83555793762,3.8776215495), test loss: 2.25998312235\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (13.3861284256,26.5966691758), test loss: 31.8181948185\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.78363752365,3.86450105895), test loss: 3.2191711992\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (13.4181003571,26.5316262975), test loss: 29.0955124855\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.16821503639,3.85166051525), test loss: 2.34475740641\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (14.0879764557,26.4659367194), test loss: 32.5710175037\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.83377122879,3.83887061099), test loss: 3.1657210052\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (15.7902488708,26.4020010758), test loss: 29.243730545\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.71723484993,3.82638381468), test loss: 2.50291462988\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (13.0705356598,26.3376762084), test loss: 33.7331882477\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.09072327614,3.81400228765), test loss: 3.13557318002\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (18.0026168823,26.2742229214), test loss: 29.2292360783\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.47072184086,3.80171362762), test loss: 2.55872431993\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (11.5315608978,26.2127262665), test loss: 33.0769227624\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.961476504803,3.78947956269), test loss: 2.97388618365\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (31.5573768616,26.1516183211), test loss: 31.9563959599\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.27904653549,3.77752366204), test loss: 3.07540149391\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (16.7943115234,26.0898943556), test loss: 34.7219694257\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.95927214622,3.76560047043), test loss: 3.12197922766\n",
      "\n",
      "MC # 4, Hype # hyp4, Fold # 7\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold7/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (173.541931152,inf), test loss: 157.557967758\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (397.783599854,inf), test loss: 414.309870911\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (39.5771713257,80.2840335503), test loss: 48.1735126972\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.48069763184,93.1512393597), test loss: 3.2541800797\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (37.5669250488,62.9074060183), test loss: 37.9616122246\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.92854964733,48.0164564941), test loss: 2.78946811557\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (171.32623291,57.1497762337), test loss: 46.865823698\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (3.78256940842,32.953951817), test loss: 3.32493696809\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (54.8283081055,54.1567009778), test loss: 42.3725307941\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.60782337189,25.4090926594), test loss: 3.51509207487\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (32.1391220093,52.3790236465), test loss: 48.7314632893\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.07221221924,20.8861707531), test loss: 3.26796452105\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (31.7007217407,51.093705736), test loss: 45.9748094559\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.7894256115,17.8697293644), test loss: 3.48933279514\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (65.5600280762,50.1610735799), test loss: 45.5521605015\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.20978164673,15.7119117153), test loss: 2.70361222029\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (33.9465255737,49.3856249961), test loss: 46.5198277473\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.42895436287,14.094395396), test loss: 3.48342882395\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (21.5319042206,48.7386245558), test loss: 41.2575377941\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.65626239777,12.83615826), test loss: 2.90565522909\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (91.7467193604,48.2572575435), test loss: 47.0759618759\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.38320159912,11.8286603069), test loss: 3.43280403614\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (27.0605392456,47.7771634559), test loss: 39.7542528152\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.62935256958,11.0029275419), test loss: 2.90092198253\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (34.7251358032,47.4081978498), test loss: 46.8883400679\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.864909768105,10.3187105926), test loss: 3.28085277677\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (19.189617157,47.0412744567), test loss: 36.867856741\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (0.929132580757,9.73904701337), test loss: 2.72169156671\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (48.0865058899,46.7168291942), test loss: 43.5043610573\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.5845015049,9.24116858915), test loss: 3.16062578559\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (124.5365448,46.390054324), test loss: 39.325679493\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (6.42205762863,8.81010596437), test loss: 3.32633346021\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (104.138832092,46.0622444334), test loss: 45.4758818626\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (6.94161224365,8.43137768618), test loss: 3.1987741828\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (38.0426597595,45.7748092683), test loss: 41.1063051224\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.82433080673,8.09642962947), test loss: 3.42224830985\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (22.4174461365,45.4709264454), test loss: 41.8610095024\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.69156551361,7.79783119568), test loss: 2.66566238999\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (89.0458068848,45.2144920316), test loss: 43.8861081123\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.81660890579,7.53200837437), test loss: 3.57778589725\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (64.9841842651,44.9383523425), test loss: 40.2227692127\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.85320234299,7.29249968378), test loss: 2.89549894035\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (27.771856308,44.663947333), test loss: 43.5183332443\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (7.09923648834,7.07467820447), test loss: 3.39439079165\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (76.9001617432,44.3973330378), test loss: 37.1853888035\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.84745073318,6.87626565802), test loss: 2.84065672755\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (144.530273438,44.1110452314), test loss: 43.5890635014\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.85951519012,6.69387080515), test loss: 2.99858129025\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (37.1265792847,43.8330026755), test loss: 33.4050654411\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.19458198547,6.52609416432), test loss: 2.63754246831\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (34.7177429199,43.5438358509), test loss: 40.7524390936\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.6437702179,6.369889103), test loss: 3.19952094555\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (23.8936500549,43.2638663538), test loss: 35.3147424698\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.844835817814,6.22634567333), test loss: 3.21778533459\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (16.0745296478,42.9668230941), test loss: 39.3467234135\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.88785719872,6.09307087591), test loss: 3.15624568462\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (47.3924713135,42.667685311), test loss: 37.1691115141\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.29905509949,5.96856645153), test loss: 3.35690592229\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (49.5084228516,42.368601559), test loss: 36.8417520523\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.44754838943,5.85151153946), test loss: 2.63650073409\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (27.2433986664,42.0519518778), test loss: 38.5247997284\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.29075574875,5.74117875708), test loss: 3.47060557008\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (29.0236587524,41.7318546628), test loss: 36.4672293186\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.17470240593,5.63728128312), test loss: 2.61144012809\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (21.5332717896,41.4103470041), test loss: 38.7927148819\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.35914766788,5.53812217755), test loss: 3.33600530624\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (17.914981842,41.0915933039), test loss: 33.1066143751\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.24513173103,5.44529000744), test loss: 2.71083723307\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (33.8333816528,40.7610042301), test loss: 40.3240962982\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.00059366226,5.35738894903), test loss: 3.23154751062\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (11.3875846863,40.4280504323), test loss: 29.3255234957\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.77269697189,5.27429769786), test loss: 2.65514060259\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (15.4775466919,40.0928122993), test loss: 36.4456322193\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (0.706242859364,5.19458641628), test loss: 2.98044082522\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (26.1993713379,39.7492744877), test loss: 31.7339586258\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.51447021961,5.11874697284), test loss: 3.16889923811\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (13.6686325073,39.4013523376), test loss: 35.3398055315\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.2994992733,5.04600531721), test loss: 3.09122684002\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (12.62541008,39.057672014), test loss: 31.9065122128\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.32259511948,4.97599492634), test loss: 3.34470370114\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (14.1557884216,38.7196108395), test loss: 31.913906908\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.624334335327,4.90939377635), test loss: 2.69744710028\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (11.2790517807,38.3798735477), test loss: 34.988466692\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.62312889099,4.84623525929), test loss: 3.42569272518\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (38.6919250488,38.042530529), test loss: 33.0012838364\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.20189666748,4.78573960333), test loss: 2.55020231158\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (26.3983840942,37.7102246895), test loss: 33.6808089256\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.30899071693,4.72746100872), test loss: 3.24665573239\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (18.9751014709,37.3765878278), test loss: 31.9772082329\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.06342172623,4.67164193373), test loss: 2.6398057431\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (21.3361206055,37.0489582068), test loss: 36.0960898876\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.979700028896,4.61768664659), test loss: 3.24520072341\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (23.9843292236,36.7331902708), test loss: 30.5857517719\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (5.86946678162,4.5657456838), test loss: 2.78557297587\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (19.8684806824,36.426610913), test loss: 35.2745223761\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.12278401852,4.51567955765), test loss: 3.09784429669\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (39.2611541748,36.1256466636), test loss: 25.4805491924\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.80675268173,4.46811196053), test loss: 2.58063605428\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (29.8671684265,35.8299503438), test loss: 35.6223683834\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.34045886993,4.42232800161), test loss: 3.16141063571\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (21.4313240051,35.5446750025), test loss: 31.2143494368\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.85330629349,4.37811523244), test loss: 3.44795138836\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (27.6188087463,35.2632729716), test loss: 33.9330340862\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.822206735611,4.33569732719), test loss: 2.74142533243\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (10.5132761002,34.9891170743), test loss: 34.2807200432\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.2459205389,4.29445226569), test loss: 3.36785048842\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (75.5259552002,34.7287529269), test loss: 34.1791260242\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (3.80140066147,4.25464373883), test loss: 2.48716845512\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (8.44555568695,34.4731089352), test loss: 33.6993227959\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.826484918594,4.21588098644), test loss: 3.23564715981\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (30.8086299896,34.2266810526), test loss: 32.4198858738\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (3.98972582817,4.17920908047), test loss: 2.70801536739\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (18.2488555908,33.983093433), test loss: 34.5186898232\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.17749333382,4.14344690731), test loss: 3.18462421298\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (11.5182714462,33.749878253), test loss: 32.1938292027\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.01025724411,4.10903120635), test loss: 2.75986718982\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (28.0916194916,33.5212116686), test loss: 36.2041640759\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.09404039383,4.07572003655), test loss: 3.10125396848\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (41.2973480225,33.297842716), test loss: 29.7215994596\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (5.04528141022,4.04333056811), test loss: 2.68675312996\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (27.1787376404,33.0861978894), test loss: 36.3046523333\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.64718985558,4.01177469449), test loss: 3.17812748551\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (13.7284832001,32.876610729), test loss: 31.6388094425\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.66153168678,3.9809397733), test loss: 3.42838146687\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (50.1334609985,32.6760094876), test loss: 37.9942625523\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.02768516541,3.95165459356), test loss: 3.25143306553\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (9.0519323349,32.4761169616), test loss: 32.813833189\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.05133748055,3.92299524659), test loss: 3.23899243474\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (9.889128685,32.2843038568), test loss: 33.8259349108\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.14391231537,3.89538899581), test loss: 2.42151774764\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (50.6616134644,32.0994810063), test loss: 35.5217637062\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (4.13612318039,3.8687462623), test loss: 3.34132184386\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (39.1076202393,31.9139406663), test loss: 33.0924118042\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (4.43068695068,3.84249483045), test loss: 2.61997437775\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (10.4360389709,31.7380767294), test loss: 35.6465592861\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.97337818146,3.81693584039), test loss: 3.32474997342\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (10.7676906586,31.5655460619), test loss: 32.7622862816\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.76916956902,3.79200023952), test loss: 2.81637488902\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (19.24152565,31.3992073685), test loss: 37.1379095078\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.00584077835,3.76791108222), test loss: 3.04071283638\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (31.2322616577,31.2331209506), test loss: 29.7804000854\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.70056796074,3.74449245671), test loss: 2.64445587099\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (18.3498153687,31.0727232377), test loss: 36.0446912766\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (2.87775754929,3.72187520882), test loss: 3.15782675147\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (4.92272233963,30.9188028526), test loss: 32.468210268\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.604390025139,3.6998799658), test loss: 3.27249977291\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (18.8658618927,30.7646024807), test loss: 37.5875273228\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.64058756828,3.67827244523), test loss: 3.12618067861\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.6463813782,30.6154408903), test loss: 31.696459198\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.17770862579,3.65719158165), test loss: 3.30512833595\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (19.5984973907,30.4712962596), test loss: 34.0179331779\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.10392951965,3.63625298122), test loss: 2.49155300856\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (11.3848524094,30.3302816738), test loss: 33.3524196148\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.62680506706,3.61614779768), test loss: 3.3276671648\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (9.78029441833,30.189404923), test loss: 34.9892145634\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.32933712006,3.59651425958), test loss: 2.56341973245\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (29.617641449,30.0543842107), test loss: 34.7748239756\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.0476474762,3.57755001114), test loss: 3.29946103692\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (9.84309387207,29.9236087558), test loss: 33.9502838612\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.724222660065,3.55907385159), test loss: 2.79541164041\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (16.6912002563,29.7920949711), test loss: 36.7902620792\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.51667928696,3.54083533761), test loss: 3.02735155225\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (14.3458127975,29.6644908339), test loss: 30.660331583\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.28845787048,3.52301712726), test loss: 2.64212328792\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (12.6068468094,29.5413011272), test loss: 36.0231549263\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.89891290665,3.50522024292), test loss: 3.10153242946\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (31.02709198,29.4211153902), test loss: 31.8073839664\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (3.46092796326,3.48812189479), test loss: 3.20730755925\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (47.5528869629,29.3005781787), test loss: 36.6344272137\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.23154377937,3.47140589994), test loss: 3.17960166037\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (16.6464538574,29.1834462876), test loss: 31.5449519157\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.27594292164,3.45522008265), test loss: 3.28655394316\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (20.1719837189,29.0706566252), test loss: 34.6341434002\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.95521306992,3.43936172967), test loss: 2.59698966146\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (12.3455963135,28.957011499), test loss: 33.038305974\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.4635361433,3.42377155182), test loss: 3.39599730968\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (5.17397880554,28.8460871147), test loss: 35.0765046597\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.25828409195,3.40839890691), test loss: 2.55328532159\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (8.69248771667,28.7393374927), test loss: 32.6146135569\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.58488202095,3.39313817836), test loss: 3.1350145489\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (11.8432302475,28.6348103689), test loss: 34.1343279362\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.41942763329,3.37828735408), test loss: 2.77027246654\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (11.4487695694,28.5307100211), test loss: 36.5052305222\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (4.70549488068,3.36394403976), test loss: 3.19291646481\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (9.16247653961,28.42814245), test loss: 30.8678527832\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.735721170902,3.34984603871), test loss: 2.81411879659\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (18.2695140839,28.3294939447), test loss: 35.2755590439\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.458256810904,3.33607481318), test loss: 2.97929775417\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (16.263469696,28.2293335394), test loss: 27.7677732944\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.68034231663,3.32257265732), test loss: 2.89934848845\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (12.0869255066,28.1318416823), test loss: 35.8970490456\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.54026031494,3.30917140117), test loss: 3.13858215213\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (10.9581737518,28.0382042494), test loss: 31.5666538\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (4.03822612762,3.29598738657), test loss: 3.35364074111\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (7.53449296951,27.9461298312), test loss: 32.6772959709\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.833751916885,3.28291938357), test loss: 2.64856007993\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (8.55563926697,27.854617769), test loss: 32.9494487286\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.24682235718,3.27035510324), test loss: 3.28745567799\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (25.5656967163,27.7636898562), test loss: 35.1313946247\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.87702727318,3.25798262008), test loss: 2.46996025145\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (6.24514484406,27.6758169983), test loss: 32.7998696566\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.51430559158,3.24585642166), test loss: 3.1453086853\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (17.7344532013,27.58769631), test loss: 35.4634885073\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.347325772047,3.23400240328), test loss: 2.74224253893\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (8.26307010651,27.5009551748), test loss: 33.1484987736\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.42987322807,3.22222035497), test loss: 3.07122653127\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (50.9987487793,27.4182338886), test loss: 32.816754818\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.16838049889,3.21061081093), test loss: 2.79971337318\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (16.3207015991,27.3353825583), test loss: 35.3633094072\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (3.21515226364,3.19907731398), test loss: 3.02416581511\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (33.842502594,27.2540863325), test loss: 27.4782511234\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (3.90820169449,3.1880079624), test loss: 2.61080551445\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (16.3612442017,27.1721756972), test loss: 35.6076442242\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.625161170959,3.17696666192), test loss: 3.01722256541\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (16.3819522858,27.0933402986), test loss: 30.9072921991\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.11322903633,3.16625115945), test loss: 3.30738019943\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (7.70212316513,27.0147253752), test loss: 38.4247356415\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (3.10956430435,3.15572286922), test loss: 3.17030974329\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (20.5591640472,26.9369891746), test loss: 32.610807848\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.52713894844,3.14527724525), test loss: 3.27348060608\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (17.5379676819,26.8626354506), test loss: 34.3759059906\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.68302726746,3.13490894126), test loss: 2.42032924294\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (9.06813812256,26.7878233987), test loss: 33.278793931\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.48144769669,3.12461405651), test loss: 3.21657630205\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (54.4115447998,26.7152069996), test loss: 32.4485708952\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (3.4598133564,3.1147226731), test loss: 2.5876080215\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (15.3969593048,26.6410807779), test loss: 34.1549587965\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.81271708012,3.10485712165), test loss: 3.13890814483\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (13.4660816193,26.5696705175), test loss: 32.6569278002\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.15358757973,3.09524751443), test loss: 2.79629746228\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (51.1262359619,26.5002846667), test loss: 37.159943223\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.94089961052,3.08589313711), test loss: 3.06440146863\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (8.91596508026,26.4288578347), test loss: 30.0256966114\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.02813482285,3.07649850683), test loss: 2.67211135626\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (5.94127750397,26.3609540516), test loss: 35.498652339\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.31668806076,3.0671944141), test loss: 3.06191123873\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (33.9737472534,26.2939704134), test loss: 30.3409448862\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.61611938477,3.05798684745), test loss: 3.10547598302\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (16.9286060333,26.227873828), test loss: 37.8062310696\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.76079201698,3.04900991855), test loss: 3.09716092944\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (26.7968177795,26.1607654769), test loss: 31.0363368034\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.64628911018,3.04013325648), test loss: 3.1907404691\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (20.3134975433,26.0957198802), test loss: 33.9912330151\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.5066652298,3.03150318772), test loss: 2.4209759146\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (33.8332977295,26.0331475025), test loss: 33.360605526\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (4.49111127853,3.02307486025), test loss: 3.28123164475\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (19.4976081848,25.9684660029), test loss: 34.6933476925\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.26377224922,3.01458841405), test loss: 2.6257701695\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (12.2430744171,25.9059644465), test loss: 34.3678692818\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (4.17416191101,3.00626724046), test loss: 3.1712862432\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (9.43302249908,25.8451362708), test loss: 32.2518164635\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.66191005707,2.99784489862), test loss: 2.72962167263\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (5.54060173035,25.7846940205), test loss: 35.7780626774\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.85370492935,2.98970298946), test loss: 2.9330250442\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (14.0658693314,25.7232355523), test loss: 29.5790816307\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.27823019028,2.98166106084), test loss: 2.60680707991\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (11.3378753662,25.6640523014), test loss: 35.1941240549\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.08626866341,2.97385174593), test loss: 3.00954410434\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (6.86345005035,25.6065542387), test loss: 30.7343912363\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.18776226044,2.9661697704), test loss: 3.08510382175\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (14.6894397736,25.5474916908), test loss: 36.5317831039\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.95103216171,2.95853545493), test loss: 3.04579640925\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (20.6842880249,25.4900909357), test loss: 30.7692872047\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.76050949097,2.9509381167), test loss: 3.17831760049\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (16.2903881073,25.4342087981), test loss: 34.1669623613\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (4.04919624329,2.94327935652), test loss: 2.54105525613\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (20.8626098633,25.3790651987), test loss: 31.677011323\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.20322322845,2.93585530314), test loss: 3.29609897435\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (27.9097175598,25.3229605935), test loss: 35.0567942142\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (2.74247479439,2.92854755296), test loss: 2.52464581728\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (14.2191152573,25.2679629059), test loss: 33.2112484932\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.973594844341,2.92140946482), test loss: 3.1100931257\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (12.8267841339,25.2149969647), test loss: 34.099005127\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.50403928757,2.91437731576), test loss: 2.73591726422\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (19.5998897552,25.1607514263), test loss: 36.2399989605\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.14464652538,2.90739886192), test loss: 3.10699560642\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (6.57071304321,25.107532435), test loss: 29.8566824436\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.944249749184,2.90044547577), test loss: 2.68285016418\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (6.99799203873,25.0561004339), test loss: 34.3181890488\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.816397607327,2.89344789858), test loss: 2.83550660014\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (17.218290329,25.0053898994), test loss: 31.4305675745\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (4.935110569,2.88667365885), test loss: 3.15899276882\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (10.8820667267,24.9540195653), test loss: 35.7787881374\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.82610607147,2.87998940844), test loss: 3.08036783636\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (13.2809534073,24.9031131236), test loss: 30.3173100471\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.20308804512,2.87341166016), test loss: 3.22384491265\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (10.3174514771,24.8540105569), test loss: 33.076588583\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.468388587236,2.86693007551), test loss: 2.64749873877\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (33.9975242615,24.8036522289), test loss: 32.5386671543\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.32912826538,2.86054653391), test loss: 3.26688089371\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (19.745475769,24.7543335159), test loss: 35.1519626141\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.26534938812,2.85416185613), test loss: 2.54835982025\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (12.6983070374,24.706781807), test loss: 32.1944113255\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.73378133774,2.84781986034), test loss: 3.11907120645\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (10.0208644867,24.6595980938), test loss: 33.6800005913\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.35101413727,2.84147954592), test loss: 2.61694397777\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (13.7554101944,24.6121709854), test loss: 33.7384443283\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (3.17234611511,2.83536486488), test loss: 3.05426959991\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (13.8732023239,24.564689636), test loss: 31.6200279236\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.21368312836,2.82927577487), test loss: 2.72476981282\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (6.92552804947,24.5188020849), test loss: 34.6744197845\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.5600373745,2.82328277538), test loss: 2.85672889501\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (11.6371231079,24.4721629415), test loss: 27.6438051939\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.680320501328,2.81739902457), test loss: 2.59290368557\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (9.69449329376,24.426369621), test loss: 35.1246514797\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.90875649452,2.81152333157), test loss: 2.99703570455\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (30.8416633606,24.3824567805), test loss: 30.6055701494\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.24440717697,2.80565962814), test loss: 3.28293578923\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (23.8612709045,24.3381300862), test loss: 36.1326825619\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (3.2930700779,2.79980429415), test loss: 2.75705765188\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (9.19487571716,24.2940977788), test loss: 32.4429460049\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.46005916595,2.79416966134), test loss: 3.20454063118\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (19.9423828125,24.2495520332), test loss: 33.9476358414\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.93519508839,2.78850496635), test loss: 2.37313195467\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (13.0479354858,24.2064932245), test loss: 32.8954332352\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.849967598915,2.78295914072), test loss: 3.09523887038\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (14.3113327026,24.1633204021), test loss: 33.9359679699\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.1835398674,2.77751374764), test loss: 2.6777885735\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (6.1002573967,24.1204721491), test loss: 33.0252245903\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.649059057236,2.77207233964), test loss: 3.07116933465\n",
      "\n",
      "MC # 4, Hype # hyp4, Fold # 8\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold8/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (301.543243408,inf), test loss: 190.493929291\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (330.821533203,inf), test loss: 372.755441284\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (56.0085449219,134.019657894), test loss: 57.0205175877\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.591968297958,84.904215803), test loss: 3.52751691043\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (22.9117336273,92.6795830841), test loss: 41.6366558552\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.96159076691,44.0350757166), test loss: 3.24878635108\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (46.3451957703,77.2462498134), test loss: 41.5516223907\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.89721655846,30.3898596039), test loss: 3.65520281196\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (18.3161582947,69.5521387515), test loss: 41.5694545269\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.00469911098,23.5714739522), test loss: 3.5602781713\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (80.9589233398,64.927278989), test loss: 39.6199653149\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.34214687347,19.4844152035), test loss: 2.93741152287\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (23.9143619537,61.75515975), test loss: 47.6763790607\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.77710258961,16.763030029), test loss: 3.61862255335\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (19.7508182526,59.4674579545), test loss: 41.6807512283\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.06661653519,14.816062545), test loss: 2.9045409143\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (39.5765762329,57.712991419), test loss: 45.3869173527\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.7557362318,13.3585324789), test loss: 3.61408357322\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (44.6136474609,56.313102934), test loss: 37.7815770626\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.97117519379,12.2240233782), test loss: 2.86550958157\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (70.9171905518,55.187022563), test loss: 43.1702002525\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (2.79116225243,11.3146382483), test loss: 3.69403802156\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (28.6337585449,54.2886279982), test loss: 35.8378671169\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.2641055584,10.5720374322), test loss: 2.81481519938\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (128.841690063,53.5260248773), test loss: 42.1919874191\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (6.19842720032,9.95431454013), test loss: 3.51940870881\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (24.1702404022,52.8450963589), test loss: 33.135854578\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.96026360989,9.43364529656), test loss: 2.65914138854\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (44.1301193237,52.2451647045), test loss: 40.6453281403\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.95463585854,8.98546883467), test loss: 3.39805716872\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (25.2498626709,51.6948147209), test loss: 38.5862440348\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.38059234619,8.59828967959), test loss: 3.39168493748\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (39.2998733521,51.1911992613), test loss: 42.5674808979\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.26247501373,8.25865461906), test loss: 3.25820965767\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (30.1504764557,50.7389260389), test loss: 41.7141497612\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.56066989899,7.95747697491), test loss: 3.52275143564\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (34.8837356567,50.352164639), test loss: 39.9499766827\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.70363807678,7.69053025698), test loss: 2.8194421947\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (47.7384262085,49.9816134657), test loss: 44.28565588\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.4602496624,7.45157897447), test loss: 3.71792201996\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (40.4308891296,49.6361101367), test loss: 37.6087721348\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.33361053467,7.23827316109), test loss: 2.78919332325\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (40.1379470825,49.3028337173), test loss: 42.092186594\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.59325146675,7.04332676763), test loss: 3.53606939018\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (40.5945625305,48.9757971087), test loss: 34.798866415\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.81580162048,6.86664402119), test loss: 2.8073507309\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (41.3035163879,48.6575544807), test loss: 42.2136117458\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.60256099701,6.70471837151), test loss: 3.562613675\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (43.055267334,48.357685888), test loss: 34.3011452675\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (6.07398414612,6.55466764791), test loss: 2.80604166389\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (15.0376300812,48.0850794246), test loss: 39.2712945938\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.39894008636,6.41698282783), test loss: 3.50874123871\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (11.1371927261,47.8105596461), test loss: 35.9966842175\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.69784855843,6.28929619564), test loss: 3.20347275436\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (72.3536376953,47.5441155613), test loss: 38.0677710056\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (4.18926239014,6.17207942492), test loss: 3.40526498556\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (33.2804031372,47.2755195334), test loss: 38.1044835091\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.05148649216,6.06142469161), test loss: 3.50363431573\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (25.7876243591,47.0012247837), test loss: 35.2921913624\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.63712191582,5.95833434946), test loss: 2.84579000771\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (25.0713710785,46.7248721706), test loss: 43.0348561287\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (5.34241437912,5.86122039843), test loss: 3.54534369707\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (38.9319000244,46.4540828977), test loss: 36.3352489471\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (5.4535150528,5.76844427445), test loss: 2.64145410657\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (37.5042991638,46.1969710133), test loss: 41.1680882931\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (5.89327144623,5.68161397834), test loss: 3.60557420254\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (14.2273273468,45.929954122), test loss: 32.4002426624\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (0.899443149567,5.59911148227), test loss: 2.71523092687\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (92.4635162354,45.6616772857), test loss: 39.4877717972\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (4.57730150223,5.52162656958), test loss: 3.47607870102\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (28.93800354,45.3879953095), test loss: 29.6308535337\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.70170915127,5.44694215227), test loss: 2.79988065362\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (22.5599422455,45.1030598595), test loss: 36.5244263172\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.05144286156,5.37591372435), test loss: 3.19900252819\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (33.4410438538,44.8111279568), test loss: 28.0253838539\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.44164729118,5.30760652332), test loss: 2.77360597849\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (8.15202331543,44.5181080293), test loss: 35.2600514412\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (0.676025986671,5.24128142465), test loss: 3.28471487164\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (19.0512275696,44.2302965341), test loss: 32.3404497385\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.09067821503,5.17823702681), test loss: 3.24008063674\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (59.7502174377,43.9290856222), test loss: 29.9029146194\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.30949544907,5.11743933976), test loss: 2.72464810014\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (20.759141922,43.617904071), test loss: 35.4475391388\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.945677518845,5.05904851979), test loss: 3.18255089521\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (34.5692062378,43.3062422236), test loss: 31.0966870785\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.63821029663,5.0023267422), test loss: 2.52747485638\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (27.5455112457,42.9793707132), test loss: 35.784614563\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.49275636673,4.94749653959), test loss: 3.39944500327\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (35.1165847778,42.6465337083), test loss: 27.7244743109\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.45051848888,4.89406132581), test loss: 2.3489205122\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (14.6634616852,42.313806936), test loss: 34.8298030376\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.97368121147,4.84172359675), test loss: 3.27053349614\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (10.4916496277,41.9828707813), test loss: 26.5079591751\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.3598703146,4.79115778104), test loss: 2.580119358\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (53.8515052795,41.6447245066), test loss: 34.6284139276\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.86554658413,4.74234503758), test loss: 3.33949701786\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.6116714478,41.3033736494), test loss: 23.5722783089\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.8297123909,4.69518383606), test loss: 2.56349085569\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (40.7995147705,40.9656084971), test loss: 31.6430685759\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.87621319294,4.6491670525), test loss: 3.22716679275\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (39.7660179138,40.6216993122), test loss: 28.2998935938\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.85719048977,4.60438202997), test loss: 3.0738896817\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (9.33253574371,40.2781885372), test loss: 31.3502938509\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.00665020943,4.56051334878), test loss: 3.28051565737\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (12.444108963,39.9429591827), test loss: 30.1488887787\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.34663808346,4.51742613379), test loss: 3.37102193832\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (6.77589416504,39.6129531447), test loss: 26.9510762215\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.381466835737,4.47568227567), test loss: 2.5396717906\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (32.5136146545,39.2829303748), test loss: 32.5992825031\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.66200470924,4.43535330399), test loss: 3.43269188702\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (5.80987453461,38.9603009447), test loss: 27.5637144327\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.5216987133,4.39647550068), test loss: 2.4212641269\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (18.5874214172,38.6459287816), test loss: 32.7057914972\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.9383020401,4.35871428757), test loss: 3.25151511133\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (20.9729652405,38.3336826311), test loss: 26.8644731522\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.10585200787,4.3219893977), test loss: 2.61644275188\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (16.5027008057,38.0282756112), test loss: 33.6076414108\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.21547555923,4.28612790453), test loss: 3.34583607018\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (6.64698600769,37.7336315097), test loss: 25.3100914717\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (5.25936889648,4.25103019867), test loss: 2.64632297456\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.1932449341,37.4459918054), test loss: 31.8477405071\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.33345317841,4.21706446079), test loss: 3.32019076645\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (36.2989387512,37.1610034244), test loss: 30.0109386683\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.66598582268,4.18426522758), test loss: 3.21025509834\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (6.40349674225,36.885794672), test loss: 32.1809871197\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.71276044846,4.15277760853), test loss: 3.44070550203\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (20.0185012817,36.6187614391), test loss: 29.0335923672\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.838227629662,4.12214882839), test loss: 3.40965522528\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.6804494858,36.3545403424), test loss: 27.1613226056\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.89440870285,4.0923878234), test loss: 2.82104426622\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (19.0236473083,36.0977142717), test loss: 31.7213854313\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.21864938736,4.06320194209), test loss: 3.41681329608\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (9.933634758,35.850425882), test loss: 29.585360384\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.53218984604,4.03444162078), test loss: 2.52656399608\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (33.4984321594,35.6088860367), test loss: 33.8673750043\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.72453212738,4.00676096039), test loss: 3.42209409475\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (10.1831932068,35.3687550587), test loss: 25.9936450005\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.11280918121,3.97979175975), test loss: 2.36440607905\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (23.230178833,35.1390519825), test loss: 34.9003685474\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.41871500015,3.95382723057), test loss: 3.36400542557\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (15.2239437103,34.9152354686), test loss: 26.8906182766\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.32726860046,3.92860126866), test loss: 2.69044348598\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (15.4117488861,34.6938870288), test loss: 33.9577423096\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.43970537186,3.90384721261), test loss: 3.37582055628\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (12.1063404083,34.4784352157), test loss: 23.9602996349\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.739471197128,3.87956237576), test loss: 2.50704841763\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (5.69858551025,34.2712547727), test loss: 33.9873726606\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.04882466793,3.85547564615), test loss: 3.41800443232\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (20.2388534546,34.0676205249), test loss: 29.6710267544\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.72688937187,3.83218069335), test loss: 3.3487601608\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (13.8158149719,33.8659642032), test loss: 34.659462595\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.40812492371,3.80946745838), test loss: 3.4725767225\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (31.3479709625,33.6725663755), test loss: 30.5678816676\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.50606226921,3.7875741104), test loss: 3.45104085803\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (4.6069726944,33.4837219902), test loss: 30.3060830116\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.05702805519,3.76620405153), test loss: 2.64440937936\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.704205513,33.2967811115), test loss: 32.0221549988\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.70796835423,3.74524521896), test loss: 3.49479445219\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (5.06004142761,33.1144236135), test loss: 30.1786964893\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.5079202652,3.72462901017), test loss: 2.52028490901\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (11.9091405869,32.9391017132), test loss: 32.5487808704\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.61363518238,3.70411955343), test loss: 3.33262248635\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (20.9742050171,32.7661660397), test loss: 29.3985937834\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.45303225517,3.68426888996), test loss: 2.64126444608\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (5.82683086395,32.5941899245), test loss: 34.0741877079\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.37133002281,3.66489175313), test loss: 3.33320529461\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (34.803768158,32.4290241969), test loss: 26.9114864349\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.637560963631,3.64615859324), test loss: 2.66640659273\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (9.70913505554,32.2683620173), test loss: 32.6172091007\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.598535835743,3.62791359716), test loss: 3.25474011004\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (13.4252347946,32.1081788426), test loss: 30.9983181\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.0920958519,3.60992456398), test loss: 3.22124188542\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (10.5385112762,31.9519124072), test loss: 34.2647783279\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (3.63241910934,3.59221041741), test loss: 3.44386788011\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (20.2415466309,31.8012951604), test loss: 29.7233121634\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.95280694962,3.57453513121), test loss: 3.26077463925\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.1916561127,31.6519563191), test loss: 28.5206840038\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.41424667835,3.55738128326), test loss: 2.71337861717\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (8.86373138428,31.5038205753), test loss: 33.1649216175\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.53647232056,3.54062503384), test loss: 3.37728803754\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (18.5670204163,31.3605976606), test loss: 30.4278808594\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.874754011631,3.52440212687), test loss: 2.54584838748\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (14.6397380829,31.2220367196), test loss: 34.9010114193\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.941424906254,3.50857915464), test loss: 3.36254754663\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (15.1221504211,31.0827898492), test loss: 28.2604035616\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (4.0006108284,3.49293605557), test loss: 2.53670544028\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (7.89559555054,30.9470929495), test loss: 35.3533773422\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (4.30357027054,3.47746896204), test loss: 3.41684703529\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (27.5077095032,30.8160140531), test loss: 26.2693957329\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.23301649094,3.46203178443), test loss: 2.60932233036\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (22.6018123627,30.6858357856), test loss: 34.2106644154\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.00541186333,3.44706706066), test loss: 3.32575032115\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (11.0640172958,30.55619956), test loss: 23.7558481455\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.278760612011,3.43239209694), test loss: 2.45347913504\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (18.7339439392,30.4306775549), test loss: 34.018019104\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.11354696751,3.41818650162), test loss: 3.28158633113\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (8.9045753479,30.3092222482), test loss: 29.8758503914\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.7105230093,3.40428800685), test loss: 3.32843686044\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (59.8635559082,30.1868873439), test loss: 34.0081882238\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.74486207962,3.39049043875), test loss: 3.02296357751\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (9.69950294495,30.0676737568), test loss: 29.990104413\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.08455562592,3.37686292294), test loss: 3.33341355324\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (20.0915317535,29.9519995429), test loss: 31.8800401211\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.21070623398,3.36329217018), test loss: 2.57918369174\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (16.9398612976,29.8372813128), test loss: 32.3881373882\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.05523967743,3.35007394873), test loss: 3.39145648479\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (36.2031326294,29.7227391912), test loss: 29.2422666073\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.76737487316,3.33710203076), test loss: 2.4544072926\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (13.3882904053,29.6113646551), test loss: 33.1113387585\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.63252615929,3.32451321047), test loss: 3.24843062162\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (6.9582695961,29.5039037417), test loss: 30.2481861591\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.395388543606,3.31219663561), test loss: 2.67847938836\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (45.7233200073,29.3950804563), test loss: 33.7685419083\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.91866958141,3.29994684577), test loss: 3.28290442824\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (18.7455539703,29.2892567579), test loss: 26.2155846119\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (5.89882850647,3.28783231925), test loss: 2.65400358438\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (9.86110877991,29.1862169766), test loss: 34.0141702652\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.19789361954,3.27574405026), test loss: 3.22688471973\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (22.6741714478,29.0839805467), test loss: 29.2281305313\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.90533494949,3.26396791697), test loss: 3.14166179001\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (12.1147489548,28.981305866), test loss: 34.5710390329\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.03469920158,3.25238203086), test loss: 3.33703516424\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (12.6725997925,28.8822475425), test loss: 28.8897973776\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.37542557716,3.24113631936), test loss: 3.25845230222\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (16.2999591827,28.7861610917), test loss: 30.0294216633\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.312936991453,3.2301455356), test loss: 2.75541750789\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (3.88702344894,28.6878451679), test loss: 32.0653249621\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.56744146347,3.21915421655), test loss: 3.35530835688\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (9.90780067444,28.5934490345), test loss: 29.4901103973\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (4.97505664825,3.20829111149), test loss: 2.35485200882\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (6.81639003754,28.5011623833), test loss: 34.2036761284\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.69076180458,3.19744718843), test loss: 3.24669035971\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (25.9518871307,28.4092447823), test loss: 27.8603824615\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.20982050896,3.18688145316), test loss: 2.46223194003\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (18.4880008698,28.3168786295), test loss: 35.5582233906\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.76440894604,3.17645463043), test loss: 3.31936372221\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (13.4609680176,28.2277235355), test loss: 26.3738827944\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.36944699287,3.16634513853), test loss: 2.66105672717\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (23.5652084351,28.1412067527), test loss: 33.5245889425\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.341084182262,3.15646101096), test loss: 3.20641170889\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (34.8192329407,28.0524698437), test loss: 25.9231244564\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (4.08320999146,3.14655042095), test loss: 2.8643599838\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (9.09815406799,27.9671633713), test loss: 33.7901631117\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.57836508751,3.13667963433), test loss: 3.20737634301\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (14.2046546936,27.8837016692), test loss: 28.7704563022\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.27396321297,3.1269114374), test loss: 3.19397078454\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (35.2019615173,27.8003701351), test loss: 28.9774837017\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.35166025162,3.11737615771), test loss: 2.68137645125\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (12.7695198059,27.7166896074), test loss: 31.9988419056\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.20717191696,3.10793728466), test loss: 3.21452856064\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (6.11960744858,27.635927989), test loss: 31.2469315767\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.62868928909,3.09878836273), test loss: 2.5393325299\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (54.8343658447,27.5573249712), test loss: 32.766582489\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (3.40605449677,3.08983670178), test loss: 3.32123742104\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (34.6547546387,27.4766805994), test loss: 28.3039915562\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.768949866295,3.08079300289), test loss: 2.27312745303\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (6.52687644958,27.3992578346), test loss: 33.7441854715\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (3.5131778717,3.07185334534), test loss: 3.16526954472\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (50.5133743286,27.3232847403), test loss: 27.1479131699\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.15572714806,3.06297198761), test loss: 2.59246658683\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (6.79266119003,27.2470203063), test loss: 34.9802552223\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.884306013584,3.05429265605), test loss: 3.29921634793\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (15.9268627167,27.1709833219), test loss: 24.4459458828\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.73467671871,3.04571326039), test loss: 2.53731134832\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (8.61130523682,27.097329008), test loss: 33.597779417\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.807211101055,3.03735926731), test loss: 3.21946993768\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (50.6545295715,27.0250533128), test loss: 28.6590771317\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (3.39303135872,3.02921132587), test loss: 3.04984582067\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (12.8554134369,26.9515809947), test loss: 34.2229246616\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.573925852776,3.0209797861), test loss: 3.28173877597\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (3.36102342606,26.8811173396), test loss: 29.3148423195\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.34928405285,3.01279359954), test loss: 3.25496336222\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (36.4212532043,26.8114082206), test loss: 29.8082037926\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (4.36079740524,3.00470300014), test loss: 2.57116109133\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (26.5448303223,26.7416144381), test loss: 31.8443999767\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.06699132919,2.99676075617), test loss: 3.31653873026\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (25.8329715729,26.6717658354), test loss: 29.8760554314\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.07739138603,2.98890033384), test loss: 2.35797564089\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (3.90979766846,26.6042576279), test loss: 32.8642629027\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.05244231224,2.98126794363), test loss: 3.15581095517\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (48.9513511658,26.5377583015), test loss: 29.4177587271\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.63707208633,2.97378000677), test loss: 2.53898977041\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (14.4994325638,26.4702967787), test loss: 33.8426341534\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.6590359211,2.96626385516), test loss: 3.20597299933\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (8.44934177399,26.405614257), test loss: 25.9056903839\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.35106563568,2.95873336582), test loss: 2.55957092047\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (10.0864200592,26.3410216767), test loss: 32.4638061047\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (3.19918227196,2.9512840755), test loss: 3.08577491343\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (48.8880691528,26.2770587786), test loss: 29.760939002\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.56208181381,2.94401688176), test loss: 3.02707040012\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (17.946685791,26.2125255296), test loss: 32.8583899021\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.01988768578,2.93679047893), test loss: 3.21050564498\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (10.7747888565,26.1504081119), test loss: 28.2717867374\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.04100382328,2.92977058258), test loss: 3.18142001629\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (13.8032426834,26.0883453153), test loss: 28.9743984938\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.10697817802,2.92286849473), test loss: 2.74612020403\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (38.0767478943,26.0265896389), test loss: 30.7684195518\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (4.79275083542,2.91596699771), test loss: 3.27080294192\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (16.1068687439,25.9669095289), test loss: 30.7080207825\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.45105600357,2.90900941078), test loss: 2.49888385832\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (16.8158092499,25.907140726), test loss: 33.5837928057\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (2.52545833588,2.90213727813), test loss: 3.2597446084\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (14.0494365692,25.847480607), test loss: 26.8711569786\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.34292602539,2.895435874), test loss: 2.2690677762\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (18.5252494812,25.7879726759), test loss: 34.5111947536\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.03323578835,2.88876901178), test loss: 3.17095865309\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (14.3771448135,25.7304791998), test loss: 26.0314625263\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.48299121857,2.88229676222), test loss: 2.51651035398\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (14.6186752319,25.6728432466), test loss: 33.6670676708\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (2.39908790588,2.87591186547), test loss: 3.18301309645\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (18.6887245178,25.6153331211), test loss: 23.710477829\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.51397275925,2.86950652208), test loss: 2.3760452643\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (14.7992601395,25.560031618), test loss: 33.4539097071\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.08221709728,2.86310039468), test loss: 3.15873145908\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (23.7439193726,25.5045446216), test loss: 28.3550995827\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.859821081161,2.85673141719), test loss: 3.06599783599\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (11.7075004578,25.4491610282), test loss: 34.537186718\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.49330091476,2.85054098779), test loss: 3.26094567478\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (14.0287799835,25.3938682706), test loss: 29.6571487904\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.58244013786,2.84437137117), test loss: 3.2124709785\n",
      "\n",
      "MC # 4, Hype # hyp4, Fold # 9\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold9/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (364.444488525,inf), test loss: 212.445678329\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (268.142089844,inf), test loss: 326.229423523\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (107.723396301,150.272051653), test loss: 74.4066830635\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (0.76620054245,83.0405266964), test loss: 3.42813237011\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (75.0746383667,102.708719529), test loss: 43.9477080822\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (3.60533905029,43.0142016327), test loss: 3.2666621089\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (98.2523498535,84.7315851329), test loss: 44.4747239113\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.98980760574,29.6636374381), test loss: 3.43268792033\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (55.7331924438,75.5298587799), test loss: 43.798086071\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (5.73704099655,22.9850141812), test loss: 3.61025147736\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (92.6553268433,70.1157767748), test loss: 43.9983967304\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (7.28774261475,18.9862767087), test loss: 2.82160049081\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (52.2019844055,66.4383325845), test loss: 47.0161242485\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.313858836889,16.3180139126), test loss: 3.6515394628\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (43.8860054016,63.7727973777), test loss: 45.108523035\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.29338276386,14.4128081228), test loss: 2.65765106082\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (31.964635849,61.7741772319), test loss: 44.6190320015\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.34819412231,12.9819560732), test loss: 3.51038839817\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (44.1551132202,60.1956533819), test loss: 42.118640089\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.81413793564,11.8688694175), test loss: 2.88620915115\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (32.1380577087,58.8807079249), test loss: 47.9447164536\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.23957300186,10.9779849082), test loss: 3.50553252697\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (26.443687439,57.8208289511), test loss: 39.7812209129\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.93024432659,10.2472145933), test loss: 2.66861841083\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (45.7855987549,56.9617350148), test loss: 44.9442990303\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.09684562683,9.63934707118), test loss: 3.45669917166\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (45.1558456421,56.2258043501), test loss: 40.53910532\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (8.39835739136,9.12638950547), test loss: 3.21673209071\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (22.0690803528,55.5613214089), test loss: 46.2500976086\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.95604920387,8.68604523271), test loss: 3.33336649537\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (46.8506507874,54.9931327372), test loss: 44.5937185764\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.2337539196,8.30368365616), test loss: 3.58354815245\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (102.34173584,54.4713682491), test loss: 44.5008983135\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.80751276016,7.96954028476), test loss: 2.675481987\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (38.4984169006,53.9844267343), test loss: 45.3275577068\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.31399917603,7.67463626196), test loss: 3.64648109674\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (35.183139801,53.5636856402), test loss: 43.8873685837\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (6.50583648682,7.41227531929), test loss: 2.68097368479\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (126.949256897,53.1740049564), test loss: 45.2111158848\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.09763383865,7.17586267408), test loss: 3.56443777084\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (33.7298240662,52.8326383154), test loss: 40.9620000839\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.18537473679,6.96294179369), test loss: 2.69731530249\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (24.5871200562,52.497903095), test loss: 47.605398345\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.6872355938,6.77007097486), test loss: 3.46017698646\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (28.3341464996,52.1903256572), test loss: 37.0181294918\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (0.956455230713,6.59519876033), test loss: 2.56825322807\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (26.3734169006,51.8964173293), test loss: 44.0086542606\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.58198952675,6.43372749306), test loss: 3.24545862675\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (44.6255607605,51.6119845138), test loss: 39.5538442612\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.51850605011,6.28603443495), test loss: 3.41377110183\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (34.3264846802,51.3278773917), test loss: 44.9524910927\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.78679668903,6.14950136013), test loss: 3.28264071941\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (49.5202827454,51.0684054501), test loss: 41.9786744118\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.69486153126,6.022912861), test loss: 3.31320072711\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (14.5864696503,50.827135351), test loss: 42.0214753151\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.43559479713,5.90502013828), test loss: 2.64515639544\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (33.7735214233,50.5999790197), test loss: 42.8224056244\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.18529307842,5.79626241512), test loss: 3.57871415615\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (58.6180686951,50.3663266264), test loss: 39.6806898117\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (5.69477081299,5.69451946212), test loss: 2.51908224225\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (25.6623382568,50.1437460505), test loss: 44.6857289314\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.20968985558,5.59860884838), test loss: 3.497588557\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (22.049785614,49.9235416406), test loss: 37.6559597969\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.97860860825,5.50901043017), test loss: 2.772612077\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (86.5241546631,49.6967284142), test loss: 43.4504972458\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.0614066124,5.42391774963), test loss: 3.10625641644\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (44.7630310059,49.4698108198), test loss: 33.7245278358\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.73405706882,5.34381204264), test loss: 2.48172760606\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (32.7699737549,49.2534824644), test loss: 42.6513467312\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.9372600317,5.26709647135), test loss: 3.3040433526\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (35.5751113892,49.0440576643), test loss: 37.8547385693\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.997380197048,5.19447531587), test loss: 3.2824839592\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (16.6644992828,48.8339902278), test loss: 38.5337754726\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.68816804886,5.12575778315), test loss: 2.71207632571\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (24.7169589996,48.622948268), test loss: 42.0176342964\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.67675578594,5.06058740637), test loss: 3.3635435462\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (69.9929733276,48.4078812276), test loss: 39.9257768154\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (5.28145694733,4.99761877104), test loss: 2.64299295843\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (45.1942939758,48.1860300549), test loss: 41.4125484943\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.49823153019,4.93749184721), test loss: 3.41956583858\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (85.7294464111,47.9583007363), test loss: 35.9646555901\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.76202392578,4.87993998943), test loss: 2.68773047924\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (47.6004943848,47.7403286412), test loss: 42.080962038\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.8419778347,4.82445675797), test loss: 3.39201495051\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (46.5024414062,47.5105054599), test loss: 34.3030901909\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (5.15816783905,4.77073858711), test loss: 2.57004563808\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (78.5644454956,47.2882567786), test loss: 39.775135994\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (3.24341249466,4.7195654392), test loss: 3.14316222668\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (30.8503513336,47.0546896904), test loss: 34.6698202133\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.626488089561,4.67000239733), test loss: 2.9060855031\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (45.9410400391,46.8167441749), test loss: 38.1673014164\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.56550216675,4.62241408183), test loss: 3.2163816154\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (24.6836299896,46.57328928), test loss: 34.7648252964\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.02538275719,4.57611784005), test loss: 3.29593417346\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (29.0084686279,46.3213197479), test loss: 33.9250406742\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.38119459152,4.53138902479), test loss: 2.6244320631\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (14.6128511429,46.0584336247), test loss: 40.0671352863\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.780607223511,4.48797822658), test loss: 3.46551665068\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (6.68069839478,45.7963721799), test loss: 34.9826133251\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.04199314117,4.4455653306), test loss: 2.39433868229\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (22.427822113,45.5338297173), test loss: 37.3556011677\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.39289665222,4.40458590488), test loss: 3.45035222173\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (29.3495407104,45.2649830257), test loss: 31.6549328327\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (6.63043498993,4.36519783022), test loss: 2.57613668293\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (20.3050384521,44.9863866014), test loss: 37.6303292036\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.21900963783,4.32679063848), test loss: 3.28599893749\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (31.1739120483,44.7061734173), test loss: 30.7348913431\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.939419865608,4.28946395486), test loss: 2.71404567361\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (78.3482666016,44.4170508957), test loss: 35.0992367744\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (2.36091756821,4.25330887124), test loss: 3.20968054533\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (11.1105909348,44.1189140406), test loss: 30.6721564531\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.62147188187,4.21813247403), test loss: 3.08999608755\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (34.270149231,43.8244278097), test loss: 33.8377744198\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (6.89751720428,4.1839808804), test loss: 3.28890621066\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (67.9725265503,43.527299612), test loss: 31.0599306107\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.69370222092,4.15034031563), test loss: 3.40559746623\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (11.9401435852,43.23046672), test loss: 30.1921680927\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.05423259735,4.117934857), test loss: 2.6917814672\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.0420818329,42.9283260888), test loss: 34.9963713646\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.95500707626,4.08644554285), test loss: 3.56664476991\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (18.0178375244,42.6306748913), test loss: 30.7546111584\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.43720507622,4.05620493149), test loss: 2.53886271119\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (19.5815620422,42.3339759917), test loss: 34.1068329811\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.78636097908,4.02656179853), test loss: 3.40326503217\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (15.0720081329,42.0332971249), test loss: 28.8675618172\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.22369062901,3.99780991804), test loss: 2.68503066599\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (18.5146751404,41.7354010093), test loss: 36.1365658283\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.22115182877,3.96977288654), test loss: 3.39433739781\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (9.41601753235,41.4450325415), test loss: 25.4602987289\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.20793294907,3.94243095761), test loss: 2.60399666131\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (20.6248092651,41.1595194003), test loss: 34.3878302574\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.7185356617,3.91545669857), test loss: 3.33627370298\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (23.2152862549,40.8782350755), test loss: 29.1639752626\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.777564883232,3.889699817), test loss: 3.27578777075\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (18.5063247681,40.5986573135), test loss: 34.9248681068\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (3.05637741089,3.8644479788), test loss: 3.37265997678\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (15.6084022522,40.3253837797), test loss: 30.7547947884\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.02855420113,3.83990023922), test loss: 3.45742856562\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (28.5968475342,40.058215545), test loss: 30.4517758846\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (4.09832429886,3.81616047474), test loss: 2.64889647961\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (22.1925029755,39.7900880087), test loss: 33.4478128433\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.53013038635,3.79275492413), test loss: 3.63936341703\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (16.0011940002,39.5294013658), test loss: 30.776292038\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.78122079372,3.77005807222), test loss: 2.58297955841\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (12.680978775,39.2772195514), test loss: 36.7491471529\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.30215644836,3.74754939546), test loss: 3.50991119444\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (13.9475269318,39.0297421996), test loss: 28.5202374458\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.404884159565,3.72566755022), test loss: 2.72817857414\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (12.9813957214,38.786456611), test loss: 37.1215980768\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.86191797256,3.70448234414), test loss: 3.45121985376\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (9.8378276825,38.5485889279), test loss: 24.9006491184\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.13743126392,3.68392049093), test loss: 2.49889698327\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (14.2820482254,38.317409188), test loss: 34.8984216928\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.16769576073,3.66378960337), test loss: 3.36289184093\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.3169841766,38.0874915806), test loss: 30.0147460937\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (1.79097318649,3.64412489748), test loss: 3.4362624079\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (38.4216766357,37.8630086352), test loss: 35.335453558\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.19445633888,3.62492419396), test loss: 3.4344574064\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (17.4993343353,37.6463513261), test loss: 31.0442424297\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.51259541512,3.6060313532), test loss: 3.41831493378\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (54.5140380859,37.4335082681), test loss: 31.6468233585\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (4.8336687088,3.58741905274), test loss: 2.6645203054\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (22.3258857727,37.2253104701), test loss: 35.0928962231\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.19988334179,3.56940143919), test loss: 3.55332440138\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (6.52196788788,37.0191013507), test loss: 29.9486091614\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.573454737663,3.55166713894), test loss: 2.44926228523\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (20.601316452,36.8201087369), test loss: 38.3014027596\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.91819286346,3.53458505338), test loss: 3.65075469613\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (14.2200965881,36.6255605376), test loss: 28.4296671391\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.07803308964,3.51785433587), test loss: 2.77574506998\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (14.3998908997,36.4314818948), test loss: 35.9772908211\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.12713932991,3.50134734969), test loss: 3.32381551862\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.17196512222,36.241783658), test loss: 24.7648644447\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.56691503525,3.48520158847), test loss: 2.51535772085\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (7.62874794006,36.0588761186), test loss: 36.3779120207\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.729779183865,3.46918099412), test loss: 3.37914086878\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (6.89228868484,35.8793149348), test loss: 30.6994645119\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (1.49198758602,3.45351632954), test loss: 3.43261237741\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (27.8576717377,35.7027219148), test loss: 30.0293774605\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (5.97128200531,3.43838570636), test loss: 2.87170629203\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (18.5365314484,35.5288769596), test loss: 34.0423547268\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.53748452663,3.42343404046), test loss: 3.49452196956\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (18.011844635,35.360043013), test loss: 31.0926791668\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.770436823368,3.40889858867), test loss: 2.65918578953\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (70.221572876,35.1937002093), test loss: 35.9351866245\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.83739805222,3.39469745599), test loss: 3.53261087239\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (4.35551071167,35.0278448053), test loss: 28.869584012\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (1.84291577339,3.38063990705), test loss: 2.62730905712\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (17.8859157562,34.8680903165), test loss: 36.9916300297\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.71938991547,3.36688765784), test loss: 3.52842967212\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (24.7611141205,34.7120422873), test loss: 28.1882410526\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.81277108192,3.3531508682), test loss: 2.75452241302\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (19.9694747925,34.5576785252), test loss: 35.0186863899\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.2138466835,3.33979036239), test loss: 3.21667082608\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (34.8166656494,34.4044041203), test loss: 31.9621997356\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.04891490936,3.3266369558), test loss: 3.18952488601\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (16.2852020264,34.2562106196), test loss: 35.6396811724\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.91425728798,3.31397847743), test loss: 3.43684376776\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (14.8076591492,34.1116455222), test loss: 29.8659286976\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.396990358829,3.30145966855), test loss: 3.37789200246\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (33.815864563,33.9662179474), test loss: 29.7288571835\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.53439748287,3.28914359448), test loss: 2.78188284934\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (20.0248718262,33.8237450957), test loss: 34.0402073145\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.77553582191,3.27702580883), test loss: 3.57645788193\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (18.669511795,33.6858865697), test loss: 30.4205086708\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.43218421936,3.26506510809), test loss: 2.46633101106\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (24.4148826599,33.5501668715), test loss: 35.4653683662\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.16453099251,3.25309190474), test loss: 3.46383576095\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (49.30235672,33.4167436404), test loss: 30.8053100109\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (2.08091616631,3.2416415002), test loss: 2.6821277827\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (19.9346618652,33.2838374706), test loss: 37.1565966129\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.60390925407,3.23022151764), test loss: 3.39221255183\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (13.638469696,33.1546394171), test loss: 28.2975422382\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.85165429115,3.21910805521), test loss: 2.76956532598\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (43.3936576843,33.0288612309), test loss: 34.5200065374\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (4.94669866562,3.20829964348), test loss: 3.37482109666\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (14.0461597443,32.901059862), test loss: 31.4075325966\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.35872507095,3.19746753424), test loss: 3.30433410406\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (16.29936409,32.7769390409), test loss: 35.2221986771\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.55149698257,3.18690710282), test loss: 3.42724439204\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (9.10713863373,32.6565412779), test loss: 31.2363092899\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.01444792747,3.17626106236), test loss: 3.48982327878\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (15.0660276413,32.5372721751), test loss: 31.1079566956\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.256277263165,3.1658495673), test loss: 2.80465276539\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (9.85410690308,32.4190284935), test loss: 34.141041708\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (3.71161746979,3.15568999084), test loss: 3.62743647695\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (17.2071609497,32.3029118442), test loss: 32.0537443161\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.64082181454,3.14577522278), test loss: 2.58311031163\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (13.5638399124,32.189618056), test loss: 35.0495467424\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.81674873829,3.13597103567), test loss: 3.44489973187\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (16.518119812,32.0759562425), test loss: 30.5218329906\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.95104432106,3.12633863249), test loss: 2.7263397783\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (29.7898216248,31.964460552), test loss: 36.0473806858\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (3.04031920433,3.11684610132), test loss: 3.42431108356\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (10.4839115143,31.8562837698), test loss: 26.0413027287\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.12656211853,3.1073953907), test loss: 2.6256611079\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (42.0639038086,31.749174367), test loss: 35.6201093197\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (4.49048137665,3.09802999854), test loss: 3.30544929504\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (15.6036605835,31.6433808356), test loss: 30.3693610907\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.33313155174,3.08888490458), test loss: 3.24429878592\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.61467170715,31.5375315087), test loss: 36.8943917274\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.871533155441,3.07976980911), test loss: 3.38626755178\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (22.4606666565,31.4353222582), test loss: 31.4506031036\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.07733154297,3.07098062775), test loss: 3.46316151321\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (14.7646865845,31.3346463553), test loss: 31.6587144375\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.53092885017,3.06233173724), test loss: 2.67037258297\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (11.9123649597,31.2327305872), test loss: 33.7946061373\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.21239578724,3.05368430192), test loss: 3.61227082014\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (8.37021541595,31.1330369806), test loss: 31.7269958496\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.54356193542,3.04519745706), test loss: 2.5481868729\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (24.3679428101,31.0366236733), test loss: 37.3870754004\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (3.39182901382,3.03669447521), test loss: 3.48210032284\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (7.31685447693,30.9406618654), test loss: 28.8466320515\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.26887130737,3.02827334343), test loss: 2.69018443227\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (22.4075660706,30.8456996264), test loss: 37.1071675777\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (4.3575425148,3.02012780179), test loss: 3.40251771808\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (23.5743522644,30.7516766228), test loss: 25.5350013733\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.91711550951,3.01200167797), test loss: 2.47495399117\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (16.0608787537,30.6599499744), test loss: 35.4947968721\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.27876091003,3.00408417638), test loss: 3.32527885735\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (75.944152832,30.56935047), test loss: 31.0374982834\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (4.51892757416,2.99631310085), test loss: 3.47549986541\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (3.63224935532,30.4772409861), test loss: 35.9280974388\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.867448568344,2.98853046244), test loss: 3.40918060541\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (15.6922874451,30.3887931152), test loss: 30.9281793594\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.66802000999,2.98088126591), test loss: 3.39163169861\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (25.2171726227,30.301881413), test loss: 31.8884937763\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.79693973064,2.97317304908), test loss: 2.61980280578\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (19.2941455841,30.2149846439), test loss: 35.1850028276\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.6786634922,2.9656440268), test loss: 3.48887708783\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (41.6565895081,30.1282293675), test loss: 30.005902195\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.65292525291,2.95815551655), test loss: 2.3878854841\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.58115959167,30.0439258087), test loss: 38.3787734032\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.46281814575,2.95095019051), test loss: 3.58099956512\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (8.36120414734,29.9614346386), test loss: 28.1301809788\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.449437469244,2.94378113669), test loss: 2.72825090289\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (31.259677887,29.877541928), test loss: 35.7024812222\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.11583220959,2.93668610712), test loss: 3.28599570096\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (15.3394365311,29.79520684), test loss: 24.6579047203\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.63989424706,2.92967784494), test loss: 2.48878311217\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (24.7004585266,29.7151370068), test loss: 35.083554697\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.34218883514,2.92269675266), test loss: 3.3064826116\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (21.6437072754,29.6356050279), test loss: 30.3633574724\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.614525139332,2.91567091678), test loss: 3.35632341206\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (43.5012664795,29.5571026853), test loss: 30.5359578609\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.25663566589,2.90893004202), test loss: 2.81357139945\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (16.0246810913,29.4781605308), test loss: 33.8697082281\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.83822619915,2.90214647937), test loss: 3.45573006272\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (9.60870170593,29.4013699314), test loss: 31.4367515326\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (3.16802978516,2.89555927273), test loss: 2.62101243138\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (27.9191665649,29.3263710876), test loss: 35.6993673086\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.21081733704,2.88909467668), test loss: 3.47600110173\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (15.1188116074,29.2492811468), test loss: 29.3240592241\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.58944272995,2.88260500856), test loss: 2.571363765\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (13.3333110809,29.1743837607), test loss: 36.6616480827\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (2.44399785995,2.87623039243), test loss: 3.44174184203\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (12.837053299,29.1014153897), test loss: 27.5718477249\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (3.79666090012,2.86977672027), test loss: 2.72706084549\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (16.0386142731,29.0286373469), test loss: 34.1921798706\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.66141843796,2.86343433639), test loss: 3.1339466095\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (10.5955762863,28.9560684663), test loss: 30.943320322\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.71215808392,2.8571916356), test loss: 3.16539228559\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (16.4605865479,28.8845226527), test loss: 35.8140084267\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.61078500748,2.85111158503), test loss: 3.34818846583\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (7.91790819168,28.814505144), test loss: 29.4894724369\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.991776764393,2.84504588611), test loss: 3.2942864567\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (31.6100597382,28.7438783796), test loss: 30.2598475933\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.25076889992,2.8390907077), test loss: 2.7570764035\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (12.9166717529,28.6742283718), test loss: 33.6994157791\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.880071640015,2.83319283769), test loss: 3.53880828917\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (4.28862762451,28.6064362575), test loss: 30.6497042179\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.88812887669,2.82728191085), test loss: 2.4525604248\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (15.7014436722,28.5390848899), test loss: 35.406309557\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (4.26168060303,2.82141689269), test loss: 3.36486383677\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (15.8292274475,28.4721327841), test loss: 30.7784749031\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.51888680458,2.81564395379), test loss: 2.6311958015\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.05879497528,28.4047733088), test loss: 36.7028628111\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.33875870705,2.80986220092), test loss: 3.32194952369\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (16.013376236,28.3396697937), test loss: 27.5476494312\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (2.78511142731,2.80428591817), test loss: 2.68694855571\n",
      "\n",
      "MC # 4, Hype # hyp4, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_4/fold10/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (301.839416504,inf), test loss: 195.698167801\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (344.878051758,inf), test loss: 387.999380493\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (35.6330184937,99.1318540411), test loss: 42.8478560448\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (5.16332244873,117.361451125), test loss: 3.10066081882\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (80.076423645,73.7743118515), test loss: 36.4923665047\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (4.55373716354,60.2518280865), test loss: 3.25048847198\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (34.0217285156,65.2658127035), test loss: 39.4333782196\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (6.89490222931,41.2161551704), test loss: 3.29671360254\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (39.3529510498,60.9467647204), test loss: 39.2194083214\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (2.55944466591,31.6895976962), test loss: 3.30584029257\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (37.0573654175,58.4302757629), test loss: 43.4310055733\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.22874426842,25.9805798938), test loss: 2.96171036959\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (53.2950210571,56.6657696416), test loss: 44.2685139656\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.45322966576,22.1749043319), test loss: 3.37567988038\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (113.721572876,55.4159335066), test loss: 43.5548351288\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (4.83721542358,19.4614432535), test loss: 3.06040818989\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (81.5810546875,54.4478760383), test loss: 42.9710356712\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (6.56374645233,17.4241852722), test loss: 3.18073610067\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (32.4259872437,53.6430169977), test loss: 39.8242734909\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.25512886047,15.8403924746), test loss: 3.20964766741\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (123.921600342,52.9918140244), test loss: 42.8944557667\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (8.71736717224,14.5739183926), test loss: 2.93835060894\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (30.6009635925,52.4550321957), test loss: 36.5564541101\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (7.17827653885,13.5379030415), test loss: 2.90464487374\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (48.2912139893,51.9970493674), test loss: 40.5619739056\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (3.5390791893,12.6723042496), test loss: 3.27132203877\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (39.3559494019,51.6331044523), test loss: 35.2152136803\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.28994607925,11.9422826026), test loss: 3.09805600047\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (35.6625061035,51.283562075), test loss: 41.0299557686\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.30564975739,11.3168048411), test loss: 2.96808853149\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (105.207397461,50.9870388533), test loss: 40.2832573891\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (4.8128156662,10.7769633203), test loss: 3.13291270137\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (60.540977478,50.7049142423), test loss: 42.9217812777\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.4263920784,10.3032545034), test loss: 3.03843618035\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (30.4905357361,50.4297014449), test loss: 42.435372448\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.39479470253,9.88571036651), test loss: 3.18460038602\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (125.23274231,50.1798495966), test loss: 39.4174287796\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (6.365711689,9.51478588032), test loss: 3.21787666082\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (36.028591156,49.952770943), test loss: 42.5016089916\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (7.76287889481,9.18298625944), test loss: 3.23343779445\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (32.2127227783,49.7387107259), test loss: 37.5546290874\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (4.22801399231,8.88272175655), test loss: 3.04028684795\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (32.1715621948,49.5587401015), test loss: 41.5771518707\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.53276157379,8.61236200657), test loss: 3.25892804861\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (33.5925827026,49.3697189627), test loss: 34.6484007359\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.8716725111,8.3667026973), test loss: 3.07471443713\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (44.627746582,49.1990731251), test loss: 38.0117071867\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.27623128891,8.14364693246), test loss: 3.23541078568\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (28.2946510315,49.0261585619), test loss: 37.106537199\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.05583620071,7.93822690447), test loss: 3.27139697075\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (25.2098731995,48.8470894854), test loss: 40.7564371586\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.41373586655,7.74933669355), test loss: 2.95922400653\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (37.9283981323,48.6755071347), test loss: 41.8522879601\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.08402824402,7.57493505317), test loss: 3.29789520502\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (38.2087402344,48.5120002402), test loss: 41.2918608665\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.37035322189,7.41340078511), test loss: 3.03921950161\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (56.4768066406,48.3528655905), test loss: 40.6534311295\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (5.91287994385,7.2622395159), test loss: 3.32511995435\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (21.8822956085,48.2068750789), test loss: 36.9085851192\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (4.00674104691,7.12203294436), test loss: 3.02631731033\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (83.3562164307,48.0553341725), test loss: 40.2105958462\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.44821119308,6.99095374842), test loss: 3.04044633508\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (38.2569465637,47.9047143755), test loss: 34.0000398159\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.14921712875,6.86911833496), test loss: 2.89612048566\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (38.0009155273,47.7535426789), test loss: 37.9679464817\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.47365045547,6.75413523179), test loss: 3.06637298167\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (98.0932922363,47.5929176881), test loss: 32.1763705254\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.34301304817,6.64560188053), test loss: 3.03307716846\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (28.7834091187,47.4117859356), test loss: 37.3146319389\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.978315770626,6.54163681495), test loss: 2.83653880656\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (27.8862323761,47.2276334295), test loss: 36.3037500381\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.80796480179,6.44243751974), test loss: 3.0480203867\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (46.4613571167,47.0377569642), test loss: 38.3727654696\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (5.34976673126,6.34767121292), test loss: 2.8607948035\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (20.8254585266,46.845266587), test loss: 38.266890955\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.52480602264,6.25898334761), test loss: 3.27258290052\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (76.550163269,46.6423214629), test loss: 34.5898639202\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.51717603207,6.17455761967), test loss: 2.98057536483\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (17.648475647,46.4337475172), test loss: 37.252893424\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.95123648643,6.0948867516), test loss: 3.08997162282\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (46.630821228,46.219881325), test loss: 32.5458553791\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (5.06925153732,6.01834014361), test loss: 3.02195495367\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (92.286315918,45.9933279667), test loss: 35.2747262478\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (3.77327156067,5.94476790898), test loss: 2.89197635651\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (16.0185928345,45.7555074709), test loss: 29.4348209381\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.5417277813,5.87373843769), test loss: 2.9197597146\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (29.8963546753,45.518529584), test loss: 32.3933124065\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (4.53497076035,5.80530783405), test loss: 2.99990904927\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (38.9917907715,45.2752447877), test loss: 30.3356854916\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.49115610123,5.73814217237), test loss: 3.09469401985\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (18.5015926361,45.0292276886), test loss: 32.9255930424\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.1674990654,5.67367250157), test loss: 2.69092597961\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (17.3035202026,44.7721749187), test loss: 33.996492815\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (4.19083023071,5.61128717506), test loss: 3.15702631474\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (17.1738853455,44.507845982), test loss: 33.4957948208\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.81610655785,5.55130724434), test loss: 2.7534796536\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (43.6185264587,44.2365731024), test loss: 32.8010590553\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (4.24115896225,5.49307470256), test loss: 2.99451280832\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (88.823097229,43.9537413996), test loss: 29.0869327307\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (6.24930715561,5.4365697056), test loss: 2.84756896496\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (110.286567688,43.6613841115), test loss: 31.6247394085\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.86655044556,5.38146308057), test loss: 2.68796803057\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (14.6078414917,43.3625442948), test loss: 26.0856592178\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (4.06696891785,5.32814979947), test loss: 2.6244327873\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (15.9498195648,43.0628934641), test loss: 29.0817352295\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.40467214584,5.27555267574), test loss: 2.88016470075\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (14.2671432495,42.7604796657), test loss: 23.2296521664\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.00298404694,5.22475029437), test loss: 2.89219289422\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (8.26893424988,42.4510083267), test loss: 28.0021419525\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.73031520844,5.1753331562), test loss: 2.75300011039\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (14.5097999573,42.1399229931), test loss: 27.8812043667\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.37170398235,5.12752034084), test loss: 2.97646119893\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (18.0053443909,41.8284280311), test loss: 29.433805275\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.93859636784,5.08095435634), test loss: 2.69490168244\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (65.5065231323,41.5151717687), test loss: 30.0064580441\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (4.84864664078,5.03553043081), test loss: 2.94734714031\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (86.8170471191,41.1982123516), test loss: 27.2678702593\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.45959234238,4.99100420371), test loss: 2.79116648585\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (9.92945289612,40.8857099204), test loss: 28.8815577745\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.69241333008,4.94784090106), test loss: 2.90079887509\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (9.38728904724,40.5800120851), test loss: 26.2579825401\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (3.10036754608,4.90512028234), test loss: 2.69120154381\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (6.58013534546,40.2792068564), test loss: 27.6348301888\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.399551272392,4.86383916452), test loss: 2.86700381339\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (7.69283008575,39.9795413932), test loss: 25.8591270924\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (4.12839603424,4.82367279695), test loss: 2.8939099133\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.7539415359,39.6865688375), test loss: 26.7601164579\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.57205295563,4.78482941142), test loss: 2.95170746446\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.0713615417,39.3999734112), test loss: 24.7661209106\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.75228500366,4.74703423532), test loss: 3.09889049232\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (61.7301940918,39.1185509458), test loss: 28.4390788555\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.77957749367,4.71018254832), test loss: 2.5893535994\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (30.8983631134,38.838365631), test loss: 28.4964541316\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.6348938942,4.67405351088), test loss: 3.09269768596\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (7.30569076538,38.5661077859), test loss: 30.2937672615\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.62575745583,4.63899816335), test loss: 2.67117317617\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (15.4117126465,38.3034748651), test loss: 28.6853023767\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (4.71184825897,4.60433609708), test loss: 3.00789815784\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (5.78693246841,38.047039604), test loss: 28.0737340927\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (0.343085199594,4.57075073473), test loss: 2.76899420023\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (12.4339342117,37.7937910685), test loss: 28.8798834324\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.38498449326,4.53812302478), test loss: 2.80211336613\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (23.6979351044,37.5485208112), test loss: 26.0768113136\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.76239848137,4.50656367276), test loss: 2.61710214317\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (20.673740387,37.3096470543), test loss: 28.8144283772\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.09955716133,4.4757590568), test loss: 2.92990138233\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (19.3014736176,37.0764597173), test loss: 24.0288675129\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.03438305855,4.4456995452), test loss: 3.01394905895\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (21.1722278595,36.845082133), test loss: 28.4924914122\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.00643134117,4.41620890053), test loss: 2.93884467185\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (11.4639434814,36.6204855115), test loss: 27.2863361359\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.644398272038,4.38752036242), test loss: 3.04493741095\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.6296291351,36.4041796682), test loss: 30.1979999542\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (4.5316901207,4.35916045734), test loss: 2.67744444013\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (13.14731884,36.1928870987), test loss: 29.3471852303\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.378156840801,4.33154506039), test loss: 3.08062246442\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (25.5959205627,35.9835267567), test loss: 28.5924988747\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.73903572559,4.3046972419), test loss: 2.74574702308\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (32.6303405762,35.7802020833), test loss: 29.2343045712\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.97077894211,4.27865710976), test loss: 2.96386137903\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (14.9687175751,35.5819637969), test loss: 28.119124198\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.88652408123,4.25321412412), test loss: 2.80395852625\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (25.5276298523,35.3886831389), test loss: 29.426684165\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (0.781947374344,4.22833037214), test loss: 2.86538244784\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (23.6821022034,35.1960155432), test loss: 28.7441894531\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.87158679962,4.20387747546), test loss: 3.02800152898\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (12.8008117676,35.0089000985), test loss: 28.1125075102\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (2.34487080574,4.18005369783), test loss: 3.03023321331\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (7.46978855133,34.82871279), test loss: 25.4042527795\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.96078920364,4.15637442919), test loss: 3.17702231109\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (13.7344741821,34.6525781146), test loss: 29.1391843081\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.81227469444,4.13331681925), test loss: 2.62252317518\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (45.1773147583,34.4780697141), test loss: 28.9961528659\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.01413369179,4.11081331508), test loss: 3.12027171552\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (24.1755313873,34.3075487986), test loss: 31.4879829884\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.43354642391,4.08895201229), test loss: 2.69907113612\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (11.2345781326,34.1412916062), test loss: 28.0586270094\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.990939736366,4.06758349003), test loss: 2.93799509406\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (21.2929458618,33.9790393416), test loss: 30.9685413361\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.864174425602,4.04663948894), test loss: 2.83138895631\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (19.4097003937,33.8164383638), test loss: 30.1768717527\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.89800405502,4.02603191564), test loss: 2.84135567993\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (20.7399578094,33.6581658888), test loss: 26.7032026291\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.1636660099,4.0058464387), test loss: 2.65451977253\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (4.01017999649,33.5047137098), test loss: 29.4391724586\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.26417732239,3.9857278421), test loss: 2.92724823356\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (13.1315813065,33.3545720262), test loss: 23.7830968142\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.03535604477,3.96610334669), test loss: 3.07494781911\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (34.7492141724,33.2049189477), test loss: 28.2078763247\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.50017786026,3.94689356173), test loss: 2.99836820364\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.52742862701,33.0586897771), test loss: 27.0505889654\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.0522248745,3.92818696965), test loss: 3.00128210783\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (7.96866083145,32.9159485643), test loss: 30.2316679478\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.16383552551,3.90988566468), test loss: 2.69657014608\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (10.7938718796,32.7764644914), test loss: 30.9709217787\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.821487665176,3.89191632885), test loss: 3.09575321078\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (15.0023355484,32.6366651279), test loss: 28.848496151\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.66926145554,3.87420204331), test loss: 2.7610923782\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (21.8087978363,32.5005322111), test loss: 29.8965208769\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.00468230247,3.85681398976), test loss: 3.03668371141\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (6.13891744614,32.3687196506), test loss: 28.3284581423\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.5542447567,3.83948421351), test loss: 2.77147945166\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (17.7356338501,32.2396702612), test loss: 28.5733797193\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (3.11030673981,3.82255782412), test loss: 2.86916692257\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (28.7461013794,32.1106536798), test loss: 27.6479095459\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (3.13913416862,3.80599368132), test loss: 3.03661532253\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (7.89144992828,31.9842009002), test loss: 28.996763587\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.32590448856,3.78980451924), test loss: 3.0147587955\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (10.0408096313,31.8606275136), test loss: 25.5346315861\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.941809952259,3.77398310453), test loss: 3.17381848991\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (8.65010166168,31.7394873345), test loss: 29.923413527\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.860190391541,3.75842801259), test loss: 2.6629264459\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (16.972366333,31.6176566393), test loss: 28.9562511206\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.72180938721,3.74306350572), test loss: 3.1304792881\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (12.147772789,31.4986974658), test loss: 30.9578704596\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.17362308502,3.72793981765), test loss: 2.68638489246\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (8.40795516968,31.3835631136), test loss: 27.9401420236\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.85075092316,3.71284537751), test loss: 2.94886934161\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (21.5089302063,31.2708357359), test loss: 29.4193527699\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (5.47988319397,3.69816140071), test loss: 2.81504358947\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (15.3432731628,31.1578374616), test loss: 29.5030253887\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.75699615479,3.68365383261), test loss: 2.91006581038\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (3.89328360558,31.0470732981), test loss: 27.0779249668\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.45634043217,3.66949815389), test loss: 2.7295561105\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (8.41123199463,30.9389862299), test loss: 29.5680360794\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.58467078209,3.65565326627), test loss: 2.92889506519\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (25.9254875183,30.8333492845), test loss: 24.9471011877\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.12843966484,3.64204506773), test loss: 3.11132471859\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (20.04596138,30.7263020593), test loss: 28.9323383808\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.79664194584,3.62853570419), test loss: 3.01393519938\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (4.79358196259,30.6217360004), test loss: 25.7023090661\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.63393628597,3.61527296109), test loss: 3.02194325626\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.09058189392,30.5205496269), test loss: 30.1792902946\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.01395320892,3.60199926092), test loss: 2.66207047999\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (22.7949295044,30.421281144), test loss: 29.4944803238\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (6.84852790833,3.58912297155), test loss: 3.10071976334\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (19.0450897217,30.321156881), test loss: 28.3711958885\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (6.0589966774,3.57636229383), test loss: 2.7289934516\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (4.68848228455,30.2227301622), test loss: 30.4423307896\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.80404543877,3.56379878887), test loss: 3.05432012677\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.72192716599,30.1266550736), test loss: 28.2940279007\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.39808940887,3.55156513624), test loss: 2.78813218921\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (36.3193626404,30.0327954076), test loss: 29.8318030357\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (4.3720831871,3.53953181964), test loss: 2.83207488656\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (17.1536140442,29.937201418), test loss: 28.2700536013\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.40958440304,3.52754955437), test loss: 3.11265765727\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (14.9134950638,29.8442004628), test loss: 29.8485687733\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.33498740196,3.5157950755), test loss: 3.02043402642\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (21.419921875,29.7542461311), test loss: 25.2375986338\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (3.21282529831,3.50403555555), test loss: 3.12490522861\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (15.289355278,29.665760809), test loss: 28.4722435772\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (5.9254732132,3.49255513954), test loss: 2.61083410978\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (22.5588417053,29.5768093144), test loss: 28.9634050488\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (6.13988018036,3.48119421716), test loss: 3.13103962839\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (5.67571210861,29.4890074352), test loss: 31.090900135\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.48658061028,3.46999134503), test loss: 2.62589919716\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (9.90615653992,29.4034269365), test loss: 28.5926746964\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.38460373878,3.45909924451), test loss: 2.93966553807\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (42.3510322571,29.3197133791), test loss: 30.6454418182\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (4.69295310974,3.44838625038), test loss: 2.81941452026\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (18.2540206909,29.2340727141), test loss: 30.0996278644\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.76041293144,3.43771956696), test loss: 2.94225442857\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (14.4775123596,29.1504217206), test loss: 26.6698762417\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.35800337791,3.42719653691), test loss: 2.72527469993\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (23.8485870361,29.0695040555), test loss: 29.3081110477\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (5.42719650269,3.41669701387), test loss: 2.84492311627\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (10.9776821136,28.9896407328), test loss: 24.3784250498\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.43643188477,3.40636784707), test loss: 3.0771329999\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (17.9783763885,28.9092078673), test loss: 28.5078997016\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (4.42372131348,3.39617502082), test loss: 2.99262398034\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (12.6054515839,28.8298875904), test loss: 25.844015038\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.708859920502,3.38611173181), test loss: 2.99822813272\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (11.7754325867,28.7525027968), test loss: 30.2628342628\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.864750027657,3.37632589452), test loss: 2.65544991195\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (17.3187408447,28.6770071708), test loss: 30.3392829418\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.33790826797,3.36670213089), test loss: 3.1202807188\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (25.2433013916,28.5998120076), test loss: 28.7675915718\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (3.9961373806,3.35710955601), test loss: 2.73132903278\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (3.93022060394,28.5242622547), test loss: 30.6208872557\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.27725076675,3.34760521991), test loss: 3.09168550372\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (25.9071521759,28.4514170626), test loss: 28.0186799526\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (6.0787820816,3.33814170031), test loss: 2.75187868178\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (11.6280126572,28.3794091513), test loss: 29.0984578371\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.717895150185,3.32882332151), test loss: 2.78914296627\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (17.8655776978,28.3066500881), test loss: 27.3938234806\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.8839880228,3.31963568291), test loss: 3.08000445068\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (27.0440864563,28.2349251559), test loss: 30.4886382341\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.50998449326,3.31055295539), test loss: 2.98708352745\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (9.80653095245,28.1644396092), test loss: 24.6371626377\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.14753067493,3.30171143747), test loss: 2.99510023892\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (18.3830986023,28.0957001822), test loss: 28.4911246479\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.38998818398,3.2929974823), test loss: 2.60688965917\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (25.7143878937,28.0250892347), test loss: 29.1664379358\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (3.53584837914,3.28429788945), test loss: 3.13221372366\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (4.96632289886,27.9559655257), test loss: 30.3337254047\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.06604385376,3.27567322086), test loss: 2.60465754867\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (13.2813177109,27.8894018884), test loss: 28.9438958406\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (5.84346485138,3.26713309657), test loss: 2.95894443393\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (10.1016159058,27.8234900354), test loss: 29.5349998474\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.931110262871,3.25862500957), test loss: 2.74626151025\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (26.8939056396,27.7572989162), test loss: 29.3439059258\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (3.84749174118,3.2503027359), test loss: 2.94120064974\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (24.1617679596,27.6915988738), test loss: 27.3545485497\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.02484917641,3.24200112541), test loss: 2.71743504405\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (13.0583143234,27.6274731599), test loss: 29.1600845337\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.11847281456,3.23395001926), test loss: 2.76074115932\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (10.3349580765,27.5647830437), test loss: 25.7102417469\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.542942166328,3.22601304573), test loss: 3.13822947145\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (6.27620983124,27.5003911343), test loss: 29.3685950994\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.610029459,3.21809267725), test loss: 2.97406662703\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (6.5959815979,27.4373385128), test loss: 24.9790709853\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.243308633566,3.21024406153), test loss: 3.00459337234\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (18.956325531,27.37653032), test loss: 30.6794686794\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (5.73837375641,3.20247796914), test loss: 2.63092364073\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.66734647751,27.3160325319), test loss: 29.7412893057\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.17497777939,3.19471759794), test loss: 3.11706741452\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (29.6130466461,27.2551003523), test loss: 28.1028831482\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.44193673134,3.18711536128), test loss: 2.68098571897\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (23.9687423706,27.1944621054), test loss: 30.9701929569\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.39376330376,3.17951717991), test loss: 3.08044494092\n",
      "run time for single CV loop: 7181.8370502\n",
      "\n",
      "MC # 5, Hype # hyp5, Fold # 6\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold6/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (312.161224365,inf), test loss: 207.60317955\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (258.688842773,inf), test loss: 298.728373718\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (24.6418209076,85.9426202459), test loss: 45.332351017\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.32227468491,29.0372511303), test loss: 3.29002683759\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (28.2656059265,64.5271948328), test loss: 39.0969464779\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.84152173996,16.155365309), test loss: 3.35774691701\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (29.0940685272,57.3900684948), test loss: 44.7866564751\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.19517850876,11.8491724165), test loss: 3.25497544408\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (14.031744957,53.8076846662), test loss: 39.4445447445\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.722734093666,9.7020894092), test loss: 3.34511876404\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (13.9020957947,51.5140176361), test loss: 40.3166752815\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.20715653896,8.41914734149), test loss: 2.87194325924\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (48.7066040039,49.9138242691), test loss: 41.5125702858\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.32820510864,7.56314672615), test loss: 3.33926189542\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (57.720790863,48.7040054874), test loss: 40.7846186161\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.74462652206,6.94637538849), test loss: 2.7040035367\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (158.180740356,47.6884367236), test loss: 43.7840778351\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.92557525635,6.48283423642), test loss: 3.56931134462\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (45.1643447876,46.8575444335), test loss: 40.0865747452\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (8.837433815,6.11985340003), test loss: 2.77459344864\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (18.3537826538,46.168427944), test loss: 41.4184692383\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (5.89894628525,5.82609837002), test loss: 3.51661410928\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (24.2302951813,45.5957307476), test loss: 36.6079826832\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.3046091795,5.58678086315), test loss: 2.74769204855\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (45.9250106812,45.0577878542), test loss: 42.8288239479\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.3134458065,5.38648710744), test loss: 3.36207799911\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (27.2452030182,44.5577555548), test loss: 36.167771244\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.65538096428,5.21435429242), test loss: 3.01169254184\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (54.5339698792,44.0579573903), test loss: 41.4287077427\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (5.15819644928,5.06530458618), test loss: 3.10363586247\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (28.9300994873,43.5729958733), test loss: 37.4833580017\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.77687418461,4.93515044291), test loss: 3.21941808462\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (51.1745605469,43.1370419599), test loss: 39.3108613968\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.532766819,4.81845983758), test loss: 3.09218687415\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (24.2574234009,42.72002202), test loss: 34.810577774\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.89068984985,4.71369627681), test loss: 3.14667389691\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (19.1543884277,42.3074806061), test loss: 33.8310771942\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.97572565079,4.61921221027), test loss: 2.58790976405\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (26.5002784729,41.8943517298), test loss: 37.8374723911\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (0.975767552853,4.53415852049), test loss: 3.10003702343\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (45.2167282104,41.4839219857), test loss: 35.7355667591\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.0228073597,4.45411305685), test loss: 2.49153301716\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (33.0920410156,41.0608230694), test loss: 39.1214741826\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.70012187958,4.38017522582), test loss: 3.33391442895\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (25.3189735413,40.6303751051), test loss: 32.618131566\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.59973144531,4.31059173745), test loss: 2.54275477231\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (43.2774429321,40.2234417882), test loss: 36.6736028671\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.57126188278,4.24413701144), test loss: 3.15940310359\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (34.9386405945,39.822671711), test loss: 30.9458003521\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.25246298313,4.18194401294), test loss: 2.5496891439\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (34.885383606,39.4107441916), test loss: 38.2264944553\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.69661211967,4.12306991799), test loss: 3.22319631577\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (19.5302295685,39.0000681721), test loss: 30.0875689507\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.577916443348,4.06636856051), test loss: 2.72489087284\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (80.7107009888,38.5913653848), test loss: 36.4893516064\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.95222663879,4.01307175008), test loss: 2.93177958429\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (67.3754806519,38.1661130387), test loss: 32.5536096573\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.71875405312,3.9604659017), test loss: 3.15511568785\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (42.8039245605,37.7638455777), test loss: 35.5486096859\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.22022628784,3.90869820298), test loss: 3.05515183806\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (20.2226276398,37.366954179), test loss: 31.084299922\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.83042955399,3.85801746888), test loss: 3.16823237836\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (31.0341129303,36.9809693661), test loss: 28.5058772564\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (6.91913890839,3.80992982054), test loss: 2.41366792023\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (13.1069831848,36.601155807), test loss: 32.9289426327\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.830840349197,3.76357322291), test loss: 3.23377531767\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (18.2614250183,36.2291408775), test loss: 31.2971893787\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (0.521975755692,3.71942285541), test loss: 2.40493503213\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (14.5258483887,35.8552503424), test loss: 34.7475145817\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.4640455246,3.67759223766), test loss: 3.2937686801\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (30.8845214844,35.4922726018), test loss: 29.3909530163\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.82843959332,3.63743831025), test loss: 2.37133948207\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (7.07158374786,35.1480254917), test loss: 34.6137352705\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (4.46988487244,3.59823849058), test loss: 3.10946051478\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (10.4978666306,34.8146970288), test loss: 29.5696958065\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.53641939163,3.56099744169), test loss: 2.73090500832\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (10.3340606689,34.484410281), test loss: 37.3985178947\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.4744515419,3.52541163061), test loss: 3.25869933963\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (44.2823867798,34.1686774472), test loss: 28.497378397\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.20053637028,3.49194844656), test loss: 2.76928182393\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (31.5365715027,33.8644016904), test loss: 35.313510561\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.38635253906,3.4600917737), test loss: 3.05920105129\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (6.58494949341,33.5559449678), test loss: 31.6848497868\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.580134272575,3.42888207864), test loss: 3.2897074312\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (19.0090751648,33.2660475183), test loss: 35.6153890133\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.32641625404,3.39873682147), test loss: 3.17565101683\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (14.2799100876,32.9860299483), test loss: 30.8613270283\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.22149515152,3.3695073778), test loss: 3.28195058107\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (19.9657707214,32.7155300374), test loss: 28.8321251154\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.16084241867,3.3418044254), test loss: 2.45052676499\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (2.37555170059,32.4506014992), test loss: 33.1419156313\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.3793284893,3.31495297094), test loss: 3.24071424305\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (11.2191314697,32.195161038), test loss: 30.9885002613\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.80449151993,3.2894209958), test loss: 2.40202299207\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (17.8463230133,31.9420512617), test loss: 33.6103513241\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.604327917099,3.26474994969), test loss: 3.29940224886\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.5151233673,31.6960818849), test loss: 30.018651557\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.929325401783,3.24082451795), test loss: 2.42299737483\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (19.9213790894,31.4617474573), test loss: 35.2298246861\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (5.46278142929,3.21718694624), test loss: 3.14961258918\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (13.6928243637,31.2351784547), test loss: 29.8789985657\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.73804867268,3.194350021), test loss: 2.81922025084\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (55.5253639221,31.0114439977), test loss: 37.1116202354\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.73725247383,3.17241441987), test loss: 3.25721567273\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (10.3406991959,30.7941160403), test loss: 27.4488882303\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.55801224709,3.15143034561), test loss: 2.80040057003\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (10.3048963547,30.584585166), test loss: 35.0677345991\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.65125721693,3.13120193527), test loss: 3.08404197991\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.9934711456,30.3742766867), test loss: 31.9555880249\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.08275151253,3.11138686378), test loss: 3.28378040791\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (14.4002952576,30.1708917609), test loss: 35.4812272072\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.73935341835,3.09205335031), test loss: 3.18981961608\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (43.2574005127,29.9753910264), test loss: 30.5379928589\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.27342176437,3.07273868843), test loss: 3.25588160753\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (38.4714775085,29.7838876095), test loss: 29.9914035797\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.05247092247,3.05424170723), test loss: 2.47823857069\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (17.9740009308,29.5942614449), test loss: 33.5496951103\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.59455347061,3.0362519407), test loss: 3.23217915595\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (8.61601924896,29.4103167779), test loss: 31.6298208237\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.448835194111,3.01897429444), test loss: 2.42760036886\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (20.8579730988,29.2293431411), test loss: 33.5155915499\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.22939920425,3.00228444124), test loss: 3.21723957658\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (15.45362854,29.0493007038), test loss: 30.0072766304\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.3844306469,2.98571326949), test loss: 2.45874044597\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (49.3557739258,28.8782149785), test loss: 34.9689088821\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.77644491196,2.96952862476), test loss: 3.10330388248\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (7.34451627731,28.7089471039), test loss: 29.9182239532\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.54017615318,2.95344478554), test loss: 2.7618961513\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (8.68410110474,28.5417539749), test loss: 36.8344618797\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.35041832924,2.93784674976), test loss: 3.19134584069\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (32.3999404907,28.3783879197), test loss: 27.7175443649\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.93229115009,2.92281652833), test loss: 2.77819812596\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (22.0238838196,28.2194986862), test loss: 35.0569635868\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.711313784122,2.90829626623), test loss: 3.03109076619\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (10.9107170105,28.0587410688), test loss: 31.6480736732\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.35521364212,2.89397591986), test loss: 3.21941349506\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (15.8214645386,27.9027025893), test loss: 35.8094301701\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.914915859699,2.87986797649), test loss: 3.15588029921\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (4.94896936417,27.7512877269), test loss: 30.3164016724\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (0.700595617294,2.86560125182), test loss: 3.19857142568\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (4.73427343369,27.6025076257), test loss: 30.5209319592\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.98907101154,2.85200848586), test loss: 2.49388101101\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (8.67349529266,27.4537072766), test loss: 34.2552797794\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.41575074196,2.83849758704), test loss: 3.16866095066\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (15.4913463593,27.3089843971), test loss: 31.9802638531\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.8332567215,2.82550918892), test loss: 2.42745895386\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.4682579041,27.1677947721), test loss: 36.2413137913\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.4055762887,2.8128962876), test loss: 3.27263447046\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (40.7630004883,27.0244535907), test loss: 29.7601739407\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.63101220131,2.80038783917), test loss: 2.44733174443\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (6.17193126678,26.8869173446), test loss: 35.1923699856\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.73937129974,2.78784701263), test loss: 3.07098977268\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (22.4675750732,26.7506266185), test loss: 29.4432478428\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.434934794903,2.77549199203), test loss: 2.76376764178\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (23.8907375336,26.6156259044), test loss: 36.5753439903\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.75435972214,2.76356131502), test loss: 3.15797365606\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (19.8643035889,26.4830887572), test loss: 29.0641790867\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.02273130417,2.7517441608), test loss: 2.67530979216\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (6.98382282257,26.3532959166), test loss: 35.3323298454\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.872979283333,2.74021694556), test loss: 2.94241125286\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (6.74865627289,26.2224570458), test loss: 31.7818648815\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.57296705246,2.72901165132), test loss: 3.20193793476\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (11.5666179657,26.0940140459), test loss: 36.065635848\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (3.1638815403,2.71789861414), test loss: 3.09433890879\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (7.18424987793,25.9691286806), test loss: 30.5104629993\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.38156402111,2.70665937557), test loss: 3.15493412316\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (4.70636367798,25.8464994523), test loss: 30.8540286779\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.357190698385,2.69570651955), test loss: 2.53539401889\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (15.6903877258,25.7221265975), test loss: 34.0469347477\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.75837278366,2.68486576081), test loss: 3.10542418063\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (7.75255727768,25.6017358146), test loss: 32.4619797707\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.49551868439,2.67444159541), test loss: 2.40392889678\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (6.40982627869,25.4836367551), test loss: 34.884100008\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.16212058067,2.66427860329), test loss: 3.09980784357\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (34.4882354736,25.3637214978), test loss: 31.4250434875\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.02004671097,2.65409182354), test loss: 2.51055969\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (16.144865036,25.2465981043), test loss: 35.7975469112\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (4.3650636673,2.64400207261), test loss: 3.01261077821\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (6.40390968323,25.1311101452), test loss: 29.4652863503\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.76671624184,2.63386313133), test loss: 2.79164472818\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.421875,25.0176006473), test loss: 37.526962471\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.980206012726,2.62402867094), test loss: 3.03253152072\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (15.280412674,24.9039760651), test loss: 28.6647915363\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.93894052505,2.61421458094), test loss: 2.71801221073\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (8.22504329681,24.7925505178), test loss: 35.9013744354\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (0.851221859455,2.60472239811), test loss: 2.86985422969\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (39.4690628052,24.6812268296), test loss: 31.5788151741\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.78900885582,2.59545579495), test loss: 3.17473194599\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (19.0702629089,24.5712572235), test loss: 36.1505065441\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.21926641464,2.58619616813), test loss: 3.08650219738\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.6761932373,24.4636427758), test loss: 29.7648817301\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.29345500469,2.57689055213), test loss: 3.1600371182\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (14.8794660568,24.3571609978), test loss: 31.4723882198\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.01126718521,2.56773195639), test loss: 2.51396430135\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (8.42077064514,24.2502257064), test loss: 32.4919275284\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.67919301987,2.5586734394), test loss: 3.10817047954\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (9.25497817993,24.1447245886), test loss: 33.4955502987\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.49766147137,2.54983896328), test loss: 2.40272186399\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (15.1609258652,24.0422704861), test loss: 35.3027161121\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.13013517857,2.54116604262), test loss: 3.08438672423\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (6.56068134308,23.9374527921), test loss: 31.3561043978\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.56055831909,2.53262017642), test loss: 2.5876437664\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.11702537537,23.8352629667), test loss: 36.3153161526\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.74746888876,2.52411624032), test loss: 2.99541907609\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (17.2868785858,23.7352248079), test loss: 29.9702723503\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.00192165375,2.51549280462), test loss: 2.80503131151\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (10.3719177246,23.6351149545), test loss: 37.4748837471\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.975474774837,2.50707038576), test loss: 2.91493323445\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (15.0008277893,23.5349023377), test loss: 28.7565644264\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.989560484886,2.49868471614), test loss: 2.67782688141\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (10.5012130737,23.4365418451), test loss: 36.3791138172\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.687189638615,2.49057025662), test loss: 2.93976925611\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (41.0616874695,23.339947257), test loss: 33.7304840565\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.58712291718,2.48266959064), test loss: 3.35819628537\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (16.7307529449,23.2420116855), test loss: 36.5563007832\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.64537620544,2.47467728447), test loss: 3.05209783912\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (16.1183319092,23.1467104141), test loss: 29.9420073509\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.859434127808,2.46665058904), test loss: 2.99960506558\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (8.2093744278,23.0518181314), test loss: 32.4663228989\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.682023406029,2.45879870942), test loss: 2.53499141186\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (20.3794727325,22.9574376958), test loss: 33.1534365654\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (4.14931249619,2.45102244498), test loss: 3.05667622983\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (14.7403583527,22.8636678381), test loss: 33.5024774551\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.75085377693,2.44326199428), test loss: 2.45278526992\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (8.79238891602,22.7714242378), test loss: 35.8745054722\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.488495200872,2.4357208509), test loss: 3.04265621305\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (5.95303153992,22.6782330116), test loss: 32.7861443043\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.892314970493,2.42835503168), test loss: 2.68642264605\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (13.3296995163,22.5875854585), test loss: 37.2155136585\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.18978345394,2.42091559583), test loss: 3.0495304212\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (4.83753967285,22.4975468999), test loss: 30.3342280388\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.70858621597,2.41340665909), test loss: 2.79511609823\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (10.9160451889,22.4079040864), test loss: 36.9348570824\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (2.74348211288,2.40611743435), test loss: 2.85633871555\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (6.50353956223,22.317779328), test loss: 28.3107059956\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.23976767063,2.39878294249), test loss: 2.62015648782\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (13.8183746338,22.2298853071), test loss: 38.08546772\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.539515256882,2.3916225651), test loss: 2.95162102729\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (19.7969245911,22.1430743521), test loss: 31.0348202705\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.01790618896,2.38464107624), test loss: 3.04021932483\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (4.90190029144,22.0544954832), test loss: 37.0559181213\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.823886275291,2.37768710938), test loss: 3.04689565301\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (5.78181838989,21.9683584398), test loss: 30.6512806892\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.45177698135,2.37064963718), test loss: 2.93910620213\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (4.74256801605,21.8829206192), test loss: 33.0065916061\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.34467542171,2.36366310664), test loss: 2.60444521829\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (17.9674911499,21.7974178603), test loss: 33.8342464447\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.717889308929,2.35679914758), test loss: 3.06771772504\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (12.7401342392,21.7122646287), test loss: 35.2136500835\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.38459300995,2.34997971876), test loss: 2.45405330658\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.89220809937,21.6283539948), test loss: 37.0002570629\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.67189836502,2.34328370131), test loss: 3.00792280436\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (6.47050762177,21.544375471), test loss: 33.9990555286\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.602910220623,2.33666148006), test loss: 2.75796573684\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (12.9785118103,21.461551326), test loss: 37.8787297249\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.885873436928,2.33010459285), test loss: 3.0578491509\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (13.8921585083,21.3793274419), test loss: 30.8654646873\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (3.01119494438,2.32348786338), test loss: 2.85473254919\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (12.025850296,21.2978389374), test loss: 38.3075431824\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.7216117382,2.31692741517), test loss: 2.86560738385\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (36.1954841614,21.216151042), test loss: 28.1070075989\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.45009255409,2.31039621193), test loss: 2.69265577942\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (4.3945274353,21.1347084819), test loss: 37.8631667614\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.87905097008,2.30402255733), test loss: 2.9793879658\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (6.19361877441,21.0550256731), test loss: 31.462077713\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.669086992741,2.29774390731), test loss: 3.1429359436\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (7.89766979218,20.9743463891), test loss: 38.5865680218\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.11011099815,2.29152216983), test loss: 3.11142671704\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (7.65767669678,20.8953601508), test loss: 30.3295847416\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.49073934555,2.2852495267), test loss: 2.96558499634\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (25.9753131866,20.8168711643), test loss: 33.9436758041\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (0.931341052055,2.27895669923), test loss: 2.53492710888\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (14.7056274414,20.7382985795), test loss: 34.4112370968\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.94285941124,2.27282437659), test loss: 3.06068389416\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (9.98896312714,20.6599211662), test loss: 35.9360681534\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.02971196175,2.26664136435), test loss: 2.51163591743\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (8.11172580719,20.5825739188), test loss: 36.9980578423\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.984117984772,2.26055105997), test loss: 2.97776640356\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (23.2663459778,20.5055048412), test loss: 34.4685680389\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.23626673222,2.25466975876), test loss: 2.78152564466\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.16614627838,20.4286019615), test loss: 38.7198441982\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.6230635643,2.24874557404), test loss: 3.04278045893\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (14.6638059616,20.352941158), test loss: 31.641558075\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (0.932277977467,2.242745713), test loss: 2.94591235518\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (2.99763870239,20.2773748626), test loss: 38.9943011761\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.59209001064,2.23683235821), test loss: 2.82776247859\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (6.65761852264,20.2017911068), test loss: 28.2696652412\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.27314567566,2.23096404408), test loss: 2.66742544472\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (14.7694339752,20.1265875594), test loss: 38.4981903553\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.901896238327,2.22517666486), test loss: 2.97509442568\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (5.02648544312,20.0525608657), test loss: 30.8956740379\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.357779413462,2.21943906399), test loss: 3.11989166737\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (10.0550203323,19.9779609179), test loss: 38.9143424034\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.03882849216,2.21378149757), test loss: 3.11299698353\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (9.67771434784,19.9045441456), test loss: 31.1001932621\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.829486429691,2.20814644052), test loss: 2.93260530084\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (5.02224206924,19.8316582921), test loss: 34.8813090324\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.20976018906,2.20243366923), test loss: 2.56805541515\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.60490131378,19.7590087832), test loss: 34.4321094513\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.45690131187,2.19683643105), test loss: 3.05715386868\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (3.93169546127,19.6861292212), test loss: 36.9929130554\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.807325661182,2.19119445158), test loss: 2.52806660384\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (10.3169822693,19.6140644231), test loss: 37.6795387268\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.77084803581,2.18572910446), test loss: 2.98087922037\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (5.54639863968,19.5426799429), test loss: 37.7209780693\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.157019644976,2.18032536241), test loss: 3.00319925845\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (9.79055786133,19.4711646363), test loss: 39.1415511131\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.171261549,2.17491336818), test loss: 3.02160668373\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (3.69552183151,19.4005675662), test loss: 31.2523836136\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.977935433388,2.16940077616), test loss: 2.86228563488\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (14.4309177399,19.3301797716), test loss: 39.6637496471\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.549642682076,2.16407287443), test loss: 2.85300513655\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (5.47240543365,19.2598259184), test loss: 29.2029569626\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.957847952843,2.15873749669), test loss: 2.77862808108\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.57443904877,19.1901668882), test loss: 41.7850840092\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.724887430668,2.15338260587), test loss: 3.01531763077\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.74277305603,19.1207940933), test loss: 31.5075164318\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.763219475746,2.148102832), test loss: 3.09458780289\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (6.57025146484,19.0514776358), test loss: 39.3831848621\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.56781923771,2.14300339568), test loss: 3.09950451851\n",
      "\n",
      "MC # 5, Hype # hyp5, Fold # 7\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold7/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (311.880126953,inf), test loss: 156.590798187\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (329.126586914,inf), test loss: 396.736222839\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (21.4897842407,70.827845078), test loss: 41.4537216187\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.43441390991,66.976511245), test loss: 3.03864420652\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (23.4260826111,59.6539298227), test loss: 37.8049544811\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.75464117527,34.9761523565), test loss: 2.80656838119\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (79.2117996216,55.8440240941), test loss: 40.4373895168\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.50215005875,24.2888097303), test loss: 3.24648144245\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (24.5656261444,53.8852382706), test loss: 37.5213844299\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.32026863098,18.9409753514), test loss: 3.20322434604\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (113.36063385,52.6640191904), test loss: 41.2973309517\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.68083810806,15.7351434334), test loss: 2.84680570364\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (28.3136177063,51.7427816373), test loss: 40.1175495148\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.19560289383,13.6000914676), test loss: 3.32645187676\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (25.9308319092,50.9956927506), test loss: 42.1364478588\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.32462716103,12.0686961527), test loss: 2.855184111\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (40.6689491272,50.403916911), test loss: 38.4716103077\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.63890087605,10.9221965817), test loss: 3.21304547191\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (43.3882141113,49.8561706563), test loss: 40.7110313416\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.20387601852,10.0280073636), test loss: 2.75437074602\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (37.7462654114,49.3842002394), test loss: 39.8291940212\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (6.01966381073,9.30987165399), test loss: 3.21075354517\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (33.4217071533,49.0016502576), test loss: 38.5490012169\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.86794114113,8.72193288804), test loss: 2.74317391217\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (19.8806991577,48.6136695031), test loss: 39.6564639091\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.11988806725,8.23352346218), test loss: 3.01168142557\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (50.2056732178,48.2287035181), test loss: 35.514986515\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.4074151516,7.82310066189), test loss: 2.56424778402\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (25.9012908936,47.8063094288), test loss: 35.1387094021\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (0.585868537426,7.4677230687), test loss: 2.89321849942\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (52.9526748657,47.3840183215), test loss: 32.4720161915\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.5266854763,7.16076313005), test loss: 2.85953713059\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (46.5507354736,46.9432556028), test loss: 34.7229640961\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.05217313766,6.89050781358), test loss: 2.94858908057\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (40.2522163391,46.5121349976), test loss: 33.4973999977\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.72347640991,6.64973233961), test loss: 3.14537909031\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (22.909702301,46.1029196787), test loss: 36.1373710155\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (6.78474998474,6.4360111567), test loss: 2.61384435743\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (12.9699707031,45.6838049668), test loss: 33.9480446339\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (4.08800697327,6.24393938584), test loss: 3.36441683769\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (79.1349716187,45.260953089), test loss: 35.0431788445\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.64887356758,6.07133447075), test loss: 2.54806564748\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (33.0263442993,44.8204327463), test loss: 31.8922050714\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.9090025425,5.9125503828), test loss: 3.02894191146\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (13.125667572,44.3681737913), test loss: 33.9933588505\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.97016727924,5.76756965364), test loss: 2.69265438914\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (17.4647407532,43.9013689433), test loss: 32.9142193317\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (4.73870897293,5.6337451918), test loss: 2.95088268816\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (5.27852058411,43.4387806733), test loss: 30.7560553551\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.806063294411,5.50839740103), test loss: 2.59514865875\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (29.8368721008,42.9842801331), test loss: 31.4714866161\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.13025379181,5.3930395149), test loss: 3.00131041408\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (24.6393241882,42.5215229718), test loss: 26.5872766256\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.5936665535,5.28572807124), test loss: 2.75352345407\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (45.6382102966,42.055171313), test loss: 29.3613620758\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.16952800751,5.18613698673), test loss: 2.94623476863\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (50.0585250854,41.5898657094), test loss: 26.1480200291\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.495262146,5.09169965505), test loss: 2.96835353076\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (32.1480407715,41.1200665027), test loss: 28.9872708321\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.91503930092,5.00302759977), test loss: 2.70305514932\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (31.1578598022,40.6510524988), test loss: 28.5717046976\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.97496032715,4.91908029999), test loss: 3.1421705842\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (12.4500083923,40.1987442878), test loss: 30.331794405\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.0338602066,4.83901641699), test loss: 2.4870978713\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (8.37743377686,39.758664156), test loss: 27.9728170395\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.34487581253,4.76347272064), test loss: 3.19586094618\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (54.9830551147,39.3250170324), test loss: 28.3587821484\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.90480697155,4.69234444539), test loss: 2.45551206619\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (17.3097724915,38.9007000258), test loss: 29.6953543663\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.843945682049,4.62532552808), test loss: 3.13484128118\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (21.9931259155,38.4907098742), test loss: 28.7875254393\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.26041078568,4.56105368053), test loss: 2.72583637834\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (15.1752738953,38.085089833), test loss: 28.3520096779\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (0.922841310501,4.49977847606), test loss: 2.80534389317\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (10.136179924,37.6889567962), test loss: 25.0487756729\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.25504910946,4.44113542039), test loss: 2.48417504728\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (14.2925109863,37.3125064348), test loss: 27.2133072376\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.8930337429,4.38463436136), test loss: 2.85144005716\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (5.437312603,36.9474151916), test loss: 23.522615242\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.433446764946,4.33065775185), test loss: 2.88340623677\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (44.6675186157,36.588679214), test loss: 27.4138060093\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.882016301155,4.27946684681), test loss: 2.99431779087\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (14.3365287781,36.2445219414), test loss: 24.5639264584\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.27952241898,4.23097197655), test loss: 3.20650053918\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (17.9618377686,35.912440522), test loss: 28.7279940128\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.72741055489,4.18408085629), test loss: 2.47481410801\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (32.8562164307,35.5844659438), test loss: 25.9651770592\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.73623609543,4.13899163232), test loss: 3.2681972146\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (18.2360744476,35.2655609697), test loss: 29.0784142971\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.53055620193,4.09548482108), test loss: 2.42347809374\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (14.3224525452,34.9611998183), test loss: 26.4178403139\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (4.37677574158,4.05316177715), test loss: 2.97054920793\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (23.9392261505,34.6653404919), test loss: 28.6624140739\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (2.00226354599,4.0126580184), test loss: 2.55259878486\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (11.5305614471,34.3718264102), test loss: 28.6384205818\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.36786341667,3.97386587666), test loss: 3.00114037395\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (9.71454334259,34.0923439114), test loss: 25.8701599598\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (2.76961660385,3.93688038862), test loss: 2.59005829394\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (8.18179130554,33.8205593312), test loss: 27.2335526705\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.91624116898,3.90098427311), test loss: 2.9000087738\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (22.5380592346,33.5504668962), test loss: 25.3716510773\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.74210929871,3.86616751552), test loss: 2.90008660555\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (10.8188829422,33.2884185375), test loss: 28.0481377602\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.526963114738,3.83237425688), test loss: 2.99523158371\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (7.27183198929,33.0364514587), test loss: 23.8002067566\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.982907950878,3.79921062418), test loss: 3.04313550144\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (20.1979484558,32.7896194178), test loss: 27.9677358866\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.88269329071,3.76746418316), test loss: 2.65986439288\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (10.3659162521,32.5444186799), test loss: 26.1421304226\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.1752320528,3.7367623543), test loss: 3.20161377937\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (29.4514961243,32.3097764321), test loss: 28.9016760349\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.37962627411,3.70736470699), test loss: 2.45978673697\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (12.6474742889,32.0806510471), test loss: 27.3515011787\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.01709127426,3.6787323), test loss: 3.03408222497\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.2244338989,31.8517056137), test loss: 28.1666567326\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.9783821106,3.65077919784), test loss: 2.46270607412\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (6.88355255127,31.6297285717), test loss: 29.3123010397\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.76694631577,3.6235738224), test loss: 3.06183720827\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (12.704624176,31.4148012132), test loss: 27.6276054382\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.1266412735,3.5966523015), test loss: 2.59015274346\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.5919981003,31.2026289849), test loss: 28.6397997379\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.861131191254,3.57076070555), test loss: 2.86491485834\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (10.7715072632,30.9920204801), test loss: 24.8922951937\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.76552212238,3.54565647542), test loss: 2.38976717293\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (25.5002231598,30.7891641096), test loss: 28.0256621361\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.32084488869,3.52150662372), test loss: 2.8303984642\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (30.2186355591,30.5910176266), test loss: 24.0962178707\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.95988106728,3.49791266222), test loss: 2.9680789575\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.9192390442,30.3920932309), test loss: 29.2726205826\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.05839157104,3.47477137721), test loss: 2.97110552788\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (7.90091705322,30.199480427), test loss: 25.1325680017\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.10275888443,3.45215094088), test loss: 3.13855677098\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.8536500931,30.0116236683), test loss: 29.2815506458\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.08190917969,3.42973396446), test loss: 2.43493387848\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (19.1095027924,29.8254428968), test loss: 27.6821343422\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.978486478329,3.40810575151), test loss: 3.16445200741\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (6.01148319244,29.6404225024), test loss: 28.9374101639\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.660580992699,3.3870023282), test loss: 2.32005602419\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (23.8884162903,29.4612184229), test loss: 28.9618125916\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.93655872345,3.36663538369), test loss: 2.99903819263\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (22.5486793518,29.28613509), test loss: 30.6986815453\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.92948532104,3.34670082853), test loss: 2.62100835443\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (12.5828380585,29.1105052855), test loss: 31.0418439865\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.12697064877,3.32707449035), test loss: 2.9639303416\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (16.0441493988,28.9396628113), test loss: 25.3483547688\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.46201229095,3.30781822702), test loss: 2.47969170809\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (19.5783462524,28.7722922452), test loss: 28.8140868306\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.93479037285,3.28870829106), test loss: 2.77697197199\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.9025917053,28.6060343909), test loss: 25.915718174\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.29454803467,3.27021779475), test loss: 2.7387111485\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (30.3763504028,28.4408212836), test loss: 28.3386517048\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.29312252998,3.25208383045), test loss: 2.89874061942\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (12.4130020142,28.2796355182), test loss: 24.396447897\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.87584853172,3.23449891039), test loss: 2.95351975262\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (15.6331853867,28.1223447295), test loss: 31.6358099937\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.537349998951,3.21725852856), test loss: 2.66818141043\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (36.3763275146,27.9646399358), test loss: 27.8310334206\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.7289917469,3.20032303716), test loss: 3.21335571408\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (6.49517536163,27.8104539823), test loss: 29.7763403416\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (4.87024402618,3.18357708477), test loss: 2.40566816926\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (41.6078262329,27.6589970422), test loss: 28.1870674849\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.63395428658,3.16696755886), test loss: 3.01839351356\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (27.0060424805,27.5081664187), test loss: 29.699606514\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.93072450161,3.15084529388), test loss: 2.42892697901\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (11.7955121994,27.3577776533), test loss: 31.0356010914\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.80148136616,3.13501196384), test loss: 2.96368456483\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (16.1702823639,27.2114636134), test loss: 27.9200331688\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.74302816391,3.11957864041), test loss: 2.52552054971\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (33.1608123779,27.0679298573), test loss: 32.8111163139\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.99770450592,3.10452688472), test loss: 2.97249554992\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (4.57779169083,26.9236864533), test loss: 24.8720129967\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.15404987335,3.08963494417), test loss: 2.35730625838\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (7.17241001129,26.783464489), test loss: 29.1214845181\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.35359764099,3.07483766643), test loss: 2.81305839866\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (10.0132789612,26.6441071711), test loss: 25.0583414555\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.15948057175,3.06028947121), test loss: 2.84279770702\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.4772186279,26.5061785125), test loss: 30.8110081673\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (3.45779752731,3.04607345113), test loss: 2.88200675845\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (17.523021698,26.3686044014), test loss: 27.0828255415\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.53562164307,3.03205628765), test loss: 3.00395076871\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (13.2831439972,26.2341209096), test loss: 30.9164284706\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (0.990435123444,3.01839123928), test loss: 2.47449368834\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (41.6614227295,26.1019934469), test loss: 30.9801457167\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.198376894,3.00504155181), test loss: 3.3495503217\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (27.1840362549,25.9699726404), test loss: 29.3994874954\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.26812267303,2.99183825637), test loss: 2.2084519878\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (8.05632781982,25.8408369072), test loss: 30.749497509\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.40565490723,2.97865363382), test loss: 3.00421527326\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (6.1504945755,25.7124031183), test loss: 27.6682686329\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.17067718506,2.9657151366), test loss: 2.49766748697\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (27.7346343994,25.5851738869), test loss: 32.5563784122\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.73914480209,2.95298185132), test loss: 2.78018070757\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (17.3173904419,25.4582148079), test loss: 27.1211490631\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.89648032188,2.94051071347), test loss: 2.45634977221\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (7.36312866211,25.334153751), test loss: 30.8128764153\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.99338448048,2.92829308692), test loss: 2.73322435319\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (14.6266489029,25.2117322718), test loss: 25.5661406517\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (0.985259175301,2.91629985958), test loss: 2.80779341757\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (10.2983551025,25.0900243386), test loss: 30.3671677589\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.8437410593,2.90446567099), test loss: 2.90812882036\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (6.05986976624,24.9709859709), test loss: 25.4642276049\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.12872385979,2.89263245672), test loss: 2.93645513356\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (36.4730491638,24.8521700615), test loss: 31.2353545189\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (4.03704404831,2.88102353968), test loss: 2.60806782544\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (14.4467248917,24.7337273639), test loss: 28.6030318737\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.624165654182,2.86956760913), test loss: 3.14315151721\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (11.4539270401,24.6161693375), test loss: 32.6413897991\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.812630832195,2.85832020343), test loss: 2.31739389598\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (4.99543237686,24.5012085236), test loss: 29.684315443\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.851294338703,2.84727763674), test loss: 2.95074105859\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (7.41836452484,24.38735625), test loss: 31.4031118393\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.05022907257,2.83645358238), test loss: 2.51962679327\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (4.55634117126,24.2742485527), test loss: 32.5123316765\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.548508763313,2.82571149669), test loss: 2.95504002571\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (13.9244813919,24.1632971389), test loss: 28.5958947659\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.56878983974,2.81496345019), test loss: 2.56635328829\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (4.48587989807,24.0520592458), test loss: 32.0094638348\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.32096171379,2.80438360191), test loss: 2.79645624757\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (11.7226104736,23.9416880538), test loss: 25.7665400743\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.95530748367,2.79399280959), test loss: 2.37023833543\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (5.80596494675,23.831834164), test loss: 32.0162920237\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.97702157497,2.78372883022), test loss: 2.91463204026\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (8.04630088806,23.7242300785), test loss: 26.5108818769\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.843589901924,2.77363321644), test loss: 2.82103580087\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (28.8149986267,23.617428193), test loss: 32.4414500713\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (2.71567893028,2.76370558456), test loss: 2.73073090017\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (14.5819854736,23.5112325337), test loss: 28.4325915337\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.09685277939,2.75388687681), test loss: 3.11181081235\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (13.6884593964,23.40656247), test loss: 33.187458086\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (0.980384945869,2.74401496257), test loss: 2.5546241343\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (13.6305685043,23.3019674605), test loss: 31.4883284807\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.97018039227,2.73431769735), test loss: 3.17431541085\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (8.51271724701,23.1976288701), test loss: 29.9013322353\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.02112734318,2.72476104745), test loss: 2.3626742363\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (16.3477210999,23.0941772634), test loss: 33.5416249275\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.03158414364,2.71533763819), test loss: 3.00686972141\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (9.54075813293,22.9923872105), test loss: 28.7239693642\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.41207170486,2.70606522731), test loss: 2.55880487561\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (9.68579292297,22.8910433944), test loss: 34.0846642494\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.762274384499,2.69689772197), test loss: 2.78926362395\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (10.1696453094,22.7908000439), test loss: 27.1026865005\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.54829716682,2.68785588189), test loss: 2.36240272522\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (10.5811710358,22.6916169679), test loss: 34.4173587322\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.553133010864,2.67878531223), test loss: 2.79461983144\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (18.8797607422,22.5926009151), test loss: 27.0623527527\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.09235501289,2.66983430716), test loss: 2.71831325889\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (11.901553154,22.4936232418), test loss: 32.6069487333\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.67927062511,2.66102425503), test loss: 2.94134213328\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (2.51787376404,22.395520698), test loss: 29.3707004547\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.8068562746,2.6523299082), test loss: 3.08304778039\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (11.1824550629,22.2991062564), test loss: 33.7081600666\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (3.14941477776,2.6437388495), test loss: 2.67191717923\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (12.6316423416,22.2030000122), test loss: 30.5632766247\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.550203621387,2.63528903042), test loss: 3.22993000597\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (4.3039932251,22.1079642422), test loss: 32.9903055668\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.11960279942,2.62691856223), test loss: 2.43037284166\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (16.9540176392,22.0138671154), test loss: 33.1432029247\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (0.722027540207,2.61853028656), test loss: 3.02950894088\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (9.23848724365,21.9198719866), test loss: 33.6781573296\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.47369551659,2.6102368369), test loss: 2.53341732621\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (8.16344451904,21.8261379701), test loss: 34.3788606644\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.9680814743,2.60207279751), test loss: 2.91274240762\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (8.03852272034,21.7333944739), test loss: 32.8311610699\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.701851427555,2.59401743877), test loss: 2.62068051696\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (8.86468410492,21.6418342931), test loss: 33.9938559771\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.13884353638,2.58602523674), test loss: 2.75404971242\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (9.17129707336,21.5505996236), test loss: 27.2610423803\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.392281472683,2.57822223524), test loss: 2.53365750611\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (6.22499513626,21.4606110977), test loss: 33.6530747175\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.735036730766,2.57044104873), test loss: 2.95024693906\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (5.87937831879,21.3711241635), test loss: 27.2196742296\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.02502572536,2.56264084621), test loss: 2.92985986173\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (5.21123313904,21.2819132798), test loss: 35.3210339546\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.747638940811,2.55489463628), test loss: 2.67552262694\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (7.06742763519,21.1928823477), test loss: 29.3167939186\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.04939293861,2.54726013941), test loss: 3.0887750417\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (10.8649997711,21.1048037912), test loss: 36.3610057354\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.66911363602,2.53978799537), test loss: 2.53205140233\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (12.2218112946,21.0176877555), test loss: 33.1944609165\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.4729886055,2.53230323809), test loss: 3.13003833294\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (5.43132305145,20.9309363716), test loss: 33.0517627716\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.06848573685,2.5250177494), test loss: 2.57418673635\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (5.68351650238,20.845190203), test loss: 35.6720679998\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.45893204212,2.51772114404), test loss: 3.09900498986\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (9.115234375,20.760004001), test loss: 29.7481404305\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (0.946695268154,2.51039999946), test loss: 2.70568964481\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (5.54677534103,20.6749500702), test loss: 35.8965361118\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.28857636452,2.50314187352), test loss: 2.77775899321\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (22.9052047729,20.5900796705), test loss: 28.0432572365\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.24560260773,2.49595861493), test loss: 2.42853803635\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (6.95858573914,20.5060084593), test loss: 34.3120197058\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.28380727768,2.48894396626), test loss: 2.78202929646\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (1.50253891945,20.4227569209), test loss: 28.8639607906\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.329331606627,2.48190425346), test loss: 2.6893762365\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (17.4562416077,20.3400448627), test loss: 36.2506453991\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.31002783775,2.47504909379), test loss: 3.021828188\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (8.15566444397,20.2580165518), test loss: 29.5391174316\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.861289620399,2.46819236227), test loss: 3.11076084077\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (7.30110168457,20.1764539146), test loss: 36.5189945698\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (3.74930262566,2.46130454537), test loss: 2.73708455563\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (5.05324602127,20.0951511435), test loss: 31.3161916971\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.353909343481,2.4544718498), test loss: 3.1799536407\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (4.84828996658,20.0138525779), test loss: 34.164402771\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.810823202133,2.44771467764), test loss: 2.53204346001\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (7.89369869232,19.9336407807), test loss: 33.672420764\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (1.75935220718,2.44110619983), test loss: 2.99007454216\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (3.05541038513,19.8539928842), test loss: 33.7097443104\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.24068787694,2.43448285479), test loss: 2.62430369407\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (5.53253936768,19.7746987198), test loss: 36.2048248768\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (0.514333486557,2.42801670562), test loss: 2.9348545\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (9.41219425201,19.6964221294), test loss: 30.1706765175\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.874653339386,2.42156318742), test loss: 2.57527603656\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (8.69485187531,19.6184177971), test loss: 36.4105097294\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (3.45418572426,2.41503337526), test loss: 2.75914232135\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (5.67902851105,19.5406769226), test loss: 29.9269078255\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (3.00461483002,2.40863883687), test loss: 2.74926994443\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (6.68281269073,19.463090967), test loss: 37.9335744381\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.38514614105,2.40227091143), test loss: 3.02325487137\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (13.1497812271,19.3864782786), test loss: 27.9876165867\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.29387068748,2.39602556464), test loss: 2.9423206687\n",
      "\n",
      "MC # 5, Hype # hyp5, Fold # 8\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold8/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (352.961090088,inf), test loss: 187.594356155\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (281.179199219,inf), test loss: 340.908590698\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (35.4179458618,86.1121476898), test loss: 47.1618959904\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.85522890091,72.9155215317), test loss: 3.33515508175\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (51.4629135132,66.3279706221), test loss: 39.4159063816\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.06265211105,38.0957230489), test loss: 2.79670773745\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (62.598197937,59.7006961061), test loss: 45.3561553478\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.60400772095,26.4868985682), test loss: 3.52590437531\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (19.7332019806,56.436737622), test loss: 42.8264046669\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.948226869106,20.6844698698), test loss: 3.26385025084\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (64.6125488281,54.3608711248), test loss: 45.898002243\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (6.10273122787,17.2073510753), test loss: 3.5175907433\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (32.754524231,53.0353126901), test loss: 46.9677511215\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.976990818977,14.8885946147), test loss: 3.49483377635\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (67.4519195557,51.985602066), test loss: 42.7600255013\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (6.40845870972,13.2319326516), test loss: 2.85232454538\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (47.7251663208,51.1790587339), test loss: 49.3370671272\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.99515461922,11.9888396651), test loss: 3.53234764338\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (26.4250240326,50.537510454), test loss: 43.8338465214\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.93837594986,11.0197937229), test loss: 2.81762290597\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (36.627155304,50.0766716784), test loss: 48.3266893864\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.46685576439,10.24174629), test loss: 3.61704027653\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (70.4168624878,49.655467605), test loss: 39.3491131783\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.8738617897,9.60707211751), test loss: 2.83463477492\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (20.3190612793,49.3093000593), test loss: 47.0334691048\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (5.35950422287,9.07983086377), test loss: 3.48854574561\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (96.4617767334,49.0004916408), test loss: 39.6429619789\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.89399147034,8.63086983842), test loss: 2.9623605907\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (12.8928918839,48.6656161196), test loss: 48.2124825001\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.63335180283,8.24477540634), test loss: 3.37632220984\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (42.4513587952,48.4239033909), test loss: 39.396298933\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.66931819916,7.91092277339), test loss: 2.78373913169\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (25.9867782593,48.1822992635), test loss: 44.4273133278\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.46802651882,7.61757263692), test loss: 3.48301383853\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (18.8024902344,47.9730604497), test loss: 42.4608654976\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.13139200211,7.35915683894), test loss: 3.14079620242\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (32.0220565796,47.7836591215), test loss: 42.9857414246\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.955891013145,7.1318595068), test loss: 3.40077540874\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (29.117269516,47.5979484189), test loss: 42.9405313969\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.07533931732,6.92580994762), test loss: 3.31379620433\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (88.8168945312,47.4055498891), test loss: 40.022170496\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.93340873718,6.73702030328), test loss: 2.72882189155\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (31.6574668884,47.219647055), test loss: 45.8186383247\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.97814035416,6.5655278694), test loss: 3.13992637992\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (51.5075035095,47.0440375405), test loss: 41.9553636551\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.32081365585,6.40816463495), test loss: 2.81411178708\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (25.3495178223,46.8941158356), test loss: 47.0769534111\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.45123875141,6.26573538861), test loss: 3.51102890372\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (19.8364257812,46.7306462693), test loss: 41.6836801529\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.98028922081,6.13462324996), test loss: 2.7469979465\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (22.1082687378,46.5785537487), test loss: 44.9689720154\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.20990085602,6.01306859913), test loss: 3.31423062086\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (44.5086364746,46.4137567781), test loss: 37.7674304962\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.78120493889,5.90088643571), test loss: 2.80949706435\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (28.0553970337,46.2424524747), test loss: 44.3081669807\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (4.38237094879,5.7960582438), test loss: 3.23072463274\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (56.601020813,46.0810714243), test loss: 36.8854099751\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.49312877655,5.69759536701), test loss: 2.65016150773\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (75.2901153564,45.9301429019), test loss: 43.9297489166\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.97317934036,5.60618954005), test loss: 3.22826180458\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (17.4216308594,45.7569650921), test loss: 35.5615833759\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.03278100491,5.5200208757), test loss: 2.4956744194\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (28.4967861176,45.5984505017), test loss: 40.8517122746\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.20943450928,5.43951879133), test loss: 3.0860522747\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (33.5749588013,45.4147446122), test loss: 38.8090652466\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.06781411171,5.3628987289), test loss: 3.03014999032\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (37.0701751709,45.2273083671), test loss: 40.7897633553\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.00765991211,5.29077138898), test loss: 3.07410767674\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (11.7213573456,45.0376294693), test loss: 40.5005233765\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.70977461338,5.22170910704), test loss: 3.12863973379\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (7.12133550644,44.8533146381), test loss: 36.4077459335\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.61591142416,5.15658249634), test loss: 2.57599191368\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (14.8242101669,44.652167724), test loss: 44.8016046047\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.10326957703,5.09479640452), test loss: 3.23502588272\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (25.4013214111,44.4222509224), test loss: 34.0098383665\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.25560259819,5.03120467841), test loss: 2.30127839446\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (78.7612380981,44.1202889834), test loss: 39.773111248\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (2.30942392349,4.96527423217), test loss: 3.11438388228\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (44.8387298584,43.7894011421), test loss: 28.4097526073\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.77531671524,4.90109969106), test loss: 2.26966054887\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (123.353057861,43.4641509798), test loss: 38.4397293091\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.35329818726,4.83969724247), test loss: 3.04232141078\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (30.6836585999,43.1263950364), test loss: 29.3796677113\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.30409598351,4.78036616282), test loss: 2.46477003098\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (45.7684860229,42.7825127042), test loss: 38.1084877968\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.28018951416,4.72335027088), test loss: 3.0742038697\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (9.4751701355,42.4339410016), test loss: 27.5249903679\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.10981893539,4.6694237835), test loss: 2.48881018758\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (7.54387283325,42.0822298978), test loss: 33.3723574638\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.962130844593,4.6172925569), test loss: 2.82911410034\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (101.837806702,41.7197451228), test loss: 31.5884385824\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.4884865284,4.5668330554), test loss: 3.01726840436\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (22.5254478455,41.3625864813), test loss: 31.4132286072\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (4.14763832092,4.51832385854), test loss: 2.94785707891\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.8586273193,41.0074794113), test loss: 31.1109998465\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.9526437521,4.47108861979), test loss: 3.17057467997\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (20.8240394592,40.6586590242), test loss: 27.2852172375\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (4.96965503693,4.42613582794), test loss: 2.49152244329\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (12.9491348267,40.3112047804), test loss: 32.9938471556\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.97213709354,4.38265851382), test loss: 3.0389064908\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (18.0416908264,39.9739847449), test loss: 28.8047412634\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.4624325037,4.34087751341), test loss: 2.40955384672\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (22.9316940308,39.6335868417), test loss: 34.8304057598\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.05455708504,4.30039477599), test loss: 3.24385721087\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (5.8546257019,39.3009264572), test loss: 28.8271036386\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.19860267639,4.26101875974), test loss: 2.39296924174\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (38.8594360352,38.9794081485), test loss: 32.907356596\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.29737782478,4.22258968745), test loss: 3.04577538967\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (40.780128479,38.6649519642), test loss: 26.8791933298\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.40732765198,4.18570542735), test loss: 2.56417044997\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (38.9123840332,38.3528666798), test loss: 33.5986527443\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.31446266174,4.14980941596), test loss: 3.00902458429\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (13.9074325562,38.053333511), test loss: 27.2787386894\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.63107824326,4.11542554662), test loss: 2.49059935808\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.059211731,37.7544369779), test loss: 33.3310370564\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.45011162758,4.08188384875), test loss: 2.90628848672\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (24.5806541443,37.4618344107), test loss: 26.0023886919\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.56924271584,4.04910996208), test loss: 2.36247372776\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (9.10098648071,37.1803790413), test loss: 32.1461760759\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (4.15150547028,4.01719616377), test loss: 2.93458077312\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (24.1086292267,36.9053755916), test loss: 30.2454886675\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.25677919388,3.98616337938), test loss: 3.12082550526\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (10.6274900436,36.6326010136), test loss: 31.1028108358\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.14539051056,3.95601301915), test loss: 2.94184679985\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (12.8917379379,36.3688596674), test loss: 30.9255781651\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.7786693573,3.92698751329), test loss: 3.09428732991\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (63.2881851196,36.1111670058), test loss: 27.5801355839\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.73191285133,3.898783379), test loss: 2.42733286321\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (17.407661438,35.8534720224), test loss: 34.4096542835\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.44872331619,3.87084824595), test loss: 3.14533548951\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (8.92402458191,35.6059756085), test loss: 27.8511609674\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.62934792042,3.8437386434), test loss: 2.34453517795\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (24.8082427979,35.3653790292), test loss: 33.768778944\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (7.34764766693,3.81718829756), test loss: 3.09622407854\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (25.7328681946,35.1256731034), test loss: 25.2475930214\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.53571128845,3.79117325478), test loss: 2.24985544384\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (23.0476551056,34.8939263837), test loss: 33.6099574804\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.14171934128,3.76622418777), test loss: 3.05035913587\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (21.8450508118,34.6681100353), test loss: 28.4270855904\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.81271612644,3.74191135214), test loss: 2.46528986543\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (4.11262321472,34.4398023818), test loss: 33.8949384928\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.652986228466,3.71776841166), test loss: 3.00553397834\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (4.07921457291,34.2216643523), test loss: 27.6572429657\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.71558427811,3.69419039624), test loss: 2.51946826279\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (23.0010490417,34.0063815858), test loss: 31.6504270792\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.68126702309,3.67098874742), test loss: 2.74225620925\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (10.280166626,33.7944579745), test loss: 30.9487612724\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.30339002609,3.64836996076), test loss: 3.03594025373\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (14.7570619583,33.5865262039), test loss: 30.633557725\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.17499768734,3.626443267), test loss: 2.88675689101\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (25.7184238434,33.3851668402), test loss: 30.1165275812\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.10443663597,3.60507256482), test loss: 2.98296797574\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (18.8422832489,33.1820719141), test loss: 30.2288654327\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.57193732262,3.58400180486), test loss: 2.48359529078\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (14.2724285126,32.9850022737), test loss: 31.2771522284\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.11046266556,3.56326511592), test loss: 2.97807680666\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (10.1002016068,32.7920983149), test loss: 29.4861561298\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.9302957058,3.54277830384), test loss: 2.37553943992\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (26.6387310028,32.6022901285), test loss: 34.0374980927\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.93267917633,3.52275325982), test loss: 3.14972032309\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (20.6230964661,32.4134604905), test loss: 29.8790509224\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.2821046114,3.50314835745), test loss: 2.31145167053\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (17.2250862122,32.2311350017), test loss: 32.3737025261\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.97999715805,3.48409512729), test loss: 2.92113662362\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (16.4159793854,32.0481976475), test loss: 28.1148830652\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.19611179829,3.46540817214), test loss: 2.41374329627\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (5.48154735565,31.868862912), test loss: 33.576803565\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.1541454792,3.44689801581), test loss: 2.98260381818\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (8.43018817902,31.6947152496), test loss: 27.8961260319\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.09837210178,3.42859958454), test loss: 2.46759091318\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (11.3414182663,31.5220333546), test loss: 32.9143600106\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.3862388134,3.41071174834), test loss: 2.75277748257\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (28.0057201385,31.3500657305), test loss: 26.1948895931\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.63357257843,3.39312280494), test loss: 2.2782943964\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (8.92115879059,31.1821945928), test loss: 32.2586114883\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.539212346077,3.37594492964), test loss: 2.79509607852\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (9.83736228943,31.0163167589), test loss: 30.0327816725\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.78073835373,3.35919857436), test loss: 2.96036552489\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (16.241268158,30.8520353412), test loss: 31.3802600622\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.09158611298,3.34245881909), test loss: 2.80701886117\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (16.8778266907,30.6916169099), test loss: 30.0730897665\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (5.14699602127,3.32607983572), test loss: 2.92942705154\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (10.7427244186,30.5335096315), test loss: 27.1450743675\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.6661336422,3.30973082923), test loss: 2.26634017229\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (9.58102703094,30.3747755243), test loss: 34.5341806293\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.45759439468,3.29378083894), test loss: 2.99766740799\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (27.5985660553,30.2207297404), test loss: 32.3479462624\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.955931782722,3.27824216136), test loss: 2.33924288154\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (18.2264060974,30.0692286302), test loss: 35.3239975691\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (2.08510231972,3.26299128606), test loss: 3.09283949137\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (32.6281242371,29.9164426919), test loss: 27.2989217997\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.53533244133,3.24785759698), test loss: 2.14045479894\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (16.7391242981,29.7682907597), test loss: 33.5418967366\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.40553927422,3.23284201801), test loss: 2.852918607\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (9.78018093109,29.6211741138), test loss: 29.9018678188\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.10625350475,3.21804307733), test loss: 2.35469266772\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (31.3205165863,29.4754940647), test loss: 34.1048602581\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (2.33765435219,3.20347632539), test loss: 2.8746871233\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (12.2757558823,29.3316538292), test loss: 29.4229405403\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.44203221798,3.1892696634), test loss: 2.45646290481\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (13.706495285,29.1910003552), test loss: 31.5711563706\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.50504469872,3.17529714507), test loss: 2.69281276762\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (12.1064128876,29.0490444986), test loss: 28.8194011211\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.33177876472,3.16147156802), test loss: 2.70756883025\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (18.6010475159,28.9108886376), test loss: 31.7260428667\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.60235524178,3.14780116387), test loss: 2.75369053483\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (27.1053657532,28.7745538492), test loss: 29.872841835\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.81277418137,3.13423037017), test loss: 2.76871752441\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (16.7982788086,28.6387935683), test loss: 31.8203848362\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.958105385303,3.12087477673), test loss: 2.69300363064\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (13.186668396,28.5043391554), test loss: 34.5835101128\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.79507035017,3.10772297964), test loss: 2.9153722316\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (8.4467792511,28.3727574346), test loss: 31.751004076\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.33087790012,3.09482567141), test loss: 2.49175928533\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (6.39150333405,28.2406874234), test loss: 34.3382381082\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.92934083939,3.08220023836), test loss: 3.01766742468\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (8.70133018494,28.1110088749), test loss: 29.6901684284\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.867898285389,3.06960337559), test loss: 2.27235589921\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (7.06110239029,27.9840717762), test loss: 33.9664111257\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.52741312981,3.05711088104), test loss: 2.83329228461\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (15.346616745,27.8570397644), test loss: 28.1257432938\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (0.994213461876,3.04480847777), test loss: 2.31585562825\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (17.012720108,27.7305997228), test loss: 35.2140939236\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.72551560402,3.03262659556), test loss: 2.95527348816\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (7.11698246002,27.6068986276), test loss: 29.7511531353\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.445520818233,3.0207260913), test loss: 2.43803106546\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (31.5089645386,27.4834327027), test loss: 35.2356723785\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.65689182281,3.00902835956), test loss: 2.70247662067\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (20.7700443268,27.3615153061), test loss: 27.0880563498\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.5528151989,2.99738294011), test loss: 2.23697303534\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (5.65985393524,27.2414214951), test loss: 32.7343894601\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.852099061012,2.98577080173), test loss: 2.73343040645\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.17806100845,27.1225218116), test loss: 30.3024904728\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.521415948868,2.97437985763), test loss: 2.87433735728\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (8.68453216553,27.003119469), test loss: 32.5726726651\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.30801928043,2.96308809169), test loss: 2.80029235482\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (15.7096700668,26.8865424706), test loss: 30.1521967173\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (4.00074052811,2.95206859238), test loss: 2.75957354903\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (15.9449729919,26.7715668678), test loss: 34.2343281746\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.480429410934,2.94118157398), test loss: 2.6310013473\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (4.65544700623,26.655472739), test loss: 35.0660552502\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.716018736362,2.93034139777), test loss: 3.00652931333\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (15.7146587372,26.5422337384), test loss: 31.3398523808\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.867688059807,2.91958041976), test loss: 2.36853685379\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (3.69413495064,26.4293930538), test loss: 35.8382957697\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.03088593483,2.90894531757), test loss: 3.01392419338\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (9.58762550354,26.3169239374), test loss: 32.1681975365\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (3.21473574638,2.89842210303), test loss: 2.40346825719\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (5.93996906281,26.2059382659), test loss: 33.6561002016\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (2.00679802895,2.88809770855), test loss: 2.88498095572\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (4.97150659561,26.0973108845), test loss: 30.1513221741\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.53053808212,2.87792375136), test loss: 2.46325324774\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (12.6918029785,25.9874756665), test loss: 36.2708735704\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.52876996994,2.86783249634), test loss: 2.87400681674\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.75931406021,25.8798466275), test loss: 28.893067646\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.9684637785,2.85780250073), test loss: 2.43929390013\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (7.05975914001,25.7729760407), test loss: 33.1710599303\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.39469897747,2.84780616018), test loss: 2.62880505621\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (11.0351676941,25.6664176027), test loss: 29.6158242702\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.90110850334,2.83799306335), test loss: 2.58352781236\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (9.43056106567,25.5607691295), test loss: 33.5340574265\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.30948317051,2.82825803126), test loss: 2.72471582741\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (2.14834785461,25.4570072138), test loss: 31.1604885697\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.247562766075,2.81867316818), test loss: 2.77167857587\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (13.7608394623,25.3528802395), test loss: 34.5673247099\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.680855631828,2.80926415853), test loss: 2.84249238372\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (8.12685203552,25.2503615851), test loss: 32.7541883469\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.37498903275,2.79987330287), test loss: 2.89611623883\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (20.5438995361,25.1491699864), test loss: 31.8423232555\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.40801262856,2.79047270788), test loss: 2.48453320563\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (22.0901527405,25.0475394778), test loss: 35.7397038698\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.68750476837,2.78124199804), test loss: 3.01587149501\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (10.5998544693,24.946326521), test loss: 34.1672389984\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (0.699965536594,2.77204107093), test loss: 2.37052655742\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (9.99019622803,24.8471266493), test loss: 35.8242388844\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.9122531414,2.76304470838), test loss: 2.902980645\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (6.82277297974,24.7477563046), test loss: 30.3290679455\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.483588516712,2.75414124934), test loss: 2.43050705492\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (9.64318370819,24.6499036118), test loss: 36.263497448\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (0.943880319595,2.74528867435), test loss: 2.99713649452\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.63604831696,24.5528259686), test loss: 30.6491868496\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.25395607948,2.73643146322), test loss: 2.50735645592\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (26.9433631897,24.4561774452), test loss: 37.624648428\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.8005657196,2.72771226419), test loss: 2.76684612036\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (1.67694056034,24.3590665281), test loss: 28.7720814228\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.42810952663,2.71902277342), test loss: 2.28620538116\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (9.97612857819,24.2636515007), test loss: 34.2627733469\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.826908648014,2.71046433254), test loss: 2.69900395274\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (32.3268966675,24.1695668766), test loss: 40.3739003658\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.58739376068,2.70211751907), test loss: 3.54803392291\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (9.80533599854,24.0750617203), test loss: 33.8930605173\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.27107691765,2.69369570164), test loss: 2.86293150038\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (15.032169342,23.981847115), test loss: 30.989824748\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.08374905586,2.68534145577), test loss: 2.71920318604\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (9.4611825943,23.8890779136), test loss: 31.4023784757\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.36091113091,2.67707366341), test loss: 2.4212968111\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (19.2308559418,23.7962328801), test loss: 35.1393971682\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.11447560787,2.668817103), test loss: 2.98409341872\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.16169476509,23.7043777644), test loss: 33.2973505497\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.98724412918,2.66073137911), test loss: 2.4284982264\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (7.04288101196,23.61395346), test loss: 36.150905931\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.441278755665,2.65273115229), test loss: 3.06720067859\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (11.8260536194,23.523021737), test loss: 30.7103076935\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.01124298573,2.64480689336), test loss: 2.32140773684\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (4.80138969421,23.4336023321), test loss: 34.8670479655\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.19097733498,2.6368716689), test loss: 2.92507295907\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (21.4312133789,23.3444093529), test loss: 32.2619054317\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.573233246803,2.62897044531), test loss: 2.49615135193\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (9.47089958191,23.2554247948), test loss: 37.0436280251\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (3.56542825699,2.62118626835), test loss: 2.91643794477\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (6.05662679672,23.1669648459), test loss: 30.9244184971\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.92362344265,2.61344847862), test loss: 2.49309910536\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (7.0332403183,23.0796887281), test loss: 34.9057164192\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.38587260246,2.60581682353), test loss: 2.68194781542\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (7.39825868607,22.9921721367), test loss: 29.2953116417\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.53378009796,2.59831078924), test loss: 2.35314733386\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (7.17759847641,22.9061502614), test loss: 34.4013830781\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.3998708725,2.59076370349), test loss: 2.74744666964\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (25.4358520508,22.8207829556), test loss: 31.3518990993\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.10850667953,2.58323928135), test loss: 2.78479877114\n",
      "\n",
      "MC # 5, Hype # hyp5, Fold # 9\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold9/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (336.427856445,inf), test loss: 198.145476913\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (332.861175537,inf), test loss: 376.485496521\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (21.2337684631,76.733962327), test loss: 43.4760515213\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.87776267529,47.423626544), test loss: 3.40175800398\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (34.598777771,60.9040722127), test loss: 35.4890595436\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.66165590286,25.3378483106), test loss: 2.98946450353\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (32.5767211914,55.3021993723), test loss: 41.7043625832\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.33988583088,17.9566879264), test loss: 3.88587296009\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (104.099113464,52.5724957287), test loss: 33.7295966625\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.47700643539,14.2702301689), test loss: 2.91436618567\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (59.7978782654,50.7482783988), test loss: 41.0346057415\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.66880702972,12.0577485772), test loss: 3.63672591746\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (23.4264755249,49.4715894557), test loss: 40.4100753307\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.86222422123,10.5718326051), test loss: 3.60487512052\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (42.828830719,48.454869061), test loss: 40.1040930748\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.88247013092,9.50814536342), test loss: 3.41937719584\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (40.6621780396,47.6567841191), test loss: 40.8505514145\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (5.96026992798,8.70542278272), test loss: 3.65419389009\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (38.0954589844,47.0420604604), test loss: 35.6240931034\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.3345272541,8.07927637969), test loss: 3.06484673321\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (53.9265365601,46.4604267948), test loss: 41.2726912975\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.26248407364,7.57926378486), test loss: 3.60436849594\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (28.6337966919,45.9613283173), test loss: 37.7425251961\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.25826978683,7.16647847168), test loss: 2.84853473008\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (45.4435310364,45.4585619235), test loss: 41.4897224426\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.46228456497,6.82026900207), test loss: 3.85323364139\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (27.5711898804,45.0014565402), test loss: 37.3864811897\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.29037189484,6.52602014819), test loss: 2.74461397231\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (34.1172370911,44.545737173), test loss: 40.6640893698\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.16832923889,6.26705981138), test loss: 3.82306780815\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (62.3256187439,44.1453239699), test loss: 35.0562102497\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (5.55495166779,6.04535280329), test loss: 2.79724473357\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (18.050693512,43.7168789851), test loss: 40.7223816872\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.92296183109,5.8501443547), test loss: 3.53111580014\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (71.1009140015,43.3017634074), test loss: 33.3777481318\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.37192749977,5.67447479362), test loss: 2.84696674943\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (74.1824035645,42.8588989472), test loss: 39.8941791534\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.94283783436,5.51634099531), test loss: 3.33512327969\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (33.5489501953,42.418995372), test loss: 31.1310090542\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (6.3196349144,5.37270718884), test loss: 2.71883717477\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (20.5694160461,41.9947078933), test loss: 36.7833703041\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.582924366,5.24128889796), test loss: 3.45771415234\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (31.3389892578,41.5506183023), test loss: 27.5526692867\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.84490513802,5.12252534186), test loss: 2.56316888332\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (31.4050788879,41.1131019771), test loss: 34.0159231901\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.82011556625,5.01334724202), test loss: 3.23756812811\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (9.9663438797,40.6449817128), test loss: 31.7801738262\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.80021429062,4.91169626187), test loss: 3.32286448479\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (16.8433246613,40.1846957264), test loss: 33.6370916843\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.12145900726,4.81639106501), test loss: 3.29707487822\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (38.1398582458,39.7177595307), test loss: 32.256182909\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.11959505081,4.72308302029), test loss: 3.59826886058\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (19.8913249969,39.2643103916), test loss: 32.4422315121\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (6.4680891037,4.63817966495), test loss: 3.44967335761\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (20.1598529816,38.8026679664), test loss: 32.2395373821\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.65768957138,4.55838243179), test loss: 3.4726947248\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (13.0233945847,38.3533639875), test loss: 26.879506731\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.70706415176,4.48340716199), test loss: 2.63225293159\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (42.5051193237,37.8989015219), test loss: 34.7814087391\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.459985375404,4.41221104356), test loss: 3.53353118896\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (11.5386457443,37.4599927182), test loss: 28.4500080585\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.77688026428,4.34512818708), test loss: 2.56377762556\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (24.9457206726,37.0383103621), test loss: 33.564460516\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.58265209198,4.28081796766), test loss: 3.6010283649\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (40.3281707764,36.6199877865), test loss: 28.6589003563\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.79007458687,4.22063129942), test loss: 2.50261304379\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (20.5022621155,36.2221212977), test loss: 32.8421990395\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.4718914032,4.16410314959), test loss: 3.40303700566\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (21.4405670166,35.8263044574), test loss: 28.7795731544\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.39211630821,4.11003754303), test loss: 2.73413561136\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (6.40238523483,35.4469692082), test loss: 35.7690144062\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.16407012939,4.05875389769), test loss: 3.43099311888\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (63.4726600647,35.0811881148), test loss: 27.5835171223\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (4.37403297424,4.00827247216), test loss: 2.7452160418\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (22.0358791351,34.7278003969), test loss: 34.6774872541\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.0606944561,3.9612217669), test loss: 3.33620933592\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (48.5332527161,34.3856523746), test loss: 25.5422819138\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.26034116745,3.91641835302), test loss: 2.68190903664\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (36.0506019592,34.0579705648), test loss: 32.1164108276\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (4.7051486969,3.87367122133), test loss: 3.19104123712\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (19.110376358,33.7332150271), test loss: 25.1248110294\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.640366137028,3.83223710796), test loss: 2.61493232548\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (18.0463523865,33.4254531008), test loss: 32.4709292412\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.4415242672,3.7925896428), test loss: 3.3325440675\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (17.8642921448,33.1297410549), test loss: 31.2604290962\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.16639196873,3.7539058334), test loss: 3.14775524139\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (7.84433364868,32.8370260953), test loss: 32.2847584486\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.813054323196,3.71704558348), test loss: 3.3030819878\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (21.2110137939,32.5608662545), test loss: 30.5787937641\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.803268432617,3.68218948078), test loss: 3.26164911985\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (35.2903175354,32.2888160471), test loss: 33.4627581239\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.85919451714,3.64838354766), test loss: 3.33219299167\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (16.0226860046,32.0270153537), test loss: 32.0868041992\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.693749904633,3.61591508569), test loss: 3.3303553462\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.1442756653,31.7736683649), test loss: 28.5862314701\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (5.30659723282,3.58352890227), test loss: 2.63959089667\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (32.9665603638,31.52680467), test loss: 33.1166451931\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.95736670494,3.55290090185), test loss: 3.52536302805\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (31.700176239,31.2868925252), test loss: 29.0769785881\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.98482882977,3.52326035712), test loss: 2.52843259722\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (22.4304656982,31.0576415802), test loss: 34.1467921853\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.83157849312,3.49499282711), test loss: 3.40135142207\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (31.3979530334,30.8275909661), test loss: 30.0314789772\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.75030422211,3.46721159656), test loss: 2.45375877619\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (34.5923995972,30.6083343476), test loss: 34.2158349514\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.90589773655,3.44026543887), test loss: 3.33139401078\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (5.85415887833,30.3952357647), test loss: 30.6406358242\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.35870552063,3.41366719457), test loss: 2.62437974215\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (8.60441303253,30.1835855424), test loss: 34.5832527637\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.25814914703,3.38819002475), test loss: 3.20220211148\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (7.38638925552,29.9817998734), test loss: 28.5360042095\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.862268090248,3.36384548973), test loss: 2.64501089007\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (25.2612018585,29.7814390472), test loss: 34.3336150169\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.36670255661,3.34010026741), test loss: 3.14699673951\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (22.0356388092,29.5881842645), test loss: 24.8906106949\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.20563161373,3.31702663646), test loss: 2.48143444061\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (11.3642711639,29.4001064165), test loss: 32.6277688265\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.05523920059,3.29393047008), test loss: 3.13506819308\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (14.7743749619,29.2135769505), test loss: 25.6078645706\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.51385807991,3.27172191328), test loss: 2.59738669693\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (11.4438285828,29.0309685219), test loss: 32.5058996677\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.816004812717,3.25012234331), test loss: 3.28313676417\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (10.6884298325,28.856354588), test loss: 31.0937917233\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.740598142147,3.22940966423), test loss: 3.10104095638\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (18.1867923737,28.6806279051), test loss: 32.4620592594\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.48651432991,3.20902147561), test loss: 3.22810811698\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (59.4195861816,28.5116496207), test loss: 29.9297626495\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.85882592201,3.18899003048), test loss: 3.14456498921\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (11.6202030182,28.3438071368), test loss: 29.2912189007\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.851384282112,3.16905276288), test loss: 2.76112137437\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (10.2758235931,28.1777947309), test loss: 32.0192646027\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.35315847397,3.14984953757), test loss: 3.18502351642\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (5.82529830933,28.0185932171), test loss: 30.2397717953\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.797165393829,3.13146775283), test loss: 2.50389907956\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (17.1202487946,27.8593014546), test loss: 32.815422225\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.77538132668,3.11345977051), test loss: 3.35957426727\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (23.398475647,27.7043728036), test loss: 30.3490598679\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.58504652977,3.09580817155), test loss: 2.44537168443\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (33.7000045776,27.5536147338), test loss: 33.5835523605\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.51841902733,3.07805733337), test loss: 3.2594006449\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (20.3341560364,27.4027121493), test loss: 29.170895195\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.26877963543,3.06091410238), test loss: 2.53703167439\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (14.2767829895,27.2532755434), test loss: 34.7995272636\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.84903395176,3.04411111997), test loss: 3.28426911235\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (7.22152280807,27.1098136836), test loss: 30.2703065395\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.819953918457,3.02791179583), test loss: 2.56185058951\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (2.92116236687,26.9658848358), test loss: 33.418904829\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.767362356186,3.01193919336), test loss: 3.14273780733\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.1809158325,26.8273450251), test loss: 29.3003583908\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.708856880665,2.99615482359), test loss: 2.59460090101\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (12.3996419907,26.6879120338), test loss: 33.1937381506\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.76185715199,2.98038027437), test loss: 3.050776878\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.8202991486,26.5493772837), test loss: 25.7721774101\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.410315334797,2.96506188502), test loss: 2.40434171706\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (10.350903511,26.4167920061), test loss: 32.7791711569\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.25578784943,2.95037235759), test loss: 3.09660618156\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (15.5642337799,26.2841045635), test loss: 24.5700123787\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.390048801899,2.93588776032), test loss: 2.50516940802\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (19.2219696045,26.1536905837), test loss: 32.5170881748\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.34479498863,2.9216551351), test loss: 3.13049042523\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (21.7559967041,26.0263827452), test loss: 30.0555277348\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.35547113419,2.90728759879), test loss: 2.99553655684\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (43.8205337524,25.8995728619), test loss: 32.7391310453\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.98313176632,2.89339707292), test loss: 3.06434151679\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (7.95941400528,25.7725755999), test loss: 30.4216907024\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.40526521206,2.87967315004), test loss: 3.01224480867\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (9.1158618927,25.6500390369), test loss: 27.7714781761\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.925056874752,2.8663867041), test loss: 2.68355374634\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (9.39292812347,25.5269010142), test loss: 30.8016181946\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.558823823929,2.85327239091), test loss: 3.12847339511\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (17.8798046112,25.4085565201), test loss: 30.9361161709\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.20485746861,2.84028371125), test loss: 2.4851120919\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (15.2489433289,25.2892150539), test loss: 31.4203819275\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (4.30142116547,2.82735447397), test loss: 3.24848965257\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (7.41009426117,25.1692525312), test loss: 30.8952117443\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.08476734161,2.81460477419), test loss: 2.41531120241\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (9.44555091858,25.0540670329), test loss: 33.3111336231\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.492132842541,2.80232533504), test loss: 3.12894246578\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (8.37337875366,24.9393755198), test loss: 30.2839803696\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.557529509068,2.79026923744), test loss: 2.46336010098\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (3.89683365822,24.8264329004), test loss: 35.4245781898\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.81265425682,2.77840183882), test loss: 3.23755865991\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (13.116610527,24.7149889077), test loss: 29.4171380997\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.84149003029,2.76637398576), test loss: 2.56618966907\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (23.0477218628,24.6036250268), test loss: 34.2224106073\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.76541507244,2.75466281547), test loss: 3.00886887908\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (24.3997001648,24.4929707614), test loss: 28.9149651051\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.61147928238,2.74314183918), test loss: 2.50948592573\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (15.3693742752,24.3853156859), test loss: 32.4981962681\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.916639506817,2.73188742211), test loss: 2.92112129778\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.5482578278,24.276428884), test loss: 26.5406204224\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.32082056999,2.72081551962), test loss: 2.43358376622\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (15.4935855865,24.1714619524), test loss: 32.9021995068\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.05512070656,2.7097677748), test loss: 3.03743572533\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (11.4773855209,24.0661539738), test loss: 30.0988850594\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.694804191589,2.69879816383), test loss: 2.81395502985\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (28.3609046936,23.960393342), test loss: 33.3334959269\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.25626778603,2.68793107131), test loss: 3.1205499962\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (13.9410982132,23.8570787177), test loss: 29.6634613991\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.0607149601,2.67742778116), test loss: 2.96168868542\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (13.5610198975,23.7545389697), test loss: 33.4754698277\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.30651009083,2.66710558124), test loss: 3.06872746497\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (15.65782547,23.6541117442), test loss: 31.2588302135\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.60914683342,2.65691487872), test loss: 3.08709894121\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (12.07787323,23.554466397), test loss: 29.7776925564\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.35179197788,2.64656904072), test loss: 2.63750346154\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (5.90684509277,23.4538818602), test loss: 33.1461213589\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.804747641087,2.63646894029), test loss: 3.16558498144\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (9.18595504761,23.3543209279), test loss: 33.3478987217\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.85073274374,2.62648720174), test loss: 2.52154511809\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (19.5353298187,23.2576521207), test loss: 33.0572070599\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.15154755116,2.61671965395), test loss: 3.21077108383\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (9.74240207672,23.1598822092), test loss: 31.9322497845\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.94925737381,2.60713107224), test loss: 2.41819844693\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (4.24247264862,23.0643177534), test loss: 34.2429978848\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.82523727417,2.59746375495), test loss: 3.1570604831\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (9.58822631836,22.968858041), test loss: 32.367536068\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.565848112106,2.58784855652), test loss: 2.54174174368\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (19.8587169647,22.8734410373), test loss: 35.4594036102\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.20969867706,2.57836552164), test loss: 3.13792020977\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.0489492416,22.779767263), test loss: 29.3272049427\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.83078312874,2.56913232644), test loss: 2.57321956009\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (29.8803672791,22.6863658054), test loss: 36.0394926071\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (2.74237632751,2.56004904358), test loss: 2.96778866351\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (11.9575405121,22.5942601252), test loss: 26.9007340908\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.00903809071,2.5510204564), test loss: 2.44374614358\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (5.89529705048,22.5035551275), test loss: 33.190964818\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (0.818385481834,2.54191338161), test loss: 2.83043979406\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (15.9098587036,22.4119055429), test loss: 27.0300969839\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (3.35145378113,2.53300634847), test loss: 2.46389578581\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (6.11017417908,22.3202560937), test loss: 33.996091032\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.780266165733,2.52414443883), test loss: 3.04855198413\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (15.5571870804,22.2313708316), test loss: 30.8341851711\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.469378679991,2.51545651023), test loss: 2.7714710921\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (9.14737129211,22.1419406571), test loss: 32.8930695057\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.59042429924,2.50697281038), test loss: 3.08585015684\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (7.32885408401,22.0541771931), test loss: 29.1690849304\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.90513181686,2.4983824526), test loss: 2.85142079294\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (6.00749492645,21.9657650561), test loss: 35.8650100708\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.45341622829,2.48984206434), test loss: 3.06025174558\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (30.2185707092,21.8781629448), test loss: 31.9371620178\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (1.64279413223,2.48137132194), test loss: 3.10046341121\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (15.4774961472,21.7915890322), test loss: 31.9301537275\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.13558602333,2.47314070526), test loss: 2.68469115049\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (8.7384185791,21.7054316238), test loss: 32.4613685131\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.68322193623,2.46501021876), test loss: 3.17002038658\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (11.7254533768,21.6198626069), test loss: 32.3259690285\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.671273648739,2.45694389272), test loss: 2.52551493645\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (5.09997177124,21.5353982635), test loss: 34.8602514744\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.40977287292,2.44879067566), test loss: 3.1529420808\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (19.6625709534,21.4508876573), test loss: 32.3257322311\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (0.986875712872,2.44080889121), test loss: 2.45621317923\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (17.5651226044,21.3661351031), test loss: 34.7626005173\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.88253605366,2.43287011115), test loss: 3.09480382204\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.89896535873,21.2827297219), test loss: 33.506301403\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.14518427849,2.42509292114), test loss: 2.60372442231\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (8.59367752075,21.1994739581), test loss: 34.8277975559\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.36442685127,2.4174804344), test loss: 3.01180279702\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (4.73870801926,21.1178967288), test loss: 29.8554481745\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.41102218628,2.40978839303), test loss: 2.62116555572\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (8.73767662048,21.0356841916), test loss: 35.2037046194\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (0.60029232502,2.4021096074), test loss: 2.90181384087\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (9.20954608917,20.9535744974), test loss: 29.0457592487\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (1.05421578884,2.39455250011), test loss: 2.52475786805\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (7.01101255417,20.8724950796), test loss: 34.7927082062\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.0690227747,2.38714906502), test loss: 2.8973889634\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (8.20714759827,20.7921564638), test loss: 26.9024766207\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.876533031464,2.37986432981), test loss: 2.52594850659\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (17.5764408112,20.7124031435), test loss: 33.3288459301\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.4493625164,2.37261003549), test loss: 3.04599384069\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (6.74992990494,20.6327058276), test loss: 30.8243508816\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.44595599174,2.36531031034), test loss: 2.76719122827\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (9.50775146484,20.5534127348), test loss: 33.7913239956\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.07820367813,2.35814543324), test loss: 2.99859946221\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (12.6719036102,20.4741879933), test loss: 31.095983839\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.769880414009,2.35097996881), test loss: 2.84972810745\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (2.96431469917,20.3959213651), test loss: 31.504253912\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.359692215919,2.3439529323), test loss: 2.71036190987\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (8.90770721436,20.3175309713), test loss: 31.0764534473\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.69435358047,2.33713705856), test loss: 2.99252832532\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (3.30362033844,20.2405996655), test loss: 33.3030495167\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.57665634155,2.33017561396), test loss: 2.57520371526\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (10.2555332184,20.1636146167), test loss: 32.9547544718\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.37599563599,2.32326071337), test loss: 3.12098987699\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.37805223465,20.0864526318), test loss: 33.6345847607\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.11389541626,2.31641345258), test loss: 2.49014052078\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (6.95491456985,20.0098829775), test loss: 35.1184885979\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.27922308445,2.30972582619), test loss: 3.10887464881\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (17.5038738251,19.934392307), test loss: 35.0198884964\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.586177766323,2.30312384553), test loss: 2.80007999837\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (8.85406303406,19.8592388523), test loss: 36.3649354935\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.911655783653,2.29654246504), test loss: 3.08548105955\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (5.76576089859,19.7841815456), test loss: 32.045411253\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.71879482269,2.2899186693), test loss: 2.63576569259\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (13.8837947845,19.7092583621), test loss: 35.8068868637\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.723874330521,2.28342830804), test loss: 2.94665623158\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (9.16486358643,19.6346546107), test loss: 32.9985293865\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.01134967804,2.2769482473), test loss: 2.64630267173\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.72766208649,19.5607790629), test loss: 34.6944851875\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.242762282491,2.27052747879), test loss: 2.81165004596\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (2.48662185669,19.4870636735), test loss: 28.7180912971\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.229036226869,2.26432407524), test loss: 2.45476763546\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (8.78744411469,19.4141817158), test loss: 35.5436807156\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.24627971649,2.25803958835), test loss: 3.07754496634\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (10.3972969055,19.3416362444), test loss: 27.5579293728\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.31965112686,2.25173942038), test loss: 2.65412456393\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (6.64356946945,19.268870213), test loss: 36.4676544666\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.48367786407,2.24548197215), test loss: 3.11471970677\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (5.06366109848,19.1966557715), test loss: 30.792945075\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.96222579479,2.23938217529), test loss: 2.78485314846\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (21.6820049286,19.1252521682), test loss: 42.8964739799\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (2.01305484772,2.23338780637), test loss: 3.6299923718\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (4.49223470688,19.0541544622), test loss: 31.3266482115\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.45308303833,2.22736607265), test loss: 2.81622840166\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (3.15367078781,18.9833653255), test loss: 31.8472500324\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.553275763988,2.22130289323), test loss: 2.72317393124\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (11.5002326965,18.912708261), test loss: 31.8371092796\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.369870305061,2.21537146234), test loss: 2.97795400023\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (8.84002494812,18.8421330501), test loss: 34.943713522\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.309850752354,2.20946530897), test loss: 2.59823945835\n",
      "\n",
      "MC # 5, Hype # hyp5, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold10/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (366.368652344,inf), test loss: 201.250828171\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (263.41104126,inf), test loss: 320.950997925\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (54.8953819275,98.235763134), test loss: 48.6227923393\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.38956212997,32.6712906011), test loss: 3.35290416479\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (65.2970123291,72.717607192), test loss: 37.0521486759\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.62616658211,17.8076831519), test loss: 2.63528125286\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (39.0616989136,64.0354608479), test loss: 47.0541246891\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.14261388779,12.8440628588), test loss: 3.41686066985\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (50.5106086731,59.7587424219), test loss: 39.3835837841\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.68992400169,10.366304571), test loss: 3.56057807803\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (87.1081695557,57.1132949812), test loss: 45.1137353182\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.52845442295,8.87576501238), test loss: 3.53285938501\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (33.6197929382,55.2823075221), test loss: 43.6331728458\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.45042443275,7.87515004386), test loss: 3.56630200148\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (64.2698516846,53.8946339984), test loss: 43.6038517475\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (6.51161003113,7.15801149442), test loss: 2.69331803024\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (39.3201293945,52.8245754598), test loss: 46.7959448338\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.96380615234,6.61631163644), test loss: 3.50598714352\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (46.7793807983,51.9706367115), test loss: 42.6791757584\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.74342823029,6.19258035502), test loss: 2.35636522472\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (33.3181228638,51.3080948447), test loss: 45.3083662033\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (0.957391858101,5.85133521703), test loss: 3.59503282905\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (22.1955871582,50.7345860537), test loss: 40.0457929373\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.58574128151,5.57242373997), test loss: 2.55004764795\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (33.0520019531,50.2229083777), test loss: 46.5428905964\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.49938774109,5.33989342692), test loss: 3.43501549363\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (97.1718139648,49.7647482948), test loss: 39.4340932369\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.21601438522,5.14045771785), test loss: 2.60918191671\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (68.8924560547,49.2989166819), test loss: 48.0844214916\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.25129795074,4.96719135083), test loss: 3.23180514574\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (58.2041282654,48.9158579363), test loss: 37.4205101013\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.10152029991,4.81585362806), test loss: 2.49104652405\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (8.7913608551,48.5349087007), test loss: 45.1245283127\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.15401005745,4.68156821141), test loss: 3.20252652168\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (46.2361526489,48.206548567), test loss: 38.3157405376\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.47833538055,4.56353806389), test loss: 3.04004437476\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (58.9279441833,47.8799662328), test loss: 42.5837034702\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.45252847672,4.45846823591), test loss: 3.29246172309\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (26.261713028,47.5518827981), test loss: 37.8255311489\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.09123945236,4.3622455313), test loss: 3.33481099904\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (36.2101669312,47.2222680236), test loss: 37.8587557554\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.37229394913,4.27462462799), test loss: 2.57604370117\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (31.653875351,46.8821182589), test loss: 39.3408987999\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (6.62339401245,4.19492929717), test loss: 3.30620483756\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (50.9976463318,46.5398342212), test loss: 39.159909153\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (5.72730445862,4.12024716282), test loss: 2.52232161313\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (19.9259147644,46.2243124864), test loss: 40.9963155746\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.39533638954,4.05331108935), test loss: 3.46488641351\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (38.4785766602,45.8889573943), test loss: 38.0265394688\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.50392985344,3.99041933838), test loss: 2.29570822418\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (34.216091156,45.5486883619), test loss: 39.9100370407\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (0.957948744297,3.93129624348), test loss: 3.26216293275\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (14.5856161118,45.192125139), test loss: 34.608773303\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.93132615089,3.87662268884), test loss: 2.47096351534\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (28.085319519,44.8283632631), test loss: 40.6677348614\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.414686918259,3.82452529939), test loss: 3.12065593004\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (32.1493339539,44.4664418084), test loss: 31.893475771\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.10909295082,3.77464491342), test loss: 2.44725582004\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (33.4754676819,44.1086174341), test loss: 40.5207244396\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.83449208736,3.72856018455), test loss: 3.16295986474\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (36.9035758972,43.7340376795), test loss: 27.6220981598\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (3.86681890488,3.68486518315), test loss: 2.39623244405\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (23.8957176208,43.3528575388), test loss: 37.6949847698\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.38020730019,3.64342013679), test loss: 3.09559440613\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (17.8564910889,42.9294690662), test loss: 29.7355093241\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.02410197258,3.60421899479), test loss: 3.20324856341\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (33.1898117065,42.4841930064), test loss: 36.098819828\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.29143762589,3.56704007159), test loss: 3.27158685327\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (4.93726301193,42.0424748922), test loss: 30.837010622\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.10663104057,3.53087673095), test loss: 3.35237776786\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (23.6991729736,41.6131515756), test loss: 30.8443288565\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.51504087448,3.49694822042), test loss: 2.56554603726\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (7.16974401474,41.1807406153), test loss: 34.2261612892\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.01719868183,3.46430833498), test loss: 3.25521420538\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (18.0401611328,40.7573943986), test loss: 31.7877755642\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (4.91626214981,3.43365883251), test loss: 2.36433170885\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (46.3121185303,40.3431163145), test loss: 34.4400237679\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.62128317356,3.40419440427), test loss: 3.29666757584\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (33.0134010315,39.9241483323), test loss: 28.1040433884\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.35397148132,3.375551625), test loss: 2.41361654401\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (90.7908172607,39.5290351869), test loss: 34.9088394642\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (3.13949918747,3.34784812003), test loss: 3.20249195695\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (17.3559494019,39.1429450139), test loss: 28.6477300167\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.786238193512,3.32099718055), test loss: 2.51462867856\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (10.7347154617,38.7667962882), test loss: 37.0999445915\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.34590148926,3.29559672872), test loss: 3.18642770946\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (20.4226074219,38.399506969), test loss: 26.5607448101\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.85069441795,3.27140209743), test loss: 2.47662602589\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (8.59396934509,38.0458896711), test loss: 36.0057953715\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.75832128525,3.24801343255), test loss: 3.06087552607\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (15.8557834625,37.6943329341), test loss: 29.8134325504\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.88834416866,3.22516473383), test loss: 3.11099689603\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (17.220659256,37.3551760953), test loss: 36.0028778076\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.57884836197,3.20285834576), test loss: 3.21964242458\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (16.369392395,37.0283800466), test loss: 28.9351448536\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.22034597397,3.18093156434), test loss: 3.22319612503\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (30.9079208374,36.712394774), test loss: 32.5667885303\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (4.82683467865,3.1605510309), test loss: 2.81808064282\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.5131292343,36.4017627295), test loss: 28.6848667622\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.49411070347,3.14026472653), test loss: 3.28467873037\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (22.2566833496,36.1043586254), test loss: 32.3118900537\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.902179837227,3.12092662097), test loss: 2.51814758778\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (16.0433654785,35.8073770403), test loss: 30.9602618933\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.29080283642,3.10210593174), test loss: 3.29683195353\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (5.89277172089,35.5198081135), test loss: 31.5256445408\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.02755606174,3.08368112865), test loss: 2.25842440575\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (29.7612266541,35.2455301607), test loss: 33.0855919838\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.80825293064,3.06528294302), test loss: 3.19555153251\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (9.09087181091,34.9770387769), test loss: 30.1224690437\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.10879564285,3.04781923074), test loss: 2.50193398893\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (18.4874668121,34.7124570993), test loss: 36.8094248295\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.04191303253,3.03070931858), test loss: 3.12303968668\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (14.7900485992,34.4575107252), test loss: 28.0938895226\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (3.85362958908,3.01420922046), test loss: 2.50791621208\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (27.984790802,34.2053125644), test loss: 37.451011014\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.391608953476,2.99809226711), test loss: 2.97279468328\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (21.806427002,33.9576528878), test loss: 25.4604443073\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.76594018936,2.98234097252), test loss: 2.38123124018\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (5.0545797348,33.7208946512), test loss: 36.5717816591\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.570728302,2.96654169107), test loss: 3.071728158\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.2983360291,33.4900865888), test loss: 28.7472726107\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.73539555073,2.95140674636), test loss: 3.06623760462\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (5.65785121918,33.2604935523), test loss: 37.1835868835\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.38922810555,2.93648271259), test loss: 3.27172749043\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (12.0175457001,33.0379436601), test loss: 28.9407558918\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.1851875782,2.92213060326), test loss: 3.20161210001\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (51.2668571472,32.8207717272), test loss: 33.302773571\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.36164474487,2.90815634386), test loss: 2.62181620896\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (24.7937068939,32.6034199226), test loss: 31.0480435848\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.08127355576,2.894267548), test loss: 3.17613150179\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (13.8659362793,32.3947893409), test loss: 32.7243571281\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.712000727654,2.88053679423), test loss: 2.47221733332\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (24.5547142029,32.1920361394), test loss: 33.5464799404\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (1.84847807884,2.86701980625), test loss: 3.19190266728\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (46.5369415283,31.9905352633), test loss: 28.8319079876\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.38182044029,2.85382581111), test loss: 2.21989036202\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (3.43967032433,31.7926170524), test loss: 34.6003065586\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.25856423378,2.84114208488), test loss: 3.105337134\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (20.7587242126,31.6016075115), test loss: 30.2788978577\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.26668298244,2.82867962115), test loss: 2.47680033743\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (10.9067554474,31.4074715737), test loss: 36.5908837795\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.24614167213,2.81631742297), test loss: 3.0186575979\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (3.7704577446,31.2220079952), test loss: 28.9926534653\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.73980152607,2.80398949269), test loss: 2.46034937799\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (23.4559516907,31.0391387614), test loss: 36.9025143862\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.04399502277,2.79183677581), test loss: 2.8676861614\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (17.2202148438,30.8590278464), test loss: 28.9454312325\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (5.37104177475,2.78018108843), test loss: 3.01831240356\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (14.1365118027,30.6813453247), test loss: 36.8622916698\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.944419801235,2.76856865396), test loss: 3.10984368622\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (27.1347122192,30.5087277617), test loss: 27.3920046568\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.6540569067,2.75728209971), test loss: 3.04887906313\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (21.5954418182,30.3347969469), test loss: 37.2121634007\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.05912828445,2.74621944208), test loss: 2.93899876475\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (6.42553806305,30.1654966234), test loss: 28.1006180286\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.8743519783,2.73523895861), test loss: 3.13734084964\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (16.2171134949,30.000363032), test loss: 32.4570006847\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.81574487686,2.72413923522), test loss: 2.43053234816\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (40.636844635,29.8381845698), test loss: 32.2913051128\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.51169228554,2.71348925458), test loss: 3.11192920953\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (21.0950889587,29.6751233272), test loss: 32.9097406387\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.57902956009,2.70285901827), test loss: 2.19008060396\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (13.0145139694,29.5174948656), test loss: 33.6168029547\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.65940093994,2.69243247968), test loss: 3.060729976\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (28.8547668457,29.3589904475), test loss: 32.5137915134\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.965432405472,2.68231118251), test loss: 2.44110693336\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (5.47995948792,29.203478685), test loss: 35.61190238\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.784729659557,2.67224939534), test loss: 3.01562072337\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (7.01822757721,29.0527067852), test loss: 28.2259618521\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.96969223022,2.66208574513), test loss: 2.41282413453\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (25.253036499,28.9035364289), test loss: 37.390493536\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (0.91056740284,2.65234899144), test loss: 2.77151109874\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (35.7503509521,28.7543288066), test loss: 26.1675440788\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.4938557148,2.6425932427), test loss: 2.31507014632\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (6.68904352188,28.6081213749), test loss: 36.9679469824\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.79726743698,2.63306801556), test loss: 2.93559427559\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (11.6972064972,28.4636826733), test loss: 28.427456069\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.89222407341,2.62376507612), test loss: 2.98227233291\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (11.602853775,28.3200512251), test loss: 36.7887547731\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.622395813465,2.61448139335), test loss: 3.14909827113\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (12.6234493256,28.1799351226), test loss: 27.8440911651\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (4.35523605347,2.60524646326), test loss: 3.06157891452\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.4865312576,28.0422568253), test loss: 32.4731725216\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.382978618145,2.5961135917), test loss: 2.53294737935\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (32.8690719604,27.9039619065), test loss: 32.823163414\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.54209542274,2.58710054942), test loss: 3.05952779949\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (20.9955368042,27.7687130636), test loss: 33.9476624966\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.10814619064,2.57836755688), test loss: 2.38097952306\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (28.5299072266,27.6362873647), test loss: 32.7976099968\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.72997713089,2.56973977149), test loss: 3.17681597471\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (23.9677658081,27.5021182984), test loss: 31.6370246887\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.70683026314,2.56118736782), test loss: 2.24292761087\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (17.289264679,27.3717088767), test loss: 34.2736016512\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.11377191544,2.55251196228), test loss: 2.98483296037\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (10.0734891891,27.2425865759), test loss: 31.0676374435\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.07624948025,2.54405408885), test loss: 2.37784593999\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (32.5549888611,27.114833979), test loss: 37.4097680569\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.85705184937,2.53573416079), test loss: 2.91043241322\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (28.4396686554,26.9877651994), test loss: 28.8256712437\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.02808022499,2.52755180831), test loss: 2.5112235263\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (17.2584266663,26.8639516288), test loss: 36.6328985214\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.75972485542,2.51948016638), test loss: 2.83909168243\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.92322444916,26.7387479702), test loss: 26.814262104\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.17703390121,2.51152667112), test loss: 2.63709876239\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (12.5628557205,26.6163831692), test loss: 38.0275584459\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.97769188881,2.50357051004), test loss: 3.08514511883\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (7.31564998627,26.4957399458), test loss: 27.151938343\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (2.73669528961,2.49558853528), test loss: 2.89283722341\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (17.0340175629,26.3760684994), test loss: 38.2491222858\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.45918369293,2.48785269042), test loss: 3.21267680526\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (6.86223125458,26.2566523873), test loss: 32.4922008038\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.15346372128,2.48009227288), test loss: 3.06760299802\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (13.760807991,26.140076776), test loss: 34.0644721985\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.16387891769,2.47247583052), test loss: 2.45828634501\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (6.22144699097,26.0227061711), test loss: 31.1328267574\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.08898496628,2.46508077815), test loss: 3.03337448537\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (22.8110141754,25.9076210377), test loss: 32.8661822319\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.21702575684,2.45765272515), test loss: 2.23540130556\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (4.98145914078,25.7943524143), test loss: 34.9148934364\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.890515267849,2.45010159299), test loss: 3.01254192889\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (11.5151643753,25.6814669693), test loss: 33.4713946342\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.70779204369,2.4428925156), test loss: 2.34277315289\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (20.3629302979,25.5686775753), test loss: 39.357313323\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.19689047337,2.43557832706), test loss: 2.99787556827\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (6.9575138092,25.458161059), test loss: 29.5260961533\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.524985730648,2.42842159292), test loss: 2.39400864542\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (30.1798744202,25.3481679271), test loss: 38.5322721481\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (2.50654149055,2.42140226165), test loss: 2.76687488258\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (27.6345405579,25.2391452695), test loss: 28.1807349682\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.72425305843,2.41443947067), test loss: 2.32856777012\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (17.048412323,25.1313227223), test loss: 37.3052016735\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (3.53992152214,2.40735857218), test loss: 2.85968279541\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (16.2896080017,25.0249897301), test loss: 29.293522644\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.01907110214,2.4004825253), test loss: 3.01775424331\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (6.24278640747,24.9177138084), test loss: 38.3603550434\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.862226545811,2.39356749684), test loss: 3.07471850812\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (8.15088558197,24.812601908), test loss: 28.6807244539\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.63075566292,2.38683113356), test loss: 2.98630896658\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (14.2243175507,24.7093943539), test loss: 40.2587273598\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.392855376005,2.38017804474), test loss: 2.93644177616\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (5.60229873657,24.6051556257), test loss: 32.0539523602\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.601272106171,2.37358159387), test loss: 3.05789576769\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (14.9790115356,24.5027679859), test loss: 36.5627860069\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (0.935459911823,2.36689002948), test loss: 2.53970067799\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (3.64450407028,24.4013184653), test loss: 36.9596009731\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.11773967743,2.3603468536), test loss: 3.12838097811\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (7.65242576599,24.2998182978), test loss: 33.8788905144\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.78741931915,2.35381616456), test loss: 2.44633517265\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (13.7841587067,24.1992162952), test loss: 34.8608906031\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.57640731335,2.34740194158), test loss: 2.93954330385\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (5.11754894257,24.100686848), test loss: 32.199501133\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.57143473625,2.34104219834), test loss: 2.49079453051\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (7.01345443726,24.0014186901), test loss: 39.1634643316\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.88751161098,2.33481382796), test loss: 2.92212143987\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.09686183929,23.9036256114), test loss: 30.3720425606\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.95469021797,2.32848049402), test loss: 2.48152886033\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (6.11661911011,23.806817044), test loss: 38.2270992279\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.72098708153,2.32219415285), test loss: 2.84661864936\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (16.9494018555,23.7102425078), test loss: 30.3966183662\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.88213372231,2.31605940826), test loss: 2.68052904308\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (21.1299057007,23.6140345869), test loss: 40.6692566872\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.11686062813,2.30987698187), test loss: 3.00735127628\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (1.89598464966,23.5193014089), test loss: 28.5443916321\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.401340097189,2.3037739184), test loss: 2.83995028734\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (16.9397201538,23.4242889235), test loss: 40.5160647035\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.667330920696,2.29783477621), test loss: 3.26447689533\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (14.7220401764,23.3306453876), test loss: 30.1777391434\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.07899284363,2.29189237231), test loss: 2.94876794815\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (11.0633983612,23.2383476055), test loss: 35.9951353073\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.872026085854,2.28583767073), test loss: 2.55065630376\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (11.9802465439,23.1456753868), test loss: 35.2654938221\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (3.70309710503,2.27998224733), test loss: 3.01560664326\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (11.6567001343,23.0531753894), test loss: 36.1253249645\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.52304363251,2.27407548183), test loss: 2.30497107059\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (7.46800136566,22.9621725331), test loss: 37.8929595947\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (3.13339614868,2.26822824051), test loss: 3.04365204871\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (12.0881814957,22.8712510873), test loss: 36.8756340027\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.692376613617,2.26250639137), test loss: 2.66905218363\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (5.61230039597,22.7811722079), test loss: 38.0746380329\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.13462018967,2.25683390269), test loss: 3.01042838097\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (3.5851559639,22.6921091683), test loss: 30.7417583942\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (0.833079338074,2.25103777158), test loss: 2.43233889639\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (12.5515451431,22.6035756562), test loss: 38.7044078588\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.04099261761,2.2454323409), test loss: 2.67656562924\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (5.45598983765,22.5145345365), test loss: 30.1406710863\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.20369315147,2.23977409398), test loss: 2.30879492462\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (10.4109230042,22.4267023085), test loss: 39.2874299049\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.85810732841,2.23415067203), test loss: 2.78926587701\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (29.4615287781,22.3400722008), test loss: 37.4243880272\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (2.22265148163,2.2286905725), test loss: 3.4805505842\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (15.1678228378,22.2530258889), test loss: 39.1689893723\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.46650934219,2.22323378515), test loss: 3.14864622504\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (7.45507621765,22.1668305106), test loss: 30.0883376598\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.11077606678,2.21769551113), test loss: 3.00875349939\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (7.65112781525,22.0816893391), test loss: 36.8827417135\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.31424856186,2.21228596828), test loss: 2.67655816078\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (7.76409864426,21.9961933418), test loss: 32.2673859596\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.16603839397,2.20684176799), test loss: 3.00214458108\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (4.98028230667,21.9111226479), test loss: 38.9067868233\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.23689436913,2.20147646848), test loss: 2.65935933292\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (5.85144615173,21.8277515879), test loss: 34.5391827583\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.72626376152,2.19615670756), test loss: 3.06006436944\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (8.57053947449,21.7435493859), test loss: 37.7955425739\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (0.485096693039,2.19092541693), test loss: 2.54257256985\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (5.16373920441,21.6606091596), test loss: 37.0823153019\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.24925327301,2.18562395835), test loss: 2.97497736216\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (14.6592693329,21.5783643244), test loss: 35.3886961937\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.36875510216,2.18037703515), test loss: 2.53270710409\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (11.3165035248,21.4960834386), test loss: 39.0833059072\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (3.25574350357,2.17520825901), test loss: 2.95183884352\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (9.23644161224,21.4140114611), test loss: 32.2049131393\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.27910077572,2.17001511182), test loss: 2.57017612159\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (3.95547842979,21.3332543402), test loss: 40.277001667\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.16377460957,2.16486990283), test loss: 2.90137423873\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (4.75185203552,21.2520155618), test loss: 30.6768964529\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.317336648703,2.15983163458), test loss: 2.44734868854\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (8.96687889099,21.1717906585), test loss: 40.7763976097\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (1.76282501221,2.15477571453), test loss: 3.06188673377\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (23.1569709778,21.0925314817), test loss: 29.6941465378\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.865301609039,2.14966761652), test loss: 2.94958649278\n",
      "run time for single CV loop: 7167.61409903\n",
      "\n",
      "MC # 5, Hype # hyp4, Fold # 6\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold6/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (312.161224365,inf), test loss: 207.54887886\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (258.688842773,inf), test loss: 300.305084229\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (26.6509189606,103.670083385), test loss: 46.4313261986\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.04256916046,46.6833347269), test loss: 3.19364905357\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (29.0901794434,73.5464403405), test loss: 39.3458712101\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (5.89034843445,24.9800851317), test loss: 3.12219153643\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (29.0392150879,63.4363150228), test loss: 44.961833024\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.26847362518,17.7207022257), test loss: 3.15057032704\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (14.0625200272,58.3769556565), test loss: 39.8145514965\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.616421937943,14.0943690556), test loss: 3.20320907235\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (14.1191978455,55.2508369905), test loss: 40.8023490429\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (1.12553930283,11.9191638148), test loss: 2.76650798917\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (48.7306404114,53.127843846), test loss: 42.1291993141\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.37032222748,10.4687666276), test loss: 3.20343346\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (59.3584098816,51.5724376243), test loss: 41.5671838284\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (2.83511352539,9.42726319702), test loss: 2.57986037135\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (159.522018433,50.3222910004), test loss: 44.592201376\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (5.25868225098,8.64557262509), test loss: 3.40472760797\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (45.5322036743,49.3287426533), test loss: 41.165098381\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (8.73211956024,8.03477216866), test loss: 2.69054737091\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (20.2645282745,48.5305252155), test loss: 42.2945868015\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.87210178375,7.54280599341), test loss: 3.38263384104\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (24.3039054871,47.8888422991), test loss: 38.0068836689\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.43641638756,7.14091205961), test loss: 2.70668756068\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (49.0258178711,47.3142476362), test loss: 44.0926920652\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.25975596905,6.80492868224), test loss: 3.28899280429\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (29.1084709167,46.8034282525), test loss: 37.7430329323\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.69817066193,6.51793232554), test loss: 2.93930416107\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (55.8811454773,46.3085176519), test loss: 42.7435278893\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (5.31629800797,6.27111761805), test loss: 3.06611796021\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (29.7627048492,45.816189926), test loss: 38.6434730053\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.62438201904,6.0595845186), test loss: 3.11512771845\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (57.7886238098,45.3867107087), test loss: 40.6180914402\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.60950279236,5.87192445768), test loss: 3.0710187912\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (26.1390075684,44.9906851586), test loss: 36.6644058704\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (4.92490768433,5.70468209914), test loss: 3.09669115245\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (21.2106380463,44.6124540522), test loss: 36.0422892332\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (3.97711706161,5.55466408541), test loss: 2.57443647981\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (30.6927890778,44.2443497858), test loss: 39.9249164104\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (0.893089354038,5.42015395611), test loss: 3.0590212822\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (49.4746589661,43.8877951028), test loss: 38.3149432182\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.1844830513,5.29589743471), test loss: 2.45432962179\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (36.8552131653,43.5273092843), test loss: 41.2342852116\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (4.64315509796,5.18250741156), test loss: 3.2722633481\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (34.5725402832,43.1656785148), test loss: 35.3820849657\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (2.85258245468,5.07743735311), test loss: 2.51325326562\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (45.0148506165,42.8321783797), test loss: 38.4027632713\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (3.54609322548,4.97914489961), test loss: 3.14862215519\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (45.3690032959,42.5103255893), test loss: 33.5750236988\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.25359511375,4.88812115164), test loss: 2.55472719371\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (46.192741394,42.1776475323), test loss: 40.1861472487\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.96331858635,4.80303333815), test loss: 3.19475457668\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (21.854341507,41.8460773288), test loss: 32.7702170372\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (0.526063203812,4.72224494313), test loss: 2.71733779907\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (93.8765563965,41.5089203906), test loss: 38.389483881\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (5.92724466324,4.6469123051), test loss: 2.87072830498\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (77.4248809814,41.1476647384), test loss: 34.0290238857\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.61907935143,4.57445927582), test loss: 2.94756069183\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (50.1514663696,40.8071359286), test loss: 36.6122074604\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.28130674362,4.50498573957), test loss: 2.94703320265\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (27.3681678772,40.4635279907), test loss: 32.1302100897\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (2.27178692818,4.438055384), test loss: 2.96677537858\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (35.2184638977,40.1243990306), test loss: 30.488722229\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (7.22947311401,4.37465169209), test loss: 2.33132838011\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (16.8112258911,39.7793067634), test loss: 34.4401078224\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.22942519188,4.31367351497), test loss: 2.96843787879\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (24.8754730225,39.4307166499), test loss: 32.4250268936\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (0.704627513885,4.25489611323), test loss: 2.24967325777\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (23.2248325348,39.0732997769), test loss: 35.605576849\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.56594061852,4.19916021289), test loss: 3.12794611454\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (36.4403991699,38.7147278769), test loss: 29.8215662003\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.05195999146,4.14545277737), test loss: 2.24288166165\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (5.86564254761,38.3677904999), test loss: 34.484352541\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (4.30785799026,4.09323331448), test loss: 3.00463836193\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (12.4297323227,38.0261643176), test loss: 29.132852459\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.46240139008,4.04341109518), test loss: 2.57265557647\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (9.2120513916,37.67946457), test loss: 36.3155745506\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.46741199493,3.99577323957), test loss: 3.14639261365\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (44.8262557983,37.3399192202), test loss: 27.4243808746\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.18848288059,3.95053300855), test loss: 2.63843798488\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (23.1552886963,37.0049422032), test loss: 34.4363829851\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.87856578827,3.90720999107), test loss: 2.85616237521\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (9.05171012878,36.6613619739), test loss: 30.8515844822\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (0.725720524788,3.8652990239), test loss: 3.0459068343\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (21.4191856384,36.3335834205), test loss: 34.2502427101\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (2.39489102364,3.82478837986), test loss: 3.03666886091\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (17.6072406769,36.0139759825), test loss: 30.0352303505\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.40670251846,3.78570436399), test loss: 3.13238116205\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (20.0156440735,35.7034293741), test loss: 28.2543290854\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.02221703529,3.74867661487), test loss: 2.38008926809\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (2.15844225883,35.3969666153), test loss: 32.091700983\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.55511689186,3.71294865821), test loss: 3.12478946447\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (11.8026256561,35.0997240449), test loss: 30.4721613884\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.03252053261,3.67885167606), test loss: 2.33973254561\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (22.1448860168,34.8046519356), test loss: 33.1875532627\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (0.62403100729,3.64609209468), test loss: 3.21424899995\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (17.340845108,34.5165596298), test loss: 29.3250439167\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.937904715538,3.61458339403), test loss: 2.37953674495\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (19.3388729095,34.2429886295), test loss: 34.9466078758\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (5.14321804047,3.58370012463), test loss: 3.12817430496\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (14.0685472488,33.9792478392), test loss: 29.2384057522\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.94014525414,3.55398571183), test loss: 2.76401472092\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (55.0958251953,33.7200959419), test loss: 36.7123224735\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (2.6481564045,3.52559600706), test loss: 3.24161475897\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (12.5436725616,33.4685420746), test loss: 26.8613227367\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.5782160759,3.49851583522), test loss: 2.76348332763\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (11.469865799,33.2267506964), test loss: 34.8362261653\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.764272689819,3.47243787878), test loss: 3.02042462528\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (16.5981712341,32.986354809), test loss: 31.4033524036\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (0.958751797676,3.44711426851), test loss: 3.21822172403\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (15.0592517853,32.7539555258), test loss: 35.3535706997\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.80759143829,3.42257875566), test loss: 3.17639867663\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (45.8256530762,32.5328660437), test loss: 30.7881228447\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (2.56176996231,3.39836690425), test loss: 3.23703728914\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (41.3652610779,32.318315364), test loss: 29.7987641811\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.95811486244,3.37521661907), test loss: 2.48853123784\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (19.066493988,32.1074157431), test loss: 32.6774195671\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.63413739204,3.35280735958), test loss: 3.19737582207\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (9.3830575943,31.9035202282), test loss: 31.4329611778\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (0.408227294683,3.33135988081), test loss: 2.46028794944\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (21.1505794525,31.7042638179), test loss: 33.4538703442\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.37039971352,3.31073172299), test loss: 3.218117854\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (20.2794113159,31.507654737), test loss: 29.6198842525\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (2.87353491783,3.29048919251), test loss: 2.46293321848\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (43.4712982178,31.322337123), test loss: 35.0195851803\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.48026847839,3.27073929632), test loss: 3.13409640789\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (10.1436319351,31.1414202417), test loss: 29.8437287331\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.85005354881,3.25136958348), test loss: 2.80259226561\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (8.93039131165,30.9641067905), test loss: 37.0594968319\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.28617525101,3.23270550653), test loss: 3.24083011448\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (35.0802192688,30.7923447084), test loss: 27.6837209225\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (2.16936421394,3.21480229048), test loss: 2.79933504164\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (28.3407878876,30.6262120975), test loss: 35.1284032822\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.956993341446,3.19752333321), test loss: 3.05605701804\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (12.5262374878,30.4594366333), test loss: 32.0722542286\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.13765072823,3.18064596419), test loss: 3.27453466952\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (15.5230379105,30.2981128831), test loss: 36.1332340717\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.1270942688,3.16419354079), test loss: 3.23414255977\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (4.51366615295,30.1446525851), test loss: 31.1124799252\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (0.661155104637,3.14768720499), test loss: 3.25743669569\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (6.38810348511,29.9949616961), test loss: 30.441180563\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (2.12411594391,3.13197795644), test loss: 2.53329835832\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.5318202972,29.8463398334), test loss: 33.9529052258\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.83325779438,3.11656828477), test loss: 3.20615459681\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (15.5660295486,29.7027614871), test loss: 31.7261095047\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (3.08618283272,3.10182004905), test loss: 2.48584416509\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (15.7407474518,29.5645157595), test loss: 34.6899265766\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (0.590524435043,3.08756599874), test loss: 3.24247332811\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (54.6475906372,29.4238246918), test loss: 29.0448862076\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.99113929272,3.07350174653), test loss: 2.46199696958\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (5.83427715302,29.2904813608), test loss: 35.2526885033\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.8435819149,3.0595451001), test loss: 3.14598884881\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (20.7148838043,29.1605581829), test loss: 29.6571317673\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.497397065163,3.0458620315), test loss: 2.80601162016\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (31.7013072968,29.0330463965), test loss: 36.808331871\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.35085582733,3.0327551027), test loss: 3.23639900684\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (27.0919857025,28.9087116827), test loss: 27.7622686386\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.11388981342,3.01985099499), test loss: 2.75830847025\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (8.63561630249,28.7876029542), test loss: 35.0599504948\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.922916948795,3.00737164606), test loss: 3.01396419704\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (5.20075035095,28.6662549672), test loss: 32.3514870167\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.79745793343,2.99527571962), test loss: 3.29309015274\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (13.8045778275,28.5475041789), test loss: 36.4538613319\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.92476630211,2.98337362135), test loss: 3.2364659518\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (8.33249473572,28.4345775865), test loss: 31.1748560667\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.6111522913,2.97140383627), test loss: 3.27611880302\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (4.95714998245,28.32477731), test loss: 30.3926918507\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.363036364317,2.95982239758), test loss: 2.53747215867\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (17.6572227478,28.2138459886), test loss: 33.8792233944\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.9903857708,2.94845469761), test loss: 3.16694645882\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (13.6032466888,28.1076909399), test loss: 31.8349733829\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.79737067223,2.93759381851), test loss: 2.43599725813\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (7.90744590759,28.0043803835), test loss: 34.5047925472\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.17192518711,2.9270824471), test loss: 3.1785683915\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (69.2811431885,27.8993169043), test loss: 29.5606239319\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (3.06224822998,2.91660285856), test loss: 2.48967833221\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (22.9061832428,27.7973794897), test loss: 35.5630832195\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (5.1202044487,2.90627252283), test loss: 3.11016767025\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (6.60131120682,27.698881638), test loss: 29.5363304615\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.29571866989,2.89590538788), test loss: 2.81639843583\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (10.4472055435,27.6031853144), test loss: 37.0491409302\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.04093313217,2.88596273886), test loss: 3.14616790861\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (27.0965862274,27.507571017), test loss: 27.5016397953\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.23407363892,2.876126054), test loss: 2.76569908857\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (13.1401443481,27.4146817333), test loss: 35.0015148163\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.26564455032,2.86669255384), test loss: 2.96841089129\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (49.1051635742,27.3222772391), test loss: 32.1618231297\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.23169827461,2.8574724521), test loss: 3.28563373983\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (23.7036056519,27.230385869), test loss: 36.0752382874\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (1.29403305054,2.84835184948), test loss: 3.21740275323\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (13.3868331909,27.1431333336), test loss: 30.5015145063\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.79763042927,2.83926052385), test loss: 3.2573532939\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (14.8714771271,27.0575715361), test loss: 30.3462237358\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.0665333271,2.83026591781), test loss: 2.51884723604\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (9.16444587708,26.9719853331), test loss: 32.9042381287\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (3.42587137222,2.82149531888), test loss: 3.13399150372\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (10.9336853027,26.8879787819), test loss: 31.9048075199\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.50504171848,2.81302462218), test loss: 2.43024511635\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (18.7764091492,26.8075350689), test loss: 34.2939170122\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.33516073227,2.80479873179), test loss: 3.15461726189\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (11.3535346985,26.7247139923), test loss: 29.7495677471\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (3.14106988907,2.7966464597), test loss: 2.5691416502\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (9.44288444519,26.6439610334), test loss: 35.4229466915\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (0.814608693123,2.78860416309), test loss: 3.09562375546\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (20.1234550476,26.5672407518), test loss: 29.5176413536\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.16735982895,2.7804551932), test loss: 2.80630055368\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (15.9117937088,26.4910585464), test loss: 36.5623129368\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.1943243742,2.77255754857), test loss: 3.03510706425\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (15.0570335388,26.4144455407), test loss: 27.4438926697\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.01382803917,2.76475050351), test loss: 2.71290068626\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (15.2002372742,26.3400868498), test loss: 35.3816901922\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (0.818358063698,2.75728044048), test loss: 3.06455387473\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (59.9598884583,26.2682592447), test loss: 32.4051122665\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (3.59332919121,2.75008474675), test loss: 3.32080755532\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (36.5609588623,26.1932713038), test loss: 36.0670452118\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (3.16527533531,2.74277869851), test loss: 3.19748982787\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (22.971244812,26.1231095383), test loss: 30.4429884911\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.00406718254,2.73545170449), test loss: 3.11294820011\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (8.44944286346,26.0536170074), test loss: 30.5039944172\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (0.683077275753,2.72826715914), test loss: 2.50977145135\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (23.1503810883,25.9847292249), test loss: 32.8104845047\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (4.80388975143,2.72128891499), test loss: 3.12189887166\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (12.3423175812,25.9169409354), test loss: 31.3623618126\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.48398447037,2.71437440693), test loss: 2.44017537832\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (13.2580909729,25.8511001627), test loss: 34.1710382938\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.454283952713,2.70769028717), test loss: 3.10873278677\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (12.4563293457,25.7835032398), test loss: 30.4200608253\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.46457481384,2.70116792176), test loss: 2.61269463301\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (19.4086437225,25.7178890964), test loss: 35.5778301239\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.75625538826,2.69466423151), test loss: 3.14082404822\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (9.557472229,25.6548423085), test loss: 29.5342386246\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (3.52989149094,2.68803501149), test loss: 2.79143353552\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (16.6648960114,25.5926010664), test loss: 35.6725205421\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (3.36312007904,2.6816202148), test loss: 2.97009032667\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (7.0951385498,25.5292620724), test loss: 26.983990097\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.02826476097,2.67522401687), test loss: 2.62328634262\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (35.9628448486,25.468861954), test loss: 35.9252210855\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.88272356987,2.6691082274), test loss: 3.10264506787\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (19.3213310242,25.4098345157), test loss: 31.6517183065\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.33471548557,2.66321600759), test loss: 3.24204386175\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (4.60332965851,25.3475136092), test loss: 35.9567196369\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.577759742737,2.65721627826), test loss: 3.18411643505\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (12.559047699,25.2888481708), test loss: 30.4138606071\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (2.04404497147,2.65123486278), test loss: 3.0401177913\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (7.62445068359,25.2314029121), test loss: 30.1938785553\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.20477128029,2.64527863886), test loss: 2.52529179454\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (25.7434635162,25.1742973904), test loss: 32.9000786781\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.854954957962,2.63949410794), test loss: 3.14113493562\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (11.7488174438,25.117466248), test loss: 31.620595789\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.90388846397,2.63375100895), test loss: 2.42794506997\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (11.6736564636,25.0622758637), test loss: 34.0605845213\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (3.42970275879,2.628233564), test loss: 3.07635056674\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (10.9473371506,25.0061052766), test loss: 30.5521909714\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.635077834129,2.62274539604), test loss: 2.60392288566\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (24.6779880524,24.9508074177), test loss: 35.3615291595\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.50668549538,2.61736454865), test loss: 3.1322093606\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (14.0757226944,24.8977555672), test loss: 29.5309744358\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (3.60809755325,2.61187440732), test loss: 2.79210817516\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (15.5242900848,24.8456999984), test loss: 35.6003504753\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.17034626007,2.60645817305), test loss: 2.9558550179\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (59.774974823,24.7931230991), test loss: 26.6331083775\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (3.79746842384,2.60115092193), test loss: 2.61288365126\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (7.4787569046,24.7409579727), test loss: 35.8725988626\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.18503499031,2.59601593585), test loss: 3.09352317601\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (12.6522769928,24.6910328227), test loss: 30.9552860975\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.623316526413,2.59100876703), test loss: 3.31698012352\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (16.7301063538,24.6390861243), test loss: 36.0286764145\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.969451725483,2.58600455445), test loss: 3.18771451712\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (16.5472831726,24.5887049613), test loss: 29.9925542831\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (3.94519519806,2.58104916095), test loss: 3.03240217566\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (35.7686691284,24.5399058657), test loss: 30.3364603996\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.27544116974,2.57594207277), test loss: 2.41080785692\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (27.8346309662,24.4916447031), test loss: 32.9913707733\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.32152938843,2.57103083959), test loss: 3.13593448699\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (10.2497406006,24.4428217816), test loss: 32.1813883305\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (1.10426402092,2.5661453429), test loss: 2.44657351375\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (8.6189994812,24.3953776958), test loss: 33.5255572796\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (1.06989514828,2.56142971199), test loss: 3.0214292556\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (46.5824203491,24.3482597556), test loss: 30.7552944899\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (2.57390427589,2.55685086373), test loss: 2.62038257718\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (18.823928833,24.3001334123), test loss: 34.9759584427\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.42239356041,2.55222800695), test loss: 3.09916487336\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (38.4959983826,24.2549948444), test loss: 29.4121723175\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.8982578516,2.54759727278), test loss: 2.8007512629\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (8.84061431885,24.2097724267), test loss: 36.0862878323\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.28125214577,2.54296545181), test loss: 2.95003534555\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (5.59218502045,24.164365167), test loss: 26.4641059875\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.23294019699,2.53840690843), test loss: 2.60900649875\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (19.5591640472,24.1199104702), test loss: 35.7986328125\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (0.936078190804,2.53399850897), test loss: 3.07853645086\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (19.3244857788,24.0766907722), test loss: 30.0934605598\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (0.68345952034,2.52969234243), test loss: 3.26076253653\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (17.8929042816,24.0316164421), test loss: 35.9170484066\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.05718111992,2.5254040486), test loss: 3.18193569481\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (12.3490800858,23.9876816977), test loss: 29.885373497\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.03728926182,2.52115342689), test loss: 3.0199198097\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (4.81249284744,23.9457141456), test loss: 30.5465542316\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (0.878884077072,2.51676582001), test loss: 2.4252305612\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (5.93208122253,23.9039691676), test loss: 32.8502880573\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.01989364624,2.5125545229), test loss: 3.12389554679\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (6.05988502502,23.8612932359), test loss: 32.2450949192\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (1.27691340446,2.50831512597), test loss: 2.4414228946\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (21.2129974365,23.8200566116), test loss: 33.6881860733\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (3.54679822922,2.50426483246), test loss: 3.00060414374\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (9.5159778595,23.7798855946), test loss: 31.6681335449\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (0.522837936878,2.50031089474), test loss: 2.66493082941\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (43.5847930908,23.7379343867), test loss: 34.89990592\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (3.0615901947,2.4963450604), test loss: 3.07899879813\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (4.42539978027,23.6977836176), test loss: 29.0550332069\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.14547169209,2.49226646056), test loss: 2.77338943779\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (21.4109840393,23.6581906763), test loss: 36.0532721758\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (0.552356123924,2.48826440923), test loss: 2.94526526481\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (14.8868141174,23.6187219207), test loss: 26.4612047672\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (1.37289428711,2.48439134148), test loss: 2.63197343946\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (17.0028305054,23.579666116), test loss: 36.2139891148\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (1.01675629616,2.48048248061), test loss: 3.06000777185\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (6.30351686478,23.5413845885), test loss: 29.7866259575\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.877960562706,2.47668882382), test loss: 3.2358004719\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (8.98034858704,23.5019461233), test loss: 36.2191886902\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.58479237556,2.47298569336), test loss: 3.1725305289\n",
      "\n",
      "MC # 5, Hype # hyp4, Fold # 7\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold7/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (311.880126953,inf), test loss: 156.773347473\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (329.126586914,inf), test loss: 398.21884613\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (21.8712768555,83.2141120596), test loss: 41.0591508865\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (4.02678871155,94.0222324072), test loss: 2.8398036927\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (23.9937744141,66.0061834848), test loss: 38.042395401\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.74962365627,48.4377542092), test loss: 2.58991255462\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (79.95703125,60.2025958425), test loss: 40.7259980679\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (2.54927873611,33.2321189499), test loss: 3.07584260702\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (25.5826568604,57.3182577022), test loss: 38.325673151\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.38281774521,25.6271679806), test loss: 2.97843767703\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (115.273262024,55.5851613498), test loss: 42.1857218742\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.66030597687,21.0657086486), test loss: 2.71900387704\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (29.3336219788,54.3558348513), test loss: 41.0010379791\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.20865678787,18.0262194764), test loss: 3.118270123\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (28.3085212708,53.416352717), test loss: 43.1254468918\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.42206573486,15.848691135), test loss: 2.6782538116\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (41.5588798523,52.7072582264), test loss: 39.4976608276\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.68781125546,14.2172149531), test loss: 3.10010063052\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (46.281955719,52.0934619999), test loss: 42.1111032009\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (2.12128448486,12.9456380588), test loss: 2.61845189333\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (39.4181900024,51.5878233029), test loss: 40.9249592304\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (5.97326612473,11.9259500391), test loss: 3.10978124738\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (33.8942947388,51.2005155998), test loss: 40.4644135952\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.60515761375,11.0917521791), test loss: 2.63155988157\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (19.7529163361,50.8505245069), test loss: 41.0787611008\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.12196063995,10.3968188479), test loss: 2.8384390533\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (55.3115539551,50.5464803491), test loss: 38.3581697464\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.37998962402,9.81063761012), test loss: 2.46690017879\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (30.4386692047,50.2404712133), test loss: 38.2401719093\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (0.542139232159,9.30490430742), test loss: 2.77213697433\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (62.9787979126,49.9672308917), test loss: 35.4135275364\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.16056799889,8.86797350671), test loss: 2.72007003129\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (52.6440582275,49.6930150141), test loss: 38.5789952278\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (2.00094342232,8.48441551054), test loss: 2.83921367228\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (46.9421348572,49.440674369), test loss: 38.0271284103\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (5.28646755219,8.14424243179), test loss: 3.05153596997\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (27.1140117645,49.2284317804), test loss: 41.0457802773\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (6.60954666138,7.84288223556), test loss: 2.54019846618\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (20.4279460907,49.0197452731), test loss: 38.4225909233\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (3.94285941124,7.57285471116), test loss: 3.24211630225\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (93.0466003418,48.8184502855), test loss: 40.8771903992\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (3.79446220398,7.33075603337), test loss: 2.4951324746\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (36.7894744873,48.6090127173), test loss: 37.2929020882\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.03047895432,7.1098241712), test loss: 2.97837319076\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (17.4639606476,48.4038657207), test loss: 39.0768556595\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.8963162899,6.90927643828), test loss: 2.58977522552\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (27.043132782,48.1924444868), test loss: 39.7660798073\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (4.82208108902,6.72556560061), test loss: 2.90912424028\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (10.8804130554,47.9876167899), test loss: 37.6157506466\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (0.859792530537,6.55529347858), test loss: 2.54354275167\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (39.4340171814,47.7830649322), test loss: 37.57827425\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.21814775467,6.40048863766), test loss: 2.87586283684\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (19.9691829681,47.5613737685), test loss: 33.2229141712\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.38911747932,6.25731604606), test loss: 2.62682636678\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (60.1609687805,47.3335465027), test loss: 35.8114985943\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (2.46445178986,6.12480657434), test loss: 2.8334027946\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (68.4560241699,47.1013437812), test loss: 33.1384250164\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (1.71688878536,6.00004254021), test loss: 2.86402568519\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (43.7230758667,46.8626384031), test loss: 36.1176179409\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (2.18541264534,5.88366313661), test loss: 2.63249852061\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (48.9277534485,46.6119431736), test loss: 36.0044206619\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.8857293129,5.77402494892), test loss: 3.02235369682\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (40.3764266968,46.3619383859), test loss: 37.1665875912\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.03238284588,5.67013945376), test loss: 2.45059313774\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (19.2045936584,46.1172601316), test loss: 34.4990348816\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.80114626884,5.57263077379), test loss: 3.09726161361\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (81.6068572998,45.8661632522), test loss: 34.7952355862\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.89963984489,5.48077216147), test loss: 2.43410457969\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (29.6231193542,45.6024088554), test loss: 34.9616806984\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.676584720612,5.39413381166), test loss: 2.94547788501\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (26.8903503418,45.3331636486), test loss: 33.8672593594\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.15483868122,5.31115489737), test loss: 2.58159955442\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (22.2814750671,45.0541209856), test loss: 33.2646494865\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.26209688187,5.23229615475), test loss: 2.61338058412\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (23.629989624,44.7615461621), test loss: 29.815882349\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.68652439117,5.15681700459), test loss: 2.34194319844\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (19.7419624329,44.4669652875), test loss: 30.2831809282\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.13814163208,5.08417830612), test loss: 2.67332859635\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (15.9014444351,44.1603873856), test loss: 26.3272027969\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.596234679222,5.01502987407), test loss: 2.61349654794\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (42.6933822632,43.8302340295), test loss: 29.3566530704\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.585973143578,4.9491340442), test loss: 2.7185554862\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (25.686756134,43.4931370629), test loss: 27.7036251545\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.58865833282,4.88618618838), test loss: 2.9233656168\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (19.5857143402,43.1477558355), test loss: 30.1619949341\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.95122814178,4.82507400336), test loss: 2.31390810609\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (40.9051818848,42.7925143941), test loss: 28.0375651836\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.78729510307,4.76630188597), test loss: 3.06692399979\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (20.9169883728,42.4291115088), test loss: 29.5769836903\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (2.56365656853,4.70945452316), test loss: 2.30586929023\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (6.65442323685,42.0680181807), test loss: 27.1736548662\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (3.84732937813,4.6542060936), test loss: 2.83978609741\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (16.4971199036,41.7080239776), test loss: 28.2179586887\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.93425285816,4.60118194665), test loss: 2.41730803251\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (8.19059753418,41.3421513213), test loss: 28.4759115696\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.0189743042,4.55025133639), test loss: 2.82947588563\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (22.3448677063,40.9820872468), test loss: 25.6475322723\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.2627158165,4.50143393237), test loss: 2.45081018209\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (8.92601966858,40.6235215358), test loss: 26.4688117027\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.71961629391,4.45407603593), test loss: 2.6951549679\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (26.0485305786,40.2641388173), test loss: 24.1093151569\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (2.66386246681,4.40834133024), test loss: 2.66127677858\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (13.1942958832,39.909124232), test loss: 26.4863188744\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.768416821957,4.36403666466), test loss: 2.79699652493\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (9.77146148682,39.563992327), test loss: 23.4377548218\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (0.889124035835,4.32081586351), test loss: 2.8885753423\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (20.3601055145,39.224934522), test loss: 26.5594726086\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.91306114197,4.27938544977), test loss: 2.58857857585\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (13.6672115326,38.888404498), test loss: 26.1525620937\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.35659885406,4.2394271168), test loss: 3.08493874371\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (28.6523971558,38.5633435202), test loss: 28.0218967199\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.3996117115,4.20108874099), test loss: 2.39869505316\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (17.0619392395,38.2462379606), test loss: 26.867047596\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.17767083645,4.16393403895), test loss: 2.95154481828\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (17.0702705383,37.9321188529), test loss: 27.2593440056\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.0606212616,4.12785985908), test loss: 2.41001226008\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (6.8327255249,37.626884113), test loss: 28.2835356712\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.80188751221,4.09289493688), test loss: 2.97408111989\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (14.9916582108,37.3328556471), test loss: 27.575418663\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.21915650368,4.05858831828), test loss: 2.53125963509\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.8787031174,37.045074963), test loss: 27.7010343075\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.922774791718,4.0256086096), test loss: 2.78622834384\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (10.7420301437,36.7617349738), test loss: 24.906144762\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.66152644157,3.99375840353), test loss: 2.36533382535\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (29.9248065948,36.4895543692), test loss: 26.5501436949\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (2.46282219887,3.96314181817), test loss: 2.76620550156\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (26.2761211395,36.2255414423), test loss: 23.8426338673\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.84349679947,3.93339825993), test loss: 2.87488497347\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (14.3927230835,35.9639772145), test loss: 27.2735930681\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.54229426384,3.90437948123), test loss: 2.84948464483\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (7.37449598312,35.7108782453), test loss: 24.7576077223\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (3.10262441635,3.87617512905), test loss: 3.06105499566\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (15.7092142105,35.4668593555), test loss: 28.7753097534\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.20097160339,3.84843942442), test loss: 2.42431357205\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (16.9227256775,35.2276232677), test loss: 26.7825801849\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (1.04329514503,3.82172949353), test loss: 3.14252148867\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (7.97509145737,34.9919751344), test loss: 28.5329118729\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (0.650469481945,3.79580615911), test loss: 2.339933002\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (25.9950752258,34.7652693614), test loss: 27.8819218636\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.92574393749,3.77082535438), test loss: 2.98556394279\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (23.4291782379,34.5459414685), test loss: 29.3942760944\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.12507367134,3.746517063), test loss: 2.53615084291\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (16.8953437805,34.3281269219), test loss: 28.8776435375\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.09683930874,3.72268113628), test loss: 2.88432865143\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (18.259437561,34.1171117087), test loss: 25.3302411556\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.44463157654,3.6994266279), test loss: 2.49121809304\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (23.9346733093,33.9132502279), test loss: 27.3189862847\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.90882122517,3.67650645686), test loss: 2.73476065695\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (20.7136001587,33.7127348086), test loss: 25.1137034893\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (2.53602552414,3.65438210251), test loss: 2.74316293895\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (33.0916519165,33.5150012314), test loss: 26.9587823391\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.61482834816,3.63282930835), test loss: 2.85446873903\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (12.3559150696,33.3239202044), test loss: 23.9699335337\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.05580329895,3.61198584396), test loss: 2.91424860954\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (21.2620449066,33.1392944692), test loss: 28.2146075249\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.787367403507,3.59164900773), test loss: 2.57464426458\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (60.6689987183,32.9554184557), test loss: 26.379691267\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.26411437988,3.5717293696), test loss: 3.15981349945\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.2033309937,32.7768130528), test loss: 28.893883872\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (5.23076725006,3.55215702473), test loss: 2.40158863366\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (43.0709114075,32.6038519177), test loss: 26.324546957\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.54073750973,3.53284370271), test loss: 2.97768315375\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (28.9922199249,32.4333385029), test loss: 28.7990288258\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (1.66540980339,3.51414540406), test loss: 2.46492129415\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (12.4328632355,32.2644238881), test loss: 28.5816772938\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.9159642458,3.49591282147), test loss: 2.95177936554\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (17.7167053223,32.1016889364), test loss: 27.9364172935\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.85013103485,3.47820625125), test loss: 2.55314139277\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (46.9219779968,31.9435346535), test loss: 28.4125691414\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.42596006393,3.46098079259), test loss: 2.84086574316\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (4.39978504181,31.7848807734), test loss: 25.0769695759\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.27799892426,3.44398923524), test loss: 2.38883432895\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (9.3091506958,31.6325347088), test loss: 27.2279916763\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.68580961227,3.42721868353), test loss: 2.80211679339\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (8.47060680389,31.4830077994), test loss: 24.348106575\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.33888936043,3.41078451739), test loss: 2.85577606708\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (20.8289871216,31.3362713481), test loss: 28.0634280682\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (2.68859672546,3.39477302592), test loss: 2.8797213316\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (16.5755004883,31.1903911968), test loss: 25.6768754005\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.93285965919,3.37909945289), test loss: 2.99955723882\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (16.7628479004,31.049366), test loss: 29.1906976223\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.13293766975,3.36388405391), test loss: 2.48075844944\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (48.3464126587,30.9117560472), test loss: 27.4970407128\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (3.27316689491,3.34904012042), test loss: 3.21993426383\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (47.3328094482,30.7745551932), test loss: 28.185216856\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.88209819794,3.33440244743), test loss: 2.23263216466\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (6.80173063278,30.6418717933), test loss: 28.4207048416\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (3.17531824112,3.31986483741), test loss: 3.00423033535\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.9392681122,30.5115075521), test loss: 27.473439312\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (4.09760475159,3.30565237883), test loss: 2.49039343894\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (43.9933395386,30.3830700259), test loss: 28.9957199574\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (2.45222878456,3.29172820111), test loss: 2.76399450004\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (15.6285638809,30.255472919), test loss: 26.3593243122\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.86371517181,3.27812530675), test loss: 2.47884891927\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (4.90777301788,30.131840179), test loss: 27.4687357187\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.99215483665,3.26490040822), test loss: 2.74152594805\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (20.4525146484,30.0106022268), test loss: 24.2939189434\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.29528129101,3.25192409616), test loss: 2.80866599083\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (24.7624359131,29.8899820898), test loss: 27.3327564478\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.68123102188,3.23913712049), test loss: 2.85713966042\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (3.79432845116,29.7738910957), test loss: 23.9285006523\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (2.06610321999,3.2264314635), test loss: 2.92140682936\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (50.699596405,29.658925697), test loss: 28.6737561226\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (5.38314056396,3.21398298517), test loss: 2.55209095478\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (22.9361934662,29.5447932951), test loss: 26.1487255335\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.802288532257,3.20178076856), test loss: 3.14602302909\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (28.9367523193,29.4323834212), test loss: 29.3877302408\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (1.62753450871,3.18983813118), test loss: 2.29360771626\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (7.0732088089,29.3231217439), test loss: 26.1570375919\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.08412134647,3.17820279248), test loss: 2.94150324166\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (9.52078819275,29.215287461), test loss: 29.6285475731\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.65061473846,3.16679041777), test loss: 2.49163714051\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (5.38570690155,29.108023079), test loss: 28.0768410444\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (0.524530887604,3.15549915533), test loss: 2.87991762459\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (18.7286510468,29.0051620029), test loss: 27.2557765484\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.00406074524,3.14426130044), test loss: 2.54145856798\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (7.13711929321,28.9019396139), test loss: 27.4799044371\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.96231329441,3.13319171593), test loss: 2.7606095463\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (15.055147171,28.8005681455), test loss: 24.7051603079\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.98446857929,3.12245212083), test loss: 2.38184065819\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (13.5461273193,28.6999351574), test loss: 27.7149596453\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.65803301334,3.11183691286), test loss: 2.8395742625\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (14.6895437241,28.6025992084), test loss: 23.8257135391\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.09644842148,3.10151146959), test loss: 2.83748205155\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (49.3109283447,28.5059623526), test loss: 28.253224349\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (3.55128407478,3.09133627468), test loss: 2.66068805754\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (32.9871368408,28.4097508875), test loss: 25.8087073803\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.94170284271,3.08131870669), test loss: 3.09108845145\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (19.5490875244,28.3171664823), test loss: 29.1869474649\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.35716366768,3.07126992548), test loss: 2.46386121809\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (20.3905830383,28.2245356459), test loss: 27.267415309\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.23226356506,3.06138839573), test loss: 3.11093889177\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (17.6670417786,28.1330705838), test loss: 26.6104016304\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.3704354763,3.05179172688), test loss: 2.27579614222\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (18.646774292,28.0423227671), test loss: 29.0574166536\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.9390386343,3.04229982055), test loss: 2.96836621016\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (13.9021320343,27.95448918), test loss: 27.1578402996\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.38391065598,3.03306190492), test loss: 2.51621724069\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (13.6882400513,27.8665711854), test loss: 29.8747634411\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.31779921055,3.0239030412), test loss: 2.81099811494\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (20.3399066925,27.7798864245), test loss: 25.1413405895\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.80059862137,3.01492181698), test loss: 2.35266451538\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (26.8447093964,27.696289612), test loss: 27.5480403662\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.12990391254,3.00592917694), test loss: 2.73884536028\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (23.1282424927,27.6123687135), test loss: 24.0324018478\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.92199158669,2.99703300895), test loss: 2.69378224015\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (20.6942577362,27.529377199), test loss: 27.4767851353\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (1.8661904335,2.9884126122), test loss: 2.86847217679\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (3.29187011719,27.4469274984), test loss: 25.2212688446\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.29406189919,2.97986982809), test loss: 3.05816698074\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (17.0409927368,27.3672476131), test loss: 29.0091800213\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (3.70560312271,2.97152298865), test loss: 2.59338321686\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (16.552116394,27.2872370293), test loss: 26.1261219859\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (0.639814257622,2.96327631877), test loss: 3.16115585566\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (9.73847198486,27.2082892063), test loss: 28.7884316921\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (1.92252123356,2.95515089244), test loss: 2.35581493527\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (40.7867507935,27.1320531255), test loss: 26.8021188974\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.16906511784,2.94704319158), test loss: 2.9562911123\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (10.2803144455,27.0555911207), test loss: 29.4938772678\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (2.45404195786,2.9389954975), test loss: 2.43179136813\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (14.882935524,26.9799052354), test loss: 28.2104307175\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (5.42812728882,2.93118597264), test loss: 2.89791952074\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (10.1231985092,26.9048658453), test loss: 27.3924203396\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.901244938374,2.92345629423), test loss: 2.50536999404\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (6.31908369064,26.8318813381), test loss: 27.6555120468\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.24503195286,2.91585648982), test loss: 2.72806796432\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (20.0413169861,26.758662784), test loss: 24.5061896324\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (0.470944613218,2.90841961709), test loss: 2.48843352497\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (14.7472229004,26.686563949), test loss: 27.652347064\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (0.995019257069,2.90102900086), test loss: 2.86310655922\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (6.37243270874,26.6163018577), test loss: 23.6038090348\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.40928697586,2.89366816977), test loss: 2.836586079\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (10.3951435089,26.5466000651), test loss: 28.2796512127\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.48876833916,2.88632474384), test loss: 2.49890451133\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (10.9307289124,26.4771054266), test loss: 25.4402719021\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (2.64680075645,2.87917382277), test loss: 3.09076943845\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (18.0790977478,26.4082166138), test loss: 29.5240061045\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (2.25145626068,2.87219281889), test loss: 2.38646160662\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (14.7421541214,26.3411856358), test loss: 27.3658484459\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.93609523773,2.86525395983), test loss: 3.06791534424\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.56472110748,26.2736136662), test loss: 27.5346154213\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (2.4993724823,2.85847846597), test loss: 2.3410746783\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (11.9169492722,26.207318009), test loss: 29.4575115919\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.05054950714,2.85171999336), test loss: 3.05367379189\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (10.2835798264,26.14274079), test loss: 26.9412809372\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.50935578346,2.84498064736), test loss: 2.51345533133\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (8.70176887512,26.0783786632), test loss: 29.5315817714\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.50650823116,2.83825843376), test loss: 2.7634408623\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (35.139831543,26.014172238), test loss: 24.7904969215\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.56038308144,2.83169081598), test loss: 2.34599891901\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (7.28494930267,25.9504140713), test loss: 27.6849500179\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.16386175156,2.82529727564), test loss: 2.70246499181\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (3.83059930801,25.8884764123), test loss: 23.8658645511\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (0.632819592953,2.81893449702), test loss: 2.65842773318\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (31.8422660828,25.8260371363), test loss: 28.6287713528\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.57137513161,2.81270884646), test loss: 2.85209976733\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (18.124584198,25.7645840518), test loss: 25.1754489422\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.5552148819,2.80650881131), test loss: 3.07137799263\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (19.6524028778,25.7047716159), test loss: 29.281757164\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (5.49048519135,2.80032137399), test loss: 2.49871198237\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (9.37403583527,25.6450489063), test loss: 25.7500863552\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.600137829781,2.79413730625), test loss: 3.12858661413\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (10.0702838898,25.5851344765), test loss: 28.608265543\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.18115782738,2.78809444077), test loss: 2.33922289312\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (17.0460910797,25.5262533524), test loss: 26.9674659133\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.21109437943,2.78221418871), test loss: 2.94549555331\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (10.3502388,25.4687343966), test loss: 28.5643694878\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.408826559782,2.7763572182), test loss: 2.4718322292\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (9.83851242065,25.4103056798), test loss: 29.2762116194\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.03898584843,2.77061206342), test loss: 2.90207812786\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (20.645980835,25.3533713594), test loss: 25.7717980862\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.02877223492,2.76489675531), test loss: 2.43147543669\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (23.4736289978,25.2977218385), test loss: 28.3141307592\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (4.9669046402,2.75913875728), test loss: 2.70477537215\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (15.7697687149,25.2422033412), test loss: 25.1603622913\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (4.61319351196,2.75348952077), test loss: 2.68733352423\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (12.8488798141,25.1863904038), test loss: 28.7561079741\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (3.86381649971,2.74790391671), test loss: 2.82670947313\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (18.6478404999,25.1313997004), test loss: 23.725263834\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.65677928925,2.74245585676), test loss: 2.87727627456\n",
      "\n",
      "MC # 5, Hype # hyp4, Fold # 8\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold8/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (352.961090088,inf), test loss: 188.11235199\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (281.179199219,inf), test loss: 341.950932312\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (35.8146591187,109.291872231), test loss: 47.185746336\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.44162940979,122.462354263), test loss: 3.13009618819\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (50.2765426636,78.1066051655), test loss: 39.2995909214\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.89763879776,62.8273976633), test loss: 2.61993889809\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (62.0926017761,67.3809073318), test loss: 44.9762802124\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (4.67688941956,42.9254417214), test loss: 3.27837913632\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (19.6453399658,62.0646011841), test loss: 42.5038292408\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (0.944790244102,32.9710555075), test loss: 3.00885290504\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (64.4144515991,58.762691), test loss: 45.5797394753\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (5.64515638351,26.9916615921), test loss: 3.19490135908\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (33.4209480286,56.6191468875), test loss: 46.4802027702\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (0.930483102798,23.0004377074), test loss: 3.1968531549\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (67.5050506592,54.9806578335), test loss: 42.525042057\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (6.40705776215,20.1481902776), test loss: 2.64263684154\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (46.6140823364,53.7303050389), test loss: 48.8636282444\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (1.65554404259,18.0070455557), test loss: 3.23194783926\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (25.8169746399,52.7399666666), test loss: 43.6093716621\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (3.38930606842,16.3400960915), test loss: 2.5973095417\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (35.0968399048,51.9939074437), test loss: 47.6301571369\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.29984807968,15.0065010287), test loss: 3.33027195334\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (70.2042388916,51.3350299023), test loss: 39.0362759113\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.419526577,13.9162518347), test loss: 2.65561488867\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (19.3905906677,50.7876670155), test loss: 46.2194721222\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (5.1782541275,13.008597369), test loss: 3.27345800102\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (95.0485458374,50.3046345108), test loss: 39.4152424812\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (3.01189661026,12.2378489272), test loss: 2.66809298396\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (12.5614566803,49.8164063658), test loss: 47.4881228924\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.54135155678,11.5757975677), test loss: 3.15157212615\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (40.7981529236,49.4385053758), test loss: 39.0330459595\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.51864492893,11.0022671249), test loss: 2.53537879288\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (24.216583252,49.0737381782), test loss: 43.6288901806\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.37030315399,10.4992601487), test loss: 3.15015203357\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (18.1165485382,48.7514930729), test loss: 41.6361341\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.18638038635,10.0552174834), test loss: 2.9270316124\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (31.6574287415,48.4582671755), test loss: 42.2058267117\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (0.883887708187,9.66228064782), test loss: 3.13020234704\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (28.0272274017,48.1761936965), test loss: 42.018464756\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.98287701607,9.30845185409), test loss: 3.04188710153\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (85.0127716064,47.8928605654), test loss: 39.2288357735\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (2.52715587616,8.98918438469), test loss: 2.57191185057\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (30.9739952087,47.6225504522), test loss: 44.7275704384\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.88727283478,8.69996270731), test loss: 2.95308437943\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (49.2495803833,47.3662745049), test loss: 41.1779057026\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.99045038223,8.43569821972), test loss: 2.64323260784\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (24.3569087982,47.1383439299), test loss: 45.7226262093\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.31299686432,8.19503380784), test loss: 3.3040871799\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (19.1076622009,46.9001250129), test loss: 40.6691906452\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (2.95321536064,7.97378800182), test loss: 2.6112396419\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (23.1351127625,46.6760840122), test loss: 43.4582401276\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.06278371811,7.76931730528), test loss: 3.13266108632\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (44.1699676514,46.4392298708), test loss: 36.746703577\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.77271127701,7.58037217941), test loss: 2.5886251241\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (26.3626251221,46.1963520954), test loss: 42.6896766663\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (3.66779947281,7.40433721526), test loss: 3.05760792792\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (54.5953598022,45.9626042609), test loss: 35.7292459011\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (3.24412488937,7.23962226026), test loss: 2.50162524879\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (70.6379241943,45.7386423918), test loss: 42.2105848312\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.93384587765,7.08631488088), test loss: 3.00718440413\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (17.1763305664,45.4944350796), test loss: 34.0737616062\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.06539082527,6.94213721004), test loss: 2.38348386884\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (24.866601944,45.2653021074), test loss: 39.1566945076\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.1787083149,6.80719664932), test loss: 2.91719151437\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (31.0349292755,45.0117857409), test loss: 36.5970111132\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (0.760763347149,6.67929313463), test loss: 2.86403253078\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (33.7659835815,44.753311422), test loss: 38.5299527168\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.49305915833,6.55861153192), test loss: 2.89713953733\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (10.7206573486,44.4870194325), test loss: 37.9060264111\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.87140285969,6.44375404565), test loss: 2.97725974023\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (8.8684463501,44.2192032979), test loss: 34.0089489937\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (0.786346673965,6.33500809831), test loss: 2.45083339512\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (13.3299722672,43.9334883905), test loss: 41.8549618721\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.41497945786,6.23169096074), test loss: 3.05500010848\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (24.2273864746,43.6482294779), test loss: 33.8318006754\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.60168123245,6.13341863392), test loss: 2.2966406852\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (77.2454833984,43.3514618892), test loss: 38.8388290405\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.97390520573,6.03964610408), test loss: 3.04246814251\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (41.0954170227,43.0332450065), test loss: 28.5285011292\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.58386039734,5.94928746742), test loss: 2.25750968754\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (125.809936523,42.7256651847), test loss: 37.505082202\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (4.86063766479,5.86298389641), test loss: 3.01289699674\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (32.541179657,42.4087520078), test loss: 29.8727489471\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.56404209137,5.78001670424), test loss: 2.41539178193\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (46.1018791199,42.0880645355), test loss: 37.1878921986\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.10565245152,5.70022868196), test loss: 2.93190321326\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (10.2961349487,41.7639166157), test loss: 28.1615383863\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.35890960693,5.62449453745), test loss: 2.3743018955\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (7.53032970428,41.4365579819), test loss: 33.0833071709\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.0647636652,5.55112081482), test loss: 2.71211155355\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (100.072860718,41.097243133), test loss: 31.1185623169\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.66744852066,5.48036744069), test loss: 2.8154666543\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (22.6906089783,40.7619697964), test loss: 30.9257297277\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (4.22172737122,5.41220128382), test loss: 2.85095010102\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (17.6361999512,40.4261428732), test loss: 30.284265542\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.91190230846,5.34611031796), test loss: 2.96390380263\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (21.743598938,40.0950206255), test loss: 27.0793202877\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (5.01666975021,5.2828879662), test loss: 2.3405359596\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (13.0938882828,39.7618225263), test loss: 32.0631230831\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.08461284637,5.22184505858), test loss: 2.88539013863\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (18.5792770386,39.436532467), test loss: 29.0194416285\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (0.629831373692,5.16309134645), test loss: 2.32948593199\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (21.7438316345,39.1064165101), test loss: 34.2205049515\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.972927153111,5.1063322998), test loss: 3.13950402141\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (7.84889793396,38.7826129443), test loss: 28.8800531864\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.38321018219,5.05132955333), test loss: 2.35026686788\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (36.4005470276,38.4684758336), test loss: 31.9479082108\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.29237055779,4.99786808447), test loss: 3.01250893474\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (39.9842948914,38.1609591273), test loss: 26.6771798372\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.78846693039,4.94650801014), test loss: 2.50650280267\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (38.1045951843,37.8554375294), test loss: 32.0830020905\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.26856064796,4.89666757618), test loss: 2.90508427471\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (14.6707324982,37.5622057668), test loss: 27.1915008068\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (4.04075050354,4.84884662222), test loss: 2.4585549891\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (19.2348861694,37.2694178276), test loss: 32.6988529205\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.53989517689,4.80232824183), test loss: 2.83487552255\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (24.8612823486,36.9839928788), test loss: 26.15873487\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.61963319778,4.75709239202), test loss: 2.37836411893\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (10.1859359741,36.7099857372), test loss: 31.3317407131\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (3.97208237648,4.71314131747), test loss: 2.86918472946\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (23.6441726685,36.4433015382), test loss: 29.7154348135\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (2.22575426102,4.67046392037), test loss: 3.14109268486\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (11.0042686462,36.1794054607), test loss: 30.5814671993\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.37115073204,4.62902413412), test loss: 2.94294760525\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (13.3576555252,35.9253321752), test loss: 30.4728472233\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.668811798096,4.5891161977), test loss: 3.17919693291\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (66.7921447754,35.6774716321), test loss: 26.9847151399\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.2726764679,4.55044753013), test loss: 2.43707208186\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (16.0650177002,35.430658436), test loss: 34.2778206587\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (3.46111154556,4.51242324812), test loss: 3.19150852859\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (10.3844614029,35.1950369849), test loss: 28.53131423\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.85363006592,4.47565855518), test loss: 2.39795834124\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (22.4062595367,34.9670401574), test loss: 33.3713639736\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (7.53509235382,4.43985517396), test loss: 3.15896817148\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (26.630607605,34.7408225735), test loss: 25.2496967554\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.74538826942,4.4048523971), test loss: 2.28529296815\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (22.7137508392,34.5232532451), test loss: 33.2335965157\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.32236588001,4.37131947257), test loss: 3.0740955472\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (18.1586551666,34.312166249), test loss: 29.2817312241\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.78097891808,4.33875352226), test loss: 2.57122457474\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (6.03632879257,34.0994290394), test loss: 33.7666893244\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (0.76051056385,4.30666439704), test loss: 3.03835248053\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (3.37168860435,33.8977819365), test loss: 27.9593588829\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.68941855431,4.27545720974), test loss: 2.62218447328\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (20.9575462341,33.7003783642), test loss: 31.1723653316\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (2.87224435806,4.24500772012), test loss: 2.81314304173\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (12.0602302551,33.5071476603), test loss: 30.7650975943\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (2.38136768341,4.21530381643), test loss: 3.19784439802\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (15.3579673767,33.3181293509), test loss: 30.5439951897\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.18495059013,4.18661155834), test loss: 2.99026896358\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (29.5806236267,33.1366958584), test loss: 30.3262680292\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.18738281727,4.1587853363), test loss: 3.21913770437\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (18.8804283142,32.9542082535), test loss: 30.5657311797\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.96679258347,4.1314025942), test loss: 2.59768297672\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (14.4264307022,32.7782689594), test loss: 31.3205025196\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (3.50101494789,4.10464956417), test loss: 3.1647718668\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (8.91487503052,32.60777386), test loss: 29.9623729706\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (2.40338683128,4.07841888494), test loss: 2.5202380538\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (30.9160003662,32.4411512397), test loss: 34.037960124\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (2.78838539124,4.05285550798), test loss: 3.29250358343\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (22.571144104,32.275519821), test loss: 30.0071922183\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.809703171253,4.02784527242), test loss: 2.44839653075\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (19.598772049,32.1175113337), test loss: 31.9053658485\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.2220826149,4.00372042542), test loss: 3.05581262112\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (16.9055500031,31.9589541691), test loss: 27.9378409386\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.27615880966,3.9800554545), test loss: 2.53139150739\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.5333313942,31.8041771638), test loss: 33.1903534412\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.53815245628,3.95678342226), test loss: 3.11971926689\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (8.24874973297,31.655888219), test loss: 28.2825192451\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.08538103104,3.93392731403), test loss: 2.60681696087\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (12.6940078735,31.5100194998), test loss: 33.1056684494\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.48586177826,3.91169287035), test loss: 2.92215929329\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (28.5820465088,31.3645720689), test loss: 26.8319665909\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (2.75643587112,3.88983614258), test loss: 2.48146164119\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (9.65679454803,31.2241801754), test loss: 31.9283920288\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (0.66378903389,3.86867516356), test loss: 2.96762295365\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (10.9392929077,31.0856322019), test loss: 30.5806288958\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (3.60981321335,3.84804660276), test loss: 3.27219795287\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (21.0012435913,30.9486558192), test loss: 31.6949597597\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (1.40798711777,3.82748471647), test loss: 3.01285571158\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (20.7689552307,30.8167757993), test loss: 30.3846774578\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (5.90175056458,3.80753561644), test loss: 3.19491896033\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (11.3045778275,30.6879056948), test loss: 27.3892536879\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.50148868561,3.78777346179), test loss: 2.41095438749\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (10.4395475388,30.5583813148), test loss: 35.0073843479\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.55295848846,3.76848340334), test loss: 3.19829393029\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (31.9650096893,30.434197964), test loss: 29.9523331165\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.68476498127,3.74981089625), test loss: 2.49223855287\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (21.3448390961,30.312968654), test loss: 33.7487951756\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (3.17015695572,3.73156245927), test loss: 3.20601436496\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (44.4788894653,30.1896238791), test loss: 27.2996908903\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (4.13983869553,3.71341888718), test loss: 2.29435039759\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (18.0352172852,30.071820363), test loss: 33.0504062533\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.36932468414,3.69557212383), test loss: 3.03692076802\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (18.0223674774,29.9558662861), test loss: 28.7500163078\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (1.38414859772,3.67807287456), test loss: 2.49077455401\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (26.7899951935,29.841238015), test loss: 33.6337881088\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.85504710674,3.66086843323), test loss: 3.04372136295\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (12.394203186,29.7287266826), test loss: 29.2802269936\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.26253199577,3.64416935756), test loss: 2.65738497674\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (12.8150033951,29.6200192869), test loss: 30.7716860652\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.60215866566,3.6278773208), test loss: 2.81161314845\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (16.6418418884,29.5097352207), test loss: 28.4327273369\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.07580935955,3.61166172207), test loss: 3.03193506896\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (22.9161014557,29.4030571714), test loss: 31.6085710049\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.57517874241,3.59575524176), test loss: 2.94685565233\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (32.501083374,29.2992439908), test loss: 30.1689326882\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.88739800453,3.58006766665), test loss: 3.12257385254\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (17.5236358643,29.1960314021), test loss: 31.7517307997\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.30869853497,3.56465966834), test loss: 2.87362222075\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (19.3363265991,29.0939798013), test loss: 32.0516903877\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.08191144466,3.54953820617), test loss: 3.17395324707\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (13.0578947067,28.9958068551), test loss: 29.5092743635\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.3504832983,3.53482446942), test loss: 2.51810450852\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (8.26861667633,28.8962439657), test loss: 33.6325636864\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.24211549759,3.52034454349), test loss: 3.25687309802\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (10.4193887711,28.7987936412), test loss: 28.9601527214\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.15722477436,3.50599059739), test loss: 2.36549503803\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (9.74124145508,28.7054070383), test loss: 32.4163157344\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.46390044689,3.49182089992), test loss: 2.99307187796\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (20.1405143738,28.6122883893), test loss: 26.836487174\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.39892506599,3.4779541974), test loss: 2.4135532409\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (17.5528411865,28.5187059273), test loss: 33.9256643295\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.87550449371,3.46420931406), test loss: 3.11350141168\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (12.7205543518,28.4288571587), test loss: 28.4675055027\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.632078647614,3.45093255591), test loss: 2.5728256464\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (44.5735015869,28.3388181827), test loss: 33.4425165653\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.9530954361,3.43784520565), test loss: 2.87916406989\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (27.4804172516,28.2495207707), test loss: 27.0198361158\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.7172832489,3.42479633904), test loss: 2.46010455489\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (8.54518508911,28.163499689), test loss: 31.6814934492\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.837875127792,3.41192395149), test loss: 2.9115069598\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (6.00520801544,28.07900842), test loss: 30.0378458261\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.487684220076,3.39933112817), test loss: 3.21109959483\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (14.2057714462,27.9933298063), test loss: 31.678403461\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (0.933909654617,3.38684875291), test loss: 2.98193500638\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (21.5387897491,27.9108339894), test loss: 29.569594121\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (3.52502036095,3.37479070661), test loss: 3.01721738577\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (20.1782970428,27.8303406801), test loss: 28.4349704742\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.233182623982,3.36289239775), test loss: 2.45269176662\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (6.79498815536,27.7471522804), test loss: 32.8510935783\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.632122337818,3.35099853714), test loss: 3.16074264348\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (25.4618320465,27.6684152371), test loss: 29.3614269972\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.79487085342,3.33931025223), test loss: 2.45655377656\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (8.00227451324,27.5901370892), test loss: 33.041676259\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.24280762672,3.32776088775), test loss: 3.16628298759\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (12.2838878632,27.5121497785), test loss: 27.2858768225\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (4.19246578217,3.31636008415), test loss: 2.29505820572\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (9.54508018494,27.4356132941), test loss: 31.3668934107\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.66899895668,3.30527028518), test loss: 2.98666232526\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (8.84455108643,27.3615608583), test loss: 29.4470572472\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.88831281662,3.29441204256), test loss: 2.50789476633\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (21.0401115417,27.2855984448), test loss: 33.2648459435\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.51954293251,3.28354109938), test loss: 3.00738354623\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.76380825043,27.2119697269), test loss: 27.7251642942\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.03494763374,3.27283669974), test loss: 2.60170846283\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (9.08371353149,27.1401127024), test loss: 30.7385095596\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.90196156502,3.26221727378), test loss: 2.77581423819\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (22.1683502197,27.0684473095), test loss: 27.0797361374\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.18452620506,3.25182185274), test loss: 2.74304302335\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (10.4887256622,26.9971133563), test loss: 31.5254649878\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.4947770834,3.24152606622), test loss: 2.8545996964\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (5.83875083923,26.9287982865), test loss: 29.8291915536\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.242634728551,3.23151507854), test loss: 3.09327321649\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (18.2047805786,26.8585412363), test loss: 32.0826470375\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.03659558296,3.22159298155), test loss: 2.95594722331\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (10.4411659241,26.7900793309), test loss: 31.0429936171\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.26668214798,3.21176678967), test loss: 3.14388175309\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (30.1929798126,26.7242358989), test loss: 28.7948099613\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.63027739525,3.20197874633), test loss: 2.47160588205\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (29.4344406128,26.6580205619), test loss: 33.9208194017\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (1.69482278824,3.1923920529), test loss: 3.20739529729\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (27.607257843,26.5912532405), test loss: 29.1601458073\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.53988659382,3.18284973323), test loss: 2.28884138018\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (13.2678098679,26.5272722193), test loss: 32.3953163862\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (3.25027322769,3.17364016167), test loss: 3.01991878897\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (11.15396595,26.4623022177), test loss: 27.3209014416\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.677898526192,3.16446251567), test loss: 2.41835156381\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (16.108127594,26.3984832243), test loss: 33.5609692097\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.46194672585,3.15536553171), test loss: 3.13002856076\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (9.30522727966,26.3369257802), test loss: 28.1658292055\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.5050894022,3.14633386844), test loss: 2.51242460012\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (35.586769104,26.275919488), test loss: 33.0102492809\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (3.64716410637,3.13746887576), test loss: 2.85673487484\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (4.32465076447,26.213460762), test loss: 26.8383825541\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (1.78695845604,3.12863187473), test loss: 2.39164042771\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (11.5574464798,26.1533535917), test loss: 31.3716421604\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (1.32270050049,3.12005096956), test loss: 2.85670963228\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (56.9876327515,26.0946055393), test loss: 31.474459362\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (3.9228618145,3.11166508651), test loss: 3.28078757226\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (26.0735664368,26.0337214021), test loss: 31.2004989028\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (2.60052442551,3.10314587756), test loss: 2.9657150954\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (28.1988887787,25.9757402835), test loss: 29.4713166237\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.23390614986,3.09480591649), test loss: 2.96629875302\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (11.0384883881,25.9185026212), test loss: 27.3855078936\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.82361018658,3.08654161896), test loss: 2.38941216469\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (35.6682090759,25.8606573174), test loss: 32.6033060193\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (1.91344249249,3.07829765906), test loss: 3.12365416586\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (3.37560868263,25.8040354052), test loss: 29.514932251\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (2.03836393356,3.07033311044), test loss: 2.40144105554\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (9.92107963562,25.7490403556), test loss: 33.2297010899\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.605340003967,3.06248325193), test loss: 3.1983497709\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (40.7704353333,25.6921492009), test loss: 27.7323478699\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (2.19329476357,3.0546099797), test loss: 2.34006011784\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (4.67477607727,25.6374509073), test loss: 31.2696675301\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.19847822189,3.04681868477), test loss: 2.98496640921\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (25.2661132812,25.5836129909), test loss: 28.6922497749\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (0.463379055262,3.03907150314), test loss: 2.43222406805\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (15.1298856735,25.5296265277), test loss: 32.0255843401\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (5.21183776855,3.03147322323), test loss: 2.96783006787\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (11.4838428497,25.4760421094), test loss: 27.6651216507\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.16682553291,3.02394308327), test loss: 2.58108114004\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (19.2909088135,25.4245723762), test loss: 30.887165451\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.911891460419,3.01661693878), test loss: 2.75407855213\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (22.543340683,25.3711079399), test loss: 26.9247641087\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (2.88552308083,3.00931101406), test loss: 2.45491099656\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (8.33684158325,25.319428466), test loss: 31.1134105802\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.06626605988,3.00205427004), test loss: 2.82483969927\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (33.7857017517,25.2693253254), test loss: 29.6367042542\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.03499007225,2.99482006747), test loss: 3.03952387273\n",
      "\n",
      "MC # 5, Hype # hyp4, Fold # 9\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold9/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (336.427856445,inf), test loss: 199.171984863\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (332.861175537,inf), test loss: 378.441952515\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (12.2853374481,100.659537914), test loss: 44.3329690218\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.87075710297,81.4085348171), test loss: 3.16402837187\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (34.4077911377,73.3759832029), test loss: 36.296837759\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.56797885895,42.3099854953), test loss: 2.80533584952\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (32.6986656189,63.9436216275), test loss: 42.4653447628\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.21993339062,29.2550340451), test loss: 3.45583791137\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (108.367736816,59.3756699383), test loss: 34.9186333179\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (4.65318346024,22.7410246121), test loss: 2.80637665987\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (62.2604446411,56.5163796095), test loss: 42.3184065104\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (3.82160639763,18.8351056826), test loss: 3.37097823322\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (23.796579361,54.5818055081), test loss: 41.6751032352\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.56930851936,16.2233919169), test loss: 3.36757032871\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (43.7890586853,53.1281096418), test loss: 41.7025705099\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (3.50639057159,14.3579573476), test loss: 3.18451578617\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (43.5884399414,52.0345417215), test loss: 42.7550423622\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (5.53588104248,12.9551440269), test loss: 3.47736932635\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (42.5765075684,51.2240370096), test loss: 37.8403776169\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (1.60092639923,11.8652359056), test loss: 2.88278847039\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (59.6342391968,50.5136669673), test loss: 43.4628631115\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (4.84743690491,10.9947216359), test loss: 3.42434279621\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (32.4093322754,49.9412333891), test loss: 40.424352169\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (2.01171636581,10.2796823352), test loss: 2.72526889145\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (49.2658081055,49.4100738868), test loss: 43.6469685078\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (2.72539424896,9.68351146846), test loss: 3.62145155966\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (31.1025466919,48.9601520493), test loss: 40.7969986677\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.70407485962,9.1785003912), test loss: 2.68589632511\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (38.8725509644,48.5402825973), test loss: 43.2748160601\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (2.37659955025,8.74088078581), test loss: 3.65181086659\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (74.7685089111,48.2111263034), test loss: 39.4498737574\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (6.13084793091,8.36489050524), test loss: 2.8318801105\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (21.2659645081,47.8774981073), test loss: 43.6858503342\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.97393071651,8.03544462477), test loss: 3.38445741832\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (88.0689239502,47.5811431056), test loss: 37.938581109\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (3.99265408516,7.74209878018), test loss: 2.83419432044\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (87.7659759521,47.2780615035), test loss: 44.6029461861\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (1.8354588747,7.48018427814), test loss: 3.23723777235\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (41.9878005981,46.9962018447), test loss: 36.6639995575\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (5.88010501862,7.24415024098), test loss: 2.71537642479\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (27.3042640686,46.7318251139), test loss: 41.2800827026\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.78964245319,7.0305886338), test loss: 3.27402600944\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (48.8501358032,46.4529093414), test loss: 33.628537035\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (2.56055426598,6.83698538683), test loss: 2.60261413455\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (46.2053375244,46.1925361625), test loss: 38.7876401901\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (4.69385051727,6.65908629734), test loss: 3.08318398595\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (17.039730072,45.9122909879), test loss: 36.2223675847\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (2.55258774757,6.49521158094), test loss: 3.08368091285\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (24.1874465942,45.6465984567), test loss: 39.1704761267\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (4.7329120636,6.34464180059), test loss: 3.14433906078\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (49.074306488,45.3729454611), test loss: 38.0510010242\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.84697628021,6.20269035044), test loss: 3.3558139801\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (28.5230503082,45.1220965858), test loss: 37.8973430634\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (6.86387634277,6.07310912856), test loss: 3.12932385802\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (31.0880012512,44.8542614931), test loss: 38.8181679964\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.09807276726,5.95191333072), test loss: 3.31175778508\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (19.8249740601,44.5904993503), test loss: 33.4147974253\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (2.0105574131,5.83814823075), test loss: 2.57763557136\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (71.3647155762,44.3096389836), test loss: 40.2960224152\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (0.693299114704,5.73094596187), test loss: 3.27366098762\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (14.5270519257,44.0265991226), test loss: 35.0988041639\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (4.62337636948,5.62986762685), test loss: 2.53260080516\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (20.8655891418,43.7546141379), test loss: 38.4797124386\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (3.59775495529,5.53415145375), test loss: 3.44485467076\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (71.6381759644,43.4689560804), test loss: 33.7480170965\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.22146916389,5.44417348089), test loss: 2.44029951096\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (39.338760376,43.158531568), test loss: 35.854435873\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (3.78827166557,5.3586773503), test loss: 3.27627090216\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (39.1054229736,42.8227448698), test loss: 32.1580694199\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.98526501656,5.27715160595), test loss: 2.71222608089\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (25.5983428955,42.4859226304), test loss: 38.0599178791\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (2.1022362709,5.19969960581), test loss: 3.23741705418\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (53.261390686,42.1386221568), test loss: 30.0041491032\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.91101455688,5.12415442602), test loss: 2.59172351956\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (24.3328609467,41.7935894891), test loss: 36.6901862621\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.07085347176,5.0530316813), test loss: 3.14562956691\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (57.1822662354,41.4363920237), test loss: 26.7882184982\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.51392245293,4.98464496184), test loss: 2.54040504396\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (20.1294593811,41.0749973177), test loss: 32.7047916889\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (3.73847723007,4.91865655175), test loss: 2.99887543172\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (26.8751392365,40.7021910801), test loss: 25.4872110367\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (0.851660370827,4.85501756116), test loss: 2.55662011802\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (27.989566803,40.3308073211), test loss: 31.438664341\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.31548047066,4.79388508048), test loss: 3.13299103975\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (10.7424478531,39.9638164682), test loss: 30.3829604626\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (0.984713792801,4.73471048515), test loss: 2.96106252968\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (14.7843227386,39.5930216952), test loss: 30.3352941513\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (0.593256831169,4.67822663022), test loss: 3.15009816289\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (25.0949039459,39.2283702558), test loss: 30.0936249256\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.884709358215,4.624242096), test loss: 3.20454366207\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (32.9161071777,38.8600505402), test loss: 32.0135086894\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.71808624268,4.57207805514), test loss: 3.16804333478\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (16.7210388184,38.4997462798), test loss: 31.3720334291\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.993047833443,4.52202076901), test loss: 3.23805147111\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (16.483007431,38.1448885373), test loss: 27.5895606995\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (5.20636558533,4.47276816866), test loss: 2.60345520377\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (30.383430481,37.7986082101), test loss: 32.6304529667\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (3.93665075302,4.42603721323), test loss: 3.43966674209\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (37.9680976868,37.457493402), test loss: 28.3231707573\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (2.46341991425,4.38091591128), test loss: 2.47255711704\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (24.2549953461,37.1276957195), test loss: 33.5610403419\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.99488055706,4.33760035072), test loss: 3.39234475493\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (32.3310852051,36.8000226303), test loss: 29.0044766188\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (3.73984360695,4.29552136722), test loss: 2.48645434976\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (49.9737472534,36.485704933), test loss: 32.7177868843\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (2.20178055763,4.25480233275), test loss: 3.32786735892\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (5.69775485992,36.1793148843), test loss: 29.8621547461\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (0.30910474062,4.21499219596), test loss: 2.66904049814\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (9.86271858215,35.8790723534), test loss: 33.501279068\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.14321196079,4.17687031209), test loss: 3.22894943357\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (9.17544651031,35.5920556571), test loss: 27.9448731422\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (0.78774535656,4.14039827096), test loss: 2.71465291679\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (24.566576004,35.3092739675), test loss: 33.4471171856\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.44236814976,4.10495951625), test loss: 3.14533889294\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (22.4287261963,35.0370661797), test loss: 24.7367616177\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.42633748055,4.07072439156), test loss: 2.57153047621\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (11.5754108429,34.7731071101), test loss: 32.0832356215\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (3.53142166138,4.03684990081), test loss: 3.20683932006\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (16.3235626221,34.5157136253), test loss: 25.1340008736\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.45247673988,4.00441604054), test loss: 2.63145518899\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (11.9395618439,34.2647562319), test loss: 32.3429950714\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.960706889629,3.97298634659), test loss: 3.31431067288\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (10.5007772446,34.0245621218), test loss: 31.2886973858\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.763237059116,3.94278010452), test loss: 3.22226373255\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (18.5218658447,33.7865457037), test loss: 32.2728528976\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (3.69185590744,3.91329677594), test loss: 3.28120863885\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (66.5115814209,33.5594574352), test loss: 29.7912782192\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (3.51718997955,3.88459234271), test loss: 3.23959575593\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (12.4605731964,33.33546479), test loss: 29.0246491909\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (1.01193010807,3.85625869675), test loss: 2.82523800582\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (12.0367164612,33.1162754702), test loss: 31.7862180591\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (1.5481159687,3.82903002087), test loss: 3.29156728685\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (8.48689842224,32.9069133245), test loss: 29.2952042341\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (0.895410776138,3.80296407889), test loss: 2.56374299079\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (13.7643146515,32.7002120812), test loss: 32.9690257072\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.27300548553,3.7775331398), test loss: 3.465864712\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (24.0776367188,32.500576215), test loss: 30.1123144627\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (1.99216294289,3.75280607772), test loss: 2.53932649791\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (35.3547515869,32.3072742318), test loss: 33.6546337605\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.86925411224,3.72814927414), test loss: 3.37106371224\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (22.1885719299,32.1166101524), test loss: 28.7436629772\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.21227419376,3.70442883279), test loss: 2.66153359264\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (15.5740785599,31.9299976607), test loss: 34.7926568508\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.75080215931,3.68133745831), test loss: 3.39053898454\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (9.03163528442,31.7515363727), test loss: 30.7352054596\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.01228225231,3.65909301688), test loss: 2.70587197244\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (3.54541015625,31.5739856293), test loss: 33.2130969524\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.00349712372,3.637259747), test loss: 3.25319917798\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (20.8651618958,31.4047729466), test loss: 28.6268280506\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.789806723595,3.61588323352), test loss: 2.69691745639\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (14.7691650391,31.2366767586), test loss: 32.8015155792\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (2.23127508163,3.59465794124), test loss: 3.18121060729\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (13.2221841812,31.0711805526), test loss: 25.8348568916\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (0.612505078316,3.57412186232), test loss: 2.5647484526\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (9.36160087585,30.9132731499), test loss: 32.8278698444\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (2.37410354614,3.5544755321), test loss: 3.24082838148\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (18.9561367035,30.7568845123), test loss: 24.7623631477\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.416041880846,3.53513082542), test loss: 2.66188567132\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (20.901807785,30.6049757605), test loss: 33.0049026966\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (1.3388479948,3.5162801368), test loss: 3.26822118759\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (25.1796264648,30.4581573741), test loss: 31.1507023335\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.40056705475,3.49734623778), test loss: 3.2038705349\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (57.0947875977,30.3130006666), test loss: 32.8831031322\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.87201452255,3.47910468808), test loss: 3.21905485392\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (7.96041107178,30.1687112872), test loss: 30.9007857323\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (1.40045309067,3.46117437661), test loss: 3.21946718097\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (13.8277244568,30.0313474274), test loss: 26.9582195282\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.41303837299,3.44389482203), test loss: 2.74224627018\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (11.7934179306,29.8940537246), test loss: 31.2643788338\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (0.779160439968,3.42684912823), test loss: 3.27835394293\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (20.7950134277,29.7633195558), test loss: 29.6792129993\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.44594967365,3.41013115673), test loss: 2.53376599252\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (21.599155426,29.632845335), test loss: 32.5243149281\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (5.13961410522,3.39352391008), test loss: 3.3841237843\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (8.26519775391,29.5030141713), test loss: 30.161498642\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (2.34789681435,3.37725588758), test loss: 2.49303251058\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (13.8042726517,29.3795970951), test loss: 33.3310214162\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.613293647766,3.36169342146), test loss: 3.26497029662\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (11.7169933319,29.2570217578), test loss: 29.2158313274\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.498111277819,3.34638384656), test loss: 2.55274142325\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (6.91015005112,29.1375038968), test loss: 35.4421237469\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.70765686035,3.33146401062), test loss: 3.40311406851\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (15.571103096,29.0217617656), test loss: 29.0691425085\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.96215307713,3.31638479995), test loss: 2.65639167726\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (31.2031059265,28.9067534967), test loss: 33.8263947964\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.57565259933,3.3018063437), test loss: 3.16751851588\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (34.656791687,28.792660492), test loss: 27.8962519169\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.89103293419,3.28749231058), test loss: 2.62198819965\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (16.0097503662,28.6830391797), test loss: 31.8357516289\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.938535809517,3.27363123459), test loss: 3.0701277703\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (17.8808746338,28.5731520805), test loss: 26.40935359\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.16390395164,3.25995783504), test loss: 2.58764368296\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (19.6026287079,28.4686176841), test loss: 32.656103003\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (2.3527841568,3.24649810854), test loss: 3.20086980164\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (14.2970352173,28.3639586971), test loss: 31.2702790499\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (0.769199371338,3.2330692911), test loss: 3.05549545884\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (41.5159301758,28.25982622), test loss: 33.5896033764\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.52636182308,3.21989676789), test loss: 3.2512968272\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (14.27384758,28.1595862973), test loss: 30.7442184925\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.20530200005,3.20726672787), test loss: 3.22590307295\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (15.7883825302,28.0603891805), test loss: 33.1962797642\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.82022976875,3.19482365511), test loss: 3.21925614029\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (23.5661716461,27.9638474537), test loss: 31.662193203\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.28145480156,3.18265827907), test loss: 3.31753349304\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (10.5299415588,27.8696665001), test loss: 28.2432285786\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.54275369644,3.17032181845), test loss: 2.66246167123\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.58347511292,27.7756621145), test loss: 33.486801064\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (0.735562205315,3.15835430852), test loss: 3.34276407361\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (12.8147144318,27.6823586086), test loss: 30.0801636219\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.18216848373,3.14658108925), test loss: 2.49548968524\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (24.8464508057,27.5930216562), test loss: 33.4783931732\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.4370265007,3.13517633915), test loss: 3.35595000982\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (15.2148733139,27.5025910538), test loss: 30.3831485987\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (2.79953694344,3.12391910476), test loss: 2.45031283498\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (4.03495883942,27.4163931245), test loss: 33.0474270344\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.89609968662,3.112755934), test loss: 3.31115060449\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (11.9750375748,27.3302915172), test loss: 30.4382410526\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (0.780759572983,3.10158670837), test loss: 2.59391081333\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (30.768781662,27.244536973), test loss: 34.5662596703\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.4719619751,3.09066437441), test loss: 3.28212241232\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (10.8630886078,27.1612831379), test loss: 28.6225296497\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (3.35647249222,3.08015533233), test loss: 2.64382238537\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (43.9070892334,27.0792660315), test loss: 34.5524436235\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (3.6395277977,3.0697693222), test loss: 3.11882130951\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (15.6218090057,26.9985325887), test loss: 25.9698747635\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (1.415402174,3.05956405861), test loss: 2.59275764376\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (11.1623821259,26.9203704263), test loss: 31.6691292763\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.64255440235,3.04923708714), test loss: 2.99663719237\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (28.7464523315,26.8420513382), test loss: 26.1123541355\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (2.9935324192,3.03920889962), test loss: 2.57228630781\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (4.3389377594,26.7632473507), test loss: 32.7517599106\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (0.905133485794,3.02926978701), test loss: 3.13574083\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (31.2696800232,26.6888334799), test loss: 31.2636495829\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.870563924313,3.0196673912), test loss: 2.99423705339\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (16.7523555756,26.6128021319), test loss: 32.3401881695\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.63768398762,3.01020972933), test loss: 3.19963052571\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (5.19698047638,26.5401006642), test loss: 29.7228696823\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (2.71514010429,3.00080099738), test loss: 3.11452252865\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (8.12265396118,26.4673430646), test loss: 34.3674369454\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (1.70434093475,2.99134442021), test loss: 3.17490793169\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (54.5317306519,26.3955707761), test loss: 31.9683763504\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (2.72821927071,2.98209482426), test loss: 3.24056466222\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (20.5803947449,26.3248148908), test loss: 29.3325879574\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (3.60288000107,2.97321004814), test loss: 2.6481416434\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (13.1976909637,26.2549885886), test loss: 32.9369242668\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (2.48382997513,2.96440989588), test loss: 3.38925771415\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (13.401638031,26.1863153975), test loss: 29.5607602596\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (0.883314490318,2.95572916117), test loss: 2.50638283193\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (10.8126983643,26.1198887943), test loss: 34.6007241011\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.86239349842,2.94695459544), test loss: 3.32777341008\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (26.8334541321,26.0534804729), test loss: 30.230351305\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (1.12882375717,2.93841135149), test loss: 2.45055357814\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (17.1413249969,25.986008067), test loss: 33.8235177159\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (1.74338257313,2.92993623164), test loss: 3.26432162821\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (18.0521965027,25.9222592433), test loss: 31.3382205963\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (0.95936024189,2.92173579519), test loss: 2.60071988255\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (18.8235855103,25.8571614622), test loss: 33.178362298\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (1.71177732944,2.91366024324), test loss: 3.16285729408\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (8.38942337036,25.7948816954), test loss: 28.489043808\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (1.988707304,2.90563109943), test loss: 2.66760437638\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (18.5359268188,25.7324536253), test loss: 33.5358471394\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (1.31078016758,2.89750467849), test loss: 3.08959328234\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (26.3405990601,25.670578939), test loss: 26.3392261028\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (2.00138354301,2.88956762166), test loss: 2.52659492791\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (19.1467781067,25.6096106298), test loss: 32.740440321\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (0.938875079155,2.88191730474), test loss: 3.08539636284\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (10.9339590073,25.5493555403), test loss: 25.9412708044\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (1.30302202702,2.87437170786), test loss: 2.59844207764\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (24.6365127563,25.4900896939), test loss: 32.3330797911\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.83589339256,2.86690728349), test loss: 3.19491256177\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (14.6647634506,25.4323871894), test loss: 31.0457748652\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (2.92219638824,2.8593385498), test loss: 3.06593018174\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (19.8404598236,25.3749953972), test loss: 32.3256654501\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (2.11574697495,2.85196081539), test loss: 3.13542099595\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (15.8065986633,25.3163717036), test loss: 30.656779623\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (0.820313096046,2.84461465774), test loss: 3.10219254792\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (13.5421066284,25.2608521687), test loss: 27.5715415478\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.442628711462,2.83750892483), test loss: 2.6529294908\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (18.6158103943,25.2041975563), test loss: 30.8838955164\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (3.13145208359,2.83055492534), test loss: 3.19881802499\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (7.00444555283,25.1498647383), test loss: 29.8117513657\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.81039249897,2.82356382901), test loss: 2.49795669913\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (14.6081600189,25.0956266178), test loss: 33.1966314673\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (2.36756896973,2.8165180836), test loss: 3.35262243152\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (7.31196451187,25.0412980717), test loss: 30.3053979158\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (3.61286783218,2.80962490439), test loss: 2.40074596405\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (12.8728551865,24.9879410629), test loss: 33.6838759184\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (0.964813768864,2.80292632134), test loss: 3.24773700535\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (48.9380264282,24.9357161875), test loss: 29.709474802\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (1.77758467197,2.79636386097), test loss: 2.5369428277\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (21.1038417816,24.8834054727), test loss: 35.0476277828\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (1.4812734127,2.78985208502), test loss: 3.28507917821\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (13.1058349609,24.8325325681), test loss: 29.3032520294\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (2.29595279694,2.78324258298), test loss: 2.60256207883\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (17.0094928741,24.7820953416), test loss: 33.3871959686\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (0.898468971252,2.77677183285), test loss: 3.12969217598\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (26.1997070312,24.7306468118), test loss: 28.233441782\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.11620187759,2.77035214863), test loss: 2.59466196001\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (13.0305862427,24.681651763), test loss: 32.0841164351\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (0.407642871141,2.76410714346), test loss: 3.00423642248\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (6.9734787941,24.6314670441), test loss: 26.2051361561\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.360334694386,2.75800706237), test loss: 2.49549894929\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (18.2190093994,24.5835904439), test loss: 32.4614681959\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (4.75903606415,2.75191844785), test loss: 3.12888142765\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (14.9974489212,24.5357026332), test loss: 25.2165222168\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (3.17197728157,2.74568807568), test loss: 2.73202887475\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (12.0623693466,24.4876032463), test loss: 33.2276665688\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (2.21941041946,2.73962111197), test loss: 3.1576367721\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (5.5250453949,24.4400958589), test loss: 30.4891261578\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (1.42982041836,2.73371119595), test loss: 3.0353448838\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (46.8439712524,24.3942888755), test loss: 33.6181443691\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (3.59247732162,2.7279552034), test loss: 3.2269903481\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (16.6488666534,24.3475367725), test loss: 30.3764732122\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.829449534416,2.72218367447), test loss: 3.06534847915\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (8.44099903107,24.3023434526), test loss: 27.3852931738\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.787065386772,2.71634253151), test loss: 2.66346799135\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (15.4477882385,24.2574089735), test loss: 31.5798145771\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (0.682005286217,2.71062214877), test loss: 3.16877804101\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (12.3812179565,24.2114832542), test loss: 30.6523590088\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (0.679134488106,2.70493336205), test loss: 2.50921344161\n",
      "\n",
      "MC # 5, Hype # hyp4, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_5/fold10/pretrained_models/ADNI1and2_ff_hyp4_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (366.368652344,inf), test loss: 201.193136978\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (263.41104126,inf), test loss: 323.980419922\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (78.1007003784,123.147911039), test loss: 55.7680667877\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (3.60679149628,55.2929292903), test loss: 3.29070163369\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (65.0469207764,86.5798798923), test loss: 37.7391358852\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.50343000889,29.1185598048), test loss: 2.51622632742\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (39.3468399048,73.4538376185), test loss: 47.3043053627\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.02307152748,20.376781183), test loss: 3.3342359364\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (50.8668289185,66.9102828856), test loss: 39.8333905697\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.66141021252,16.0148822036), test loss: 3.38216266483\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (87.8883666992,62.9104436302), test loss: 45.3482423544\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (2.05211949348,13.3986111263), test loss: 3.3694827497\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (35.6156463623,60.1859709581), test loss: 43.8840466499\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (1.39103484154,11.6513367133), test loss: 3.50493879318\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (63.8085746765,58.1673428053), test loss: 43.98122015\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (6.90132045746,10.4053821072), test loss: 2.66494150758\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (39.081703186,56.6303995762), test loss: 47.2256597519\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (2.19062685966,9.46927076552), test loss: 3.4262838006\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (47.6047134399,55.4198372889), test loss: 43.261544466\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.57395172119,8.7401484808), test loss: 2.34268618822\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (34.2188301086,54.4820546906), test loss: 45.6957718372\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.13199734688,8.15729537517), test loss: 3.56843736768\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (22.3386325836,53.6931558409), test loss: 40.8831460476\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.26352024078,7.68177732817), test loss: 2.56709100604\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (34.5821075439,53.0120635692), test loss: 47.083914566\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (1.14298474789,7.28640053352), test loss: 3.44298660755\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (99.0858612061,52.4203225396), test loss: 40.4915234089\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (2.69824981689,6.95038965866), test loss: 2.63596825898\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (69.6283035278,51.8509186828), test loss: 49.0023506165\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (3.74774098396,6.66124071807), test loss: 3.27948045731\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (59.7385749817,51.3896572852), test loss: 38.7941596985\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (1.39910185337,6.41042623544), test loss: 2.50733869076\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (9.25598812103,50.950573512), test loss: 46.1298001766\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (1.18191194534,6.19042407589), test loss: 3.26612681448\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (47.1871185303,50.5848384133), test loss: 39.7537341595\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.65225362778,5.99758956512), test loss: 3.00469453633\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (60.0475845337,50.2378148932), test loss: 43.7341046333\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.70158243179,5.82694211339), test loss: 3.24009745121\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (27.6759910583,49.9036796458), test loss: 39.3691007614\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (2.50561213493,5.67245768567), test loss: 3.32156502604\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (38.682510376,49.5829731913), test loss: 39.7603339672\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.97393965721,5.53332430208), test loss: 2.62880398333\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (32.6402778625,49.2800660963), test loss: 41.8289628029\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (7.00135231018,5.40722363897), test loss: 3.36055357158\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (57.4012832642,48.9927891662), test loss: 41.9550973892\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (6.60436820984,5.29091862062), test loss: 2.51368828863\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (21.3400726318,48.7486729845), test loss: 43.9676309586\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (1.27429807186,5.18675407126), test loss: 3.49024822116\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (43.8307876587,48.4961205772), test loss: 41.572618103\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.77431690693,5.09014747024), test loss: 2.37411545515\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (38.9521560669,48.249420719), test loss: 42.9458675385\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (1.32299041748,5.00020097005), test loss: 3.3392331779\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (15.5998067856,47.9997664283), test loss: 38.2789348602\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.57077002525,4.91780516103), test loss: 2.50698155761\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (34.9286880493,47.7565509804), test loss: 44.670100069\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (0.746182203293,4.84045215265), test loss: 3.2369358778\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (39.7487678528,47.5235591415), test loss: 36.7002058506\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (4.05555915833,4.76740057321), test loss: 2.4639875859\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (39.518699646,47.3086174666), test loss: 44.8879653454\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.87685704231,4.70022294013), test loss: 3.21259129047\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (51.0862617493,47.087932568), test loss: 32.8236531258\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (5.86892414093,4.63698254376), test loss: 2.39791152775\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (35.9815101624,46.8704531241), test loss: 42.4493979931\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (1.558583498,4.57681193845), test loss: 3.06564074159\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (29.1075630188,46.6377212335), test loss: 35.0523741961\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (1.20325040817,4.5199773152), test loss: 3.11440599561\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (45.0379981995,46.4076730537), test loss: 40.7203151703\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.57801890373,4.46647652439), test loss: 3.16764237881\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (7.01366186142,46.173314058), test loss: 37.3775424242\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (0.741308808327,4.41546387577), test loss: 3.33542815149\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (21.0809898376,45.9474678143), test loss: 37.362016964\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (3.29166865349,4.36862171861), test loss: 2.52881861031\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (16.9398345947,45.7127146582), test loss: 41.0756686687\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (1.05442738533,4.32371121716), test loss: 3.26599346995\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (28.4885692596,45.4738452483), test loss: 37.5861756802\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (7.32165288925,4.2804112543), test loss: 2.29634281844\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (45.4751548767,45.2293969308), test loss: 39.5441619873\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (1.67583060265,4.2383894444), test loss: 3.30115431547\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (59.8903503418,44.9674212697), test loss: 32.3829851151\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (2.6609609127,4.19739643), test loss: 2.34452319145\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (154.267608643,44.7167995002), test loss: 39.2073680639\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (5.38959980011,4.15782012819), test loss: 3.22097411752\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (35.2349853516,44.4578245026), test loss: 33.0494944572\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.38821768761,4.11942890082), test loss: 2.46177577376\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (13.3976192474,44.1993620576), test loss: 41.0584586143\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (4.01003170013,4.08264334863), test loss: 3.20435295105\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (41.0941047668,43.9334304633), test loss: 30.586700201\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (2.51057577133,4.04735253884), test loss: 2.3640812099\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (13.4535198212,43.6606019765), test loss: 38.4248378754\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (1.61034691334,4.01262117531), test loss: 2.97025052011\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (17.6602134705,43.3710877782), test loss: 31.515419364\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (2.36434030533,3.978886711), test loss: 2.94945476651\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (24.944480896,43.0631331579), test loss: 35.8413624287\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (3.98594284058,3.94588061976), test loss: 3.09214945138\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (18.2063865662,42.7486155515), test loss: 29.7136816502\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.92642736435,3.91334051632), test loss: 3.1239389807\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (31.9117889404,42.4397030326), test loss: 32.6340001583\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (4.73543643951,3.88271421123), test loss: 2.6920657292\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (10.6674461365,42.1208522809), test loss: 30.4374028206\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (1.28995871544,3.85227599176), test loss: 3.21284188256\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (25.8378257751,41.8029606512), test loss: 32.1626365662\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.31571984291,3.82279933861), test loss: 2.41663609892\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (14.9384841919,41.4775796446), test loss: 33.004180479\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.35224318504,3.79421145209), test loss: 3.2736328274\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (13.4150753021,41.1540596795), test loss: 31.3753530025\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (1.39205420017,3.76623100571), test loss: 2.23027730137\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (27.8242855072,40.8369412655), test loss: 33.5300884724\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (1.77173089981,3.7385264965), test loss: 3.24919519126\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (11.4080486298,40.5235493415), test loss: 29.2380537033\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (1.03162908554,3.71212690506), test loss: 2.47473653704\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (18.7759838104,40.2097706309), test loss: 35.2149373055\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (1.04352366924,3.68638208415), test loss: 3.15131387413\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (21.1909542084,39.9025282648), test loss: 27.7242537975\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (4.25248098373,3.66149923782), test loss: 2.49441361427\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (27.1883678436,39.5944551374), test loss: 36.6137944698\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (0.354087531567,3.63719102727), test loss: 2.95353138447\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (21.3866462708,39.2911530828), test loss: 24.8648941517\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.782558918,3.61358422758), test loss: 2.38356663287\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (8.83759212494,38.9983583122), test loss: 35.8545893431\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (2.43589687347,3.59021406618), test loss: 3.12084364891\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (14.1478319168,38.7130524026), test loss: 28.0063213348\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (1.70063602924,3.56777691167), test loss: 3.15476720333\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (9.41933250427,38.4300861595), test loss: 35.6891121387\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (1.30898499489,3.54589431306), test loss: 3.34731894135\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (15.2179803848,38.1549456862), test loss: 28.6336366653\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (1.14524650574,3.52483756861), test loss: 3.27154337764\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (56.8347091675,37.8855096703), test loss: 31.1073578835\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (2.47560214996,3.50441712723), test loss: 2.55817693919\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (25.7140197754,37.6175717058), test loss: 32.0581757069\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (2.45551228523,3.48424873083), test loss: 3.23406406343\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (18.8398246765,37.3605830783), test loss: 32.2529732227\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (0.814795553684,3.4645423389), test loss: 2.45253131241\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (24.6752548218,37.1116658809), test loss: 33.4925619602\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (2.03487801552,3.44524998347), test loss: 3.29487690926\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (53.4983139038,36.8664544154), test loss: 28.6598647118\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (2.72481775284,3.42656691484), test loss: 2.28628978133\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (5.40719604492,36.626571121), test loss: 34.2735287189\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.51030063629,3.4086780329), test loss: 3.24017215073\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (20.8880271912,36.3952015228), test loss: 30.6795171261\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (1.30729794502,3.39117254184), test loss: 2.53160528839\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (13.0812911987,36.1626780442), test loss: 36.7363373756\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (1.45134675503,3.37392564293), test loss: 3.14888369441\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (3.97824144363,35.9414128821), test loss: 28.9164081573\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (1.74321365356,3.35693184661), test loss: 2.52201314867\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (21.3324966431,35.7247215132), test loss: 36.6318098307\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (1.16008234024,3.34024582298), test loss: 3.06358582675\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (20.9197540283,35.5134734027), test loss: 29.5473495483\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (6.17477893829,3.32434680975), test loss: 3.12063526809\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (16.1777687073,35.306423389), test loss: 37.3300169945\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (0.955056667328,3.30865202044), test loss: 3.26268422902\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (36.3977355957,35.1060753484), test loss: 27.8357099056\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (0.79199641943,3.29349092299), test loss: 3.2081111148\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (27.5040378571,34.9060384002), test loss: 37.2364521503\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (4.51813364029,3.27862770227), test loss: 3.05338197947\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (6.01090097427,34.7116694811), test loss: 28.7005687714\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (4.31807851791,3.26400068515), test loss: 3.30079458952\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (17.7985935211,34.5234632219), test loss: 32.5136273146\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (3.6734354496,3.24940081477), test loss: 2.48402591944\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (47.8221664429,34.3403821038), test loss: 32.0338425636\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (3.57038116455,3.23542274377), test loss: 3.26918798089\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (23.7391891479,34.1579203556), test loss: 32.1278344154\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (1.68696844578,3.22160347675), test loss: 2.28466295153\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (16.7462730408,33.9824969178), test loss: 32.9969755888\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (2.32874250412,3.20822393496), test loss: 3.22758055031\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (28.2362823486,33.8075574009), test loss: 31.2427898884\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (0.87356710434,3.19517862239), test loss: 2.47320859134\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (6.56838130951,33.6362889565), test loss: 35.3659463406\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.88167989254,3.18229923916), test loss: 3.19557985663\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (9.32641601562,33.4722012817), test loss: 28.7778964996\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (2.84578275681,3.16943215387), test loss: 2.5454891175\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (24.4983711243,33.3114384749), test loss: 37.4193040848\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (1.06177246571,3.15708973688), test loss: 3.00667371154\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (37.680305481,33.1514618758), test loss: 26.2873884678\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.84427809715,3.14485584676), test loss: 2.42815660834\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (7.79463481903,32.9961252414), test loss: 36.8252027869\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (1.74964106083,3.13304919775), test loss: 3.15331422687\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (14.7810335159,32.8433800071), test loss: 29.2836522102\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (4.29407644272,3.12151848991), test loss: 3.22316911221\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (14.7624378204,32.6918145002), test loss: 37.2674064159\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (0.77569180727,3.10998847172), test loss: 3.37654908001\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (20.7446937561,32.5463081416), test loss: 28.4592648268\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (5.43205738068,3.09871034087), test loss: 3.2348469913\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (12.301197052,32.4042418981), test loss: 31.782315731\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (0.246006697416,3.08750633317), test loss: 2.56761918962\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (40.8871307373,32.2624179654), test loss: 33.4431940079\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (1.7131998539,3.07656452306), test loss: 3.24862315357\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (31.18201828,32.1250732817), test loss: 33.1783944845\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (2.73744487762,3.06609566298), test loss: 2.44059497118\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (30.6090431213,31.9911477095), test loss: 32.3152704239\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (4.67105102539,3.05578852427), test loss: 3.33222229481\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (42.8880805969,31.8554000864), test loss: 30.9904765606\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (3.67501020432,3.04551801208), test loss: 2.32962144911\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (22.8828411102,31.7256032555), test loss: 33.0117261171\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (1.40546011925,3.03525190832), test loss: 3.15500819087\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (20.9023170471,31.5977288361), test loss: 30.0684181213\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (2.39334869385,3.02519262183), test loss: 2.49032347202\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (37.7357215881,31.4720857138), test loss: 36.6747308254\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (1.84336745739,3.01538830728), test loss: 3.10427666306\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (40.7613182068,31.3482207927), test loss: 28.3597195148\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (2.63060045242,3.00587428455), test loss: 2.52589655221\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (21.8192443848,31.227984762), test loss: 36.0967929363\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.93300819397,2.99653865224), test loss: 3.05642415285\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (14.6136131287,31.1069751043), test loss: 25.6365455151\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (1.17780852318,2.98726700126), test loss: 2.77986871302\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (18.4085712433,30.9892774876), test loss: 37.6070817351\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (2.40003299713,2.97812554789), test loss: 3.228126508\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (14.1374158859,30.874733996), test loss: 28.4004915953\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (4.44985675812,2.9689504868), test loss: 3.14909265637\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (23.3988380432,30.7619605953), test loss: 37.9550149441\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (1.43860816956,2.96011463844), test loss: 3.38417562246\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (8.85371303558,30.6498398314), test loss: 29.7365256071\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (1.53218436241,2.95131783977), test loss: 3.26288450062\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (18.7280635834,30.5415159005), test loss: 32.6424173832\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.33303570747,2.94276884995), test loss: 2.46280071735\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (6.92252588272,30.4321818742), test loss: 31.2183851719\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.12902951241,2.93442565528), test loss: 3.2396070078\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (27.0636043549,30.3253061045), test loss: 31.5116142273\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (1.6355445385,2.9261109395), test loss: 2.25165373683\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (10.3433322906,30.2221308992), test loss: 33.9083331585\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.38064575195,2.91768650915), test loss: 3.16987934709\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (18.2424736023,30.1201581242), test loss: 30.2848912716\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (2.95564341545,2.9096699654), test loss: 2.42340460569\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (19.1792488098,30.0178837742), test loss: 36.187796247\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.037368536,2.90159460562), test loss: 3.15056183636\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (11.3262557983,29.9190539581), test loss: 28.9340917587\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.696470856667,2.89380720948), test loss: 2.52669475079\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (40.4143371582,29.8206053968), test loss: 36.9282810688\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (3.64042377472,2.88612669926), test loss: 2.92236118317\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (25.8545799255,29.7226532961), test loss: 26.821267271\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.91004800797,2.87851402547), test loss: 2.38885781914\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (18.6360988617,29.628153592), test loss: 36.3065522909\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (4.03561735153,2.87085899909), test loss: 3.04260410964\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (21.8564796448,29.5356611563), test loss: 29.1119973183\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.30290567875,2.86341588422), test loss: 3.17566496283\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (8.95098495483,29.4418293344), test loss: 37.1864892721\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (1.18246722221,2.85595911037), test loss: 3.29132321477\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (14.9452762604,29.3513264371), test loss: 28.5907241583\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (1.21959674358,2.84884910977), test loss: 3.20179913342\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (21.8726158142,29.2630195941), test loss: 33.4320302486\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (0.601081728935,2.8418027728), test loss: 2.59244050384\n",
      "ADAS Loss Iteration: 60000, train loss(batch, sum): (13.3992891312,29.1722441623), test loss: 31.3257616043\n",
      "MMSE Loss Iteration: 60000, train loss(batch, sum): (0.54202580452,2.83478286553), test loss: 3.23741059005\n",
      "ADAS Loss Iteration: 60500, train loss(batch, sum): (25.443069458,29.0858895076), test loss: 33.0412692547\n",
      "MMSE Loss Iteration: 60500, train loss(batch, sum): (1.32125294209,2.8277415133), test loss: 2.42792199254\n",
      "ADAS Loss Iteration: 61000, train loss(batch, sum): (5.81778240204,29.0001907446), test loss: 33.8942925453\n",
      "MMSE Loss Iteration: 61000, train loss(batch, sum): (1.36484289169,2.82078496697), test loss: 3.2513234973\n",
      "ADAS Loss Iteration: 61500, train loss(batch, sum): (9.09839057922,28.9151317504), test loss: 31.0018036604\n",
      "MMSE Loss Iteration: 61500, train loss(batch, sum): (2.21328544617,2.81398387767), test loss: 2.34160729051\n",
      "ADAS Loss Iteration: 62000, train loss(batch, sum): (16.502620697,28.8311280569), test loss: 32.3094037056\n",
      "MMSE Loss Iteration: 62000, train loss(batch, sum): (1.4677118063,2.80734502095), test loss: 3.0821796\n",
      "ADAS Loss Iteration: 62500, train loss(batch, sum): (13.0739908218,28.7498179404), test loss: 31.5558060169\n",
      "MMSE Loss Iteration: 62500, train loss(batch, sum): (2.18131017685,2.80083226158), test loss: 2.51560168564\n",
      "ADAS Loss Iteration: 63000, train loss(batch, sum): (11.4793491364,28.6670785248), test loss: 35.7625598669\n",
      "MMSE Loss Iteration: 63000, train loss(batch, sum): (2.40991806984,2.79434717633), test loss: 3.0423500061\n",
      "ADAS Loss Iteration: 63500, train loss(batch, sum): (6.92552947998,28.5863784176), test loss: 27.9978197813\n",
      "MMSE Loss Iteration: 63500, train loss(batch, sum): (2.2222943306,2.78783258704), test loss: 2.47792488933\n",
      "ADAS Loss Iteration: 64000, train loss(batch, sum): (9.4061050415,28.5077591282), test loss: 36.1498747826\n",
      "MMSE Loss Iteration: 64000, train loss(batch, sum): (2.45396351814,2.78135364661), test loss: 3.02308601737\n",
      "ADAS Loss Iteration: 64500, train loss(batch, sum): (32.0480651855,28.4296997139), test loss: 26.4202552557\n",
      "MMSE Loss Iteration: 64500, train loss(batch, sum): (3.75518679619,2.77514975926), test loss: 2.54643731415\n",
      "ADAS Loss Iteration: 65000, train loss(batch, sum): (32.5632858276,28.3519909618), test loss: 37.3892745972\n",
      "MMSE Loss Iteration: 65000, train loss(batch, sum): (2.75445556641,2.76887430371), test loss: 3.13247270286\n",
      "ADAS Loss Iteration: 65500, train loss(batch, sum): (6.7056760788,28.2765364755), test loss: 27.5265010357\n",
      "MMSE Loss Iteration: 65500, train loss(batch, sum): (0.368090867996,2.76277232574), test loss: 3.09497423247\n",
      "ADAS Loss Iteration: 66000, train loss(batch, sum): (32.6527481079,28.2000854374), test loss: 37.3836838007\n",
      "MMSE Loss Iteration: 66000, train loss(batch, sum): (1.71269392967,2.75677984557), test loss: 3.33107405007\n",
      "ADAS Loss Iteration: 66500, train loss(batch, sum): (18.7774162292,28.1248906364), test loss: 28.5337073565\n",
      "MMSE Loss Iteration: 66500, train loss(batch, sum): (2.15744137764,2.75082867458), test loss: 3.17748217583\n",
      "ADAS Loss Iteration: 67000, train loss(batch, sum): (11.527173996,28.0526456563), test loss: 32.1183785915\n",
      "MMSE Loss Iteration: 67000, train loss(batch, sum): (1.29827332497,2.74474386331), test loss: 2.41778318882\n",
      "ADAS Loss Iteration: 67500, train loss(batch, sum): (25.4177570343,27.9804675579), test loss: 32.2690249443\n",
      "MMSE Loss Iteration: 67500, train loss(batch, sum): (3.01316452026,2.7389315603), test loss: 3.18102904856\n",
      "ADAS Loss Iteration: 68000, train loss(batch, sum): (13.5097866058,27.9078433437), test loss: 32.1921293497\n",
      "MMSE Loss Iteration: 68000, train loss(batch, sum): (1.49653446674,2.73307582645), test loss: 2.21543880701\n",
      "ADAS Loss Iteration: 68500, train loss(batch, sum): (15.6860618591,27.8376031271), test loss: 33.522560215\n",
      "MMSE Loss Iteration: 68500, train loss(batch, sum): (3.5469751358,2.72740709061), test loss: 3.1547603935\n",
      "ADAS Loss Iteration: 69000, train loss(batch, sum): (17.1194210052,27.7668171697), test loss: 31.1545641899\n",
      "MMSE Loss Iteration: 69000, train loss(batch, sum): (0.888498485088,2.72175710038), test loss: 2.43744337857\n",
      "ADAS Loss Iteration: 69500, train loss(batch, sum): (7.59856653214,27.6966682761), test loss: 34.7138050556\n",
      "MMSE Loss Iteration: 69500, train loss(batch, sum): (1.54725527763,2.71622015525), test loss: 3.12305702865\n",
      "ADAS Loss Iteration: 70000, train loss(batch, sum): (5.70851230621,27.6292036616), test loss: 28.3793805599\n",
      "MMSE Loss Iteration: 70000, train loss(batch, sum): (1.34629619122,2.71056162142), test loss: 2.44805800021\n",
      "ADAS Loss Iteration: 70500, train loss(batch, sum): (16.2764835358,27.5624651868), test loss: 36.0079238892\n",
      "MMSE Loss Iteration: 70500, train loss(batch, sum): (1.31350207329,2.70510305742), test loss: 2.83824496567\n",
      "ADAS Loss Iteration: 71000, train loss(batch, sum): (6.21208286285,27.4947291147), test loss: 26.3132072926\n",
      "MMSE Loss Iteration: 71000, train loss(batch, sum): (2.07352638245,2.69961840855), test loss: 2.32211404443\n",
      "ADAS Loss Iteration: 71500, train loss(batch, sum): (14.6300449371,27.4290837828), test loss: 35.8127425671\n",
      "MMSE Loss Iteration: 71500, train loss(batch, sum): (2.16596484184,2.69431442662), test loss: 2.91045015752\n",
      "ADAS Loss Iteration: 72000, train loss(batch, sum): (55.8285598755,27.3647032797), test loss: 30.4520390511\n",
      "MMSE Loss Iteration: 72000, train loss(batch, sum): (3.67166733742,2.68912694356), test loss: 3.18911376595\n",
      "ADAS Loss Iteration: 72500, train loss(batch, sum): (41.1255378723,27.2986079141), test loss: 36.0122726917\n",
      "MMSE Loss Iteration: 72500, train loss(batch, sum): (5.07770824432,2.6839154538), test loss: 3.22930408418\n",
      "ADAS Loss Iteration: 73000, train loss(batch, sum): (24.9733085632,27.2350000638), test loss: 28.1584666729\n",
      "MMSE Loss Iteration: 73000, train loss(batch, sum): (1.53606534004,2.6786247532), test loss: 3.1381908834\n",
      "ADAS Loss Iteration: 73500, train loss(batch, sum): (12.8881816864,27.1726373478), test loss: 31.7196475983\n",
      "MMSE Loss Iteration: 73500, train loss(batch, sum): (1.97821712494,2.67344482838), test loss: 2.5416649878\n",
      "ADAS Loss Iteration: 74000, train loss(batch, sum): (26.0203170776,27.1100141948), test loss: 31.1704344034\n",
      "MMSE Loss Iteration: 74000, train loss(batch, sum): (2.49125504494,2.66830392598), test loss: 3.18390862942\n",
      "ADAS Loss Iteration: 74500, train loss(batch, sum): (5.45429897308,27.0478266348), test loss: 33.8641271591\n",
      "MMSE Loss Iteration: 74500, train loss(batch, sum): (1.3628565073,2.66333282822), test loss: 2.44178058803\n",
      "ADAS Loss Iteration: 75000, train loss(batch, sum): (10.8095541,26.9878503638), test loss: 31.6248607159\n",
      "MMSE Loss Iteration: 75000, train loss(batch, sum): (0.833475351334,2.65843437999), test loss: 3.20287395418\n",
      "ADAS Loss Iteration: 75500, train loss(batch, sum): (49.8781967163,26.9261640541), test loss: 32.9045643091\n",
      "MMSE Loss Iteration: 75500, train loss(batch, sum): (1.04529750347,2.65353061549), test loss: 2.37510095239\n",
      "ADAS Loss Iteration: 76000, train loss(batch, sum): (4.82606315613,26.8662717445), test loss: 32.6820275307\n",
      "MMSE Loss Iteration: 76000, train loss(batch, sum): (2.95954608917,2.64859564835), test loss: 3.04787465632\n",
      "ADAS Loss Iteration: 76500, train loss(batch, sum): (24.774143219,26.8076516695), test loss: 30.7644620895\n",
      "MMSE Loss Iteration: 76500, train loss(batch, sum): (1.22623229027,2.64368116064), test loss: 2.38789793998\n",
      "ADAS Loss Iteration: 77000, train loss(batch, sum): (18.990858078,26.7490196901), test loss: 34.3530486822\n",
      "MMSE Loss Iteration: 77000, train loss(batch, sum): (4.37511920929,2.63895722945), test loss: 3.00893200934\n",
      "ADAS Loss Iteration: 77500, train loss(batch, sum): (14.9902973175,26.6906534003), test loss: 28.1088364601\n",
      "MMSE Loss Iteration: 77500, train loss(batch, sum): (1.3643643856,2.63418600697), test loss: 2.46704319715\n",
      "ADAS Loss Iteration: 78000, train loss(batch, sum): (12.4325466156,26.634242976), test loss: 35.8425202847\n",
      "MMSE Loss Iteration: 78000, train loss(batch, sum): (0.933917403221,2.62955888343), test loss: 3.00553139448\n",
      "ADAS Loss Iteration: 78500, train loss(batch, sum): (11.7828102112,26.576199075), test loss: 26.9749352217\n",
      "MMSE Loss Iteration: 78500, train loss(batch, sum): (0.338278949261,2.62495893467), test loss: 2.38234511316\n",
      "ADAS Loss Iteration: 79000, train loss(batch, sum): (5.51865291595,26.5196203953), test loss: 36.6002961636\n",
      "MMSE Loss Iteration: 79000, train loss(batch, sum): (2.10772681236,2.62039915959), test loss: 3.09800584018\n",
      "ADAS Loss Iteration: 79500, train loss(batch, sum): (41.8897171021,26.4649198942), test loss: 27.6071914673\n",
      "MMSE Loss Iteration: 79500, train loss(batch, sum): (1.12847173214,2.61573306211), test loss: 3.06097799987\n",
      "run time for single CV loop: 7067.19902205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:124: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:126: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Deign Net Architecutre and Run Caffe\n",
    "exp_name = 'Exp13_MC'\n",
    "cohort = 'ADNI1and2'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'HC_CT'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "multi_label = False\n",
    "start_MC=1\n",
    "n_MC=5\n",
    "MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "start_fold = 6\n",
    "n_folds = 10\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "niter = 80000\n",
    "batch_size = 256\n",
    "\n",
    "pretrain = False\n",
    "multimodal_autoencode = False\n",
    "load_pretrained_weights = True\n",
    "\n",
    "if Clinical_Scale in ['BOTH','ADAS13_DX'] :\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "# dr: Dropout rate, lr: learning rate of each modality, tr: loss-weight for each task\n",
    "hype_configs = {\n",
    "            'hyp4':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':25,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':.1,'CT':1,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':2,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}},  \n",
    "    \n",
    "            'hyp5':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':25,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':6,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':2,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}},\n",
    "    \n",
    "#             'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':50,'COMB_ff':50,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':8,'COMB':1},\n",
    "#                       'tr':{'ADAS':1,'MMSE':2,'DX':1},'solver_conf':{'base_lr':3e-6, 'wt_decay':1e-3}},  \n",
    "        \n",
    "#             'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':200,'HC_CT_ff':25,'COMB_ff':50,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':0.5,'CT':0.5,'COMB':2},\n",
    "#                       'tr':{'ADAS':1,'MMSE':4,'DX':1},'solver_conf':{'base_lr':3e-6, 'wt_decay':1e-3}}, \n",
    "        \n",
    "#             'hyp6':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':50,\n",
    "#                                        'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "#                                        'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "#                       'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':1,'CT':1,'HC_CT':1},\n",
    "#                       'tr':{'ADAS':1,'MMSE':1,'DX':1},'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}},   \n",
    "                    \n",
    "                }\n",
    "\n",
    "CV_perf_MC = {}\n",
    "for mc in MC_list:\n",
    "    CV_perf_hype = {}\n",
    "    for hype in hype_configs.keys():    \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "        tr = hype_configs[hype]['tr']\n",
    "        solver_configs = hype_configs[hype]['solver_conf']\n",
    "        \n",
    "        if hype in ['hyp1']:\n",
    "            HC_snap = 20000 #for ADNI2 #5000 for ADNI1\n",
    "            CT_snap = 80000 #for ADNI2 #5000 for ADNI1\n",
    "            pre_hype = 'hyp1'\n",
    "        elif hype in ['hyp2']:\n",
    "            HC_snap = 20000 #8000 for ADNI2 #5000 for ADNI1\n",
    "            CT_snap = 80000 #28000 for ADNI2 #5000 for ADNI1\n",
    "            pre_hype = 'hyp1'\n",
    "        elif hype in ['hyp3']:\n",
    "            HC_snap = 20000 #8000 for ADNI2 #5000 for ADNI1\n",
    "            CT_snap = 80000 #28000 for ADNI2 #5000 for ADNI1\n",
    "            pre_hype = 'hyp3'\n",
    "        elif hype in ['hyp4']:\n",
    "            HC_snap = 20000 #8000 for ADNI2 #5000 for ADNI1\n",
    "            CT_snap = 80000 #28000 for ADNI2 #5000 for ADNI1\n",
    "            pre_hype = 'hyp4'\n",
    "        elif hype in ['hyp5']:\n",
    "            HC_snap = 20000 #8000 for ADNI2 #5000 for ADNI1\n",
    "            CT_snap = 80000 #28000 for ADNI2 #5000 for ADNI1\n",
    "            pre_hype = 'hyp4'\n",
    "        else:\n",
    "            print 'unknown hyp config'\n",
    "            \n",
    "        CV_perf = {}\n",
    "        start_time = time.time()\n",
    "        for fid in fid_list:            \n",
    "            print ''\n",
    "            print 'MC # {}, Hype # {}, Fold # {}'.format(mc, hype, fid)\n",
    "            snap_prefix = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}'.format(mc,fid,exp_name,hype,modality)\n",
    "\n",
    "            train_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/train_C688.txt'.format(mc,fid)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "\n",
    "            train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "            with open(train_filename_txt, 'w') as f:\n",
    "                    f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "            if pretrain:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_train.prototxt'.format(mc,fid)\n",
    "                with open(train_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(train_filename_txt, batch_size, node_sizes, modality)))            \n",
    "            else:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(train_net_path, 'w') as f:            \n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            if pretrain:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_test.prototxt'.format(mc,fid)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(test_filename_txt, batch_size, node_sizes,modality)))\n",
    "            else:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            # Define Solver\n",
    "            solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "            with open(solver_path, 'w') as f:\n",
    "                f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, snap_prefix)))\n",
    "\n",
    "            ### load the solver and create train and test nets\n",
    "            #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "            #solver = caffe.get_solver(solver_path)            \n",
    "            solver = caffe.NesterovSolver(solver_path)\n",
    "\n",
    "            if load_pretrained_weights:                    \n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_HC_CT_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                snap_path = baseline_dir + net_name.format(mc,fid,cohort,pre_hype,HC_snap,CT_snap)               \n",
    "                print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "                solver.net.copy_from(snap_path)\n",
    "\n",
    "            #run caffe\n",
    "            results = run_caffe(solver,niter,batch_size,multi_task,multi_label,multimodal_autoencode)\n",
    "            CV_perf[fid] = results\n",
    "            \n",
    "        print('run time for single CV loop: {}'.format(time.time() - start_time))\n",
    "\n",
    "        CV_perf_hype[hype] = CV_perf\n",
    "        \n",
    "CV_perf_MC[mc] = CV_perf_hype\n",
    "\n",
    "# pickleIt(CV_perf_MC, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.4444444444\n",
      "3.54166666667\n"
     ]
    }
   ],
   "source": [
    "#Time for training \n",
    "#HC_CT fold 9,10, number of iterations: 40k, two hyper-params\n",
    "#start time: 14:47\n",
    "#end time: 15:31\n",
    "#runtime_mean = (13+31)/4\n",
    "#print runtime_mean\n",
    "\n",
    "tx=140 #time for 10k iters\n",
    "itx=5 # num of 10k iters\n",
    "hx=2 #hyp choices\n",
    "fx=10 #k-folds\n",
    "mx=5 #mc-folds\n",
    "\n",
    "num_hrs = (tx*itx*hx*fx*mx)/(3600.0)\n",
    "print num_hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbcAAAJoCAYAAABVztwOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl8HOWd7/vvI5nNxgYbgm2M7TiQAQIhYcsEMgQHhpAF\nnHA44WYCGJhMcl9MFu45yT2B+ISBkDCZGw4Tzp0ckjmBAGEJJDmThJucbDC2Y2N2G8t4wYs2S5Zk\nWZYsydbadf+ollWSuqVeqvqpeurzfr38cqu7ll91V1d3f+up5zGe5wkAAAAAAAAAgCSpsl0AAAAA\nAAAAAADFItwGAAAAAAAAACQO4TYAAAAAAAAAIHEItwEAAAAAAAAAiUO4DQAAAAAAAABIHMJtAAAA\nAAAAAEDiEG4DAADgMGPMXxhj1htjuowxXzTGPGiMWTHJ9BljzDsqUNc/GGN+EvV6cqz3UmNMY6XX\nG1fGmD3GmItt1wEAAABIhNsAAAAY679Iet7zvOM8z/sXz/Nu9Tzv25NM7xW6YGPMj40x/caYA8aY\n7uz/pojaCl5XyCJZrzHmC8aYV4wxfcaYh4uY77eB528g8JweMMb8jzLq+UdjzL+WOn/UjDFfN8a8\nmd3OHcaYL08y7enZEy/Bfe2rgcf/izFmV/YkTqMx5jtF7osAAACIgWm2CwAAAECsLJb0VBHTFxsI\n/pPneXcWOY+rmiTdI+lKSccUOpPneR8buW2M+bGkxpQ8p8OSPi1pk6QzJf3RGFPned6v80w/5Hne\nrDyP/VzSDz3P6zLGzJH0K0n/p6QfhF00AAAAokPLbQAAAEiSjDHPSfqQpO9nW7qelm1t/c3ANP+3\nMabZGLPbGHOLKtua+ihjzKPZ2mqMMedla/qqMebn47blvxtj/jl7+9+NMfcaY17KttT9N2PM8UWs\n1xhj/rMxptUY02SMuTl75wXGmJZgi19jzH8wxqzP3v4HY8zPjDE/zdb8qjHmnJFpPc/7ZTaY7Sj5\nGclf8DXGmDeMMfuNMauMMWcGHvtG9jXsyraE/oAx5hOS/rOkm7K1vljAOo42xnw/u6wGY8z/Y4yp\nzj421xjzv7Pr32uM+dNk6y9kmzzP+yfP82o832ZJ/5+kgubNsaxdnud1Zf+slpSRdFopywIAAIA9\nhNsAAACQJHmed7mkP0v6gud5szzP2xF83BjzEfkB6OWS3inpr8c9/jfGmA1TrObvjTHt2e44/kOR\nJV4t6UlJx0l6VtL3s/c/LulKY8ysbB3Vkv4PSY8G5r1R0s2S5slvAfz/Bup+wxjz6UnWO0/STEkn\nS/o7+eH/cZ7nvSqpXdKHA9PeMG69yyQ9LWm2/BbxvxwJgKNijHm/pH+RdJOkOZJ+kl1vVTZcv1nS\nOZ7nHSfp45J2e573K0n3S3o0+9q/v4BVfVPS2ZLOknS+pKXyu7WRpK9J2ppd/3xJd2Vry7n+7GOX\nGWOaC9xGIz/YfnOSyaqzXY7UG2P+1Rgze9wybjbGHJDUKn9//lEh6wYAAEB8EG4DAACgUJ+S9GPP\n87Z4nndI2cByhOd5T3me995J5n9Afoh4kqQ7JT1ijLmoiPWv8Tzv957nefID23Oy622RtDpbnyR9\nVNJez/OCQftPAnV/Q9KnRlpce573Hs/zfjrJegck3eN53rDnef9bUo+k07OPPSY/OFe2e4sr5Qfw\nI17zPO/fPM8blh8eHy2pkOC4HJ+X9C+e523ItnL+kaSj5AfQQ9kazjbGVHueV+d5Xn2J6/mMpDs9\nz9vved5eSd9S9rmQNCj/ZMDbPc8b8jxvTfb+vOv3PO95z/NOLnDd35HUK+mJPI/vkXSepEWS/lLS\nXEk/Dk7ged4j2W5LzpD0PyXtLXDdAAAAiAnCbQAAABTqZEmNgb/rVUSf29mwdb/neZlsSPyEpGJa\nb7cEbh+UdLQxZuT77GPyW01L0vXyw++g8XUfKenEAte7z/O8zLh1H5u9/bikq4wxx0i6TtJqz/Pa\ncq03G8rvlv88RmmxpK8bYzqy//bL39YF2e48bpf0bUmtxpifGGPeVuJ65klqCPxdL2lB9va35QfM\n/26MecsY858kKc/6TypmpcaYr0j6pKSrsicNJvA874DneW9kw/0WSV+W9HFjzJE5pn1L0i5J/72Y\nOgAAAGAf4TYAAAAKtUfSwsDfi1Ven9ueih+QMp9fSjrHGHOWpKs0sUXv+LoH5HcpUhbP85olrZN0\nrfxwfXyofni92Zbip0gqqOuNMjTKb1E9J/tvtud5x3qe98tszT/xPO8Dkt4hfyDLb2XnK/a1bJH/\nXI5YLH+QzJFw+f/yPO/t8p+b/zrSSj/H+u8pdIXGmL+X9AVJl2Vbixcr3/42LVsPAAAAEoRwGwAA\nAIV6RtLNxpgzjTHT5XctUjBjzLXGmBnG92H5Lax/HXi81hizvJhFjtzwPK9f0i/kdwnykud5u8dN\ne4Mx5oxs3XdL+lm2JXUYfiK/r+mzJf2vcY+db4z5ZLaf7f8kqU/Si5LfN7gx5mj5AxpOM8YcFeyP\n2xiTMcZ8sIR6/lXSl4wx52eXc6wx5ursAJBnGmM+mG3B3C/pkPzBFCW/7+klRaznKUn/YIyZk219\n/XVlw/3s+kaW1S2/O5LMFOuflDHmbyWtkHSF53lNU0z7fmPMqdnbJ0n6Z0m/z+4nMsb8nTHmxOzt\nd8t//f6Ub3kAAACIJ8JtAAAABI0PfA//7Xne7yR9T9Lzkt6S9FxwQmPMZ4wxNZMs+zb53XLsl/RP\nkv7O87zV2XmPkD/44Itl1PqopHfL76JkvJ9kH2+W3yXJbYG6Nxlj/qaM9f6b/FbL/8vzvL5xj/1K\n/uCW++WH+dcEutL4r/K7OPla9rGD8sNbGWMWSjogabLnM1ct8jzvBfndcPww2yXJVkl/k532GEn/\nTX7/0k2SZsjvg1ySfippRrYrkzXjl5tjfXdK2ix/UMfX5Q9G+t3sY2fK75LkgKSVkr7red5Lk63f\nGHO5MSbYpct435J0gqT1xphuY8wBY8z9Iw8aY3YYY67J/vkXkv5kjOnO1rZf/gCbIz4kaXP28X+T\n9DP5Jz0AAACQICa8Bis5Fm7MQ/IvC231PO+c7H2z5Y8Yv1hSnaTrPM/ryj52h6S/ld+y4zbP8/4Q\nWXEAAACIDWPMByT9ved515exjIWStkia53leT+D+f5c/oOTD5Vead907JH3e87znA/f9g6RTPc8r\npjX6yLzXS3qX53krQiwTAAAAcErULbd/LH/E+KDbJf3J87zT5bf6uUOSjDHvkj8Iz5nyR7j/HyMj\n2AMAAMBtnuetLTPYrpL0FUk/DQbblWCMuVZSJhhsl8vzvCcItgEAAIDJTYty4Z7nrTHGLB539yck\nXZq9/aj8yxRvl7RM/o+RIUl1xpjtkt4n6aUoawQAAECyZfvRbpVUK7+RxHiRXaqYbRV+pvzBJAEA\nAABUUKThdh4neZ7XKkme57VkB3iRpAXyR5of0ZS9DwAAAMjL87yDkmZO8vhlEa77Q5M8Rh/OAAAA\nQITiMKBkdJ1+AwAAAAAAAACcZKPldqsxZq7nea3GmHmSRkZEb5K0MDDdKdn7JjDGEIgDAAAAAAAA\nQAJ4nhfJ2IqVCLdN9t+IX0u6WdI/SbpJ0q8C9z9hjPln+d2RnCbp5XwL/fOvO+SZKg0fe9zh+76x\n6RqtmXeBdOIHJEn3Tz9b5x48MbwticjRR0t9fVNPN2eO1NGR//Hjj5c6O8Op6W1vk/buDWdZI449\nVuopY3inqiopkxl7nzGSlz3VUc72j5932jRpaKi0ZeV77hYulBobS1tmdbU0PDzx/qOOkvr7/dsz\nZ0rd3bnnnzFD6u2VzjpLevPNiY8X+ty9613S5s25lz1i9mzpn//5Lt18811jpjvnHGnjxsmXn++5\nO+EEad++yec94ghpcHDyaZYulVat8veZ446Turr8+yfb/pGa3vUuaf9+ac+esdMX+tydf7702muT\n1xnc/uBzfdpp0o4do9uwcuXEeU86SWprm1jT9OnSwYOTzzv+/uD7aqTeiy+WXn/dP1YFn7v3vEdq\napLa2/2/Tz5Zam72by9eLNXX+8tvaJB27fIfnzVL2rrV397580f3iwsukF591d/fzztPeuUV//Z7\n3+s/d+NrDd4+/XRp2zb/9pIlUm2tP++8eX59c+dKZ56Ze97g7SOPlAYG/HkzmdHnId/0l14qrVkz\n+v485hjp0CH/9sjztHSpv40dHf57sL3d/3fqqf60I8eFU0+Vdu70b595prRliz9ve7u0aZN/+8AB\n/3U4+mj/uX8pOyrFhReOPl/veY8/TXW1v5xNm/z/5871637kkbv0yCN3Tdie6mr/NWlp8W8fd5z0\n9rf7+9555/nTHH209P73T/08Bv8Ovu+WLpXWrfOPr+ef7+/jPT3+7dra0c+44Lzbt/uv4dKl/v5U\nWyudcop/7Nm2zd+HTjxRqqnx5z3vPH/7L73U3/dfeUW66CL/M2TtWn9dM2f6yz/3XH87V670X5u3\nvc2//Rd/4e+rdXV+3UuW+PeffLL/2Ehtkn979mz/eV+50q/rwgv9983LL0sf+MDodMHtGrlvZHtX\nr/b3ueBj429v2OC/Jscf7z93bW3+/Tt2SLt3Tz7v8LD05z/7tzs7/WUtXeo/Ry+/LP3VX/nTr1kj\nve99/rFj/XrpHe/wn6M33/SPMyPP0Rln+O+vlSv9aRYt8m+fcop/zGpr8/fdkWNnfb3/Xh5fW3Cf\nqqqSPvjB/NsQtHat/xwvXeq/xr29k29/UG3t6LGpqcnfx6aad80av87hYX8fvugi/3ixatXYferd\n7/Y/s7Zt84918+f7x77qav+YGLRypf/46af7t084wZ//jjvu0n/8j3fp/PP9z/jXX/fXN35eaeJ7\nsJDtn2yacuZdvdrfj6qqpBdf9F/vo4/2j9/vfKf/fARt3uxv89y5/j589NH+/tPQ4B8j3vEO/3jU\n2envb0Gdnf7reO65/j5cUyP95V/6861bJ11ySeW3v5B5g9au9Y8VRx7p78Pvepd//HjjDf/9NHt2\n/nnr6vz/3/52/zO3p8c/Nu3dK7W2SmefPfm68yl2G9Lorrvu0l133WW7DAB58B4F4suYSHJtSRGH\n28aYJyUtlXSCMaZB0j9I+o6knxlj/lZSvaTrJMnzvM3GmGckbZY0KOnvPc8rqoV2tZmmYC8nGXo8\nAcYYGLBdAQDAhslOjgMAAABAUkUabnue95k8D/11nun/UdI/lrq+ajNN8kab9hJtA2Pla9ldqJFW\npQDSqZCrjAAAAAAAqJQ4DCgZGr/l9iiPeBsI1UgXKFN573uXRloHgNLx/gTi7ZJLllZsXSNdXsE9\nPT0Tu5KT/K51amsrX48rltJXCxBrvEeBdHIw3KblNmAb4RmCpuoLHZXF+xOIt0qG21yN4a7h4dyN\nEoaH/fFDUBqCMyDeeI8C6VSJASUrxu+WZPTvDOk2gIQqZ/DVuGlq8gcYAwAA8dLc7A+WCwBA0rz9\n7W9XfX297TIwzuLFi1U3Mvp1hTgVbleZagVbbtN2G0BSHTpU3PTFDb8LhI8W+gCQPG+9RbgNAEim\n+vp6efwQjh1jTMXX6VRbOr9bktEdO5N/UsA55Q4WWareXjvrLRWffUA01q61XUFybN1quwKkWWOj\n7QoAAACA8LgXbgeSKzIsAEEtLbYrAOCKfftKn5djEWxqbw9nOVytgaBXXx29XegA5AAAAGFwL9we\n03KbeBvAqAyXcyAmhodtV5BeQ0Ojt8u5kqOmpvxagCRzaWwIlC+4P6xbZ68OAACQPu6F24FfqgeL\n7LM27gjmUK6BAdsVoFB799quAHDTG2+M3t6wwV4dAAAAAJDPrbfeqm9/+9u2y0gEpwaU9FtujzaH\n82i5HStxbuHT2Wm7gsro6LBdAZKAfskBAAAAACjdkiVL9NBDD+myyy4raf4HH3ww5Irc5V7L7cAw\nkuQzAMJiYcDfWCDoBgAAI1580XYFAAAk3zD9VIbKvXA7EMTQiwcAAAgLJ3sApF1fn+0KAACIv+XL\nl6uhoUFXXXWVZs2ape9+97uqqqrSww8/rMWLF+vyyy+XJF133XWaP3++Zs+eraVLl2rz5s2Hl3HL\nLbfozjvvlCStWrVKCxcu1P3336+5c+dqwYIFeuSRR2xsWiwlNtyu7uvVUR17xtxXZao1NtLmVyiA\neJksHCukdXgaw7UwtjntfZincb8JQ1qv2AAAAABQuscee0yLFi3Sb37zGx04cEDXXXedJGn16tXa\nunWrfv/730uSPvaxj2nnzp1qa2vTeeedp+uvvz7vMltaWtTd3a3m5mb96Ec/0he+8AV1dXVVZHvi\nLrF9blf1H9K0rn1j7vO7JRn9Be9ay+2pwom0/whP+/an3eBgtMvv7Y12+YVau9Z2BcnEgLzxtmOH\nVF1tuwrEXUODtGiR7SqA+OK7MAAAo1payr/i6OijpXnzSp/fCwR5xhjdfffdOuaYYw7fd/PNNx++\nfeedd+p73/ueuru7NXPmzAnLOvLII/WNb3xDVVVV+uhHP6pjjz1W27Zt0/ve977SC3REYsPtXPxu\nSUZ3HBqqlS7trRyB8erqpPnzbVchDQ3ZWS8tfxGlri5pzhzbVSDudu1Kfri9caN0zjm2qyhdWgbg\nBgAAyVdOKB2VU0455fDtTCajr3/96/r5z3+u9vZ2GWNkjFF7e3vOcPuEE05QVdVoBxzTp09XT09P\nReqOu8R2S5LL+JbbHvE2LMh3YqCxsbJ1AC5gnA1grIMHozvJxUms6HV02K6gPP394S2rrW309u7d\n4S03KWhlDQCA20yOD/vgfU8++aSeffZZPf/88+rs7FRdXZ08zxvT2huFcS/cpuU2AMRKXV158xMA\nAKN27vRbugPAVJLy27i723YFAACEb968edq1a5ck5Qytu7u7ddRRR2n27Nnq7e3VHXfckTMQx9Tc\nC7dpuQ0A1sTphzStvgEAiL/XXrNdAQAA4bv99tt1zz33aM6cOfrFL34xIbhevny5Fi1apAULFujs\ns8/WxRdfXNTyCcJHudfndmAYyRhlLAAAAEAq8dsLAACkzbJly7Rs2bLDf3/lK18Z8/iMGTP0y1/+\ncsx9N9xww+HbP/7xjw/fvvTSS9XQ0DBm2pFW4XCx5XYg0c7knxQAUKJyQoq+PmnbtvBqCcPAgO0K\nMF4x46Ls3x9dHcgvTldp2FDs9hPuohQZfswAAABMyalwu0rVGhtpp/yXFwBU2J49kz8+PBy/UKym\nJvp1xG2b466YweX6+sJZ58DAaFc2AwPS4ODEaZIcUFZ6H0z7Pp/27Uc4tm8fO/BmGvDeAQAAxXIq\n3B7f5zaNHQAAiFcoG9fgYtcuqb3dv93YKLW0xLdWAOmQyaTvOLRuXfq2GQAAlCfR4bYZ1zLb75aE\nb0MAcuPwAACAW/bts11Beq1fH/4yBwbidUIWAADEX2LDbTM0KDM8NOa+iS23SbIAjNq+3XYFKBQn\nIirn0CHbFbglk6EfcCBqwc+InTvt1ZF2XV22KwAAAEhwuJ0r+RgfbpONAIA9DISVDM3NtisoTVxP\ngAwMxG/QVKBcHR22KxjrjTdsVwAAAIC4SG64ncP4bkk84m0AeYQ1CB3ya2y0XQEQrrgG6ghHZ6ft\nCuJr48app+H9AQAAABvcC7cDw0jyHRsA4IqGBtsVoBLq621XkF6Dg7YrSK5Dhwi3k4TXCgAAuMTB\ncHtU2r630cdneIaGpp4GkBj0qBLy/Qivqyt9mVN1mTI8XPqyo1LOMb7UICPtXcvYeH/X1pY3fxz3\nXbgv7ccK2HPggN8dFAAAaXfLLbfozjvvtF2GFU6F21WmWvKCLbfTFm8DuU0VbBHQIomC4Xax+/Da\ntaGWUhE2Wtpt2FD5daI8TU22KwCSo7fXdgUoV2MjA1sCAOJpyZIlev7558taxqOPPqpLLrkkpIpK\nMzg4qE996lNasmSJqqqqtHr1aqv15OJUuD1+QEkakaQLLdcRtaS0iCw2BE1jgJmU1xIoBsE2UJzX\nX7ddAQAAQH6e58nEoDXiJZdcoieeeELz58+3XUpO7oXbdCIHICJtbbYrKEyxLZhsDaI2NCTV1NhZ\nN+Ci7dttVwAAhWF8AQCAy5YvX66GhgZdffXVmjVrlu677z699NJL+sAHPqDZs2fr3HPP1apVqw5P\n/8gjj+jUU0/VrFmzdOqpp+qpp57S1q1bdeutt2rdunWaOXOm5syZM+V6Ozo6dNVVV2nWrFm66KKL\nVJvt8/CLX/yivvrVr46Z9hOf+IQeeOABSX4r8+985zs666yzdMIJJ+izn/2sBrL9fh1xxBH68pe/\nrIsvvlhVVfGMkadNPUlyTGy5TdANAHGVyYw9H8m5SffFsV/epHdL0NFhuwIAKF5trbR4se0qAACu\nMneH29rZ+4fifqw+9thj+vOf/6yHH35YH/rQh9Tc3KxzzjlHTzzxhK688ko999xzuvbaa7Vt2zYd\nc8wxuu222/Taa6/ptNNOU2trqzo6OnTGGWfoBz/4gR566KGCuwJ5+umn9bvf/U7nnnuuli9frhUr\nVujJJ5/UTTfdpGuuuUb33XefJGnfvn167rnn9NBDDx2e98knn9Qf//hHTZ8+XVdddZW+9a1v6Zvf\n/GZR221LPCP3QuRolj8+3CYnARAFQthobNliu4Jwjewn5YSncQyDyxHH7envt11BeRgAGSjOnj22\nK6iMGFzBXHHt7f4AkwAAxIWX/VH4+OOP6+Mf/7iuvPJKSdLll1+uCy64QL/97W8lSdXV1aqpqVFf\nX5/mzp2rM888s6T1XXPNNTr//PNVVVWl66+/XhuyfZBeeOGFOu644/Tcc89Jkn76059q6dKlOvHE\nEw/P+6UvfUknn3yyjj/+eK1YsUJPPfVUydtdackNt3OkS+O7JWFASSA+Cun6Yu/e6OsIAwMXRcNW\n8Bn1yYpSlj8yTxIHvgSAONu2zXYFiMr+/VJ3t+0qAACYqL6+Xs8884zmzJmjOXPmaPbs2Vq7dq32\n7Nmj6dOn6+mnn9aDDz6o+fPn6+qrr9a2Er+wzJs37/Dt6dOnq6en5/Dfy5cv1+OPPy7JD9tvvPHG\nMfOecsoph28vXrxYzc3NJdVgg4PdkoymI65F24F9EkichgbbFSAKnhfP1riuYOBLAHHClUujeC4A\nAIivYrsRiUJwIMiFCxdq+fLl+uEPf5hz2iuuuEJXXHGF+vv7tWLFCn3+85/XqlWrQh1M8oYbbtC7\n3/1ubdy4UVu3btUnP/nJMY83NjYevl1fX6+TTz45tHVHLbktt/N2SzLK/q5cmL6+wqYj5AAqj8tb\nJ9fQEH6fv5U+1qXx0m2gWEm5sgbxlqAGQFNiQMTocOIAAOCCefPmadeuXZL8YPnZZ5/VH/7wB2Uy\nGfX19WnVqlVqbm5WW1ubfv3rX+vgwYM64ogjdOyxxx4euHHu3LnavXu3BgcHy65nwYIFuuCCC3Tj\njTfq2muv1VFHHTXm8e9///tqampSR0eH7r33Xn36058+/NjAwID6suFlf3+/+mPWt2Nyw+0cqk21\n5AVbbvPNCEB5YnbMBiZFIBCttPTV6yquMrHvrbdsVxAe9icAADCZ22+/Xffcc4/mzJmjZ555Rr/6\n1a9077336m1ve5sWL16s++67T5lMRplMRvfff78WLFigE088UatXr9aDDz4oSbrssst01llnad68\neTrppJMmXV8hrbxvuukmbdq0ScuXL5/w2Gc+8xl9+MMf1mmnnaZ3vvOdWrFixeHHTj/9dM2YMUPN\nzc36yEc+ounTp6shRpfnJ7dbknx9bgcCbb5zAgDShLAFyG/NGumDH7RdRbxw5QqSaGhIqq5m/wUA\nxNuyZcu0bNmyMfetXLky57T57j/iiCP07LPPFrS+hx9+eMzfl1566YQAetGiRVq4cKE+mONL8YUX\nXqivfe1rOZddW1tbUA22JLfldr5uSWi2BgAAKqivj3EFkoCTP4AbamroNg4AgGINDg7qgQce0Oc+\n9znbpYQuueF2DhNbbhN0uyTXj1LOZQCIq5FzsByn3DcwILW3264CQBzt3j31NHxOxA+vCQAgjs4+\n+2zNmjXr8L+ZM2dq1qxZeuqppyadb+vWrZo9e7ZaW1t12223TXg8zIErbUhutyQ5jA+3+U4CAAjb\n5s22KwAA2LBvX/Hz7NghnXLKxPtHwtNMRjp4MPe8BKx2DA5Kb7whXXCB7UoAABhr06ZNJc13xhln\nqKenJ+/jIwNfJpV7Lbe9YLjNN0IA8TI0lP+xhJ8sTY3ubtsVpAvhTnS2b7ddQenYL2DD3r3hLKe/\n3w9QER8DA1J9/ejfDCgOAEByuBdu03IbAAAND9uuAGELO9Dt7Q13eUgvTjYUZ7Lni+eyskae78FB\nqa3Nbi0AAKA0ToXbVarWmD63+XYIRxBSAQCkqa/w4KsPABSms1N6803bVQAAgHI5FW4bY2TolgQA\ncqqrs11BdA4csF3BRISM4eM5LR8nSxF3vM9RKZ7HMREAABc4FW5Lkgls0rAyFisBgHhJUrhdbLgx\nWV/mKExcf+CnoS/6Su6/lRwQNQ2v3WTSvv1AHO3bJ+3cabsKAAAQJufC7arAL4mMR7gNYGqdnbYr\nwHivvVbc9LT0Q5xN1bf1mjWVqWMyGzcywB2QRHz+FWdoyB88EgCAuLv11lv17W9/23YZiTDNdgFh\nq6LlNgCgCEkKBpJUK0a98kq4y4tiP+jtlTJ8bQISY+Q4sH27dPLJdmsBAAATLVmyRA899JAuu+yy\nkuZ/8MEHQ67IXe613Fag5TbhNgAARYtrFyVhcfkkQaW6wujqqsx6wtLWZrsCpF1fX3jdg011NQgA\nAIi3Ydd/cFWY2+E23ZIAgFPq68NfJv3iTi6Og3UWoqHB7RB7hK1tbGmxs95SudD91N69tivIj+Po\n1AYGpI6OcJY1fl9Iw7EOAIAkWb58uRoaGnTVVVdp1qxZ+u53v6uqqio9/PDDWrx4sS6//HJJ0nXX\nXaf58+dr9uzZWrp0qTYHBsi55ZZbdOedd0qSVq1apYULF+r+++/X3LlztWDBAj3yyCM2Ni2W3Au3\ng31ui2+MUUvyAAAgAElEQVR6AOCS2lrbFSAp6uvT0c3Gli22K0ClvPmm7QqA6CX1RMnu3aO3OdkA\nAHjssce0aNEi/eY3v9GBAwd03XXXSZJWr16trVu36ve//70k6WMf+5h27typtrY2nXfeebr++uvz\nLrOlpUXd3d1qbm7Wj370I33hC19QV9Iup4yI031u03IbAAAgXt58UzrrLNtVAEB4duyQTjnFv712\nrfRXf2W3HgCA/EsN+/rKW8bRR0vz5pU8uxc442mM0d13361jjjnm8H0333zz4dt33nmnvve976m7\nu1szZ86csKwjjzxS3/jGN1RVVaWPfvSjOvbYY7Vt2za9733vK7k+VzgXbhv63AaAVKPFFBBvce5e\nAwDKNTRkuwIAgKSyQumonDJyJlRSJpPR17/+df385z9Xe3u7jDEyxqi9vT1nuH3CCSeoqmq0Qe/0\n6dPV09NTkbrjzsFuSUY3aZhuSQAgtqK69DjOA21FFbxPtdyDB6NZL5KNE0FA+gQ/e7dvt1dHFCpx\nTOvujn4dAAA3mBw/eIP3Pfnkk3r22Wf1/PPPq7OzU3V1dfI8b0xrbxTGvXA70HLbo+U2AABqaLBd\nQeWk4btgWCeGurqS93zRIhIIT1OT7QqS57XXbFcAAEiKefPmadeuXZKUM7Tu7u7WUUcdpdmzZ6u3\nt1d33HFHzkAcU3Mw3A72uZ2wX2yAA/bvt10BkD58B0JarFsnDQ/brgIAAACY3O2336577rlHc+bM\n0S9+8YsJwfXy5cu1aNEiLViwQGeffbYuvvjiopZPED7KuT63x4TbtNyG4/iBj6Th8xdAOTyP4whQ\njrY22xUAAJAOy5Yt07Jlyw7//ZWvfGXM4zNmzNAvf/nLMffdcMMNh2//+Mc/Pnz70ksvVcO4y3FH\nWoXDwZbb1SYYbtNyG24L6/JsWltjKpW4EKacwIqwC0DUuCAQlVDu59lU+6lr/Wzn09rq/wMAAO5z\nsOX26DdCuiUBELU0harlHFI5HAOAuzjGI24OHbJdAQAAqBTnWm5XGbolAYAkIAwpDs8XCsF+AkSH\n99congsAABAX7oXb3mgzSo9uSQBJ/ACJA14DJAH7afzU14/9m9cIQBzU1NiuwB0tLbYrAAAg2dwL\nt+lzG5iASzPte/31aJff0RHt8sMK1OIYzMWxJiRLlIP7BvuMNYaBhIG0iWv3Z4ODtisoXdw+97du\ntV0BAADJ5l64LcJtAOkTVeA18gPwpZeiWT4KF9eAI0q2g9ymJrvrT6KhIam/33YVABC99nbbFQAA\nAMnBcLvaVB++nfHocxsAULikBshxa4UWlqivSJjKwIDd9SfR3r1SXZ3tKgAgeps22a4AAABIDobb\nVaLPbcA1tgOuybgaKqZJUgPttBh5j9GKOjxxPm7RjRYqieN/cTZutF2B+w4csF0BAADJ42C4HWi5\nTbgNOCHOQcz+/bYrAOypZBBpu4uSOKjUsbCz0+9epFyl1EsXSEhr4Dzyfonzd57e3nCW09kZznJc\nFPUYLQAAd91yyy268847bZdhhXPhdnVgQElabgMAihHnUKEStRUbKr36ajR1hCFDz2Ql27lT6uuz\nXUXxamoKmy7O73MkU6mDK6Z1X+QKDQBAGixZskTPP/98Wct49NFHdckll4RUUWkGBwf1qU99SkuW\nLFFVVZVWr15ttZ5cnAu3GVASAJIhqtZ5aQ0LgLiqVEvcffvyP8ZxAVFqbbVdAQAAcJHneTIxuKzt\nkksu0RNPPKH58+fbLiUn98JtWm4DQGhi8Dkaa+O7yqA1mhuS3AVKnN6zlaiFFvruKedERJz2fwAA\nkF7Lly9XQ0ODrr76as2aNUv33XefXnrpJX3gAx/Q7Nmzde6552rVqlWHp3/kkUd06qmnatasWTr1\n1FP11FNPaevWrbr11lu1bt06zZw5U3PmzJlyvR0dHbrqqqs0a9YsXXTRRaqtrZUkffGLX9RXv/rV\nMdN+4hOf0AMPPCDJb2X+ne98R2eddZZOOOEEffazn9XAwIAk6YgjjtCXv/xlXXzxxaqqimeMPM12\nAWGrpuU2AMROJQOH9vbKrWvtWqmYz/e+PungwejqQfjq66UlS3I/lsSuO1yT5BMRccNzCQAAXGFW\nrgx1ed7SpUVN/9hjj+nPf/6zHn74YX3oQx9Sc3OzzjnnHD3xxBO68sor9dxzz+naa6/Vtm3bdMwx\nx+i2227Ta6+9ptNOO02tra3q6OjQGWecoR/84Ad66KGHCu4K5Omnn9bvfvc7nXvuuVq+fLlWrFih\nJ598UjfddJOuueYa3XfffZKkffv26bnnntNDDz10eN4nn3xSf/zjHzV9+nRdddVV+ta3vqVvfvOb\nRW23LfGM3MtAn9sACtHfb7uCieLe4iyOz5lthbQwDE7T3i51d0dXT1LEfV8v1I4due9PWxcYLS1S\nc7PtKlCunTttVwDXvflm+Mvcuzf8ZSZJ2j5vACBpvOyB+vHHH9fHP/5xXXnllZKkyy+/XBdccIF+\n+9vfSpKqq6tVU1Ojvr4+zZ07V2eeeWZJ67vmmmt0/vnnq6qqStdff702bNggSbrwwgt13HHH6bnn\nnpMk/fSnP9XSpUt14oknHp73S1/6kk4++WQdf/zxWrFihZ566qmSt7vSnAu3q1R9+HaGD3sAAFBB\nrgT3xejv5+RX2qVxvy/FgQO2K7AnkyGIjsKrr3LVBQAkQX19vZ555hnNmTNHc+bM0ezZs7V27Vrt\n2bNH06dP19NPP60HH3xQ8+fP19VXX61t27aVtJ558+Ydvj19+nT19PQc/nv58uV6/PHHJflh+403\n3jhm3lNOOeXw7cWLF6s5Qa1X3OuWhJbbAIAY2bRJmjHDdhXJRIs0ACP27JFiOoYRCtDYmPv+wcHi\nlzU+zE3zZ0V/f7q3HwDyKbYbkSgEB4JcuHChli9frh/+8Ic5p73iiit0xRVXqL+/XytWrNDnP/95\nrVq1KtTBJG+44Qa9+93v1saNG7V161Z98pOfHPN4Y+DDur6+XieffHJo646acy23q4Mtty3WAQAY\nleYfXtlxOFInza95nLnwukSxDevXh7/MNKnEflViA6ZUSWIL+lJaswcaoQEAEFvz5s3Trl27JPnB\n8rPPPqs//OEPymQy6uvr06pVq9Tc3Ky2tjb9+te/1sGDB3XEEUfo2GOPPTxw49y5c7V7924NlnI2\neJwFCxboggsu0I033qhrr71WRx111JjHv//976upqUkdHR2699579elPf/rwYwMDA+rLDjjU39+v\n/phdtuleuE3LbQCIrST+8I6jJASUGc4wO2f7dtsVhKujY/R2V5e9OgDkVspnXVK+ZyThcxwAUJ7b\nb79d99xzj+bMmaNnnnlGv/rVr3TvvffqbW97mxYvXqz77rtPmUxGmUxG999/vxYsWKATTzxRq1ev\n1oMPPihJuuyyy3TWWWdp3rx5OumkkyZdXyGtvG+66SZt2rRJy5cvn/DYZz7zGX34wx/Waaedpne+\n851asWLF4cdOP/10zZgxQ83NzfrIRz6i6dOnq6GhochnJDrOdUtSZUZbbhNuA0iSOP7QiWNNYUvD\nNsKOEBpYFC24P4fdv61rVyF0d0vTSvwmPDQkjWvsAiBkr7wivfe90pFH2q4kHMPDUnX2p+qLL0oX\nXWS3HgBAtJYtW6Zly5aNuW/lypU5p813/xFHHKFnn322oPU9/PDDY/6+9NJLJwTQixYt0sKFC/XB\nD35wwvwXXnihvva1r+Vcdm1tbUE12OJcy+0qBVtuAwAAVM7QkO0KRrkWRsfJq6/arsC+pLSQjaNi\nnrtyTsAm/eRtnI6nYVizZvR2FFdz9/Rw1RQAIL/BwUE98MAD+tznPme7lNA5F25Xm2Cf2wn/RgcA\nCE1LSzjLIdABwjM0JL32mu0qUElJD1yBUkW972/eLGW7QwUAOOrss8/WrFmzDv+bOXOmZs2apaee\nemrS+bZu3arZs2ertbVVt91224THwxy40gbnuiWppuU2gIRqbbVdgdu2bs19f8I/x4FE8zzCmLTp\n6pKOP952FUir4Gd+TY103HH2agEAoFibNm0qab4zzjhDPZOMyDwy8GVSOd1ymz63AQCIH9cuNa80\nWr7mN8l3dgAYgy48AABwg3vhtoLhNgAAcAGBbmUl9fneuXPyx6Po5xbhSOo+h2Rpbs7/GPsgAADJ\n5F64bQi3AQAAcgmzG544BMXF1tDWFk0dYSBYSy5eu+To7LRdQXz09EgHD9quAgCA8rnX5zbhNgAA\nCDh0SJoxw3YVxYt7YNbebrsC6dVXk/naAoViXAg74nz8Xb9eOvfc8pfT1iZNmyYtWlT+sgDAhsWL\nFyd+IEQXLV68uOLrdC/cFn1uA0ClDAzYrsDHdxpMpqZGev/7bVdRvOFh2xUAwKg4B75p0tVluwIA\niIe6ujrbJSAmnOuWpCqwSXz/AoBocTkrgLTgJFr4wn5O0x6+pmn7bW9rba3d9QMAgFHOhdt0SwIA\nQDLELSy0HZbYELfXoBJGtrm7u/B5Mpnw1l/MesPS1CQNDVV+vVNJ43sunzS+F5Osvt52BeHKZLha\nCACQXM6F29MItwEAQJH4UZ8+hfYZ3tMTbrj92mvhLatQu3dLg4OVXy+AZGhupjU6ACC5nAu3x/a5\nDQAIW0+P7Qrgoji0WoxDDQCQS0tL8fNwTAsXVxoAABBP7oXbwZbbfKEDgNBV+tL2tPyY3LnTdgVw\nAWFWsqTl+Oa6Srzvtm6NRx1p9sILtiso38AAVyoBANzjYLg9zXYJAIAEIVyCi8LsRiNKNkIW3vOI\nO0LqeHKha59du6S9e21XAQBAuJwLt6fRLQkAAEi5xsbS5x0JnOMaAhP8AUiyuB5bAQBIKufC7WoG\nlATgmB07bFeAyXB5L+IkDaFJGrYRCAPvlXjiex0AAOEi3AYAAM5LUsgz1aXvSWm5vH+/7QoAJFGS\njtelcKF7EwAA4sTpcBsAgKnYCgoHBqSentyP7d498b6k/dgvp1uMtHv11XCWY2OfCV7JMDBgrw4k\nW1JO4AAAAMA+58LtaRodUNIT34wBIK46O21XYNf+/flbtgZD76QGg3V15S9j8+byl5FkcXjtkzIw\n5Yg4PGcAkocTKqXzvLGDVNbU2KsFAJBOzoXbdEsCAABQPs+T3njDdhXxt3mz1N1tuwrYxEkVTCWK\nq5kGBqShodG/be2Hnidt2TL69759duoAAKSXc+H2NDPacptT8AAQDyPdEwCAawYGGFi2GATBiLso\nfkJGEfgGg+1SFfN+PHhQ2r69/HUCABA258LtahPslgQAEAdNTbYrAIBwcdIOqBxOivgqMRhlR4e0\nc+fE+4eGuEoFABBPzoXb0xhQEgBiK+39bJeDi5EQhWBgRHhUnDBaTRZr48bKrxMIW7GfZ0ND4Q20\nm3S9vdGvY2hI6u+Pfj0AAITFuXB7bMttkgAAAIBSHDxou4LiJe0kULH1dnREUwcQd4StAAAgH+fC\n7WkKtNxO2A8cAEB6hBnCJTGERHEqfSm451W+H+lMprLri5ukBfOFcuGKALqgmdxk711aXAMAgKg5\nF27TchsAkDZ79tiuAIDkD7ZWzsBxLgTBLnrhBdsVxNvmzfkf6+mpXB2onOFhqaXFdhUAAPicC7eP\nCITbNN0GAEwlqWFSEutOYs2usfUapGUQssHB0lu8HzggbdpU+rp37y593kp6443c97vacj0NOLYn\nSxiv1+CgVFdX/nIAAAiDc+F2NQNKAgBCtmuX7Qr8H6P19aN/v/yyvVqAYlW6i5MwVSq4y2TKe552\n7Mj/GOHj1NraipueMB5J19BguwJf2rukAgCUz1q4bYz5T8aYTcaYjcaYJ4wxRxpjZhtj/mCM2WaM\n+b0x5rhilzst2C0J3zoBAFMo5JLp5ubo65hKJpP8weTGfyynOXBL87Yjfl56yXYF9tXWRrt83vNI\nu3wnkNaurWwdAAD3WAm3jTEnS/qSpPM8zztH0jRJfyPpdkl/8jzvdEnPS7qj2GVPo1sSAAAmNTho\nu4LKimOo5HKYGMfn24bOTtsVFO7QoeKm7+2Npg4A0bLZT3a+vtmTfGUPACAebHZLUi1phjFmmqRj\nJDVJ+oSkR7OPPyrpk8UulHAbAFCqgQHbFVQGraTs6+uzXYHb4vBeHgmRXAz749KdAZAGAwPxH7fA\nxeMcACA5rITbnuc1S/pvkhrkh9pdnuf9SdJcz/Nas9O0SDqp2GVPM0eEWSoAAECixT0UiUJUJw/C\nDnAOHZL27Qt3mYCU3D7BK113EkLZrq70nFDieAgAKMW0qScJnzHmePmttBdL6pL0M2PM9ZLGf73I\n+3Xjf/7y+6ru61XfhpV673uX6r3vXSpJqlZgQMmkfqsDAAAoQyYjVWe/ErW0SMcVPYpJMkUZVEWx\n7O5uqbU1/OUWopDxBlAYfnKgVMUOZOq6mhpp6VLbVQAAwrBy5UqtXLmyIuuyEm5L+mtJuzzP65Ak\nY8y/SbpYUqsxZq7nea3GmHmS8n7cf+6TX9CRnW3qzIbaI6pNINyWked5MnzjBACkxNCQ7QoKs2OH\n7QrcVl8vnXaa7SqSqbdXmjFj4v3r10vnnDP5vEnqOzaTsV0BgAMHyl9GElqfAwDSZ+nSpVoaOGN5\n9913R7YuW31uN0h6vzHmaOMnz5dL2izp15Juzk5zk6RfFbvgahPYJFOljBL0KwMAgDLt2WO7gsJ0\ndNiuAElUifYKjY2570/bQKxhiUMbkzffjHb5hItIm6jfUwAAFMNKy23P8142xvxc0npJg9n//1XS\nTEnPGGP+VlK9pOuKXbYZN4jksDesamOrgToAAAhb2oOkQrc/DqFiPk1NtitAmuzda7sCoLJ27pTm\nzct9FUoYurulo4+OZtkAABTLWurred7dksa3Se+Q32VJycb+jqvSsDck6ahyFgkAABIi7cF3UJyf\ni+Zm2xWkz86d0tln264CcX5fwh0HDkgnnhjuMltawl0eAABhsdUtSdkGZ52Q8/4xG2RMNtwGAMRd\nkvqqBYAodXcXNl0xLZLb26WDB0urB+WL85UUQCHieFKSk0UAACnB4fbw9Jl5HjFjbhNuA0AyDAzY\nriC5wgpNCvmRGNWAlfxABYpXbL+3DCIZPy6E3lEfv/l8QD4vvGC7AgBAHCQ23M5nbMvtKsJtAKmW\nltbQfX3RLr+UH9ZJDOsLCb7WrIm+DhcU2vI2DaI6IeKa4WFCPADlifpkSdyOUQz0CwCQHAy3xw8o\nOeTxiQcgvfr7bVdQGV1d0S6/lNaOUQfuiLe4vvdstBIttnUxKoNW3OWJW8gHAACQVs6F25Ikb/Tb\n+hAttwEAZSLEQNokdZ8PO7wvpk/rpNmwwXYFANJo1670XFkIAKgMN8PtAMJtAEi2bdtsV+C2pIaY\nhXBp21zaliTZs8d2BcV58UXbFQBIm2JPKu7ZI738cjS1AADSyc1wO/ALcNjjtDAAJNnBg7YriEZn\np+0KfHSfArjDxvt5587Kr9MlnLhCGozvqiuuXXcBAJLJzXBbo98SabkNAEiTOAclca4NQGkaG21X\nYIeN/uvjiucinYr5TF+3Lro6RmzdGv06AADx5GS4bRRsuU24DQBIj/Z22xXEB4ELSsFJmKm1thY+\nLc8nkAxJf6+2tNiuAABgi5PhdhAttwEAANIhjuEMJ1kAlCOOxzUAAOLEzXDbo1sSAEB68MMXyK/Y\n9wfvp7EyGdsVuIH9CiM44VW6Awf8fwAABLkZbtPnNgAgRgg1gHCU8l7at6+46dvawlt33BUSsjU1\nRV9Hod56y3YFAGzq6PD/hWHXLgbVBgBXJDfcnuTbOH1uAwDipLMz3OWVErJt3hxuDXALLQkRpr4+\nacOG0ufv6cl9f3Nz6cuEXXE9OcSxL3nq6vz/i+n7P5f9+6XBwbLLAQDEQHLD7QINadh2CQAAWJev\nNWoSEUYA8dffX/q8xba2B1y0fr3tCuJtiDZsAIAsJ8Nt4wVbbhNuAwDcRtgLm4b5qjWpXC1WaS2Y\nXrt2VX6dW7aEv8zg505cW2UnXVeX7QrioaenvJNlAAD3ORluB9HnNgAASDtXwidXTuTU19uuIBrt\n7bYriL+wu6kqRLndNyA+pjoGunKsD2pujuZqDk4yAoA7nAy3g5/5tNwGAABIJrqnqLxyAuqGhvDq\nACrBxTAYhWEwSQBwh5PhtgIDSg6JltsAUCq6GwDip5zWy4UGOXFpIV1TY7uC9OG4nwyEsvHR2Ghn\nvXE5TidJJpP7/rfeohsYAEgyJ8PtsS23CbcBAChWFMFJX19lgzOXwh9CDLhuxw7bFaRbvtAvqSp5\n/K+trdy6ULqBAWn16tz7Rn8/A1QCQJI5H24PieYnAADkU8nQdPt2WoUCQKleeim6Za9dG92yw8AJ\nPoRlYMB2BQCAsDkZbgdPx9JyGwCA4hEkoFJcamEPROnQoamnKfX9FPWJRz5TkoXjMgAgSZwMtxlQ\nEgAAwD20/MdUkrqPECYiDKWeRODkAwAgyZwPt4cItwEAAIqS1KAtrgGNa/0ZAwAAAHHhZLgdlKHP\nbQBAgrW3T7wvrgEeopPUsLkQYe7PcX2e6upsVxCOYp7fSr4WhXTXMZmDB4ubvqOjvPUBSLakXiEC\nAK5yMtymWxIAgFR8YBFHra1TTxPXQC8ueH4QZy7tn4ODxU1/6NDYecoJqbdsKX3el18ubvq4DUhn\nYx9yab9FehWzHw8NjR6v1qzJv7y4HR8AIA2cD7fplgQA0mv/ftsVRCNJocKOHcXP09ISfh1IjmID\nUsRHf39xr19j49irU9raSl93Z2fp88bNhg22KygOVxMhDV54wT9mSWO/h/X2Srt3+7d7eqSamsrX\nBgBp53y4TcttAADsGfnBNxUu88eItWttV1C+JJ2AmkwwbK6ttVdHoQhZkcuePbYrgAvGj50w0jVJ\nX5+7jSkAICncD7fpcxsA4Li0BTquBIdwU9z2z2LrCU7f3z/6Ny3qkVTbttmuoHyufs43N9uuoHSl\nXJkGAIiG++G2N2StDgBAcsUtoELpXA0FkHyVGJRsfGvDtCKIik4lPi+Hh8MLqfl8j4+mJtsVAABc\n4Gi4Pfordlh8owcAAMkTZgCTxoC/0C5xbJvqtenvr0wdrkvK/oDcMhlp717bVeRGWA4AgF2Ohtuj\naLkNAABcQpBSGJcGGEy6KE6uDA9P7Ks/jPdGsctoaCh/nUlU7GtaymvDsQ6Se/uBa9sDAHGQgnCb\nltsAAJQjqYM9prG1ci78kIaLDh2Sdu0ae9/LL5e/3GL7Fh9fA4rDcRqVZvsz8aWX7K4fAFzkfrjN\ngJIAAJRl40bbFSCIgf2mZju8gB0DA7YrGEVf54VJy3s1LduJ3PbtG73d12evDgBwlaPhdqDPbVpu\nAwAcV4lB6RAfYQ2qhspobLRdAWxgAMvkIYD20Zo+fDU1/r9c+A4HAOVzM9wOfDHJeHxaAADc1tUV\nznLiOlhX0hCQRKu11XYFKNeBA7YrkHp7w1/m/v2lz9veHk4NSe1Gqhwcc1GuSuxD+b6rrVkT/boB\nwHVuhtvBltt0SwIASCkui4eL3nrLdgUoVxy61nnllfCXWc4gpnV1oZWReLmCxji3Jo5zbZLfP32c\nxP35KkVwny02KOfkDACUz8lwO7hRw7TcBgDERKV/wGzeXNn1AUA5yjlGljJvISFbkk4SuhgaonwM\nYAgAcJ2T4bYxwZbbCfpGCgCIvYMHS5+3uzu8OtLmwIH8LRtbWiaft9DQK26tp8IMquK2bUmR9uct\n7dsvcZIwyQj7UQqbx709e+ytGwCSzM1wO3CbltsAABRvaMh2BWNN1o1BsX2FpzmwI+wBipOr5XZ/\nf+XrKEUcj3WlfrZUelvi+NyhMnbtktra7Kx7qgGjt2yRenoqUwsAJImj4TYttwEAKEccBnxLuy1b\nbFeAUnESwW21teEsJ6wANUlBbGOj7Qrs4JiQHEND0nCM2scNDIy+x/v64lUbAMSFk+F2VSDczhBu\nAwAckKTwIp+4DWoVdx0do7cHB5PV92/SFRMeEDRMzvOk/fttV1GaKN5zURzLW1vt1xCWTEaqr7dd\nBRAf69f7oTYAID8nw+1gy+0M3ZIAABALk3UtUoympvyPBftELzbAiXPLuq1b091nezl93Zeiq6uy\n6ytG8KRHEmQy0s6dtqsoXPC4Uen9Dr7OTtsVAACAJHEy3A5uFN2SAACQfIUG1du3R1tHpcW5hWUl\nvfqq7Qrio6bG3rrjfAKoEop5P/LeTQ9eayTN4GC8T+ICQLGcDLfH9LntEW4DAJIv7aFSmlstAwCm\nVk7InKSrCxAPcTipsWtXafP19Eh1daGWAgBWORluj+1zm25JAADIZ+9e2xVEJ+0nBNIuDsGDa3hO\nkystVz+U2pXMvn3h1hG2tjbbFSRXHMerCKumhobCp/U8xogA4C4nw21abgMAXOPiYIwEZe6arF90\noBgcJ8LR02O7gvIVEgju3h19HUkX9nvKVt/0hZzA7u2VNmyIvpZirVlT+XW2tUlvvVX59QJAJSQ2\n3PZM/tLHttwm3AYAJF8cWx6Vq7fXdgXRiHOL8clCjTADD1dfW1Rea6vtCnz9/bYrgIufgy545ZXo\n11Hq56rnRb/flFJbGDXt2VP+MgDAFckNt488Sn3z3p7zMWNouQ0AgMvyXVobDGgLaaGUr0V82vpf\nHRqyXQEQb5ywiZdiT8a5Nthw2iSlO41KntyOe1c6AFBJiQ23J0PLbQAAolfoj80ouhUoJHwu5DL8\n/ftHbw8MlF7PeHHqSuHgQboJAeJo06ZwjzsuCiss5BiYbDt22K4AABBnTobb9LkNAACKdeBAeMsK\nu1/LcgKevr5wty1pMpl4nWxwRZy730mK3t7JuydIw36baz9ycbvD2KawuqHgvesmuu0BkGZOhtu0\n3AYAAPk0NES7fBeDmSSrrZVaWiq7TvYBXyYT3WC4leimIKrXMenhIq3N7ejosF1BcqSxj/y1a21X\nAIHd9tUAACAASURBVAD2EG4DAACned7Ybkzq6qJdH8GPfevXjwaIBM328NxP1NOT/BaWwdc1qpMX\nCFfa3ovr1tmuoPIq2S/5yy9Xbl0AUAg3w+1Ac4gM3ZIAAJB6ra1TT7N3b/R1ABgrDaHb+AFbo9rm\njRujWe5kwtqWKAZ8rNS+lYSW+C++aLsCxE2xA0m/9tro7YMHw60FAMrlZLhtAptFy20AAACkVVdX\nZVv0YaKtW21XUBldXaO3i22dPjgYbi3lSEJYnSu4z2Sktrbc06exm45ipe1qn+7uaKcHgEpyMtyu\nDg4oqZR8OgEAgKLFMcQotjUVki3qIOWtt+IVHKIywjq21dQUvo8Gp2tvD2f9hWhqqty6colLGDo4\n6I8xALeUs3/t21f5MScAwAYnw20juiUBAMAFnZ22KxirEpd2x6GV7Vtv2a6g8uISUOVreQlE5Y03\n8j821SCGcXjfHDjg/19uoE//4UiCYq4EOXhQ6u2NrhYAiAsnw+0qE+yWJAbfuAAAQEmi7tex2GCm\nry+aOuIm+GPYRj++adbYGN2y4xBEjhfHqycqZfzrUc5gtOUMlLt/f+nzvvJKcdNXch+Mw77F8dO+\n4WG3utTgJAwATORmuE3LbQCAw8oJQOC2YEv3Yvu8zWeqlptJUmywNjycrO2PQ5gXtZFWui4q5+RZ\nlIHX669Ht2zXJen44apDh6Rt22xXkVuhA6na7noHAOLO0XCbASUBAEA4ktQHdrAF5q5d9upIolzB\n8MAAfdjGTTktlF0yVV+6nif19ISzru7ueLb6d1VwYM4o8FrGR6Gh9e7d0dYRtWCjjIYGe3UAcJej\n4Xag5TbhNgDAAfwYtWfNGnvrHglcK/H6hxWEsa8iTsI8ORV2y/hClrd5c+776Us3HHEb10GqbE0c\nryGFd6VXPi+8MHqbE+8AouBouE2f2wAAYGpp6MahUGENItnaGs5yJlNIIENoM1Ea93ebJ6eASuBY\nh0LlO9m3bl30ATcARGma7QKiUGXocxsAACQfoQVsamsrvnWwy/usy9sGwH379uW+f3i4snUAQNic\nb7ntGcnjmygAAEDFMfjp1MJsTR3GV97goIZDQwS6LkrSOALsf6gU9jUASC4nw20z5leCod9tAAAQ\nqXytWyvRRUecBfvZRDIwYGM6VDLIO3CgcuuKu8FB2xWgHATgow4etF0BAIxyMtweu1FGw16CmicA\nAIDEyXdJ75Ytla0D4Th0KJzluHypNyFPsr34YvHzlPqad3eXNl+xkrBPxnkQ0HxdViTJ7t22K0iP\nsAahBoAwOBluGwVabhvCbQAAkNvevZVb18svFz8P3XrYsX17OMtZuzac5QBhS0rXJGkcBDUMxQb9\nQ0NjuySKg1LCdgJX9wQHuuTKBwD5OBpuj/2LcBsAANhWSnDgcstfGyodlGUc7hkvDaFjc3NxrY6T\n0HI4CXgeK6+21nYFE9XU2K4A41XqKoygNWtGb3PCGEA+hNsAAABwQvCHNwGZfUl/DfbvD6elYKHP\nQ9Kfr0Il6cRIqaGvyye2kF65rvg4dEjq749unbyXABTC0XA72C1JFeE2AACwjh9o0Qu2dO/osFND\nWP11u6CUfp0Rf6+8Upn1eJ79Qevq66X29uLnK/aqm3JObDQ2lj4vUK6mpsp28QYAuTgabo9FuA0A\nAOIs6cF3fX34y6x0K9ampnCWU1cXznJcMFW/zmlpqWxDlK2jK9Vf99CQtGlTZdY1XjCcnqqGMJ7r\nlpbS521rK3/9GCtJVxcAABwNt6touQ0AAJBo69ZVdn2dnZVdXy5Rh71xC5OTflIH5QmOQxC3fTOJ\nXA9ko95HXH/+MKqry07/4QCi42S4PZZRxmM0JgAAAFtddZRiYMB2BYXbsMF2BUDyhN1tzZ49Y/9O\n0jEEUyukdbuNkyRvvln5daI8+/b5YyoAcIeT4fbYjWJASQAAEL7eXtsVxBetMMu3bdvo7WL774V9\nQ0McI3JpaKjcusLqaijt4nBVixT950qp+wvH58LY7j+/rY3W2oDLnAy3xw4oSbgNAADCx8CBybNz\np+0KJhcMb/bsGW1ZtmaNnXpQuu5uaccO21WU5/XXR/dJTliVznaoN5lCxghIy5Up+/bZrsBttbWV\nX2dn5+gVHB0dnHAEXOZouD32L8JtAAAAxN34VmVbt/r/pylYTMq25uqft5KtkithfChLn8SwrZx9\nkP03fRoapJ4e21UAqATCbQAAAIclJSxEfHR12Vv3SEvS4GCDAOItqZ8zhdSd9JbrQ0Px7X9+cFDa\ntct2FQBc4Gi4HeyWpIpwGwAAAKmxb5+UyZQ+fxQDbRUbfoU92KAraIWIfJIaMMddIX2Ox/m537NH\namyMdh2lbv/QkLR3b2nztrSU9zkHwC2OhttjEW4DAADEU5xDgaTats0PDcZ7443Sl8nrFL5SntMo\nTjzAvnz7QktLZesoRlTHhMlaGXMcGhXHblY8r3Kv0Y4ddsJtz6PvbiCOnAy3q2i5DQAAAIyRK/AO\n2+Dg1NOkJaAqpMVnGqTl9Y7C+H74JxNF2LlpU/7HourqIuldEqVlf8+1nc3NUn19acvLZAob4NS2\n/n6ppsZ2FQDGczLcHi/jDdsuAQAAIHZ27oxmua5cKhzHlnFRCQ4eWGw4E2xN/Morxc3rchBUzrYl\nPeBraop3q2MUpr3ddgWolFyf25mMtGVL4csYHi798z+TkXbvLm3eqPX2Sps3264CwGScDLerxv1F\ny20AAICJOjpsV4CoFRqwltPKOKqTJGmWxD7Hg/taf398B7GzKSknc5JSZ7nSdAJzMpmMtHHjxPs9\nLx4nOGy3lB4eTv4JR8B1TobbYweUNITbAADAWXFt6VQowgUgXK5cOWFbMS1WwxRWsFxOGBeHQDMq\nhfQLndRwv7+/9HkredxoaChu+n37oqkDgDuSHW7n+dQx4/4i3AYAAIALbJ0MSMtJiKSGWkFr19qu\nwA2trbYrKJ3n2Qvng7Zvt13BKBfe21NJSj//hNUAwpbocPvo1tyjFUwIt0W4DQAAUK5yWoXlU8gA\nhC5LQ+AyngtBeZxft+GIhhuytc3FrjfYf3zUaCU/uaam/I+F9dxVYqBcxBNdqwEYkehwOx+6JQEA\nAAjfgQOjt/fssVdH3MWpxWeSguS6unCWw8Bf6fbyy5VbVxxaycf5REslFNvFRSGKPW4GB9XNJ+2v\nUxSiPtZnMowdACSFo+H22L8ItwEAAMLV02O7AkwmjO4ASg3GbQfqbW2lzzs05FYIZTOYKfV5HBiY\negC5+vrwjkGNjaXPG1Ur+bTJt59G8V60fXzCqLfesl3B5Do6wqkxuB9zpQEQDSfD7aoxLberCLcB\nAACyXAruXLB3b7KW67pNm8ZeoRBXhb6PX3gh2jqikMlMHVx3dY0NRLu7S18f7xX78nVRUm7fzFO9\nT15/3f+fkxSlK+dkQSEt3sNk6/tP8Di8Zo2dGgDXORluj0e4DQAAAMRPWK0oCwmnih1srZQgpJxW\nwGlWbt/7vb3h1BGWt94qrO9vTjbGQ1eX7QpQrCS9d9I+tghQCU6G21Xj/sp4nIoFAAAAXFVIuL1h\nQ/R10F1PaYo98RB33d2j+2SlQjhaH0+ulBNp69eHX0eUwtjXNmxIXz/T+/ZJ7e22qwBQDifDbQaU\nBAAAiK+4tbKEPdu22a7AF+wWIUktApE8UfT5TD++E4XxOZPGFt19ffm7iXFVT095XRsBsM/RcHvs\nX4TbAAAASKOwglobgW85A0MWq7+/9HnL6Rd4/GBlpbZgJpBPnt7ewoJuV1/bUrarmBMDO3cWv/xi\nVLq/aExu0ybbFQCwiXAbAAAAqdTQUPq8SRh0sFy2Q7U4tfAf/1wEQ7ZgIF3sc+ZadxxhiKJlc1BP\nj7R7d7TrKMT27cXPE/Vzk0RRHKcKacWb74RY8LiVxpbftkTdrUhwP7P92QhgIkfDbbolAQAAwETB\nH6XltNaNS3caccQPf5RjshMJYejri8dJhclqSNt7KG7b29QUznKS1md3qSp5lc2Ico8Le/eGU0fU\namttVwAkg6Ph9ti/CLcBAADSKazQJE19kMYtaEoLTphA8oPCHTvsrd/11uFJPb7FeZDHJPZXnZSa\n6+tHb4d10gVwkZPhdtWYlttVhNsAACDRxv8Y7+uzU0c5wgyHh4fDW1ahMhlpcDC8ZUXJ9XDKVQcP\n2q4gPq0Ey7mqIukymfKOcQMDHANcMP5z3sbnHuKllK6MgLRwMtwej3AbAAAk2dC4rzJJ7O/5hRds\nVxAfzc22KygMAVnpkvrcBVsJIrc49QWfC8fasZL6XnzllYmBdrHb4nn+PFG2Vm9sjG7ZuYwfhBcA\nJEfD7apxf2U8TnMCAABI0pYtdtY7PqCP+3IxubgHfIhWrpCtq0s6dGj07ygCtbCunijHK6/YrgDF\nSGo3JHFQblcoUTz3xZ4cHh5mHwDSwMlwmwElAQAAcisnHKqrC62M0KSt+4JK9P3d3j71NDb7BEY8\ntbSMHSQxiuNF2t7vlRbWQJsbN4aznHLFqdV2JWsZHi7/s2J4WHrppfKC4SivIii0rs2bpf37o6sD\nQDw4Gm6P/YtwGwAAoHxpa60bp2AkCvm2L+q+n+k7NnxtbbYrKM3evZVfZ1StOIPLTWpL0Q0bwllO\nR0fp8yb1uYuTurrCBh9sbY22jjhcacH+BKQD4TYAAACQAz+Ko7F5s+0KohfHfaeQsKvSWlom3uf6\nSaUki+N+nXa5xuAY6QfbjL2gfYJiTy7x+k+Uydg5SVeIbdt4zZAejobbdEsCAAAAuIZW36Xbvdt2\nBaVxKZzZuXPs32Fsm0vPT9hce25ybQ/HxFGTdVs02Umz2trS1zk8nH+Qy0p0IzaZ4MnDvj733g9A\nkKPh9ti/CLcBAACAZAp2cdDVZa8OjCIkKU1Pj+0KyhP3+nPtl8VcCVDu1Q2TrSsp75mBgfxhbSH6\n+oqbPsznZarXetu2cNbT31/YYNZr14azvjCsX1/8AKF9feUPKgpUirVw2xhznDHmZ8aYLcaYN40x\nf2mMmW2M+YMxZpsx5vfGmONKWvaYeLuKcBsAAABFS2r3CIUOulfIj/M4yDc43WRBW5x/kLs2GGc5\ng7UF32P19eXXUq5cXTygsgYH7bVG3r7dznrjJJMp7z1dTv//SflMkgprlR3XVvVtbYX1h97YGN8u\nV4DxbLbcfkDSbz3PO1PSeyRtlXS7pD95nne6pOcl3VHKgsdslBHhNgAAAP5/9u47yo3ybBv4Ndre\ni3vFDdu4UIxtenlDL6H3QChJXgiQkEJCSEIICW8IKeRLJUAIJCRAgNB7NcY27t27Xtu73mJv79rV\n7qrN94es0YxWZZo0I+n6neNzNNLo0SOtpPXec8/1GJaqxe5ojBQw7C7RXZJdXdr2l89H633l7FAA\nDmfWa93Zac44ANDbq+9+mzebNwezRPveSbfvo6B9+8x9L1BiVFdbPYPkEMX0+6w1NWnvsCeyO0uK\n24IglAI4RRTFpwBAFEWvKIp9AC4G8I9Du/0DwCW6xldsOeCDTQ+ZERERESWY2i5eIlJvcNCax5V3\nNjqd2u47NGTuXMyQqIMQjK8xLtUXfk2VGJBMF63IascDTXZiVVe4223d7z+iWKzq3J4JoFMQhKcE\nQdgsCMLjgiAUApggimIbAIii2ApgvJ7BlbEkzNwmIiIiIuNYLCG1ktHpp7c72a5cLqtnQHJG4iWs\nZPUifsmm9vdSqv3+sjoiyOdTNgcEDyZG+m634rU1M8/b7Vb/HLq61C1O7POl99lhZD9WFbezASwB\n8GdRFJcAGEQgkiT8I6Xra0LxfSOwuE1EREREZESqFUaMyKTnaoWWlsjX2zWfVq2aGnU5tlol+/2Y\n6pnwdlrELxWk48EAM55TT4+9M9jN/LlVVek/28XrjXxgcmTE3q8fpZ9six73AIAmURQ3Htr+LwLF\n7TZBECaIotgmCMJEAFGPFz/99E+R31qP4a0rcPTRp+Poo0+Xbgvv3PazuE1ERESUstLhj+9UL9y1\ntgJlupZ6Ty3plq1qR0Y6Mjdt0n/fRHe49vQk73OupdNSK7tl8ap5nt3docup/l2bbFbEFSV6wd+t\nW4G5c7Xdhwc1o2tuDvw/bOrU0bd1dwcy8hcsSP68yP5WrFiBFStWJOWxLCluHypeNwmCMFcUxT0A\nzgCw69C/mwA8DOBGAK9FG+Omm36K8q0r0Csragexc5uIiIiI7ETNabxkHqsiLoIFEhZKEkP+um7f\nHrq8e3f8+65bp+4x1qwZfZ2a8ZPp88+B446zehb2ESxoRyqaWvFZbG8HxusKWDWXlueezNcp0QX1\nZB4Q37NHuRZCOvJ40qPJgJLv9NNPx+mnny5tP/DAAwl7LKs6twHgmwD+LQhCDoA6ADcDyALwgiAI\ntwBoAHCVnoGVWSssbhMRERERZRK9nYF2XHSRzKG2IzlSEae11dy5BPFMAXOZ8fk142ddVTW6uG1G\nRznfL4knitoK/WrPRHG7gdxcfXMiovgsK26LorgNwLIIN51pfHT5t76DxW0iIiIiIkoaq4tQjY3W\nPK4Z3Z/J7rRNVEeilrM1hoeVxfeOjvj3SbcFRYnsoK0tsHhkRYW543Z2ApMnK69ragKmTTP3cYgy\nlVULSiaU4kkJYHGbiIiIiAyzumBJpFZ71JWL0oOZBfCBAfPGkmtri79PMD6nrS2Qaxukpst3zx59\n8zJT+DxrauLfh9+jZGeimLwIjtra5DwOUSZIy+K28velAz6Rq0oQERERkTHMUSaicNFywNWQL4QY\nj8+nPnYjWd9Va9cqt1ta1N93/35z50Kx8aBCcmzbpm3/ZP6/IlGPxf8bjeZyGVs8mbRL0+K2oNhi\n5zYRERERUWBhKKJY1HQtJqtQlgpFE/kc1SxkqjdOxOlUv7Cl0zn6OnlnuJzefHqjuMhuZGZkcwOp\n8dlJtHivQWen+Y/Z06Nt/9Wr9d83Ffj9QENDaNtui/MmSk+PurN3yDxpWtyWb7C4TUREREQEqCu+\nUUCmFofkxZZoMvW1MUO0YrI3zp+s0Q5MeTzqDkgEY0zCf3YHD8a/b3iHdiKYVdQNl8yO5Xg/QzXW\nr9d/wCETP5fy5xzp+QtC9NdlZCQxc9JC/p6Rd32nS2SJ3x/IFg9K1OK8VmloSMxBEtIuTYvb7Nwm\nIiIiIqLokpWrmmrUFBkTlVNttmhd0nYsAkbrrFYj0WdkyBe7TJSdO42PYXX0xpYtxsew43uTRlNz\nUCgaNT9jeUFYDTM+P6Td8DDPiLOLNC1uyzdY3CYiIiIiIiUWkczX2GjueEaLlVu3mjOPdNLTk5iD\nE3Y4BZ+fae3kr1n458XqgwV2ZuRglNbCtZqDS/LuYTMO3Dqd6jP+iewgLYvbyifF4jYRERFRprLD\nacdEdpOoCIhkdPjSaFpyvLu6gL4+8+dQXW3+mJRcevPg7SrRxXlR1Pd/jEjF51iLy2rtyl61Stv+\nkbS1RZ8TDyKRHaVlcRthsSR+FreJiIiIiEzDjj4Kl0qF7XR7/+7da/UMKBUls0iZjgVRM5+TmTFZ\niYjcSsefH6WXtCxuK54UY0mIiIiIyATpUBBLpedg5657/qGvTbTXy6r3o94F+0g9sz4jwfeI1xt/\nwcZU+n5LhJ6e2B3AQZn+OlmtoyN5j6X1LJ3eXuWBynif40S/l/btS+z4ZuD/B+whLYvbys+XAz4x\nQefdERERERGlEP4Rps+OHVbPgMzU32/1DNLfunXq9/V64383HTgQ+AcECnbBRdzi3U8UA+O3tKif\njxW0vF7R9PfzvZ0KBgeT91hr1mjr5G5uTu57KN4Bq+BnHuAi0BRbmha3leVtdm4TERERkVEsDOuX\nqIznaBL9s0rXzsdoz8vl0vaasjPaHHb8zpEXmGLNT0tMzc6dyrzneM+7tRWor1c/vtcL7N+vfv9w\nRu5bV6duPzMW79PzfjHzu0ztc00HdvxsRuL32/v31erV6vc1I0uc0leaFrflGw4Wt4mIiIiILJTM\nTjUA2L07uY+X7rZti99hJ6e28JMqBaJ01tCgbf8tWxIzj2iC7xEr3ytaXyO5xkbz5qFHpn7GMuF5\nb9hg9QyU3G6gs1PbfaL9nAYGRl+X7M7tZB+UJ2PStLitXFCSxW0iIiIispKRzj8iSm927qwMJy8w\naTngQYmTCYVcK9j9c5nsg8bxuFzKGBEjNm40ZxwjNm+232tM0aVpcVu+weI2ERERERGp43RaPQOi\n+IwUNPfuNW8eRrAoq0/4Yrt2iAOxy3sq3TU3Wz2DxGltteZxDx605nHJXGlZ3FY+KRa3iYiIiIhI\nHTsUigAunkX27xwlawQX0wwKxif4/dqyyAHzDjBkQoEw1mul9XXXa88effdLhe8Ss+PE1J5ZwgMz\n6SEti9tgLAkRERERRdDUZPUM0kM6dFwm4o/9/n7zxurqMm8sIjsI//41YxHFdKfluzYdDoiZ+Ts6\nmb+n6uuBlpbkPR7Fl4wFKFPhoEGmSMvituJJMZaEiIiIiHRwuayeAaWa8LiATMU/+LUZHrZ6BslR\nW6vc1lt81LpoXTJs22bueOGL2SW6cB3eDZ5swazm8PdIoskPsBj9nd/ebuz+yRTv/RTrszkyYr8s\navnimpnyfWpnw8PJX5AzLYvbyv9LOeCHD2I6tJcQERERERERpSgzirQ9PcrLeqMatIpVUpDPKRG2\nbzf3zJBEcru13ydZsR7h5GfIrF9vzRyssGWL/mJ+T4/9utTlxfa1a0ffnujPpx4+X+Q1PkQRqK5O\n/nzMtG9f8l/zNC1uj24V8CMNztEhIiIiIsts2WL1DIiSh71BlAjxuvmGh4H9+9WP5/Wq6zpO9tkE\nZmf3+/36P5N9ffriPoKPp7VLd80a7Y+V6fh9m1hqz6xI5s/B5Yp+YC6VzgKwizQtbss3Ak+R0SRE\nREREZEQ65JmSOZL5B3AyF2qTPy95EVJrZEGmF2oy/fkbFSneJ9W+fxsbrZ5ByPAwMDCg//7yyAet\nrF6gN5mfxUR1qlr9fZJuMVOxCt2bNsVeiNLtTr3vokyRpsVt5YKSAIvbRERERJR5Yv1RbPUfzKTO\n3r1Wz4DSSap+7s3KbxXFyLEFanAdBu2CRf5k5KQPDWkrPKbqZ8FKdil09/bqv2+sgxAjI7HfF/v2\n2TPzn9K2uC3fYHGbiIiIiDKTnuxTokSyS3GEMpfehV+7u82dR9COHfrvywJtII8cAHbvjpxhTPZi\nRrb51q3GxzCb3595Xd3V1fb5f2ZaFreVT4rFbSIiIiKicPGKjJlehDSrU5QokwuQegvJahn9nooV\nQZAs8kUNo7Hi+zgVfgeIYqBj24xxErl/ujHy/M34eSVaXZ32766GBnvFESVDX599CvppWdwGY0mI\niIiIiAzJ9D/eWdxOjEx4X9mlk80OPv88seMbfT+tWmXOPFJVKhSwKT0ZzWPft8+ceUTS1RU68KWl\neKv185QJvw+TJS2L24onxVgSIiIiIiJTsSBClHh2LHxoXVxUDTt+nyS649wu7Pgeo8xgtMv5wAFz\n5hHPrl2BDuVE42fRmLQsbit/Nwaeol9k6wUREREREZFd2LGoSbHZrSs9Ue8hNR3nTU36x+/vj3x9\neIEr0vPr7gaGh/U9rtutLgYlU4S/3jU11swjFqNFz0QXTZNZlLUi9sPnS/xzdLuBDRsS+xjpLk2L\n26N/A7Bzm4iIiIhIPRYezcFCkhLfV/pl+mtnt+dfW6v/vps3679vc7P+hRMHB5PT8aq3+G6Vnh5r\nH3/Xrui3uVz6x03kZ2Z4GOjsTNz4kRiNMrErUWQUmlFpWtyWbwSeIovbREREREREiWe3IiRRugl+\nxvbvB9ra9I2RyG7UtWsTN7ZWAwPxY2YSEbcjZ1XkhPxxoxXwe3v1jT04qP+9R2S2NC1uc0FJIiIi\nIiKiVGenIlksLOirw1xZc3k85nR8ptPPJfyzePCgvs5sUQy8vu3t5sxLDb2FZjXsGLmSTFZ353s8\ngUihRJD/bDNlvYBwKV3cHp44I+L1QoQtFreJiIiIiELSqZhhd5n2Woc/XyPFN7/f2FySxY4/444O\nq2dgPzt2WD0DSiVDQ8ZiXLZs0bb/1q36H4ti27Yt+Y8pjxtxuYCGhtBtZh4QbWkJXVazXkA6Suni\ndjSKJ8VYEiIiIiIispAVf1QnkpaCsygmJ+OXRouV42sVqzvcjWZB79ljzjzUsuNBk3QS6/1oxnu1\nr8/4GHag5n3I9+ponZ3A7t1Wz0K9VP4ZpmVxO7x3G2Bxm4iIiIiI9HO7rZ4BEYVL9sKFzc3mj6mm\niJrKRSdKrMbGxD9GU1Ps2wcGgO3bEz8PrRIxp7a2QEd/UKp/NuUH7FIlBiyStCxus3ObiIiIiIjM\nlMgs1ExiRj5wKtMas1Jfn5BpZBR58SnVC1HhrO6EN4PX5FJNvJ9xqrxm8QrKQXV1iZ2HGhs32vOz\nFS3j2sgCol1dgYgRq5l1Zo78gF0q53WnZXE7Uua2X8zw/0UREREREWlghz/e7MqOf8TbSaoUj8wU\n7Tl7PEB/v/5x5R2CmY6fuwAjhblw0Q62JPO1XrXK3PEGBswdzyq1tdr2b23Vtn+yfsZ2/NyuXh37\n9kTO2ax1JBK9poLWz1FHh7VnuKVpcVtQbAHs3CYiIiIiMsJIgc6OMr2DmJLD6TRvLHnBJRMPIKQ6\nNQcqPJ74+8m/u4wU4QYHgc2b9d/fbrxe7Qdl7Vh41cuu2c4+X/Ljg8xm1vukrw+oqkre4xmxcaO2\n/ZuarP05p2lxW77B4jYRERERUTitfzy1tOh/rHRbUJGIEs8OBR4zrVunbr+2tvj7pNtrE4+a6BKv\nF9i/P/Y+ra2J73gFAtEVqUBrd7hemfZ+jUVL53a89zOFpGlxm53bRERERER2YdZpuESU2qz4LhCE\n9C2uJbuD36ozXsyKLnG5khP1s2NH4h/DDMFO98HB5M05HT6LyXoODQ2hy+3tyXlMrezy80zTKv6x\n7QAAIABJREFU4vboLRa3iYiIiIiIEs8uf+xSZqupsXoG6SdeVnGymZk/nq4GBuJ/J/v91uYlc+Hc\n+Kqq+Ls1lrQsbiueVDCWBCxuExERERERUfKY2amcaYUNUTTWZZsuCwvaifz9vGGDdfMI2rw5/nsk\n0z434XbujF+49vmsXVw0VYvb4WdOJPq1WrMmseOnsrQsboOxJERERERECcPFGInUYWaqfiMjgQXY\nyJ4GB62eAZmlvj7yz/PAgcBCgXr4/UBjo/45BYvt8miOTMczFaJLy+K2snM7sMXiNhEREREREVFm\nkXdTJjuj2kypPPdwahbXNLKIcaJlSje4z6f/YLbfr78wDoQicNQsJpoOzH5PZcrrFpSWxe1I3/l+\nke0lRERERERERKkoUwqKiaC1MD4ykph5BMWKEgl2pzIz3Ryp+rnJlDPEgot6ms2sRVhTRZoWt+Xf\n3OzcJiIiIiIiIqLUlcwOdCvztOPFnRw4YF6WfTp1w6th5hoAFGLkAML69ebNI5aOjuQ8jlXStLgt\n32DmNhERERERkRW6u62eAWWKVO1QVcNoEdaM18Yur299ffS5aH2doo1jl+dqtvDidrKeZ7q+nkCg\nw3ztWn33TebrsmtX8h7LCtlWTyARBC4oSUREREREZLmeHqtnQJkuHeIN3G6rZ5AatBQL7VJwtfMB\nwGgHC2K9dm43UFen/jHq6zVNaZRNm4zdX4tor4fWfGufz9jvxuZmbftXVel/rFShqnNbEIS7BEEo\nFQKeFARhsyAIZyd6cnoJEbZY3CYiIiIiIiJKT5kUMaGmMGu0aJjur2dfH9DVpX5/UUzMIn3bt5s/\npln0HADweoHOTvX7G32fOp3G7h9NIt//IyNAba3+++/Zo23/9vbQ5a1b9T+unamNJblFFMV+AGcD\nqABwA4BfJmxWBimeFGNJbE+0yyFTIiIiIiIiHYaHk/t4/BNKn1R63UZGjC3syHzl2JxOoLc3cFnN\n+2JkBNi4Mf5+duuSTfRBCjMXH42UP33woHnj65FuB3l6e+Mv1ppK35NBaovbwR/n+QCeEUVxF8Ib\npG1ldCxJj7s98q5kqecaH8a5qwrxcM0t8Iv87UtERERERAGp+Ac2kVm0dBUD9i/CJWvhPKu1Ryg9\n2eW7LHweZrxnzIzMcblGX6f2IE2yDzD29gIDA8l9TLO0tMS+PVqGuMsV6Di3y/tZTm1xe5MgCO8j\nUNx+TxCEEgC2rUQqO7cDW7udGfJNqtKQb8DyjunmoTr8bf+P4PYP493Wp/Bso21PBiAiIiIioiSz\n4x/QRHYQqShp5POSjM9apMKlFnYv3seyb5/VM0i+ZH9/613UUa/OzkC0TTr+norWjb9lC9DUZM+F\nWNUWt78C4AcAlomi6AKQA+DmhM3KoEjfebud6+ETzV1J4uWDf8Qje25D10icwx4281rzo7hwVTnu\n3HIiRnxDls3j+aZfwY/Qz+Sp+p9gR98qy+ZDRERERERE9pfKhU4r2S0yQ49E5F6Tfqlc3I2XXe3x\nmP/8Uvn1sjO1xe0TANSIotgrCML1AH4MoC9x0zJGkJW3BSELAODyOdHo2m3aY+zoW4U/7vsm3mh5\nDH+p+65p4yaaKIp4puHn8MOHKudavNP6lCXz6Bxpxrthj+2HDz+vvhZ9Ho3nXxERERERERGloGQW\n6j0e9fuGF+EaG82di1Z2Kwoa7UQ3w6ZNVs8gtbW2xr59165QLns0sT6/at+ze/eq24+iU1vcfhSA\nSxCEowB8F0AtgH8mbFYGyd9bDiFHulzVb955Chu635cur+16E16/ht8SFto3sBVd7lCn+fNNv7Jk\n7i8eeAQeMRDONKvoSJRmVwIAOkYO4Fc1t1gemUJEREREpNeQdSdH2gK7aonsxYw/r/ts2N5o1XeN\n369ucclEczqV21pzp2O9L/g9bkxnZ/yFG+X7huvpMT6HeNnaaqxbZ3yMZFBb3PaKgWrjxQD+JIri\nnwGUJG5axsg7tx2HOrcBoNrE4vbO/lB8hsvnxM7+1aaNnUhru99SbLeNNODjjueTOoc+Txdeb/6r\ntP2VGQ/i+/NCXdxrul7Hfw/+IalzIiIiIiIiIrILO3QGZ6pduxI7fqJ6+eIVRLu7GesSyeBgYuJH\nfAaSkbdtMz4HtcX1WCIdrA++VnY6AKK2uO0UBOFeADcAeEsQBAcCudu2pOzczpYuVznNKW57/Z5R\nXeBru942ZexEW9c9ep7PNj4Ev5i89UFfPvgHDPsHAQS6tk8YcyFOGnsRLp9yl7TPY3XfQ43TBodC\niYiIiIiIyFYy4URfM7ouY4m2aFy6sVMBLujzz6153P37tXd3Bw0OBorj8aTiZ3PHjsz5PFglXryL\nUWqL21cDGAFwiyiKrQCmAvh1wmZlkCNsy3HomvrBXRj09hsef9/AVoz4lYcv1ve8Y3jcROvzdEpF\neQECCrKKAQANrmqs7notKXNweZ14WdaV/aXp90I49Nvmf2c9jLnFxwIAvKIHP6u6xpSfFxERERER\nEdmT3mIbUTRutzmxDonidmvbv64uMfPQor8f6OhQt68dDyjEIormFeU7OoCmJnPGSie2KG4fKmj/\nG0CZIAgXAhgWRdG2mduK3m0BmFm0GAAgQsRu5wbDo++QRZIE7R/cibZhi1dYiGN993sQEfjELig9\nHpdMvkO67dnGh5KSc/1a86MY8Abe1VMK5uC0cVdKt+U68vCTBf9BYVYg8aZ5uBaP7Lk14/K3RVHE\n7/feiS+tm4M1nW9YPR0iIiIiIiKykQz7E1kzeaSLXV4rI4s/yhfz3GC8pEVhRkaAhgZzxnK7ecDO\nCqqK24IgXAVgPYArAVwFYJ0gCFckcmJGyJ+UHyIWlB4vbVf3G09D39EXKm47EMr0Xtdt7+5ted72\n8ZUX4Iqp30KOkAcA2O3cgM29HyX08Ud8Q3jxwCPS9rXTfoAsWSY6AEwpmI3vzn1c2v6443m83fpk\nQudlN9XOdXi1+c9oHq7FX+q+Y/V0iIiIiIiIiFKSx6OvwG1293H44o96DQ6aM47bbZ/CfyKp7eCP\nlo+t9n0Qvl8mvLbxdHaa976PR20syY8ALBNF8UZRFL8MYDmA+xI3LWPC33uK4rbB3G1RFLGzL7R4\n5HkTb5YuR8qztguf6MOG7nel7eMqz0dl7kScP+kr0nX/bnwooXN4p/Up9HjaAADj8qbi7Ak3RNzv\nC+OvwQUTvypt/2HfN7B/cGdC52YnG7rfky4fHNqH9mGe00JERERE6ccfZ9kfI4txEaWzZMQ+pEK0\nBAuI+u3enZjolnjf68kWa2FGve8f+f3MjDTRo7bWuseOp7vbfsVthyiK7bLtLg33TTpBVt72Azii\n5Dhpu6p/raGYi+bhWqlAW5xdjqum3S3dtrnnI7j99kyhr+pfC6c38M01JncS5hQfDQC4Ztr3pO7z\nLb0fo8qEzvZIvH4Pnm/6lbR91dS7kePIjbr/nXN+jxmFCwEAbv8wHqi6GsO+zFguekPPe4rtbX2f\nmjr+rr7P8WD1l7C683VTxyUiIiIiIiIi+0hm4VUUjcWvpKLm5tgF/US//pHyvffuTexjyg0PA11d\nyXu8aNQWqN8VBOE9QRBuEgThJgBvAbBtm7L8AKMIEdMK56EoqwwA0OvpQMvwft1jyyNJFpaeiOmF\n8zClYA4AYNg/iB19n+keO5HkXeXHVZ4vLeI4MX8GzphwnXTbswnq3v6w/Vm0jQRCjMpyxuLCSV+L\nuX9+ViHuX/AC8hwFAIAGVxX+uO+bCZmbnTg9PaOic7b2rjBtfFEU8fPqa/FR+7N4sPpaDHj7TBtb\n/hhERERERJQ4qdBVS/bg8Vg9g8Th54CsFusMI73vT6Pv64MHjd1fi4EBoKUleY8XjdoFJb8H4HEA\nRx7697goivckcmJGyDu3RQAOwYEjSpXd23rJI0kWl50MAFheeZ503douc2r+3e42rOx4WVp80ai1\nXbK87TEXKG67dlroR7m66zXTI0B8ok9RNL9iyreRn1UY934zihbgG3P+KG2/3fokPmx7Nu5j1Q5s\nxxvNj+Phmltw66al+HXNV23bUR9uc+/H8EN52M/M4nbd4A7pIMOw32VKBr3cjr7VuGrtVHxr6+kp\n85oTERERERFR6klEX1Uq92pFm3sqPyctvF6rZ0BWUR0tIorif0VR/M6hf68kclJGKTu3A+S521UG\ncrd39Ic6txeVngQAOL7yfOm69T3GF5X0+j341tZTcX/V5bh3x4WGO2E7Rg6idjAQNJQt5ODY8jMV\nt88sWoiTx1wibT/X+LChxwu3qvMVNA3VAACKskpxyZTbVd/3/Im34Avjr5W2H9l7Kw64QudYdLvb\nsLrzdTxR90N8Z9sXcOGqMnx101F4ZO+teLf1KewZ2IS3W59ULGRpZ+GRJEAgCses3O1NPR8otqv6\nPzdl3KBnGn6OTncztvV9itWdr5k6NhERERERUbprbrZ6BukrU4q8cuncuR9Oa3HbzM7/dHxv7dlj\n9QzUi1ncFgTBKQhCf4R/TkEQ+pM1Sa3C358iRCwokS0qqbNzu8/TiUbXbgCBIvH8kmUAgKPKTpPi\nMxpdu9E8VKdr/KDVXa+jaSjwLtrZvxo7+1fHuUds8kiSI8tORWF2yah9rpt+r3T5o/bnDD+HIFEU\n8e/GX0jbl0y5E8XZ5arvLwgCvnP4XzE5fzYAYMg3gPt2XYqfV1+H69bNwuWfT8SPd12MZ5sewpbe\nTzDsj7x08LOND6Hb3WbsySSYKIrYKFtMsjR7jHTZrNztTT0fKrZ3mVjc9ok+7OpfI203uKpMG5uI\niIiIiEitTFyMlBEd8XV2qt83vFiZjsXLSLze+LEWXi8wGLn0YgvJjOUwU3t7/H2i0ZN1Hm/xTyMH\n2lwuYCSJJ/PHLG6LolgiimJphH8loiiWJmuSWglh5W0RwPzS5dL2voGtcPuHNY+7sy9UuJtXshR5\nWYGCdl5WAY4p/4J027puY93bb7Y8pth+rflRQ+Ot61LmbUdyROlyLCk/AwDghw//afq1occM2tDz\nHvYObAEA5DkKcPmUuzSPUZRdivsXvIAcIbAAZb1rFz5ufy5qdvrY3Mk4dezluG3Wr3FY4REAAJfP\niafr79f5LJKjaagGbSONAIDCrBJcMuUO6TYzoknc/pFRRfKq/rXwi+YsZ7x/cCdcvtBSuMEDQURE\nREREZK5MKbRFE6+Q63YnZx5WG5aVNTLxPZGMgn4qvq4DA/rnrebAUF8fUGdOP6Qh0Qq6iVhQsa9P\n+XlLhCoD/YFOZ/x9wq1aFX8fvQ4cSO5Ck6pjSVKN/ImJEFGWMwbTCuYCALyiB3udWzSPKV9MMhhJ\nEiTP3ZZ3SmvVPFSHjWHREZ92vIget75DOG7/iGK88LxtuS9N/6F0+Z3Wp9A1YjwV/l8N/yddvmDS\n11CRO17XOHNLluDWWaML7jlCHhaVnoSrpn4XP13wIl44vgkvnnAQDyx8CVdPuxtfn/Vbad+3Wp4w\nPU/cTBu635cuLyk/A0srzpK2zShu7+pbgxH/kOK6QV+faUVoedc2wOI2ERERERER2VMqFq3V6uiw\negajDzwkoptaT0FXTu17INjN3m/b/Ap94nVuxxPp9TM6pl5pW9yWC77e8+WLSurI3d4pz9s+tJhk\n0HGy4vbW3k8w4lMWEdV6q+WJUdd5RQ/eaf27rvF29H0mRXVMzp8lFfgjOab8fzC/JNDh7hFH8OKB\n3+l6zKDtvZ9JGeXZQg6unnq3ofEum/IN3DXnz7hg4lfxjTl/wF+XbMBbJ/fjj8eswtdn/wanjbsC\n4/KmKu6zvPJcLK04GwDghx+P1hqbQyLJ87aXVp6NeSXLpLgbM3K3ww+aBJkVTSJfbBUIdKKb1RVO\nRERERKnDqj9uKb0MDFg9A/tK58JsIg3pK9OQCRLRTZ0su3bF38flSvw87Cx4MGP37uR2bAelbXHb\nIYsmCf7fSp67XaUxd3vEN4Qa50Zpe1HpiYrbJxfMwvTC+YF9/UO6MpI9freiiH3auCuly683/xU+\nUXtw2Nqut6TLx425AEKM83YEQVB0b7/e8iicnh7Njxn0r8ZQ1/bZE76M8fnTdI8VnN8lU27H3fOe\nwGVTvoF5JUuR48iNe5+vz/oNHIfe6ht63sP67ncNzQMA2oYbceeWk3DThoWKBS71cvtHsE3Wnb2s\n4hzkOvKwUPY+M5q7vbk3lLcdPIgBmLeoZHg2vNs/jPZDMStERERERERatNl7ySTT8GBQakiHTPN4\nB0XMXHwy1mNpLXQnOg5ErVjPKVq3fCpEJPXoKPtFey38fmsOvqVtcXt06jawoFT/opI1zo3wioFP\n+vTC+SjPHTdqn+UVoe7ttTqiSdZ0vY4eTyB+ZGzuFNwz7+8oza4EALSNNOgqysojUo6Pkrctd+KY\nL+KwwgUAAos3vtL8J82PCQA1zk1SJ7IDDlw77R5d45hhVvFinDfpK9L2o7V3wydqXEZXxu0fxk92\nXYZd/WvQ4KrCE/vvjX+nOHb2rcawP3Cob0rBHEwumAUAOLr8dGkfI9Ek/Z5u6eCMAw7FQQwzOrc7\nR5rROlw/6npGkxARERGRFl79/00nymjpUHwFgB07kv+YkYpx6fJ6aqGnyKmH1oiStdqDF+JKVgE2\nVpSJXQ5sbdum/T7bt9vnoAOQ1sXt0Z3bs4oWSzEPbSON6BxRv/Tnjv7oedtBx48JFY/liziq9WbL\n49Ll8yd9BQVZxThv4i3Sda81/0XTeAeH9qFpaA+AwGKO8kJpNA7BgS9NDxVr/3vg9xjyaV8G99nG\nh6TLp4+7ClMLD9c8hplumfEzFGQVAwgsSPlWy5O6x/r93juxZyC0csHnXW+gz2PsvAtFJMmhGBXA\nvOL2lt6PIR46yDOvZBmOrTgTDmQBABpcVRjw9uoeGxjdtR1kZnHb7R/GL6pvwI92XqQ7g56IiIiI\niIjsS82ChlZIlygYrc/Dbs/brAL8unXan5uaAx4NDerHs0txWw+7daSndHE7v7U+6m3y91zw/Zrt\nyMG8kqXS9dXOdaofS54nvDgsbzt0/SnIdxQBCGQka4mrODhUK2UiO+DA+RMDncZfnHybtM/67nfQ\nMrRf9ZhrZQX2JeVnINeRr+p+Xxh/DSbmzwAA9Hu7IuaAx9IwWI3POl+Wtq+bbryz2ajK3Im4blpo\nHk/V34dBr/bVAN5seQJvtyoL4x7RjY/bnzc0vw3doeL2sopzpMtm5W7L87aXVpyFgqwizC4+Srqu\nun+9rnGD5J+PkuwK6bKZxe33257BB+3/wpquN/BU/U9MG5eIiIiIiCiVZGJXMRln9H1jh0K3ni7j\nSBJVnI03rtbX0OiimVZJ9nslpYvbsSiL26FX9YgSeTSJuuK2X/QrOlOjFbdzHXk4tuJMaVtLNMnb\nLX+TLi+vPA8T8qcDAKYUzJaKnSJEvNHymOoxFZEkYy5Qfb8sIRvXTPu+tP2fpt/A7R9Rff9nm34p\nveYnVF6I2cVHqr5vIl059dsYnxfI/e71dODZxl9qun91/3r8Ye+d0nZwLAB4v+0fuufV7W5F7WDg\nGzpLyMYx5f8j3WZW7vYmWXF7yaH36MLSE6TrjOZuyz8fZ47/knTZzOK2PPP+s86XDUXLEBERERER\nEWUSIwVHHlCxhpk56LGoiRix83sgjYvboVdd/vmV526rXVRSHttQkTMek/NnR913eWUod3udyuJ2\n+EKSF076X8XtF0++Xbr8duuTqgrNQ75BRYzFcSrytuXOm3gzKnImAAA63QfxQdu/ou7rF/04OFSL\nzzpfwdP1P8WHbf+Wbrv+sB9petxEyssqwFdnhuJSXjzwCFqH1Z0z0uvuwE+rroBHDByGm1V0JP50\nzBrkCIEFLXc7N6B+sErXvORd1YtKT0JhdonidqPRJM1DdWgZDnT85zuKpKL2Allx20ju9pBvEHud\nW6TtcyfeLF02s7i9fzAUvtbr6cD23s9MGzuTvNXyJO7aeipWdb5m9VSIiIiIiIjIZCMxSkZaCpTh\nxXA7FzfV2K8iCGFwEHC5It9mRed6sqJLoj1nANi8Wbktfx06OgLvN6u7+tO2uC1/YorO7dLjpMs1\nzg2quj93yCIXFpWdDCHGJ/o4WXF7W++nqvKqwxeSlGd3A4Gu62CXcJ+nE592vBR3zC09H8MjBr7R\nZhYtkjrB1cp15OPKqd+Rtp9vehg+0YcBbx929K3Cqwf/gkf23IY7t5yIC1eX4fr1c/CTXZfhHw0P\nwI9ASNYx5f+jOJhgB2eMvxbzS5YBADziiKrFIH2iFz+rvgbtI4FIkOLscvxs4csYlzcVJ465SNrv\nPZ3d2/JIEnnedpDR4ra8eH5U+WnIcQQK8srO7bXwi/q+NWucG6Sf+YzChZhdfJQUgdPjaYPTYzwU\nSxRF7B/cqbhuZed/DY+baWqcG/GbPV/F9r7P8GD1tejzdFo9JSIiIiIioqSxugiXDK2t5o+ZDq+b\nmjzs8BgQq5/3zp3x90m0WIti9vfHLownS9oWt+Xk78VxeVMwLm8qAGDY7xpVMItkZ19oMclokSRB\nE/KnY2bRIgCB4unW3k/ijh++kGSWkK24PUvIwhcn3Sptv978aNwx13a/JV3W2rUddNHk21CcXQ4A\nODC0F1d+PgVfXF2Ob249Bb/fdwfeaHkMu/o/x5BvYNR9HcjCLTMe1PW4ieQQHLh99iPS9sftz6Eq\nTjzNk/t/jC29H0vbP5z/L0wpCHTvnzPxJun6D9qe0RyV4Rf92NjzvrS9rPKcUfvML1luKHd7U1je\ndtCk/JkozxkHABj09enust6pOPhzErKELEwtCC0g2jRUo2tcubaRBrh8yt8yn3W+rLsgn4lEUcRf\nar8rbY/4h/DqwT9bOCMiIiIiIko2tcU6vz+1F7xLBKsLnXaUzq+JmZEg6fw62UHaFrcdsliS8O/j\nBSXaokl29IeK24tKT4q7v7yYHC93O9pCkuHkRe+d/atRO7A96piiKCrztivV523LFWWX4tLJoYzp\nHk9b1H3LcsZiSfkZuGLKt3HPvKfxr+V7sajsxKj7W2lx2ck4dezl0vZfar8DMco3zcqOl/Fc08PS\n9o2H3Y8TZPnlyyvPkeJbutwt2Nj9wagxYqkd2IZeTweAwGt4ePExo/bJceQq3ndacrd9ok9RmD9W\nVtwWBMGUaBJ53nYwH3xa4XzpOjOiSeoGdoy6rsvdYihOJdOs7noN2/tWKq57+eAfVZ1dQkRERERE\nlEmsKuzvGP2nL9mMKKorVgf36e+P/HP1+dQ9npo4lWj27dN/31SStsVtZXCI8l2nJXe7Y+QgWofr\nAQB5joKIxcdw8miSdV1vRy2cAtEXkgxXmTsRp4y9TNqO1b29f3CnFKFRlFWmiJ/Q6vKpd2FM7iRp\nO1vIweyio3DWhBtw26xf41eL38N/T2jBKye047dHfYg75jyCcyfeiEkFM3U/ZjL876yHkS3kAAB2\n9a/Bp52jo14aXbvxy5obpe3jKs/Hlw/7iWKfLCEbZ024XtrWGk2yoScUSXJsxVlwCJE/knqjSfY6\nN8PpDcSCjMmdhBmFCxS3G11U0i/6FQXmxWWBIvz0ApOL24ORf8OvVBHRQ4Fc/7/WfW/U9f3eLrzb\n+pQFM7KfEd+Q5rMiiIiIiBKpudnqGZBdsQtUP7vnRnd1JedxkvEekhd300l9PdDSon5/UYx8sETt\nzyBanEqPigRYLfNMZWlc3I7euX2EhuK2PHJhQenxyHbkxH3sRaUnoTArsChg20hD1OJevIUkw8kX\nlvyg/V8Y9Eb+hpB3bS+rPEfVnKMpyxmLvy7ZiP9b+DqePHY73j55AH9buhU/nP9PXD3tbiyrPBuV\nuRNj5pDb0ZSC2bhsyjel7cfr7lEs1OnyOnHfrkulyJXJ+bPwo/n/ilh8PmdCqAC+qvNVTRnT8rzt\nZRWjI0mCjtJZ3JbnbS+pOHPUz8lo53a0xVanm9y5LV9M8rSxV0iXV3b+N+bBIwp4rflRHBwKHLIt\nzi7HjYfdL932woHfao7TSTd9ni5cu24mrl43He+2Pm31dIiIiIgoDsZlkFFejX8CVVUlZh6ZYLfx\nkoCt+Hyh7yAryxHbtsXfR213uBF2KMmkcXE7JPx1nlu8RIr4aBqqiVmM1BpJAgDZjhzFwoDrokST\nxFtIMtxRZafisEOdt0O+AXzQ9q+I+5mRty03Nm8yThz7RcwqXiwtRpgOrp/+I5RmVwIAWob345WD\nfwQQiHV5uOZmqSib68jHAwtfRklORcRxZhUvxuHFSwAEctY/6XhB1eMP+QYUkR7LIiwmGTS/ZJmu\n3O1oedtB80qWwoEsAMpCtVo7wvK2g8Vzs4vb8s7ty6Z8EyXZgZ9F+0gTdjs3GB4/nfV7uvHPhgek\n7Rum34erp30PpdljAACtw/VY0fGiVdOzhVWdr0ixS2+2PGHxbIiIiIiIKNG0HiAZGL3UWMoZGYm/\nD1EqypDitrK8nZdVgDnFR0vb1c71UceRLya5KM5iknLLZdEk0XK332h+TLocaSHJcIIg4OLJX5e2\nX295dFTXqtPTg519a6RteUQKKZXkVODGGT+Vtp9p+Dl63R34z4HfYGXnf6Xrvzv3ccwpPirmWOfK\nFpZ8T2Xn59beFfCKgRUKZhUtxpi8SVH31ZO7PeQbVBTPjy0/c9Q+BVlFmC17bvEW1wwnH18+v2mF\nc6XLzcO18Pr1r8Tg8bsVi1LOKT4aJ429RNr+lNEkMT3T8KAUTTM5fxYumXIHCrKKcOmUUJ7+802/\nyugOePkZPHWD27lQKREREVkmxU6IJbIVMxcApOgy+E9HTTLt+3xgAOjstOax07i4HXoXRfrcHVFy\nnHS5Oko0icvrRO1AoM/fAYem7Gp5UXlH32dweZ2K2w8O1WJT74fS2NEWkgx31oQbkO8oBBDI1t4h\nK74DwIae9+FH4LyD+SXLUJE7XvWcM9FFk27DtIJAIXbQ148Hqq7CE3U/kG6/ZPIdOHvyS0opAAAg\nAElEQVTCDXHHOWP8tVKGd5VzLRpdNXHuoT6SJEhr7vb23pVS8Xxm0aKoxXMjudu7wjq3gwqyijE+\nbxoAwCd60Txcq2lcuUbXbik2Y2L+DBRmlygWBP3MhGgSr9+DH+28CGevzE+rzt2DQ/vwavOfpO3/\nnfUr5DryAACXTrlTOhtg38BWbOr50JI5atEytB+dI+aHT8ojeYZ8A2gZNrBiBxEREZEBLBoR6cfP\nj3nCX0s1hdp07gw3q1AtiulxFkAkVsZFpW1xW/7Ewju3AXWLSlb1r4X/UGL3rOIjUZRdqvrxx+ZN\nlrrDvaIHm3s/Utz+lqyAFmshyXDF2WU4U7aA4WstyoUl5REox1deoHq+mSrbkYPbZv1G2t7at0L6\nmS8sPRG3z35E1ThlOWNxwpgLpe33WuMvLLmx533p8rLK+MVtZe72J3H3Dx48ASJ3bQfpzd3udrei\nebgOAJAj5EnRLEFmRZPII0lmFS0GABxbcSaKsgKfx+bhOuwb2Kp7fAD4sP1ZrOl6Ax5xBH/Y+w00\nD9UZGs8uHqu7RzrAsbj0ZJwqW5S2LGes4qDac00PJ31+Wmzofh/XrZ+Fa9YdZvjnLTfg7UWDSxmg\nZ+b4RERERFoMD1s9A8pELAoTmS/S52p42B757dEWLm1t1Tee1V3qaVvclov0PS0vblc710U8DV1P\n3racPO9aHk3i8bvxbutT0na8hSTDyaNJVna8hG53ICvWL/qxvvudiI9P0Z0w5kIcU/4/iusqcibg\npwte1JQxfrZsYckP2p+BT4ye3N86XI+moT0AgDxHARariLxR5m7XoW24Meb+8rztYyPkbQfJO7er\n+yN/FiKRx9/ML1k26rWaZlJxW76Y5MxDxe1cRx5OHHORdL08RkYrv+jH87LCrkccwaO139U9nl1s\n612Jzzpflra/Pvu3oxYUvXLqd6TM9c29H6HGuSmpc9Titea/AAicCaDm4JFa1f2jY6mCZ+wQERER\nEVHq6e62egakl10OtIQXa+0yL7Ps2BH5+p4oSxLu3q19EdZkStvitkMWSxKpVDc5f7a0oJrT24MD\nQ3tH7bNTFrmgpvgYTh5Nsq7rbSk6YXXXa5oWkgw3p/hoqSDpFT14p/XvAIAa50b0ejoAABU54zG3\n5FjNc85EgiDg67N+K0XZOJCF+xe8gLF5kzWNc3zl+SjPGQcA6Bg5gC09H0fdd70skuSostOQ68iP\nO76W3O1ud6vU8Zwt5OCo8tOi7jspfyYqcgLxNYO+PtWFaEXedtnogz/TC8wqbu+ULs8sWiRdPnVc\nKJrk044XdUeTfN71Jhpc1YrrVnW9amlMx4hvCL3uDt3394t+PFoXKtCfMf46HFG6fNR+kwpm4vTx\nV0nb/2n6te7HTCSv34MtvaHP0/a+z0wbO9KZO7WDLG4TERERpbJ0K0SRNtEKdGR/n2tLSk0qq7uT\nAWCTRf1oXV32/l5N2+K28j03+icgCELMaBKv36O4Tk9xe0Hp8SjOLgcAdLoPSkW6N5sfl/ZRs5Bk\nJBfJurffaP4rfKIPa7vfkq5bXnkeHELa/nhNd3jJMbh77t9wVNlpuH/BCziq/FTNY2Q7cnDG+Ouk\n7Xfbno66r9ZIkiC1udvywuzC0hNRkFUUdd/AZ0F7NIn84E+kMxvMiiXZHyGWBAjklOc7As+raWgP\n6l27NI8tiiKebXxI2g5GnQDAH/fdZWghTL0aXTW4ccN8XPb5BPx2z61w+7Wfm/pR+7OocW4EAOQ6\n8vG1mQ9F3feaqd+TLn/a8SIODunPR/+04yXcsH4ufrfn66YuULmr/3O4fKF1C/YNbBm1joFekXLm\na02OJfGJXtQPVmX0op1ERERERKlgaMjqGWQ2t9vqGdib05w/g5NGFIG2tsC/RErb6qcQp3MbGB1N\nIlc7uA3D/kEAwIS86RiXN1XzHLKEbMVCgeu639a9kGS408ddKXWet400Yn33O8q87THM29bq/Em3\n4P8dvQKnjrss/s5RnDvxJunyqs5XMODtG7WPT/Ric08og13NYpJB8tztbSqL20tjRJIEaV1UcsQ3\nhL0Dm0P3Lztx1D7hxW09hb0Bbx/aRgLxK9lCDqYVzJNuy8sqULzPV3ZojybZ3vcZqpyBg1g5Qi4e\nOeoTFGQVAwAaXFV4rfnRWHc33aC3H/ftugRtI40QIeLNlsfxza2nxI2gkRv2ufDE/nul7SumfDtm\npv/hJcdgacXZAAA//HjxgLqc+XDru9/Dz6quwYGhvXi95a8xzyzQakPPe4ptP/ya8uGj8Yv+Ud/9\nQOA71ekxp91DFEV8b/vZuHnjQjy0+8umjElERERERInB3HtKVy6XNY87NJT4g0ZpXNwOiVZSUxS3\nwzq3d8i7UnV0bQeF527rXUgyXK4jH+dNvEXa/mfDz6VOTQeyVBU0yXxzio/GrKIjAQAj/iF82vHi\nqH2q+tdh0Bcoeo/Lm6ooAsejJndbFEVF3vaSiuiLSQZp7dze7dwgLVQ4vXA+ynLGjNpnTO4kFGaV\nAAjEnfR4tB+qk0eSTC+cj2xHjuL208ZdIV3+tPMlzeM/1/RL6fLZE76MuSVLcMP0+6Trnm6431A8\niBZ+0Y9f7L5hVJd7jXMj/nfTEtUxKS8d+B06Rg4ACMQTXTf9B3Hvc82070uX32n9O3rc7RpmDux1\nbsFPq66AH6Gc+dWdr2saI5YN3e+Num5730rD4x4Y2gOnN1DELssZKy0CDAC1g9sNjw8EPqdbDi0A\n+1H7c/D42YpARERERGRXdoiesKt0OhFV63NJh4Me60cvN5UwnrCT4BP9ucqQ4nbkd+28kmVSh3ft\nwHYM+Qal23b2hRaT1BNJEiSPnNjZtxpvtz4pbWtdSDLcFyffKl3e7Qy9SxeXnSzFoVDyybu3Iy18\nJy/ULas4Z9Qif7Goyd1ucFWj090MACjOLse8kqVxx51XslSKx2lwVWHA2xtzf0XedpTFVgVBMBxN\nEmkxSbnjKs+T8sr3D+5Eo6tG9di1A9ux7tACrAIEXD0tEM9x+dS7MKVgDgBgwNuLv9ffF3UMMz3T\n8HOs6QoVhM+e8GXpZ9Lv7cL3t5+DZxt/GbMDvtvdimdlBfubZvwMRdmlUfcPWlL+BRxevAQA4PYP\n45WDf1I977bhRty78wIM+QYU13/e/YYpMRy97g7FWQJBZuRuy6OnFpQcjzlFoeL2PpOiSar7Q53h\nfvjQbCD2hYiIiIiIiBLPHxYB4fEYK+5Hum86HSywWhoXt0MFw2jvl+LsMkwvPAJAoOiwxxlIZhdF\nETv6Q8XtaMU7NSpzJ0jFRT986PN0AtC3kGS4KQWzsbzi3FHXy7vFKfnOHP8lqSi5o38VDg7tU9yu\nN287KF7u9kZZ1/Yx5V9AlpAVd8z8rELMLjpK2q7qHx3VIKfI246wmGTQNIPF7booedtBBVnFioVb\ntUSTPNf0sHT51LGXY1rhXABAriMPt88KRXO82fK4aYXOaFZ3vo6nG34qbV859Tu4d/4/8LujVmBM\n7iQAgSiOJ/bfi5/suixi3A0A/L3+J1KReUbhQlwwSV3skSAIuHbaPdL2q81/GlWsjmTA24t7dpyH\nLncLAKAoqwz5jkIAwMGhfWgaUn+wIZqNPR9IByinFcyVrq/uXwe3f8TQ2PKzFBaUnoDZxaHPgFmL\nSsoPPAJA45D+/HkiIiIiIjKGBcXE4WubGerrgRFjf4qbLm2L2/InFq1zG0DERSVbhvej290KIFCs\nmVG00NBcIhWb9S4kGU6+sGQQ87atVZE7XlFwfa/1n9Llfk83apwbAAQy15eUn6F5/Hi52/JIEi3x\nNMrPQvRoEr/oV9we6+DP9ILEdm4DwCljL5cur+xUV9xuHqrDJ+3PS9vXTr9HcfsJYy6UstBFiPjj\nvm8mbDHARlcNHtp9g7S9pPwM3DorUHhfXHYSHj92M44sO0W6fVXXq7h983LsH1QuoFk3sAPvtITO\nDPn67N9o+o45ddxlmJw/CwDg9PbgLdlYkbj9I7hv56VocFUBCGSWP7joVcUBmzVdb6h+/Gjkedtn\nTrgek/NnAwA84ogUxaSXonO79HhlLIlJBzRGFbcNLK5KRERERERE1jKrNJBqETjB593RAXi91s4l\nXNoWt+Vive+Ui0oGCh07ZJEki8pOVNX5Gou80AkYW0gy3PFjLsCEvFBu94S86ZhRuMCUsUm/cybc\nJF1+v+0f8IuBc1o29XwI/6ElTueVLENpTqXmsWPlbnv9HkU397GaitvqcrebXDXo93YDCOQUTy04\nPOq+RmJJRFGM27kNBArROUIuAGDvwGY0D9XFHfuFA7+Vfg5Lys8YFd0iCALunPP/pOLw9r7P8EnH\nC5rmr0ZwAclBXz8AYELeYfjJgucVRenK3In47ZEf4Yop35auaxrag9s3L8fHsgL9o3V3S89pWcU5\nWF45+qyOWLKEbFw17W5p+6UDj8Dr90Tc1y/68auaW7C1b4V03ffnPYWjy0/HCWO+KF23xmDutl/0\nK2J8llecqyj07zAQTeLyOlF/KNPdAQfmlyxTnL1QP7gr6vNXy+v3YI9TGanSpCE6h4iIiIgo2dh9\nS5Q40T5fPl/k60mdtC1uO2SxJP4Y+y0oUXZumxlJEjSvZBlKs0ML7hlZSDJclpCFCyeFsrdPHHOR\npgxnSozjx1yA0uxA4bptpBHbegPZ2EYjSYDYudtV/Wsx7A9kx0/MnyF14qqxUFbcru5fJxXkw8nz\ntheWnhjz/WakuN3pPihlfxdllWF83rSI+xVnlymK+PG6t3vc7Xin9e/SdrQFF6cXzselk78hbT9W\n9z0M+8xbXjh8Ack8RwF+vvAVlOWMHbVvtiMHd8x5BPcd8ZwU+zHsd+Hn1dfiz/u+jTWdb0jvLQcc\n+Prs3+ia07kTbkJ5zjgAgfdttIL+k/t/hI/an5W2vzbzIZw54ToAwPGVF0ixULv616DP06VrLgBQ\nN7hdWoi0NHsMDi9ZgsWy4raR3O3dzg3SwYAZRYtQmF2CkpwK6X3mEd2GY1VqB7fDIyrP12LnNhER\nERElWjosfmeVdC/up/vzSyXyn0WwuF1fb8lUDLFDCTJti9tK0T+9hxUtQEFWMQCgy92CjpEDYXnC\n+heTDMoSsnDK2Eul7Ysn3254TLmrpn0H5064CSeNuRjXH/ZjU8cmfXIdeThj/HXS9rttT0MURUXE\nwtKKs3WPHy13W563fWzFWZoOdEzKn4mKnPEAgEFfHxpc1RH3U3w+4hz8mVwwGw4EznxoG2nQVByu\nG5BHkiyK+VxOG3eFdDle7vbLB/8Atz/wv725xcfGjIa5ccZPpGJv+0gTnm/6laq5qxG+gOTdc/+G\nw0uOiXmfL4y/Bn9Zsl7RLf/Swf+H+3ZdIm2fP+mrmFm0SNec8rIKcOmUUEH/+aZfjYpjeb35r4pF\nKy+a/HVFXndF7njpjBg//FjX/bauuQDKxVeXVpyFLCFL0bm9s281fKK+Q9zyaB35gR15NInRrPXw\nSBIAaBqqSVjEDRERERERAHR3Wz0DItKjocHqGaSmtC1uq+3czhKyML9kubS9tvstKUM2W8jB/JJl\npszna7MewtVT78Z35z5ueCHJcLmOfNwz/yk8uOhVVOZOMHVs0u+ciTdKl1d2vITdzg3oGDkAINCJ\nvKD0ON1jR8vd1pu3DQSiOOTRJNFyt+Wd27EWkwQCRf7JBaHu8QNDe1TPR00kSdCJYy6SojyqnevQ\nPtwUcb9Bbz9ebf6ztH3d9B/ELJoXZ5fjqzN/IW0/1/QwWoeN/7aJtIBksPM5nplFC/Hokg04eUyo\noB3sQC7IKsbNM35maG6XTL5D6g6vG9yuOCDzedeb+P3eO6TtEyovxDfn/GHUa6iIJjGQuy1/7OCZ\nDlMK5qAiJ/A9N+jrw/5D0SJaKfO2Q+97eTRJ7YCxRSV3948ubg94e9HjaTc0LhERERERmYe9J/HZ\nLWPZCnboULaSnZ9/2ha35a95vO+pI0pCRcaXDvxOujy3+FjkZxWaMp+ynLG4bfavceGkr5kyHtnf\n3OJjMaMwsBjpsN+F3+0NLf65pOIMQwuKRsrdHvD2SZ2iAgQcU/4FzeMujJO73eNux4GhvQACCwjO\nKzk27ph6o0nULCYZVJpTqXi+Kztfjrjfmy2PS1EnUwsOx8myMyqiOXfizTi8eAkAwO0fxl/rvhf3\nPrHEWkBSreLsMjyw8L/42syH4JB9jV837V7DB7hKcypxgex7Ktitvrt/A35WdbUsM34p7gvLBw86\nUVbc3tD9Ljx+t+Z5DPkGFOsfBM90EATBcO62KIqocioXkwyaLV9UctBYcbta1rmdI+RJl82MJnH7\nR7C683W0DO03bUwiIiIiIgI8xpbgSSv79lk9A7IrOxwcypDiduxXWl7YaJJ1lsbrSiWKRRAERff2\n3oHQwnJGIkmAyLnbW3s/kQqPc0uORVnOmGh3jype5/au/jXS5bklS5HryI875rQCncVtV6gjN17n\nNhAeTfLSqNvd/hHFwaurp31P1WKxWUIWvjHnD9L2px0vKqJgtFCzgKRaDsGB66b/AL868j0sLD0B\nZ0/4Mq6a9h1d8wp3xdRvS3EyW3o/wSftL+CHOy/EsD8QKzMpfyZ+sehNFGQVRbz/jMKFmJg/AwDg\n8jmxvW+l5jls7V0Brxj43+SsosUYmzdZuu3I8lOly9t0jN08XIs+TycAoCS7AtMK5kq3zSkOdW7v\nG9iqO0Jk0NuPxkPRPg5k4fgxF0i3mbmo5J/3fQs/3nUxbtu8DL3uDtPGJSIiIqLUZYdiUzIwW9wa\nbu29S2khEz5XbW1Wz0CfNC5uh8rbcTu3o8RDLDYhb5sy21njr1d01gYt17mYpFx47rYib7v8TF1j\nzitZKhVaG1zVcHp6FLdrydsOknduqy3q+UQvGgZDmd9qMqRPHnOJ9Frv7F+NrpEWxe0ftP0Lne5m\nAMCY3Ek4e8KXVc0FABaXnaTIUP/jvrvgE7WdlxVpAckHF70acQFJLY6tOBN/OmYN7p3/D1UHG9SY\nmH8Yzhh/rbT9s+qrpSiN0uxK/HLxOzE7xAVBUESTrJZli6slz9teVqH8vIR3bmstQMvPSlhQerwi\nVmVS/ixpHYZeTwe63a2axg6qcW6UDqzOKl6Mw4tDeepmdm6v7noNANDv7cKKjhdNG5eIiIiIiNJX\nJhRqSb1Ufz+kcXE7JF7ndmXuBEzKnznqerXFO6JoxuRNwrLKcxXXTSuYK3W1GhGeu70pbDFJPfKz\nChWZw9VhC+LtlHVuqz2zQRFLMqSuqHfAtRcecQQAMDZ3CkpyKuLepzx3HI4qPw1A4DP/Wecr0m0+\n0Yf/yBaDvGLqt5HryBs1Riy3znpYkUX9ZssTmu4faQFJ+eKFdnP1tNHxKzlCHh5c9BqmF86Le/+T\nxlwkXf686w3NBehIedtBM4sWoyirFADQ7W5F83CtprGVxe0TFLc5BAdmFR0pbetdVFK+mOT8kuWY\nJnvNmobM6dzu83Siyx06iLOi4wVTxiUiIiKizGXnXF0iSi0HDybncTKkuB3fEbJoEiBQgCzPHWfq\nnCgznTPhRsX2UhO6toHRudvBLOw8R4GhSJ2FUaJJ3P5h7HFujLhfLIqinqsGfjHWEq8BWhaTlDtl\n7OXS5ZWdoWiSVZ2vSpFDRVll+OKkW1WPGTQubyqum36vtP33/T9GvyfyMuRevwcHh/Zhffd7ePXg\nX/D7vXfqXkDSKrOLj8RxledJ2wIE/PCIZ1Sf0XJk2alSAbp1uB71rl2qH7tlaL/i/Rz+mFlCluI9\nvl1j7na1fDHJkuNH3T5bFk2iN3dbXtw+ouQ4TC+Qn8FgTud23cAOxfb2vpWjzlggIiIiyhSMqCAi\n0idRB7VcrkBXeKI7w9O2uO2Qlbfjl9JGFzgWMZKETHLS2ItQnF0ubS8zmLcdFJ67HXRk2amau5Ll\nFkRZVLLGuQkeMRCuNbXgcFTkjlc1XlnOGJTnBA4UjfiH0D7SFPc+WhaTlDtl7KVSJNG23k/R6+6A\nKIp4vim0YOPFk29HUXap6jHlrpr6Xanrvt/bjUdrv4uVHS/j+aZf45E9t+HubWfhunWzcM5nBbh+\n/eG4Z8e5+P2+O/Bq85+lMfQsIGmVm2f8DLmOfAgQcPvs3+H0cVeqvm+OI1dxIGdN1xuq7yvv2j66\n/PSIcSuLZdEkWorbQ75B1A5sBxAo2M8vXT5qH/nZC7UD+orb1f3rpMtHlC7HlII50nuzdbgebr/x\nv75qB7crtkWIWNn5X8PjEhERERERZZpYBd6hoeTNI9XEK1z71RRlDUrb4rZS/EMEC8I6t5m3TWbJ\ndeTjxsPuBxDItDa6mKScPHc7aKnOSJIgeUd2df86qdNaT952kCKaREXXqqJzu1h9cXts3mQsLD0R\nAOCHH6u6XsWW3k+w27kBQCBW4/Kpd6keL1xeVgG+Puu30va7bU/j/qrL8Vjd9/FGy2PY1PshWob3\nww9fxPtPK5inewFJK8wrWYp/Ld+Lfy6rwRU6XrcTZbnbn2spbsvytpdWRD7TITx3W60a50bp53NY\n4QIUZ5eN2kceF7NvUHssScfIQSnfPd9RhOmFRyAvq0A6MOKHHweHjC83Lj8IFMRoEiIiIiIiIm06\nOjJ3oUytwovZogj09Vkzl6C0LW5r7dyeU3w0coRQtyvztslMV0z9Fl4/sRt/OWYdchy5po17VITi\n9pIKfYtJBk3Mn4GKnMBigYO+PjS4Ags77uyXFbc1xp5M01jc3q8zlgQATht3hXR5Zcd/8VzTL6Xt\n8ybdEnMhRDVOGXspjin/Qtz9xuZOwZFlp+LciTfjKzMexP0LXsDflm41vIBkso3Lm4qphYfruu9x\nledLi3xW9a9Fj7s97n28fg82934kbUdbfHVeyTLpO/vg0D7VCz/Ko3aiRevMLFokdVkfcO3BiE/b\nYXp5JElgkdYsANoP8sRTF9a5DQA7+lahc6TZ8NhE+wd34Y7NJ+AXu78Mnxj5gB0RERERUTrp6rJ6\nBqRH2ha3tWZu5zhyceXUbwMATh17OaYW6CvmEEVTklMBh2DuR06euw0AFTnjNReDwwmCMCp3WxRF\n7JIvJqm1c1tD3vCQbwDNw3UAAAeyML3wCE2PdcrYy6TLm3o+wMZDC2064MDVU+/WNFYkgiDg+/P+\njiPLTsG0grlYXnEuLpl8B+6Y/Tv838LX8dTSXXj3ZBdePOEAfn/0p7hn3t9x/WE/wunjrowYr5HO\nynLGSJ30IkSs63477n2q+tfC5XMCACbkTce0gsiLV+Y68nCELFJEbTRJlSxvO3ythaCCrCJMLZgL\nINBlvX9wp6qxg3b3y/K2S4+TLsufS5PL2KKSPtGH+sFQjnlwEUxGk5BZ/rb/h6hyrsUHbc8oFsM1\nS8vQ/lG58URERGSNROfhkv14PKk9vlHRIkh6e5M7DzJHhhS31X1Tf23WQ3jjpB48sPAlCFwimFJA\neO72koozTSmgy2N6dvV/jqahPejzdAIASrMrFYtEqqGlY3W/rGA3rXCu5vzwCfnTMb8kUPT0y87b\nOH3cVZhcMEvTWNFMzD8Mvz96Jf65vAYPH/kO7jr8T7hi6rdw4tgvYkbRAuRlFcQfJEOcOPYi6bKa\nApk8b3tp5Tkxv4uPLDtVuqymuC2KoqrObUC5qKTWaJJqZyhvO/heBLSfwRBLy1Adhv0uAEBFzgRc\nPPl26bYV7eZEk4iiiFWdr2FTz4emjEepwyd6sbX3E2lbflDIDPsGtuL69XPwlU1H4rPOV0wdm4iI\niChd+UU/Pm5/Hqs7zW88MFMqlNMSfUDHp+LEx1Q6qGT3uVpa3BYEwSEIwmZBEF4/tF0hCML7giDU\nCILwniAIo8NQVZLHkmj5GcgX/iNKBaeOu1y6fPq4q0wZc0FY57Y8b3th6YmaC+iK4vZQvOK2vsUk\n5eTRJEHXTr9H11hkjDx3e0P3+3EXUpQXt5dFydsOUiwq2bsy7lxah+vR4wlEoxRllSnel+H0Lirp\nE32ocW6Uto+QFbenyw4KNQ0Z69yuC4vuOXXsZVIEzI7+VegYOWhofAB4tfnPuG/XJbh7+1lY0fGi\n4fHsqnW4HtevPxyXrBmn+Nllsj3OzdIZFABQc2jdArN83P68dPDx046XTB2biIiIKF290/p3/Lz6\nWvx418X4vOstq6cDwP5Fz0RQ85yjLaJo19fLrvNSy+rO7bsAVMm2fwDgQ1EU5wH4GMC9ZjyI2s5t\nolR0waSv4Z55T+H+BS/gpDEXxb+DCoGc4MCihw2uaqztDv3iXFh2oubxJuQfJuUjd7tbMeCNfq5P\neNFOj1PHXq7YXl5xrmKRQEqeaQXzMKVgDgBg2D+Irb0rou7b5+nEHucmAIFImmMrzog59sLSE6SC\nbt3g9pjvKyBwFkLQEaXHxTxII3+/aCluN7lqpKJgZe5EjMubKt0WfgaDaOB/EPK87VnFR6I8d5wi\nC95owdDjd+Pfjb+Qtp+q/4m0uGy6+Uvtd3BwaB/6PJ14ZM9thn4u6ULetQ0Ae5ybTP35y3PpG1xV\nMfbUx+0fRsNgNX+WRGnAJ3pRP1jF7H/SbGTE6hkkhts/god234gf7bwI3e62hDwGf33a18qOUPxg\npCjCvXuTORtSq1Xd8lBkgGXFbUEQpgI4H8DfZFdfDOAfhy7/A8AlesfX27lNlGqyhCycO/EmnD7u\nStPidPKzChWdq6s7X5Uu61lsNUvIwrTCudJ2Y4y8YXm+sd7O7ckFszCvZKm0fe30H+gah4wTBAEn\nVIa6t9d0vRF13409H0gHI48oPS7umTRF2aWYfagILULEzr41MfevcoaiFRZEydsOkseS1A5uU13Y\nkxftjig5TvGZrMiZgKKswAlJLp9T9SKYkSiK24fytk8fHzpzY0WHsWiSj9qfQ5e7RdpudO1Oy/iI\nnX1rFM9rz8CmmO/RTLG592PF9qCvHweGzPlrKfzshkbXblOLVh6/G1/deDRu2mzIvogAACAASURB\nVLgAf9v/Q9PGTYbage24ddNSPFj9JXj8bqunQ2Q5URRx97azcPPGhXiw+jqrp0NkC2+3PIn32/6J\nNV1v4LnGX1o9HUoin+hTrIO1vffTUfuYfWDC642/j9oSxLut/8BdW0/Dqs7XjE3KRMmKT+nsTM7j\nZDIrO7d/B+B7UNaeJ4ii2AYAoii2Ahivd3D5ezQ9e82IEkueRxw8fTxbyMH8kmW6xlObN7zfhM5t\nALh77t9wythLcefs3+Po8tN0j0PGyaNJPu96I2o35YZu9ZEkQUfKokl2xMndVpu3DQBjcyejNHsM\ngEAhunW4XtV8qvsj520DgUK/PK/eSO52pDMcThl7KRzIAgDs6l+D9uEmXWOLoogXD/x21PX/bvxF\nWnXCiqKIx+q+P+r6pxvuT5nnOeDtw8qOl03t3PL43djZt2rU9WZFtsjPbgACXdZtww2mjA0E8sGD\nsT+vNv8Zbn/qtO49Vvd97BnYhI/an8W7rU9bPR1KMlEUcXBoX0q9ZxOt3rULW/tWAAgctJWvy0Jk\ndx0jBzDsc5k+7qaeD6TLW8LOtDJLKuQlZ6K6ge0Y9PVL283DdegYOWDhjNQb9PbjkT23YnvfSvy6\n5isZdzZOivxpEZPXa+/nYUlxWxCECwC0iaK4Fco6dDjdL51yUBv/BIhsakGE4t/hxUt0L5Y4vSBU\n3G6KUtTrdreh19MBAMh3FGFi/gxdjwUAc4qPws8WvozLp35T9xhkjsVlJ0td2O0jTaiVdR0HiaKI\njT3vS9vLKtUVtxW52zGK2yO+IewbCC0MeUTJcTHHFQRBEU0iv28s1fLO7dLlo27Xkj8fzZBvEM1D\ntQAABxyYUbQAAFCWMxZLZFEukU5VVGNjzwdS8TzfUYhcRz4AYO/AZkUmeqpb3fU6dvYH1hPIFnKQ\n5wh8t+0b2IpVXa/GuqttPFB1Fe6vuhx3bT0FXr85S9Lvdm6QFiuV22NScXt3hPzuBle1KWMDgWJY\nkMvnxLYIXU1GiKKI1uF602N6Bry92Nz7kbT9duuTpo5P9vdM44O4fv3h+MrGxXHXp8gU4Qsav9Xy\nhEUzIdLmw7ZncfXa6bhm3WGmHoD2i35s6wv9XlMTy0fpY1vf6DWGtpr8/xw91BQ8awe2wSMGDt72\ne7tQNzD678FUEu0AUH9/5Osp8azq3D4JwEWCINQBeA7AFwRBeAZAqyAIEwBAEISJANqjDfD00z/F\nX999Gk8//VNs3bpi1O2CrLzNzm0i7SJ1ti4q0x5JEhSeNxyJcjHJRZoXriR7ynbkYHnledL2mq7R\nq3vXDe6QYjBKsysVsTKxLC47Wbpc49wQtSCwZ2ATfGLgvLrphfNRklMRd+zwaJJ4RnxDiriQ/8/e\neQY4UbVR+EzabrZke++wLCy9dwRBELFh77L2rqjYsKF+FuxdVJrYRVGxgGCh97a0XWB7772lzvdj\nyM29qTNJkGXN+TWTTCbZbGbmzrnnfV57f0OSmmoq6QTP40xF7UcIviUxIIOYzwAwJeoKsuwumuQ7\nKrV9XtwtOD/uNrJOc7jPZBl5Az4ttOCKLo6/G7Pj7yHry4ue9Yp5yfM8fir/EM8cuQx5EtjtYlSr\nLSMTQmWdJ5xO7kgRzduOVCWQZXumtDui0T1meZO7XWSV7LR3vvFEb564E9fsTMPD2dO8anBvr/+N\nnKMA4XvKP8Nv/HwSL57n8WP5ewBOHs9N3jmez3RZm9t/VH8GrbHzNH0an3wSr7VVy8CDR7O+Dn/X\nfOO1/Ra0H0SroZGs8+BxpHm7k1f41JNkr0o120k/I3dV6l7xp1PltbNBIXtGvSc63LwN9++fhP9t\nWeDV/TqSI0O/w/vFGme0DhzYgOXLF+CddxZg+fIFp/S9TotzxPP8fJ7nk3me7wXgagB/8zx/A4Bf\nAGSd3GwOAIcwnqysBbhzZhayshZg6NApNs/TEym+3LZPPklXrH8qwpQxzGPu8LbNEpNYLbAyt33q\nObJGk1iLTgQPDzsHck4uar/hqhgkqQWeu57XIafF1jgDBFSBWf2DnfO2zZLaVDKv7QBjoNtjhouZ\n5HEle7xtsyYyaJLtqO4qkbTv/LaDxDCVQYbLE+biqsR5pMHswebNPcJ0WVO1jHz/gXINbkh5Clcn\nPQp/WSAA4Vy0uW6Vx++zoXYl3sm7B5vrVuGV3Dke74/WjvrfmfXt9b96Zb/7Kd725YlzyXJe237G\nfHVX9s1t7yW36b4NgGBuewsz06Crxu+VQquYA80bmCa1nsre782X3v7vqKjjCKlcA8RNqEoRzwt9\nKU607vfqfk+lDCa9TRPqNkMTNtZ51jDZpzNL2U2bsKL4BdRpK073R5EkMx4LYDEinspeY3Z6clsM\nH1mMujN64EyQzqTFzvo1XkWG8Dxv39xu9n5yW3cK2n6caGOvPwe9bG6/m3cfDrVswYtbn8Px1n1e\n3bc3RCe99SKKLXvKMTh06BRkZS3AAw8sQFbWglP6Xt0tFvkKgOkcxx0DMO3kultize0e8svwyad/\nURzH2aS3PUlu0w0lyzvz7JbQs+a2+7xtn7qfRofNJKZrbutu1Gsrmefd4W2bNTjkLLLsiLtNm1D2\nkDv2RDdVFYMlyWl1zNs2iza36RsfKbLH2zYrRBmBEWHnkPWNtdJMgJVlb5LlSZGXIl7dCzH+yZgR\ncyN5/IuSF6V+5G6lTmM7lhc9S9avTnoMIcpIhKqicGnCfeTx5UULPErmNulq8W7evWQ9vz0b5Sdx\nMt7QzgbW3N7R4Lm5rTN1MY1Zp0ZfQ9LbXaYOjzjx5v3bmygqbvdicruDTW7XaEtFY4VcaXPdKtKD\nAgD+8VIaT2vsxO6GtTaP/1n9hQ9P8R/Rvka2iauYCVUp2lT3A+47MAF37Bth1xzrjspp3YkuU7vN\n479UfHwaPs2pV6OuBj+Uveu06fp/TbXacjx2aCaWFT2DN47ffro/jmh1GttRo7VEXw80bfAaOsze\n8Xu4xbZPhk+nV+/nPYDHD8/CbXuHoUXf4JV9lnWeQKNeABsEKUKh5PzI49b3Vd1R+VZjsYPNm7wW\nPmjS1eJEm8XQpjFvPv13dNrNbZ7nN/I8f9HJ5Qae58/heb4vz/MzeJ53GyAlo+xtn7Xtk0/uiTYB\n4/17I1wV42Rr51LLgxDtlwRAQAJUdBXYbOOtZpI+dT8FK8OY5o87Gn4jy53GdsaUHhU+Q9K+XXG3\neZ5nmkn214hLbicH9IOCUwIAqrXFLpmGNG/bkbkdr+4N2clLb3VXsVuNhmhGnXVyGwCmRF1JlqWg\nSeq0Ffir5iuyfmXSPLJ8TdJj5HPvbvwDx1r3SvrM3Uk/lL1NEDiRqngmnXxl0jyo5UEABJN0Q+1K\nt9/nvbz7mSQmYL9qwR3pTFqbcv2yzhMo7Tju0X6PtuwgPMQkdQai/BKYJsKeokny2rJh4IUbfI0i\nnDxe0pHjlRucRl0NmvW27ei9hSbZaPV72FC70isNkXY3riOc8yR1Buk30WJowJa6M4P/7pNnoism\nAO8nt80TnTx4/Fj+vlf3faq0h0q7jo+4kFQQHW7Z2uMaSxp5Ax7Knor38x/A3AOT0W7wQVsB4Zqp\nNQkYmj2N684YJE2Z1bW4y9TOBCDclZE32k3p5rTs8jWi7UZq1TdibdUyAECzvo655/FEdNJ5kGYi\ncz9zKtLb3pTepLOprGvW13kcmjDL3HiYrP/Lk7g9JWV9puu0m9v/hnzJbZ98ck9jI84nhtb4iIs8\n3p8zJIORNzKsVJ+53fM0jkKTbKNMvuymjdDzQv1basAARPklStovbZofadlmg06o0ZYSM1MtD0Jq\n4ABR+1XKVMy2rvi3uRQSJdOBua2S+SFO3QuAcG0q6zwh6rOYxfM8k9zuHWRrbk+MnE1MgJzWnajq\nKha17x/L3yPGozBotjTdTArIwGSK532msrebdLX4unQhWc9KfQ7+8gCyHqKMwGUJD5D1z4oXuGVe\nbqn7CX/X2qZ6t3nJ3D7YtMluotFTNMl+irc9NPRsACw7/piHTSVpJMmo8JkIlIcAANqNLajTeV5y\nTt84yagh7tZ6h5Q70WrU1dg0p2zUV3ulYeWWuh/J8qTIy3Be7M1k/beTGBSfeq6MvNHmRry4I8er\nZlUeVQ6+vf5Xr5qnPM9jRfELeO7oVajqKvLafukJvHOir8eEiIvJ+q+Vn3jtfbqD1ld/QapOGvXV\nWFu1/PR+oG6inQ1ryLKB1zOIue4se/hF6wlpd1TQZmkeGa6KRYI6HQCg57UeX5998p421K4k9zUA\nW53qiegAz+DQszAkdDJZ93bzbG+ruCOH3GPQ8hZ3e79V9dOh5s1eQek5kxi0CC2fAX7q1WPNbZmv\noaRPPnmstMABeHXwH7g//X3cnPa8x/tLopEMVuZ2ZWcBSWeEKWMQqory+P186l6iudt7G9eTBA7N\n2x4dPlPyfmP9Uwk6ocPYatO4j74Z6hc8WjTPGxCPJmnW16GiS0BOKDkV04zSWp40lazXVaLFUA8A\nCJAHI8YvxWYbjTIcI0ItaJJNItAkHYZWrK5cRNavTHrYZpvrkueT5c11q1DkRZTEv6UVJS+gw9gK\nAEgJyMTM2Cybba5IfAiBcg0AYRJOaiOoFn0D3jpxF1kfEz6LLGc3bUSrvtHeyySJTgFFqOKoxz00\nt6mbg2GhUwEAfank9jEPk9v0BFC/4NFIDexP1ku8wN2mkSQTIy8hlRcn2vajpsuz7kjWSBKz/qn9\n1qP9GnkDk+ifFHkJZsZkkcbo+5r+QmVnoUfv4VP3Vl7bfrQbm5nHjLzBK8cEIJzf6YlUPa/1akXA\n1vrVWFb0DDbUfof38+a6foEItRtakNMiJF05cBgeNhUXxt1Bnl9XveKMSfG6ksGkx+fFLzCP/Vjx\nnlcb1p6J0pm02NfIogVORTq109jm9X3aG9t5w9ym06lDQ6YwlYuHm31oku6iP2u+YNZ3N/7hleOZ\nrnIdHDIJQ0Ioc9uLx8baquVYlP8oGnU1XtsnfQ/FUT6dt7jb1tVPHcbWM6rHhE/eUY81t30NJX3y\nyTsaEXYOLkm4h5Tqe6JktePktjOOsE89Q4kBfUh6X2vqJDw0T3jbgMCHp9Pb1txtGklizZF3Jaap\npJMy8dwWi+mXHjQMSpnK4baeNJW05tJzdHcSSlOiLWiSf0SgSX6vWkrSQInqPkzK3qzeQYMxLvwC\nsv5VqdttMU6Lyjvz8UuFxcC/Le0VknCnpVGG4zIKVfJ58fOS0h8f5j+EBl0VACFZNb/f5wTtYYIR\nuxpt2cpSRZvbt6VZ/g8Hmze7xOc4UpexgymbHho6BQCQETyCPJbflg29yf0uQzTWJDN4NJIDMsm6\nNyZL6OqfAZpxJH0OeJ6apxE1Z0ddRZY31X7vEUs1u2kTWgwCjzPKLxF9g0ci2j+JmehbU7XU7f37\n1P1lzds2y1us+Pz2bJsq1r9rvvbKvgFgDdX4dFfDGnQYWj3eZ3bTRpggVM2kBw1DiDISw8OmId5f\nqHxqMzR5hI3qTlpX/bkNqq+8M89rac8zVYeat9hUKHm7Ad3iwidx/hYNXjh6jVf3a29sd7Rlh8cV\nEweo6qohoVMwSDORrNvD8kmV6b89n+IVVXUV2fwvmvV1DA/aHdVqy1DZJUx0+8nU6BM0HP01Y8kk\nfnFHjlfM6OymTVh47CZ8W/YalhQ+6fH+zKKrh+h7DG9wt2u1ZSjttMXyWaNKTre81ey1u6g7JtH/\nI+Z2N/zmffLpPyhnpl6hr5nkf0Ljwlk0SVVXMWms6CdTY3DoJEcvdSpn3G06uS22maRZYpPbNG6h\nn8Y+ksSspAAquS2xqWRBO83bdnycTIyYTQa8ua27nJaKG3kDfih/m6xfnvigw3T79SmWge5f1V+d\nUYnSJYVPMtiV8XYMfLOuSHyQIDNKO4/jz+qvHG5La2f9GvxR/RlZf7DPImiU4QzWaVudZ/znso4T\nKO/MAwD4ywJxdvRVxDw38gbsctMQOdy8lXw/qQEDEKaKBiCgWuL80wAAet6WmShWbYYm8nuXcwqk\nBw1FaoB3k9v0Z0sNHMBgDDzhbgtIkg0AhMTRXb3fIPikFkODR4m8zXWryPLEiNlkwmpW7K3k8TVV\ny7zC9u4u4nke66o/x2YKx/JfFp04Mx9rgPeaSp5os02v7WlcjyZdrZ2tpalBV4Ud9Zbmtnpe5xW+\n7J4mC2/b3CRZxslwftxt5PGegCbRm3T4vMSS2o5UxZPlHyveOx0fqdvIumkyIDQH9xaup9PYhm9L\nXwMPHn/XfoPqrhKv7Bdgx3YqmT8AYXLbE3PeyBuR3WR5/dDQKUyw43DL1v982r87aH31F3Yf32Wn\nabQU0fc2/TVjoZSp4C8PQGbwGGobzyd/6IlPuu+Bp6LvoWbGZiFAHgxAMKarteLwiY5EI/VksNy/\ndHdUi0/eVw82t30NJX3yqbuJMbc7c5mZWia5HeQzt3uqaENxR/2vzGBvSMhkchMgVdbJbfNvS2fS\nMmkJmiMtRjRepKj9iMMEL91M0hFv2yxnFQyu5KqZpFnByjCMCJtO1p0l3DbVriLmt0YRgXNj5jjc\ntr9mLIadTMOaYMQ3pa+K/einVbktuxl8xB29XnWYegeELvRXJD5E1leISG+3GZrxxonbyfrUqKsx\nMVIwV+mUys6GNR4lfWnjaETYOVDJ/DCWStTvcJO7TRtsw6jEM8CiSdxtKknzQHsFDoafXM0mtzs8\nS27zPM9gSdICBzLnm/1Nf7udmqORJINCJiLKL4Fp3OoumsTEmxg8xMTIS8jyuIgLEKYUJhjqdOU9\nKsW5ovgFvJx7I545cqlk7E9Pk96kY6qNLk24nyx7q6mkPXPbBCM21rlGVrnSn9VfkoS1WZuoCRt3\ntY+aMKIxWzNjs3pUY8k/qj9jrr+vDFpD7mF3NqzxuEnwmaxdFG/b3ENBZ+ryGI9lVnbTJoYB7O7E\nrbVMvInBkkyLvpYse2IW5rdlE3xRhCoOSeoMJKjTyXWizdDEVC/59O+L53nG3B4bfj5Z9qa5PTjk\nLLLsTe62kTcyk87V2mLUays92icgfC+0ud03aCQGaMaTdXrSxh3RSL0ZsTeS5X+Du+1T91IPNrct\n8iW3ffKpeyhCFUdmatsMTWjUW8qn6EGlD0vSczUgZBw0inAAQJ2uAivL3iDPjQqXjiQxKzVwAIIV\nYQCAJn0tKU/La9tPmrokqvsgRBkpab8aZTii/ZIACJzSEjscRZ7nCRsUAJOisKdkhj1/TFI5nhR8\nD22+bXCAJuF5Ht+VvU7WL46/m2mwaE/XJVvS22uqlqJO63kjQDFyt2yR53l8UvgYWZ8UeSkGhLhO\n8F+W+AD5TVV05WNd9edOt/+44FHUassAAKHKKNyX/i55rnfgYMT4JQMA2o3NHpUP02k2M897XMQF\nzPPupHzp5MuwsKnMc3RTyeNuNq3KsdNw1ZvM7XpdJUGyBMo1iFQlINo/CX2ChgEQmpHRfH8poieH\nzI1Vp0ZdTR7bUvcjdKYuyfs91roHdbpyAIBGEY4hoZYbVqVMhRkxlpu036t6RmPJOm0FvqaQRt+U\nvuZxSfKZrNzWXegydQAQUtsTI2eT5/Lbsr3y3ZxotUzwTo68nCz/JbIixZF4nseaqmU2j++s/90j\nHnatthzFJ88HSs4Pg0Is6IVwVSwmRli+ozM5va036fBF8f/I+lVJ8wT8F3U+/6nig9Px0SRLZ9J6\ntbqkqquI/AZUMn9MibagoDw1wszaa2U005OjnqhWW0Z6CIUoIxmMlSdVPtZIEo7jwHEcU7l4qIXl\nbv+HT62nRcda95DUvloehAf6fEAmq4627HAbGwewyEX6f+5N7vbRlh1o1Fczj9G4OndV1VVEJmY0\ninBE+SUyBr0niXOe55lwxvmxtyI+SOjD1G5s8RreyydbOckInTb9R8xtn3zyqTuI4zi7aBKtsRPl\nJ5sdceCQQpWq+9SzJOcUGBNhSTLQTa7c4W2bJeNkGKiZQNbNg8AjFG87UzPWrX27QpNUdhWSJo9B\nilDSvd6RQpSRxDTtMrUTc8uVDCY9iql0q6sKh4mRFxM0ybHWPajoLLDZ5lDzFpLEVXJ+mJ1wj8vP\nMTx0KjHw9bwOK8veFPX5PdGO+t8xe1sk5uzOxPrqLyWV3u5qWEuMWxnkuDXtJVGvC1KE4MrEeWR9\nRfHzDhPXexv/YkyW+9PfZ5richzHokncRGR0GtuYZM7YCMHc7hM0jJSztxgaGBSPGHUYWkmymgPH\n3CwBQsrGrGNt7pnb9tA90X7J8JcJkylN+lo06+vc2jdgiyQxJ/MZNIkbSBhrJMlZkZcBEAx/M/+3\n3djiFg5mC5WQGh9xkQ0DflbcLWR5W/0vhOV+Jmt58QJi/ADAibZ9Xrl5PlO1z6qJa4xfCkEitRga\nRF8fHEln0jKm3a1pL5Gy7UMtWzxCMRxr3UP27S8LIEiVLlM7djeuc3u/dBPBQSET4SdXM89fEGep\nkDmTG0uuqVqGaq3w/YcoI3FJwr0A2PT+2qplXmGYn0oVtB3CjbsycMGWEGYS0xPtpFLbQ0OmYHSY\npQeBtxrnWaeovZXcpivyktX9MChkIpSc0IeluOOo24GAAyevQ4DwnZhFT/54g7v9X5ankwHrqUaS\nZ0Vehlj/FNK3xAQj9lo1SBWrZn09+X3KOQX6U/czA0LGk7FDQfshNOvr3f34DCbNLKnjSXuiq4fS\ng4aB4zhmMt8Tc7uyq5CcR9XyIPQLHoVJSVPI8/Rx0111pk5CdcfP3YPNbYu97aNP+eRT91GSHXO7\nuCOHlHwnqNNdJkd9OrNlj3Uc7ZfETHy4o8HMQEkY4NODMqnNJM2i0ST2GKiMaRc82inuAnA8yeNK\npZ3HSQlttF8SghShTrcPUoRiZNgMsr6x1rYE/TsqOT8j9kaEq2Jcfg6O43Bd8nyyvrpikUeDaVeq\n1Zbjxdzr0GJoQElHLl7KvR537hvlsAkbLSNvZFLbF8TdhmSKee5KlybcB40iAoCQPFlbvdxmm05j\nG14/buEjT4q8BFNOpntpWZvb7iQy9zb+SSoRegcOIdxnjuOYSSOpaJKDzZsJWqB30BBolOHM83RT\nyYK2Q5LNJJ7nGQPTzAiXcTLmmlDc7n56mzbwUgMHkmX6e9/R8JtkJMzmuh/J9WmgZgIi/YRJBI7j\nMIVK5P3jBl6DLv+lkSRmJQf0Iw3DjLwBf1StkPwe3UlF7UexpnKJzeM/lZ8Z6dRTIRYHNBUcx6F3\nkAU5lechd5vGacX790ZiQB8MD5tGnncXqQMAa6otqe3JUVfgbKqawZ5JIlZ0otbM26bVExpL6kxa\nfFnyIlm/KvER0rh9eOg0pJxENnUYW5k+Dt1NncY2LDh6Baq1JegytWN50bNe2S+NJBkTMYtBLxxu\n3uoR2gsQxhXWSW1vmds0bzspoC/85QEYGGIJX5ibqUuRkWd53TQ6jDa3DzezyW1P1B1Tmd1ZBpOe\n4VVPj7keAJiJmd1uokkON28ly32ChkMtDyTrankgE0Bw1yjmeZ6ZcDeLrkx1V3QwKD1oKAABd6fk\n/AAIQSd38Sf0NXRwyFlQyJRIMk4hj50Oc7s7mr5SVNyew6AEzyT1YHOb1hn+C/PJpx4kmjdcetLU\nK/A1k/xPaVTYuSRRTD/myhR2JWvuNgAcpZLb/YPdS26bB2KAfQaqPdyCKyWpLQarPdSJPbHNJB3z\ntmk5Q5OUdBxjEsRXUoxpVxoXcQHBonSZ2rGq/F0Xr3BPPM/j1WM325Rynmjbh4cPTsPjh2Y5vSFd\nX/0FOb/4ywIxJ1XajXeAIhhXJz1C1r8o/h/0Jh2zzaeF8wkzNVgRhrl9PrT7Wx4SOplgmSq7Ct1i\nTO+wgyQxaxzF3d7eIM3ctjbYrBWo0JDfrAlGySzgOl05SR37ywIZ1naKl7jbTHI7YABZTg8aSiYB\nWg2NONyy1ea1zrTRDpLErKnRFjNvW/1qdBrbRe+3uD2HmCD+skCMpBj5tOj09pqqJWc0wuPTwsfJ\nRAE9wbeh9js06mocvazHqsvYwVyjzIYVXS3kaVNJuueEGdEzLfoa8hhtxkiRztTFvHZm7E04K+pS\nsr6tfrXNuVKMeJ7H3iaKt23H3O4JjSXXVC1FjbYUgICxoqumOI7DJQn3kfUfy9/rto0C3z5xD2Pm\n7mlchwZdtZNXuJbOpGXS+2PCz0OsfwpBe3WZ2u1y5KXIHh6kpCPHK2gVJrl98jw3nOLGu4MmyWvb\nj3aj0DMiUhXPVAimBw2Fv0wwO2u0pUw1xhl8uTjjtLtxHZr0QpPeSFU8hp48n48Kt5jbuxrXunUN\np5EkQyicB3nMC9zt/PZsVHYJTeLp/ke5rbtFcaud/Vn57bbmtkrmxyTQ3a062EeNX4efHL8ODZ1C\nHhO42z2nIfep1spjX+LmPQNx575R+KXCu9fWjg6v7s6ueqy5LfMlt33yqVvKXmK1UAJH2KczX4EK\njQ32wBPetll9gobDTyaUL1d2FSKnZRe5efSXBbjdqJQ1GmyxJHQiNVNkw0qWuy0uuV3QRh8n4szt\nCZEXkXLY4217Ud6ZT577vuwt0pNiXPgFkpLzMk6Ga5OfIOuryt91u1mfM62uXIQ9J8vbOXA4P/ZW\nZtC9s2ENbt0zBK8duxW1WrZ8X2fqwrKip8n6lUkPI1wVK/kzzE64B6FKATFSrS3Bmqql5LmDTZvx\nY/l7ZP3e9HccvodSpmJucqSiSXiex856i7k9lkpqA0Ka0ZyCKWw/TAx3MWJ421bNJM3qRzWVlJro\noJtQ9g0eCTln6WZPY6g84W7TjbTSAi3mtjUSZqsENEmTrpbhnJ4VdRnzfK/AQcSc7zJ1YEf9bxAr\nOtk6OnymDXrBrMlRV5BJkdLO4zjkxWTev6kDTRuxrf4XAMKx/HTm1wze6Pcq20R3d5OJN2F1xSI8\nefgi7GlwvzGcWUdatpFKjJSATET4xQGwqhbysKmkdTk4IFQJmM8VJ9r21mjUlgAAIABJREFUi55g\npbWl7mcy6Rjv3wuDQyYhI2gEMSDbDE1uJeaKOo6SibBgRRj5zNaaGXvTGdtYUkhtW/BY1yQ9xiQx\nAWBGzA0ET1PaedyjRoSnSmurPsO6araaxAST2xMmZh1s2kQ49AnqdGLkDg71HlvYmrcNAFpTJ6pO\nmnueiG4maZ4Upidp9jX+KdngpI8lM2/bLDmnYCoTaZNQ70HA3WeMS9OfVCPJadHXkXFOf80YcizX\nasvcmsQ/6IC3bZY3uNubai1jkkmRl5JQQJep3e3zq+mkCWfvOgTAY+42z/NMM0lzOCPevzciVeK4\n277fuUV7G//C/X/dREIIiwrmeaWhqFn1p67Il6jHmtu0fL9Zn3zqPmLM7U57ye2BNq/xqedpHIUm\nkUGG4aHTnGwtTkqZikkBfFv6GlnuGzzKhmcrVvHq3iQV06ivYbi3BpOeScb1pQxAZ6JRDHTqyZmY\n5LZIoz5IEcpMHJhTqE26WqbU+cqkh0Xtj9aUqCvJTWeboQmrKxZJ3oczlXWcwKJ8C/P6ysR5mNf3\nU3wx+gRmxt5E8GMmmPB71RLcsKsPlhQ+RUz2VeXvkcmNMGU0rqL42VKklgfh6qRHyfoXJS9CZ9Ki\ny9iBV4/fTB4fG34+pkdf73RfE2g0iUT+c377QcLfDVaEob/VRIpaHojhVCPI7SLRJK36RuSdvPGQ\nQWb3xgkAMqimkrRZLUa5LSy6h1ZKoCW5XexmcpvneabE3Po6wnC3638WbSxsqlvFIEmi/BKY5zmO\nY5qF/VMrHk3iCklilloeiGnR15L1387AxpI8z+PjAksFxPSYG5AeNJRJq/5Ssahbp6sadNV44tD5\neOvEXdhW/wuez7nKY9azNW/bLG8mt/MoUyEjeDgAoZ+AmdcPuJfeXks1kpwRMwcyTgaO4zAp0pLe\ndgdNQqdah4dOYybCaIWrYs7YxpK/VS4mzYfDlDG4KP4um23U8iCcF3sTWT9V1VHuqrg9B++cuJus\nx/qnkmVXzZddieZt0xVKjIHnZjoVOFkdQP3OwpTRZNkbkyT2ktsZwSMISq5OVyEaR2cWbW7bm4Cm\nr9veRJP4JE7thhZsqf+JrJuRJIAw+UBPbuyW2J+j09iO4217yTqNuKEfk5209fLbstGqb5T0HgB7\nvp4UeSlT7SqVu00PsZr1deR8p5L5M2hAT7nbxR05pAFmsCKMTAxzHMekt7NdTLSeboOb53l8U/oa\nFube5DU8klTltx3Es0cuhZ5CPnUYW/FRgXv3TqdLPdbcpv8w3mdv++RTt1G8ujdpZlTdVYwuYweT\n3PZhSf4bmhg5mySKh4dNQ7AyzCv7pQf4m+p+IMvu8rYBIaXsiIFa2H4YOlMXACDGL0UUsxoAM7gT\ne5PDVjiIS24D9tEkP1d8SD53RtAImyS9GMk5Oa5Jepysryx702uNvYy8Aa8cm0PSW2mBA3Fz2vMA\ngCi/RDzWdyk+HXGAYRlqTZ34ouRFXL8rHd+Vvskk425MeRYBimC3P8/F8XcjTCn8b2u1ZfitcjGW\nFT2D8s48AECgXIOHMha5ROuMCZ9Fzn85rTsllW/TqeBR4TPtTtbQk0Zize3s5k1knCTcgIfY3a4v\nZW5LTW7nUFz6TI2VuU0lt4vdTG5Xa0vQaWwDAGgU4eR/ZRaNhKnoKhCdnKKRJPRxROvsaIu5vaP+\nd1EVDNVdJeRmVc4pMM4qhW+t8+MsTPeNtSvRZmh2+R6eKq8tG88cuQwry97yGIWyoXYl07j25tQX\nAABToq5AiDISgPA/FPub/be1u2Edbt0zBLsaLbzUVkMjY2a4I0c4oLTAgcSoKOs8Lgl3Q8vIGxlz\nnE7M0RMmf9V8Jel/XKstYypqzo2dQ56jze0tdT9KnrCgE7XD7SBJaF0YfwdZPlMaS+pMXfiKTm0n\nP+awz8zshHvIJO7Oht9R1nHC7nb/trTGTjyfcxW5PicH9MO7Q7eQMd2Jtn0oancfMbWrkTa3zyPL\nNI7BE9RAQfshYohpFOHMub3IQ2Opw9BKJqEVnBJxaqHJqpyTM8e4lCS+kTcwxh9t2pnlayp5erWp\n7gcypu4VOJi5ZwCE6iyzdjdK424fbdlBsCCpAQMQooyw2SZQoUGfk5OXPHjJFV4lHcdIQEAl88fo\n8JlMWCjHg6aSdGq6V+AgZuzaXzOOaYbZom+QtG/6Gjo09GzIOIsDSKNauntTyT+qV+Djgkextno5\n7tg7Et+Xvf2voqhqukrx+KHzCPrI3GsIEMYH+xv/cfRSuzqdvP4ea27TDSV91rZPPnUfqWR+iFcL\njYB48DjasgP1usqTz/kzHDmfeq5i/JPx4sBfcF3yfDyS4b1ydJq7TU9sZmrc422b5QhNQpt2/TTi\neNuAUDJnHtDVaEtdmhdthibSDVzBKZGkzhD9XuMjLmJK0AvaDuGnCksDtyuTHnabdz4j5gZSutio\nr8bvFLLDE31T+hqOnGTRyjkFnui7gsGRAEDvoMFYOHgNXh+8nuGiN+lr8VHBw6RkPlHdBxdQfFZ3\n5C8PwDXJlsaUy4uewfdlb5H1u3q/Sb4HZ9Iow0nqhgcvCWOxo8Gy7dhw+2Yo/fiBpn+I4etMrnjb\nZvUJGkYMt5KOHHQYWl3uGxBQDseopLd1cjtB3Zsw+Gu1ZW7hbWgkSWrgQJvfs0rmJxkJ4wpJYlZy\nQD/y+9PzWmyt/9nlvrfUWUzR4aHTXDaHzQgaQSa0tKZOj8v+XalWW4ZHDk7H5rpV+DD/IXxV+orb\n+9KbdFhcaEEYXZb4AGL8BXSFSuaP82Mtxv3PFd2rsaTepMOi/Efw6KFziRlGi04vS1WboZkcFxw4\n5kbcT65GYoBwjufBu53kKus4TgzICFUcM/k6Nvx8MuFT1nkCx6kKJFdaV/05ub4OC52KWP8U8tyA\nkPFkcqlRX4MjzdtE79dg0jMmhD3eNq1hoVPPuMaSv1Z+ijpdBQAgXBWLi+LudLhtgjqdSS7/1E2O\njw/yHyQVlyqZP57N/A5RfgnM5Op6CtEgRZWdhWTCXyXzZybeE9TpiFAJ6J52YwsK2g7a3YcrWU+g\n9KKMSE9Tk6Wdx8lygjqdMfIYNEmTeO72idb96DAK19tIVQLi/XvbbJOpGUPeq6jjiGST8L+q6q4S\nbKhdaXcMrtWK3w/9e58Rc4PN86PCLBWU2U0bJU1Y0rztwaG2vG2zPEGT0I0kR4fNhFoeyNw3HW31\njrndmxqrA0JlWkaQpWE53ThTjJyNX4eGTCHLB5s3ddvKsA5DKz4ttASF9LwWH+Q/iEcPnmuDWxSr\nHAk5kTZDEx47dB65LgWrNHhryD9MVeI7efdI6qFxOpPwPdjctsiX3PbJp+4lGk1CJ+NSAvo7LEH1\nqedpVPgM3Jr2IqL9k7y2z/6asSQZa/24J6IHZHkUA5XmbfcPFsfbBgCFTEluygHBhHAm+oYrJaA/\nFDKlk61ZBSo0DJrkhZxrSNObGL9kTI68XPS+rKWUqXBVogU38G3pqzCYPIA8QhgILy+yNH7MSlmA\nPsH2uauAcMP48fC9eKLfCkT72f6Wbk17SdL35UgXxd1JeNothgaCqxgReg5mxd7s7KWMGDSJSO52\ns76eJGc4cBjtgFEf459MTFA9rxPVuIo2cIc64G0DgsGfepJlzYMX3dCrtOMYuTEPU0bb/I/knAKJ\n1GSN1HJtAAySJJXibdOi0SRb61wb0JvrfnSKJKFF3wT8XeMaTbJFJJLELI7jmPT2b5WnDk2iM2nx\nzJHLyDkCABYXzsffNd+6tb/VFYtQ0VUAQCgbvpaq9gCAC+PvJJMmexrXo9TFufDfUlnHCdy7fzy+\nLXudPBamjMGjfZeSAM3exj+Z5m1SdKh5M/l9pQcNtUnjeQNNQh+jfYKGM8/5ydWYEGnBeoidMOF5\nnjH1Z1LoDEBIqE6k9ktXULlSTutOdJkE0yfWP5W5RtqTjJPhgrjbyfovlR+Lfq/TIa2xE1+VvEzW\nr0l63CFr36xLE+4ny2urlomeVDxV+rvmW+Z7vrf3OwSTNp0y9f6q+dKt5CGNJBkWOpX5fjiO8wpb\nePfJqgMAGBk2HWkBFowVfS1xR/T1i24cDghjBbMONG0QPVY60LyBLA8LPdtuGEEtD2SOcakmYXcW\nz/Neb6RsMOnxZcnLuGFXBp47eiXu2T/W7cqPWm0ZGUdx4DCVathrVrR/Eml0red1krA6dBJ/sANs\nHOCZuc0gSU42Bs4IGk4mTEo6ct1CnQBWvO3AoTbP03+TFDSJkTcyk6HDrcztBHU6IlXxAITJME8R\nX6dKX5a8xOAuzdrb9Cdu2TMIG2u/l7zPapFFoTqTFk8dnk3OewpOiYXDfkSvoEG4q/cbUMuDAAhV\nld+XvS35c5wO/UfMbZ988qk7KUltMbfpC6qvmaRPnkotDyJcUbPi/NNE40IcKT3IfnLbGUvYlZLs\n8OcdKZ9KKLlznDBlt9TN22WJcz02fs+Pu5VpuLi+xr3EFiAMtF7KvQEGXrjpywwewySmHUnGyTAj\n5gZ8Pvo4bk9bSJr3jAmfhbMi7SdupcpPrsZ1yfOZx/xlgZjX91NJyXe6ueGexnWibqh2N/xBjLD+\nmrEE5WBP4yIuIMuuMA9NulqSwJNzCgyyw3KkleEGmoSpbggebfe7MjdlBNzjbtOTP+YbSGuNCT/P\nCgljezNBi554nRx1hdNtaXN7T+M6p6m5Zn0duYHjwGFi5MUOt6V1TvR1pALjeNtepw2SPNF7efcj\nl/qfmfVK7hwckmiYtBmasaL4ebJ+ffJTNgiqWP8UjKV+sz9XfCjxE3tXgnn7GW7bO4zhnI4JPw9L\nRh7EebE3kQQmD57pXSBFjnjbZtHVKO42laT7QfSx05hxWpTFhPmn9ltRZuThlm0o6xTwGIFyDSbZ\nmZyhz7mb61aJNqboybgRYdNFnVdnxt5EKj+OtGw7bbxSMfql8hNSqRihisOFlDHvSCPCziEmabux\nBX9YNXD8N1XemY83jluqoM6OuoqpihoTfh40inAAwljAHTzGrgb7SBKz6OSqO9xtnamLMdBGhE1H\nSiDd1DjXowl6upmkdZPuBHU6abjaYWwV3buCnoAeYgdJYhaNJjnU0r25213GDlR2Co3ft9f/it8r\nl+KrklfwQd5DeDHnejxycAZu2zsMV2xPwIzNfrhwaxg+zH8YTbpa1zt3oZyWXbhj30gsLpwPPS/E\nswvbD2Nx4ZNuJU7/rP6KBCmHh05zOBFOo0l2iUST6E06HD1ZxQg4N7cHh04iE68nWveJroKr7ioh\nv0U5p8C4cOF67CdXM9chqb1WzKLHKvauQ3RTyWwJ5nZ+WzZaDYLhHq6KtTneOI5jjpfuiCYp78zH\nyrI3yfojGUtwbdLj5P/YamjEgqNX4JXcLLeqGp3JxJvwSu4cZiLksb7LkekvjEei/BKQlfIceW5F\n8XOo6Sr16mc4FerB5rZlQPTvEWt88sknMaIvQI36GrLsM7d98obogRIgMN08VVrgIHJdKe04Bq2x\nEx2GVmLEySAjvDuxoo8D+obInthmkuJ522aNj7iQGGNmBco1mBV7i+R9WctfHoDLEx8k60sLn0ZO\ni605JkbLi54l5oSfTI0n+q2Q1AhUJfPHNcmP4puxxfhw2E68OPBnt5Er9nRB3G2kAzsA3NHrVaaR\nlhglBvQh/3utqRP7mv5y+RoaSUKXqduT+cbE/DpnhhWdCMsMHkNSGo7Uj2qYekzkjU6uCHQPbS64\nw92msSSOmhJrlOHMjaEz479ZX4f9lKEw2QGSxKw4dRqZ3DLyBqeN9LbWraYmKsaRagBX0ijDSaIK\nAH6v9B7OyazfKpcwjfmyUhaQ36qe1+KpwxcTzrwYfV2yEC2GegBCEpduIElrdrzl8bVVy91mTHuq\nNkMz/pd7HRYeyyIJYiWnwj2938JLA39FmEpoPEenlf+oWu5WQtUVDsjbye10O6bCiLBzCFuzVlsm\nitNKp7anRF9llxc9NHQKQe3UaEuZSQJnYsztUOdIErPCVNFMAv2XU9xYstPYhoK2Q4SBK1Zdxg58\nTeF9rk1+wmVqGxAmbi9JuI+s/1TxvtdTrGKkM2nx/NGrSBVOvH9vPJzxCXN9VcpUmEL1IFgvsbGk\nztTFHBej7ZjbdDr1YPNmycfeoeathI2cqO6DWP8UBClCSEWRgddLOsdZiw4qJAWwyW2O4xiOvJjK\nKoG3bZkksMfbNou+vh3qptztZn0dXsnNwgVbQ3Dtrl64e/8YzD98IV47fgs+LXwC35e/hT9rvsSe\nxvXIazuAOl0FDLwe7cZmrCx7E9fsTMOSwqfcShF3Gtvwft5c3Lt/HDOmNuuH8rdxoEHa98bzPNbX\nWH7n0+0gScyi0SR7RDaVPNG2D1qTEICI9U91ir8LUoSShoommESn92lM2rDQqcwEdCZVkSq1qSQg\nVKuUnqxm4MCRKg9ag0Imkvur4617ReH0ANtrqL2xPtNUkhrvdhctyp8HPS/gPvoHj8XM2Czc1utl\nvDVkA5kIA4A/qj/DrXuHSGapO9MnBY/hn1pLRd7taQtxTsy1zDaXJtxHxtRdpg58kP8gxKj51LeF\ncagebG7T8mW3ffKpO8l6dtUsXzNJn7yhQVbJBk+RJIBQ8pmo7gNAGDQWdRzB8ba9JK2RGjgQanmg\npH3SJauuUAwFbjaTNCtQobFJQV0QdwcCFRrJ+7Kni+PvJmnpOl057t0/HsuLFkhKQB1q3oJvSl8l\n63f0eg1JAeLZ4rSCFCHI1IyWZIyLkUrmj2f7f4cBmnG4OulRXBTvmJfqTOPCLWzSbfW/ON3WyBux\nu8GS8nHE2zarn2Y0SXY36KpwvNWxsUQbuMOcIEnM6htEJbfbxCW3nfG2zWKS2xIbkZl4E5P2TnOA\nJQGACVRK2hkSRkCSCHzGAZrxonjqU6OvJsv/OEF40EgSe6lXZ6L51OtrvvBqA73clt1454TFZJ4a\nfQ1uTHkGLw/8jVRmtBjq8cSh89Gsr3e5v1ptGb4vt3Dpb0l9ESqZn91tR4SdQ86v7cZm/Fn9pSd/\nCpr19TjasgMlHcfQrK8Txdo82rIDt+8dxuA5ktR98cGwHbg8cS7TqGpixGxyvqvoKpCcUG3W15E0\ntgxyu2m83lS1UEH7QckmHs/zyKPMbeuKJkDAY02JtlT1uEKTdBrbSVNiADgv5ia72ylkSqZCZVOt\nazRJu6GFMU+Ghznm/1vrwjhLY8n11Z+jy9gh+rVSVKstw027B+KWvYORtbs/1lV/LtrkXl2xiFSL\nRKoSJPWBODfmRsJHL+nIFWWKelufFjxOJikUnBLP9P/G7vhhRrTF3NtYu5IYyWKU3bSJMOIT1X2Q\noLZlS6cEZJLrW4uhXnKlD83bHhE2nSzTk6KFHe6n/50lt4X3lMbdPt66j5h9UX6JTlE9AzWWyqtj\nrXu6VYNVnufxd803yNrdH39UfyZ5csisLlM7vih5Edfu6oUvil8UjenZWb8GN+0egB/K3yGTy34y\nNe7s9RppTM6Dx8JjWaLNVUCoqqEDGc6u6YNDJ8FPJkxolXYeR0Vngcv9s0gSx7xts2jOtFg0CYMk\noRoCA+z9U44b3O2C9kPk+05UZ9gNUAQrw8j9vwlGHGnebrONPYnpF8OgWpq6F3d7b+OfTFPqe9Pf\nIeOMIaFnYfHIg5gefT15vqqrCHMPTMaSwqc8xj/+UPYug1ybHX8Prk56xGY7hUyJuemWarpNdT9g\nl4iJmbo6jz6eR+qx5rbMl9z2yaduK+s0g1m+5LZP3hBdmgkAA7yQ3AaAXpTZkNd2ADktFt52pkY8\nb9ssJrnd6Ti5zfM8Chlz273jhDYx5JwClyXe72RraQpShGB+v8/JwNUEIz4rfg73HZiAEhepdEBI\n1LySO4dMFowMm46L4+/y2ufzpgaGjMf7w7bhjl4LGcNLiiZEWoyf7fW/ODWuclp2osUgYC4iVHFM\nmag9yTk5k+7e3uA4oSyWt21Wr6DBBAFQ3pnnMj2lM2mZklQ6+U0rJcD95HZlVyFJNoUpo10gWyyT\nCnsa1ztMCG+osRh4U1wgSSzbXUnSR/ub/kaDzhZ62GFoxR7KXBHD26Y1NHQK00Bvk5OEuBQ16Wrx\n7NHLSIl2r8BBmJch4Hbi1b3w4sDVpKFraedxPHPkUuhMzrttLSt6lhhbfYKGM+a/tWScDBfH303W\nf6r4wO10amH7Ydy4KwP37B+HObv7Yfa2KEzfpMTFWyNx466+uHf/BDx5+GIsPHYzFuU/iq9LXsXH\nBY/hvv0TUdlVSPYzK/YWfDxir13ev59cjWlUwklqY0m6PDpTMxoBimCbbSJUceS33GFsRVVXkaT3\nqNYWk5LtYEUYYvxS7G5Ho0k21K50etO8uW4VSe4mqfs6nTim0SSb6n5w+f/MbtpIJpT6BA1zehxb\na1jo2aTJ3qlqLKkzdeGZI5ehWlsMQGjC+XLujZizOxN/VK1watZ1GtvxTelCsn5t8hM2DZKdKUAR\nzFQLrCp/142/wH1trVuN78stvNU7er2GvhSiilZ/zVjyv2g3tricvKXFIknsVyhxHMeYfAckokno\n8+9IytymcVbuom1MvAllVENJa+Y2IGArzDrSst2lOWt9jXZWiRaqiiLjSgOvdxsj4W3Vasvw5OGL\nmH4vgGDWZwSNwOiwmZgRcyOuSpyHO3q9isf6LsfLA3/DouG78c2YYqyZ2I4XBvzITEC0GZqwpOgp\nXLurF74rfdOhkd+oq8ELOdfi8cOzSFN2QPjfLx15GFclzcO8votJpUlFVwE+LnCNwjOLbiQ5MfIS\nu+dys1Qyf2actbvRtUl4SCRv2yy6MbEYbE+jroa8hz1MGn2OP9qyQ/R1WXey9yA9/nM2dh1CHdNi\nuNsGkx7ZTZbtrHnbZiWq+1BNaJtPKXdbypDFyBvwft5csn5uzBxkWlU2BilCMD/zczyd+TX5fZpg\nwhclL+LeA+NF3VvZ08baH/BBvuW9J0bMxr3p7zg8twwOnYQZMTeS9Xfz7nU5/jud6rHmNi1fbtsn\nn7qXQpQRNjcuGkWE6PJsn3xyphBlBLlpSVCnM+XdnohuhJLfls2whDMl8rYBdpKntOOYQ4OzWltM\nDAWNIoIM1KRqfMRF5Kbzkvh7RaVRJe0/8kIsGXEQgzSWyYXc1t24fe8w/Fju3Kz6KH8eaToXKA/B\no32Xum0cnwnqrxlHcAD1ukqn6eqdDb+T5THhs0RhVsRwt+u1laRiQMmpRE0CqWR+TOWAK9xAfls2\n4acnqNOhUYbb3S4pIIM0FazsKpCUOKORJKkOkCRmJah7ExNDZ+qym4C0RZKIa7ga5ZdAJtZMMNlt\nArSrcS1lIA+2m0x0Jhknw3lU89I1VZ6jSYy8AS/kXIMarcBSDJSH4PkBq5hKlP6asZjfz1J6fbB5\nE14/dqvDY7qg7RD+qFpO1u/s9ZrL43lmbBb8ZQLioqD9IA63SG+I1qpvxFOHZ5PJILN48Ggx1KO0\n8ziOtGzDtvrVWFu1DN+WvYZPCh/DN6WvEmM1UB6CZzK/xSN9FzutxqFTyxtrV0pq9LdPROKM4zgW\nTSKRu80iSYY6PG8MDJlArgUthnrG/LMW20gyy+m5aGTYdPjLhO+vrPOEy0Z9e5tY3rYUWTeW/NXL\naBKe5/H2iXvssujLO/PwyrE5Tk3u1RUfEQRflF8i0xxWrC6Jv5cs72j4DeWd+ZJeX9h+BK8euwXv\n5T2A7fW/iUb/VHeVYOGxLLI+PuIiXJbgeGKc4zhMj7GkDaWgSehmkvaQJGaxaBLxjN4mXS2pZpBB\nzhiN9LWDvqZIUY22hEzohSmjbfoLAAJGx3xcG3mDS8YwPRFGp3IdiR5/nW40iYk3YXXFImTt7s9M\nskf5JeKlgb/iu7Gl+HjEHiwcvAZP9PsMd/Z+DVcnPYKZsXMwNmIW+gaPRIx/MvzlAZgYORuLR2Tj\nqcyvSJUPIFyvPyp4GNft6o2fyj+E3iS4qubeCVm7M5mKFI0iAk/0W4FXB/2BeHWvk58nAfelWyaM\nfq74UCQyxoi/ar4i6zOcIEnMYrjbDc652ybexGAorKtS7YneJrd1t8sU+rb61UzjbOv78Hj/3mSs\n2mpoJP0WxCqPaWrsuDE8w9IXcUzntu5mmg/HqdPsbtddudurKz4m10S1PAi3pb3scNup0VdjyYiD\nTHXlsdY9uH3vMHxW9DyymzaJHn8cat6KF3OuI0GiAZpxeCrzK8g5udPX3dHrVVKtVt6Zh29LXxP1\nfqdDPfbOkf7DeJ+97ZNP3U7JarZcr1fgIK+ycX36b+vZ/t/h6cxv8PaQjR43TDSLLhPPb89mWcJu\nmNv0JI/W1EnMJWtZN5N09zhRywOxeGQ2Ph1xAHf1fsOtfbhSnDoNbw3dgNvTFpKEr9bUiXfz7sVj\nh85DnbbC5jU769fgl8qPyfoDfd73uvHe3STn5IwB7QyRQfO2x0Y4R5KYNSrsXIJkOdG2D7Xacptt\naAN3gGa8KP4rACax56qppNhjRCXzR9zJG00ePEqp9Jsr0aZZqhMkiVk0LsHe984iScZJ+i2eHWVJ\nJ2+otUWTbPYASWLWubFZZCJgf9M/HvFhAWBx4ZMM9/3JzC+RoE632W5y1OW4o5cFG7S+5gssL15g\nd5+fFD5ObpjHhJ8nCjERpAjFtJjryPpP5R+I/RMACEbDCznXoKJLMP1UMn8kqNNJ4kmMBmjGYfHI\nAzibqnJxpL7BI8lESZepQ1JaeL+LZpJmMdcciYmzE620qeC4H4SMk2Eq9bt1hCap7Cwk5wwZZEyS\ny5785GrmfLWp1nmVgTu8bVozY7NOWWPJXyo/xpqqpWT9ltQXcVPq88xviza511ZZsAtCatty3FyX\nPN8hnseZEgP6ELQYD1708aEzdWFp4TO4fe8wrKlailXl72L+4Qtw8dZwzMueju9K30Bh+2G7E1UG\nkx4v5FxDKgCi/ZLwWN9lLscgtLm9s2ENmvWua9QrOgtIBZufTI0Nk7n+AAAgAElEQVShVALVWtbp\nVLFp0n1NfxFPIFMzBkGKEPIcjbNy97dD4+UcVagCVmgSJyaqwaRnGkM6422bNZBuKulFPq9UlXYc\nx4PZZ+OtE3eRcAYgIOyWjTyCcSLHMrRknAzToq/B8lFH8WjfpUw1Sr2uEu/k3YMbdmXgx/IP8MjB\nGVh4LIuZ6Dwn+jp8NioHM2JusPkNT4++HhMiLKnlV4/djDaDc3Dwvsa/SIPYMGUM8391JJq7vb/p\nb2LG21NR+xFy7IUpo5Gkdo3pC1FGkOpOE4w43LzN6fYMkiTqUpvnOY6zSW9LEZ3c7u0kuU2n0nNa\ndrrEGYlBkpjlDqrFU2mdBJub9fVYVvQ0Wb8++UlE+DkPLUX7J+H1wX/irl6vQ8mphPcwdWJ58bOY\nmz0ZF2wNwZzdmXgp5wZ8X/YODjVvsZnALOnIxZOHLyQBi0R1H7w4cLWosX+4Kga3pP2PrH9R8iIq\nOwuh94yOckrUY81tuqGkz9r2yafuJ2sWXZqdJhM++eSughShmBp9FSL94r22z3TKaMhp2YlabRkA\nwF8WgFSqIZ4U0WWrjppKMkgSN5pJ0lLLA5EeNOSUpqLlnBzXJD+Kj4bvZspIdzf+gZv3DMQ/FPKh\nWV+P145bmlqeFXkZzom+Dv8F0YgMR6XbtdpycnOg4JSiTZ9AhYZJt9Hpb7OkIknM6ss0lXRubucw\n5rZ9JIlZDHdbAkeVNiLo0nJHornb2+t/sWEwbqRMyilRrk1OWpOjLifG88HmzeQcAQiIlp31lokK\nqUgSs6L8EpiS/d8rlzrZ2rk21n7PGG9ZKQucmg5XJc5jErIrip/HH1UrmG32Nf5Nfm8cONyethBi\nRTeW3FT3A2EUi9HSwqeYMu/5/b7AF6NP4JcJjfjzLD1WjavGspFH8PaQjXh+wCo8nPEJbk17CVck\nPoTzYm/Gg30W4Z2hm0Q3iOU4DudRqAja/HSmWm05MfGUnJ/Tigk6uU2bBGJ0om0fWbbXTJLW1GgL\nmmRL/U92mdXrqi3/51Hh54q6ttL8VmdNVmu15eSYV3J+NmgxMQpTRTPHlLcaSx5u3ob38ixJ5enR\n1+O65CdwY8rT+HpMkV2Te+GxLNy4qx/WVn2GH8reISiGGL9kpvJCqi6lEtNrqpa6TGUeaNqIW/cM\nxeclL5AKGrP0vA57m/7ERwXzcPOeQbhyRxJeO3YrNtSuJLipZUXP4EiLYI7JIMdTmV87rL6hlaBO\nR/9gwRAz8gb87aQHgVl0antY6FSn2Ja0wIHkO2/UV4ueDHWEJAGA5IBM4h2Ud+ZJYoWbRWMCkhz0\nFgJYc3uvE+728TZLc70Yv2TE+dtPp9KiTcIjLdv+dcawwaTHVyWv4JY9g5lUfZI6A+8M2YS5fT7w\nuNeLnFPgvNib8Pno43gg/QOmmrFaW4x38+5lvtdY/1QsHLQWT2Z+gVBVlN19chyHhzI+JinlGm0p\nPsx/yOnnWF9jQZJMi75GVI+XRHUf8n/sNLaR48ueaN72oJBJooMtDGfaiZnbZmjGvkbLxLajCXfG\n3HbC3baeYzLyRqZxpzMsSbgqlpj3el6L3BbnSB1J5jY1KXSw+fRztz8rWkAmLeL9e+HyxLkuXiFI\nxslwZdLD+HD4LpuxLg8eJR25WF/zBT7In4v7D0zCBVs0yNo9AC/nzsH3Ze/g0YMzmcmShYPWSsJ/\nXRR/F0nf60xdeD//AdGv/TfVg81ti3zJbZ986n6yNrd9vG2fursiVQlk4Gvubg0AGcEj3W5cSB8H\njppK5rezye0zRelBQ7Bo+G5clTiP3DS2GhrxfM5VeDHnerQZmvDOiXuY5MuDfT76z1RwjAqbQRIY\n+e3ZqOoqttmGZpAODjnLKc/RWq7QJFKbSZrFJred34DktoivbmC42+3iudt0CXmaCyyJ8DlGIUwZ\nAwBo0tcy7PxmfR32UalasUgSs8JU0cyNFp3m3d/4N9qNLQCAOP809HajMaxZsyiswR/Vy91qzlXU\nfhQLj1nM2bHh5+OGlKedvEIwAOb2+YBJn71+/FZS6mviTfi44FHy3LmxWeglYeI6PWgIaYpm4PX4\ntfJTUa/bULsSX5W+QtavS56PyVEW5rOcUyBMFY3UwP4YEnoWJkVeggvibsN1yU/g7t5v4NG+S3BR\n/B2Sz+PnxFwPGYRy3sMtW1Ha4dpkoyeVBoY4r5iwrhaSIrHl4ObnzcZCp7GNqRYBhP/r2urlZJ3m\nPzvT2PBZUHJCSjm/PdshSoM2WAaGTBBdRWIteuJlXdUKjxtL1mkr8OzRy4gx3CdoGB7K+Jhco4IU\nIcTkvjn1BcbkrujKx8JjWVhS9CR57LrkJ91KbZs1MmwG03h1nQPkR6u+Ea8fuw0PZk9h+nkM0IzD\nlYkP2z1P1unK8XvVEjx39ErM3haJu/aNZo6pW9L+h0EhE2xe50jTKUTDnxSX2JF2iUSSAMIEOm3i\nikGT8DzvsJkkIEz8x53sZ2CC0S2ebSk1hku2w9s2a1DIJFJlUNh+2OEkHo1QGBI6RdTYKM4/jWIM\ntzDhiFOt4637cNf+0fi08AmSDpVzClyXPB+LR2ZjcKhrrIYUKWUqzE64G1+Ozsddvd6wMepkkOGK\nxIewdORhjA4/18FeLApXxWBuH0vzvDVVSx1i3TqN7dhMVaNMF4EkAYRr6CiRaJJDVua2WInlbu9s\n+J3cy/QJGuZwYpdpKikhuV3eeYI0iI1QxSFcFeN0+8EiudtaYyeTSHc1fqW5222GJhRQ1bBSVNKR\nixt39cMte4a4DHY4UmH7Yfxc8RFZv6v3G5L6LwDCOOnjEXswL2MxZsbehF6Bg8g4hJYJQrP1ddUr\n8EH+XNIvwl8WgJcH/UawPGIl5+TM8bGt/hdsqxPfU+Hf0n/E3PbJJ5+6m6xTDWeSaefTf1McxzFm\ng1nu8LbNEtNUkm0m6Vly+9+WSuaPO3u/hjeH/IMYv2Ty+J81X+K6nen4h0I3zMv41GGqpicqQBHM\nGKHb7aS3d9RLR5KYRZvb+xr/ZDjW1V0lBN/gJ1NLaoiaGtCfDMartSVo1NXY3a7N0Ex+0zLIXRps\nyW4kt428gZkUShOBJZFxMoxnUvMWNMmWup/cRpKYdXY0jXj4hixvrrcgSSZGXuLRJM7Y8FnEoK/X\nVWJn/RoXr2DVZmjGM0cuIanAeP/emN/vc1EVHXJOgWf7f0eu2QZej6ePXIKSjlz8U/st4bCrZP64\nOfV5SZ8LAGYnWNLbv1R87NK4L2g7hIW5WWR9TPh5uMmN93VH4aoY5jhbS3HGHUkMb9uslIBMYoJV\ndRW5LJM3q0FXjTqdgIDyk6mdIhIA4do2LdrSINMaTZLdtJE0tAxWhDHHjzMFKIIxKnwGWXeU3qZT\nltaJWikaFno2Qeq0G5vxaeETThtkOpPepMOCo5cT41GjiMDzA1bBXx5gs22QIgQ3pDxFTO5ghS1r\nOcYvBTNjs9z6LGbJOBlmU+ztH8vfY5AcPM/j75pvMWd3Jn6rWkweD5RrMLfPh3h36Bbc1ft1LB15\nCN+NLcUjGUswOeoKG3SPCSamGeHIsBm4OulRSNHZ0VeSyaKjrTtQ1uGY1aszdTFJzDEuzG3AKp0q\nonFeaedxgn4LkAfbHbeleogmocdwzpLbankgBmjGk/W91OQOLYa3LQJJAgjH8iDG+D/13O0uYwc+\nKXgcd+0bzVSYZASNwKLhe3Br2ouSDTwp8pOrcWXSQ/hqdAFuSX0RUX6JGKiZgA+H78Tdvd9w2jvB\nWmdHX4mzo64i668fvw0t+gab7bbU/UiYzykBmS7HN7RGh7k2t3meZwxeMc0kLdtaTOLc1l0OJ/lo\nc56usrFW3+BRJKCS33ZQ9KShdd8HVxL7uz3Ssp1MniQH9HNZRcRxHHO+ONC8weVnsRbP83jj+B0o\n7TyGgvaDuG//RIeTi8728X7eXDLGHB46jUHhSJFK5o/z427BY32XYsnIg/htYgveH7oN96W/ixkx\nNyI1YACpIqQlgxzP9l/psCGwK/XXjMX5sZZwxXv593s8iext9WBz23LTYL9Fl08++XQ6ZZ3cFlNO\n7pNPp1v2mlP207hvbtNYEnvJbZ2pi6QBOXCimMLdUUNDJ2PxyIM4N2YOeazFUE+WZ8XegvGR4gyT\nnqRxDkxWQMBY0KYPjaIQowR1Ovl9dZk6mKQ2vTwoZCKUMpXo/SpkSuZGzlGChX68d9Bgl2nMVDq5\n3SEuuV3emU9uciJV8aL5yjSaZGv9z2R5Q60FmTM56gpR+7LWpMhLiCGZ27oLFZ0FMPJGbKv7mdnG\nEylkSsYoe+vEXXg3735sqfvZpQFq4k1YmJtFSvn9ZGo8P2CV3eZnjhSo0OClgb+S5lNthiY8fmgW\nFhfOJ9tcnjDXrcmBsyIvI8Z9na4cW6jvzVqt+kY8feQSkg5LUKeLao7kTdEp5nXVK5yWPPM8j/2U\nkeXK3FbKVExFg9jEGZ3a7h04RNT3QaNJdtT/hjZDE1lfU21pJDkt+lpJRhWDJrHD3eZ5nuEOi+HW\nOpJ1Y8lV5e9ibvZku1UxrvR+3gM40rJd2C9keKb/ty6RNbTJfUvq/xiT++a0FySdZx1pZmwW1PIg\nAMJ50szLr+oqxvzDF+KFnKvRqK8m20+KvBTLRh3FxfF3MZNXUX6JmBV3Mxb0/w4/ja/Fe0O34saU\nZ9AveDRzDx2uisUT/VZIRpmFKCOZaxaNcLDWgaaN0JqEydckdV9RiULawMtuds3dplPbQ0PPttuL\nJc3DppL0GC7ZxYQSPYljj7ttMOkZZraU6qpB/xJ3m+d5bKz9AVm7M/F16UJi2qlk/rij16v4cPgO\nBud3qhWgCMb1KfPx3dhSvDdsi9sG3gN9PiDXoAZdFd7Nu89mm/VUNcJ0OwxvZxoWejYZI+S3Z9vt\nRVPZVUgmKAPkwXaDNY4UpoommDcDr7fLydYaOxlcnTNzO0gRQsIHJhidNkCnRU90uEJjAcAQqqnk\n4ZatDie2pSBJLPueQpbdaSq5q2EtM9mg57V4OfdGfJD3kOjKua31q8n5WgYZ7k1/22uVqv7yAAwI\nGYdLE+7DE/0+w7JRh/HrxGa8O3Qz7un9NqZHX4+hIVOwYMBKjI2Qdi9hrdt6vQyNQsBTVXUV4asS\nx80wT4d6sLlNy5fd9smn7qY4/zRiaA8PnSap3N4nn06X7KUP3GkmaRaT3LZjbhe355Abhnh1b0kJ\nlO6mIEUIHu+3HM/1/4HgXQAh0XZ37zdP4yc7faITkAeaNqDd0ELWDzVvZpK1YpoJWYtBkzRYymvd\n5W2blSGiqaTUhqv0sVDWeVxU4pI2IFJFIEnMGh46DX4ywWwv6chFacdxWyRJpDQkiVkaZThGhlnS\nqhtqv8PRlu1o1AsJ9zBlDPo74SyL1axYC6u+TleOH8vfw9NHZuPireG4e99YLC58Evsb/7Fhx35d\nuhBb6n8i6/MyFqO3Gyz/GP9kvDTwV/jLhCRrZVchSfdqFBG4JvlxN/4qwdC9IO42sv5zhf3GedYN\nJNXyILww4CdJDSS9ISFFHw1A+D/QJpq1KrsKUa0tAQD4ywJdcugB99AkTGIuWFyiMCkggzSe1PM6\n0vy03dCCTbXfk+3EIknMGh9xESmZPtq6g+HQA0BRx1GCpgpWhIkyQZzp0oT7GWzOkZbtuG3vUGys\n/UH0Pn6rXIzVlYvI+h29XsWIsGmiXx+o0OD6lCfx9ZgiPNZ3Of434GfMEIktELPvmTFZZP37srfx\nfdnbuGn3AAYnE6lKwAsDfsTzA35AlF+C033KOQUGhozHTanP4aPhO/Hj+Bo8nfk1bk59AR8M2+4S\nJ+BIM6zQJI4MaBpJIia1DQB9gochQC7cN9Rqy8i5x5Gc8bbNYsztDmnJ7XZDC/kdKzmVy4mQ4dQk\nzp7G9TbfzbHWPSQZHOOXIroXAAAM0ljM7cPNW0Q33JSiko5cPHroXCw4ejk5pwFC876lIw/h6qRH\n3Mb1nW6FKCPwcIaF2f9XzVfM+aNeW8mc56X2iQlQBBP8FgDsaVxnsw2dXB6omSB5wtYVd3t34zoy\nKZyk7sv0PLEnsdxtWqy57Tq5HeOXgmi/JAACHstRnwl3zG268uFQ82aYePHRVxNvwqeFT5B1M04Q\nAL4vfwuPHpyJZn29vZcS6UxafEQx3C+Mv1MURs8TqeVBGBQyEZcnPoD5mZ/jraH/eBysAISJy1vT\nLIb2N6WvOq3M+bfVY81tmS+57ZNP3VoyToY3h/yNBf1X4rkB37t+gU8+dQNZpyfClDEMbkOq4tRp\nJMFRp6tAh6GVeZ7lbZ9ZSBJHOivqUiwbdRjnx96KkWHT8fKgXz1uMHSmKto/iaSgDbyeaYhHI0nG\nRMxyK+FBm9s76n8Fz/PgeZ4xt6UkwszqG2Qxt4+3OTC3JfC2AeGGz3xjY+QNxLR0pqIOytyWUP3j\nJ1czBvT2+l8YJEn/4LGI9k8SvT9rTbVCk5iNQkBIjXsjWZwY0AdzUp5lbrQAASmQ07oTX5a8hIcO\nTsWFW8MwL3s6vi5ZiN8qF2NJoYUBfHnCXJwTc631rkWrb/AIPJX5NZP0BIAbUp5GkCLE7f1eGH8H\nMUT3N/2DonZbTI11A8nH+34mCkvjbSlkSpwTcz1ZX1O1zOG29E35kNCz7KZHrUVXC+W3STe3M04a\n1mI0jUpvm9EkG2pXklRtr8BBkvYHCJM99DlmS91PzPN7qdTqsNCpHh8bKpkfXhn0O25Pe4X8htoM\nTVhw9HK8deJuBs9kT0dbduKdExY0ztSoq3FFovPGco4UqNBgZuwcTIi8yK3XO9LsBAuaZEfDb/gg\n/0FihHLgMDv+HiwfdRQTI2e7tf8QZSSmRl+NG1KekmSqWmtcxAUIlAvngYquApKEtxadIHXF2zZL\nzikYg9BZ4zyDSc9c8+hzPy36GiI1uU03BI9Xp7s0dvsGjyDfTZ2u3AZLR6MTxCJJzOoVNJgY/3W6\nClR2FUp6vTN1GFqxKP9R3LxnEDNhEKqMwiMZS/DmkL8JGuhM1oTIizAj5kay/taJOwmC7e/ab2A6\n6S4NCZmMGH/p9wCjKe727oY/bJ6nU8JSeNtmueJu04ioSZGXuhxfMua2CO42z/NMBVF6oGtzm+M4\ntiKjyZa73WFoRQ41thR7bCSpM0ilWauhkWl06Up/1XxNJpb9ZQFYMvIQgxPZ1/QX7to3CgVtjvn2\nP5S9jYquAgDCJO6/hU47VTo/7lYyrtfzOryTd+8pmURzRz3W3PbJJ5+6v8JU0Zgcdfm/nrTyySd3\nRTNQASBTM9qjsjI5p2BuBKxvcFjeds/h0oerYjGv76d4bfC6U55e6O4aH2ExPrbVWdAk9A3/2HBp\nvG2zBmomkBvoGm0pCtoPMenRAHmwW6W7dOI0t3W33UEtndzOFInuobnb9gxNa9FcVKnIHhZNsppp\n/uguksSs8REXEXRDfns21lKGpzeSM2ZlpS7A6gmNeHXQH7g66VFkBI2wMZp1pi7sbfoTnxQ+jteP\n30aarA8OOQt39HrV488wIfIi3NP7bbIe55+Gi+Lv9GifUX6JzP/n54oPmeetG0hen/wkzopyXFZ9\nqnUelWbeWveTXUYrII23bZZbye3WfWRZShKaZs3ua/wLDbpq5rc7M/Ymt653k6j/zSarBDWLJHGf\nt01LxslwTfJjeHfoZsT4pZDHV1d8hLv3j3F4bmnQVeHZI5eRJmu9AgdjXt/F3a7JcXJAXyadblZq\nwAC8N3QrHujzfreYMFbJ/DGFOpfaQ5OUd+ajrFNI/fnLAhg0gSsNDqWNMMfmdk7rTnQYheBAjF8y\nacppraSAvmRCpKKrAJ3GdtGfhR67WWMX7UnOKZhJn71WaBKWty1tAlrOyRmmtzfQJDzP46+arzFn\ndz98W/YaQTHIIMMlCfdhxahjmBV3c7c7VjzRfenvIFIlVD006+vw1om7wPM81lOsZbGNJK1FN5Xc\n07jOBmdFN5OUwts2i05uH23ZwVRwGUx6BoMn5trZP5g2t7e7NDLrdZVo0tcCEBLE8ereoj63q6aS\nB5s3kxBCetBQhCgjbLaxJ47jMDRkClkXiybRm3RYVmRptH1Z4lwkBWTg+QGrkJWygDxe2VWIu/eN\ns1shVK+txOcl/yPrWanPif7c3VUyToa5fT4k4809jeuwqU58ddSpVI81t+k/zOTDkvjkk08++eQF\nWTNQPUGSmEXfCJV0sOZ2QQ9MbvvEiuZu72z4HUbegPLOPMJE9pcFYCiVwpEihUzJlHlvr/+VSY8O\nCpnkVulwYkAG4b426KoIG9KsWm05ecxfFsiY1s5Ec7dLRHC36XSd1EmSseHnk4H54eYt2EexkCdH\nuYckMStQoWF4s62GRuFxuUa0qSlW/vIAjAqfgTt6LcTHI/bgp/F1WND/e1wUf5dDEydSFY9n+38r\nKjksRpcl3o95GZ9iavQ1eGngr1DJ/Dze5+x4S3p2XfUKUtVi20ByFrJSn/P4/TxRWuBAMkmk53X4\nq+Yrm20E3rZ0c5tm1ha2H3bK9AaEZqHmqgc5p5B0XET7JxFzwQQTvix5CYdbtpJ9SS2/N2tixGxy\nrB1s3oQmnWB6GEx6JqHqCW/bngaEjMPikQcYxFBB+yHcuW8kfqtczBg0epMOC45cgTpdOQAhXffC\ngB+7LQrs8sQHybKS88Mtqf/DJyP2YUCI58gjb4quathQ8y30Jh3zPI0kGRY6VRLPnTbw7BlhZtEJ\n4xFh0x0asCqZHxIDLOfMYhETrGbRvG26l4oz0WgS2tzWm3Q4TBnS7lz/aUP0kIdNJQvbD+PB7LPx\nv5xrmWv9IM1EfDxiH+5Pf1dSz4YzRUGKUDzSdwlZ31y3CkuKniKVMUrOz+2xQu/AwSRJ3GJoYPBu\nDboqMuGj5PzQT+MaX2WtCL84cv3X81om7XygaQPpqRDtl4SMoBEu95cS2J+M+ep1lTZ4KWudsOr7\nIJbZT09u2cOHuIMksex7ClkWa27/WvkJqXzQKMJJY10ZJ8Oc1GdPXiOE76XL1I4FRy/HksKnmM+9\nuOhJghhMCej/f/buO8yR9a4T/bdKWZ27p3tSp8nTM2fmzJmTg+3jhA1ebC8YE+xrWDBgDJc1mLtg\nG5NsjAHbXO6a5GUJtlkWw15sbLMYjPPJaXLoyakndPf0dI6S9o9Sld4qVa6SVJK+n+c5z9F0S6WS\nVFKrfu/v/b5448Zgg/9RsavtXrxx089o//7k2fdoj7OWGra4LXausLRNRERhOShkb5p1T3klnggZ\nc7fPzYnF7cbp3KaSna0HsS6prPY+s3Ybx6afxNOTpa7tg12v9nTCb/SQGE1y+0u6xST9RJIASmeY\neEJkzN0Wu7Z3tt3rOmpA17m9YF9YWM2v6LrlxMK4G13JPi37Oo+81g000vagr2nGRmI0ierBnjeE\nsqicnfZEN17R+/34hR1/jM88MIq/e/AyfnnXX+I1fW/DuuRmbEpvw4f2fl47sQ7LGza+Ex8c+R8Y\nbvH2Oli5p/OVWg7oQm4W/3rzM5hZvY0PHn+zlhXan9mBXx35m6ouIGnluzf8uHbZLJrk0sJJbaG/\ntniX6wXCOhLrtM+H5fwiri3aZ1uK0SXD2b2eBxrEaJL//9r/p11+qPsN6Er2edqWqie1UeskzSOP\nJ4pdgydnn9VOhjekh7Ep7byQoFet8U78+p7P4Rd3/Jn2ObqcX8THRn8SHzr5w9oCrH9y7r04OqMU\nFGXI+ODI/3S1sGGtPND9Orx356fwfZt/Hv/9viN4+9AHKv7Z4sf+jpdp0W0za7d1M5IA4BmhuO02\nkkS1q+0+be2EsaXzlgW3FwzFbTtbsmLutvtoEjGWxE3ntrIvpeL2oTtf17qhlbxt5TNuQ3rYVzTM\nXeKikjP+Orfn1qbxybPvwTufP6CLfelObsD7d38Wf3jgW1VdMLIWHuh+nW6R2r+5/BHt8iPr3ug7\nfkuSJDzQJUaT/It2Wey0H2l/wPf3P6vcba+RJIDynU9s5nHK3T4n5GXv8DB7aCCzC52JXgDK58Ul\nw/fAIMVtcZDoyPS3HHO3F3Nz+MylD2n/ftvg+8te78fWvRl/dM/Tuhm4n7382/jVY2/C3No0Ts08\np5v99HPb/t/Qmgqi4CeGP6y9XhMr1/A3EVhcsoGL2yUFlreJiCgk7xj6Nbxzy0fwG3v+3ldHhdGA\n2LktFOumVm5pxZC0nMXGCJ9ok3+SJOmjSSb/Sbcw2IM+I0lUD3S/HnLx696Jmad1J1FBuoh36RaV\nfE73u1MzpX+PeJjdIBZGnTq3ry6e0YoB61NDvhYlfrSnPAv38YCRJKqHut+AtKzv+gwzksStvvQA\nXr/hx/CBkc/i7x++ir958Gwon1uVJkkS3rTp3dq/Pz/2SXz45I9ouZW1WkDSyqt6fwgJSSkkn5l7\nUTcwCehPyg90Pu6pIC8Wwq0W2VKdmStFkngpKqhe0fsW09kcXheSNHp57/drl79dnL6sKzp2vqZi\nkQaSJOF7N/0U/vTgc7qZV18f/zv81Av34M8vfAD/OPZJ7efv3PIR3N9tnsscJf9h40/i/97+hxjI\nel9suFpkSdZ1b//bzVI0yXJuUfe+cLuYpCohJ3WL8x4yiSaZW5vWulYlSI4Lg4ozHcTYKyeXF4XO\n7ay7zu2BzE5tnYn53Iw2SKyLJBGiFLwYaXtAi9C7vHBKmy3h1tdvfQ7veHYn/te1P9QGfmNSHG/t\nfy8+ff9pvHb92xoqgsTOz2z9mOkAw3f1BVsk9v7uUnPMs8L3ssMB87ZVYqeyGtuTK+R0a4C8bJ37\nOC8xd/vkzDO219V1brtYTFJll7s9vTqp/f2TEfMc1zKQ2YWuhLI4rpK7bZ2RDQCfu/IJbSHw3lQ/\n3rz5Z02vt6VlL/7knmd1zU5P3f4S3v3ig/jEmVKX9iM9b8R93eFEb0VFW6ILP7319wEoET3ft/nn\na7xHTVPcJiIiCkdrvANvG3xf4OgC1WDWvHNb/OI13LI3EqgMNkkAACAASURBVN2JVBlicfub4/+A\nw8LJ7UNCvIUfHYkerWuygAJm1pQ84NZ4p+vuUTP64rZ15/Zul3nbgL5z+/LCKdvOGjGSxGvetkrM\ndVa9PKT3dTqWxSPCQnIJKeW5eNPsvmv9O7Qpv5cWTuoWkHzf7k+H1iUehrZEl27w4l8M3dtBOs68\nLCopFhV2tHlb/BFQOsXvM3S3diZ6A38Oic/NC1NfxdzatC6K4WDIkSRmtrTchT89+BzesPEntZ9d\nX7qg68Z8xbq3aFPPKRyv7SsVt5+a/CJmV5WYpsPT39SygAezu7Exs8Xztp2iSQ7d+bqQ0XsPOhLr\nbLcn/i1xu6hkrpDD1YXSjIpBl8VtSZJ0x70anyIufuk1b1uVimV0f6PVeCE3vjj2KfzWyR/UCnuA\n8pn15/cexs9s+1gk8tyrKRtvw3/Z+Re6n7XHe3SLQvpxb9drtaSBU7PPams1BM3bVomdysdnnsRq\nfgUnZp7WmmY6E724q+NRq5uXGWl7ULvstKikOAi73UNxG7DO3T5855taw+rutvs9H4eSJOkWoLSL\nJrmzMo7PXf2Y9u8fG/pN2w76tkQXPrr/y7q/HVcWT2uDzXEpgXdv+7in/a0Xr1v/Dvzpwefw/t2f\nRndyfa13p5GL24wlISKi6NPHkoxqmar6xSSZt93I7ul6pdble3P5krCg2T70pQcCb/9hIZpEdXfH\nKwINmOwSFpU8Pfu8ll+bL+R1ndxecuk7Ej3oSijRB8v5RdxcumR5Xd1ikll/xe2BzC5dLvVI24PY\nkB6yuYU3Ykbxg93frRVqyZ2WeLvpgl3/1+Cv1qQL3onY3fxvtz6r5QvnCjndibTn4raHRSXPisVt\nH53bAPAqIZoEAF7T9/bAU6k3pIexo1Uptq8VVvG1W3+rK5A4ddSGJR3L4pd2fgq/NvJ3aInpiyPD\n2b345d1/2TQdqdUy1DKixVitFla0xXt1kSRd/gb+7hYKeGaLSj439a/a5fu7nLvxdZ3bC+46t28u\nXcJqYRkA0JVY72k2yb2d+tzt1fyKrhB9t8/1NgDgrvZSNMkRl7nbX7nxafyB0G3am+rHr+/5HD6+\n/6uRGkystnu6XqnrSn3t+uCfiR2JHu37UR55bdBPHcCUIesWBvWqN9WvRT0t5xdxavY5XSTJo+ve\n7Ok74Eh7qbg9OvdCWX5+vtiLML82Y1j3wdv3M2NevPrdUjdA3OVv1uEBXTf7Nyyv9zeXP6ItQjuU\nHcHrNrzDcdsxKYaf3vq7+ODI32pxSaq39P+CLrqkkUiS5Gth+kpp4OJ2CWNJiIgoqtoSXdpUudXC\nMm4tXQYAnONikk0jKadNp8E/FDCSRNuOSXHbb0eYalN6q3YSP7M2iRtLFwEAVxdHMZ+bAQB0Jfq0\nvFW33OZui3moXheTVEmShJevK8UlGIt6QT3U/T34ieEP41W9P4Sf3f4HoW67WYgLSwLKe6LWC0ha\nOdj1avSm+gEA06sTeGrySwCUbmt1UdGuxHotS9wtsfPNrnN7ObeIi8VF8CRIuo5vLx7rebOuS+27\nA0aSqMT32l9e/DWto3aHi47asL2y7634b/ce0opLXYn1+PBdn+cAVIW8Vogm+eotJZpEzN/2O6tl\nT/uDSEhK1viVxdO4vXJD93svedsAsDmzXdve+PJVbeE9O37ytlXiGi4nZp7CS3e+juX8IgBgY3pL\noMFWfZHQOXf7a7f+Dr93+j9pdZNdbffhL+47isd7f4ADPgB+euvv4W2D78ebNr0bP7Hlw6FsU+z+\nfnbqX3B8+knt+d/WeiBwl/x+w+DPd4RIkpd7iCQBgO7kemxMK7MrVvJLugXvReLfqKHsiOfM8K2t\n+7WBx4mVMS2KLMjsJ5WuuD39TdPZgTeWLuELY3+s/fudWz7iaeH1V/X9EP7rgSe0777rU4N4++AH\nfO0vedewxW1ZKG/bx8UTERHVlpjReLkYTaLv3OZiko3uEZP85wcDRgGohrN7yjIjDwbI2waK3Rqt\nQjTJnBJNouabAkrXtteT4iGXudthxJIAwI8Mvg9v2PiTeMvmX8CbhYznMEiShLcPfQAf3PO3vhYF\nIyXP8tV9PwJAGeT7wMhnIUvRPH2JSTG8bv2Pav9Wo0mMHWde3xObMzu0TrCJlTFMr06YXu/C/DGt\nYNyf2eErhx5QpuG/a+vvoyOxDm/tfy+2tobz9+flvaViyp3VUgbwwc7KR5KY2ZjZgk/e8yT+9OBz\n+MwDow3bWRcFr+r7YchQukSPTH8bL0x9FdcWzwJQ1hTZ3/lyu5tbSsppXUep2KF8Y+midh8pOeMq\ngiEmxXXroKiDRXb85G2rupPrte93a4VVfPrSb2m/CzoAvbej1PV7Zu5FLObmLa/7nYnP47dPvg35\nYtVkW8vd+L19X4nMmgZRkJRTeOeW38Z7dvxRaINgYk7z87e/olv4MUgkiUqM7fnS9T/D9aULAICW\nWLuvAvGIkLttFU1ydl6MJPE+eygmxXQLoh6Z/hYml6/jUvH7YEJK4i6fHe1ucrf/6uKva7Mn97Q9\nhEd7yuPrnOxouwf//b6j+M09/wt/fPCZpovyqaVofjskIiJqImK3z5XF08gVcrrYhbCKCxRdD3W/\nQRep1hbvwt6Oh21u4Z4kSXi4u9S93ZFYF6ggrBKjSdRFJMW8bfH3bg256NxeyS/h2qKScSpB8twJ\nK2qJt+OXdn4KP7v9Ew21in0j+ZVdf4U/v/cw/vTgc5Evtrx+w49pl5+5/b9xe+WGrrjtZ1ApJsV0\nsxPOWnRvjwqLSfopKoj+4+afw+cfGcfPbPuY85VdGszu1i3oqLq3CnnbVmJSDLva7mPxocK6k+t1\ns5M+PvpT2uWDXa9GUk753rZYwBOjSZ4Xurb3d7zc9X14XVQySOc2oB/cOT7zpHZZ7DL1oyPRo0V2\n5QprlosAPj35z/jNE2/VBsaGsnvwsf3/hvZEd6D7J2e72+9HW7wLgDJw+ZWbf639Lshikioxd/vm\n8mXt8sM934uEnPS8vT1tzsVtcd0Hr3nbqrvF3O0738JLQg79nvaHkYplzG7mSJIk2yijC/PH8K83\nP639+ye3ftT3rIWWeDte3vt96E5u8HV78qdhi9viA8szloSIiCJMzN2+vHAKY4vntIWWepIbqz5l\nm6qvM9mLPe2lYvb9Xa/zNBXSibhQ4oPd3xNK96uYszda7NwWi9sjHhaTVInFr8vz5p3blxdOax1m\nG9NbkY5lPd8P1Y+4nMC21v2+TsarbXNmu7YgVh45/PP1v8DhO6VFsfxOp3azqGQYeduVZpwKn5BS\noXQoUvS9RlhYUu0gBYAHAi60qytWTZsXt42LpNoR13Bwk7t9WVgIXPwu55bVvh3oeNzztoz2CR2w\nZtEkL0z9O37t+PdhrbAKQPn8+vj+r6Iz2Rv4vslZTIrr4nLEWJ0wPhc3pIdNo+Ee87lmxR4Xndvn\nhMUk/f4dMi4qGUYkiUofTfIN3e/+/ML7tViYB7u/Wzc4QPWhYYvbXFCSiIjqha5ze+G0LsuOedvN\nQ4w0eI2QURqGA52vwC/s+BP8x00/h5/a8tFQtrlb6MwenX0BK/klXeEtaOf2pYUT2mJCorAiSYgq\nQcyo/h9XfgdLeSUOYH1qSMss9crNopJndMXtg77up9JeZihu39XxqO8uPKovj617s2mcg9+8bdWe\n9oe1geAL88cwvTqJXCGHl6b+XbuOm7xtldi5Lf6tsXJlMVjn9v7OlyEu6WcNbUpvC2UxaX1xW7+o\n5JE738avHnujthjmhvQwPrH/a+hJbQx8v+SemLutGsjsQleyL5Tt7zcUaJNy2vQ+3djeegAJSZkB\nMbZ0riwiazW/opvt4Hfdh51t92pRXGNL53VZ4Qd9LiapEgeNDt8p5W4fnf4Onpz8ova7d275nUD3\nQ7VRt8XthMPMUS4oSURE9UKXub14SpcDx7zt5vGGje/EB0f+Jz5y15fwcE84i0mK3rjpXfj5Hf81\ntJPX3lQ/uhLKCdh8bgbfHP8HLatwU3obOhI9nrfZk9yoLSY0n5vB5Mr1suuIJ09ilx1RFLyi9y1I\nyy0AgMXcnPZzP3nbKqfO7VxhTTcouqMtmp3b21sP6Ar8tYwkoepKx7K6RUUBZTAz6HoEmViLbv2H\no9PfxpnZFzGzdhuAsliol+9R+uK2fef23Nq01m2bkFJY72MByEysVTdrCwgeSaISoy2OzzyFXGEN\nAHBi5hn8yrHvwVJ+AYDyt/wT+78WSkGdvBFzt1VhzmYRY3sA4IGu1yMTa/G1rYScxM620sDpCUPU\nzaWFk9osgPWpIbQlunzfj/ieUN/LaTmrLQLs12B2t/a9dWbtNi7MH0OhUMCnzv+Kdp1X9/0Itrf6\nK8xTbdVtcXu7w5of+uI2ERFRdG1IDyMhKVPub6/c0HXYbG1l53azkCUZr+r7wYoUtitBkiTsFKJJ\nxBXmd/uIJFG3KUaTXDLJ3b64UOqmEwsRRFGQibXi8b63lv08yCKu4t+BSwsnsJpf0f3+8sIpLcqq\nN9Uf2SgrSZLwpuLCrUk5jVf2/mCN94iq6bWGGUlBI0lUxhxdYySJl0GlDelhpGUl6mpq9RburIxb\nXlfM2+7P7kBMinnZbY1xkCes4vb61CB6U/0AgKX8PM7OHcLo7Iv4L0depw28dSc34BP7v4aNGX+z\nSiiYdalNZYMvfhdYNWOM1nhZ7/dZXNOdkbbSAq4nDdEkZ0OIJFGJuduqfR0vCxxPpuRuP679+9Cd\nb+Dp21/GsZknAABxKYEfH/5QoPug2qnb4rYTxpIQEVG9iEkx9Gd3av8WFzlh5zZFmRhNcnzmKe3y\nSIDumsEWMZqkPHebsSQUdd+z4cfLfhYkK7Q13qF1uK4VVsveF/UQSaL6gf5fxB/c/Q38xX3HsCmz\ntda7Q1V0oPOVWJfcpP07aCSJSszoPTz9TbwgFLe9RJIAyiDzUEtpgFUcTDUKmreturezMsVtSZJ0\n3dtfHPsz/D9HvgvzuWkAyuLSH9//7+jP7gjl/sif+7v0MSFhdm5vSm/DQEY5v0jLLbrFxf0Yscnd\nPhvCYpKq/SbF7aB52yrx/fXSna/hv114n/bv79340/y7VMcauLhdwlgSIiKKOvHESF0sT0YMg0IG\nMVHUiItK6n/uPW9bNSx2bs/rO7eXcgu4vnQeACBDxmDWf0GBqFLuan8U/ZlSwWgwuxvrUptsbuHM\nLprkzOyL2uWoLiapkiUZBzpfgc2ZbbXeFaqymBTDu7f9AdrjPXhF7w+EVqza1/EY5GJZ4+zcIa0L\nE/AXfaNbVNImmiRo3rZqd/v92ufFvvbHtG7rMOxrL+Vuf/nGn2NmbRIA0Bbvwsf2fxXDQiGfakPM\nwO5N9WN9ynu8jRVJkvBre/4Ob9r0bnz4ri/4jgpRiYtKnpx9BrlCTvu32Lm9PeDfoZH2B8uy6IPm\nbavEqJYnJr+gvcfTcgvePvSrodwH1UbDFrdlobydr+F+EBERuWF2YjSY3Y2knKrB3hC5Y1bclhEL\nVGAbzFp3bl9aOKk1LWzO7EBSTvu+H6JKkSQJrxcWlvTaPWrGblHJM7qOuWgXt6m5vbLvrfj8I+P4\njT2fgyyFU4poibdrx30BBS33dzi719egkttFJXWd2wEGWmNSHJ+4++t4/+7P4kN3/aPzDTww6wJu\nibXj9/Z9hbnCEXF35yvwcPd/QFJO40eHfsP32gxWtrcewHt2/BHu7Xp14G2tTw2iO7kBALCQm9Xe\nA4VCwVDcDta5nY5ldd8vW2Idof1tG8qOoDPRW/bzH+j/RXQn14dyH1QbDVvcJiIiqidmJ0aMJKGo\n605uKOsy29q6D+lY1vc2h20ytxlJQvXiLZvfg9f0vQ0Pdn8P3jbwPucbOBCLBWLndr6Q100H3xnx\nWBKisIt3gHmMwX0+B5XE4rZt57aQuT2Y8d+5DQC9qc147fq3hZ6XP9yyFy2xDu3fabkFH933v7G7\n3f/sKgpXTIrhI/u+iC8/OoM3bPyJWu+OLUmSsKetPJrkxtJFLe6mLd6FvlTwxUnF9/SBzsd9Z9ob\nGXO3ASWi5wcHfimU7VPtNGxxW3xgecaSEBFRxJmdGHExSaoHu1r13dtBV7Nfnx5CSs4AAO6sjmN6\ndUL7nVhoEKeOE0VNKpbBB0Y+i4/u+zJ6UhsDb08XSzJ/GIWCcn5zfekC5nMzAID2eE+okQZE9eJu\nw8J5gP8ZE+LflosLx7X3mihXWMO1xTPav4N0bleSLMl4zfq3AQBScga/s+9LuKvjkRrvFZmJywnn\nK0WALpqkWNw2zh4KYwDr9Rt+DAlJmb36xk0/E3h7ogMdj+v+/fbBD6Al3u57eyYfEVQDDVvc5oKS\nRERUT9i5TfXKmK8dtLgtS7IupufSfCmaRFzcS+yuI2p0G9LDyMbaAADTqxOYXLkOQL+I146QigpE\n9cYYvxGXEqYFbzd6U/1oiSmFrtm1Ke29JrqxdBGrhRUAQE9yY6DCWKW9e9vH8dt7/wmfeWA0tMUq\nqXmNGHK3AWPedrBIEtVgdjf+/uFr+NxDV/BA9+tC2aZKzOJfnxrEGze9K9TtU200cHG7hAtKEhFR\n1LXE29GT1Hf3bW1h5zZFnzF3eyRgcRuwzt1mLAk1K1mSdX8T1GgSsWNuRxsjSag5tSe6dQ0Bd7U/\nikysxde2JEnCsEM0yWUhkkRcEDyKknIaj6z7Xs7qoFDsartXW8D1wvwxLKzN4tx8qbgd5qLGHYnK\nzEYayO7Ef97+R3ik5434rb3/yPVbGkSTFLeJiIiiT+xWbYl1hJJZR1Rpu9rugwwlC7El1o6hlhGH\nWzgzy91eWJvFzeVLAJQFuPozOwLfD1E9MVtU8szsi9rPwiwqENWbezpfpV2+P2Cnpzh4Ks4YUl0R\nFpM0WxCcqFFlYq3YUhxIKqCAU7PPGWJJwuncrrQ3b343fvuuL2AnB4UbRgMXtxlLQkRE9UXs/tna\nso/Ty6kutCe68dNbfxcDmV34ue1/iJgUD7xNs87ti8LikgOZXUjIycD3Q1RPxNxtdRq4MeuUqFn9\n8MAv40DH43io+w1486afDbStLVmHzu1FoXM7onnbRJUi5m4/NfkljC9fBQAkpFTkZzJQ4wp+9hFR\njCUhIqJ6Iy4gyenlVE/eOvBevHXgvaFtb7ilvHObkSTU7Iyd25PL1zG1ehOA0k3H2QzUzHpSG/EH\nB74eyrZ0ndvz7NwmEu1pfwhfvP5nAICv3Pwr7edbW/fVzcKY1HgauLhdKm/na7gfREREbn3X+nfg\nqckvYm7tDn6g/xdrvTtENbMpvQ0xKY5cYQ3jy1cxvzaj657jYpLUjLa03AUJEgoo4OrCKI7NPKH9\nblvL3ZClhp2US1RV4t+YiwvHkS/kde+vK3WUuU0UNrFze3ZtSru8vaU+IkmoMTXsNyBO5CYionqT\nibXgo/v+GZ+850lsSA/VeneIaiYuJ9Cf2an9+/LCKV3u6XCWndvUfDKxFq07O488/u3mZ7XfMW+b\nKDxdiT50JNYBABZzc7i1fFn73ezqFKZWbwFQFmvsSw/WZB+JaqU/sxOt8c6ynzMai2qpYYvb4gPL\nM5aEiIiIqK4M6XK3TzCWhAjANmGxrqdvf1m7zCgrovBIkqQbRL0g/P25IuRt92d2ICbFqrpvRLUm\nSzJG2h4s+3m9LCZJjan+i9sFq8I1F5QkIiIiqldD2VLu9vGZpzCxcg2AsmDR5sy2Wu0WUU2Ji0rm\nCmvaZXZuE4VLF00ixGJdFvK2B5i3TU1qpF1f3JYgYZuwdhBRtdV/cduC+MC4oCQRERFRfRlqKXVu\nf2fiH7XLg9ndiEkNu2wMkS1xUUlVXEroBoOIKDixuH1BiMUS87YHmbdNTWpP20O6f/dndiATa63R\n3hA1cHFbzNxmaZuIiIiovojFujur49plRpJQM9tuUtze0rIPCTlZg70halxiLAk7t4n0drc/oPv3\nNkaSUI01cHGbsSRERERE9Wogs1P3fU4ldtMRNZt1yc1oj3frfsZIEqLwiQOplxZOIlfIAdBnbg9m\n2blNzakj0YMBYeFv/h2iWmvg4nYJY0mIiIiI6ksqlsHG9Nayn4vddETNRpKksmiSHa1cTJIobO2J\nbvQkNwIAVvJLuL54HrnCGq4tntWuM8BYEmpi93e/Xrt8oPOVNdwTIqBhAwvFTp98DfeDiIiIiPwZ\nyo5gbOmc7meMJaFmt63lbrx05+vav9kxR1QZW1ruwuTKdQDAhQUlmmStsAoAWJfchGy8rWb7RlRr\n/2n4N9Ee78bmzA7sMSwwSVRtTdG5TURERET1Z6hFv0heSs5gY3pLjfaGKBrEzm0JEra27q/h3hA1\nruFsKQbr4vxx5m0TCVrjnfjR4V/Ha9b/SK13hahxO7fFqn2esSREREREdWcoO2L49x7IUsP2ZhC5\nsqvtPu3ycMteZGItNdwbosYlzhS6MH8MSTmt/XuAedtERJHRwGcHXFCSiIiIqJ4NZfWd24wkIVKi\nEn6w/5cwlN2Dd239/VrvDlHDEhcwvjB/TNe5PZhh5zYRUVQ0Rec2F5QkIiIiqj+DhmnfYqGBqJm9\na9vv413bWNgmqqRhYYD1yuJppIVZEuzcJiKKjobt3BYzt1naJiIiIqo/LfF29Kb6tX8PZ9m5TURE\n1ZGNt2F9aggAkCus4fTsc9rvjIOvRERUOw1c3GYsCREREVG9e6znzQCAjsQ63NXxaI33hoiImok4\nY0idEZ6SM+hLDdRql4iIyKBhY0n0ndssbxMRERHVo3dt+xju7349trXsR2u8o9a7Q0RETWS4ZS+e\nvv1l3c/6Mzu5uDERUYQ0cHGbndtERERE9S4pp/BwzxtqvRtERNSEzNZ6YN42EVG0NOxwIzO3iYiI\niIiIiMivLdny4jbztomIoqVhi9viA8uzvE1EREREREREHgxmd0M2lE0GMuzcJiKKkoYtbut7t4mI\niIiIiIiI3EvFMtiU2ab7GTu3iYiipf6L2wXzrmx2bhMRERERERFREMbc7YHszhrtCRERman/4rYF\nZm4TERERERERURDD2b3a5d5UPzKx1hruDRERGTVwcbtU3mZxm4iIiIiIiIi82tKyT7s8mGEkCRFR\n1DRwcbukwPI2EREREREREXn0SM/3YnvrAaTlFnx//3tqvTtERGQQr/UOVAo7t4mIiIiIiIgoiFQs\ng08dfBGrhWUk5XStd4eIiAwauLhdwuI2EREREREREfkhSRKSEgvbRERR1LCxJOIDy7O8TURERERE\nRERERNRQGra4re/dJiIiIiIiIiIiIqJG0rDFbXZuExERERERERERETWuui1uSw6N2czcJiIiIiIi\nIiIiImpcdVvc7uy0/70klLdZ3CYiIiIiIiIiIiJqLHVb3Hai79xmeZuIiIiIiIiIiIiokTRwcZud\n20RERERERERERESNqoGL2yUsbhMRERERERERERE1loYtbosPLM/yNhEREREREREREVFDadjitr53\nm4iIiIiIiIiIiIgaScMWt9m5TURERERERERERNS4Gra4zcxtIiIiIiIiIiIiosZV/8XtgnnpWhLK\n2yxuExERERERERERETWW+i9uWxA7txlLQkRERERERERERNRYGri4zQUliYiIiIiIiIiIiBpVAxe3\nS4J0bnd3B98XIiIiIiIiIiIiIgpXwxa3xQfGUBIiIiIiIiIiIiKixtKwxW1wQUkiIiIiIiIiIiKi\nhtWwxW195zbL21R9EmPfiYiIiIiIiIiIKqZhi9tiXZGl7dpo9uJuV1et94CIiIiIiIiIiKhxNXBx\nm7EktcbiLhEREREREREREVVKAxe3SxhL4l9ra633gIiIiIiIiIiIiKhcAxe3a9+5zc5lIiIiIiIi\nIiIiospo4OJ2Sb5G5e0gmdOZTHj7UY96emq9B0RERERERERERBRlDVvcrvcH1t1d6z2orba2Wu9B\n/Wpvr/UeEBERERERERERVV6914AtibEk+RrtA/Oq/SsEaLZv9q7vvr5a7wEREREREREREVHlNXBx\nu6RWC0pu3uz/tkGKu2GKyn5QdcgN+4lARERERERERESNpmFLWVFYUJL8a/aieizm/7bN/txxxgQR\nERERERERUXOoSXFbkqR+SZK+JknScUmSjkqS9PPFn3dJkvSvkiSdliTpK5Ikdfi+D+FyPdb6mr1A\n2ez6+2tzv0EWQSUiIiIiIiIiIqqmWnVurwH4xUKhsBfAwwB+VpKk3QB+BcBXC4XCLgBfA/A+pw1J\nFqVr8YHVKpaE/Gv24n6z54YTERERERERERE5qUlxu1Ao3CgUCoeKl+cAnATQD+BNAP66eLW/BvBm\n//cSrVgSZhl7k83Weg+oluLxWu8BERERERERERFFXc1LrpIkDQM4AOBpAOsLhcJNQCmAA+jzu13x\ngeUjUN7OZLxdv9E6l9Npb9dft64y+9EMGuHYaW+v9R4QEREREREREVHU1bQ/UpKkVgD/AOA/FwqF\nOUmSjGU5yzLdhz70Gxh/7iKWXvwaDhx8FQ4ceFy/7dD3lrwSC9o7dwJHjtRuX4jc2LYNOHeu1ntB\nRERERERERFS/Dh36Bg4d+kZV7qtmxW1JkuJQCtufKRQKXyj++KYkSesLhcJNSZI2ALhldfsPfvA3\ncOKPv4HpfY+hECt/GJJQ3s6Hu+vkUns7MDZW672IhnQaWFryd9uuLmBqyv31G6Fzm4iIiIiIiIiI\n6tOBA4/rGpH/+q9/s2L3VctYkr8AcKJQKPyh8LN/AvBjxcs/CuALxhu5JXZu1+OCkq2ttd4DClMq\nVes9qC/MqCciIiIiIiIiIic1KSFJkvQogLcBeJUkSS9JkvSiJEmvB/C7AF4rSdJpAK8G8FHf9xGx\nBSW9kiKYq9LSUus9oGbR1ub/tuxcJyIiIiIiIiJqDjWJJSkUCk8AiFn8+jVh3Ie+c7t6Nm0qRXFE\nsUAdRCYDzM/Xei+c1WtxU5LC2fcoPv4gsSxetbfXpDvXBwAAIABJREFUx3FKRERERERERETBNOzk\nf/GBibEkXruPN2woXd6yxdttk0lv16fa6+ys3X3zeAkHI02IiIiIiIiIiJpDA5eBSm3TYgf1ffd5\n20o2W7o8NBRwlyjyGiEbO4qd226ENdOhXh8/ERERERERERF5U7fF7bhDoIr4wPJC57bXAlpYCztW\nM6JkYMD6dx0d1duPWqlmcbO93fp3MavgnQjr76/1HgTH4jYRERERERERUXOo2+K2U/SAl8xt47bq\nfeFEu4L8Pfd421aQmI6eHm/X37jR/32JnAY+7HgtjNodh/UYj9HV5f+2lSrme40DYnGbiIiIiIiI\niKg51GH5zR1JKG/nHa5rjKKw68atJr/FQmNRdf16b7cXu8y3bi1dHhnxtp1Ewtv1163zdn0yV6vi\nblQWUG2E4nYzzLAgIiIiIiIiIgqqgYvbJYVGqHZ50Nur/3dY0SpBOnPD2gc3xEVAiYiIiIiIiIiI\nqDE1cHHbunP7wAH/2x0c9H9bJ9u3+79tkIUQN23yf1u3ksnK34cqSLRGVHgdSBC79Zu9Az7IWFaQ\n20ZlxgcRERERERERUbNo4OJ2ibFeFSQ+IUgGtZMgRcm+vvD2w47XiJNqEYu7lcq6Njtu7I6lbNb/\nfXnNfd+2rXQ5KvEgteL19U+nw7nfWs4YqObMCCIiIiIiIiKiqGiK4jYAFByXlVTUsngrFtmy2ep2\nO9e7HTsqs11xMcM9e7zddmgo3H2x09ZWvfuKOjEn3mvhul4TjMIq0FdCpRYaJSIiIiIiIiKq/+K2\nSTVq7159LAlQ3r1tJSpFwq4u4IEH3F8/Hnd/3eFhz7tTdVbdx1Y/t+uadepqtYuaEW9rzDL3wsvr\nU6+iUhj22rkuXj8qj6FWvEYUcQCOiIiIiIiIiGqp/ovbJtRCovjg3HZuO6nm9H8vRbqeHv2/+/v9\n3eeWLfYFvkpFfoRNjOmIQgGuklntRl4fbyWjdupNsxe32WVNRERERERERPWkTkqV1qTcmunP9+3T\n/zusmlUYhdIgWcxu90MsWG7c6H67mYz+38YCezW7voMs/Dkw4P66Xgp6O3d63xcgWA6212MulQIS\nCffXF6NXqinMbvawHkMjLEZKRERERERERNQs6r64HZ+fNv25JAGyEE2Sr9YOGTjlNLstQnotBIqL\nU3q5rbG4Z+zUruZihWF1kRoLnx0d+n/bDTYYX59adLa6uU/jfj74YGX2xY7XYyPM5zKszOkodPkH\nNTJS6z1oPOJMkGYX5Xx3IiIiIiIiaj51X9y2yxHwl7odTKVO/B95pDLbrbQwutSD8pKjbizQtreH\nuy9eeHnuGCcRjigcr0FVcwCqWTRDZj4RERERERFRPar/4rYNqQad25WKNaiXrOt6V6mIDj9Zzt3d\n5T9LpYLvixfizAOzoqlY9DNG2oi3DTLzoNKCvLeqmcFfr5o9x5yIiIiIiIiIKqehS6ZiLa5a9RWr\nQs6OHeY/b6SO20bIK476lPuWlureX19f6fJjj5Xnt4sLZRqL2+Jtvarm4zTG1JixeizGhVzrHQvR\nRERERERERFRPmqi47b5qE6RIaxUJYFU0jWKEwN69/m8b1oKTtYwBYASBuSADMVu2OD+vYb0XZLn+\n4kWicsx5LW6LGeUbNoS7L0REREREREREThq2uC1J+lgSLzWbIB2njcBLRrVRI8SnRG3xuGpHX1Ri\nUcWeHucCblhdw7EYMDTk/vpm8S+1VqtBL69F9t7eyuwHEREREREREZEbDVCKNNfZqX9wXjq3yZtE\nQvl/FLvQo8Tv4pSbNoW7H3ZiMeDhh6t3f250dlZ2+2Hle0fx+BdnobiJUIniY4gaY/xO2BopqoqI\niIiIiIio0hqyuN3SUl6kCVLadupkjUqkQLOpVN6xl9fT2G1sFWljVTSsRrb0zp3ur9vWFr0CZ5Qi\nRqKeyR5UkO75/v7w9iPKKn0McJFSIiIiIiIiIvfqvridvXK67GdqMVoWYknyAe7Dqdg3MBBg4wHU\na1G9ErEXYfIbtXDggPfbVKOQVcnO71ouQOi1E74ZO2K9PkdBXs+ofx65WTjUStQGfIiIiIiIiIhI\nUffFbTv6eoR91WbrVh/bL95BrYpmGzfW5n69SKXKf9aoOb1BC2C1LBRXQ9hZ5m6OI/E1UeNzvPAb\nJRMVXhfHXb/e+TpeY1warTBcjdkWUdZorycRERERERHVtwYvbrvv3LbLUXUqOm7eXLpsVsxtJsaC\nu9MU/mrmSVfjtdmwwd311OclaPG1noQR51DtxR8PHlT+72fgYfdu//cb5Fj1OjNCXEC3EY7BIIN+\nbrq7m724TURERERERBQlDVXcNnZZig1mlWyKFQuUtYrciEousdfimNvO0rvv9r4v1WDsYnTT+Ur+\nee1EtmJXaN+zJ5z7EAc6vOZRj4w4X0e2+PTu7DT/HBKP1V27SpfdxCq1tXm7vtVgodXn1OCg8zar\noRKf31GPayEiIiIiIiKqZw1V3Fa7LFX64nY0Mx+sClS12g7gvhgTJMPWjN1jCKuoWU8qOVAS1qJ4\nYUYUGPPHK9lFbLffYidzWLZvD3+bXo8P8fn0eluxOO/mfW/12nmNNKHmE5WBWiIiIiIiIqoPDVXc\nNhJjSdyWtqudJ3rvvdW9PzfcFrfDLKhXYntRUok87fvu839bscgYZmZ8kAUyje89cb/C7n71WsBW\n960WuehhDyKFyW0MTxhqGfkU5O8CM6q9adQ1GYiIiIiIiKgyGricqH9wjd65Xa/Ex2+1eN/wsPN2\nWEDyL8xjMEiHst1r6Pf1tSpGDw35214QfiNrxEx/wF2Rt1rvB6/d9Z2dldmPsFgN9IjHkXi5GT+/\n+VlLREREREREUdI0p+bRLG03BrNF/tx2Wfb0lC5bFU2sCmL1UGSpVQZ7pTh1LtsV+8Tbiq+73/uq\nN2EtROgltiGs+JkwdHRE5z0rfqaIr4txIEFltd9BZirUq4WFWu8BERERERERUUldF7edCqiyEEuS\nt7iOmyxnswKlnyiHWhZCKrnQoZsF5irBzaJ7VtwUV53YHTteFxAEvHe1Rq1rNB637r73I4rZu5Jk\nXQCthq1b9fvixGtxu5LHVC0K224+c8P6bNy92/zn4ufEli3h3Fcji8oACBEREREREdWHiJXHwqU/\nR/beBqoWG83yft1EZRgFyUgOKsxc5ahw85isumXDKJzaFWHa2qyvK+YUi93JXjt7a1H8tXvMO3ZU\nbz+cVLJIa7ftSsVumMV/VKKzPR43n4lhJerd9eLAZJDCspuCq/h3QiyqZzKlyxs3+t8HIiIiIiIi\nIirX4MVt585tO2JRolq83GczTon3Si1uyXJlu9e9sHrdNm2q3j6E0blu5FQArEVHZiVmFdgt8Hjg\nQPj3B5QPllTStm3hbMc4WBPG65/N+h/U8fp5LnbJi6+5m8dRzYU2iYiIiIiIiJpZgxe3SyLeYKi5\n//7yn1kVQ42xGH6KUlFf4C0sqRQLTiI3cTyNoLsbuPdef7e1KowHHVRqbw+n0FuNwYIgs03UzzI3\n3d3G61gtVLlunf/9AUqDXW6eu74+b9uOWkwQVUYjzoIiIiIiIiKqZw19Oi7WL7Ztq5fydjm3XbZO\nXapmsRF+C2TNUhwNqpHzY/0eA9U+dqrZ9eyG28VWa0UsNNdqXx95pHQ5alEeVlFCzV70HBqq9R5Q\n1NViNhwRERERETW+Bi9ulyqLko9H2siFSWMh3GsBqbu7vp+fqJxku30O9+3zt/3t2/3drpK8ZDqT\ntajnXQcR5LNFXNR0cND8OmI+dpDnUYwuaXZRXAC2Eir9dy9qgzlERERERERR19DFbfHB5VEom9Ie\nxcJfrZidsIuLsdUDsdvTjPgYW1qUTsuuLn9FmWrmYwP+M7IrEZWwa1fzFLIaidcFS8Piphjo5jpu\nO6PF4qDVbaxiT0RR67APGslSTV4jXaiEg39ERERERETeNHRxW+SnOy/Mzkin4k21CylhFpOiwk8x\nfu9epejntZNbfG7sujedjqFaFRy9Mj4/TsdG1AqDRmEtbug1f3vzZuX/tei67u/3dv0oLVjb3R3s\nvWL3WMIqvofF6r6s3nOVWBzWD/GY5voGZKaeZ3sREREREVF0NXRxWxZiSfI13I8okmXnTsB02v73\nlSjQ7dkT7vbCOpkWow783IfVc6UWO2vJzSCG11kOds+Xmc2bS13mYXR9Or0mYRwXmYz3znir5zrs\nok8Y700/kRsHD7q7ntccdFn2N9gW1vPqZ+DMS5RQo3Xren2t/CyGHDVcUNQZnyMiIiIiIqqEpjnV\nKMC82iPL0clfrieV6sASs3CjxGux1k41u3adXqfHHqtMl7XZ4qV2kkn/ueJREaSD1ssxEeWsbfV9\nYnfcxeNKJ7WXTuzubiUOp9KCRDEZu8O9HA9WiwH7KQZG4TM0yEyYehWF552IiIiIiKgZNXRxW+zc\nzhkKQu3tSgEmnQb27/d/H1EojD/6qP/b9vZaF1ZEbh6nUyHVa8Gz1rq6wttWVKdjBynIxGLWxTc/\nxap67+rzUhjt6PD3eKtxHJndh1WntZf9yWb1x5uX6ApJKt1XkHgSN/FQTgv6Wb1nnI75aq1h4PU9\n3QiFZSIiIiIiImpedV5OsifWMYzNjsYih1nxxk3n3YMPlv+so8P5dmFyszialWTSXeHaqhDnJUak\n1hEcXjtenYrbHR3AyIj//al3g4P+X1NJqq8FS8N+T2ezQGdnuNusJLMFRL0W53fssC6Se3lvBhmQ\ncXM/TgXwamSRBxmwFLlZ+LVWA29RynQnIiIiIiKi+tUQxW2r4qyuuC1UNdLp8q5BY750Nuu9K1st\nMrs9aX/sMW/bF3V1eS8u2RUx3BRBzATJR960yf9tRUE6D4MUdowFWq/b2r3b/30bxWLuOvD9sBrk\nEbtpvUom3eczBxFW4c6pm9fIrCu5p0eZKWFG3M9777Xertq1bDaoVg1hzmYIwu1nc9AIl7DXAHBL\nHLAMUswX37teB5PEfbD6nA8SaWT33vT6fqsXVoM7XjPoiYiIiIiIqKQhittWJKG8LdY43Ew996O/\n39v1gxQtRkb8337duvLFJL3uexh27ixd9hJRAChdwyqvXY6trf6fu1gsvOfK62MWiUXGjg5lv9wO\nFjgtFFotQQrPTvnLZkVNv4Mg4n5u3epupoTZAoEtLaUiVjLpvaAVj5f2xaq4qz5GMSP+4Ye93Y+d\nu+92d72oxvCEze/jNL4H7YrwQWbmiNwsECpeR/x8shrUEAdFxccgSea3cTs4EtZjrhdDQ7XeAyIi\nIiIiovrV4MXtkrzFgpJGUSn8VYJafGhtddddHuWMbLEI4ycewZjba5fju3176fK6dfp/h81t17/b\nIqOZhx4Kd1/C4LVI6GdwSu2IfuQRb7cT962vL5yM4tZWYMsW99eXZWB42P31xeKgsbvWy2CQ38ca\nlcX1rI6rWhffK/n8eP3cdvNcWL3f7G7rpphuJQqRRUGy3a062mdn/W+TiIiIiIiIzDVNcdtNaTse\n13c8Nju1K08sHquXg073j5q77qrefdWqsGYXd2HGrDs6m/UeRaB2MasFV6+PPxYz72z0uh312K1G\n4cwqfsQvWfY2Y8D43Ij746Urtt4X+bQS1ueX1XYq+R53Gthxs1aEG1aPwer9H2a3dS1mEhm5ydm3\nGgCs9robREREREREzaxBSxcKWShv5xusGOvW3r3BtyEWRePxUkebsfhhjDqhaAkj13VgwDxyw456\nnKTTwOOPey+YWhV27RazrHVnrnj/27a5u82BA/7vw+l3YXwONAuxYO113YUHHvB3n9U6Xr0Musiy\neda21foMtcpjD9JhbUf8nBLXM6j1Z0s943NHRERERESV0NDFbVHBone7Wh3IUT2pMz7+IJELlSoy\nBBHWopVmovKaBlnUMwxBiuZeY4CMub6NpLOz1nug8PuZGJUokjAZj22nRVvtjud0OvgxG3SAqtad\n+EH+vljl7Nt9xgd5vt0M4gXZfiVmifldGJqIiIiIiKieNXRxW9e5XcP9AIJN2a5mR5yb4seBA9Es\nZJtxKhhGJV7Frgs5yG3dLlp5333+7z/Ise01HzgKcQWVsH69u+vVoqDvNntdLG6L0Rles9vddrrX\nQpB9O3gw+gslGo/DsAcH9+wx/7l4XFsd424Lt489Vros/u00u72f95PVbaLwt6Sa6yQQERERERFF\nRV0Xt51OTL1mblvZuVP/7yCFSD+i0tGpqnaBpl4K6XZSKfsO6zC6+MzeD26LN1EtihiL39XsPK1m\nzI5TR7CTMBa5VBmPGT8DH2Lus/Hz00nQ58JIlr09P1GdEVCN/TLmaTt1inv921SNxxCVGQRBFtT0\nQvycCvP5rcSMoCgMABARERERUeOp6+K2KDkxBqyu6n6mK26LZ1WHDyvXzefVX5YumzB2fNmeQOZy\n7nZYsGMHcP/97q8b2NpaCBupnqh3O6rEArGxIJ9KKV3HzZJLHlaRRXzvWXV9DgxYHyNB9iOsReGC\n7IPb2JaoFmStVKvI1doKPPoocPfdpZ+FEQ9ixe/jcrpdX5/3CJ+g3DxHXjPJyVk87n1AqBLCWhTX\nakBS/GwPaxFUIiIiIiJqTg1T3E5NXANWVnQ/k4TydkE8UZ+dBS5eRGzsinICPzYGnD/v/s7yechT\nk6V/T01hcLB4+TvfcbeNQgE4cwaA0gkudpvZFRW0rvETJ1zvbhlhH8Mo8lSzm7aSGdpBqc+DLNsX\nfYIU9uxeryh1xVXimLDqJIzFlHzcoaHy3xk7UY3qeVZApV/vMBYgBYBk0t313C566fVxx2L64/Gh\nh7zdPgq8LB7qVRQ+Nyo12LBlS/jb9LqvXgfJZNl9TFE9sCqSiwOS9TKATURERERE0dQwxW0z4jlo\n3uwMXvxZ8bKUWwMWF+03vLqK+PnR0u2OHNFtJ5OxzzrWIijGxoBLl0q/uHHD/n5Ft265v26FZbPe\nYzVii3PAxYue70vrXD961PNttaKEj9tqatz17rYA2CgqXXhzO2MiyvxE2jgV/YHwCk5ut2NWBDNG\nQNVbh7odL4XX9ev9FTyHhvzHaLlZUNENN/EclSrmeh2gEfdDkkoDam6OO3HmjlWGeLU73cX88WoK\nMsNs9+7w9qPeRSVih4iIiIgo6pqmuO26RjY1Zd/FXSigo72AbKYAeXYamJgov1/JPuNVd/J24ULp\n8qlT3oo35855uHJITp8GFhYAWBceYzHrooL6vEirK8DMjP/9mJx0vo6L23oulpl05rspFIZFlqFE\n33gZCDG6csX/bX0MSLgRpEvY7UJzUeK2aK9+ltgVOfwUfKOasW7kNa6gmu9FwN1zb7VPxsdmdxxn\nMv4Ko/G4/1kUYXXuh7GegMhvvryb50/82ywWt62Ir7+bQQSr16JSM0jCKo5Wc52RIANY2gw6IiIi\nIiJqKnVd3M5nTM4ICwWl8Le6Clkob+eXllxt07HodOkSepevorW12H08NeVhj0MmFim9diOvrpYe\nrJf22Pl5x+7lri7rzvVHH3XuWAy7GFIp6tPmZeGtTCZ4R6y8tqLv+PcqyKBIhYrbXoo70uqKbnaF\nm+dft/BdkPfs+Lh2ccuW6hyrPT0hZe3XIa/dw3bFLbdFM/V97afIpt5GjE9yG6X0wAPurud1NkM8\nXp34HVkGhocrfz9+O8rjcecO8WrODLDq7g7KbtaYkdsc90rPoAlrUCrM5zGszHEiIiIiIqq8ui5u\nF2A4k1laQmxpHjh1Cpibg5QvnZHFzp5GbMqky3rJEEEiLCwp3TTpji0UHM/0HE8Ejx1zuIJ7WqeZ\n107mo0chz88ql0+d8nb7hQVId4oFwtVV4M6d0u8cHrws23STzSr7E5WpuH4Ll3Yn2Nu2KYXW5I3L\nwPXrnrYblefFt5AqJInpCc/d5wcOCP84fNj/nR8/rl1sbfU4UCHss9ciTFhFG2llOVik0alT/m/7\n0kv+b+tALY5Vogi3fbvwD5uFh0Wmr9f8vP/buvidFVlWCsKVLtxKUgVnBFSouup1wMPLbcLidWaK\nl+7lqCxwLA5QehksBpTB9EoIUnAXO93ZTe7fwECt94CIiIiI6kVdF7dVsdk7Snfc0aO6s1Bd5jaA\n9NliUVnoPE4ffka7LC/MQTp1snT700IhR92u0DErGcJO5OVSoTx50qSAdvKkUlwyRpksLytREyqz\nzui5OcTmpst+vKFzyTaiwvZEXH1MuZy34sHMDKRbN5EYu6Tkhl++rPx8YUGfP26UyymPVSzyHD2K\n1lagvx/ACy+43wcAWFmpaLyCrts3DLduAXNzAABpbdVzfvcjjwj/iMIqcF698ELZoq9NJaQYIWl5\nCcjlfBXZYiuLynvWryBxONPln19+xOP+uyqTt646Pn7j89rfX/z5yjLw7LP+7hgAnnvO/20rmPXv\ndBypMSDy4jwS0xOejzv19i3njvguDLcff8rfDYV9CFqUlpcWlFkjLuk6s9dWkbgzbn1lGw88AKTH\n3C94HVZMk7j/4sCq1QKx4kCf1XPtZsBYkoA9e5yvp7IrgO7c6X47YQr9uwORR5Ua9CEiIiKKqoYo\nbmdHD0GSlbOplvOleA5JKG/blgILBeD6dSTuWHc0dhwtz1rOXDur3T71/HeQfKlUKI/NlKIPErdv\nKhfu3MHedTdL96k6exZ4qnTy3jb6gha7IK8sKd2CU1NITpucHC8uAjdvav+UV5a0TGzx8SUumxfW\nYkde0u+Pi8509XqxhVmlWK1eP58HVlasbz4+rmSMi0WeyUncd5/Pjr8nn8R9B/PK/XotHN66pdxu\nerp6i0ROTtp3bxYKtvm4brJz3U4zr4kwC9v1WNwPSerqOeD2be3f1V6krhIcuySFz7REwt0ihWYd\noNLaqjLbxK9CIbSFNt3YtUv5v/xk+d8fP2K3x5XnwIf44qwya8IjNbs7MXvb/ooWHn20GMUUQNDC\ntiQBPXOXEJ+dslw4UmT8rJaXF5G65X+tg/Sty5a/c8pGjy0qA6puOpFt44+KA/BW0TN2Hdfx2SnE\nZ25j48bSz1y/JrkcMlfPuLqq8XnftAnIXBl1eUfWxGJ1WBEq1V4foJrE6LmozA6oR/WyNkajaaSF\nq4mIiJpJQxS3ReJJsLFz29LiIpDPI32zeAJpLJwVCpDyufLbCSShQCrN6hdKbLlc7AZfXcW6GaUD\nq+Xicf39qLdfXdV1gLecPazFdbjJyG4/8bQWdxGfuQ354nkgn0f85jX9F7biP+TZae1+0zcuAt/8\nZqlYbpbjPTOj3TZ+Z0K5PDUFaWlR97hNI13OnjXf6clJ52LllSvaSbrOxYvA1av2MRUrK+Vd7ydO\nKEWeEyeAp5/2VuC+fh3y7LTy3E1O6mNZoH8oZXm7uZy+wK3GHOTzwNNPo6PDXeEuDLH5Gd3AiF/b\ntln/riwft/jkJC6M6nKzay3ykS8LC/rZHQKxYBSWamQ0i9TuaCvJQ+46psXPOGPRP6zxELG4Xekx\nliCFIbNjOnHjiu7vSyWFVSAIOpjgpyBvlM2WHo9Y3BX3TTzeduwwX/jS6wKpbjjNYmg7/bx22Slz\n3C4fvv3Us5BWlv3NGJmfQXze/cwNXdd7IY/ElL7xwKozOpUq/+xKTbqfqRJkbQOrjnYASN6+oTQe\nuGDsuJUX5313/QPwNKgiPga/2faAuy5+csbuayIiIiL3Gqa4LS+UFz5jwsmEbQ1iedn/HZtUN2IT\nzgVDy+65J5/Ub2tlUbmPXA6piWvA5GT5iY66QN7zxZPY4qJ30tqqEmEAAPk85EmbE6TR0VIBWC24\nCjnc8Sn7k6uWY89APlvqkIqfLUW6yEvFrsu1Ne35aj0vxJccPVrq5s7l0PLcN8rvYGpKiQUwI8S8\ntJw7onV5akWN2Vkl49rI5YKa8oSho396GtLSIpLHX1Ke+5kZ8xtCydvUuskWFpQs+NHTiM1PA0tL\nwJNPlo6FYkep6cngxER55/PYGLJXTivbNUbdiHI506zg2NJ8oMUV5VMnANgXQsXsUZE0O2NZrDV1\n5074i7fm89prr4t8Mejrs3hNvOy/BbUI4NjFNzpaGuQCgEIBIyPKxdZWYGgo8K5E1tAQ0NFR+nf8\n0jnH5z6TcS7kRZUu4xtKwdHP4ItWSA3hOPXKT0HL7jEGGWxpuVBa40JacJd7bmQV9SEWsFtaSp2W\nsmxeKK5m178fdq9bZ0fB0wurW+cgwH6Ydab7LZgat+VlO06v3d13W//Oqbht/KwSi5qxJaW47fc9\nkBlzP6tNjI3Zv9/b/dh93mYvnbT+pcHevfp/JyfGEJ+dsh08sCKtLCN103rWgxPtu2sAkZ5N18CY\n105ERNR8Gqa4XaZQQEwoINt2bhsLgxMTiN+2WXRNLDIZI0CM3HQEm8U1iAXX2VmlQxkoyxXXKeY5\nY0k5idI6xouLwMUuWeR2Fs/wEnPWxcPM+eOWv9OZny8baGg/5aLrsvg8xk6fKP2sUEDnoW+UtnP9\nNAAgdeMSdNTi8vHjytT34nPecuFY6TkpMu3+BoDTp9F6+gXg2jUAgJRb04qpiTMnzG8DKAXS8+eV\nE6jTp0uZ7EsmJ7LCcROfL+7z2ho6jj3hvFDdsWNaJ31saV45BkZHce+uOeXxmixSqp1MX7hQ3r32\nfKmbz3aBwaUlpEYNHfwrK8pshvFbrhfYK9u3hTlvba937phmNscWZk2u7E726mhpIMimyLF7d/nv\nY3PTWr68r0JL8bEHKcDG40oRNxZrjGgSK+m0vvAZu3Xd8djRvSbCIEY9MBaxBgbMu4DdSh55HtLy\nUmhPQaXiFOwGmNyyK0AWCkDmWIDcc58kqX4jKMSMcadYLHEACrDvZPbi4EFv1/fS7WoVsaISv2t0\nt+q/pxlnVdgVn832SfyMMh4fYX2eB52RJHb9O9mwwfp3ySn/M8RiC7OQV5Z8DeDKayueut6Nx7ir\n765FYcWvGGfCJSfGlO+jPsiL8/6/I6nRgzUIcu1FAAAgAElEQVQUZPYAERERNZ/GLW6PjkIWOtZM\nS3A2Z/tax8axY+UF6pmZ0m0dFkmTllxMATfbRpCFy0SFQqmAZ7IvhQK034u0L9Ojo6a/t6N1i8Oh\no9Sso1B8TQy/37drBTh0SOlmN2P2OOaUL+exhVml0G18zSVJ+W98HPHFWS3SRV5eBM6XBgMsX8fi\nGWrm+nnltmqe79NPmz8mi0po4sVngHwesfEbSD31DfP7AoA7d5SifXFAJBaDFvciXbmMzNlSIbr9\nxNNlN9emd4tF/xMnTLPAW1uhdPyrMQbq6/HCC6Wix+HD7vPOV1aA8XHI48UT3YUFxA69oBQQcjn7\nmJKLF7XnUSvUFwpKPj2gbNfq02xszDxn2aboqR3/q6vmAxUuWcVtxJ/4pu9telac+WEkLy34Lvqm\nxq8GyhAOzMt+nzkTbCHMqPE4oCTBPsvfK7fFQ68FebN9fOCB4NsQeRmIkiRAmpywnjHkwf33B95E\nTXQc188ks3vfWWVuS1J1O9aNMx/s2B3LsgxkblzQ/p09pH8u7Ba99Drg6WYgwOqpt1uk89FH9f/2\nehyKzQBeBxmiwrjwe8XuJ6T4FWNBN33rsuvitnGwPDFjMtPTrXwerWcP+bttAMac8SCxUvLyov+Z\nSxbfm4iIiCi66r64bXlSsLQE2++a166hr886rzZ1/aJyYWJCKY54ELtxrfyH58757nL1pXgmZBdF\nsmtXebcVAGB0tPSFeGwM8kmbru1r5Y9VjFxJzxm+mIoZz2ZFfbMzuNlZYGZGOXkw5Fs7yuWAo0ch\nra5AmtIvaJZMQilciq/L3Jy+mDmmFFKTJw+b76dxf65eLXud244biszCY5QkpUsrE9dHkwBKQT17\nwfDcHzpUtg2tc/36GBIzhvzy4uXBFiVixnJq9HPPKfelFrnz+fKTQrPXJpcDrlxROuzdxPssLEBa\nLA4cTU9DmptVCgiTk8oMg0LBeoG0fB64dKl0wi1G+Bw/jkRuqfR+zuWUTHFAeU2K3enaz8aETnYx\n97xIW0D21i0tz72syFXNbmDhvsSIoOTkdefbjo8rswoMWs8ewsAG83gks5gnkZRbc3XiZ5xiDgBY\nW0PynPtp6n4EeWmCdElXQ+alJ52vJOjtDRBbs7qq694F7BcOVIU1m8AqDqRa5OvXEFsOHk1QrQKb\nnUIBSF61mL1lwqpYV0eTIBwZC8PiY3bq6vZD/L7q9XlUC5fG95UxDkWNqwJqnHcd8vddp4VTVXYd\n5NXS26v/ruV2QVQg2CwP0079QqEmx0F8NpwYOTFWyqvM1TOesv5FiZnJ0uxXP7e/HXw9m6D8Dsx2\ndkL5gGqkD3siImoKdV/ctqObmW7y5U6WXRYy7GIb7IhxI8bOUasvDeLPT5wo347d9UVO+cTLy4jJ\n1l96bb+YOkUCiF0mTrEtbkxNeVvwUSy62ezr/fcXX3/jSVihoDz+2Vmlcx02XxJNOp6N9ymvWtx2\nbAySpAwyqCesYhFEWphHcloYnJj1MUV0aQlYXcXmrMVzKHS0JqfHtY7uziPfAi4LWZVuvuQ+80yp\n28UkJsWOdPoUMD2N9tg8eq5YdAvl8/qFQ9X3lBq7c/USUkeL06iXlpQ8ckB53MUBqvT1Yide8XVF\nLgc88QTw3HNoP/6UbcE2vlAsjL/0UumH584he9E6tqYs0kZcGDKXQ3L0WNkAhmlRUB0IEgdGLl5A\n+tJpZXu3b5vcqOhEaR90XVyFgvlioPk8sqdeVC7fvu2/c71QQG9X+TEnoYD2xZv+c5S9fBbYsTim\nk0lDx71JkSZzZdQy4khcvNasUzK2MIvEuPtF7ozsuvjM7k+dnOLEtKhy/TpS41fd7xxgWkwxdpAa\ncdE550VVVcbnyktXdOK6+/zh7m77AQrH/V1TBsDicWDfXdEokjgdZ2Kzg3EWgJdjNJ0GNuWvlq2t\nYtftrUomzWcgqK+FsbPXuF9ui8CeeVwAuqzrPyC3s0/soli8dPUbB/G8DnaIM+dSEyYNLxViNiAY\n5swdtzouHna+UpXUIrYtSGE8DOl08Rj0UaBOp4H02Hkkb/ub8SavLCF7+ZTzFSuNnfdERE2nIYrb\nsZh5kVoWG1jVC3aFYjicwKh5yiayWZOFY558EsmzNnnNTtSi+hUf0/+PHLH/vUOBKHnHpqA/Nmb7\nhSl5pRhTUcxmBmC/4KGH/XLl8OFSIVjsFDds2/K1fvFFJWZEvK6HzjutCApohVeNSVd1IGYFb7Ej\nfmKi9BwYTk5NTwInJ0ud4AsLwKlTkJcWIJ8/q7+euu9ivEk+Dzz/vPL41de7UEDysuG2ZoQCojSt\ndMPHx69Dumnx5fq60LEsFOidOo7LjI4qX4Dn55XX2OzLsHF2glhovnIFHcu3kLlc3hmNQgEDqVv6\nheVGRyHPF1+z06cRm5rQD2DA5uR8crK0aOvyMqTLl7TLOHJEKbaqMzamJk27ulMT15TXVSzY5nLW\nESVjY9pir66LnOq279wBjiuFe2NBtqsrnDzNvs4VT53W6n709prH9gCAPDuNxPni6zk3V5otIYgt\nzQO5nOlnSNvZ0uCHWQFfXl5EbFY/46Oz0+UDcCAWOr0WjC0HGzx8TknLS6ZT2cX9UjN0/TL7PIlk\ncXxhwbY4YNxnq8V3VVYFqkpGftg9r3bvX0lSBt/Ux+/UjWp8DwfNivYjmfQeW2P3u7bZsVJx2+E9\nJD4/iQSwdWtpO3azF4z74KV4a/Yc2xXGY88/o9tHp+dK/MxXB0LU2xgX+9MWv3WQSDgv0BikkCnG\n1LgZiLBiOiMyILcLUwb5LNy3z/9tjfx20Ad57qyic9wOHEZRy1l/gwRu31NWpELe/+yLXK7mee0A\nlPWMfGo5f9TX95S2NiA+c9v3wAAREQVT38Xt4re4DRvM/5DrOrfVC5OT/u/vwgXLX3V2mn8p60gW\nC4ozM+W/9Mjp5NcVtQszrO5HO089Vbofl9EuakY28nl3Xd+XLTrRiosvisVX6YrLrjWzfGaRwzGk\nfal78cXyxzDm0LFp9mVSXUzUgaSeQB86ZH4ybYimMM3fvHWrVLy9ebN03OaFoq9dN4TYyX7hAuJP\nfBOJmxZFUbPnQthveWEOsTP+uz9iS4aueofOMym3pjx36mvmtBBhcWBh504gdfu6dqzFFma151CW\nXXZNnTgBfOMbynNbLKZKa6vlGfTqcy92U19Vnl95aQHtR5Uv8/LyYvnjV62sKN3qADA/j9iT30b2\n6qjuREA3mFO8T20dAvF1W11F6xmhkx2GEwq12G44SVAfVvbyKXfRKibS1y+g5bB5d2Bsfqb0PhYG\nQpLjykBFR4f1jAopt1aK4hgfdz0TpMzamutZP9XorHNb9EhM3XKcUp4es4i3KBRKz10uh7ZT5Qs4\nJifGbLfvtJ/xG+WfJ489Zn8bN9v1wqrQFJu+jeRE8f2xsIDE9IQSzeNw52pcRcuFY5ZXdep+t7Nl\nS7DHb3Z8uh1UcnO/6TQwPFTQisFuB3va2709rsxoqVAkxncA4S76KUn272njbIre3vLfq+65x/39\nGrdjp6sL2LFD/zO38UV9feXvOXFwbMOG8uxk9edA+XdlYzygtLJs+l3I6XkFghW3jcVdq0Kr2UCg\nODAQtJgan54s+1tjHBCoBLv8did278P0NZfrskA5JrXP0JD2oWKzGSpEXchWkoDEXDjxLvXEtFnE\nB6ngPxpJXl70VdyXZSC2vGA5s89RoYCW80edr+dyW361nn7B921T41chL1qcfziQVldCizQiouZU\n38VtB+L3HO0jvpq516IQ4jlMT1xO2HeGl50IBOkWNovgcOIUj2LlW99C7HAxGuG8+5xQjUOHfiAO\nxW+t8DMz43lQI371onZZ+6LusGipRsy8Nnud5/RftixPRqwWiFS36XAMxeeKXak2Mx2s9iF566rl\n78zy3XXbUt/l4mDApUulY+GZZ8puU0acbXDxorZYp+lipEYXLwLHjyuLXE5OloqqVp85YuGzeFlC\nQctxbz3zUmkxTyPxuRBmJ4jdcvFVw2eOeBu1YHu4VOxJx9dKnyd2j1WNdAGAJ55AfH5aKRqfOgXc\nuQMpXyzCFwrlmfRipMupU0jevlGKj0GxqGGVqz85CWm+eAyvrSF98xIkCTiwP4+7284rOfHFrv/U\njUtl7z3teDJkw0sL85CXF0vFe/HxX7qkvX6tZw+5+lzR7ufyZe35NC7qpS3s6sBv0duYk61tb670\nnFhNG44vzFgOjPRvVI6b9C13A4XioIabTj5dvqrwvslmlU5Tq24wp0Kr8fd2xY7E1K2yOIl0unyx\nNjPyylLZSa2xQzY+d6fsM0F9ncXF0zJXRnXXicUAFAq658C4T1af6W4WLLRjtl2zwp+8vFg2SyOR\nsNkxQXrpTiniaWkJ2UsnHTu4vebjt+f8D6oYF5907FyWzP/hVLiXJACT5jNvVGaFY5XbnHrjR7zX\nfHvjcy8eY06Rf+vWlf9MfE/uxKhW4HDaL2MxW5xREGZB2G79GgAYHLT+3ZYtwnZcfKa3XDxe9gLF\nYu4GL8y2v3On8+2AYLNAxNfb2EGdHvc2+zR7ddT5Sg1MXMi2GaVu+2t4aBTxGZuYQQ+sZie62odF\n/5338dkp37PzYotztV2svsjYtONFeuy871pLbHFOfy5CRJ41TXFbO5UUO4gbYbEMhw5s244Fr4X+\n58q78GwVCmUF1Uip5uvv9blbWws0ECN9+1vKBatCNeAtbkCMP3E4q0/ftClqG7q129v1J0Wmi/C4\nHCDRCp8XL5Yem7GQ6RQvI0mlwq9YZHUzsDMxoXT6qtQi8FGhC8PjIFf2nH0HR6EAbX87O/UFxMEW\nwxdku9kThYJyYut2fYFCARuXL5b289oZJZ9bHfixGtQRB2rEGRBra8DNm0qkTbEIX/blcmKiFOmi\nLvIpAW13riA7cRlSPofE8VIkhnxBOPbFLuqxsdIshkIBycPPITF1C+2nntWuHpsuP7nQOr3X1krH\n1+oqsheOo+3087YzGqy+6BsLzJnLp5VjtlAodfFafQ7k88r9AnjoIeVH6luz/cTTpdtZZKa3zNpE\nZqgFSuF4TV87h00J+wKPcf80uZyr4rZY3NUWdYUya8lTRrvw2TYyAuzerf+1WmA0KwSlb13WdfV3\ndyuFO3F2mN+uJADIXjoJec05Hi01Wd69mBq/quuoUruPnYrXxo/6TEaJvjArMpYdx4UC0ml3nc3p\nsfOBuq60z+dczrTzLTY3XTbwIO6nkyCz3/x2vrvNvN+QmER8dkq57qIy88b1YEU+X1os2SVjtIjT\nQEEYi7taPR5Z1neRp1Luo+Dsjv0wFwVNnrFZWN3Erl2ly2LR2E2BOpWC6fFs9nllHGQym8WqzjTx\nElURZGFlYwe4+DqIfwesjgezgQJfA72NcI7ngXFx2UZgNvur0uo5xkb83mW53lMDs2rs8MrvQrRA\nMcLR52dP4vZNJGZ8Jgzk8/5nDBjYre3jxMtMHaP4zG3/953PB9pvahx1Xdze9OCA7Rdb08xtkVWk\nRZHtCcnqaqBsuCCjooGyvI4LX9CdcrltuJkeqrMc3h9ZrTPYg61blRMMXcek14VCjfnZXgjFUS16\nxc53vlNajNBv97sZH39wEwkgvjxfKlj6+aMtPn7D+2rPHpOOIfExH3aXOagVt4N6sThjQCzQej1+\nxc8WQ5HPjrFotnvI/SJeqdmJ8udRvW+TY13OOcTv3Lypy043Ghw0fAYUCqViaKHgPGtBnAFx6RJw\nUr8Ak+2XS3HbVp/j4kDC0pI+lqjYVV9WQHezeOZ3lKJrbGkeePFFpdN5cQ54+mnLCBNt4U4h2kq6\nchnJ2zeQuVoadEjdvq4858vLukKvts/FwUx5cR6dR76FdWnlmE+ni9No1YGUQkHf/bOyUjqGCwX0\nLl1BPK5E36gLYCauXdSunr5RvCzk2bckrY8Xscs0tlI8Zg8fVrr4gbIZRnYLrLVmcsDERGlKsU2c\nkFXBVYzGSaXKu24BYGPnouXtTWdMCDMkWk9an3DrTi6EY0C3FoPALArA8m+rxWfvww/rZweo8Tsq\nSYLu2IzHlefErItYHFRQ92P9+lJhTJLKYySsmBUoU7eueM7bl1DAxo3KcXnXcPnn/PCw/jW3WgNB\n5CbD2KyTNZMpX2xQ+921s/rP+0IB69bpO3cB84Jsd2zaOafW5PWXJODuPatKt2/xOuJnikh+7hnd\n7Zz2yXg/KjezGMwkEuYF5+5uoD1hHwNg9X08FgPSs+OlE+qZGcvHHxY3A21Wsx10a3AUGeNhxAKw\nsTP9rrvs73c4eyuUzk+vMUh2syzEw9bss8NYzDb7vO7rczkwcPOyaYHLTTTJy1/ufB0/WkdfdH3d\noSH/A0mdndCteeFlrSCgNmsdOLGM17PQdlJpUggyGBfJNTxcCnsh33ojPv4ga7vUI3l1GdkL3gZh\nrYjfA73yOlNHlLl+3vfrlrx9wzoysYqCFPfLzvs8kNZWa5dOETF1XdzuWp903V0Q9sstSeUnLNUS\nKIMtpE7qjg59PmNPT/Cpz0aWXT4+PvjSaeVEXfeha5Ohbkoo9PT3B1+wpRb8THccGChOg1YLZ35W\nILfpXDd9Dx8+bD41y2VWfPxkcV8NneJi8bh1wWRwwypy5vx5xJaF/fHy7XdpSV/0taEtRnjiBCQU\nlOKSWtxzyGuPTRYfj7gA4ovuTqpMC8liUdEh/kijvqeOHCnFw9y+7VwMKHZit1w8ru9+FyOJxD/a\nc3PYsEFftPMzYq+eOGkngU8/7XrsJnt1FNLSYuk1tclo14phQlSPfFF5bKmJa8rz5jQYcP689hjV\nKBets/v4caVrYXlJezybpoXBghs3ELuuRP60XjiKgZVzykLMC7NoGX0JscU5JMcult2lLJe6mLRO\nRPF1WFJiOMRoGY1NVnlscc58ewC6W5b170Or2Q65HHZ23tIGmZPj15AYV94jUm7N8e9R+rB1TJFa\nKJQW9CfWyelx50xwtYgtflaNe+h4B/TPidMaEFCOA624ieJMiiKtkOzywFbfC5JkKAALkTyx2zaP\nx+zL9dqaNgCSGTunbd8t8W9t7Oxp7bZDQ0qhcXhY//4Xp7WrxUTjSYNVc4JTYTSZtOh4h3JyZZbz\nanysDz1U/rO+PiiLEAtRGsbrtB970rLArSkUTCNN4jO3dZ/pmYz+O5vZZ7TVd1yrXGvLGC2rfRWd\nOKF9HsfiypXSafNisGhkBMqiyOpg7dpaaT/EOzN53tSFO90QN2X1+vtlHGjp7indmbEoa/a5Fpuf\nUSK9AGB21nNB0IxxgC1zzX5h8AcfdLfdILnebiTv3DKdGePm8ybMtS/E77VWA5tmtmxxP3hoxqz5\nx+25qllxu966mHXf0xuA1xk57FwtCRLLQuRXkOK+LprRo8yVUf9d/yHy+plVCXVd3Haiayqs2V40\nLvGLYE9PsLy+ehOPB5u2GQmj/nMNZbny3Q2xGedBHOMCUCrtRM9A7FY2LijmROtmBbx1ry8vA88W\nIy/yeXfP261bpRN0IStcnnAx20CMU/ExECEfLhbYxezy8XHnx2xVhLtzpyyD05E6q0Tsyp7QF6eM\n77/2ky4y1Q3EwknZ+1ns1n/2Wf3vxIVP11aUzz7x+RH31c2xcv26+UCEWBQVXkux2Jc58YJp8VTK\nm7/24smP1ilqMdqvdemKhdpi5zpyOSReeLqUjyjG71h1D1wtZeq3Lk8ie+mkcgJy7Jh5EduqQF4o\nQF5bQefkOS1/OHvtjO7E9u609aJU4ntwZETZX7Ppz8YOwMT0hOnLuX3uUNku6mb6XNF/2RW3axpx\non42FwrApUuuI1nE2SvyypJSVE5aDDzbrOWgZV+j1FUqFj7UiAazYlDLZf0MjGQSwLVraEms2H/2\nSZL5rKy1NWSTpeNPHcABlE5Fy0EMnx0sqYlrln8/3DAtLggHRubEC7bPg933KPXvlzpgYfnRYvIL\n40nPwIBSpO3t1Z+MJG/fKH/ubJ5L8RhoP/lMsGnhhvtJJPSfdWpH8r33Cj+Ll8cOmWkdfRGxa6W/\nJ9ms8vi3bQNazpZmh2WunjF9/sRufbvMc7u/uXZ/CtbPng10zJq9d+SY85cNeXG+NNNhba1s1od2\nH+MWC4MXace00+CLHZPbOs0SCLoQrLreRiYDYGFBm8kUhBoTprI9XiqoAG9f0mNL82WxT27ixOx3\nouBq0CsMTrManMiL87WPsjHcf9sZ913/QQXpGlX5ndUTBVHI+q6lIB3HjUAsLDv9vWs0sYXZ0Bby\nreZnlpWGLm5Lwt+Iajfq1/O0Jle8RnrUSJidGDqnw/kQqFf7l5617N5yEhsL7wtEkAGGIO9Rq8Jh\ntVnObrhTnJZsE+mgXVc2FFQsIlh8L/Jz+bK+m9yN28K06uOl6fZOyqIG1IK7xWKk6hfx9I2L5VOe\njxW/6CwuWnYP63apUChF6Bw7VipoioMEVsTi8dRUaZbI8fKFxQB9J5haHDUWFe2ox426MKRtt8//\nYe+84yM7q7v/vXe66qqtpO29e70YGzC2CRA6BAghBIgdDOYNLS8JCQkJbzq8lNASQl4wEDsUA6ZD\nKDYhYMcd7663N622aLUraVe9a0Yz9/3j3PLcNnNnJO16zf4+n/lIGs3c8tynnOd3zvmdfB4eecT5\n2yRgVBmIhgZsDfW6OmwZIl0HxsaceziuRP/t309yqE+i7MbGHFKoR+lnasaAcg11hx4VZ8bMjKuP\nqxtxvU85zokTtl57bHwEbWyUeBya48O0ZkaJT4yUHe24qN6QyH2gKhsslaVu6q3+pedmqDm+x34G\nVQfdpLo20G+38bXt0m9t2QJ1wBcKPoeSSspWH98L2SxtbSZBqGkuIii2T5xYS5eKzqNawDI5HCBj\nVGQttaTSLBJH00A7J9fe1kZR0s4i0+xunsvBmTPEpidE4uYJf2Gn0Lk7n7clrTLnOiPVSli82K/r\nWn9I+loYcRY7EkzuV6UL1B161H99iiOhmPSIK7PKajPD8D0P7/ETCVNf1ZynNIzI0hRbt8L6MWcz\nUtV1xBc9pPbjUkEMVlp4WGS8NxtChZW5FBbdW3V8n08ST9NMJ8dgr1OYdWzQRzLHZiaJd/lTlpcv\nd2dDpvrP+ja23ra0zuO9ztojj1N31DOeA/pqUOR3oq/bNaeWC3X+1wp5dN3JrtS08ChGLT9LarBH\n1oBcTpyElrSSOSi1XDZScc7EYF/JCO8wLFtSkMwED1THXhhJXH1iv31/ruhjT+MHSeIkh/qcehvD\nw072nBclHA8bN4odER8bKip5FDR+amtNJ8MZf8CJKh0VFtGtaVLLoRhCZTQNg9T5M6xfD6u1U2XL\nPqrzWaqvyzfutPxsJFmW+Pgw8fFhGhsrCJQqFEgO9lacUWHViag9/kT5tr1yvyVlpYpAz05XLikw\nV61hc5xb8365smEq5rznvoTOBSur7CmBQsEVpBAFc4k4firAVdC9wnXscoWWn31Kyfg8pcntSznH\nlvpuWcWxnoJYMNLZg6e8k+ESQZ+LuyiiAdfQMPeonKgot5/UnKhcr17FQo2DoIKIYUin3RJDqvzA\nvCFETiaK7q0dlVyi38RiZrq8KoHzkEIWBEkyBUTZxXtNYmPMvVEJkhvwYSxgc3P6dOk+r/6/vz+Y\nEC+xqMSmxsOJo+HgDWtLo9x/zYl9QlCbTgXX5jJEDsdqj+RQQBFY81q3bSNaf1LvTY1yVh0tynFc\nhZIUbfj2fEi0xblzxAo5NE3uNXVwN/E4rK4fhN27XZIeXsTGR1zyONoxcWzu2DxDZt9jLgkI7zwS\nm5m0n2087o6e1ceV1D21jZU+WVMd8swtwvOhh1zkfmx4wCZSbe1zE+l08KZ1/XohxqIUnwqK7EoO\n9tqFUb3kU81M6RRJVUIFkL5gOXdComDtdvaMt9jZLodINgx3v1cejp6dtvvZli0SsZg649/MrF4t\n88q6dW4pDn1A5iRdF51Dm1hQ+rE25XaIeWulbNxoyqVZ8ltq5xkfJzVwTt7y1CJIHvdvWKurzTGb\ny8HkpOjpF/IlpaxUuCI2i801huEal6qDWbVrrb62dKmbBE7uLV4gzirAabfpzIyr7a65JlxPV+1/\npfSG1VvUtHCtfPBLHaTT8NznmoUv87N2P13WEDD/BrSl1ylfXS2O1mLEmqZB+uCu0P+7rq/3FJrm\ncfYFbFxd85U61jzrVVXXkVDCMTY5Zn8+MToQeA9B3ck7F6V3Pli0DohhFHE2TY7Z97dxY/AJExfO\nlVVQVv1s3cFHWDsW4KA3DJsMbm+XugeVZi60t8wGOljVjIGwAqnxsaHgdVhBWIZL9ckDNqnX1ETg\nWhgnhDjN56k9ZvbJQoFMz4mKAz8scnv7drcUTyR9+/zsnLR2azr3OjJp5dRmMAxHm1htiwpQdfpw\nxeR4YvgCme6OigOO6g4+4vq7nP2Q7QgtFMoutK1miQRpReszU4EOHy8Sg30VSyFoszl3La4yYZ+3\nEsJImWe17IysAZXoDATtLwzj4kpUVCJX+hTCXIrMP+lgGGU7y+ajzsd84ilNbqsoPMlITm8BmYVG\nS8vCa92Vg7mmj13O0LRopO3lqOnthbVRrQSNjeWT25eDM0NdNNraCN1wldIafbJgx46Ld6746QiR\nFRMTxEcqN+pU2QMVamR32VEy5er7q7DkSkzjOZNx5vK6OrdjwpfJYEZTq5Hnra2OHb5smTPGUilE\nDsb651yM9d27RQdXxy1XEgbDcEfoGqbevDdiPmiAnzrlRNkXua41ayQKzrX57nIi18OyQJLnTtnX\noeuKEWtK1aT7TB31Xbuorg6PkGtthbYLTsHPzOHd1O+5X/4uJhFlPQdTnqi2VrSX7T6okP6W5rgP\nR47wtB0GiYSk3hqzZhso7Ru2EdLP99rR0LGpcWcTMzhI8nx3UXLfiojzIjYkJHkpCQD1eJrmfkOf\nnoRdHjIhoM82NPj1gLXsjE/KJ9HncUIOT6AAACAASURBVIwox6qpCc8Sqj/wEDVHd/kvGImAt4ga\n770mkxDPTjpRoypMcj9o/VPJs8BozaNHHSJyMGTDUSj4NsTJw5J1YmvsBjnqkLGi7doJSCHTa9hF\nIi9zZpBdW10dTqzVZPJOO3vaznKYMDjoqgmhPgev3MHVV+PuA4cP+z4TdC3V1SZJaxhF5TW8WLJE\nCm9ZRfwCiTjV4aGBvn+v7xqSSb+2tp6dpqUFmqqd9cjKPqkUqT538eUoRRZLofbYLnsObI/5SaKo\nkcBRnMeubJwQGAY2yRiPOW2fOl2eDJ9KJOu5mWAt/dkcNcf32GM7iJwuNsfFx4dtKYB0d+WRgs1n\nIxRdDwkwCLJlvPNu3f7gjALV2XnVCuc+XZJ/hUIoYZcc7C3ZP0IjzhWU47TwItUrfdfab3mlKexi\n4AFInzth35ttB3gQGx/xjTsLiaHzflIw4iZGm8355LOi6t57oc9mKy8SbPaBms69vqyVUlAd28kR\ncz1W7l/Lz0Yi/BPjQxVrrevZ6TnJkVgSDJXoe6vFL61ssYqu4diuiqXUkv3nKr//QsGWsqikAKXq\ntA4KEIiKIGJVm825HdYh0HLZuRVhNK+33L7/ZIY+PUlNR0gGUwjmK+BvvvCUJrfL1dy+VLpo5aIS\nkjqdnv+Cj3PBxYrc9iJMo3muKKfoSiJR3GCycNlreuMn3xYa6ro43/1dfR5RDO4wpDocIk7T8Mth\nmFi8+MlL1ocZ8lFQUWSC9d2LUSwnJAJB3UBZUbHeQm4LAo+0QlWV0/+8KfqAHZEai+EilvWz8r43\ntdn7fX1aIj+5/37nzYhRGfGjSvSNGrEdpW1UA/P8ebZvZ05GpyvS0NS6tyI+gy7H63ANHXvWfZnX\nZkdEjo0Rj5sEl3qCR/2bHrtoI4bvPPrRkPRyM0J71SqIGzknbbK3t3Sh6D4nqi9zrpN0l0n07NpV\nsmBlvPOoTYKnBnvQT5uOmv37fWROQ0PAAZS+Y0W9V504QOboHqftQgrWqm2jjykE5/CAMwcdOWIT\nN1a7btjgkGWJhKcAovVsRkejFfosUhi1tta9znjHVql5MtkbTHpY921dd5C9pOVnS0ZrhiF9vgse\nfjj03jSMSIVMUylITI2FbuyDCluqqDn8OIv23u96T9Pw1VdwwezrWi7rq7PgnfOYmAiPJPU8y8ZG\noKPDmTe6gp9NbHzE/l9Dg+wbfMSn59iuKPHhIdauFduoaVDInaDnW3f0cTKZ8OKdlvzGypVFCgae\nOeOKhl63xE0APe1phE90EYt3A/YNBtr16vjPz7qfbTYb3kcC5v5459GS+zTrdjQtGnlfVJc6ahHe\nIsS8TdoFHFLLZe05UHVc2J9R2iCVIvSZqLbp1VeLTIsXc5G8iSJzpAbjqDZt7fEnQu222PiI7SyO\nonUeRt6m9joOwjDbPMzOVqWgDDRbqk3eMDAMWNwfsj4p9xUWsFVzfE8oyZbpOeEUPzUf+tOeFnwc\nL5KDvfOiC2yRorZDoowIXJdGc1jdljnUryiGTHeHr8/MpdDvjTfO7Xr03AyZs8ddMkKlEEToqvNR\nbGo8dFwkhp06SFp+tuJ9lTab8+nsR/6uUbDnt0rObzuwKZ8cV6WOak7s82VzxSdG3HWywq7h9GGX\n87wcxEcGqDpTuURtdWflhLCa9RDlPsMwF519LZd90hawfUqT2+VqblspqE92lFPh/amIuRB+lTzf\nxsbSRTK8i+qTgZS8VA4ELy7VdVSaohcG1XiNWn3+yYwoDhYVc4mOUVG736+rGYRL5YybU0G0JwM6\nZcPhbb9Fi6I54Rrjo2UXkSo534XIotjwEmnq50NIplKIx/FtuIrN/9Y92JGeYcTGdBFdOusgKlkb\nIMuybl1EZ3oUcr+/PzxC14LnXhKDJimaz/vId3W+bmjwE0Sxnm77mCsSPa6NgXVPrr6nFErVBsSQ\nXrkSFjFMdddhcTirNTxUMkzpV6lDT9j3ke7Y79zTwIDTTmZbV1V5xrFSlNdFpkYg8Or3PWA7Fhbt\nvd/J7LjvPq5eOSyyJqazSw08sNcf8xyx7JTTzx95xE3ylaiN4HoGZvtYGyOtQxwVZfvWZmf9928+\nh7ANtZaflRPlcqGR3Sqqzhy1+1PQBqyiDbmZjeKVeYmKeFy+W7vbTaq3tWuSWm/1/xCSR89Ou5xJ\nzc3QmJpw/9+UbSm2lmzYAGtSZ0n2B2RbeM6tZiLV1Mh3GyfOUHvoMRobHeJO03DLOmWzrnZSx3Zs\nfMQ/b6u1ZPbuDSRt4+PDpPJCkl93nfmmt/OF1O3QZ6ZEpgtxAjYfe5jMuU5aW/2kZBjZYd+rSZp5\nHWqWjERTE6xeNOS+Ns9cXDRQqM/tODLmwajX8rPECjIHpFJQfbq4Jq5LKiqfR3vUH925caN7vtU0\nJwtn2VLn3mO6UXYRdXV8JqeKEEDFJp8ia5haxyU95c/yKZaBp2enyXR3iHNJIViWLMGWmbDmzfr6\nuRFJmbHKZSvC5rjY1Ljt2NByWbuOijoOwjKfvFiarTwz0BsxHDTuEsPBTmDVcRNE7mcycnxbeks9\n5qAzvkJlQYr0Kz077SPWtm4N/Xg4zHOoWv1R96zedkld6I6871Tn/Ux3R+Bn4mNDoeRj1enDRdun\n2NqautDtu/bnPrfIxS4wXFHCEedZte4QlMczpPq6fGtzJXyBZhTmJMeSGBO7veborrJrHahZD4Hk\ndj4faf/gra9SDtK9pyqW9AmaE+YTTxL6q0KUGASuoLr5P/wVLDDCJCkutqRJLOYpUnOJsWlT5d99\nMpCyixZdPpIbKp5q80G5KclhkS8LhS1bLu75LhZKbWiLIX1KiXQoM3tA1yM69y5hQZ9AjARvqkON\n2dNKtOxDZUardXejFfLU10Nmot/WwrXaxNs0oXOCSRoW0wa2np8V6aN1u1NDV6wgmpxNLufSBAdT\nniHogoPgLQCm3FN1dXHN0dZWZW00o6Db2qCxpnjUvotk8/bJnSJ5sWUL7GgOj0yziywqx9cvhEQy\nT025nAmpfjPqxZTnsIvlhWUbhGlpF9GTt8+nPEO909yQzMygdZ9xNgaPuSOQgxA7JNGY+uQ4if27\nnc2dqqsd4nhRNyCuaD/vs7EiwRSLOTY1ztOfLr+7iNjOaMW3YrrpkFA3YPfdx/p1RuD4KLnOnjwp\nGtlWunqZ2R2NdbN2GrZLfsG856JSaNa5zp+PRO7XHXqUlStEDsjVdkclArm6Gqq6S8tlqAVF9c4O\n0mmpq+HV1S/ZdqbcTWKk3y6caM0VmkZpJyRCSjUNy7OvrhZy2RcdX8TZZl3j8uVig2j5Waqr/e3u\nItYVuRirWHb9oUdI9XXR3AzJhNKPFcdDLOZpX7OWhZbLUnfgYX8NArVPe2WLLIkTdb5UG1xtu95g\nx0uy/xzr09EkAKpOHQqeg8w+WHX6cNmOeC/hVV9PYKepqnKKGVu49toyThQ2JoPWI/P8QeNO1YmP\njw+jnXCeT3xsiPjECNXV/luwbCxrn6Fp0KyX1oS1180AlIpQtMgxq8BtSYSsL+rv9joVBEXuRW2n\nZcvkf+TzjiNoNscNa4P7pFf+MiibrO5MeE0SC7GAPbK13wyKjFUdN+pY37LZuf/ajt3h7a60k5VV\nEHWPpk9P2hkji064i50GZpKE9OditVoseItVW1DJ5bDsoqI68t7sILTIzis9O+20/xw2tt75pxxp\nlnSPYheZASXegJz4+PCCRAYnh/p80erbt8/7aSIjPjVmS5uVjRAbP9NzIjQrqhIJnVLnKxeRaljN\nAZc3uY1Z0CdkbM6V3L6CS4sw8uJyiK5fSH4oUhG+EMxFTmO+MFdnwVwqeT/V8VQj4OcTmja3sePF\nXPpwpXOY19u9EPPMnIwOr0b2fMMwXAW2AqESwioRcTyCpqn5mfp6SPR0EY+bBPVANA33xAkz8tHU\nqN6wofR37A2ouXHXdWBmxpnnFMJyxYpIl4GmBchBlTs5lNu5Dh6E6WmJ+u/tJZGg9LMKgrmRTCbN\n6FzDoKnJJCqC7qFU8UMTKnFhk4UdHXD8uItYKRUBlD6mRABGkFMJgzY6ErxpVpw59V2KrNCQ0we1\nMU/xLPOE2hGPpI1hCImqONRsORsQnfuJ4oWQao/upLpGjp/pUTbbEYhQwDV2tFFFWuZsV3kp6+pY\nnp11ogYffjiwwcMK0WnTU057Pe7RO1frDVi6/wpSAwpBHVT01wvDoK1N+pRL17SntHa0ivjEiNOV\nzwrhpU+W1hMNQnu7Eq2Vz9vRy7pWerzrM1NoGCxJDfD0p5myTFYbWf0oZDzakZpjYzQei6Yvu369\n6QBQ0tcTh53xl+o/W3JNVwkYrUeeX/XpQ+izWb/D3uPoU+sQtNVIvYHaoztZtOc+/4nU/qCS+54a\nFvHZaRcZGctOsWyZQuyY5LyqNawWK7OK/yWH+lyavV4kk6adoZKAO4XUSJ87IeS5B1q/Q7h5bRT9\nzGn7PtSoWxtHnGfErl2BUgdVXUf8WSHe/hIih6TN5mB83LaBqs4cLblGef9t6WuHRQ2q92xpOFtT\ni9oHvfN2a6v7uyvTlclFBcE6f2LovCu6U5VT8aL61EHik6O2g1XPzZDoPeOSwbAKIZZaq9rbzX1j\nESfili2QCLCHq6rKj4iNlLGZz9tRr4Aj7aIex6PnrM4D+syUHRHtdUT4omALhaLjDODZzw7/X2x6\nInJ0r667HVBh0dfxsSHHsTE1YRPVqh0THxuKVNzVtaaXCateiJXBo2enfYWHwyQzgiTbvI6WmuN7\nAguFqvOPKwJe7Wtz0dEuBc+c/hu/IX8GFW0OQ2DGVth5PFDv33KwlMPtlHOdXlR1HZlTTauLhcue\n3F6xIjy1TFf6RSoTfWM4H7hCMl3BFSwM5jKO7YrolxBtbfNTyCkIpfgotUjSXDCfmQwXS7YmHhc5\niPk6f0UpkCbWri1t9AcRo5e9ZArhUSpRcP3y7lDSqiTKJd4jRGbON5JJs1+o0eoKORhJGsgk6Bsa\ngMfnUORGjVxU2qKobaN8LhYrryiyS7/TA00r4hDK5+Hhh33yBOrmtyjUyGfDcOpyWFHMldhyxaRr\nSkEhPttTpe/BN+dbJJpJhATNb/aGc2bGliLRpyYCIwi9482VzhxBl1uFStKFLVZqe2f6Tjnf3adE\nNj2opM6r0ipK26lkrbrBTh/cRWxqnGTSTXKpxLuFWEyIEUsHNTnQ46QDDw3B6KhzG2pbeCLgNQ1f\nlPWcUCg45OHp03ajRbErVtUoae4PlqdzmrrQTXKwl5oaqB31RJX29xOLmRtn8/5dm/Aux/GiZ2dg\nZqakRm4mYwZDBMgztLUJaZdMhmfIgJ8g0nLZwBTwKCRccvg8seyUFEovQ6c2FhMyR5+ZQu8/bzuY\nmpvlOBY5Gp8YCZRKsouV5fO2hnqpdXDlSgkEUWUmLGdS+nwXyeHzJQu2W2SwBX16kkV773c9Sxte\nKRqFAFq8WMZOcqiP2uNPuOcBPE5Q03mjOr6s69BGhl0SDipJo88GF4ZTSU6LEK478qvgIo/mGK6q\nMse+4ujf2irjJj42xKJ9/+P6mqbhkk+ryrtth+TwefTpSapPHgjsNy6y6ezZ0D6butDtcg4YaMHP\nsFCw5wT789PT1B6VrKjaozvtSNEo61tVVXHNdnVtrq6G9ISb/CqpS6yuBUGkZD4vc4lJEheT3LDI\ncX160uaGkoO9rMl32P0ufaF49oQ3WnjDukJRMtIOggiByylaBMlktP2ten3q7+qcoEZGe9da1XHm\ntavSPSddfTQ2OVZS01k9r7c/lqMHnUr5rzUIaiZAkGMDimTIGoYz3goFEuPRpH8sJAd77Yh66xy2\nA8xb4NswQh1pUTK2vPVILKhtaj0rbzZ8aNZHUPsaRmTZUa2Q942/Ys6dMCw0QX7Zk9vFJmZX5Hax\nzdEVAPOnqXsFv36Yz4jYhcR863BXgnS6cnI4mZxb5Ho5xU6KYS7Erop4PFp060KiubmyCNO5ODCj\nbii8iMfn33FaW1vZ2lhxHx6uXL8yHitt+M4V1dVPnnoFPpQgE5ctI5yUjxIteuqUQ5CpUckhafYg\n7VVUyz2iU6Ehbl53xEi+SFAjKKNE8piFRwGYniYWM+WRokSxHzzonCPsmotFcJ0pseE2rysed8br\nVVcRWoDR2tzYDs2w9P8SRTW94zx9RtEH3bWr6Hd9x+pyosTqh04569GvnAhqbdDZ9NQaStRWlGeg\n9FOVwFzFKd9H160ziRAz4lg/H9zHVXIwNj3hkAmFgvs5h+mm5/PO/Kp+fr+/6J8X8bxCGCqkZ/xh\nhVxTMlUWtyhyMiFzuqpzrLZpa65IcTqLrFcdk5OTthMurCZNcqgPDcO2D60oPF3HlhqxbDL7UoJk\njBQCI5GA2jOH7PGQTJZXh6ju0KM2AaZ1OlkMUeZ8lVS5ZkU/6XS0SNP2diHbUoM98nnDQJ8Yq2gt\nr6mJVnzRgjcTS9WeLqo5Dq5ipBBNhiHou8mku+1cBbk1jS2rp3zPX43stLM0DIP41BiNNVmamgIc\n5QGOAZWUrKlx5oViEauNjTK31h5zz2/Vnfuo6dwb+j0XlH68erWQhomRfiGJPevAlpxyzPNu+8gw\nTMfA8HmSo/2udtQKefdwMR3hKvlkyQ+pH4xNTwRm6KnBL3V17mh1rZAnNjFKzbHdwZkLpt2RSkkU\nqff4VacPkz7bGfhdlfyuOe3uY7HsFJmzx6nuOkzN8T1OTZQgmHN5fGwINA1Nc2S4anOD9u9h+sa2\ntEx+lljMIYHjcYeMTIz0222prqXWPiYeN+/dMFi2TNoic65TNMJz2cDiopZdq2lOTRMrMCB94Qzp\n3lMyd5YTjawUm7Ta3mo7tQ9598R6bsbOWNGnJtByWZbFesh0yTOKTYzajpGowQtBBW6D0NQENR4H\nbar/bOTI9yCHX3x00EUw69OT9rwRKgejkMfx8WHpB9b4yecjZ7fqM1Ny7xGvP7Ag+1wj0D3rqTof\nhhUJToz0Rzqv5bSx1sCgKPtQ2ZQimSfzgSfrFm5ecEWWpDyEaerq+uVDXl7BpcFcdMDnE0/1jAmV\n3I7HL51Ey3wVdV27dn7nlkquS9Mun36zenXxCJFK0NpaQmc2BHNxcJRbsNLG+cqJcRXFnveaNQsr\n3zSnegMlDM6iBE2JooUrVgCnTpWd3bF6tb+gslcD3EI50Y5BWLEigiVnRhDG47jJ7fEIMg5nlWgX\nM+o9FiNaBLxKEpubmSVL3MdUpUV8UPWcLQRsDNranHnfnkuPycbb168nJtB16c+h/U51Pqh9RCVi\nw7TF1Qj1MJJc1StXoswyGUWXfHISRkf912CipPPNKv6nZDxkTjgkSdH5XdXotz7v2TGERsmqUhRh\nxUmV+6/Zq0Q/qlJHahspG2ftgtMXYt3+6/RBiR5N4RDj+lEloixEYml9xk9up1LuDbCruFlPj+0w\ny6TD0+fXr3cIFCvqfdMmSO6uTPezqQnaY5WtA/G4O3svM9Bta0mrWvpR1sPq7qM84+lu0kLtJ2p6\nvqbhkLZp+X39kgmnXweMr1h2KtTxX3v8iYod3a6MlpDCn1GwbtIhyUuNzyAbT8tlZfwbhuPYU9c3\nz9xnRWxr+VkyB3e67j2IP2lvl+etFp5LJnFp25broI+cDYQ7crKUPVGKJIxlp9Dyszb5pWUlY0El\n57RctniB5EIBPTtNzfE9oUEu1vtLl4rDWs9Ou9q5tmO3r5BfEBob/Y6QxOhAYMR0PO7XOveSjtZ8\nHJ8aK67BbPYjlWTTZ6bEkVbIC0FvkssqSa4V8jAz4wrwW7vW1N82DEfnvusI1ScP2BG1QeNvxQrp\nZ7XHdrnmkbpDj1J9+pAdPa/i6nb3fKYV8ui6Y8/FRwao7djtyxgA3GT54cM+h1RysFeyNC50uyK2\nfTAM1/UuWgQ1J4WUbm2aFYdUPk9tx+6ishZWoJCVGbFmjfnse06iZWd8GRPV1bB9mzPu49MTdntb\nxXHr9z8I+XxgtK/6rFvS7vtLjPSTHOqj7sivJNq6CMlsORhik2OOE8As6FnTudeWFVMLOQaN23TB\nlDMyM0/0fM6WbwnCTTc5v9u638qEVnfwEclaqECDMjY1Tu2xXW5pLoLtMRWZ3pN2P4pNjDp2u2GE\nFjgFmR/VvpEc7JWi4lOKZJn5DBa6htevDbm9EOo754BHgYVMFN+xYwEPHhGp1PzJGfw64krGwBUs\nBOLx8IipJzM2b3Y2FeVuzpqafr011+eLhJ+P41R6jObmOZDbEaEWDywXsdjCOjsq0qG2MFp6YxmK\ns+HFqVatujzGVWuUtjMJN5/zJUr0THeR6NUyYElozEVzfC592IZJPq9eDcuXOhvGUKjkiKqhW6RA\noA01enyvEol4MHq0J0B9Stk4RzlvhM9Zz6G5WUgvFwIcZuneU65I+quvCtlBqFkSQ0p68/HjGIZJ\nkirjLnR/euCA87v6DBSy3lX80vsMre8oxHX6oBN1qp0vogG8e7f/PcOwj1lzQkhMS6Il6KOx8z32\n+IpPjLjkA4LGgM8JNxSSGq7owWszIYTKToUsmgiRvVHGflxXnAeaQ0KnTjtp4ipJ7Up79zxAbb+0\nTVA0bzLpRMl5SVTNKNB4/gi6Dq2LZkL7b2ur86wzp53xaDm45rpOpXYr2udlkidLM841V+eddcmr\nuQs4JDYm0WsYZM65Zai8DgUvSafPZh27MYI82/Llpfdea9ZUHlyxvV0pCBilxkIELWUV+vQkGIYv\nM0CfzVJ3+DFXm1rv279PhddRsKKWi7VNQ4P0X9UZXc5ew7t2rV0b/llXcIklqaVEFpciwUrJG2j5\nWbuv6VlHmkWfzdpkfmxm0nb06LkZ2trNqG+PnIgqcVPsfqzzgnvsB0WM19cHSIOUIeWhkq2xiVHf\nHG2R34vqDRe56EN3t2RyKHJkBhoaBgk9jz4zZUvTWASmppn3qTilLFkVS7Pevqe+09QfesSXMbFh\nA8RxX3Ntx27SaWdsahgs2v+ATbarcGmv9/UQm54InIPS57uKRlwnBuTZWlHesZjjANm4dhZtNufI\nbIUFmWgaz7xGxozVh5csngXDoPbwr1i05z47E8xyVAWNQ7Uegp7PUd11mHTPycCsB3WstMUuuMZs\narAHLZcl3XMytLCphfoaj0MpPyuFpzt2U3v4V+JEiLBGZEakHVMXukVOaPgC6fNdQrQf3VlW9s9c\n8JQmt1XN7fmM3J4A3g+sBq4HmoHfBb4EFE/wvIJfF9iV3XUzrblClJNueQVXcDlgLhuyZLJ4GvBC\nFnJVUcxYLwWvsV5J1LSFhSBhFzqK3SthMZf7D0OQtvpCwNtWdXWltVDnhLmQ20UwX3JFUWCth0HR\nbCUJ3TASLAC+flyE3A/8/CVGxX14fJyamoCAhJEIUgZnzwZH/0cpqKpGcZfxnLzQzoY4GKJE3iuw\nMlzUzaOmlchuCFhANI3wyHUV6oZXba+uAG1fL5QIWm1SIaYC5GYCYX1OmR/SunPN5RJ4tcef8N1z\n4LymyhWp99/ZGfx+GNT7VNtLyZjI7A2J9Fb6hZp+nnzE0St1Ra6HoGbWIZ7iw871XD3hFJNblfE4\nCYL6i1GwHSaWlEEspkhrEEByqX17n1KwVskMUCPmamqcuTJ97kSwTXSutM6vmnETO+u0uzbiXF9y\nPKA4JjIuLO3tqqNPONdZMElCPbhw2erVEtWbHJK2tMg5XRdi0XYwBvQbLxFUV+eOuHc9jwhrZXVq\nNpjkjRDRHjvikDSxvBIZqRDvYVl2tUcedxwiyn1qRsF2mMWmZR5QNZjr6+X4mlGwnU2x0SEnM8Ds\nd1p+lvWrHcKw5pg4sNR+Eo8XyeYxCVDrGiCAhAsb1wHvV1XhejZBxUzDvuu1l/XsNNXHHWeSHSFv\nGIFrhBUFnsm4r6FYTY11a8M3ElHmUvVcpeSOXP83i2db11XOfkZ1mLh0ns2DNDRAYnyIRfsf8H/X\nKMDsLIYh/WvrNn+j6LmZcHJ4AYo32rJhEWFH908UsXOKNagyX65fLz/17DSpC91iN+RmHCeJ6UjS\nNLPtTOekPjVh2xiW/nZLi3zeLlBrOh/UfrdtG673rOOrNnL6fGk7IhYL1vJODfa4xrL9vpJdkuk8\n4NMft9YcPZ9Dn5nySerY96A4+a1MKOuzWiGPlp+l9uhOYtMTkTI/5gNPaXJbRblDL2jBM4C7gU3A\nh3EitseAbwO3Aq3AyxbBv9bAPq7IoUTBQrXRmA6fB37MwkTuR8Vc9FtLaeJdwRVcwcXHXAqCemsb\nbN48t2sJwlwcalFgGX/zgYW4/4sF79xeW7vA5HYRzKeUykI6Va0NSyqlpGGauFhOiYVEUNRQEFIp\naWe7gGURNDUJQRB13C1dGuCwmAiP5lMRGMEf5btRyPOLCGue3brV3IRGKWQbEMW9ZAnR9NBLOE/s\nzwVBJWX2Bmv66meLX4MX6nOca/ZgpKJb+byPO0gkcGv3h0ElRxQNcRVFiR5TQifd6RCOqpasNlDE\nSWCeT80mSh93IunVedVnj5sSOps3OxfoLZ5XVEbBQlhBvYD+mEy6n2dDatK5dpXQPeaQHPqAEOv1\n9ZK5YPUNTXO0gvU+R4s9cfaUcxwlm2VR0h2NGbQ/UaUP6g49apNOei6YMFafU3JIigZqmkRkbtki\nJGxYPShNEwLGHlfq2FEyEhI9wcRQbL9SpFZx5miPhUvmuJxlJtmnZknUDTnyQdfVBRdS1HMzdr+o\nO/o4GIZTjBV5xpa8gerwtaLeMX8sqxokMTogWs0zky7N7dYLTh8OIyXToyHyPg+FF4+0r+XAQ66i\noRbi+5RMkOFgbevk8Hk7CrpWd9aW6lGnD8bM6EQvwZkY6Sc+NUY6DdUn5R5tO0yZR70BKLGYQ4A2\nNUFLY96dJaM6JB97zHcdFko6wXfuZPVqd+aBd01NXjgb+L9E5xG7f1l2pJfc13JZWzvc1e5K0V1r\nrVu+3HNt3knUmmsKBVueS8Og6xLCeQAAIABJREFU/pzjDPQVSQyBvefYu5e2tmhR7l7o+/eG2k92\nEIwq2aHMP8neLl9whPqskkm5JlXr2XKy6NlpF0HbUCv9wpu1Y/+t6tdPjUM2KzI++RwcPsyqVWZf\ni+idsKP7Ozpc75eSoFT7RrzzqH26lpZoa77lCEgMOfOAlp+VjKJW7OtfulTmY0uaxXdfSu0V7fQp\nl3b48uXuuT/SejgPqLAk1OWBxYoeTrkE6vr17oFxPA3vAe7zfK4eUE16A9iZgJ318FFgBfCspUJ8\nPw9IA3kkwrvHfJ1Tfu8BOtfLMa2pVkckVvQSrySgb4QWoBqYXQVLgRrz72rP7zlgMuR1bqUcz/o7\nAbwUuBnwzpeVIAv8EPh34JfAc4C/BbYX+1JEzACfBf5hC1hL6zbz+L/D/Hh09iLE+STwv4AKisUW\nRS/QAawHFjiD/ymFixW5e7nCAL4D/AJ4FfDiS3s5lxTzkfavaWJIhNjwJbFQEbPlRDxUgmIE7uU6\nBgvmq5hR1N4uRqNlaC9ZUlzmshgymfl1EqxePX/HulhO1Sdt4c6LAE2LnrVgPY+FHtde1NbOv8b/\nfCJKgehf5z42H6iri5gwElCwViWGly4twv9HKXYbAfMlARhGYFlRm21tAZ8ZGfGtfU1NMDHhl4Tx\nSSYFLJqWA93bf61IXltdQI2mfiJAYgZIHDuIYR4rHncHJtcVxHipTThR1rGxYTDnJn04XPJnzRrY\ns8f9XubMMTAd1tu2wYEDpjyDJ1p90ybpD+3t+O5f12VtXXShAxoBxlGpspYW8WfkcrBmyTSTk6bk\nv1k/wPtsUmdPBG+k1KjpXYq8jSr3ccTtnEmnxddXVQXJk/JsLX1d77lVzfxiWrUWqs4chSbpx1sy\nJzk4uMh3Pa4MgDi0tQJlqGmpBYn1ScWppha4VdpFHw02bhfV5hmaFGtJP+dcgKtmwLhDGHqdi1Yk\na/Whx8EkolW7suXMLuwead7/5s1w+LAznjQMamogYUDBbDsQbe6WyX6ySVjeCr0jThFbcJ6RqwCe\nWntBjTKfGQJNxqOqQpU5c4x02p2wBKBNjINnHqqvdysPVZ3tIFdjMoBKbQu1jdroZUSTebNH8ZHU\nH3QySRYffYCqRQH7D8NAQ6LAO0MSnrYkj3OkR4pmu56NV3opwClcUwOjanS4enNDQ7S2SlJPGFGt\nT0m9Aa9CijY2yooV5uFCsqXqDj5Crk4edDIJzW1yLqsIqIrVq+GIyfnX1sL49IQ9DlODPU4ksjqH\nFvIwMEBTE3SH2fbe+frIEWee9naIEASuVWeVLATCbRfru5kMrN8i/mN1rKXTkI372zfZfy5Q0knL\nzzq2ZaHAtm3BpV/UuceLTAZ7vFZVAZMGsalxGpbBqYgJaPOJpzS5rXaMncBJREokCqzJbxj4e+Az\nG4WUttAK/BNC9h4GfmS+HsYdJdwFdLXAN4EqhAzvo0gksYYwz5UiA7a/fi7ESUDV1geA/wM8H3gT\n8BqiXao6iA8ihPZXALW//5f5em4VvCNtr3WRoposFICfNMBvA6fA1bsPAK8DtgJ/A7wW3/rjKjQT\nhFmEkP80cL/y/n8g5PZ7gVdGv1wfDODRJHy5Fn5ing9gGXCt+boOeDr2Gv6kwgWkn397NaxB+shN\nuLXvLwamgL4knKmTfqAjTqVMyM8n2yQ4BfwMGEfIZ2+ttmKw5q1Fi/yL5wNIH7VKW3zWPP7HgVWV\nX64Lu4F/bZc54vWIU2y+MAr8AHHKvRjIxMI3s0H3r0LT/FGip2PiVCrHmaTrcyO3r9QyuPTIA99u\ngpchzsr3I47soL5r6cdaUUFzlbKYS5T35aCRvZDwZic0Nl588rdcxGLzd42l7JX5gFrUqhKUW7Ct\nXJTTBlbqbxDmyyE3H8cJmlPm8gxWriz/O+rauWTJ/DhhW1oiBbcHoqHB3S5NTfL3PHHiPpQao1H1\nh+vqAqInCSB9ikiIpFJSeylUuj6KZFAEqI6iMFIqnXYTfBZUHfui2vAmGho88g5lPsj6eidRxCVL\nYkK9fnXsWM81kzGD7pViuvps1r8pBPR+U2KmKrxgqOvc589KVBmE6htv3AhHjzrXEjZxWA4BS8JF\nhdW2S9oK9Hq6j2obeOUtMhnluajXF9KPUof3yGbJg2XLYLyzQD4H+slOITfwyGOc7PR/EXFsqCUd\nLCT6e8CUSlnfNs5+k0uz7t/rDN64ETJL4bhSry+RcBwJS5YAzdB7ILgvxoZLj53MiYMOIaFAn5ly\nNo/33WdHzanniR87FMr1WNkwqZ0PgWlX6poSiX3BGRPxqfBCkKtWiYOpZrzHJgZqO3ZjpMwBVqRI\nq5UxkBzsA3OdsAjfsDmwuVn4W5ezKiQ7pz4WzKzXHttFMm2OwWElK0glmUdlgK9c6fY76LNZUoPS\nNvV77qdtmZDbat+oHz7NLGZQgHJ4Va7l2rZu9g7KIE4O9jobbaU2QHdI92iODTFl6Y4HkcVF9gXJ\npMfR5ZEtW7cOjh8flTUmgEuuObrL9bf1nDRN5uH+fmdOvOoq2GsG3icSbsmSxl5337TWfM0o2EXI\nCUnsWMlpugZmQ+2dhgY4NTZLcnRACCwFiQQXJQLqycbrzCvUsXmX+doMvAJ4OUJIhpEvBURD+y8x\nn6/l6QPeDfwd9lzOVvP1PoSwvQchuu/BHdVtRUFfzjCA/zZf70BI4j8AnosTEZ3JuDc1K7fBFxFS\nu1R99PvicN9S+ALiVLghQiSGgRDj7wP2eAz55cjcZk2xBxHSbTNCcr8Ox54JiwIaQq7/34CwevUP\nI2T/euD3GmEd0X0U48BXzeMfCCie0W2+vq+8txohui3C+xrsQAsMJHp9TJe+O4N41NSfs8BG5h4V\nPokQ/nch/X0WoF4yHO5AJHz+EOkjcyXkJxCnQgcyzgaUVz9woQEGG03vYYZAgywIcfOj8a3igDKA\n7FbpFwXPy/D8vQzJaLhxlfSpSuWDZ4B7gc+tEBLa6q8ZxEnwHmBDGcdTo5KPImPjBwGfuxcZO7em\n4H/FpN+WCwPYWSPn+BlAqzh8/hoh029D2rVS9CEOpX/DPZ9e1wivRl5eZQtf0bAAZIH/QWSLftgC\nJ8w5q7URXjUJbwd2UL5zxrtuH0Xa/hRwA1KfYSE4OGvcTyFjoNjP0ThMVEtmURLoq4J2HeqAC0lo\n0mW8Jsz/x5H5Paa8dPNYOU0I4krMlXIJujFkjG3ZErw5Khf3AX8C7FUIiPchztfPAjdGOMbSpcXJ\np0lk/diLrJcvIbhPFdXADMCKFTLffhfpw09HnO1ROO+GhmjE40lkXq9H7CYQ4zRKxOxCw7v5Kkcq\nyPvd6mq3FEu5ldyjFo7MZNwaql6U4+xQn8F8bxUaGmR+/A/g68DTkPm8XPgiVD3IEx7ooa5hq1eb\nBZ6UcVaMrA5COUS7de7qajeJ1dRUHpc4Xzru27ZV7kBtCAhUgXDnWCwm5FOUz1pYaL36lSvdHEA5\ndRrC7r8YypmHo6KxMXp91EuNMImrpUvNCOkin4/SFypxuIQev8jA0DR35klbmxBz9fVCmKVSMtdP\nTLgdOuqYV8/X3u6WmU8mJcC0qYzNTVOTe+7eUMSwL7YO2XZmEcdIdbVkhykKNS7E+84STL8HnCcA\nVvE6dT3Vpqd8Bq41d1s8a5hudVGN7zB4BpUqY6NpwJgTne3VHU6cPxu6T9R1WQfCyi0kxofQGpTz\n4P89NnC+LEnU2PCApN5DoMyXN/tFnafiShS6psH2jUK+WsR4bS0kTwlbaY3X2PSE61mp8l319eKj\nUVQ8XJ9LDZwrSXJkDu6EFiHE1bIKanR/bP9ekRTA4xg4Kt68ohrnqiyLUpSytv+kzWlbRHIs5ncS\naIW8jHVFB7rqbAe0yNhZWQM9ZvfyFoG1ZFeCisPW1pr1DI4+7pIjqK8XQr/65AHnOZ9wF9isqZFr\nTQ8Ee4LjU2MyloD0o/fZziDrHgFigzJJ67pbH1zFxobzHDbnssaC83Bqj8g1ZzKEaqk3Jsfp6RsQ\nO6n7WOkIPDUTosotobNQeEqT268CPoNswC0cNl8fQzZsL0aI7pfi9LWdwB8Bj+HG8xGSpZi93oxs\nMG9GIgwfBO4agQfqlYhqhDhoL/FaTDDBFvaaRgjACYQcK/ZzApnTqiK+OoEvI6S21U0nEAfAlxAS\n+RaExNzYLp95GCG079bks14sA94MvAgnmttaaH9uvl6ASXKHtPcuhIz4b8/7TcC7BuD9TUKIfAp5\ndhZpeBh4I/CPyKbt9fid9geBfzWvy+uUiCFERQYhdq3Eog7gg8vhdgP+N/BOwkndo8D/QzaQQVmf\nmxFCbCrgfyfN1zeV9+qSML3J1IKPIVosJbAGcfI8G2jMwFUEBi+4kEdkLe5CJC6KlXk6Avwp8FdI\ne70NIYyi7IUM5BncY74ewNG5D0SFKamzSB8hoTh5I4YcH0f6yL8uknX+RcBVjZLZUYpfzSF9/G7E\ncTECPmZqCvgccDuSFfBnRG+/PuAfEPkc1YBNIXPZvThzxx0JuHuljIW3RTg25vf+E/gI8GgAK96F\nOAI/gJDz78RxCEbBSSSq/A6cdCcVj5uv/4M4lW5oE/mnGwnvCueQrIgfI21v911lJeyLwedrpd22\nIHP5G7EDG0oiDzyEENo/wD3vfxb4C+BdSDvPxeFjAA/H4FuIg2nw6jJILq/zR/Wuh+hbBmIRrgwh\nrQ4StVJo+fcQGahivF8UkrQPGSNfQ9bkGuDmJLwgU3wtLoauGHywXvpBEA4gWSdvRjK0itluYaTZ\nOPK8P44TAHEHMsf+BdI+6jQTi0WPkJzWnGNbpvGXzeO+BngrboezF8XI2HFkXr8Td4ZSGnjOSnhX\nnRD05WLtWsf4PouMw/8y//daxEkVxelTVxeNyO4GrIx5Vb7tRA1sWCfPxHovpoEeU6TalorjS5Vq\nmwJOtUMhA/mUzH+zyHjPLzF/ApNrIZZw/rZes+vECb1Jea3CbYSXkqnJImvqfvO1z/zZD1y3Et6n\niS1bKtA3KCPO2vxkgXtWwoeQORzECfQV4B2N8DtlSGuHEV2WTfYpILZVAg3e7vmMmlljkc2qM6wY\nWV1v2tvnEOd/OQkSLS0OWaMSo5oGy5bDvdPwCeAQ4vx8TVzWiflClMjteFw232XW2HShmJNFPZ9K\nbllk3qWGgYzJgbjsTcaBPdWyVowDBxtkDV4EvHQlwRuQCpAHHqoVycljwBuA50Uk9u10+yIYjMGf\nA9+vlrnin4hud4Rh2zaHWDKAYzFI6fNfcGsA+GYTzKYkIGOhUI5TS9etSMjg/ycSwWtuMYJeHQ9b\ntkjUrErAhX13wwYhma1IfjV6OwwWSVtd7S570NoaGhTuQjw+t+wZdR5oaZGo/aDIfcuO649DkyZB\nEl6EtUuUtTynwb2LxGa4NgGLc8VJd9c9VzhhpVJisxwOkZxeutQJ/C/WX4Y1uDcD8Sp4TsodqAzl\nyX55s1/UvqteQ0uL8+xqE9NMZN3OAqvt4uPDrj2nd51pqZlyyQGpiE2Nhy6smib9O0JNW/ceIIBM\njVrDJNHf49rH2BHNJlnf3u4vYeDVoq+qciVz2PrQarR1VZVJPveeKroxWLbMmXfLXTe3bYNzpzsg\nKQ6MsHZ0zTvTU/aGIn7G75FYtcrt5FIdxPGxISEtgKqUM7HUGSP23lvNHrFrHShjNx4328SU7Kqr\ng6kQObOaE/scwnWBcHmT2wG9Xo0wuAExQH6AbGJ/iZskGUHIwW8iZNEzkEi2b+MmCZYDn0Q26uUE\nKCQQne3nmYzOGWRD1EblEZ6XCs9GyOtuJMr4S8gmy8IZZDP0IeCZSNsGBdYlEKfDbcALcUioGxGS\n6u9ycHcc8mZDWyT3C5FoeYvk7kSIuG94jp8x4E81MRDrTeYoBfxfhGS1SG5rfT6CkFf/iBPJfS/w\nsU1CpnrRhJBS78CZRz+IEJyfw4ksvaCJxveHgbeY516DGMc/Rpwu/4UfVQX43Wl4b5Vw07MIEb8T\nIfJ2IhGAQfPkqEbZYaYnzNdXATYKcfRMYE2bkCTPQjYIBvAEQmh/HdGGD8KzgRu6YWyZfNZq5xnc\n2RNWNLc3IGhEh+/qQp7fi5Ag5SKJPKcm5fhq1Kr39/mKepsAvgd8b4X0p2uRaMdXIJFvOvI870PI\nuu8CYXudDQj3aJXHMXDI0uuQiOjX4J/ADUM2fZ9ENmDeve/NSH9diZAif4bTDyd0cUJ8NgXvrpXI\n/qDulEPG3UcR54MKHbhxBA7XOxlVFxCph48gTsM/pjjhuc889t3giypZj8zH93v+1wF0tIijaDHi\nCHg1QvDtQ8bcT5A+HIZMAap1t1zSIfPa34/UBbgZIeK8wWBTmvl8NPj+VlcmnA/nkLnug8gY+BOE\n6IqKbmT+/cIyOK0ugRdb/ycAhgZZTZ7P/cjzfh5C5L6G6GT+GDKWvobM/+qzHkfm2s+tl/nGyiCK\ngnHgM23w1cUwo7RXOg/vj4kj9+9wuJA7kef6UWQej0IIjCFZBp/A3Zcs7EfW0vcj68JbsQNXSmIQ\n+IwG/7w5uI/NIPPz15H15i2Iw6eUv8JA1rs7EWdJEBc0DfysQbIzGpEMhN9H1uRS7ZIHHo/LOPwx\nDuls4VvIuH0LUsOiWD1LTQvfTI4i9ttXkXk2cG6fS9HKKHIEYZFGNf75J4nMaZs8rw3I81UJ7P2I\nQzxMBvKBWnmGGxHH+h8AXt7AarugqLgZZFx9CLHnvOgHPrACfjgNt2tiJ5SLGeCuKvgXlIzbhMyB\n/wzc1lDcwR4lmu8B4MOr4afm31XAbyFE5EsQe7BYwdSgWgx9yJz7RaBDcUDsBu6oheVp6buvKZGO\nXAwF4GBGrvsJoLpd7Ngg4jyTkVefX60gFGvXyjP8IfDEYrFfrSycnmYJxIkBvQ2wTBPbQtsIu3Sx\nkyaAyS1wolfmq0nzvf5m0ffMAsPNMNkg3802wqw5APNIv81uc/+dT4O2wXQyJYCrnCwhY5vsFywH\nlLFU+q51XgPcARyqY0hhhBPA81Pw/Fqx3ctJOrG2mCdSYvN8BTinSBTsBprXwV9p4ryH8qLKLQwB\n/zcDn91iBrPE4HidrIFvR9aKShGPS9+6JyUO+8cWQawenlMjztAXFvlulCjsY2lZ774KTJvE7ccQ\nR92bNHjWPNdvmC9d9SgoN4MH/JI1BrJfHaiCo1XyfAuAkYGj1c46frwa8gVxup6qhqqYrLljwOh6\n6Dwvfff0Sog3QP+M2OxTQKEZ6moko7euGhYlZJlbbL4G0xLguWotJHQh2sbGpH8Xyzrbts0hUzVN\n5rB43F1npCche4ivA09sA90QruSVVbIWlYLqaPf2tylN9usf2ixSkwDpzfCGC/C2IpkspWQhVOSB\n/66X+f1ZMbhamf/V49TVuYn9sH5ofSe9Dv5Ohy+nYVIHNsCaHLwsAX+YgbyZBl5u7R+r/buSskf+\nKeJQv35K1vx4zN0H16/36+Krx/JCzUKYy1hTSeugPmYAF+Li0KtCxoTVdqpco3qNK1a4M3jazUDK\nKQ3O67IuHKiCWBWkV4od05+Bujys1OQcunI9loxHGKxCxGo7VFWZZHdE0v0McF8tLB2LXlNO7XfW\netLS4s6aOZGCLzcKf9m4Hv6KPK9HslMyGX+djCAniuVgC8o8MIAjqSx9NfC6CIum+pxiMViSGqDT\nPJ6VMdHc7JKYX1Bc3uR2wNPyRiMsQ6Lk3oUYRb/A2WCpBryBRIWp0dpJxLj8K+Ymg21hPgoxXmos\nQ6Ra3ocQrV9GFjU1U9Mb8Q5ioN+GbOrD7IV1wF0JiTb9IGJIWn41S5P7hebnvogTLQ1ilN8G/J3m\nytJwock87p8iG6l/wYmYPmZe2x9iGpeeDerVCCn3evzG8VKEuHu/eV2fLMBZc/KcQgy/zyIZAvsI\nljbZgPTRF/ZCUxwWm107jmz2rkKiCEE2iAdwyO7HEZJRJYAS5i2klJ/q77PmtXjrl49jys60iTQM\nyLMrEOyssK7dim5dC+zphx3LxMD9BhJ1rJRq4TASyfuXCEHyWoTEvQd4bB0Uihgm2xBHSCsOgd1s\n/tQGITMJm5ZF5/kMpB9NA48fgA3b5LuHDsD2bcHFXK2/DUS/+kdIBLNXXW6n+fp7xKH1bGQDGSJj\nxWrguX3w7lbpbyAOuU8gxKyFxxGycCVCCtyGkBh54KtJ6YteR+/zkedxjfLeVYhxdA/wngIcNfts\nlw7vXSqE0ydxiqVOIZGnH8PfhxMFeLMu8+X4SdiwQz77Tzjz7ChCnHwKIbDeizMnGmbbvH+1/PTi\n6cg8/GpkrA8hc/gPEONOJePOI+PwiwHH8WINMi6vH4Drc7CsDb4zDHcn4d4qd8bG/5ivP0IcFm9E\nnFnfi8PPN5mOUw1f1H8VkiG0ATGgrUiIKWRs3I5EW76H8ECAafNe70TIRQP/eUDWrGL68tZPcjAx\nA8ka6f/Dk6CnIG9Gr2ZxfmbxR6EWrN8NeRW04HFbwJGxeieSifM65Dl6HQRZ4P466b8/JDhjxYuH\nzdd7gFe0i4MyiLcqIJvvvwR6PFpMNwM3H4EXm2Hgr0Pm+u+Z/x9E+uudCPkXVgZiBHFafhK/02o5\ncu/fxOmrZ8zr/kdk7v/fhDt9uszjfhHZ0KpscoN5D4/gnmdPIA7gv0X0xN9q/lRxGif76gR+6Eg2\nSg+Oow3z/qy+uxIhDn8fN9c0jMwvlkRbqZoy55Fn/1HznG9DxlmpPUTOPM9XkTESrZzPkwNZZO0O\nk9OtBEeROer9COH6RziSoem0Pzp8BvhSBj5d4183ms1jfAMnintvWjIz/hAJGvBm4wRtiAvAjxvg\nC61wOoRMOQX8zUpZdz6MzInlrOMP1soY8q4fk4ij9G7zWl8DvKFOnG7FNkB5ZK79ArK+F6sbe8a0\nW/+hBp62Tua6V8fdcg1btkBHh/t7p4HvNsr9/hwYVNkgU97rmYjt92INdE+DRNlg29JxtYp0nNdI\nXhbye9Cz8mrZqduwBM6ADSJGvA2ueX6PKb97P1sh0ZID7o3Dveb68KI43FQHN5SIoh4C7t8i/f9X\nXt0zBf0JCRL4JPDWevjjMsiqMQ0+Vyt9bCSAQMghgTP/DrxhsexfypESnEHmxQ83Q6fSnnkNfrlI\n7MuVwCtaZb7wdouwKOk8sj5+GrgvxDPfkTGljNZIkMxNzfCiIoSkNxJ3ArG3rH35g1vkmVwL/CZC\nRK4N+W5UzCKO1m82C/G/GViblIwMkOjccuWA7OykpNjtd2xU9gZeCRJ1LlYNF49OLeAYB5bhpDpR\n4kCNuUYHSTKaz0hLyZy+RYfNLfDqJrh2Fv/Eb8JLSiYSQlAd6ofb4/C1zfCIZ1wWNJnDv7UBPpeH\nN9bIbWYy7kwTL1kHDpE3rosd8PFtpt2gENLTOtzZCt9phr8siA1VCaY0+Olq+GcNTtrapNC6Smyl\n23DXaItaM+Awsu+5q8bNUQCcSMBnlggn8MwYvHJQuoQ1hVrzeZim/2HgW5vgGzk4pj7jWni8VsZj\nw2Z4mSaO3BdRPJBo61Y44/FkZ5MiH7vPfPUsk/3mUh2qy9FYUaBGTZ9MSW20nzbKGLGxQ4ITq4Cq\nRRJwkkxLcF1+DVRrUF0r3xnXYSIGU3Gxu2evVo7jHWPKmqpfDbV5qM9Dqw7NrbLvqc1JFujSKnhO\nOenFATCQoKjvAV/bAId17I3uthl4yTD8Zn/0wGXLhtA0WLUZPtML322GJxRS8my17Ec/Afx9m+xp\nozhNgjJnZxFb6ePATjMq5h+Al7bDqwZgTZl9YKEly0LPa1wEYe/5hqZphn3d991X0TEMhCC0UtQf\nxk0OvgIhYeYS5PPrgizSjl9C2tKa0GsQMvg2xEAvt493IJsnleQOwm8jpFk50Y8ghpJFcgdl2urm\nsd9NeYURcziTw94in9ORiKJ3IcaajnhkwyKrimEaGJmBsydgx+ZoEYZZxLCzCKKHCLVxfFiMEBo3\nI8Sj2jZ79kgBHBW7EePxLorLmHixCHFoWAt1kM1nob9f9KyCCvhEgXrdQfdQDAbiIPkR8I0x2FNb\nfDNsYTlCpv0eYrTvDTnvIWQ++gp+h0Q9kgZ6bxaOeqIStiKGVimiYHgc/mUSPrPYT0K9DiH0Po2/\nvkUNEjn7wgPwQnMjpLZdFom+/QhCuqiIIw6lFyDG3sP48QKEkHx+keufRsjTOwbhwQY4X+RG40gE\n9ssRos+KTu/pkXHX1iYGdzYL9UtFLuariGOtnDW9FRnbr0LGtmVDzCBzw6fwR68CrJuCv8wIUZjC\njAxEHIhBkbp1yDh8MxDfA0+P2GfHxsTYtAojHjsmG7lyCxwOD8OpU5IxtXwFDI3A0SHYvUru80GC\no2cTCOH/OmQMfAPZEIXxDTchbfJahAj8f0jmg3fzoJnHfSfyfGOIof7HOIVULTwDmfufhejoenUz\nf4wQg6eU92KIQ+nvcaKth5Gx8SnzdxUrEdLgTcjzHEQcna7IVRNp5Dn+Gc6mfR/iTPo6/iyGFYij\n9jblWvYgRMhXA64FhBh5SR88p1U+84uAz4Bs8t+MzO+WvNIB4JN98N+tDtHpxXak/R9D1pOwzOkE\nzjgcQUj7oH1jO7LRfCtyvxYMxMn3VaTvBNVw15Go8noch0wBGBmTTZJX1i2PzA+qHFvG8/vwOVi3\nRP5Om/cRi/ja3Qmza8VJfASZD6OutypW4Ti8rVcc+Odp+FrKzOBSoCHt/G5kPrX+PYP0lQ8j2SAq\nWpCozncggR2T5uc+WoCcYlw0mu//QQ7S5sbcMNzRPz9BxsA+zzmWI+NoDJGu8kpZ/waybjwrvCnI\nI/PAh/FHxWvI+DsV8t3FiHP99UBLByxtlw3hacSRdQfBEex1BtysST//KfD1gmSceZFG7Mc/wMlS\nfKITji6BBzNCnHf4vxaKtCHHe4sm66GOu63VdfdwB5xZL/bWdynP5rpckAHSOYlSrQH0cWitkd+z\ng7CsUfpEULANSHu+TBNJ8Ku7AAAgAElEQVTba/k+uH672Gyf64QH1oqjzGtrATTl4NaE2KIfxz9v\nrSjA3+pw4wloa3I7OKxnNInYPB8pwKCn72wDbp2SOX+Xh3yoN+C9mqxnVXkpMrl9u/vYAA/ug0e2\ny/7GO8fEDZgNsJFiiM3ygk54+1rHl5DPS724q66S9esDZ+G77RIE4cWmSdhaJWtnkJNRN0TG5Y2I\nk2mgU0jDmjr473PwqAHdS+HBLBxNOBm8YcgU4Dd0uHEG1p2C390IuRno7HQXHD5+XGy72Rq4dxh2\nV8EvZ+BgbXAdrO1mW7wSWDcMI0Oi+z8+LrbixETwXuEXB+DQNrF5Hyl+6U8aaAZsmBaZtxuAhoPw\nEkXv7cwZIcESzfAfI/C9NDyYDH428QLMBvSLDcC78/DMI7B5lWNnevdcy3eILfXPeRj17IEbc7As\n4V9HlgHv6oMX9oj9e/KkyEmpkh3WeXI5ODcLn87DnRkYKrLP1jD3nmfgrYuhNgV790qkeV+fRBeP\njYntuGOHPO/3j8B9AeToeqDHgPGANqtD5qBnd8Cb1jvrcz4PBw4C2+HfeuHni+BkMf3pEDwdeFov\nvKkN0nvEVqirg4Ym6K+HX1yAJ/Jwvg2emIVTIR7flAHPHIVX5+E5I9Bi6qj398v9Z7PivFXrbHSa\n43uqDj7RDT9qhMNzKcB0ERAz4BpN9hzXTUHbcXjuVU70e3OzRG5fuCBze38/bN8BPxuF7xuSBRCi\nguTCM2bgN8/De5aLveXlHM6elfO0tMC3j8A9y+A7NcF2vRfXIYT0iw3Yt1f4pKuuckfwq+OuthYW\nr4XbZ+HTGpwpMi6eMQlvi8OWw9BSI2Ngxw6pYTA05Bx3bEye/44d7rbL5UROvrkZlt/yPAzDWBD6\n+9eW3PZiCIn+eRwh0V48L0f99YNVUDOB6T2ah2OGkdw3IsTd9XM8/jBCNnwK2Wg3IJF672RuencG\nEo3zMdwSJE3m8d8+x+MHIZ+vPJXIQDZzDwP3jsH+WiFLLIKiGtlc3YwQdmFRT8WI4TGcaO5dAf/X\nDLjWgJfpMgavK3IeLy4lue09zqodsnn9EbK5VzfubTiE9rNwOyJKnbcPIfb+DT8ZoKINIQxuJVr7\njY9LoZyWdfCPOfjXmJvE8KIZ2WC9CxkvpdoujxDFH0II22LQkLTG9yGEf1Ts3w8bt8DOmJzr+8jc\n0YYQnS9HyJ2gLNkgctvSOwSJtv4GQqYF9VsQQvCVBmzpgJs3FHcwGYhsx6eQqEDvKtyCEOQHAr6r\nIeTGW5DxaO1/y+2z6lxRKbk9MiJG2OLFYqyMjkr7WaT5WYS0vpvSxYS9uAohtF9P8DzZh5BztxNM\ntq5AMiD+0/N+ax7+KSbzWCkn4CSy9nwMN5G+DFl7jhDsHF2NSM/cQrCO9BTiDP44/owPHen/4zjS\nCirCNLu9x/8eQhr/MuQzXixCHCW3IvNukMW5Z48Y8g8hxNk3KS7Bo6LYOJxFCJHbEfvBOx50xEF3\nK9LmX8GtZa9iB/Js30BwFtdc5/ZKv9vZ6YwLC6MIyX3E8+pAsnG8JPZWgucvC2NINt2nCW6fzUh0\ncwEhhL3E3GKkb72d4GzFHxyC27f4++W1yJr0DOW9hxHHpFferREZG+/ESZAbQcbCJ/GTTUHBC1mk\n/30E/30mEEL5LxBSYT8yd3+dcKK7PQevMeB4UsmM8eAmxMnyWtzByj1D8N0c/HSx9N0gh0474pT4\nlVGcsGvKwUsTYtP+EiFYg7J4VyAOs1txAj6f2AOFHY7DJ0wr9XrzXgwcp07vBWhscbJxCpo7S8dy\n8lQDUxdgRYv8Xg1MXIDGFLTXQTIHaQ3ScShkQStAdVqI0jhwaD/sMKVHYsDMpJBn6zfCdBaOHYdN\nW+Qa9h2ArVc5zqeublmfljXIeWOE2x3q76eAu3JwtwH7Q6Rh0nl4QUzW9iDJvQRCdN4KtO6B68xj\nTwP/2At3tEKf57muLsBfF+BNcYcofmwvPHa19GevoszaPLz1DPzFKhgfhfMX4NhaGSteR3gLErV6\n00G47irnntt2yHr0mTyMe/YBtQV4pw63DML5abgzDj9ZHGxHrkT2KG9G1ssfdMK9G2Te9WZUxQyR\n43k3UL0HnrZD5qEfIGPuZwQHeiSBGyakfsETcUfCcC5oBn4jD9v64JYlsoY9BPxkBPbVwOFY+TKE\nrQV47gS8sRaeNQ6jvWIvq46EE9tNCbWQ8V2FZMDOTEgftjJAJ8ehtloyMqbGoboKYjpMTUBLGhbF\nZL6vBcbOwaYl8nsdMH0BGuKwskHGb8cw1K+HE+PQk4eZesmGOg+cmYaRdHjwgBfLkf31DUBhAO6p\ngZ+nguUwYwb8pibr7dp9ULdd5vGv4X/uDbPw7rjM/Ytxxmkv8L7z8J3Ffkm0ZcAbu+G3B+AZV4vN\n8df47aZ1M/DxFFx1EhoDyO2qHRLZ+iX8Tqu6Wcm0uacg8hZeLDbgVg1ecAqubxZnyY4dUDDg307C\nt9YEy5g+KwvvGIVbmuHsIHynAN+uhocywf1wPTKv3wD81IBvFZSocg+SBXiJLrZi92k4sAx+Hgt2\n9FuoN2DHBAzHoSMFkxXSinEDrp+Gl07Djf1w03o/uT0B3N4HP22CX8aD18VMXuR3rJomlSJZgEUa\n1GkSzFCPcE9TQPckjMRgPBVc2ywKtgDXjMGWAfjNFKyKwbkB2N8sUpQPNvvncwsJA3ZosKcQvKeO\nIU6UZ5+GP17p2HdDM/CdGNwRD3aUxQx4lSa2zrfOw3dapBaPimca8Acn4MZJ2B5Cbv/sIHxnKdy9\nyL+PSRWgOg+DARuNOgN+awRe0Qev3yjPPxZz9pQquT01JfInV8jtElgIcvsKnvzoQDZAp5HF8beY\nX5nZUcQrfA3uzct8YC+yKG9GNkcVOGAvCSaQVPcpZEMUhfs6caK4pqWFXUg09x5k8XgxsKoDrl4W\nrdCcF3Mlt48fd4pYzScBkkeih/YhG/SbCM+wjXreSYTE+CTu6K9qA/5ck+jPchxLFrm9bp204UPn\n4Atr3QVLQTbV70WiRdUx0tsrxHCpezCQzc6HkJRTFQngFQPwkSZ/ZlkU7N8Pmzc72l8GQhBWU5rE\nLEVuqziMECv3IG38klnYfhJetl7qoezbV17fOY5sSO+keN2r1cjm+k0Ek71z6bOVktvgjh70ktsq\nTuMQ3Tv9/wagLQu3JiWyK0z+w4s84kD66Cg8XBe+eU0h4+KvKN/pehiJYr2/xOfWIWTE7xOtJq0V\nefpRwp0mFp6HEHYvprx17zgShXonfsLLkh25FckyKLUueftYFiE670KcCGq0noaQ5C83X1bdgVI4\nhaTp/zvhGwYvliJtfgul6yhfKnK7HBjMzbYpIA71T+OWtArDYsSZ+HaK2z4XLkBzi8hc/AluslhD\n1oVbEcfPDz3frUIyDd5LeGHhXkSm5wu4iREdceb9BTLvfgx/VHW6AG/TZYwHmQCWjNjXkXUtrG6I\nihZkvr2N8MzAoSHZrK1aJf31awiBUixrz0IGuHYCXlUtG9wtedFMtdBvHu9OgjN9QOpKPAv4xjSc\nChnAG5HxYUnHeTGXft3VJeuGN/MlCN7zTJrk9saNfnJkctKtPNnVJZH1al2lMEL77Nng9fs4sv58\nE38EaBCejvTnN+DUi/Dew6FDsGQdfDEpkdgDnkluI1LHYQz426yjHWxh+Sx8IA4vHYTpcZHWVNfR\nAlJH4H0zcMqj592ahQ8khYj8m374UbOfuGsHbhuDmydgY5vYejMz0u6bdogT9BPjsCtgUYwBWw3Y\nFzAZNSHSRM9RIn2D+lE/UqD48+OSSh+VytAM2KxJ1u+zgPoj8NxNYjf+HPivfDjxVw5WAFuG4MYG\nccj9vADZkIUqbcD1k/CKAqyqFSfSDwswE/D5OLJWv6EAr9bFDvW2z549cPXVYj8dOCDjIJGQDN5Y\nzJ3S7/1ud7fIeLS0iP3d2yv/D9oH2ZHLSCT/A1m4dxwONsJeo7gUZBhuRMbFpgPw/ICszW7gUzn4\ngg5jnueUQki5Z3bAE+vFCe/tt+sQ5+gtwCFz8lMzQj/QC59v9WdqPn0GPpiDl9SYRdeBvx6B++v9\ntuEq4O2T8NJe2L4Ghsbh62Pwn+0S7BhkSz5nFl7SDctXid0WFIDySmQ9XXdeyLylS6WY7NgYrFwp\nwRhfAT4/DV1lkAFVSIDA7yDZJjd4sjYKSLbKPebrEcKz58IQM2CTJtkL24HTPfBIe/h6phlwgwav\nysN152Bmudzb9wjezyQNeLkBrxyFLafFYQGwe4/MR2oRb+s1OA3dA7BkqdgOw10QG4frtwgZfLjI\n+rVH6TsnzsCpYRhLQNN6GI7Bvm4hvyfaxUGxn9IOsOV5ydbyZslZqEGe045T8I6VQrw/sB8ebYfv\np+HR6uAxl0Js5cWInRKkKLB0Bn5nCF7R72RLHzgATdvE2X87/rF09QR8rBqa9zh2pWY6e74ekMnT\njGStvvw0GENwbgd8Zgp+kQ6+7msQG+mNSJAMXCG3K8IVcvsKruCpid5e2byUKr4UhLmS2youFQFS\n7ncLSHT4l4D6ETGoNlagGeYlt0+fhk2bxDD8e2Rj9nZk8SpF2kW9hwcREmQfkh7+J8CFObSdl9wu\nB+WQ216om/JKyG0Lw8A/nBUvukXcZBBn2FsQGYdi5OClIrdVFCO3VZxACIZvI5G/L0LIl6o9cE2F\n95DPw+mYOMz+Hbe8zu8ghNjqwG9Gg4FERf4Z/uiYDUg00RuorJCJgRRA/CiyqbJgZTH8Oe6o2Eow\nixCdnx6G/CLZeN9C6WKTKor1sVGEqD+AEMwvJVr9xTDkEIL0cwiZ4UUNMjZuQSQsovIclwO5PZ84\nhkRV34k/OrIVWTPeRvkO/SkcjfQg+QYLcfP4f010veDj5ufvjvDZepxCxVF1LPMISfYNZA5SIxo1\nZD56K0JSlDJFCgVx8Hkz5vYiDui7cJw0GuLkeaH5uoHogQ57kGd4F8Wztiy04mjhe6XjfMe+SOS2\nl3RWye18XmyQsHX3zBk5j0punzwpchGV3MNhZA262/zdwmIDbtHEqRHkYB0dlbR+C4cOid2UTMr4\n+gyy1pTKaFkK/PEovHYMVpvk17hJbufzYleoQR57D8KvNsIH4+GyUCo2IOvGLcBQr/TR9nbn/8PD\nTnTrnj2Q2SFr5x15IX1Cj2tKp1n1h9R27+ry172y0NEBJ7JwYKspueL5f3Mers3DTUl4WlYco83K\n4PM+3+Fh2D8Gx5bDPTn4hQaDJRbfGJLNdQOw9BT8/iqJDFaPvfMInNkkztofUTwSNgg3IXbya/HX\nUClGbh88CBs2hOvolyK3QezXqSlTVi9ADseCauMPzcK3zsDZ1RLl/nABpkIMzW05+O0puPEcvGiT\n/9iHDrnlYPJ5OD8Fd9fApwzoikBjbUUCBH4Xx5ZSCUoLhw9Dy2r4bFrksiY81/wyZF4PyhjckYO3\njcFbG2FMcU4ahsznsZgEY/w78Pkc9EWIVIjz/9s78yA7r/LMP++9vdxWt1pSS+rW0urW2pKspbXL\n2hsCxuzJgBMcFk8Cw4R9ZgjDMhnwVIbCMJVKKJJJhYQAcTGsKQaYyYChiIwxGNvI8iLLq7wIW6ul\n1mpt3Wf+eO7hnPv1Xfvedi96flW3+vZ3v+98Z3nfs7znnPcAN1wA/iwTDgE+cYITFe3tucZtz317\ngRfX8GyFr+fZbQFwpf7rAax7CnjPgtBOl7PTuB902fi1F4B7pg91P9YB6vMqAJubaMxejlx37j7s\nJ0CD9beuAPcOo5O7+QLwlgHgpubgNj7HheWlwuP+c+fYdvREq57iZ48dK3z4a1J2Ll4EHnmE5wmk\n07n1N8B6+06wf3AHgHsLuHFKMhNcIPJ74M7aDFjeLS3U7717Gf/+fmBgBhf63HoZ2FvGmKvOcZX8\nB5uAWQ8D7TOA558PaYoXGD0H9su+iKE7LdacBW44B/ygCbgrzxbAJeD45h1g3X7yJOuJNWvY/u4/\nBfywHfjeDOBAAVdsbwb94PdG7bk3bs+axfw/eZJt0ay3jJxxe3wfKCmEmFDMKnf0m4fROrhgNEmB\nA/A3AHjqBH3N1ZKt4ErrkWB79hNT6UBiJGhsrMy1T319bqdruEwFcNMx4HNz2SE9j8JuVMY7C8FV\nOR9LXC+0OrEc0mmGewvob+474ErNfwMaP6vFQEPBa0Efwv8IGhA+AboIqWYRmYErs18GGsX+Duwo\nvge5Z05VQx1YT3Q9PTIG2lZwlWOtqAcN+28CB1Z/D+rFbNBg9wbUfofVRKQH3Bny5+Ak6N+BdcsH\nwZWXw83DJlDP3gFOTP6fPPfcmH1vibmuISwGDc8fAeuIfJMb7eAg6j0ovBK8EGkEfftCNvyfgKtR\n34bK3MUlD13z9IIroz4L7vg4CxrVCh0aXIo1YDl+DszrL4O7JmJXfS1gffdWcIA91gZ4xSaM0+ni\nv8+dO7SP5w3bw2E5uKL6k6BP0xeWcRXvK634BH5rkQZ5Mrgz6L1gWf0Fhm6F7wDbjHcDOPVicGPR\n0hKM2en00N2LdQBuGqS+fRHAf3f5zxfZBE5YvRGhTcpn+IndNgBcYf4XAD6d5iTlFxF2KqVAufog\ngGXngJkFdlYWMmwDPMj23F5Oqn4YnHT75lFgXgvwsknArOyuhTRQekYpS+cVGpP/4CLw3CHg4hK2\nET8BjVMp0A3PsheAV0wC+pqi8yn685/hs2EZ3Sz9HjgJ9osBGrr/b5pn3+SjFzRo3zAALBhmR2DZ\nsuG7lIxpaqps5+tkAFvOhomcyyn2Qe4EF6A8dwHYMQD8cTMwJTur1l6gv7s8cfBqOg3MbmH78O+v\nAH9zCPhmV/6dexsc8F+M7Xo5O7wAluV/BXDd08A/dQJ/3xDcOOXbsfQ6cOfQ8n7g8qU859taKINu\ncBfRmx8HnlwCfLmertOSZ+80DQB/kmZbNC8xUxlPxOXDwDHWVgCfz7pT/Cq4I2oL2Pd5BdgX3Huq\ncFsdTyjETM2Gsegg0Dud8vtrAHYIuLYZWNIKPHssOzlZQmYWg+3xu88BD/UDe7qB7wwAP08VXvm/\nHOwz/yEAO8o6bloBY+5wFrR5Chm289GY2P2SnEyaBsrJ67L///JB4MXVNHT/DJz8uZAV0O4rwPUv\nAm+bzPJKqu/kybnvqasD5mR95X1oAHjFI3Tj801wwi+5On4x6B7q1cc4+Te7Cb+dOXk+Okwhbhfn\ngn2aj4Ku5/7BAZeyv+9t4SfJDlAvXodc3Zs2jZNonnWzgK7jwJ9PBu5pAv7yDHBbS3CHcgFcAPQK\nAOsS7Xkmw4NSBweZD5kRdl8w1vo+QggxLNraam/cFeOLWkxwFBu8Fnqn7yDV4v314AGqVyPVrhz3\nNIIGnrfWJrgc2sDVxF9Aea5HKqUX9KsvAotBI6EYPq2gv+0P1DjcRaDx5wegEeMAWH99BjTIVsN6\n0L3Kj0Ej9x7QAPmfwd0sw/BeNoQGcJXfa2oQVj7qwDNKakUjwqTP8+BK7mfAieLhTvhUY1iYMmX4\nzzc1lW+gLjSJEMdjOBiAJRe4UrRWTAGN5h8AXcd9AZSzDzvg/Rbc+8XbzsvNw8ZsuDcNAp88Bnx1\nFldobj0NfLqVE7nJbkglRtMMaIz6QwT//72IDvQd7uxMgh4AN54GOppoYC3XoF2IFIIrhf+Y+O2J\nk8Csxlx3ZOWMFdIAdqRp/PkcgMcHge8NAj+qY9ldB2DtI8CblkUPVIjvM1Zj2Dbj6s1aUA8a9zeA\nu2HODdAY1QjgbHYVaiFdLNb/rQNw/WlObNwBuhj9f4PAthQne15pw3fFtaEb2Jymge6TYJ3os6MB\nwGteAD49PayormQRTUMKeM0gJzueAycVv5Jd3f4eADseBnaV60OvCE3gZPCNw3i2nLrDwFXxKwAc\neBGYnm0ozCobt6TTwDzjRO07LwL3Pw/sW8wJsZ+CE8RvASeJ1yCU6fN1tZm8qRU+zUtKrB7paOGi\nmZdn/3/qEPDkJGDuNGDaC8DgADBncrEQyPLl+fVmPmiI/ii4g+jbYN3yWtDlWArAyXrAVWit7QR3\n7H0UwH+7CNzamHt+RwpcZf1hFN8V6t2EeeP8ggX83gdg9WW6GflOA10L7QXr8jcXCMtPHs2dy90S\nI4mM20KICUGljXQxCs2ET1TMhufOQ4haUqqjOZYYCcO2GHmSq8tEbXg9OCDrByeAaskrwdVAB8FD\nQtVUkTngarpqqaa/M1yjMsB+RzWG9ZhqVnFXQzGDzTRw58KnQGNCKtE/zWTouqEcvFHBMzkFfGYK\ndyqdAtDWVPs2aRkK+5ofDqtqYATMR1NTWBGZj7q6oWOD7kq2Z2RZkgL+NEUjqmfvhYK3D6GSFaaV\n0N5e2f2V9PfjBQctlR5WEpFKceGIgS72dgLY88Dw3dDFeB1cAPp7/lMAfwvu8no3gCMHgWsKuE1K\npwu7ggG4K9MbJeeC7rL+LDJS7i1Tf5uaqjPuJsurmnqzoSHEJd+OmJjOxPaGlpZcOWgboAuvd5V4\nZzH9LEUmM9TdaLluI1tbh+rH4sXl2wqS54fN6wC6jPNYF6aUP6lUanIW4Er3T+a5nm8irlxZ6jLg\nS40M9xbQ/eF14EKEcppMH+98u+r9zoT3ZT97wAPSk2uU6uqGui2rVbtfCPURhRAiQTUVbzUd+JGu\n8AvR3Fy7VbPi6iWVGlurM0RtmQiG4eS21EqYJD8oRUmh9oZtjyFaPSpEDanGaNfTU9pQUmigXcnE\nQLLuMQvuJzLAuJhtTfYNWlrKN7L6FYSe5ubQX06ni/df588vO4ojStIg19SU6y+3GEmXL5MmhfSX\nej5pWMtkwrkoqVSuL/aRIp0emoZy3Y80NQ3VlSlTCvc1e8HddZ4LiWdbW+kewX8vtluzHKNkOVTq\nLiZJUv4rmZBMjkljg3Wp9M0oslOjqan0+Tq1IJ+bpnInipLGaaC6sW5cX1XjWsOsvHMqCrGy1Mnp\nCbrBCZ+RZF32k6S+fmh5VaML5SDjthBC1JBqjHvVrqCqxnDjSaW0ivtqo7u7+OqVcmlp0STJWGfx\n4uE/W4v6ZTxTC9/6QoixRTV14kQ466VSQ0nM0qXDf7aSM3aS/ZP6+tr0WYCRW1FeikryLum/uZQ/\n55h85evlNpUqbsAcScrdbZEvnypZCZx8Ty37MfmMp+VSiXG0mnpmpBacVLvjt9IdBxOJVKr81ef5\nmAjtzkgiE4YQQkwAKunsFqOxsbrBnhgdqhno1bKzr07X2KaaVYpCCCEmFtUYqEZ6Bd5LQTXGv2oM\nVC8VtVp9nI9qXMlV49JorFDpGT0xy2rp82ccUo2rEiGKIeO2EEKIq5pp08a/UfZq8xMvhBBCCDFa\njJQf7fGCduoJIcYaMm4LIYSoGdVsca3Gf1tX1/BXqFTjO62WJA9vqYTxbpwXQgghhBBCCCGGg4zb\nQgghakY1W1wnTx7+s2Nli2M1/gtHy/ehEEIIIYQQQggxXhlBT0wvEcM5BrlaJ2HJI3/zMXVq+B47\nuSz3BIWxij+euRSFlhGWc4JAbOGaaHueyrH8xfJZ6XLM2Hnu1biUc6LJy0tJnHfl6vl4J9aXGjie\nNqtu9fW4ZazMLIwWV4u+iNozkg5RhRBCCCGEuEoYk71qM7vezB4xs8fM7KNFb06eolVXB0yaFAc2\n9JnNm4deyzc4Xb586LWNG8szUMYG7fi0ivj0iThuyVMp8hnppk0L34st8St1Osa11+a/Hhvkgdx8\n9GzZUp4hI/YvEOdtIQdl8SRFXKZxHsVWozj9lR5XnEynJ196k/T1lfeO2bPzv7cc4358T3xaRaGJ\nkfhEjzi/Cg2aqzk9pRq/EddcU95AvtAR6oVObonlIp4Yib/HfifiPKp0AqCQ8bwcw2i+eqcSCslt\nTEdH+B6Xc6FT5Lq7w/dY7wrVcdUYYgqdUunLqRYTE4UmLguFHddlcf7G91c6GVqpTGUyw5+IKteo\nnE/nm5py67xCJ+MUO4HQy0mhCcly2srhpn3x4qD7w5EdX896ma40Hk1NQTamT89/TzGjty+TefPC\ntUr846xYwb91dZWfzBPXpbXwyVNKDvPVG1728vWzKqW5ubKTnVavHv67yuknVMJo+EQqZ4FGIeI2\nphZU0x+ptO9XC9auHf6ztXbQW2tZLIdqJjKXLq1dPKqlkoVR1WxHi6nGZ1s129ryMRqTatUsIKjm\n5L5aU02dNVy0cEcIIcpizBm3zSwF4K8BvArACgA3mlnhM2Xnzg0Gms5OGt82beKnuxvYuTPcG3cs\nfOPkOy3XXsvB4tq1fKa3N9eA6t9RVxeMQOk0B8QzZ/LeBQt4XybDeO3axft8g97YmDuA3rgxfJ8/\nP8Rl3brwW3Nz6MD29ob7406i72xnMvy+bRs7/dOn56bZdw7iwVSc/p4epn3HDuZlHD8/GDQLA3b/\nd906DtBbW3lfVxff7dPqDb0rVuQO9r2xq7WVafQd/zlzGIcksXHMD+yB3IHahg00InrDgy/D2Ni8\nZk347suyo4Ppv/ZahtHWxnR5YiOgx+fjjh0coHd3A+vXs+xjg7M3XqxaFcoyNph1dTF+/pnp00NZ\nxx26OA2+PDIZDja8AaqnJ8hDLGtx3m3dGr57g54ZsH0782bhQr43NqZ7GY4NMT4NO3fys3Ej0z9z\nJnZfuBDu85My7e3hGf/e5cuDXKxcOfS9vmw7OkI4ZsE4s3Ah4+bjNXNmyLu4I93TE757I/PUqUyz\n7+Rv2cLjq6dO5bP5OtOxTvjyX7CAstLby3pnyhSWtccbvuIyb2sLebdlC2V+1SrGf/v2cJ/XiQ0b\nwuAmloO1axmW1/N584IcxQasuNx8XixdyjB9/BYtCoameOAT66yXnaSBo6+P4a1Zw3yL89vLYTwJ\n4cuyr48ys24dn29vZ34kietxb9DwcgcwjKlTmU++bo/T7w2ZPT1B53fu5HevG11dIR/jAbwv5zlz\nQvrXrw/1VksL06NKMc8AABj4SURBVLF4MevxadPyTz76ibF580KYfX2h7t2yhXkUl79PSzyp5tPf\n18fynzWLf7u7Q5sD5A6GfX5v2wYA2H3pUtCDDRsoA6tXh7ojru98XFeuDLK3axflrL2d8u9lp6Eh\nd3LOP9vWFsoqLt+NG0NYfX3Mh7id8+n3cW1rC7rT2ck87+1lHbJmTf7Jx+ZmymB9fWg3+vrCCZzb\ntjEPdu0K8h+3r36yzevVihV8vrmZ6V6/Psg+kKv7vl6eMyfIlI9jayvfvWgRw965k3rg89jrYCYT\nyiOuu2fOZFiLFvGza1fu7/F9Pk0+73x++zzZsCG3zY3128uR7yv09fGTTvOZNWtYBsuWMc7xyaa+\nbvHvBZjmWbP4Wb2a7/J16LZtod729e/UqSEcH0ezEA/fV1m3Lrfe8fjwNmwI19ra+OzChXx2587Q\npuabzInrkk2bQjuwcSNlZs0apmfmzFwDptc73+5v3Uo9ymQoNz09zK+2NuZBZOjf/cQT/BLrsdfP\n2bMZj+3b+Y6+PqbB17X+fS0tueXmWbgwtCl9fcz3QpPHybxbvjxXT+bMCTqSz9gctx99feHejRsZ\nPy/bXn48vvy93jQ2sqy9jE2fHurZ7dtzy94bgpqbQ/nH/eTt25mGdJp6U2qyI65Xp0wJ6Vy5kvnv\n25R8E7jJfmusg3Fc16/PfS5pfLzmmiCbPi2+HxX3VeMwY3w+z5/PvPZt3YYN+fu3QOizxPXK2rVB\nVrZsyV10k4/42dmzQ/qT74xlLB87dwZZSuZz3N8G8htuvTz5NsDXKcUMzr5cffjTplFu58/H7r17\nKQdx3ZaPWO9mzMjtb8UsyzPMjfNo/fqgy0ldTaa/WDhedr1elbOb2Ovs4sV83tfx5UzWeNlMp5kG\nr7PlPBuP1datC/VbUnZKTXLFspXsu+ZbiJTEy52fRPa6Ws5Ej8/fefMoa4V0LR9xPyQeeyTJ12bF\n479svw/AUNkptJjIk2/y1bdJ5UyuxQsA2tpyx+6lSI51KqG5mToK5NZByQmWUnJYTD7KneiLy6LQ\n4sJStLdXNpkZT8jlG1OVS9yuJuVsOBNO1UxSDXcyu1jfphTF+galJuuKycdwJkzjsdFLxXC8Y3iK\nLcir1YRxoeBHNPThsQnA4865ZwDAzL4B4I0AHin4hK9oFy7M7dzHjXZy1eS2bbx3cBC4445gpPbE\nHbZly9ggP/MMhXn2bODKFSrMgQOs/Ds7+TuQW3ktXszG49IlNozpdBgEAWw0t25leP5dXvnXr2fa\njh8HHnqI1xYs4Hvr6tg5nzYtNLzz57NTm0oNXZnT1wecOgUcOsT/161jnFIp4IEHhjZgcedxxozc\nCmnFCuabGXDPPUzTokXAU08xf3wFtH078OijTO8zzzAtmQzvnTMHOHqU8Vy4EDh2LDevAXYC5swB\nHnkEOH+e15YvZyXvyznuOPb0hMrXl/f588y/pUtZbv65WbP4nuPHgcOH+d64AYortOnTmefPP8//\nt2wBzpxhxXXnnUx/PmOAGcujqws4eJB53dnJd9fVUXamT2c4jz3G+3fuDB3zmTN578GDLPeGBnZw\nMhng8mV2vK+9Fjh7NuSN72ivX08deOYZ4Nln+d6ODsY1lWIce3sZj927w0Bv6lR+uroA51imK1cy\n3AMHgmzU1VFmn3gixNcPqFaswO5vfxt9fX2Uh5UrqWMAO5kXLrDDs28f05hKAadPMx98R3jbNmDv\nXpb/s89StpuaaNxIp3n/1Kl8/uBBPrNqVRg4zJvHZ48fZ35549rgYKhU16wJctzby7h6o4tzwMmT\n1I0tW/ibL//WVubBgQNMi++sJgfRAOO6ciXwi1/w/40b+UxjI3DiBNPe2Bg6ED7+06YB585xEHH7\n7ZTrlpYwIJoxg/nl05JKcQBuxvteeCHkXU8Py37qVN6XTgMvvkh9OHGC//s0Tp7MvPL1wqlT1KXB\nQeZlXR3f6+Xl9OkwAPGdwHXrmEZ/benSUH8tXMh4HD7MsD2+fokNY5kM5Xv3bsanry+Ul08zwHS2\ntIR4bN5MfVq4EHjuOdZDdXXMs6Ym1h+pFD/19UyHr0eamzlQnD2bcWxpYfmdP0+da2hg/kyezHrw\nqafCoDQ23h87xvxatIjX9+8P5dbdzd/iNsbnXVzvtrRQX+6+m/9v3cq49vfn1sf+/b4jtWED379g\nAetnPygaGODzmzdj9y23oO+661jm6XQwyLa2Ml0zZ7IMm5sZt9mz+X3GDMqTGfPSLAz4OjpCPdjc\nzLqns5PxOn2a+b10adCTzs5QZ/jOZ7wCedYspu3AAb5r9WqGtW9frkEhaUDMZJj2jg7gySdzjQjO\n5bbzbW1Mv283Zs4Mk1tXrjDs5mbWJ5MmMU7e4N7cTPnx9f3UqUEHe3upv94Ank4zXgMDvHfVKuqg\nN5749Dc0UD7q6/l99mzqSVsb497QwPDjvk08+GpoYB3S2cm4HznCfBgcZP5nMnyvJx70AsFA6vtD\nTU2hjJyj7Hn8dd+BnTWL7zJjXr3wAuVl/nz+H692mz6d/+czdK1Zwzg6R3kcGGC8jx4NYcRlGuuM\nH6hPmsR4nT7N8unvZxlPm5ZrLI77SW1trH/85MypUywHL+eLF1PffdozGf5N5gNA3Xn2WdYl588z\n3g0NLIMZM0LYXt7j/kZbG3DiBHb/8Ifoe/vbwyKKwUE+441xyUUPqRTriAsXKLtTprDOGhxk3nm8\ncaGlJRhp6usZVz95ZMZ4X7rEdGYylBWf/o4OlmFdXZA/Xw91dfG5AwdYV6RSrI98vi9bFuoWb3Ty\n5TBrFuO7bx/ze2CAz9fV8ZNKUSaam0Od5XVuzhzG6/x5PtfSwnh4eff1Y08Pw+rooIybMc07dvDe\nxkb+fvky823SJN7j2y4f37lzWTa+v3rlCp/r7OT3J59knqZSfN631atXA3v28PnkoLuvj3J/992s\nt8+eZV61tfH5mTMpV35CFWA++fbRObZ5U6dSbk+f5j2ZDNPl6zYvO5s2hQna+fOZb1eu8J6LF1lu\ndXWU3cbG0LdZsoT519gY5M+n3zl+nniCet3QQNk5fDik/+RJxiVpWPVpuPdeymYqxfR0dPD7li3M\n187O0N769/q8/PWvQ5z6+4P8DgwwTc3NIe9WrGD59fUxnAsXQj158iTloaUl6Pg111BXmpux++BB\n9HV1URYGBhhvn/7BQY594oU9vp1atYp91s5OtivOMV6ZTKir9+yhnE6ZEtoOgPm4fz/LfMECytXp\n0wx7zpwwnt21i2nxfQXfdp0/T73t6uK9Tz7JMuruZnpPn+Z9DQ3U38mTQ9o7OxmO73edORPq4ZMn\nea+X9ZMnKTuNjXzet+UA669HHw2GW9+38n2as2c55vN91t7eoMM7d7JvN29eaGuOHAn6ePo0cP/9\nzLv2do53p09nvLdsYbw6OpjeS5cY1tKlzMsTJ/jeXbtYnn7s2tBA2WhtZVnExj6vDwcOUDd93fHQ\nQ6Ht8uPMuO9+6lTorz73HO+dPJly8JvfMG+9rSDuY/T1AffdRx26cIF5de4c66YLF1g/HznC8Nau\nZVj19fxs3co0dXWxbmhoAB5+mOUyf354dtky/nbsWFiscOkSw0yl+B4z4K67WH8MDlImr1yhfpw9\nG8rg8mXqz+TJQ3exeR28/37qRzrNd+7fH/rl3d25k5i7dnEstWkT4/DYY9SROXMo648+ynem08DG\njdj9zW+i74YbQh30q1+xXjl0iPcdPsx3nzvHOsf3sVau5D11dZSN2bNDP+T48VBOa9eyrPbvpxz3\n9FBWDx5kPvt61vdLfN0Yp/8XvwgLjR56iDLc3s44LV7Mv52d4bnbb2dZ1tfz2dWrQ3/7mWfYVpw8\nybHT0aOsGxob2b7deSfHZ+fO8d7WVtblx4+HMQrAe/r7Wd5z5jAdfizpx/xHj4b7/CQCwPuffz7Y\nPpxjGV68yI8Zx5Z+PHfnnUzLokVs9wHqd39/6FPW1YU+7IEDYbL59tuZLjPgZz/jtXSaZbVhQ+gH\nAZTxO++kjDY3c2zkbWhPP804ZzKU782bqQu+j9rTQ1nbtIn5dvw4w1izJsisp6uLv58/z/Lr72d8\nBgaoR+fOUUe2bx+ad562NsajvZ3h+fby4sWgB2YcG3u715494dkTJyhTU6YwTX7cfMcdYQHaz35G\nOTpzBnjwwSCnhw+z73H4MNPY3s7yefxxyopv2+fPp148+WRu/7K7m/Vbfz/j8OSTTPc111C+X3yR\neREv4hoBzHmFGSOY2ZsAvMo59+7s/28DsMk598HoHjck3g8+mLtiKubuuymUhfAdjXzbkr0REWAB\nJSvo8+dDZ+jIEQpRvpm+Q4dChzPGd8x8YxobdzxnzlBZis2we8HLx/33F5/xOXy48Oztnj1DV4XE\n+MFAqS1u/f1UtGQeO8dr584xnfnicewYy6DYLNCzz+bfanvpEss3uUrCv/fyZSpiITcvDz1UfGWH\nNxjlw78DCBVzIfr7KVv5ZmaPH2faC810DQww7/Llz5kzzINC2+YBlmE+1wvOsSEp5IrE62Aevbn5\n5ptx89veVtgVRSUUy2MgDODzzaKeOcPrSb31epf8ngz37NmhK5MuX+b9x45RpgvN3j7/fOXuAoaD\nj0+++qtU3gG5chozOMjfhrsF8+LFwnlz4QLDTbqVKhWnSikVTrHfq41DMZ1/8EF2Sorlzwi6K7j5\n5ptx8803j1j4JenvL8/NTj7OnmXejPDM/5hl3z72E67GMxWA6mRnHDEiOnrq1NXtH79QWy9EKRL9\ngVFvQwtRaqwx0alV33G8Uk3516p+rKYMiowrywrbG/LMSutocnx05UqYTC5FoXFzOcRllEyPn9At\nFIc4zsn4+4k1s/wyEN9/5Qo/8Tjj0iVeK2eF+KFDubYuv2irnLw7diwsMknKa7Fxo3+Pj3OyDK5c\nCe/PN249ezYsZDlzJkzaepm7dImffC6g4nK6eDEsIPO/eaNtKfdRztHW5yf7fHk7x7+R/A5hcJD5\n7idy4/T4OGQyhcftJ08Ge8axY7QL+XcODoYJ4Hyr6+N3nTgRJi38hM25c/yU2gly5QrHML29MDM4\n50aksp44vbxChm2guGEbyN2ynyQ2vuTznxlXAsV8ERba2uIbknQ6v2EboLKUUphi29JKbWUoJozF\nDNtA+ZV7ocGoV+B4BVSScravFfIh2dCQ3zjr31tfX9x/eSkfecUMf+X4v/YUG6wXi5+PQ6Hny/HT\nV6gMzYr72C7ViNXCsA2UNq4WMpAChdMfd+AKdebq6/NvufXvK+U//aUwbAPF01+OYbpQOVY7QCrW\nQSlluK3V4KRUOMV+rzYOxfKvWHsFjI4f3peSaoyTxXyAXw1Usq13InIVGLZHjKvZsA3IsC2Gz3gx\nmF7Nhm1g/JTTSFFN+deqfqymDKrpswOVpT85Pqok/cM1bAO5cUymp9SYLf49ea9Z8efj3/xuqJiG\nhvIPR0/atCoZs8Q2nWR5lXL1Eb8nWQalyi8eO8S2AV8GjY2F3x+XU2Nj7vje71YsB7NcF1S+TJI7\nkvORSuXu9orTU04cYntGsgz8DrlC6Y/fFe+c9Yb4cuyUAN/xErhXGYsrt68FcLNz7vrs/x8D4Jxz\nn43uGVuRFkIIIYQQQgghhBBCCJGXkVq5PRaN22kAjwL4HQCHANwN4Ebn3P6iDwohhBBCCCGEEEII\nIYS4ahhze/SccwNm9n4AtwFIAfiSDNtCCCGEEEIIIYQQQgghYsbcym0hhBBCCCGEEEIIIYQQohTj\n7uQJM7vezB4xs8fM7KOjHR8hJipm9iUzO2JmD0TXppnZbWb2qJn9yMymRL993MweN7P9ZnZddH2d\nmT2Q1dm/iq43mNk3ss/80swKnAoqhEhiZp1m9lMz22dmD5rZB7PXpaNCjAHMrNHMfmVm92V19FPZ\n69JRIcYIZpYysz1m9v3s/9JPIcYIZva0md2fbUfvzl6TjgoxBjCzKWb27ay+7TOzzaOtn+PKuG1m\nKQB/DeBVAFYAuNHMlhV/SggxTL4M6lrMxwD8xDm3FMBPAXwcAMzsGgC/D2A5gFcD+J9mvz1e+G8B\nvNM51wOgx8x8mO8EcMI5twTAXwH43EgmRogJxhUA/8k5twLAFgDvy7aH0lEhxgDOuYsAXuacWwtg\nDYBXm9kmSEeFGEt8CMDD0f/STyHGDoMA+pxza51zm7LXpKNCjA0+D+BfnHPLAfQCeASjrJ/jyrgN\nYBOAx51zzzjnLgP4BoA3jnKchJiQOOd+DuBk4vIbAXw1+/2rAH43+/0NAL7hnLvinHsawOMANpnZ\nLACTnXP3ZO/7p+iZOKzvgIfICiHKwDl32Dm3N/v9LID9ADohHRVizOCcO5/92giec+MgHRViTGBm\nnQBeA+AfosvSTyHGDoah9irpqBCjjJm1AtjhnPsyAGT17hRGWT/Hm3F7LoCD0f+/yV4TQrw0tDvn\njgA0rgFoz15P6uZz2WtzQT31xDr722eccwMA+s2sbeSiLsTExMzmgytD7wLQIR0VYmyQdXlwH4DD\nAH6c7bxLR4UYG/wlgI+Ak04e6acQYwcH4Mdmdo+ZvSt7TToqxOizAMBxM/ty1rXXF81sEkZZP8eb\ncVsIMbao5Ym0VvoWIUSMmbWAs9kfyq7gTuqkdFSIUcI5N5h1S9IJrlBZAemoEKOOmb0WwJHsDqhi\neiP9FGL02OacWwfusHifme2A2lAhxgJ1ANYB+Jusjp4DXZKMqn6ON+P2cwBiR+Kd2WtCiJeGI2bW\nAQDZbSRHs9efAzAvus/rZqHrOc+YWRpAq3PuxMhFXYiJhZnVgYbtW51z38telo4KMcZwzp0GsBvA\n9ZCOCjEW2AbgDWZ2AMDXAbzczG4FcFj6KcTYwDl3KPv3GID/DbqoVRsqxOjzGwAHnXP3Zv//Z9DY\nPar6Od6M2/cAWGxm3WbWAOAtAL4/ynESYiJjyJ0l+z6Af5v9fhOA70XX35I91XYBgMUA7s5uRzll\nZpuyhwa8I/HMTdnvN4CHDgghyucfATzsnPt8dE06KsQYwMxm+FPizawJwCtB3/jSUSFGGefcJ5xz\nXc65heB48qfOubcD+AGkn0KMOmY2Kbs7EWbWDOA6AA9CbagQo07W9chBM+vJXvodAPswyvpZV1Wq\nXmKccwNm9n4At4GG+S855/aPcrSEmJCY2f8C0Adgupk9C+BTAG4B8G0z+2MAz4Cn3sI597CZfQs8\ncf4ygPc65/w2lPcB+AqADHii7g+z178E4FYzexzAC+DgQghRBma2DcBbATyY9enrAHwCwGcBfEs6\nKsSoMxvAV80sBfZZv+mc+xczuwvSUSHGKrdA+inEWKADwHfNzIE2q685524zs3shHRViLPBBAF8z\ns3oABwD8EYA0RlE/LYQphBBCCCGEEEIIIYQQQowPxptbEiGEEEIIIYQQQgghhBBCxm0hhBBCCCGE\nEEIIIYQQ4w8Zt4UQQgghhBBCCCGEEEKMO2TcFkIIIYQQQgghhBBCCDHukHFbCCGEEEIIIYQQQggh\nxLhDxm0hhBBCCCGEEEIIIYQQ4w4Zt4UQQgghhBgmZvbz7N9uM7uxxmF/PN+7hBBCCCGEEMScc6Md\nByGEEEIIIcY1ZtYH4MPOuddX8EzaOTdQ5PczzrnJtYifEEIIIYQQExGt3BZCCCGEEGKYmNmZ7NfP\nANhuZnvM7ENmljKzz5nZr8xsr5n9u+z9u8zsZ2b2PQD7ste+a2b3mNmDZvau7LXPAGjKhndr4l0w\ns/+Rvf9+M/v9KOx/NbNvm9l+/5wQQgghhBATlbrRjoAQQgghhBDjGL8N8mPgyu03AEDWmN3vnNts\nZg0A7jSz27L3rgWwwjn3bPb/P3LO9ZtZBsA9ZvbPzrmPm9n7nHPrku8yszcBWO2cW2Vm7dlnbs/e\nswbANQAOZ9+51Tn3ixFKuxBCCCGEEKOKVm4LIYQQQghRe64D8A4zuw/ArwC0AViS/e3uyLANAP/B\nzPYCuAtAZ3RfIbYB+DoAOOeOAtgNYGMU9iFH34N7AcyvPilCCCGEEEKMTbRyWwghhBBCiNpjAD7g\nnPtxzkWzXQDOJf5/OYDNzrmLZvavADJRGOW+y3Mx+j4A9feFEEIIIcQERiu3hRBCCCGEGD7esHwG\nQHz4448AvNfM6gDAzJaY2aQ8z08BcDJr2F4G4Nrot0v++cS77gDwB1m/3jMB7ABwdw3SIoQQQggh\nxLhCKzmEEEIIIYQYPt7n9gMABrNuSL7inPu8mc0HsMfMDMBRAL+b5/kfAvgTM9sH4FEAv4x++yKA\nB8zs1865t/t3Oee+a2bXArgfwCCAjzjnjprZ8gJxE0IIIYQQYkJidMcnhBBCCCGEEEIIIYQQQowf\n5JZECCGEEEIIIYQQQgghxLhDxm0hhBBCCCGEEEIIIYQQ4w4Zt4UQQgghhBBCCCGEEEKMO2TcFkII\nIYQQQgghhBBCCDHukHFbCCGEEEIIIYQQQgghxLhDxm0hhBBCCCGEEEIIIYQQ4w4Zt4UQQgghhBBC\nCCGEEEKMO2TcFkIIIYQQQgghhBBCCDHu+P92scUte8AlJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f442ffe1e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "#niter = 30000\n",
    "test_interval = 500\n",
    "n_mc = len(CV_perf_MC.keys())\n",
    "\n",
    "for m, mc in enumerate(CV_perf_MC.keys()): \n",
    "    CV_perf_hype = CV_perf_MC[mc]\n",
    "    \n",
    "    for hype in CV_perf_hype.keys(): \n",
    "        CV_perf = CV_perf_hype[hype]\n",
    "        n_CV_configs = len(CV_perf)\n",
    "        pid = m*n_CV_configs + 1\n",
    "        \n",
    "        for f, fid in enumerate(fid_list):  \n",
    "            train_loss_list = CV_perf[fid]['train_loss']\n",
    "            test_loss_list = CV_perf[fid]['test_loss']\n",
    "            \n",
    "            for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "                #plt.figure()\n",
    "                ax1 = plt.subplot(n_mc,n_CV_configs,pid)            \n",
    "                ax1.plot(arange(niter), train_loss, label='train',linewidth='.5',alpha=0.25)\n",
    "                ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test_{}'.format(hype), linewidth='3')                            \n",
    "                ax1.set_xlabel('iteration')\n",
    "                ax1.set_ylabel('loss')\n",
    "                ax1.set_title('fid: {}, hyp: {}, Test loss: {:.2f}'.format(fid, hype, test_loss[-1]))\n",
    "                ax1.legend(loc=1)\n",
    "                ax1.set_ylim(0,100)\n",
    "            pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp11'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'R_HC'\n",
    "batch_size = 256\n",
    "encoding_layer = 'code'\n",
    "weight_layers = 'code'\n",
    "multi_task = False\n",
    "\n",
    "if modality in ['L_HC', 'R_HC']:\n",
    "    snap_iter = 10000\n",
    "    \n",
    "    if modality == 'R_HC':\n",
    "        output_node_size = 16471\n",
    "    else: \n",
    "        output_node_size = 16086\n",
    "        \n",
    "    hype_configs = {\n",
    "                   'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':8000,'out':output_node_size},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "\n",
    "elif modality in ['CT']:  \n",
    "    snap_iter = 100000\n",
    "    \n",
    "    hype_configs = {\n",
    "                    'hyp1':{'node_sizes':{'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-4, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "elif modality in ['HC_CT']:  \n",
    "    snap_iter = 20000\n",
    "    hype_configs = {\n",
    "                 'hyp1':{'node_sizes':{'En1':10000,'En2':2000,'code':500,'out_HC':16086,'out_CT':686},\n",
    "                  'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},\n",
    "                  'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}}\n",
    "    }\n",
    "    \n",
    "hype = 'hyp1'\n",
    "pretrain_preproc = 'ae_preproc_sparse_HC_10k_CT_100k_{}'.format(hype)\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "\n",
    "# Save either L or R HC encodings. Turn on CT + CS save only for one of these. \n",
    "# (Also decide if you want to copy CT raw scores or encodings)\n",
    "save_encodings = True\n",
    "save_scores = False\n",
    "CT_encodings = True\n",
    "\n",
    "for cohort in ['inner_train', 'inner_test','outer_test']:\n",
    "    data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "    test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)       \n",
    "    input_nodes = ['X_{}'.format(modality)]\n",
    "    #input_nodes = ['X_L_HC','X_CT']\n",
    "    net_file = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "    with open(net_file, 'w') as f:\n",
    "        f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "\n",
    "    test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "    test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "    with open(test_filename_txt, 'w') as f:\n",
    "        f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "    sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "    if modality == 'CT':\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "    else:\n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)  \n",
    "        \n",
    "    print 'Check Sparsity for model: {}'.format(model_file)\n",
    "    \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    \n",
    "    encodings_hdf_file = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,pretrain_preproc)\n",
    "    \n",
    "    if save_encodings:    \n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        input_data.create_dataset(input_nodes[0],data=encodings)           \n",
    "        input_data.close()\n",
    "        \n",
    "    if save_scores:\n",
    "        input_data = h5py.File(encodings_hdf_file, 'a')\n",
    "        if not CT_encodings: #copy raw scores for CT instead of encodings\n",
    "            CT_data = load_data(data_path, 'X_CT','no_preproc')                    \n",
    "            input_data.create_dataset('X_CT', data=CT_data)    \n",
    "            \n",
    "        adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "        mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "        input_data.create_dataset('y', data=adas_scores)           \n",
    "        input_data.create_dataset('y3', data=mmse_scores)    \n",
    "        input_data.create_dataset('dx_cat3', data=dx_labels)\n",
    "        input_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting TSNE embeddings \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "exp_name = 'Exp11'\n",
    "#preproc = 'no_preproc'\n",
    "#preproc = 'ae_preproc_sparse_HC_10k_CT_100k_hyp1'\n",
    "Clinical_Scale = 'ADAS13_DX' \n",
    "batch_size = 256\n",
    "snap_interval = 2000\n",
    "snap_start = 2000\n",
    "encoding_layer = 'ff3'\n",
    "weight_layers = 'ff3'\n",
    "cohort = 'outer_test'\n",
    "multi_task = False\n",
    "\n",
    "modality = 'HC_CT'\n",
    "snap_end = 21000\n",
    "\n",
    "\n",
    "hype_configs = {\n",
    "            'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':25,'HC_CT_ff':25,'COMB_ff':25,'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':2,'CT':2,'HC_CT':2},'tr':{'ADAS':1,'MMSE':2,'DX':1},\n",
    "                      'solver_conf':{'base_lr':1e-6, 'wt_decay':1e-2}},\n",
    "}\n",
    "\n",
    "hype = 'hyp1'\n",
    "fid = 1\n",
    "node_sizes = hype_configs[hype]['node_sizes']\n",
    "dr = hype_configs[hype]['dr']\n",
    "lr = hype_configs[hype]['lr']\n",
    "tr = hype_configs[hype]['tr']\n",
    "\n",
    "data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "test_net_path = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)       \n",
    "#input_node = 'X_{}'.format(modality)\n",
    "\n",
    "net_file = baseline_dir + 'API/data/fold{}/ADNI_ff_test.prototxt'.format(fid)\n",
    "with open(net_file, 'w') as f:\n",
    "    #f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "    f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "    input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "\n",
    "test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "with open(test_filename_txt, 'w') as f:\n",
    "    f.write(test_filename_hdf + '\\n') \n",
    "\n",
    "sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "\n",
    "n_snaps = len(np.arange(snap_start,snap_end,snap_interval))\n",
    "tsne_encodings = {}\n",
    "net_weights = {}\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    print sn, snap_iter\n",
    "    model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "    encodings = np.squeeze(results['X_out'])      \n",
    "    net_weights[snap_iter] = results['wt_dict']\n",
    "    print encodings.shape\n",
    "\n",
    "    adas_scores = load_data(data_path, 'y','no_preproc')\n",
    "    mmse_scores = load_data(data_path, 'y3','no_preproc')\n",
    "    dx_labels = load_data(data_path, 'dx_cat3','no_preproc')\n",
    "    tsne_model = TSNE(n_components=2, random_state=0, init='pca')\n",
    "    tsne_encodings[snap_iter] = tsne_model.fit_transform(encodings) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "plot_encodings = True\n",
    "plot_wts = False\n",
    "\n",
    "color_scores = dx_labels\n",
    "cm = plt.get_cmap('RdYlBu') \n",
    "marker_size = 100\n",
    "marker_size = (np.mod(color_scores+1,2)+1)*50\n",
    "\n",
    "for sn, snap_iter in enumerate(np.arange(snap_start,snap_end,snap_interval)):\n",
    "    plt.subplot(np.ceil(n_snaps/2.0),2,sn+1)\n",
    "    if plot_encodings:\n",
    "        plt.scatter(tsne_encodings[snap_iter][:,0],tsne_encodings[snap_iter][:,1],c=color_scores,s=marker_size,cmap=cm)\n",
    "    if plot_wts:\n",
    "        plt.imshow(net_weights[snap_iter][weight_layers])\n",
    "        \n",
    "    plt.title(snap_iter)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp13_MC 8 HC_CT\n",
      "Weights Tuned for: both\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.43468058794407005]\n",
      "ADAS mse: [56.331764018840886]\n",
      "ADAS means: 0.434680587944, 56.3317640188\n",
      "\n",
      "MMSE corr: [0.35415910139956397]\n",
      "MMSE mse: [6.025714111978882]\n",
      "MMSE means: 0.3541591014, 6.02571411198\n",
      "\n",
      "opt_hyp: {5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {5: 1}\n",
      "Weights Tuned for: adas\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.43468058794407005]\n",
      "ADAS mse: [56.331764018840886]\n",
      "ADAS means: 0.434680587944, 56.3317640188\n",
      "\n",
      "MMSE corr: [0.35415910139956397]\n",
      "MMSE mse: [6.025714111978882]\n",
      "MMSE means: 0.3541591014, 6.02571411198\n",
      "\n",
      "opt_hyp: {5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {5: 1}\n",
      "Weights Tuned for: mmse\n",
      "Clinical_Scale: BOTH\n",
      "ADAS corr: [0.43468058794407005]\n",
      "ADAS mse: [56.331764018840886]\n",
      "ADAS means: 0.434680587944, 56.3317640188\n",
      "\n",
      "MMSE corr: [0.35415910139956397]\n",
      "MMSE mse: [6.025714111978882]\n",
      "MMSE means: 0.3541591014, 6.02571411198\n",
      "\n",
      "opt_hyp: {5: {'tr': {'ADAS': 1, 'DX': 1, 'MMSE': 2}, 'node_sizes': {'COMB_ff': 25, 'HC_L_ff': 50, 'code': 600, 'DX_ff': 25, 'ADAS_ff': 25, 'En2': 2000, 'CT_ff': 50, 'HC_CT_ff': 25, 'out': 686, 'MMSE_ff': 25, 'En1': 10000, 'HC_R_ff': 50}, 'dr': {'HC': 0, 'HC_CT': 0, 'COMB': 0, 'CT': 0}, 'solver_conf': {'base_lr': 5e-06, 'wt_decay': 0.001}, 'lr': {'HC': 0.1, 'COMB': 1, 'CT': 1}}}\n",
      "\n",
      "opt_snap: {5: 1}\n"
     ]
    }
   ],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "#exp_name = 'Exp11'\n",
    "niter = 20000\n",
    "# modality = 'HC_CT'\n",
    "# start_MC=1\n",
    "# n_MC=10\n",
    "# MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "# start_fold = 1\n",
    "# n_folds = 10\n",
    "# fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "#preproc = 'no_preproc'\n",
    "#batch_size = 256\n",
    "snap_interval = 4000\n",
    "snap_start = 4000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "#Clinical_Scale = 'ADAS13'\n",
    "\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "idx = 0  \n",
    "df_perf_dict_adas = {}\n",
    "df_perf_dict_mmse = {}\n",
    "df_perf_dict_adas_tuned = {}\n",
    "df_perf_dict_mmse_tuned = {}\n",
    "df_perf_dict_opt = {}\n",
    "\n",
    "for mc in MC_list:\n",
    "    print exp_name, mc, modality\n",
    "\n",
    "    for hype in hype_configs.keys():      \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "\n",
    "        for fid in fid_list:\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "            #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "            #with open(test_filename_txt, 'w') as f:\n",
    "            #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                    f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC']\n",
    "                elif modality == 'CT':\n",
    "                    f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_CT_SpecCluster_dyn']\n",
    "                elif modality == 'HC_CT':\n",
    "                    f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC','X_CT_SpecCluster_dyn']\n",
    "                elif modality == 'HC_CT_unified_hyp1':\n",
    "                    f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_HC_CT']\n",
    "                else:\n",
    "                    print 'Wrong modality'\n",
    "\n",
    "            #print 'Hype # {}, MC # {}, Fold # {}, Clinical_Scale {}'.format(hype, mc, fid, Clinical_Scale)\n",
    "            data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            if Clinical_Scale == 'ADAS13':\n",
    "                act_scores = load_data(data_path, 'adas','no_preproc')\n",
    "            elif Clinical_Scale == 'MMSE': \n",
    "                act_scores = load_data(data_path, 'mmse','no_preproc')\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                act_scores_adas13 = load_data(data_path, 'adas','no_preproc')\n",
    "                act_scores_mmse = load_data(data_path, 'mmse','no_preproc')\n",
    "            else:\n",
    "                print 'unknown clinical scale'\n",
    "\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort)\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            sub.call([\"cp\", baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort), baseline_dir + \n",
    "                      'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)])\n",
    "            #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "            #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "\n",
    "            if Clinical_Scale in ['ADAS13', 'MMSE']:                \n",
    "                multi_task = False\n",
    "                cs_list = ['opt']\n",
    "                iter_euLoss = []\n",
    "                iter_r = []        \n",
    "                iter_pred_scores = []\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = np.squeeze(results['X_out'])            \n",
    "                    iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                    iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "                fold_r[config_idx] = np.array(iter_r)\n",
    "                fold_act_scores[fid] = act_scores\n",
    "                fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                multi_task = True\n",
    "                cs_list = ['adas','mmse']\n",
    "                iter_euLoss_adas13 = []\n",
    "                iter_r_adas13 = []        \n",
    "                iter_pred_scores_adas13 = []\n",
    "                iter_euLoss_mmse = []\n",
    "                iter_r_mmse = []        \n",
    "                iter_pred_scores_mmse = []\n",
    "\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = results['X_out']   \n",
    "                    encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                    encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                    iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                    iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                    iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                    iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "                fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "                fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "                fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "                fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "                fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "                fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "                fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "\n",
    "    \n",
    "    if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "        fold_perf_dict = {'fold_r':fold_r,'fold_euLoss':fold_euLoss,'fold_act_scores':fold_act_scores,'fold_pred_scores':fold_pred_scores}\n",
    "    else: \n",
    "        fold_perf_dict = {'fold_r_adas13':fold_r_adas13,'fold_r_mmse':fold_r_mmse,'fold_euLoss_adas13':fold_euLoss_adas13,'fold_euLoss_mmse':fold_euLoss_mmse,\n",
    "                         'fold_act_scores_adas13':fold_act_scores_adas13,'fold_act_scores_mmse':fold_act_scores_mmse,\n",
    "                          'fold_pred_scores_adas13':fold_pred_scores_adas13,'fold_pred_scores_mmse':fold_pred_scores_mmse}\n",
    "        \n",
    "    opt_metric = 'euLoss' #euLoss or corr or both\n",
    "    #task_weights = {'ADAS':1,'MMSE':0} \n",
    "    save_multitask_results = False\n",
    "    CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/'  \n",
    "    save_path = '{}{}_{}_NN_{}'.format(CV_model_dir,exp_name,mc,modality)\n",
    "    \n",
    "    # Create dictionaries of perfromance based on differnt task weights \n",
    "    for key, task_weights in {'adas':{'ADAS':1,'MMSE':0},'mmse':{'ADAS':0,'MMSE':1},'both':{'ADAS':1,'MMSE':1}}.items():\n",
    "        print 'Weights Tuned for: {}'.format(key)                              \n",
    "        results = compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path)  \n",
    "        adas_pred_scores = results['adas_pred_CV_scores']\n",
    "        adas_act_scores = results['adas_act_CV_scores']\n",
    "        mmse_pred_scores = results['mmse_pred_CV_scores']\n",
    "        mmse_act_scores = results['mmse_act_CV_scores']\n",
    "        \n",
    "        # populate the perf dictionary for 10 MC x 10 folds\n",
    "        model_choice = 'APANN'    \n",
    "        for f, fid in enumerate(fid_list): \n",
    "            if key in ['adas','mmse']:\n",
    "                r_valid = results['{}_r'.format(key)][f]\n",
    "                MSE_valid = results['{}_mse'.format(key)][f]\n",
    "                RMSE_valid = results['{}_rmse'.format(key)][f]\n",
    "                \n",
    "\n",
    "                if key == 'adas':\n",
    "                    df_perf_dict_adas_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                                    'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0,\n",
    "                                                    'pred_scores':adas_pred_scores, 'act_scores':adas_act_scores}\n",
    "                elif key == 'mmse':\n",
    "                    df_perf_dict_mmse_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                                    'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0,\n",
    "                                                    'pred_scores':mmse_pred_scores, 'act_scores':mmse_act_scores}\n",
    "                else: \n",
    "                    print 'unknown key'\n",
    "\n",
    "            elif key == 'both':                    \n",
    "                for cs in cs_list:            \n",
    "                    r_valid = results['{}_r'.format(cs)][f]\n",
    "                    MSE_valid = results['{}_mse'.format(cs)][f]\n",
    "                    RMSE_valid = results['{}_rmse'.format(cs)][f]\n",
    "\n",
    "                    if cs == 'adas':\n",
    "                        df_perf_dict_adas[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                                  'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0,\n",
    "                                                  'pred_scores':adas_pred_scores, 'act_scores':adas_act_scores}\n",
    "                    elif cs == 'mmse':\n",
    "                        df_perf_dict_mmse[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                                  'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0,\n",
    "                                                  'pred_scores':mmse_pred_scores, 'act_scores':mmse_act_scores}  \n",
    "                    else: \n",
    "                        print 'unknown key'\n",
    "\n",
    "\n",
    "            else: #single task dictionary\n",
    "                df_perf_dict_opt[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                \n",
    "\n",
    "            idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.19165413   7.69251163   4.17063713  -3.10327045  -2.14980698\n",
      "  10.57621574  -1.15747173  -4.83323853  -6.83380318   8.38311577\n",
      "   9.55613224  -6.59080124   7.21377563   1.60269936  14.48505211\n",
      "   0.56687355  -3.14930813  -0.40178482  -8.33746147   8.44085312\n",
      "   1.70791348  -4.32915298   1.81736938  -6.0538121   -0.60653496\n",
      "   5.22998428   2.5357093    0.42538158  11.50906364  -6.96784218\n",
      " -19.69970894  -2.13622673   5.93899734  10.40242489   1.36518185\n",
      "  -0.51688202   8.01778595 -19.82795723  -8.83472355  -1.93143265\n",
      "   1.81590454 -15.35012436  -1.74704552   6.89324284  14.00945091\n",
      " -11.23031624 -15.04920292  -8.98065083  -6.33333199  10.14040955\n",
      "   6.93676758   1.70050232  -7.15806492  -1.638218    -6.45249836\n",
      "  -2.77902977   3.77913094  -0.15781013   4.1582814   -9.81236267\n",
      "   1.04980095  -4.55159958  -3.78840446  16.3254718    2.83596428\n",
      "   3.84368324  15.03104584   1.22026627 -15.21646309 -15.39570713\n",
      "  -3.44275761   8.76411438   6.71894646   0.52087593   6.51832199\n",
      "  -0.12314606  -4.85672092  -5.64736557  -8.82988787  -2.49696732\n",
      "   2.45319748 -13.0667305   -3.41227531   4.17386055 -11.59353542\n",
      "  -1.87021065 -11.44768524   7.08463955   3.88779163   7.33945084\n",
      "  -6.01238632 -15.11613178  10.88659477   3.71912193  -6.38169861\n",
      "   1.06531715   8.70314503   3.54639816  21.35590744  -5.62894821\n",
      "   0.53701019  -4.90799713  15.43747711   6.62040138   0.65046787\n",
      "   7.36891556  20.15582657 -15.77274895   7.70636082 -15.57774067\n",
      "   5.15675354  -2.80807304  -0.11309242  -4.39125347   7.40314293\n",
      "   3.32567787  -4.89025593   9.46137428   1.23850822  -0.81269455\n",
      "   4.59230614  -6.77495384   6.79185581  -9.25320673  -0.33506012\n",
      "   3.26252937   5.59076405   1.46875763   5.96831512  -5.20798922\n",
      "  -0.14884853  -2.89777374 -12.4953661    1.35099316   0.6576519\n",
      "  -3.5523777   -9.55607128]\n"
     ]
    }
   ],
   "source": [
    "print adas_pred_scores[5][0] - adas_act_scores[5][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results at: /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/\n"
     ]
    }
   ],
   "source": [
    "#Save df style dictionaly for seaborn plots\n",
    "cohort = 'ADNI1and2'\n",
    "update = 4\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_up_{}.pkl'.format(exp_name, cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse,df_perf_dict_path)\n",
    "\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_tuned_up_{}.pkl'.format(exp_name,cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas_tuned,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE_tuned_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse_tuned,df_perf_dict_path)\n",
    "\n",
    "\n",
    "print 'saving results at: {}'.format(CV_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 25)\n",
    "n_rows = n_folds\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "plot_perf = False\n",
    "\n",
    "if multi_task:\n",
    "    euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "    corrs = [fold_r_adas13,fold_r_mmse]\n",
    "else:\n",
    "    euLosses = [fold_euLoss]\n",
    "    corrs = [fold_r]\n",
    "    \n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        \n",
    "        if plot_perf:\n",
    "            plt.figure(fid)\n",
    "            plt.subplot(n_rows,n_cols,1)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "            plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "            plt.legend()\n",
    "            plt.subplot(n_rows,n_cols,2)\n",
    "            plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "            plt.title('correlation, fid: {}'.format(fid))\n",
    "            plt.legend(loc=2)\n",
    "\n",
    "print preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path):\n",
    "    fold_r_dict = {}\n",
    "    fold_euLoss_dict = {}\n",
    "    fold_act_scores_dict = {}\n",
    "    fold_pred_scores_dict = {}\n",
    "\n",
    "    if multi_task:\n",
    "        fold_r_dict['ADAS'] = fold_perf_dict['fold_r_adas13']\n",
    "        fold_r_dict['MMSE'] = fold_perf_dict['fold_r_mmse']\n",
    "        fold_euLoss_dict['ADAS'] = fold_perf_dict['fold_euLoss_adas13']\n",
    "        fold_euLoss_dict['MMSE'] = fold_perf_dict['fold_euLoss_mmse']\n",
    "        fold_act_scores_dict['ADAS'] = fold_perf_dict['fold_act_scores_adas13']\n",
    "        fold_act_scores_dict['MMSE'] = fold_perf_dict['fold_act_scores_mmse']\n",
    "        fold_pred_scores_dict['ADAS'] = fold_perf_dict['fold_pred_scores_adas13']\n",
    "        fold_pred_scores_dict['MMSE'] = fold_perf_dict['fold_pred_scores_mmse']\n",
    "        \n",
    "        NN_results = computePerfMetrics(fold_r_dict, fold_euLoss_dict, fold_act_scores_dict, fold_pred_scores_dict, opt_metric, hype_configs, \n",
    "                                        Clinical_Scale,task_weights)\n",
    "        adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "        mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "        adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "        mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "        adas_rmse = NN_results['opt_ADAS']['CV_RMSE'].values()\n",
    "        mmse_rmse = NN_results['opt_MMSE']['CV_RMSE'].values()\n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        adas_pred_CV_scores = NN_results['opt_ADAS']['predicted_CV_scores']\n",
    "        adas_act_CV_scores = NN_results['opt_ADAS']['actual_CV_scores']\n",
    "        mmse_pred_CV_scores = NN_results['opt_MMSE']['predicted_CV_scores']\n",
    "        mmse_act_CV_scores = NN_results['opt_MMSE']['actual_CV_scores']\n",
    "        \n",
    "        \n",
    "        print 'ADAS corr: {}'.format(adas_r)\n",
    "        print 'ADAS mse: {}'.format(adas_mse)\n",
    "        print 'ADAS means: {}, {}'.format(np.mean(adas_r),np.mean(adas_mse))\n",
    "        print ''\n",
    "        print 'MMSE corr: {}'.format(mmse_r)\n",
    "        print 'MMSE mse: {}'.format(mmse_mse)\n",
    "        print 'MMSE means: {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse))\n",
    "        print ''\n",
    "        print 'opt_hyp: {}'.format(opt_hyp)\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        \n",
    "        return {'adas_r':adas_r, 'adas_mse':adas_mse, 'adas_rmse':adas_rmse, 'mmse_r':mmse_r, 'mmse_mse':mmse_mse, 'mmse_rmse':mmse_rmse,\n",
    "               'adas_pred_CV_scores':adas_pred_CV_scores,'adas_act_CV_scores':adas_act_CV_scores,\n",
    "               'mmse_pred_CV_scores':mmse_pred_CV_scores,'mmse_act_CV_scores':mmse_act_CV_scores}\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        NN_results = computeSingleTaskPerfMetrics(fold_perf_dict, opt_metric, hype_configs)\n",
    "        opt_r = NN_results['opt_r'].values()        \n",
    "        opt_mse = NN_results['opt_mse'].values()        \n",
    "        opt_rmse = NN_results['opt_rmse'].values()        \n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['actual_CV_scores']\n",
    "        print 'opt corr: {}'.format(opt_r)\n",
    "        print 'opt mse: {}'.format(opt_mse)\n",
    "        print 'means: {}, {}'.format(np.mean(opt_r),np.mean(opt_mse))\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        print ''\n",
    "    \n",
    "        return {'opt_r':opt_r, 'opt_mse':opt_mse, 'opt_rmse':opt_rmse,'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    \n",
    "\n",
    "\n",
    "    if save_multitask_results:\n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')        \n",
    "        save_path = save_path + '_' + st + '.pkl' \n",
    "        print 'saving results at: {}'.format(save_path)\n",
    "        output = open(save_path, 'wb')\n",
    "        pickle.dump(NN_results, output)\n",
    "        output.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predicted_CV_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6f668dc9c41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted_CV_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual_CV_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predicted_CV_scores'"
     ]
    }
   ],
   "source": [
    "a = np.squeeze(results['predicted_CV_scores'][7])\n",
    "b = np.squeeze(results['actual_CV_scores'][7])\n",
    "\n",
    "print a , b                               \n",
    "print stats.pearsonr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    \n",
    "     # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "\n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    opt_rmse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "\n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    opt_rmse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "\n",
    "    print 'Clinical_Scale: {}'.format(Clinical_Scale)\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        print 'This function does not work for single clinical scale models. Use the code from the next cell'\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "\n",
    "        fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "        fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "        fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "        fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "\n",
    "        for fid in fid_hype_map.keys():\n",
    "            r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "            r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "            r_perf_array = np.array(fid_r_perf[fid])\n",
    "            euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "            euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "            euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "            if opt_metric == 'euLoss':\n",
    "                h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "            else:\n",
    "                h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "\n",
    "            opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "            opt_snap[fid] = snp\n",
    "\n",
    "            opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "            opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "            opt_rmse_adas[fid] = np.sqrt(2*euLoss_perf_array_adas[h,snp]) #RMSE\n",
    "            opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "            opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "            opt_rmse_mmse[fid] = np.sqrt(2*euLoss_perf_array_mmse[h,snp]) #RMSE\n",
    "\n",
    "            actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "            opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "            actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "            opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "        opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'CV_RMSE':opt_rmse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "        opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'CV_RMSE':opt_rmse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_snap':opt_snap, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "def computeSingleTaskPerfMetrics(fold_perf_dict,opt_metric,hype_configs):\n",
    "    corrs = fold_perf_dict['fold_r']\n",
    "    euLosses = fold_perf_dict['fold_euLoss']\n",
    "    act_scores = fold_perf_dict['fold_act_scores']\n",
    "    pred_scores = fold_perf_dict['fold_pred_scores']\n",
    "\n",
    "    \n",
    "    NN_multitask_results = {}\n",
    "   \n",
    "    snap_array = np.arange(snap_start,niter+1,snap_start)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = {}\n",
    "    opt_mse = {}\n",
    "    opt_rmse = {}\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "    actual_scores = {}\n",
    "    opt_pred_scores = {}\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "        # if want to find best hyp from mse values\n",
    "        if opt_metric == 'euLoss':\n",
    "            h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        else:\n",
    "            h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "            \n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        r = r_perf_array[h,snp]        \n",
    "        opt_r[fid] = r\n",
    "        opt_mse[fid] = 2*eu_loss\n",
    "        opt_rmse[fid] = np.sqrt(2*eu_loss)\n",
    "        #print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)        \n",
    "        opt_snap[fid] = snp\n",
    "        opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "        actual_scores[fid] = fold_act_scores[fid]\n",
    "        opt_pred_scores[fid] = fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp]\n",
    "\n",
    "    #print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r.values()), np.mean(opt_mse.values()))\n",
    "    #print opt_r\n",
    "    #print opt_mse\n",
    "    return {'opt_r':opt_r,'opt_mse':opt_mse,'opt_rmse':opt_rmse,'opt_hype':opt_hype,'opt_snap':opt_snap, 'actual_scores':actual_scores,'opt_pred_scores':opt_pred_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "\n",
    "model_file = 'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-05-06-17-14-49.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {4:0,5:1}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_ADAS13_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(5.64658243068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update fold performance for multitask\n",
    "print NN_results['opt_ADAS'].keys()\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "#model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl'\n",
    "model_file_soa = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-23-52.pkl'\n",
    "print 'current state of art: ' + model_file_soa\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file_soa,'rb'))\n",
    "\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "#load old file\n",
    "# model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-08-24-14-02-48.pkl'\n",
    "# print 'updated fold result file: ' + model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "idx_pairs = {1:1,3:3}\n",
    "CV_data = update_multifold_perf(boxplots_dir + model_file_soa, NN_results, idx_pairs)\n",
    "\n",
    "print \"\"\n",
    "print 'updated CV_data'\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "#print 'saving results at: {}'.format(save_name)\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(CV_data, output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():        \n",
    "        for idx in idx_pairs.keys():            \n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "\n",
    "    return CV_data\n",
    "\n",
    "def update_multifold_perf(saved_perf_file, new_perf_dict, idx_pairs):    \n",
    "    perf_metrics = ['CV_r', 'actual_CV_scores', 'CV_MSE', 'predicted_CV_scores']    \n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        if key == 'opt_hype':\n",
    "            for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                CV_data[key][idx_key] = new_perf_dict[key][idx_val]\n",
    "                \n",
    "        else:\n",
    "            for pm in perf_metrics:\n",
    "                for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                    CV_data[key][pm][idx_key] = new_perf_dict[key][pm][idx_val]\n",
    "    \n",
    "    return CV_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "model_files = ['Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl',             \n",
    "             'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-05-23-07-31-29.pkl']\n",
    "\n",
    "#'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-03-19-29-16_Fold9_10.pkl'\n",
    "\n",
    "Clinical_Scale = 'ADAS'\n",
    "perf_array = []\n",
    "for mf in model_files:\n",
    "    CV_data = pickle.load(open(boxplots_dir + mf,'rb'))['opt_{}'.format(Clinical_Scale)]        \n",
    "    perf_array.append(CV_data['CV_r'].values())\n",
    "\n",
    "print (np.max(np.array(perf_array),axis=0))\n",
    "print np.mean(np.max(np.array(perf_array),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "\n",
    "\n",
    "save_detailed_results = True\n",
    "multi_task = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results    \n",
    "    if not multi_task:\n",
    "        NN_results = {}\n",
    "        NN_results['CV_MSE'] = opt_mse\n",
    "        NN_results['CV_r'] = opt_r\n",
    "        NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "        NN_results['actual_CV_scores'] = actual_scores\n",
    "        NN_results['tid_snap_config_dict'] = opt_hype\n",
    "        print opt_r\n",
    "        print opt_mse\n",
    "        \n",
    "    else:\n",
    "        NN_results = NN_multitask_results\n",
    "        print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "        print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "        print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "        print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])\n",
    "        \n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_BOTH_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    for modality in modalities:\n",
    "        in_data = load_data(in_data_path, 'Fold_{}_{}'.format(fid,modality), preproc)         \n",
    "\n",
    "        # HDF5 is pretty efficient, but can be further compressed.\n",
    "        comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "        with h5py.File(out_data_path, 'a') as f:      \n",
    "            if modality == 'X_R_CT': #Fix the typo            \n",
    "                modality = 'X_CT'            \n",
    "\n",
    "            f.create_dataset('{}'.format(modality), data=in_data, **comp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp6'\n",
    "exp_name_out = 'Exp6_MC'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "modalities = ['X_L_HC','X_R_HC','X_R_CT','adas','mmse','dx']\n",
    "dataset = 'ADNI1'\n",
    "preproc = 'no_preproc' #binary labels\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "\n",
    "for mc in np.arange(1,3,1):\n",
    "    for fid in np.arange(1,11,1):\n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_train_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_NN_OuterFold_MC_{}_fold{}_valid_InnerFold_1.h5'.format(exp_name,dataset,mc,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_ADAS13_NN_valid_MC_{}.h5'.format(exp_name,dataset,mc)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name_out)\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modalities,preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC: fid[1:4] 8000; fid[5,6] 6000, fid[7,8,9]: 8000 fid 10: 6000\n",
    "CT: fid[1:5] 12000 fid[6:10] 6000\n",
    "\n",
    "#spawn net\n",
    "ADNI_FF_train_hyp1_CT.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_6/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_7/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_8/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_9/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 1\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold1/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 2\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold2/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 3\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold3/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 4\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold4/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 5\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold5/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 6\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold6/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 7\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold7/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 8\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold8/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 9\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold9/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "fid: 10\n",
      "AE_branch: CT\n",
      "Spawning new net\n",
      "target ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "pretrained ff1 weights are (50, 686) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "AE_branch: HC\n",
      "target L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "target R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "pretrained L_ff1 weights are (50, 16086) dimensional and biases are (50,) dimensional\n",
      "pretrained R_ff1 weights are (50, 16471) dimensional and biases are (50,) dimensional\n",
      "Saving net to /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_10/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n"
     ]
    }
   ],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "cohort = 'ADNI2'\n",
    "modality = 'HC_CT'\n",
    "pretrain_snap_HC = 20000 #ADNI1: 5000\n",
    "pretrain_snap_CT = 20000 #ADNI1: 5000\n",
    "n_folds = 10\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/' \n",
    "#Custom snaps per fold\n",
    "#HC_iter = [8000,8000,8000,8000,6000,6000,8000,8000,8000,6000]\n",
    "#CT_iter = [12000,12000,12000,12000,12000,6000,6000,6000,6000,6000]\n",
    "#mc=1\n",
    "hc_hyp = 'hyp1'\n",
    "ct_hyp = 'hyp1'\n",
    "pretrain_hyp = 'hyp2' #This is hyp generated using optimal combination of hc and ct hyps. Prototxt file is saved with this hyp extension\n",
    "\n",
    "exp_name = 'Exp11_MC'\n",
    "\n",
    "for mc in np.arange(6,11,1):\n",
    "    for fid in np.arange(1,n_folds+1,1):\n",
    "        print 'fid: {}'.format(fid)\n",
    "        #for AE_branch in ['CT','R_HC','L_HC']:\n",
    "        for AE_branch in ['CT','HC']:\n",
    "            print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "            if AE_branch == 'L_HC':\n",
    "                params_FF = ['L_ff1', 'L_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'R_HC':\n",
    "                params_FF = ['R_ff1', 'R_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'HC':\n",
    "                params_FF = ['L_ff1','R_ff1']\n",
    "                AE_iter = pretrain_snap_HC\n",
    "                hyp = hc_hyp\n",
    "            elif AE_branch == 'CT':\n",
    "                #params_FF = ['ff1', 'ff2']\n",
    "                params_FF = ['ff1']\n",
    "                AE_iter = pretrain_snap_CT\n",
    "                hyp = ct_hyp\n",
    "                #fid for pretain is 1 because it's same definition for all the folds.\n",
    "                #Only use this during 1 of the modalities to avoid overwritting\n",
    "                print 'Spawning new net'\n",
    "                pretrain_net = caffe.Net(baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,pretrain_hyp,modality), caffe.TRAIN)\n",
    "            else:\n",
    "                print 'Wrong AE branch'\n",
    "\n",
    "            # conv_params = {name: (weights, biases)}\n",
    "            conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "            for conv in params_FF:\n",
    "                print 'target {} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "            # Review AE net params \n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hyp,AE_branch)\n",
    "            #model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "            model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hyp,AE_branch,AE_iter) \n",
    "\n",
    "            AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "            #params_AE = ['encoder1', 'code']\n",
    "            params_AE = params_FF #if you are using pretrained NN\n",
    "\n",
    "            # fc_params = {name: (weights, biases)}\n",
    "            fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "            for fc in params_AE:\n",
    "                print 'pretrained {} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "            #transplant net parameters\n",
    "            for pr, pr_conv in zip(params_AE, params_FF):\n",
    "                conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "                conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "            save_net = True\n",
    "            if save_net:\n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_{}_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                save_path = baseline_dir + net_name.format(mc,fid,cohort,pretrain_hyp,modality,pretrain_snap_HC,pretrain_snap_CT)\n",
    "                print \"Saving net to \" + save_path\n",
    "                pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_data['opt_ADAS']['CV_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print n_snaps/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
