{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import caffe\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Take care with the paths -defaults ones from protobuf are not correct. Need to change snapshot and train / test data paths \n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(2)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#Useful resources:\n",
    "#http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "#http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.y  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.y,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1, concat_param=dict(axis=1))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.y,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.y3,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT,n.y  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_CT,n.y,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.y,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.y3,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.y  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT,n.y,n.y3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "#     n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['ff2'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "#     n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "     #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,n.ff1, concat_param=dict(axis=1))\n",
    "    #n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['ff3'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=2), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.dropC1 = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "#     n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "#     n.dropC2 = L.Dropout(n.ff4, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        #n.ff1_ADAS13, n.ff1_MMSE = L.Split(n.ff3,num_output=2) #This is done automatically by caffe! \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        #n.ff2_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2ADAS13 = L.ReLU(n.ff2_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        #n.ff2_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        #n.NLin2MMSE = L.ReLU(n.ff2_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.y3)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.y,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.y3,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ae(hdf5, batch_size,node_sizes,modality):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    if modality == 'CT':\n",
    "        n.X_CT = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder1 = L.InnerProduct(n.X_CT, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "    elif modality =='R_HC':\n",
    "        n.X_R_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "    elif modality =='L_HC':\n",
    "        n.X_L_HC = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5)\n",
    "        n.encoder1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    #Encoder layers  \n",
    "    n.NLinEn1 = L.ReLU(n.encoder1, in_place=True)\n",
    "#     n.encoder2 = L.InnerProduct(n.encoder1, num_output=node_sizes['En2'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.ReLU(n.encoder2, in_place=True)\n",
    "    \n",
    "    #code layer\n",
    "    n.code = L.InnerProduct(n.encoder1, num_output=node_sizes['code'], weight_filler=dict(type='xavier')) \n",
    "    \n",
    "    #Decoder layers\n",
    "    n.decoder1 = L.InnerProduct(n.code, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "    n.NLinDe1 = L.ReLU(n.decoder1, in_place=True)\n",
    "#     n.decoder2 = L.InnerProduct(n.decoder1, num_output=node_sizes['En1'], weight_filler=dict(type='xavier'))\n",
    "#     n.NLinDe2 = L.ReLU(n.decoder2, in_place=True)\n",
    "    \n",
    "    n.output = L.InnerProduct(n.decoder1, num_output=node_sizes['out'], weight_filler=dict(type='xavier'))    \n",
    "    #n.accuracy = L.Accuracy(n.ip1, n.label)\n",
    "    if modality == 'CT':\n",
    "        n.loss = L.EuclideanLoss(n.output, n.X_CT)        \n",
    "    elif modality =='R_HC':\n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_R_HC)\n",
    "    elif modality =='L_HC':\n",
    "        n.loss = L.SigmoidCrossEntropyLoss(n.output, n.X_L_HC)\n",
    "    else:\n",
    "        print \"wrong modality\"\n",
    "    \n",
    "    print n\n",
    "    return n.to_proto() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.01 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_caffe(solver,niter,multi_task):\n",
    "\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 100\n",
    "    #n_feat = solver.test_nets[0].blobs['data'].data.shape[1]\n",
    "    # losses will also be stored in the log\n",
    "    if not multi_task:\n",
    "        train_loss = zeros(niter)\n",
    "        test_loss = zeros(int(np.ceil(niter / test_interval)))        \n",
    "    else: \n",
    "        train_ADAS13_loss = zeros(niter)\n",
    "        test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_MMSE_loss = zeros(niter)\n",
    "        test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "        \n",
    "    #output = zeros((niter, batch_size))\n",
    "    #solver.restore()\n",
    "    #the main solver loop\n",
    "    for it in range(niter):\n",
    "        #solver.net.forward()\n",
    "        solver.step(1)  # SGD by Caffe    \n",
    "        # store the train loss\n",
    "        if not multi_task:\n",
    "            train_loss[it] = solver.net.blobs['loss'].data        \n",
    "        else: \n",
    "            train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "            train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "        # store the output on the first test batch\n",
    "        # (start the forward pass at conv1 to avoid loading new data)\n",
    "        #solver.test_nets[0].forward()\n",
    "        #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "        # run a full test every so often\n",
    "        # (Caffe can also do this for us and write to a log, but we show here\n",
    "        #  how to do it directly in Python, where more complicated things are easier.)\n",
    "        if it % test_interval == 0:        \n",
    "            t_loss = 0\n",
    "            t_ADAS13_loss = 0\n",
    "            t_MMSE_loss = 0\n",
    "            for test_it in range(test_iter):\n",
    "                solver.test_nets[0].forward()\n",
    "                if not multi_task:\n",
    "                    t_loss += solver.test_nets[0].blobs['loss'].data\n",
    "                else: \n",
    "                    t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                    t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "            if not multi_task:\n",
    "                test_loss[it // test_interval] = t_loss/test_iter            \n",
    "                print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                    it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval])\n",
    "            else:\n",
    "                test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/test_iter\n",
    "                test_MMSE_loss[it // test_interval] = t_MMSE_loss/test_iter            \n",
    "                print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                    it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                    it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "    if not multi_task:\n",
    "        perf = {'train_loss':[train_loss],'test_loss':[test_loss]}\n",
    "    else:\n",
    "        perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "    return perf\n",
    "\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,fid,exp_name,modality):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    \n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    #s.solver_type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 100000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 10000\n",
    "    s.snapshot_prefix = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}'.format(fid,exp_name,hype,modality)\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "fid = 1\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "\n",
    "train_data_CT = load_data(train_filename_hdf, 'X_CT',preproc)\n",
    "train_data_y = load_data(train_filename_hdf, 'y','no_preproc')\n",
    "test_data_CT = load_data(test_filename_hdf, 'X_CT',preproc)\n",
    "test_data_y = load_data(test_filename_hdf, 'y','no_preproc')\n",
    "\n",
    "print train_data_CT.shape, train_data_y.shape, test_data_CT.shape, test_data_y.shape\n",
    "np.mean(train_data_y), np.std(train_data_y), np.mean(test_data_y), np.std(test_data_y)\n",
    "\n",
    "ct_corr_train = []\n",
    "ct_corr_test = []\n",
    "for c in np.arange(0,train_data_CT.shape[1],1):\n",
    "    ct_corr_train.append(stats.pearsonr(train_data_CT[:,c],train_data_y)[0])\n",
    "    ct_corr_test.append(stats.pearsonr(test_data_CT[:,c],test_data_y)[0])\n",
    "    \n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(np.mean(train_data_CT, axis=0)-np.mean(test_data_CT, axis=0))\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.array(ct_corr_train) - np.array(ct_corr_test),label='train-test')\n",
    "#plt.plot(ct_corr_test,label='test')\n",
    "plt.legend()\n",
    "\n",
    "mean_diff = np.mean(train_data_CT, axis=0)-np.mean(test_data_CT, axis=0)\n",
    "\n",
    "print mean_diff[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hype # hyp1, Fold # 6\n",
      "<caffe.net_spec.NetSpec object at 0x7f4ec8179490>\n",
      "<caffe.net_spec.NetSpec object at 0x7f4e84227e90>\n",
      "Iteration: 0, train loss(batch, sum): (11276.1923828,inf), test loss: 11232.4283008\n",
      "Iteration: 500, train loss(batch, sum): (2242.77270508,3075.3705708), test loss: 2173.23351685\n",
      "Iteration: 1000, train loss(batch, sum): (2253.24609375,2610.98507739), test loss: 2112.43954712\n",
      "Iteration: 1500, train loss(batch, sum): (2191.82348633,2415.44238883), test loss: 1983.33509521\n",
      "Iteration: 2000, train loss(batch, sum): (2070.70483398,2285.26187592), test loss: 1882.84239746\n",
      "Iteration: 2500, train loss(batch, sum): (1627.47595215,2186.68351353), test loss: 1794.0854248\n",
      "Iteration: 3000, train loss(batch, sum): (1627.30651855,2107.42970752), test loss: 1716.07245605\n",
      "Iteration: 3500, train loss(batch, sum): (1701.70703125,2041.98427546), test loss: 1656.68284912\n",
      "Iteration: 4000, train loss(batch, sum): (1697.29321289,1985.95783649), test loss: 1612.10940674\n",
      "Iteration: 4500, train loss(batch, sum): (1427.7923584,1936.99308236), test loss: 1576.80087158\n",
      "Iteration: 5000, train loss(batch, sum): (1522.30810547,1893.71814595), test loss: 1529.70558228\n",
      "Iteration: 5500, train loss(batch, sum): (1347.6237793,1854.33167729), test loss: 1498.1713147\n",
      "Iteration: 6000, train loss(batch, sum): (1442.91394043,1818.23528131), test loss: 1468.30490601\n",
      "Iteration: 6500, train loss(batch, sum): (1537.98400879,1785.01434054), test loss: 1435.1309668\n",
      "Iteration: 7000, train loss(batch, sum): (1469.76196289,1754.01676991), test loss: 1408.74893555\n",
      "Iteration: 7500, train loss(batch, sum): (1212.44091797,1725.16804282), test loss: 1385.16772339\n",
      "Iteration: 8000, train loss(batch, sum): (1427.37268066,1698.5119836), test loss: 1366.98839355\n",
      "Iteration: 8500, train loss(batch, sum): (1211.34204102,1673.44340866), test loss: 1348.58789917\n",
      "Iteration: 9000, train loss(batch, sum): (1156.37207031,1649.9768544), test loss: 1333.10078369\n",
      "Iteration: 9500, train loss(batch, sum): (1228.51550293,1628.09188196), test loss: 1315.90887817\n",
      "Iteration: 10000, train loss(batch, sum): (1050.41296387,1607.22754125), test loss: 1300.2177124\n",
      "Iteration: 10500, train loss(batch, sum): (1194.0255127,1587.47912469), test loss: 1283.26796143\n",
      "Iteration: 11000, train loss(batch, sum): (1081.1986084,1568.7948099), test loss: 1273.5382019\n",
      "Iteration: 11500, train loss(batch, sum): (1083.10546875,1550.8645393), test loss: 1259.72099365\n",
      "Iteration: 12000, train loss(batch, sum): (1098.32788086,1533.75756246), test loss: 1245.68080933\n",
      "Iteration: 12500, train loss(batch, sum): (1152.92126465,1517.45244224), test loss: 1237.98878784\n",
      "Iteration: 13000, train loss(batch, sum): (1139.83154297,1501.72593546), test loss: 1225.82038086\n",
      "Iteration: 13500, train loss(batch, sum): (1035.85705566,1486.64568262), test loss: 1211.39946289\n",
      "Iteration: 14000, train loss(batch, sum): (1217.64099121,1472.18231648), test loss: 1206.25236938\n",
      "Iteration: 14500, train loss(batch, sum): (1114.3269043,1458.16502249), test loss: 1196.17408569\n",
      "Iteration: 15000, train loss(batch, sum): (1191.94567871,1444.67667813), test loss: 1184.26771484\n",
      "Iteration: 15500, train loss(batch, sum): (908.883605957,1431.66065821), test loss: 1179.08070801\n",
      "Iteration: 16000, train loss(batch, sum): (937.6640625,1419.04061847), test loss: 1168.98545166\n",
      "Iteration: 16500, train loss(batch, sum): (1161.69177246,1406.87012337), test loss: 1157.71982117\n",
      "Iteration: 17000, train loss(batch, sum): (1078.63659668,1395.06244255), test loss: 1149.61105042\n",
      "Iteration: 17500, train loss(batch, sum): (806.401000977,1383.57006464), test loss: 1143.1920697\n",
      "Iteration: 18000, train loss(batch, sum): (1026.79748535,1372.50225485), test loss: 1137.25399414\n",
      "Iteration: 18500, train loss(batch, sum): (881.542175293,1361.71483736), test loss: 1130.43287476\n",
      "Iteration: 19000, train loss(batch, sum): (939.033203125,1351.22380132), test loss: 1122.8219342\n",
      "Iteration: 19500, train loss(batch, sum): (1057.01477051,1341.09769229), test loss: 1120.46972534\n",
      "Iteration: 20000, train loss(batch, sum): (936.980651855,1331.18974792), test loss: 1111.3261084\n",
      "Iteration: 20500, train loss(batch, sum): (869.360412598,1321.54962631), test loss: 1104.80346191\n",
      "Iteration: 21000, train loss(batch, sum): (1032.72229004,1312.24124669), test loss: 1103.85536255\n",
      "Iteration: 21500, train loss(batch, sum): (825.761291504,1303.1022133), test loss: 1096.19071533\n",
      "Iteration: 22000, train loss(batch, sum): (880.823120117,1294.20740627), test loss: 1090.65161133\n",
      "Iteration: 22500, train loss(batch, sum): (885.730529785,1285.60824327), test loss: 1084.5237738\n",
      "Iteration: 23000, train loss(batch, sum): (775.951538086,1277.15316824), test loss: 1080.07106262\n",
      "Iteration: 23500, train loss(batch, sum): (916.860656738,1268.9226401), test loss: 1075.48870728\n",
      "Iteration: 24000, train loss(batch, sum): (817.277832031,1260.93284221), test loss: 1073.62001221\n",
      "Iteration: 24500, train loss(batch, sum): (837.517944336,1253.08242974), test loss: 1068.66811646\n",
      "Iteration: 25000, train loss(batch, sum): (820.043579102,1245.43189512), test loss: 1066.73305176\n",
      "Iteration: 25500, train loss(batch, sum): (898.817199707,1237.99422898), test loss: 1064.02130615\n",
      "Iteration: 26000, train loss(batch, sum): (919.082763672,1230.68164049), test loss: 1056.77082764\n",
      "Iteration: 26500, train loss(batch, sum): (804.030395508,1223.54530994), test loss: 1054.78438538\n",
      "Iteration: 27000, train loss(batch, sum): (946.503173828,1216.59165237), test loss: 1052.32326904\n",
      "Iteration: 27500, train loss(batch, sum): (868.744750977,1209.74702279), test loss: 1047.529729\n",
      "Iteration: 28000, train loss(batch, sum): (928.972167969,1203.06227782), test loss: 1043.96566345\n",
      "Iteration: 28500, train loss(batch, sum): (700.681945801,1196.52600397), test loss: 1040.56179321\n",
      "Iteration: 29000, train loss(batch, sum): (718.346740723,1190.10597044), test loss: 1037.31262329\n",
      "Iteration: 29500, train loss(batch, sum): (969.593322754,1183.83463443), test loss: 1033.81231018\n",
      "Iteration: 30000, train loss(batch, sum): (877.655639648,1177.67707463), test loss: 1030.51152649\n",
      "Iteration: 30500, train loss(batch, sum): (639.335693359,1171.61262545), test loss: 1031.59872009\n",
      "Iteration: 31000, train loss(batch, sum): (859.738647461,1165.70631737), test loss: 1027.57900452\n",
      "Iteration: 31500, train loss(batch, sum): (728.295898438,1159.88407469), test loss: 1022.71522095\n",
      "Iteration: 32000, train loss(batch, sum): (791.478942871,1154.16089778), test loss: 1022.30609924\n",
      "Iteration: 32500, train loss(batch, sum): (831.733764648,1148.57927987), test loss: 1020.76345703\n",
      "Iteration: 33000, train loss(batch, sum): (731.246459961,1143.06108756), test loss: 1015.00074707\n",
      "Iteration: 33500, train loss(batch, sum): (724.571777344,1137.6389764), test loss: 1013.04308411\n",
      "Iteration: 34000, train loss(batch, sum): (882.395874023,1132.34859197), test loss: 1013.74374817\n",
      "Iteration: 34500, train loss(batch, sum): (698.253356934,1127.10793832), test loss: 1007.39146179\n",
      "Iteration: 35000, train loss(batch, sum): (740.863647461,1121.95628116), test loss: 1005.05255859\n",
      "Iteration: 35500, train loss(batch, sum): (716.29119873,1116.92628817), test loss: 1003.48445984\n",
      "Iteration: 36000, train loss(batch, sum): (636.843017578,1111.94250085), test loss: 1002.81021729\n",
      "Iteration: 36500, train loss(batch, sum): (804.232299805,1107.04239715), test loss: 1000.66811218\n",
      "Iteration: 37000, train loss(batch, sum): (685.767333984,1102.24259688), test loss: 998.081456299\n",
      "Iteration: 37500, train loss(batch, sum): (709.186218262,1097.48943761), test loss: 998.822581177\n",
      "Iteration: 38000, train loss(batch, sum): (707.898254395,1092.81789253), test loss: 996.992770996\n",
      "Iteration: 38500, train loss(batch, sum): (794.936340332,1088.2355662), test loss: 993.093981934\n",
      "Iteration: 39000, train loss(batch, sum): (764.338317871,1083.70059009), test loss: 991.227810059\n",
      "Iteration: 39500, train loss(batch, sum): (673.347717285,1079.23548713), test loss: 990.585745239\n",
      "Iteration: 40000, train loss(batch, sum): (845.437866211,1074.85331808), test loss: 986.352310791\n",
      "Iteration: 40500, train loss(batch, sum): (738.765319824,1070.51091525), test loss: 983.624822998\n",
      "Iteration: 41000, train loss(batch, sum): (848.104309082,1066.2377079), test loss: 982.685927124\n",
      "Iteration: 41500, train loss(batch, sum): (620.970458984,1062.03047769), test loss: 982.92987854\n",
      "Iteration: 42000, train loss(batch, sum): (607.111328125,1057.8710969), test loss: 980.703203735\n",
      "Iteration: 42500, train loss(batch, sum): (884.36151123,1053.78164224), test loss: 978.090747681\n",
      "Iteration: 43000, train loss(batch, sum): (767.165405273,1049.74071548), test loss: 978.773449707\n",
      "Iteration: 43500, train loss(batch, sum): (528.741882324,1045.73732798), test loss: 979.112224121\n",
      "Iteration: 44000, train loss(batch, sum): (731.984069824,1041.81388535), test loss: 974.144714355\n",
      "Iteration: 44500, train loss(batch, sum): (626.569580078,1037.92535431), test loss: 973.226671753\n",
      "Iteration: 45000, train loss(batch, sum): (705.096679688,1034.08051885), test loss: 974.111488647\n",
      "Iteration: 45500, train loss(batch, sum): (695.089477539,1030.30936716), test loss: 971.014249268\n",
      "Iteration: 46000, train loss(batch, sum): (625.623413086,1026.56394182), test loss: 967.117268677\n",
      "Iteration: 46500, train loss(batch, sum): (625.026184082,1022.86209866), test loss: 967.357642822\n",
      "Iteration: 47000, train loss(batch, sum): (795.996520996,1019.23221153), test loss: 967.580426025\n",
      "Iteration: 47500, train loss(batch, sum): (644.885314941,1015.62117516), test loss: 964.263398437\n",
      "Iteration: 48000, train loss(batch, sum): (643.67980957,1012.05131679), test loss: 963.020814209\n",
      "Iteration: 48500, train loss(batch, sum): (615.069091797,1008.54950192), test loss: 965.367926636\n",
      "Iteration: 49000, train loss(batch, sum): (551.962524414,1005.06801257), test loss: 962.797527466\n",
      "Iteration: 49500, train loss(batch, sum): (726.747314453,1001.62591632), test loss: 959.403068237\n",
      "Iteration: 50000, train loss(batch, sum): (597.495422363,998.241099371), test loss: 962.374958496\n",
      "Iteration: 50500, train loss(batch, sum): (619.436767578,994.876151884), test loss: 960.772579346\n",
      "Iteration: 51000, train loss(batch, sum): (655.740600586,991.554473421), test loss: 957.710270996\n",
      "Iteration: 51500, train loss(batch, sum): (725.325317383,988.282850042), test loss: 956.190123901\n",
      "Iteration: 52000, train loss(batch, sum): (681.566955566,985.034596217), test loss: 953.94838562\n",
      "Iteration: 52500, train loss(batch, sum): (568.379882812,981.821878146), test loss: 953.455639038\n",
      "Iteration: 53000, train loss(batch, sum): (730.949951172,978.658152578), test loss: 951.994406128\n",
      "Iteration: 53500, train loss(batch, sum): (656.465454102,975.513182639), test loss: 950.528345947\n",
      "Iteration: 54000, train loss(batch, sum): (764.444763184,972.40585149), test loss: 952.879259033\n",
      "Iteration: 54500, train loss(batch, sum): (580.763366699,969.33757449), test loss: 951.073518677\n",
      "Iteration: 55000, train loss(batch, sum): (527.24432373,966.293003067), test loss: 948.386730957\n",
      "Iteration: 55500, train loss(batch, sum): (846.438720703,963.29075803), test loss: 949.667915649\n",
      "Iteration: 56000, train loss(batch, sum): (685.436767578,960.31438654), test loss: 948.492908325\n",
      "Iteration: 56500, train loss(batch, sum): (447.124755859,957.356689995), test loss: 947.157679443\n",
      "Iteration: 57000, train loss(batch, sum): (666.492553711,954.449612588), test loss: 946.348997803\n",
      "Iteration: 57500, train loss(batch, sum): (557.117004395,951.560125662), test loss: 944.741273193\n",
      "Iteration: 58000, train loss(batch, sum): (640.717407227,948.694548318), test loss: 942.74130188\n",
      "Iteration: 58500, train loss(batch, sum): (597.83013916,945.875664357), test loss: 943.203938599\n",
      "Iteration: 59000, train loss(batch, sum): (559.039367676,943.07003702), test loss: 940.262425537\n",
      "Iteration: 59500, train loss(batch, sum): (572.094482422,940.288345292), test loss: 942.196149902\n",
      "Iteration: 60000, train loss(batch, sum): (719.514953613,937.553412536), test loss: 943.332324219\n",
      "Iteration: 60500, train loss(batch, sum): (599.395996094,934.827678138), test loss: 937.533352051\n",
      "Iteration: 61000, train loss(batch, sum): (575.489868164,932.124006971), test loss: 941.664486084\n",
      "Iteration: 61500, train loss(batch, sum): (545.147216797,929.465880525), test loss: 941.945979614\n",
      "Iteration: 62000, train loss(batch, sum): (500.841033936,926.818629123), test loss: 936.886939697\n",
      "Iteration: 62500, train loss(batch, sum): (667.476013184,924.193350961), test loss: 938.121295776\n",
      "Iteration: 63000, train loss(batch, sum): (535.486022949,921.606343377), test loss: 939.950318604\n",
      "Iteration: 63500, train loss(batch, sum): (559.929870605,919.029682429), test loss: 935.353933105\n",
      "Iteration: 64000, train loss(batch, sum): (631.026489258,916.479633508), test loss: 934.048459473\n",
      "Iteration: 64500, train loss(batch, sum): (677.047058105,913.962522719), test loss: 933.438128662\n",
      "Iteration: 65000, train loss(batch, sum): (615.909912109,911.458923235), test loss: 933.752024536\n",
      "Iteration: 65500, train loss(batch, sum): (476.701477051,908.976465164), test loss: 934.140895386\n",
      "Iteration: 66000, train loss(batch, sum): (652.311096191,906.527394181), test loss: 930.978388672\n",
      "Iteration: 66500, train loss(batch, sum): (585.341491699,904.088626534), test loss: 933.68897583\n",
      "Iteration: 67000, train loss(batch, sum): (662.68737793,901.673356994), test loss: 934.077301636\n",
      "Iteration: 67500, train loss(batch, sum): (526.974121094,899.284720851), test loss: 929.920657349\n",
      "Iteration: 68000, train loss(batch, sum): (470.07913208,896.90964948), test loss: 930.368370972\n",
      "Iteration: 68500, train loss(batch, sum): (788.898254395,894.563315327), test loss: 932.62117981\n",
      "Iteration: 69000, train loss(batch, sum): (624.902770996,892.233132108), test loss: 928.057501221\n",
      "Iteration: 69500, train loss(batch, sum): (397.573760986,889.913413112), test loss: 927.722197876\n",
      "Iteration: 70000, train loss(batch, sum): (620.454284668,887.629579878), test loss: 929.199055176\n",
      "Iteration: 70500, train loss(batch, sum): (514.807067871,885.355539401), test loss: 926.929760132\n",
      "Iteration: 71000, train loss(batch, sum): (594.392089844,883.096480734), test loss: 926.712895508\n",
      "Iteration: 71500, train loss(batch, sum): (525.922607422,880.870282123), test loss: 926.953775635\n",
      "Iteration: 72000, train loss(batch, sum): (513.094482422,878.651677066), test loss: 928.131365967\n",
      "Iteration: 72500, train loss(batch, sum): (562.437438965,876.448576027), test loss: 927.452446289\n",
      "Iteration: 73000, train loss(batch, sum): (638.553222656,874.278077036), test loss: 926.091427612\n",
      "Iteration: 73500, train loss(batch, sum): (507.637664795,872.112590524), test loss: 926.033222046\n",
      "Iteration: 74000, train loss(batch, sum): (515.248168945,869.961008665), test loss: 927.750391235\n",
      "Iteration: 74500, train loss(batch, sum): (495.292205811,867.842492691), test loss: 925.334613647\n",
      "Iteration: 75000, train loss(batch, sum): (467.607879639,865.730331972), test loss: 922.690527954\n",
      "Iteration: 75500, train loss(batch, sum): (650.523681641,863.632386885), test loss: 923.529439087\n",
      "Iteration: 76000, train loss(batch, sum): (470.442779541,861.561363214), test loss: 924.038187256\n",
      "Iteration: 76500, train loss(batch, sum): (537.851013184,859.497206378), test loss: 922.022270508\n",
      "Iteration: 77000, train loss(batch, sum): (576.821350098,857.450060037), test loss: 921.632393188\n",
      "Iteration: 77500, train loss(batch, sum): (617.197814941,855.427525338), test loss: 924.243610229\n",
      "Iteration: 78000, train loss(batch, sum): (557.936706543,853.413408844), test loss: 921.79996582\n",
      "Iteration: 78500, train loss(batch, sum): (417.138000488,851.41308395), test loss: 921.311019287\n",
      "Iteration: 79000, train loss(batch, sum): (592.930175781,849.437357434), test loss: 922.137678223\n",
      "Iteration: 79500, train loss(batch, sum): (523.32824707,847.467959653), test loss: 922.218718262\n",
      "Hype # hyp1, Fold # 7\n",
      "<caffe.net_spec.NetSpec object at 0x7f4e83bac1d0>\n",
      "<caffe.net_spec.NetSpec object at 0x7f4e83ba9b10>\n",
      "Iteration: 0, train loss(batch, sum): (11275.4570312,inf), test loss: 11232.844834\n",
      "Iteration: 500, train loss(batch, sum): (2278.17114258,3080.16836621), test loss: 2199.12500488\n",
      "Iteration: 1000, train loss(batch, sum): (2093.82177734,2617.44307227), test loss: 2109.96711182\n",
      "Iteration: 1500, train loss(batch, sum): (1825.94934082,2423.94727686), test loss: 1990.90596069\n",
      "Iteration: 2000, train loss(batch, sum): (1702.64233398,2293.88667133), test loss: 1878.09170288\n",
      "Iteration: 2500, train loss(batch, sum): (1940.63928223,2196.40676685), test loss: 1785.85951904\n",
      "Iteration: 3000, train loss(batch, sum): (1632.4407959,2116.62139376), test loss: 1715.96525513\n",
      "Iteration: 3500, train loss(batch, sum): (1784.64208984,2050.1612371), test loss: 1660.13206177\n",
      "Iteration: 4000, train loss(batch, sum): (1562.08532715,1993.5074444), test loss: 1602.30667358\n",
      "Iteration: 4500, train loss(batch, sum): (1642.99609375,1943.99611705), test loss: 1564.70942261\n",
      "Iteration: 5000, train loss(batch, sum): (1443.79199219,1899.98444885), test loss: 1533.16971191\n",
      "Iteration: 5500, train loss(batch, sum): (1590.22070312,1860.41011741), test loss: 1498.09433594\n",
      "Iteration: 6000, train loss(batch, sum): (1539.4296875,1824.18640263), test loss: 1462.81723511\n",
      "Iteration: 6500, train loss(batch, sum): (1392.83654785,1790.65807927), test loss: 1432.0819458\n",
      "Iteration: 7000, train loss(batch, sum): (1336.4621582,1759.51161204), test loss: 1404.47756226\n",
      "Iteration: 7500, train loss(batch, sum): (1322.875,1730.64520345), test loss: 1384.15539063\n",
      "Iteration: 8000, train loss(batch, sum): (1220.84228516,1703.69606551), test loss: 1363.05743652\n",
      "Iteration: 8500, train loss(batch, sum): (1220.76745605,1678.53269007), test loss: 1344.33119385\n",
      "Iteration: 9000, train loss(batch, sum): (1386.83410645,1655.15476259), test loss: 1327.22010742\n",
      "Iteration: 9500, train loss(batch, sum): (1165.07141113,1633.03001877), test loss: 1311.47734253\n",
      "Iteration: 10000, train loss(batch, sum): (1403.57580566,1612.1075137), test loss: 1298.5278186\n",
      "Iteration: 10500, train loss(batch, sum): (1115.7520752,1592.33507478), test loss: 1282.94987305\n",
      "Iteration: 11000, train loss(batch, sum): (1224.61120605,1573.47151809), test loss: 1266.90126587\n",
      "Iteration: 11500, train loss(batch, sum): (1149.79162598,1555.45597629), test loss: 1251.15433838\n",
      "Iteration: 12000, train loss(batch, sum): (1276.19750977,1538.23226443), test loss: 1240.19647705\n",
      "Iteration: 12500, train loss(batch, sum): (1176.02172852,1521.77422272), test loss: 1230.39757324\n",
      "Iteration: 13000, train loss(batch, sum): (1017.02978516,1505.95678787), test loss: 1219.49468506\n",
      "Iteration: 13500, train loss(batch, sum): (1054.06115723,1490.76164375), test loss: 1209.14683594\n",
      "Iteration: 14000, train loss(batch, sum): (1067.55749512,1476.2268501), test loss: 1196.83186157\n",
      "Iteration: 14500, train loss(batch, sum): (1111.15905762,1462.15525075), test loss: 1185.04123901\n",
      "Iteration: 15000, train loss(batch, sum): (980.923706055,1448.56229441), test loss: 1180.29763672\n",
      "Iteration: 15500, train loss(batch, sum): (1097.75683594,1435.53438619), test loss: 1174.39946045\n",
      "Iteration: 16000, train loss(batch, sum): (893.625854492,1422.86824147), test loss: 1161.98772644\n",
      "Iteration: 16500, train loss(batch, sum): (1095.9765625,1410.60324482), test loss: 1152.25374329\n",
      "Iteration: 17000, train loss(batch, sum): (911.678649902,1398.79146307), test loss: 1144.75158997\n",
      "Iteration: 17500, train loss(batch, sum): (1051.32849121,1387.32105484), test loss: 1140.5945929\n",
      "Iteration: 18000, train loss(batch, sum): (1057.02404785,1376.19070513), test loss: 1134.25561096\n",
      "Iteration: 18500, train loss(batch, sum): (1017.6998291,1365.39003219), test loss: 1126.81112244\n",
      "Iteration: 19000, train loss(batch, sum): (871.060668945,1354.93725778), test loss: 1118.33013\n",
      "Iteration: 19500, train loss(batch, sum): (943.439086914,1344.76184516), test loss: 1111.31552795\n",
      "Iteration: 20000, train loss(batch, sum): (882.161804199,1334.86759266), test loss: 1111.06745483\n",
      "Iteration: 20500, train loss(batch, sum): (839.938415527,1325.29679561), test loss: 1106.43871765\n",
      "Iteration: 21000, train loss(batch, sum): (990.01184082,1315.93716436), test loss: 1097.03406494\n",
      "Iteration: 21500, train loss(batch, sum): (901.457336426,1306.80921983), test loss: 1089.22777405\n",
      "Iteration: 22000, train loss(batch, sum): (937.851928711,1297.97998717), test loss: 1087.55346741\n",
      "Iteration: 22500, train loss(batch, sum): (809.063903809,1289.33069076), test loss: 1084.45870422\n",
      "Iteration: 23000, train loss(batch, sum): (854.558654785,1280.87380614), test loss: 1081.36431152\n",
      "Iteration: 23500, train loss(batch, sum): (906.849853516,1272.67966088), test loss: 1073.17574646\n",
      "Iteration: 24000, train loss(batch, sum): (865.182067871,1264.65227701), test loss: 1066.11145142\n",
      "Iteration: 24500, train loss(batch, sum): (924.324279785,1256.81149708), test loss: 1062.31815918\n",
      "Iteration: 25000, train loss(batch, sum): (787.418151855,1249.14224198), test loss: 1063.90781982\n",
      "Iteration: 25500, train loss(batch, sum): (808.334838867,1241.67355669), test loss: 1059.7111438\n",
      "Iteration: 26000, train loss(batch, sum): (863.640380859,1234.35032816), test loss: 1052.36701294\n",
      "Iteration: 26500, train loss(batch, sum): (791.919311523,1227.18067414), test loss: 1047.70440247\n",
      "Iteration: 27000, train loss(batch, sum): (628.353942871,1220.19957831), test loss: 1045.8922052\n",
      "Iteration: 27500, train loss(batch, sum): (797.616516113,1213.33428604), test loss: 1042.89716248\n",
      "Iteration: 28000, train loss(batch, sum): (902.609130859,1206.60110362), test loss: 1039.52611206\n",
      "Iteration: 28500, train loss(batch, sum): (861.373779297,1200.04436882), test loss: 1037.11297852\n",
      "Iteration: 29000, train loss(batch, sum): (861.886779785,1193.59145398), test loss: 1029.68800232\n",
      "Iteration: 29500, train loss(batch, sum): (671.41619873,1187.24314065), test loss: 1029.35611572\n",
      "Iteration: 30000, train loss(batch, sum): (967.500915527,1181.06463698), test loss: 1028.16721436\n",
      "Iteration: 30500, train loss(batch, sum): (779.17388916,1174.97721155), test loss: 1024.50660645\n",
      "Iteration: 31000, train loss(batch, sum): (725.954589844,1169.00175261), test loss: 1019.59409119\n",
      "Iteration: 31500, train loss(batch, sum): (652.641662598,1163.13686923), test loss: 1016.93876221\n",
      "Iteration: 32000, train loss(batch, sum): (732.12878418,1157.39819055), test loss: 1015.23689941\n",
      "Iteration: 32500, train loss(batch, sum): (756.8203125,1151.74496362), test loss: 1014.23008728\n",
      "Iteration: 33000, train loss(batch, sum): (857.255126953,1146.19483551), test loss: 1011.86674744\n",
      "Iteration: 33500, train loss(batch, sum): (675.74432373,1140.76467265), test loss: 1008.57272827\n",
      "Iteration: 34000, train loss(batch, sum): (704.368164062,1135.4030645), test loss: 1002.80440735\n",
      "Iteration: 34500, train loss(batch, sum): (780.05078125,1130.12915594), test loss: 1001.09632324\n",
      "Iteration: 35000, train loss(batch, sum): (737.851379395,1124.97378203), test loss: 1003.39041199\n",
      "Iteration: 35500, train loss(batch, sum): (868.801025391,1119.88505745), test loss: 999.207008057\n",
      "Iteration: 36000, train loss(batch, sum): (583.978210449,1114.86086315), test loss: 997.408181763\n",
      "Iteration: 36500, train loss(batch, sum): (952.827209473,1109.9529367), test loss: 994.13661377\n",
      "Iteration: 37000, train loss(batch, sum): (691.269592285,1105.10561723), test loss: 990.515020752\n",
      "Iteration: 37500, train loss(batch, sum): (786.592407227,1100.33067702), test loss: 991.386618042\n",
      "Iteration: 38000, train loss(batch, sum): (692.588867188,1095.62848406), test loss: 990.663022461\n",
      "Iteration: 38500, train loss(batch, sum): (606.541748047,1091.01116071), test loss: 988.762425537\n",
      "Iteration: 39000, train loss(batch, sum): (684.592956543,1086.45409235), test loss: 984.241618652\n",
      "Iteration: 39500, train loss(batch, sum): (778.024719238,1081.96404968), test loss: 981.21048584\n",
      "Iteration: 40000, train loss(batch, sum): (697.685668945,1077.56018166), test loss: 983.386789551\n",
      "Iteration: 40500, train loss(batch, sum): (708.576660156,1073.19850735), test loss: 980.352629395\n",
      "Iteration: 41000, train loss(batch, sum): (647.988769531,1068.89226636), test loss: 978.130223389\n",
      "Iteration: 41500, train loss(batch, sum): (743.797424316,1064.67571569), test loss: 977.848619385\n",
      "Iteration: 42000, train loss(batch, sum): (865.313964844,1060.50081304), test loss: 972.641604004\n",
      "Iteration: 42500, train loss(batch, sum): (579.758666992,1056.37191398), test loss: 974.728773193\n",
      "Iteration: 43000, train loss(batch, sum): (866.170227051,1052.31989827), test loss: 973.550438843\n",
      "Iteration: 43500, train loss(batch, sum): (653.173278809,1048.31404383), test loss: 970.509173584\n",
      "Iteration: 44000, train loss(batch, sum): (740.074523926,1044.35765143), test loss: 969.105037231\n",
      "Iteration: 44500, train loss(batch, sum): (823.732116699,1040.45091602), test loss: 965.90218689\n",
      "Iteration: 45000, train loss(batch, sum): (644.282714844,1036.60477598), test loss: 968.897467041\n",
      "Iteration: 45500, train loss(batch, sum): (582.443115234,1032.79896773), test loss: 965.841499634\n",
      "Iteration: 46000, train loss(batch, sum): (733.886352539,1029.04095819), test loss: 964.098042603\n",
      "Iteration: 46500, train loss(batch, sum): (643.436401367,1025.34817327), test loss: 964.327009277\n",
      "Iteration: 47000, train loss(batch, sum): (642.854614258,1021.68233566), test loss: 959.798123169\n",
      "Iteration: 47500, train loss(batch, sum): (610.479003906,1018.0563997), test loss: 960.187275391\n",
      "Iteration: 48000, train loss(batch, sum): (740.551696777,1014.49662447), test loss: 959.981483154\n",
      "Iteration: 48500, train loss(batch, sum): (702.449951172,1010.96413471), test loss: 957.945061035\n",
      "Iteration: 49000, train loss(batch, sum): (607.023010254,1007.46875984), test loss: 957.523161011\n",
      "Iteration: 49500, train loss(batch, sum): (758.55267334,1004.02428068), test loss: 955.196716309\n",
      "Iteration: 50000, train loss(batch, sum): (709.778076172,1000.62019519), test loss: 953.955770874\n",
      "Iteration: 50500, train loss(batch, sum): (664.545227051,997.246838642), test loss: 952.626435547\n",
      "Iteration: 51000, train loss(batch, sum): (808.383117676,993.912400881), test loss: 952.90069397\n",
      "Iteration: 51500, train loss(batch, sum): (579.568237305,990.624331135), test loss: 954.069550171\n",
      "Iteration: 52000, train loss(batch, sum): (516.382446289,987.36498285), test loss: 949.713748169\n",
      "Iteration: 52500, train loss(batch, sum): (758.782043457,984.139816309), test loss: 949.23284729\n",
      "Iteration: 53000, train loss(batch, sum): (607.795166016,980.966982626), test loss: 949.232359619\n",
      "Iteration: 53500, train loss(batch, sum): (659.41998291,977.813229883), test loss: 947.598756714\n",
      "Iteration: 54000, train loss(batch, sum): (640.171325684,974.687753979), test loss: 947.640872192\n",
      "Iteration: 54500, train loss(batch, sum): (720.06036377,971.61444141), test loss: 945.002668457\n",
      "Iteration: 55000, train loss(batch, sum): (534.434326172,968.560472806), test loss: 944.77730896\n",
      "Iteration: 55500, train loss(batch, sum): (619.97253418,965.536563914), test loss: 943.475151367\n",
      "Iteration: 56000, train loss(batch, sum): (656.592468262,962.549508053), test loss: 943.702843018\n",
      "Iteration: 56500, train loss(batch, sum): (688.63079834,959.596333012), test loss: 942.827480469\n",
      "Iteration: 57000, train loss(batch, sum): (663.57043457,956.66552655), test loss: 939.032321777\n",
      "Iteration: 57500, train loss(batch, sum): (686.15637207,953.762137797), test loss: 940.344881592\n",
      "Iteration: 58000, train loss(batch, sum): (571.307189941,950.89994805), test loss: 940.784698486\n",
      "Iteration: 58500, train loss(batch, sum): (505.078887939,948.058302016), test loss: 939.078239136\n",
      "Iteration: 59000, train loss(batch, sum): (645.185424805,945.240035294), test loss: 939.767993164\n",
      "Iteration: 59500, train loss(batch, sum): (559.889709473,942.467589924), test loss: 937.442666626\n",
      "Iteration: 60000, train loss(batch, sum): (571.063659668,939.708100869), test loss: 935.364126587\n",
      "Iteration: 60500, train loss(batch, sum): (686.798034668,936.97092919), test loss: 933.134752197\n",
      "Iteration: 61000, train loss(batch, sum): (620.199645996,934.273991156), test loss: 936.602503662\n",
      "Iteration: 61500, train loss(batch, sum): (621.920043945,931.594864787), test loss: 936.275263672\n",
      "Iteration: 62000, train loss(batch, sum): (610.386779785,928.937649057), test loss: 932.391342163\n",
      "Iteration: 62500, train loss(batch, sum): (746.917175293,926.309013652), test loss: 931.258699341\n",
      "Iteration: 63000, train loss(batch, sum): (688.484802246,923.708259458), test loss: 930.022071533\n",
      "Iteration: 63500, train loss(batch, sum): (589.325012207,921.126416095), test loss: 930.919074707\n",
      "Iteration: 64000, train loss(batch, sum): (669.52166748,918.564603943), test loss: 934.260935059\n",
      "Iteration: 64500, train loss(batch, sum): (547.90447998,916.039064815), test loss: 930.801003418\n",
      "Iteration: 65000, train loss(batch, sum): (513.035400391,913.528633775), test loss: 926.417254639\n",
      "Iteration: 65500, train loss(batch, sum): (569.718994141,911.03472006), test loss: 926.82289978\n",
      "Iteration: 66000, train loss(batch, sum): (592.922058105,908.580977917), test loss: 930.565315552\n",
      "Iteration: 66500, train loss(batch, sum): (555.193847656,906.1360076), test loss: 930.539078369\n",
      "Iteration: 67000, train loss(batch, sum): (615.087341309,903.708747978), test loss: 927.073199463\n",
      "Iteration: 67500, train loss(batch, sum): (580.855163574,901.316528847), test loss: 925.350296631\n",
      "Iteration: 68000, train loss(batch, sum): (694.642883301,898.938323936), test loss: 923.99418396\n",
      "Iteration: 68500, train loss(batch, sum): (508.252258301,896.575559368), test loss: 926.116993408\n",
      "Iteration: 69000, train loss(batch, sum): (656.909362793,894.237659883), test loss: 928.18550293\n",
      "Iteration: 69500, train loss(batch, sum): (704.389892578,891.923524363), test loss: 924.374526367\n",
      "Iteration: 70000, train loss(batch, sum): (552.619689941,889.623841861), test loss: 920.732419434\n",
      "Iteration: 70500, train loss(batch, sum): (588.181945801,887.340406994), test loss: 922.378982544\n",
      "Iteration: 71000, train loss(batch, sum): (444.993652344,885.087952652), test loss: 926.388280029\n",
      "Iteration: 71500, train loss(batch, sum): (462.13394165,882.847939399), test loss: 924.928164062\n",
      "Iteration: 72000, train loss(batch, sum): (506.81362915,880.619466935), test loss: 922.384613647\n",
      "Iteration: 72500, train loss(batch, sum): (632.916137695,878.427017759), test loss: 921.302165527\n",
      "Iteration: 73000, train loss(batch, sum): (562.467407227,876.240904554), test loss: 919.556538696\n",
      "Iteration: 73500, train loss(batch, sum): (573.29498291,874.06741923), test loss: 923.327717285\n",
      "Iteration: 74000, train loss(batch, sum): (607.499084473,871.92533755), test loss: 924.915532227\n",
      "Iteration: 74500, train loss(batch, sum): (559.486633301,869.79321344), test loss: 920.132775879\n",
      "Iteration: 75000, train loss(batch, sum): (425.707977295,867.675990129), test loss: 917.58977478\n",
      "Iteration: 75500, train loss(batch, sum): (639.459899902,865.576702674), test loss: 918.74394104\n",
      "Iteration: 76000, train loss(batch, sum): (647.087463379,863.499131345), test loss: 922.724700317\n",
      "Iteration: 76500, train loss(batch, sum): (671.29876709,861.434942826), test loss: 920.448025513\n",
      "Iteration: 77000, train loss(batch, sum): (463.113800049,859.380952967), test loss: 920.217923584\n",
      "Iteration: 77500, train loss(batch, sum): (472.466827393,857.356351031), test loss: 918.686262817\n",
      "Iteration: 78000, train loss(batch, sum): (481.010528564,855.340519397), test loss: 915.997109375\n",
      "Iteration: 78500, train loss(batch, sum): (506.117279053,853.334276292), test loss: 921.304446411\n",
      "Iteration: 79000, train loss(batch, sum): (587.877258301,851.357825391), test loss: 921.324976807\n",
      "Iteration: 79500, train loss(batch, sum): (512.304992676,849.387190704), test loss: 917.221991577\n",
      "Hype # hyp1, Fold # 8\n",
      "<caffe.net_spec.NetSpec object at 0x7f4e83ba0c90>\n",
      "<caffe.net_spec.NetSpec object at 0x7f4e79701b10>\n",
      "Iteration: 0, train loss(batch, sum): (11276.4355469,inf), test loss: 11232.8781934\n",
      "Iteration: 500, train loss(batch, sum): (2050.51147461,3081.86807983), test loss: 2196.49610352\n",
      "Iteration: 1000, train loss(batch, sum): (1857.06481934,2616.37477356), test loss: 2122.96475952\n",
      "Iteration: 1500, train loss(batch, sum): (2228.37719727,2422.72648958), test loss: 1989.74819702\n",
      "Iteration: 2000, train loss(batch, sum): (1689.98620605,2291.98232098), test loss: 1881.2125293\n",
      "Iteration: 2500, train loss(batch, sum): (1770.84985352,2193.66116558), test loss: 1788.90419067\n",
      "Iteration: 3000, train loss(batch, sum): (1874.41809082,2113.92381925), test loss: 1710.55655273\n",
      "Iteration: 3500, train loss(batch, sum): (1613.06555176,2047.37370553), test loss: 1654.88506592\n",
      "Iteration: 4000, train loss(batch, sum): (1718.91296387,1990.96488129), test loss: 1612.26797607\n",
      "Iteration: 4500, train loss(batch, sum): (1662.14038086,1941.88947138), test loss: 1574.10772827\n",
      "Iteration: 5000, train loss(batch, sum): (1306.75854492,1898.23815366), test loss: 1531.37854736\n",
      "Iteration: 5500, train loss(batch, sum): (1477.23010254,1858.57275557), test loss: 1498.93728027\n",
      "Iteration: 6000, train loss(batch, sum): (1550.45568848,1822.37121574), test loss: 1461.55015015\n",
      "Iteration: 6500, train loss(batch, sum): (1319.42932129,1788.63384503), test loss: 1429.42748779\n",
      "Iteration: 7000, train loss(batch, sum): (1407.19995117,1757.55023537), test loss: 1402.5999231\n",
      "Iteration: 7500, train loss(batch, sum): (1344.10864258,1728.64640439), test loss: 1379.74426758\n",
      "Iteration: 8000, train loss(batch, sum): (1340.85192871,1701.80963312), test loss: 1362.63836914\n",
      "Iteration: 8500, train loss(batch, sum): (1272.19238281,1676.85649567), test loss: 1348.05091919\n",
      "Iteration: 9000, train loss(batch, sum): (1346.39428711,1653.47949574), test loss: 1330.47192871\n",
      "Iteration: 9500, train loss(batch, sum): (1407.41540527,1631.5989781), test loss: 1314.57038696\n",
      "Iteration: 10000, train loss(batch, sum): (1179.83886719,1610.83084733), test loss: 1298.65996216\n",
      "Iteration: 10500, train loss(batch, sum): (1229.37475586,1591.23724711), test loss: 1285.97397705\n",
      "Iteration: 11000, train loss(batch, sum): (1016.390625,1572.50715969), test loss: 1273.52341675\n",
      "Iteration: 11500, train loss(batch, sum): (1078.52453613,1554.69944803), test loss: 1259.59146362\n",
      "Iteration: 12000, train loss(batch, sum): (973.155151367,1537.60156767), test loss: 1245.69299316\n",
      "Iteration: 12500, train loss(batch, sum): (1039.63525391,1521.31273114), test loss: 1235.48741943\n",
      "Iteration: 13000, train loss(batch, sum): (1116.27807617,1505.57632083), test loss: 1222.24811523\n",
      "Iteration: 13500, train loss(batch, sum): (1153.97241211,1490.51394793), test loss: 1212.74229858\n",
      "Iteration: 14000, train loss(batch, sum): (1015.82409668,1475.97489121), test loss: 1199.86234985\n",
      "Iteration: 14500, train loss(batch, sum): (1198.82629395,1461.9840648), test loss: 1191.04909485\n",
      "Iteration: 15000, train loss(batch, sum): (1177.02001953,1448.4803573), test loss: 1183.32390869\n",
      "Iteration: 15500, train loss(batch, sum): (873.737365723,1435.40326055), test loss: 1172.03149414\n",
      "Iteration: 16000, train loss(batch, sum): (948.352600098,1422.81688313), test loss: 1164.97536255\n",
      "Iteration: 16500, train loss(batch, sum): (1021.08709717,1410.5858036), test loss: 1155.4811853\n",
      "Iteration: 17000, train loss(batch, sum): (861.342407227,1398.79220319), test loss: 1149.99528564\n",
      "Iteration: 17500, train loss(batch, sum): (1021.07403564,1387.33054944), test loss: 1141.33020874\n",
      "Iteration: 18000, train loss(batch, sum): (874.543579102,1376.25879994), test loss: 1136.57507874\n",
      "Iteration: 18500, train loss(batch, sum): (1051.27514648,1365.49054104), test loss: 1130.35124512\n",
      "Iteration: 19000, train loss(batch, sum): (756.056640625,1355.08836011), test loss: 1123.97204346\n",
      "Iteration: 19500, train loss(batch, sum): (1029.32531738,1344.9454406), test loss: 1115.4197113\n",
      "Iteration: 20000, train loss(batch, sum): (930.31262207,1335.11288242), test loss: 1113.04259094\n",
      "Iteration: 20500, train loss(batch, sum): (918.821533203,1325.51828707), test loss: 1102.89715576\n",
      "Iteration: 21000, train loss(batch, sum): (903.785461426,1316.21283995), test loss: 1100.38704224\n",
      "Iteration: 21500, train loss(batch, sum): (928.994506836,1307.14894791), test loss: 1095.69109924\n",
      "Iteration: 22000, train loss(batch, sum): (1008.35198975,1298.30447599), test loss: 1091.98696411\n",
      "Iteration: 22500, train loss(batch, sum): (859.046020508,1289.72914784), test loss: 1084.42047241\n",
      "Iteration: 23000, train loss(batch, sum): (870.049987793,1281.32326764), test loss: 1082.92287354\n",
      "Iteration: 23500, train loss(batch, sum): (910.657531738,1273.17395091), test loss: 1076.9393396\n",
      "Iteration: 24000, train loss(batch, sum): (856.753051758,1265.18589305), test loss: 1073.66926331\n",
      "Iteration: 24500, train loss(batch, sum): (922.992858887,1257.4242399), test loss: 1070.25300659\n",
      "Iteration: 25000, train loss(batch, sum): (836.391357422,1249.81836022), test loss: 1065.5458075\n",
      "Iteration: 25500, train loss(batch, sum): (968.587524414,1242.42168858), test loss: 1064.06094604\n",
      "Iteration: 26000, train loss(batch, sum): (772.25012207,1235.15605072), test loss: 1056.45389221\n",
      "Iteration: 26500, train loss(batch, sum): (937.893920898,1228.08194847), test loss: 1057.66957092\n",
      "Iteration: 27000, train loss(batch, sum): (810.56842041,1221.12769196), test loss: 1050.38576111\n",
      "Iteration: 27500, train loss(batch, sum): (905.500610352,1214.34313242), test loss: 1048.2577771\n",
      "Iteration: 28000, train loss(batch, sum): (768.334411621,1207.68725481), test loss: 1043.82069885\n",
      "Iteration: 28500, train loss(batch, sum): (925.115966797,1201.1720979), test loss: 1042.98139832\n",
      "Iteration: 29000, train loss(batch, sum): (794.253662109,1194.79324532), test loss: 1036.97630493\n",
      "Iteration: 29500, train loss(batch, sum): (863.937194824,1188.52028532), test loss: 1035.27943359\n",
      "Iteration: 30000, train loss(batch, sum): (795.181640625,1182.3955752), test loss: 1031.52596069\n",
      "Iteration: 30500, train loss(batch, sum): (855.959533691,1176.36352384), test loss: 1028.82325317\n",
      "Iteration: 31000, train loss(batch, sum): (936.096069336,1170.46880396), test loss: 1025.02146118\n",
      "Iteration: 31500, train loss(batch, sum): (839.271057129,1164.66032332), test loss: 1023.50022583\n",
      "Iteration: 32000, train loss(batch, sum): (739.039794922,1158.97643729), test loss: 1022.83949646\n",
      "Iteration: 32500, train loss(batch, sum): (757.588989258,1153.38010741), test loss: 1019.53635803\n",
      "Iteration: 33000, train loss(batch, sum): (726.161499023,1147.89945884), test loss: 1017.77085144\n",
      "Iteration: 33500, train loss(batch, sum): (827.927246094,1142.4923511), test loss: 1013.3308905\n",
      "Iteration: 34000, train loss(batch, sum): (863.678527832,1137.19002297), test loss: 1011.11690613\n",
      "Iteration: 34500, train loss(batch, sum): (756.778137207,1131.96589614), test loss: 1007.86420837\n",
      "Iteration: 35000, train loss(batch, sum): (872.332214355,1126.83400336), test loss: 1005.4783783\n",
      "Iteration: 35500, train loss(batch, sum): (708.45123291,1121.78707254), test loss: 1002.65710266\n",
      "Iteration: 36000, train loss(batch, sum): (904.631286621,1116.81667795), test loss: 1001.24019104\n",
      "Iteration: 36500, train loss(batch, sum): (751.967590332,1111.93742462), test loss: 998.442559814\n",
      "Iteration: 37000, train loss(batch, sum): (799.718322754,1107.11783168), test loss: 997.460698242\n",
      "Iteration: 37500, train loss(batch, sum): (776.281494141,1102.39064431), test loss: 994.416321411\n",
      "Iteration: 38000, train loss(batch, sum): (684.297241211,1097.71458419), test loss: 994.922823486\n",
      "Iteration: 38500, train loss(batch, sum): (686.133605957,1093.12476777), test loss: 993.627184448\n",
      "Iteration: 39000, train loss(batch, sum): (550.117126465,1088.59239837), test loss: 992.654464111\n",
      "Iteration: 39500, train loss(batch, sum): (696.912597656,1084.14369611), test loss: 989.216305542\n",
      "Iteration: 40000, train loss(batch, sum): (692.15411377,1079.74266641), test loss: 986.070100098\n",
      "Iteration: 40500, train loss(batch, sum): (702.348571777,1075.411746), test loss: 985.105812988\n",
      "Iteration: 41000, train loss(batch, sum): (712.586608887,1071.1327308), test loss: 982.31607605\n",
      "Iteration: 41500, train loss(batch, sum): (913.904602051,1066.9250261), test loss: 981.586968994\n",
      "Iteration: 42000, train loss(batch, sum): (700.448120117,1062.76758062), test loss: 978.35894043\n",
      "Iteration: 42500, train loss(batch, sum): (605.422302246,1058.6585061), test loss: 979.496212769\n",
      "Iteration: 43000, train loss(batch, sum): (851.279602051,1054.62602141), test loss: 975.849910889\n",
      "Iteration: 43500, train loss(batch, sum): (657.189147949,1050.62545184), test loss: 975.05215271\n",
      "Iteration: 44000, train loss(batch, sum): (666.899902344,1046.69497257), test loss: 973.228932495\n",
      "Iteration: 44500, train loss(batch, sum): (604.924682617,1042.80247523), test loss: 973.194190063\n",
      "Iteration: 45000, train loss(batch, sum): (633.052246094,1038.96814145), test loss: 972.890715332\n",
      "Iteration: 45500, train loss(batch, sum): (737.712646484,1035.18059676), test loss: 969.185830078\n",
      "Iteration: 46000, train loss(batch, sum): (629.174072266,1031.44716097), test loss: 969.674622803\n",
      "Iteration: 46500, train loss(batch, sum): (590.162780762,1027.75377451), test loss: 966.596766968\n",
      "Iteration: 47000, train loss(batch, sum): (646.573059082,1024.10760714), test loss: 967.321072388\n",
      "Iteration: 47500, train loss(batch, sum): (801.635925293,1020.50189351), test loss: 963.404968262\n",
      "Iteration: 48000, train loss(batch, sum): (632.35144043,1016.94050911), test loss: 963.896372681\n",
      "Iteration: 48500, train loss(batch, sum): (630.094604492,1013.42073502), test loss: 962.009760132\n",
      "Iteration: 49000, train loss(batch, sum): (788.786437988,1009.93883857), test loss: 960.846991577\n",
      "Iteration: 49500, train loss(batch, sum): (640.102905273,1006.50842037), test loss: 959.467148437\n",
      "Iteration: 50000, train loss(batch, sum): (582.624633789,1003.10243398), test loss: 959.355673828\n",
      "Iteration: 50500, train loss(batch, sum): (658.661193848,999.75362634), test loss: 957.388634033\n",
      "Iteration: 51000, train loss(batch, sum): (616.749938965,996.427824972), test loss: 956.26869812\n",
      "Iteration: 51500, train loss(batch, sum): (782.588989258,993.152129488), test loss: 956.677039185\n",
      "Iteration: 52000, train loss(batch, sum): (670.497436523,989.905770322), test loss: 954.374123535\n",
      "Iteration: 52500, train loss(batch, sum): (578.806335449,986.702026533), test loss: 954.729887085\n",
      "Iteration: 53000, train loss(batch, sum): (635.054321289,983.529450027), test loss: 951.617904053\n",
      "Iteration: 53500, train loss(batch, sum): (728.677856445,980.395917962), test loss: 952.627786865\n",
      "Iteration: 54000, train loss(batch, sum): (674.305664062,977.289215865), test loss: 947.634837646\n",
      "Iteration: 54500, train loss(batch, sum): (639.423400879,974.217663153), test loss: 949.439710693\n",
      "Iteration: 55000, train loss(batch, sum): (650.758605957,971.176437406), test loss: 949.1175177\n",
      "Iteration: 55500, train loss(batch, sum): (767.047424316,968.165907899), test loss: 948.802893066\n",
      "Iteration: 56000, train loss(batch, sum): (630.533203125,965.192442483), test loss: 945.791447144\n",
      "Iteration: 56500, train loss(batch, sum): (683.213745117,962.240460138), test loss: 946.892290649\n",
      "Iteration: 57000, train loss(batch, sum): (696.027648926,959.331016072), test loss: 944.76659668\n",
      "Iteration: 57500, train loss(batch, sum): (750.969787598,956.440090004), test loss: 944.697072754\n",
      "Iteration: 58000, train loss(batch, sum): (707.331298828,953.588983657), test loss: 943.414938965\n",
      "Iteration: 58500, train loss(batch, sum): (575.641601562,950.757868658), test loss: 942.628388672\n",
      "Iteration: 59000, train loss(batch, sum): (567.254455566,947.961646855), test loss: 944.455485229\n",
      "Iteration: 59500, train loss(batch, sum): (502.98916626,945.190395232), test loss: 940.511536865\n",
      "Iteration: 60000, train loss(batch, sum): (569.60723877,942.449872618), test loss: 942.226530151\n",
      "Iteration: 60500, train loss(batch, sum): (646.738891602,939.731514395), test loss: 938.337704468\n",
      "Iteration: 61000, train loss(batch, sum): (660.469970703,937.040912588), test loss: 940.417362671\n",
      "Iteration: 61500, train loss(batch, sum): (571.520996094,934.37231646), test loss: 937.637340088\n",
      "Iteration: 62000, train loss(batch, sum): (768.142822266,931.732371124), test loss: 939.193236694\n",
      "Iteration: 62500, train loss(batch, sum): (746.024902344,929.117928368), test loss: 936.033376465\n",
      "Iteration: 63000, train loss(batch, sum): (540.347595215,926.519930895), test loss: 936.276640015\n",
      "Iteration: 63500, train loss(batch, sum): (465.699859619,923.956601008), test loss: 935.115570679\n",
      "Iteration: 64000, train loss(batch, sum): (691.519287109,921.409554562), test loss: 934.868431396\n",
      "Iteration: 64500, train loss(batch, sum): (540.036254883,918.893245034), test loss: 932.723233032\n",
      "Iteration: 65000, train loss(batch, sum): (623.089782715,916.39200803), test loss: 933.83859436\n",
      "Iteration: 65500, train loss(batch, sum): (483.749786377,913.917658663), test loss: 935.336925659\n",
      "Iteration: 66000, train loss(batch, sum): (450.041931152,911.464010598), test loss: 934.379254761\n",
      "Iteration: 66500, train loss(batch, sum): (491.44732666,909.038306327), test loss: 933.596546631\n",
      "Iteration: 67000, train loss(batch, sum): (652.499389648,906.631714159), test loss: 931.126234131\n",
      "Iteration: 67500, train loss(batch, sum): (600.729248047,904.243959275), test loss: 932.546522217\n",
      "Iteration: 68000, train loss(batch, sum): (528.443725586,901.874616593), test loss: 930.445059814\n",
      "Iteration: 68500, train loss(batch, sum): (582.758483887,899.529323864), test loss: 930.020081177\n",
      "Iteration: 69000, train loss(batch, sum): (577.824523926,897.203616513), test loss: 928.187753906\n",
      "Iteration: 69500, train loss(batch, sum): (588.881225586,894.890705299), test loss: 930.384257202\n",
      "Iteration: 70000, train loss(batch, sum): (672.059448242,892.610092537), test loss: 927.391329346\n",
      "Iteration: 70500, train loss(batch, sum): (522.321228027,890.336632966), test loss: 929.067626953\n",
      "Iteration: 71000, train loss(batch, sum): (517.592834473,888.093027359), test loss: 926.911315918\n",
      "Iteration: 71500, train loss(batch, sum): (537.946289062,885.860204721), test loss: 928.292271729\n",
      "Iteration: 72000, train loss(batch, sum): (519.333740234,883.649993565), test loss: 928.089578857\n",
      "Iteration: 72500, train loss(batch, sum): (535.832214355,881.457594877), test loss: 927.662853394\n",
      "Iteration: 73000, train loss(batch, sum): (660.377380371,879.287481767), test loss: 926.991420898\n",
      "Iteration: 73500, train loss(batch, sum): (426.835479736,877.130806995), test loss: 924.503356323\n",
      "Iteration: 74000, train loss(batch, sum): (539.423461914,874.992766951), test loss: 926.358988647\n",
      "Iteration: 74500, train loss(batch, sum): (572.448913574,872.870848146), test loss: 923.984863892\n",
      "Iteration: 75000, train loss(batch, sum): (583.958984375,870.765936882), test loss: 925.482410889\n",
      "Iteration: 75500, train loss(batch, sum): (478.442901611,868.67735059), test loss: 923.676104736\n",
      "Iteration: 76000, train loss(batch, sum): (660.16809082,866.603078028), test loss: 924.304225464\n",
      "Iteration: 76500, train loss(batch, sum): (515.403869629,864.550795018), test loss: 923.279442749\n",
      "Iteration: 77000, train loss(batch, sum): (567.098693848,862.506979527), test loss: 923.67355957\n",
      "Iteration: 77500, train loss(batch, sum): (530.854248047,860.487836199), test loss: 922.778502197\n",
      "Iteration: 78000, train loss(batch, sum): (531.975402832,858.476373442), test loss: 923.685742798\n",
      "Iteration: 78500, train loss(batch, sum): (715.817749023,856.48916476), test loss: 922.332553101\n",
      "Iteration: 79000, train loss(batch, sum): (609.448730469,854.510733984), test loss: 920.693776855\n",
      "Iteration: 79500, train loss(batch, sum): (456.374816895,852.551200924), test loss: 922.791300049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:54: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "#Generate API style txt files\n",
    "#ADNI1 ANN_CT is bad for 1,4,5,8 (all baseline models outperform for these fids)\n",
    "\n",
    "exp_name = 'Exp11'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'L_HC'\n",
    "Clinical_Scale = 'ADAS13'    \n",
    "start_fold = 6\n",
    "n_folds = 8\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "niter = 80000\n",
    "#node_sizes = {'L_ff1':5000,'R_ff1':5000,'L_ff2':500,'R_ff2':500,'ff1':500,'ff2':100,'ff3':50,'ff4':50}\n",
    "#node_sizes = {'En1':500,'code':100,'out':686}\n",
    "#L,R: 16086,16471\n",
    "pretrain = True\n",
    "load_pretrained_weights = False\n",
    "HC_snap = 4000\n",
    "CT_snap = 6000\n",
    "\n",
    "if Clinical_Scale == 'BOTH':\n",
    "    multi_task = True\n",
    "else:\n",
    "    multi_task = False\n",
    "\n",
    "#Hyperparameter Search\n",
    "#CT': 128, 'L_HC': 64, 'R_HC': 64, 'concat': 16\n",
    "#hyp1 and 2 work for ADNI1+2 MMSE\n",
    "# dr: Dropout rate, lr: learning rate of each modality, tr: loss-weight for each task\n",
    "hype_configs = {\n",
    "                'hyp1':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':300,'COMB_ff':25,'ADAS_ff':25,'MMSE_ff':25,'En1':8000,'En2':4000,'code':2000,'out':16086},\n",
    "                      'dr':{'HC':0.0,'CT':0,'COMB':0},'lr':{'HC':1,'CT':2},'tr':{'ADAS':1,'MMSE':2},\n",
    "                       'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-3}},\n",
    "#                 'hyp2':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':300,'COMB_ff':50,'ADAS_ff':25,'MMSE_ff':25,'En1':4000,'En2':200,'code':500,'out':16086},\n",
    "#                       'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':1,'CT':2},'tr':{'ADAS':1,'MMSE':2},\n",
    "#                        'solver_conf':{'base_lr':1e-5, 'wt_decay':1e-4}},    \n",
    "#                 'hyp3':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':100,'COMB_ff':25,'ADAS_ff':25,'MMSE_ff':25},\n",
    "#                       'dr':{'HC':0,'CT':0.5,'COMB':.25},'lr':{'HC':1,'CT':1},'tr':{'ADAS':1,'MMSE':2},\n",
    "#                        'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},\n",
    "#                 'hyp4':{'node_sizes':{'HC_L_ff':200,'HC_R_ff':200,'CT_ff':300,'COMB_ff':100,'ADAS_ff':25,'MMSE_ff':25},\n",
    "#                       'dr':{'HC':0.5,'CT':0.75,'COMB':0.5},'lr':{'HC':1,'CT':1},'tr':{'ADAS':1,'MMSE':2},\n",
    "#                        'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-4}}, \n",
    "#                 'hyp5':{'node_sizes':{'HC_L_ff':100,'HC_R_ff':100,'CT_ff':50,'COMB_ff':50,'ADAS_ff':25,'MMSE_ff':25},\n",
    "#                        'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},'tr':{'ADAS':1,'MMSE':1},\n",
    "#                         'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},\n",
    "#                 'hyp6':{'node_sizes':{'HC_L_ff':25,'HC_R_ff':25,'CT_ff':150,'COMB_ff':50,'ADAS_ff':25,'MMSE_ff':25},\n",
    "#                       'dr':{'HC':0,'CT':0,'COMB':0},'lr':{'HC':2,'CT':2},'tr':{'ADAS':1,'MMSE':1},\n",
    "#                        'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}}                \n",
    "                }\n",
    "\n",
    "CV_perf_hype = {}\n",
    "for hype in hype_configs.keys():    \n",
    "    node_sizes = hype_configs[hype]['node_sizes']\n",
    "    dr = hype_configs[hype]['dr']\n",
    "    lr = hype_configs[hype]['lr']\n",
    "    tr = hype_configs[hype]['tr']\n",
    "    solver_configs = hype_configs[hype]['solver_conf']\n",
    "    \n",
    "    CV_perf = {}\n",
    "    for fid in fid_list:\n",
    "        print 'Hype # {}, Fold # {}'.format(hype, fid)\n",
    "        train_filename_txt = baseline_dir + 'API/data/fold{}/train_C688.txt'.format(fid)\n",
    "        test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "\n",
    "        train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "        test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "        with open(train_filename_txt, 'w') as f:\n",
    "                f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "        with open(test_filename_txt, 'w') as f:\n",
    "                f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "        # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "        if pretrain:\n",
    "            train_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_train.prototxt'.format(fid)\n",
    "            with open(train_net_path, 'w') as f:\n",
    "                f.write(str(adninet_ae(train_filename_txt, 256, node_sizes, modality)))            \n",
    "        else:\n",
    "            train_net_path = baseline_dir + 'API/data/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(fid,hype,modality)\n",
    "            with open(train_net_path, 'w') as f:            \n",
    "                if modality == 'HC':\n",
    "                      f.write(str(adninet_ff_HC(train_filename_txt, 256, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                elif modality == 'CT':\n",
    "                      f.write(str(adninet_ff_CT(train_filename_txt, 256, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                elif modality == 'HC_CT':\n",
    "                      f.write(str(adninet_ff_HC_CT(train_filename_txt, 256, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                else:\n",
    "                      print 'Wrong modality'\n",
    "\n",
    "        if pretrain:\n",
    "            test_net_path = baseline_dir + 'API/data/fold{}/ADNI_AE_test.prototxt'.format(fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                f.write(str(adninet_ae(test_filename_txt, 256, node_sizes,modality)))\n",
    "        else:\n",
    "            test_net_path = baseline_dir + 'API/data/fold{}/ADNI_FF_test_{}_{}.prototxt'.format(fid,hype,modality)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                      f.write(str(adninet_ff_HC(test_filename_txt, 256, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                elif modality == 'CT':\n",
    "                      f.write(str(adninet_ff_CT(test_filename_txt, 256, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                elif modality == 'HC_CT':\n",
    "                      f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                else:\n",
    "                      print 'Wrong modality'\n",
    "\n",
    "        # Define Solver\n",
    "        solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "        with open(solver_path, 'w') as f:\n",
    "            f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, fid, exp_name, modality)))\n",
    "\n",
    "        ### load the solver and create train and test nets\n",
    "        #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "        #solver = caffe.get_solver(solver_path)\n",
    "        #solver = caffe.NesterovSolver(solver_path)\n",
    "        solver = caffe.NesterovSolver(solver_path)\n",
    "\n",
    "        if load_pretrained_weights:    \n",
    "            if hype in {'hyp1','hyp3','hyp5'}:\n",
    "                pre_hype = 'hyp1'\n",
    "            else:\n",
    "                pre_hype = 'hyp2'\n",
    "            #snap_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_HC_CT_iter_10000_concat50.caffemodel'.format(fid)\n",
    "            #snap_path = baseline_dir + 'API/data/fold{}/train_snaps/Exp11_{}_HC_CT_iter_10000.caffemodel'.format(fid,hype)\n",
    "            snap_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_{}_HC_CT_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'.format(fid,pre_hype,HC_snap,CT_snap)\n",
    "            print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "            solver.net.copy_from(snap_path)\n",
    "\n",
    "        #run caffe\n",
    "        results = run_caffe(solver,niter,multi_task)\n",
    "        CV_perf[fid] = results\n",
    "\n",
    "    CV_perf_hype[hype] = CV_perf\n",
    "# in case the kernel dies... \n",
    "\n",
    "pickleIt(CV_perf_hype, baseline_dir + 'API/CV_perf/train_loss_{}'.format(modality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Time for training \n",
    "#HC_CT fold 9,10, number of iterations: 40k, two hyper-params\n",
    "#start time: 14:47\n",
    "#end time: 15:31\n",
    "runtime_mean = (13+31)/4\n",
    "print runtime_mean\n",
    "\n",
    "print multi_task\n",
    "\n",
    "hype_configs\n",
    "\n",
    "# modality = 'HC_CT'\n",
    "# pkl_file = open(baseline_dir + 'API/CV_perf/{}'.format(modality), 'rb')\n",
    "# CV_perf = pickle.load(pkl_file) \n",
    "# pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "#niter = 30000\n",
    "test_interval = 500\n",
    "n_hype_configs = len(hype_configs.keys())\n",
    "pid = 1\n",
    "for hype in CV_perf_hype.keys(): \n",
    "    CV_perf = CV_perf_hype[hype]\n",
    "    n_CV_configs = len(CV_perf)\n",
    "    for fid in fid_list:        \n",
    "        train_loss_list = CV_perf[fid]['train_loss']\n",
    "        test_loss_list = CV_perf[fid]['test_loss']\n",
    "        \n",
    "        for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "            ax1 = plt.subplot(n_hype_configs,n_CV_configs,pid)            \n",
    "            ax1.plot(arange(niter), train_loss, label='train')\n",
    "            ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test', linewidth='3')                            \n",
    "            ax1.set_xlabel('iteration')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_title('fid: {}, hyp: {}, Test loss: {:.2f}'.format(fid, hype, test_loss[-1]))\n",
    "            ax1.legend(loc=1)\n",
    "            ax1.set_ylim(0,2000)\n",
    "        pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get encodings after training\n",
    "#train_filename_hdf = baseline_dir + 'data/train_CT_C688_normed.h5'\n",
    "#test_filename_hdf = baseline_dir + 'data/test_CT_C688_normed.h5'\n",
    "#fid=2\n",
    "#exp_name = 'Exp11'\n",
    "#niter = 24000\n",
    "#modality = 'CT'\n",
    "#start_fold = 1\n",
    "#n_folds = 2\n",
    "#fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "#preproc = 'no_preproc'\n",
    "batch_size = 256\n",
    "snap_interval = 2000\n",
    "snap_start = 2000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "Clinical_Scale = 'ADAS13'\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "print exp_name, modality\n",
    "\n",
    "for hype in hype_configs.keys():      \n",
    "    node_sizes = hype_configs[hype]['node_sizes']\n",
    "    dr = hype_configs[hype]['dr']\n",
    "    lr = hype_configs[hype]['lr']\n",
    "    \n",
    "    for fid in fid_list:\n",
    "        test_filename_txt = baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)\n",
    "        #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "        #with open(test_filename_txt, 'w') as f:\n",
    "        #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "        test_net_path = baseline_dir + 'API/data/fold{}/ADNI_FF_test.prototxt'.format(fid)\n",
    "        with open(test_net_path, 'w') as f:\n",
    "            if modality == 'HC':\n",
    "                f.write(str(adninet_ff_HC(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                input_nodes = ['X_L_HC','X_R_HC']\n",
    "            elif modality == 'CT':\n",
    "                f.write(str(adninet_ff_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                input_nodes = ['X_CT']\n",
    "            elif modality == 'HC_CT':\n",
    "                f.write(str(adninet_ff_HC_CT(test_filename_txt, 256, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                input_nodes = ['X_L_HC','X_R_HC','X_CT']\n",
    "            else:\n",
    "                print 'Wrong modality'\n",
    "    \n",
    "        print 'Hype # {}, Fold # {}, Clinical_Scale {}'.format(hype, fid, Clinical_Scale)\n",
    "        data_path = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "        if Clinical_Scale == 'ADAS13':\n",
    "            act_scores = load_data(data_path, 'y','no_preproc')\n",
    "        elif Clinical_Scale == 'MMSE': \n",
    "            act_scores = load_data(data_path, 'y3','no_preproc')\n",
    "        elif Clinical_Scale == 'BOTH':\n",
    "            act_scores_adas13 = load_data(data_path, 'y','no_preproc')\n",
    "            act_scores_mmse = load_data(data_path, 'y3','no_preproc')\n",
    "        else:\n",
    "            print 'unknown clinical scale'\n",
    "        \n",
    "        net_file = baseline_dir + 'API/data/fold{}/ADNI_FF_test.prototxt'.format(fid)\n",
    "        test_filename_hdf = baseline_dir + 'API/data/fold{}/{}/{}_{}.h5'.format(fid,cohort,exp_name,preproc)\n",
    "        test_filename_txt = baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort)\n",
    "        with open(test_filename_txt, 'w') as f:\n",
    "                f.write(test_filename_hdf + '\\n')  \n",
    "        \n",
    "        sub.call([\"cp\", baseline_dir + 'API/data/fold{}/{}_C688.txt'.format(fid,cohort), baseline_dir + 'API/data/fold{}/test_C688.txt'.format(fid)])\n",
    "        #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "        #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "        \n",
    "        if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "            multi_task = False\n",
    "            iter_euLoss = []\n",
    "            iter_r = []        \n",
    "            iter_pred_scores = []\n",
    "            for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "                results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                encodings = np.squeeze(results['X_out'])            \n",
    "                iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "            config_idx = '{}_{}'.format(hype,fid)\n",
    "            fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "            fold_r[config_idx] = np.array(iter_r)\n",
    "            fold_act_scores[fid] = act_scores\n",
    "            fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "        \n",
    "        elif Clinical_Scale == 'BOTH':\n",
    "            multi_task = True\n",
    "            iter_euLoss_adas13 = []\n",
    "            iter_r_adas13 = []        \n",
    "            iter_pred_scores_adas13 = []\n",
    "            iter_euLoss_mmse = []\n",
    "            iter_r_mmse = []        \n",
    "            iter_pred_scores_mmse = []\n",
    "            \n",
    "            for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                model_file = baseline_dir + 'API/data/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(fid,exp_name,hype,modality,snap_iter)        \n",
    "                results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                encodings = results['X_out']   \n",
    "                encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "            config_idx = '{}_{}'.format(hype,fid)\n",
    "            fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "            fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "            fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "            fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "            fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "            fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "            fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "            fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 5)\n",
    "n_rows = 1\n",
    "n_cols = 2\n",
    "pid = 1\n",
    "if multi_task:\n",
    "    euLosses = [fold_euLoss_adas13,fold_euLoss_mmse]\n",
    "    corrs = [fold_r_adas13,fold_r_mmse]\n",
    "else:\n",
    "    euLosses = [fold_euLoss]\n",
    "    corrs = [fold_r]\n",
    "    \n",
    "for fold_euLoss, fold_r in zip(euLosses, corrs):\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])    \n",
    "        plt.figure(fid)\n",
    "        plt.subplot(n_rows,n_cols,1)\n",
    "        plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_euLoss[hype_fid],label=hype_fid)\n",
    "        plt.title('Euclidean loss , fid: {}'.format(fid))\n",
    "        plt.legend()\n",
    "        plt.subplot(n_rows,n_cols,2)\n",
    "        plt.plot(np.arange(snap_start,niter+1,snap_interval),fold_r[hype_fid],label=hype_fid)\n",
    "        plt.title('correlation, fid: {}'.format(fid))\n",
    "        plt.legend(loc=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_r_dict = {}\n",
    "fold_euLoss_dict = {}\n",
    "fold_act_scores_dict = {}\n",
    "fold_pred_scores_dict = {}\n",
    "\n",
    "if multi_task:\n",
    "    fold_r_dict['ADAS'] = fold_r_adas13\n",
    "    fold_r_dict['MMSE'] = fold_r_mmse\n",
    "    fold_euLoss_dict['ADAS'] = fold_euLoss_adas13\n",
    "    fold_euLoss_dict['MMSE'] = fold_euLoss_mmse\n",
    "    fold_act_scores_dict['ADAS'] = fold_act_scores_adas13\n",
    "    fold_act_scores_dict['MMSE'] = fold_act_scores_mmse\n",
    "    fold_pred_scores_dict['ADAS'] = fold_pred_scores_adas13\n",
    "    fold_pred_scores_dict['MMSE'] = fold_pred_scores_mmse\n",
    "    \n",
    "else:    \n",
    "    fold_r_dict = fold_r\n",
    "    fold_euLoss_dict = fold_euLoss\n",
    "    fold_act_scores_dict = fold_act_scores\n",
    "    fold_pred_scores_dict = fold_pred_scores\n",
    "    \n",
    "opt_metric = 'corr'\n",
    "\n",
    "Clinical_Scale = 'ADAS'\n",
    "task_weights = {'ADAS':1,'MMSE':1}\n",
    "NN_results = computePerfMetrics(fold_r_dict, fold_euLoss_dict, fold_act_scores_dict, fold_pred_scores_dict, opt_metric, hype_configs, Clinical_Scale,task_weights)\n",
    "\n",
    "adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "\n",
    "\n",
    "print 'ADAS corr: {}'.format(adas_r)\n",
    "print 'ADAS mse: {}'.format(adas_mse)\n",
    "print 'ADAS means: {}, {}'.format(np.mean(adas_r),np.mean(adas_mse))\n",
    "print ''\n",
    "print 'MMSE corr: {}'.format(mmse_r)\n",
    "print 'MMSE mse: {}'.format(mmse_mse)\n",
    "print 'MMSE means: {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse))\n",
    "print ''\n",
    "#print opt_configs['opt_hype']\n",
    "\n",
    "save_multitask_results = False\n",
    "if save_multitask_results:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    \n",
    "     # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "\n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "\n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "\n",
    "    print 'Clinical_Scale: {}'.format(Clinical_Scale)\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        print 'This function does not work for single clinical scale models. Use the code from the next cell'\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "\n",
    "        fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "        fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "        fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "        fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "\n",
    "        for fid in fid_hype_map.keys():\n",
    "            r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "            r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "            r_perf_array = np.array(fid_r_perf[fid])\n",
    "            euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "            euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "            euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "            if opt_metric == 'euLoss':\n",
    "                h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "            else:\n",
    "                h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "\n",
    "            opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "\n",
    "            opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "            opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "            opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "            opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "\n",
    "            actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "            opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "            actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "            opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "        opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "        opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "euLosses = [fold_euLoss]\n",
    "corrs = [fold_r]\n",
    "act_scores = [fold_act_scores]\n",
    "pred_scores = [fold_pred_scores]\n",
    "Clinical_Scales = ['ADAS13']\n",
    "NN_multitask_results = {}\n",
    "for Clinical_Scale, fold_euLoss, fold_r, fold_act_scores, fold_pred_scores in zip(Clinical_Scales, euLosses, corrs, act_scores, pred_scores):\n",
    "    snap_array = np.arange(snap_start,niter+1,snap_interval)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = []\n",
    "    opt_mse = []\n",
    "    opt_hype = []\n",
    "    actual_scores = []\n",
    "    opt_pred_scores = []\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "        \n",
    "        # if want to find best hyp from mse values\n",
    "        h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        r = r_perf_array[h,snp]        \n",
    "        opt_r.append(r)\n",
    "        opt_mse.append(2*eu_loss)        \n",
    "        print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)\n",
    "        \n",
    "        # if want to find best hyp from corr values\n",
    "#         h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "#         r = r_perf_array[h,snp]        \n",
    "#         opt_r.append(r)\n",
    "#         eu_loss = euLoss_perf_array[h,snp]\n",
    "#         opt_mse.append(2*eu_loss)\n",
    "        opt_hype.append(hype_configs['hyp{}'.format(fid_hype_map[fid][h])])\n",
    "        \n",
    "        print 'fid:{}, best hype:{}, snap: {}, r:{}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],r,eu_loss)        \n",
    "        actual_scores.append(fold_act_scores[fid])  \n",
    "        opt_pred_scores.append(fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "        \n",
    "    print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r), np.mean(opt_mse))\n",
    "    print opt_r\n",
    "    print opt_mse\n",
    "    NN_multitask_results[Clinical_Scale] = {'opt_r':opt_r,'opt_mse':opt_mse,'opt_hype':opt_hype,\n",
    "                                            'actual_scores':actual_scores,'opt_pred_scores':opt_pred_scores}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ADNI1 fid=[5,6]\n",
    "# r: 0.57381259803575846, 0.50716443899467833,\n",
    "# mse: 65.989461305815198, 68.314810097414082\n",
    "\n",
    "# Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-10-27-02.pkl\n",
    "# old CV_r: [0.58713291550558833, 0.74574527911517141, 0.71271529005993939, 0.71772651594819259, 0.68488215732574409, \n",
    "#            0.64485628063795464, 0.59854968077229798, 0.6848414923548054, 0.60031497549309831, 0.57364714888913393]\n",
    "# Exp11_ADNI2_MMSE_NN_HC_CT_2016-05-15-19-59-42.pkl\n",
    "# old CV_r: [0.48038349554607179, 0.60131716074726682, 0.53198020733649787, 0.64770981175015141, 0.56547528703482142,\n",
    "#            0.52085739989088009, 0.65811053610310188, 0.57630405337929402, 0.51059593904907996, 0.48178577037836778]\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "# model_files = ['Exp7_ADNI2_ADAS13_NN_HC_CT_2016-05-01-13-46-16.pkl',\n",
    "#                'Exp7_ADNI2_ADAS13_NN_HC_CT_2016-05-02-10-29-55.pkl', 'Exp7_ADNI2_ADAS13_NN_HC_CT_2016-05-03-19-02-13.pkl']\n",
    "\n",
    "model_file = 'Exp6_ADNI1_ADAS13_NN_HC_CT_2016-05-06-17-14-49.pkl'\n",
    "print model_file\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "print 'old CV_r: {}'.format(CV_data['CV_r'])\n",
    "print ''\n",
    "print 'old mean(CV_r): {}, mean(MSE): {}'.format(np.mean(CV_data['CV_r']),np.mean(CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'old CV_mse: {}'.format(CV_data['CV_MSE'])\n",
    "#print CV_data['tid_snap_config_dict']\n",
    "\n",
    "NN_results = {}\n",
    "NN_results['CV_MSE'] = opt_mse\n",
    "NN_results['CV_r'] = opt_r\n",
    "NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "NN_results['actual_CV_scores'] = actual_scores\n",
    "NN_results['tid_snap_config_dict'] = opt_hype\n",
    "\n",
    "# #Combine second saved perf file with the previous one\n",
    "# model_file_2 = 'Exp11_ADNI2_ADAS13_NN_HC_CT_2016-05-06-00-03-01.pkl'\n",
    "# print model_file\n",
    "# NN_results = pickle.load(open(boxplots_dir + model_file_2,'rb'))\n",
    "\n",
    "idx_pairs = {4:0,5:1}\n",
    "updated_CV_data = update_fold_perf(boxplots_dir + model_file, NN_results, idx_pairs)\n",
    "print ''\n",
    "print 'new CV_r: {}'.format(updated_CV_data['CV_r'])\n",
    "print ''\n",
    "print 'new mean(CV_r): {}, mean(MSE): {}'.format(np.mean(updated_CV_data['CV_r']),np.mean(updated_CV_data['CV_MSE']))\n",
    "print ''\n",
    "print 'new CV_mse: {}'.format(updated_CV_data['CV_MSE'])\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_ADAS13_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(updated_CV_data, output)\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_data['tid_snap_config_dict'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update fold performance for multitask\n",
    "print NN_results['opt_ADAS'].keys()\n",
    "\n",
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "#model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl'\n",
    "model_file_soa = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-07-14-17-09-40.pkl'\n",
    "print 'current state of art: ' + model_file_soa\n",
    "CV_data = pickle.load(open(boxplots_dir + model_file_soa,'rb'))\n",
    "\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "#load old file\n",
    "model_file = 'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-03-19-29-16_Fold9_10.pkl'\n",
    "print 'updated fold result file: ' + model_file\n",
    "NN_results = pickle.load(open(boxplots_dir + model_file,'rb'))\n",
    "\n",
    "idx_pairs = {9:9}\n",
    "CV_data = update_multifold_perf(boxplots_dir + model_file_soa, NN_results, idx_pairs)\n",
    "\n",
    "print \"\"\n",
    "print 'updated CV_data'\n",
    "print 'ADAS'\n",
    "print CV_data['opt_ADAS']['CV_r']\n",
    "print CV_data['opt_ADAS']['CV_MSE']\n",
    "print np.mean(CV_data['opt_ADAS']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_ADAS']['CV_MSE'].values())\n",
    "print 'MMSE'\n",
    "print CV_data['opt_MMSE']['CV_r']\n",
    "print CV_data['opt_MMSE']['CV_MSE']\n",
    "print np.mean(CV_data['opt_MMSE']['CV_r'].values())\n",
    "print np.mean(CV_data['opt_MMSE']['CV_MSE'].values())\n",
    "\n",
    "save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "#print 'saving results at: {}'.format(save_name)\n",
    "\n",
    "save_updated_perf = False\n",
    "if save_updated_perf:\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp11_ADNI2_ADAS13_MMSE_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(CV_data, output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def update_fold_perf(saved_perf_file, new_perf_dict,idx_pairs): #idx_pairs={'old_idx':new_idx}\n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():        \n",
    "        for idx in idx_pairs.keys():            \n",
    "            CV_data[key][idx] = new_perf_dict[key][idx_pairs[idx]]\n",
    "\n",
    "    return CV_data\n",
    "\n",
    "def update_multifold_perf(saved_perf_file, new_perf_dict, idx_pairs):    \n",
    "    perf_metrics = ['CV_r', 'actual_CV_scores', 'CV_MSE', 'predicted_CV_scores']    \n",
    "    CV_data = pickle.load(open(saved_perf_file,'rb'))    \n",
    "    for key in CV_data.keys():\n",
    "        if key == 'opt_hype':\n",
    "            for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                CV_data[key][idx_key] = new_perf_dict[key][idx_val]\n",
    "                \n",
    "        else:\n",
    "            for pm in perf_metrics:\n",
    "                for idx_key,idx_val in idx_pairs.iteritems():\n",
    "                    CV_data[key][pm][idx_key] = new_perf_dict[key][pm][idx_val]\n",
    "    \n",
    "    return CV_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplots_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'\n",
    "model_files = ['Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-04-13-15-43.pkl',             \n",
    "             'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-05-23-07-31-29.pkl']\n",
    "\n",
    "#'Exp11_ADNI2_ADAS13_MMSE_NN_HC_CT_2016-06-03-19-29-16_Fold9_10.pkl'\n",
    "\n",
    "Clinical_Scale = 'ADAS'\n",
    "perf_array = []\n",
    "for mf in model_files:\n",
    "    CV_data = pickle.load(open(boxplots_dir + mf,'rb'))['opt_{}'.format(Clinical_Scale)]        \n",
    "    perf_array.append(CV_data['CV_r'].values())\n",
    "\n",
    "print (np.max(np.array(perf_array),axis=0))\n",
    "print np.mean(np.max(np.array(perf_array),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "#print 'modality: {} mse: {}, r: {}'.format(modality, np.mean(2*np.array(fold_euLoss)[:,-1]),np.mean(np.array(fold_r)[:,-1]))\n",
    "#CV_perf ={'fid_list': fid_hype_map.keys(), 'hype_configs':opt_hype,'fold_mse':opt_mse,'fold_r':opt_r}\n",
    "#pickleIt(CV_perf, baseline_dir + 'API/CV_perf/outer_test_{}.pkl'.format(modality))\n",
    "#print \"Saving opt perf for fids: {} with modality: {}\".format(fid_hype_map.keys(),modality)\n",
    "\n",
    "\n",
    "save_detailed_results = True\n",
    "multi_task = True\n",
    "\n",
    "if save_detailed_results:\n",
    "    # dictionaries for summarized results    \n",
    "    if not multi_task:\n",
    "        NN_results = {}\n",
    "        NN_results['CV_MSE'] = opt_mse\n",
    "        NN_results['CV_r'] = opt_r\n",
    "        NN_results['predicted_CV_scores'] = opt_pred_scores\n",
    "        NN_results['actual_CV_scores'] = actual_scores\n",
    "        NN_results['tid_snap_config_dict'] = opt_hype\n",
    "        print opt_r\n",
    "        print opt_mse\n",
    "        \n",
    "    else:\n",
    "        NN_results = NN_multitask_results\n",
    "        print 'ADAS13, opt_r: {}'.format(NN_multitask_results['ADAS13']['opt_r'])\n",
    "        print 'ADAS13, opt_mse {}: '.format(NN_multitask_results['ADAS13']['opt_mse'])\n",
    "        print 'MMSE, opt_r: {}'.format(NN_multitask_results['MMSE']['opt_r'])\n",
    "        print 'MMSE, opt_mse: {}'.format(NN_multitask_results['MMSE']['opt_mse'])\n",
    "        \n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    montage_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/'  \n",
    "    save_name = '{}Exp6_ADNI1_BOTH_NN_{}_{}.pkl'.format(montage_dir,modality,st)\n",
    "    print 'saving results at: {}'.format(save_name)\n",
    "    output = open(save_name, 'wb')\n",
    "    pickle.dump(NN_results, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "# Some defs to load data and extract encodings from trained net\n",
    "import collections\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    print net.blobs.items()[0]\n",
    "    print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        data_layers.append(net.blobs.items()[i][1])    \n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "    \n",
    "    net.reshape()            \n",
    "    print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}\n",
    "\n",
    "def generate_APIdata(in_data_path,out_data_path,fid,modality,preproc, CS_only):\n",
    "    print in_data_path\n",
    "    #Input config file generation\n",
    "    in_data = load_data(in_data_path, 'Fold_{}_X_{}'.format(fid,modality), preproc)\n",
    "    # get labels (no_preproc)        \n",
    "\n",
    "    # HDF5 is pretty efficient, but can be further compressed.\n",
    "    comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "    with h5py.File(out_data_path, 'a') as f:\n",
    "        if not CS_only:\n",
    "            if modality == 'R_CT': #Fix the typo\n",
    "                #Because we have separate ADNI1+2 cohort for ADAS13 no need to create 'y'\n",
    "                in_label = load_data(in_data_path, 'Fold_{}_y'.format(fid), 'no_preproc') \n",
    "                modality = 'CT'\n",
    "                f.create_dataset('y', data=in_label, **comp_kwargs)\n",
    "\n",
    "            f.create_dataset('X_{}'.format(modality), data=in_data, **comp_kwargs)\n",
    "        else:\n",
    "            if modality == 'R_CT': #Fix the typo\n",
    "                in_label = load_data(in_data_path, 'Fold_{}_y3'.format(fid), 'no_preproc')\n",
    "                f.create_dataset('y3', data=in_label, **comp_kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate API style data (options: scale / normalize)\n",
    "exp_name = 'Exp11'\n",
    "exp_name_out = 'Exp11_norm'\n",
    "cohorts = ['inner_train','inner_test','outer_test']\n",
    "modalities = ['L_HC','R_HC','R_CT']\n",
    "dataset = 'ADNI2'\n",
    "Clinical_Scale = 'ADAS13'\n",
    "CS_only = False\n",
    "\n",
    "for fid in np.arange(9,11,1):\n",
    "    for modality in modalities:\n",
    "        if modality == 'R_CT':\n",
    "            preproc = 'scale' \n",
    "        else:\n",
    "            preproc = 'no_preproc' #binary labels\n",
    "            \n",
    "        for cohort in cohorts:            \n",
    "            if cohort == 'inner_train':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_{}_NN_OuterFold_{}_train_InnerFold_1.h5'.format(exp_name,dataset,Clinical_Scale,fid)\n",
    "            elif cohort == 'inner_test':\n",
    "                in_data_path = baseline_dir + 'caffe_input/CV_{}_{}_{}_NN_OuterFold_{}_valid_InnerFold_1.h5'.format(exp_name,dataset,Clinical_Scale,fid)\n",
    "            else:                \n",
    "                in_data_path = baseline_dir + 'CV_{}_{}_{}_NN_valid.h5'.format(exp_name,dataset,Clinical_Scale)\n",
    "\n",
    "            out_data_path = baseline_dir + 'API/data/fold{}/{}/{}.h5'.format(fid,cohort,exp_name_out)\n",
    "\n",
    "            generate_APIdata(in_data_path,out_data_path,fid,modality,preproc, CS_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC: fid[1:4] 8000; fid[5,6] 6000, fid[7,8,9]: 8000 fid 10: 6000\n",
    "CT: fid[1:5] 12000 fid[6:10] 6000\n",
    "\n",
    "#spawn net\n",
    "ADNI_FF_train_hyp1_CT.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Net surgery AE --> FF pretrained weights\n",
    "#Review new FF net params\n",
    "modality = 'HC_CT'\n",
    "pretrain_snap_HC = 4000\n",
    "pretrain_snap_CT = 6000\n",
    "n_folds = 10\n",
    "\n",
    "#Custom snaps per fold\n",
    "#HC_iter = [8000,8000,8000,8000,6000,6000,8000,8000,8000,6000]\n",
    "#CT_iter = [12000,12000,12000,12000,12000,6000,6000,6000,6000,6000]\n",
    "\n",
    "hyp = 'hyp2'\n",
    "for fid in np.arange(1,n_folds+1,1):\n",
    "    print 'fid: {}'.format(fid)\n",
    "    #for AE_branch in ['CT','R_HC','L_HC']:\n",
    "    for AE_branch in ['CT','HC']:\n",
    "        print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "        if AE_branch == 'L_HC':\n",
    "            params_FF = ['L_ff1', 'L_ff2']            \n",
    "            AE_iter = 12000\n",
    "        elif AE_branch == 'R_HC':\n",
    "            params_FF = ['R_ff1', 'R_ff2']            \n",
    "            AE_iter = 12000\n",
    "        elif AE_branch == 'HC':\n",
    "            params_FF = ['L_ff1','R_ff1']\n",
    "            AE_iter = pretrain_snap_HC\n",
    "        elif AE_branch == 'CT':\n",
    "            #params_FF = ['ff1', 'ff2']\n",
    "            params_FF = ['ff1']\n",
    "            AE_iter = pretrain_snap_HC\n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            #Only use this during 1 of the modalities to avoid overwritting\n",
    "            print 'Spawning new net'\n",
    "            pretrain_net = caffe.Net(baseline_dir + 'API/data/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(1,hyp,modality), caffe.TRAIN)\n",
    "        else:\n",
    "            print 'Wrong AE branch'\n",
    "\n",
    "        # conv_params = {name: (weights, biases)}\n",
    "        conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "        for conv in params_FF:\n",
    "            print '{} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "        # Review AE net params \n",
    "        #fid for pretain is 1 because it's same definition for all the folds.\n",
    "        net_file = baseline_dir + 'API/data/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(1,hyp,AE_branch)\n",
    "        #model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "        model_file = baseline_dir + 'API/data/fold{}/train_snaps/Exp11_{}_{}_iter_{}.caffemodel'.format(fid,hyp,AE_branch,AE_iter) \n",
    "\n",
    "        AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "        #params_AE = ['encoder1', 'code']\n",
    "        params_AE = params_FF #if you are using pretrained NN\n",
    "        \n",
    "        # fc_params = {name: (weights, biases)}\n",
    "        fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "        for fc in params_AE:\n",
    "            print '{} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "        #transplant net parameters\n",
    "        for pr, pr_conv in zip(params_AE, params_FF):\n",
    "            conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "            conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "        save_net = True\n",
    "        if save_net:\n",
    "            save_path = baseline_dir + 'API/data/fold{}/pretrained_models/adni_ff_{}_{}_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'.format(fid,hyp,modality,pretrain_snap_HC,pretrain_snap_CT)\n",
    "            print \"Saving net to \" + save_path\n",
    "            pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "CT_data = pickle.load(open('/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/montage_data/Exp4_ADNI1_ADAS13_NN_CT_2016-03-01-11-18-48.pkl','rb'))\n",
    "CT_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fold_pred_scores['hyp1_1'][5] #CT_data['predicted_CV_scores'][1]\n",
    "act = fold_act_scores[1] #CT_data['actual_CV_scores'][1]\n",
    "print act\n",
    "#print CT_data['tid_snap_config_dict']\n",
    "print 'Euclidean loss: {}'.format(0.5*mse(pred,act))\n",
    "print stats.pearsonr(pred,act)\n",
    "plt.scatter(pred,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_pred_scores['hyp1_1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "exp_name = 'Exp6'\n",
    "preproc = 'no_preproc'\n",
    "train_filename_hdf = baseline_dir + 'API/data/fold{}/inner_train/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "inner_test_filename_hdf = baseline_dir + 'API/data/fold{}/inner_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "outer_test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "train_data = load_data(train_filename_hdf, 'X_R_HC', preproc)\n",
    "inner_test_data = load_data(inner_test_filename_hdf, 'X_R_HC', preproc)\n",
    "outer_test_data = load_data(outer_test_filename_hdf, 'X_R_HC', preproc)\n",
    "train_y = load_data(train_filename_hdf, 'y', preproc)\n",
    "inner_test_y = load_data(inner_test_filename_hdf, 'y', preproc)\n",
    "outer_test_y = load_data(outer_test_filename_hdf, 'y', preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data.shape, inner_test_data.shape, outer_test_data.shape\n",
    "print np.mean(np.sum(train_data,axis=1)), np.mean(np.sum(inner_test_data,axis=1)), np.mean(np.sum(outer_test_data,axis=1))\n",
    "print np.mean(train_y), np.mean(inner_test_y), np.mean(outer_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(17358, 16086) (4340, 16086) (71, 16086)\n",
    "3377.85793294 3403.70506912 3427.98591549\n",
    "15.4938933057 15.7380184332 15.7605633803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_data['opt_ADAS']['CV_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
