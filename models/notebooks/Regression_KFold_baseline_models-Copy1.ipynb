{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "#from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import KFold\n",
    "import pickle\n",
    "import re\n",
    "import collections\n",
    "import tables as tb\n",
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasets\n",
    "\n",
    "#input data\n",
    "train_val_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_train_plus_val.pkl'\n",
    "test_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_test.pkl'\n",
    "\n",
    "#k-fold indices (from a saved file)\n",
    "kf_file = \"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_train_valid_KFold_idx.pkl\"\n",
    "\n",
    "#save dir for trained model \n",
    "CV_model_dir = '/projects/nikhil/ADNI_prediction/models/CV_pkls/'\n",
    "\n",
    "inflated_train_val_file = '/projects/nikhil/ADNI_prediction/input_datasets/inflated_datasets/total_HC_vol_CT_inflated_CV_subsets_ROI_74.h5'\n",
    "\n",
    "inflated_train = True\n",
    "boxplot_config_r = collections.defaultdict(list)\n",
    "boxplot_config_R2 = collections.defaultdict(list)\n",
    "boxplot_config_MSE = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grab CV data with specific feature columes (independent vars) and specific clinical scale (dependent var)\n",
    "def load_CV_data(in_file, kf_file, feature_cols, clinical_scale):\n",
    "\n",
    "    data = pd.read_pickle(in_file)\n",
    "    data_trunc = data[clinical_scale + feature_cols]\n",
    "    # remove nans\n",
    "    print 'NaN shape: {}'.format(np.sum(np.isfinite(data_trunc[clinical_scale[0]])))\n",
    "    data_trunc = data_trunc[np.isfinite(data_trunc[clinical_scale[0]])]\n",
    "    X = np.asarray(data_trunc[feature_cols],dtype=float)\n",
    "    y = np.asarray(data_trunc[clinical_scale[0]],dtype=float)\n",
    "    \n",
    "    kf = pickle.load( open(kf_file, \"rb\" ) )\n",
    "    X_train = []\n",
    "    X_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    for train, valid in kf:        \n",
    "        X_train.append(X[train])\n",
    "        X_valid.append(X[valid])\n",
    "        y_train.append(y[train])\n",
    "        y_valid.append(y[valid])\n",
    "    \n",
    "    #print valid\n",
    "    # Return train and validation lists comprising all folds as well as unsplit data\n",
    "    return {'X_train':X_train,'X_valid':X_valid,'y_train':y_train,'y_valid':y_valid,'X':X,'y':y}\n",
    "\n",
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "def load_inflated_CV_data(in_file,feature_cols,s_fold, e_fold):\n",
    "    X_train = []\n",
    "    X_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    for i in np.arange(s_fold,e_fold+1,1):\n",
    "        for m,modality in enumerate(feature_cols):\n",
    "            train_name = 'Fold_{}_train_X_{}'.format(i,modality)\n",
    "            train_data = load_data(in_file, train_name)\n",
    "            #valid_name = 'Fold_{}_valid_X_{}'.format(i,modality)\n",
    "            #valid_data = load_data(in_file, valid_name)\n",
    "            \n",
    "            if modality in ['L_HC', 'R_HC']:\n",
    "                train_data = train_data.reshape(train_data.shape[0],1)            \n",
    "                #valid_data = valid_data.reshape(valid_data.shape[0],1)\n",
    "                \n",
    "            if m ==0:\n",
    "                train_X = train_data\n",
    "                #valid_X = valid_data\n",
    "            else:                 \n",
    "                train_X = np.hstack((train_X, train_data))\n",
    "                #valid_X = np.hstack((valid_X, valid_data))\n",
    "                \n",
    "        X_train.append(train_X)\n",
    "        #X_valid.append(valid_X)\n",
    "        \n",
    "        train_y = load_data(in_file, 'Fold_{}_train_y'.format(i))\n",
    "        #valid_y = load_data(in_file, 'Fold_{}_valid_y'.format(i))\n",
    "        \n",
    "        y_train.append(train_y)\n",
    "        #y_valid.append(valid_y)\n",
    "    \n",
    "    return {'X_train':X_train,'X_valid':X_valid,'y_train':y_train,'y_valid':y_valid}\n",
    "        \n",
    "#Load test data\n",
    "def load_test_data(in_file, feature_cols, clinical_scale):\n",
    "\n",
    "    data = pd.read_pickle(in_file)\n",
    "    data_trunc = data[clinical_scale + feature_cols]\n",
    "    # remove nans\n",
    "    print 'NaN shape: {}'.format(np.sum(np.isfinite(data_trunc[clinical_scale[0]])))\n",
    "    data_trunc = data_trunc[np.isfinite(data_trunc[clinical_scale[0]])]\n",
    "    X = np.asarray(data_trunc[feature_cols],dtype=float)\n",
    "    y = np.asarray(data_trunc[clinical_scale[0]],dtype=float)\n",
    "    return {'X':X, 'y':y}\n",
    "\n",
    "def innerCVLoop(model_clf,hyper_params,fold_X, fold_y,save_model,save_model_path):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn import grid_search\n",
    "    clf = grid_search.GridSearchCV(model_clf, hyper_params,cv=3,verbose=0)\n",
    "    clf.fit(fold_X, fold_y)\n",
    "    #Save classifier\n",
    "    if save_model:\n",
    "        save_model(clf,save_model_path)\n",
    "        \n",
    "    return clf\n",
    "\n",
    "def save_classifier(clf,save_model_path):\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    save_model_filename = save_model_path + '_' + st + '.pkl'\n",
    "        \n",
    "    f = open(save_model_filename, 'wb')\n",
    "    pickle.dump(clf, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN shape: 581\n",
      "NaN shape: 110\n",
      "(581, 76) (110, 76)\n",
      "(-0.33570121839753708, 9.0329730741718841e-17)\n",
      "(nan, 1.0)\n",
      "(-0.30697138385748041, 0.0011067044047419824)\n",
      "(nan, 1.0)\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Baseline models with non-inflated training data\n",
    "# Grab specific columns as features from the original table\n",
    "\n",
    "data = pd.read_pickle(train_val_file)\n",
    "data_cols = data.columns\n",
    "regex=re.compile(\".*(CT_).*\")\n",
    "CT_cols = [m.group(0) for l in data_cols for m in [regex.search(l)] if m] \n",
    "\n",
    "regex=re.compile(\".*(_HC_)(\\d+)\")\n",
    "HC_cols = [m.group(0) for l in data_cols for m in [regex.search(l)] if m] \n",
    "\n",
    "feature_cols = ['L_HC_VOL','R_HC_VOL'] + CT_cols\n",
    "#feature_cols = HC_cols + CT_cols\n",
    "clinical_scale = ['ADAS13']\n",
    "\n",
    "cv_data = load_CV_data(train_val_file,kf_file, feature_cols, clinical_scale)\n",
    "test_data = load_test_data(test_file, feature_cols, clinical_scale)\n",
    "\n",
    "print cv_data['X'].shape, test_data['X'].shape\n",
    "print stats.pearsonr(np.sum(cv_data['X'][:,:11427],axis=1),cv_data['y'])\n",
    "print stats.pearsonr(np.sum(cv_data['X'][:,11427:11427+10519],axis=1),cv_data['y'])\n",
    "\n",
    "print stats.pearsonr(np.sum(test_data['X'][:,:11427],axis=1),test_data['y'])\n",
    "print stats.pearsonr(np.sum(test_data['X'][:,11427:11427+10519],axis=1),test_data['y'])\n",
    "\n",
    "print np.mean(np.sum(test_data['X'][:,11427:11427+10519],axis=1))\n",
    "print np.mean(np.sum(cv_data['X'][:,11427:11427+10519],axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Baseline models with inflated training data\n",
    "feature_cols = ['L_HC', 'R_HC', 'CT']\n",
    "cv_data_inflated = load_inflated_CV_data(inflated_train_val_file, feature_cols,1, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.style.use('ggplot')\n",
    "for ct in np.arange(76):\n",
    "    plt.subplot(9,9,ct+1)\n",
    "    plt.hist(cv_data['X'][:,ct])\n",
    "    plt.title(feature_cols[ct])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this data for computing subject wise performance during outerloop cross-validation + held-out testset\n",
    "fused_data_dir = '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "CV_fused_file = 'HC_CT_inflated_CV_OuterFolds_valid_partition_fused.h5'\n",
    "test_fused_file = 'HC_CT_inflated_CV_OuterFolds_test_partition_fused.h5'\n",
    "\n",
    "CV = True\n",
    "L_HC_offset=11427\n",
    "R_HC_offset=10519\n",
    "\n",
    "if CV:\n",
    "    cv_data_valid_X = cv_data['X_valid']     \n",
    "    cv_data_valid_y = cv_data['y_valid']\n",
    "\n",
    "    CV_fused_data = h5.File(fused_data_dir + CV_fused_file,'a')\n",
    "    for f in np.arange(10):\n",
    "        all_data = cv_data_valid_X[f]\n",
    "        CV_fused_data.create_dataset('Fold_{}_X_L_HC'.format(f+1),data=all_data[:,:L_HC_offset])\n",
    "        CV_fused_data.create_dataset('Fold_{}_X_R_HC'.format(f+1),data=all_data[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "        CV_fused_data.create_dataset('Fold_{}_X_R_CT'.format(f+1),data=all_data[:,L_HC_offset+R_HC_offset:])#Typo : R_CT\n",
    "        CV_fused_data.create_dataset('Fold_{}_y'.format(f+1),data=cv_data_valid_y[f])\n",
    "    \n",
    "    CV_fused_data.close()\n",
    "\n",
    "else:\n",
    "    test_fused_data = h5.File(fused_data_dir + test_fused_file,'a')\n",
    "    all_data = test_data['X']\n",
    "    test_fused_data.create_dataset('heldout_X_L_HC',data=all_data[:,:L_HC_offset])\n",
    "    test_fused_data.create_dataset('heldout_X_R_HC',data=all_data[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "    test_fused_data.create_dataset('heldout_X_CT',data=all_data[:,L_HC_offset+R_HC_offset:])    \n",
    "    test_fused_data.create_dataset('heldout_y',data=test_data['y'])\n",
    "    test_fused_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# K-fold validations (nested)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import grid_search\n",
    "import datetime\n",
    "import time\n",
    "import collections\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "model_list = ['LR', 'LR_L1', 'SVR', 'RFR']\n",
    "model_choice_id = 3\n",
    "model_choice = model_list[model_choice_id]\n",
    "\n",
    "if model_choice == 'LR':\n",
    "    model_clf = LinearRegression()   \n",
    "    inner_loop = False #only needed to optimize hyper-params\n",
    "    feat_imp = False\n",
    "    if inflated_train:\n",
    "        print \"no trained model yet\"\n",
    "    else:\n",
    "        saved_model_name = 'LR_2015-10-13-16-38-28.pkl'\n",
    "    \n",
    "elif model_choice == 'LR_L1':\n",
    "    model_clf = Lasso()\n",
    "    hyper_params = {'alpha':[0.2, 0.1, 0.05, 0.01]} \n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = False\n",
    "    if clinical_scale[0] == 'ADAS13':\n",
    "        if inflated_train:\n",
    "            saved_model_name = 'LR_L1_ADAS13_inflated_train_2015-11-07-17-27-08.pkl'\n",
    "        else:\n",
    "            saved_model_name = 'LR_L1_ADAS13_2015-11-06-12-25-21.pkl'\n",
    "    else:\n",
    "        saved_model_name = 'LR_L1_MMSE_2015-10-16-16-27-29.pkl'\n",
    "    \n",
    "elif model_choice == 'SVR':\n",
    "    model_clf = SVR()\n",
    "    hyper_params = {'kernel':('linear', 'rbf'), 'C':[1,2.5,5,7.5,10]}\n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = False\n",
    "    if clinical_scale[0] == 'ADAS13':\n",
    "        if inflated_train:\n",
    "            saved_model_name = 'SVR_ADAS13_inflated_train_2015-11-11-15-57-34.pkl'\n",
    "        else:\n",
    "            saved_model_name = 'SVR_ADAS13_2015-11-06-11-47-23.pkl'\n",
    "    else:\n",
    "        saved_model_name = 'SVR_MMSE_2015-10-16-18-02-11.pkl'\n",
    "    \n",
    "elif model_choice == 'RFR':\n",
    "    model_clf = RandomForestRegressor()\n",
    "    hyper_params = {'n_estimators':[50,100,200,500],'min_samples_split':[1,2,4,8]}\n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = True\n",
    "    if clinical_scale[0] == 'ADAS13':\n",
    "        if inflated_train:\n",
    "            #saved_model_name = 'RFR_ADAS13_inflated_train_parallel_2015-11-10-15-01-42.pkl'\n",
    "            saved_model_name = 'RFR_ADAS13_inflated_train_2015-11-10-15-08-52.pkl'\n",
    "        else:\n",
    "            saved_model_name = 'RFR_ADAS13_2015-11-05-18-02-11.pkl'\n",
    "    else:\n",
    "        saved_model_name = 'RFR_MMSE_2015-10-16-16-22-42.pkl'\n",
    "else:\n",
    "    print \"Unknown model choice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Outer Fold Computation\n",
    "\n",
    "def computeOuterFold(train_X, train_y, valid_X, valid_y, model_clf, hyper_params, inner_loop, save_model, save_model_path):\n",
    "    import collections\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn import grid_search\n",
    "    import datetime\n",
    "    import time\n",
    "    import collections\n",
    "    from scipy.stats import mode\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from scipy import stats\n",
    "    \n",
    "    print 'Starting Outerfold computation'\n",
    "    \n",
    "    hp_dict = collections.defaultdict(list) #store best hyper-parameters for each fold\n",
    "    \n",
    "    if inner_loop:     \n",
    "        print 'Starting Inner computation'\n",
    "        save_model_path_fold = save_model_path + '_fold_' \n",
    "        clf = innerCVLoop(model_clf,hyper_params,train_X, train_y,save_model,save_model_path_fold)\n",
    "        for hp in hyper_params:\n",
    "            hp_dict[hp].append(clf.best_estimator_.get_params()[hp])\n",
    "            \n",
    "        print 'Ending InnerFold computation'\n",
    "\n",
    "    else:\n",
    "        clf = model_clf\n",
    "        clf.fit(fold_X,fold_y)\n",
    "        \n",
    "    #CV_scores\n",
    "    r_train = stats.pearsonr(clf.predict(train_X),train_y)\n",
    "    r_valid = stats.pearsonr(clf.predict(valid_X),valid_y)\n",
    "        \n",
    "    R2_train = clf.score(train_X,train_y) \n",
    "    R2_valid = clf.score(valid_X,valid_y)\n",
    "        \n",
    "    MSE_train = mse(clf.predict(train_X),train_y)\n",
    "    MSE_valid = mse(clf.predict(valid_X),valid_y)\n",
    "    \n",
    "    print 'Ending OuterFold computation'\n",
    "    \n",
    "    return {'r_train':r_train, 'r_valid':r_valid, 'R2_train':R2_train, 'R2_valid':R2_valid,\n",
    "           'MSE_train':MSE_train, 'MSE_valid':MSE_valid, 'hp_dict':hp_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most frequent hp:{'n_estimators': array([500]), 'min_samples_split': array([1])}\n",
      "CV r (mean, std_err): 0.18, 0.05\n",
      "CV R2 (mean, std_err): 0.01, 0.02\n",
      "CV MSE (mean, std_err): 77.92, 3.91\n"
     ]
    }
   ],
   "source": [
    "#Parallelize!!! \n",
    "from functools import partial\n",
    "\n",
    "save_model_path = CV_model_dir + model_choice + '_' + clinical_scale[0]\n",
    "load_model_path = CV_model_dir + saved_model_name\n",
    "\n",
    "# train a new classifer? (if false then load a single pretrained classifier based on most frequent hyperparams found previously\n",
    "# This will NOT load N different classifiers each for outer-CV fold  \n",
    "train_clf = True\n",
    "\n",
    "if inflated_train:    \n",
    "    cv_X_train = cv_data_inflated['X_train']\n",
    "    cv_y_train = cv_data_inflated['y_train']\n",
    "else: \n",
    "    cv_X_train = cv_data['X_train']\n",
    "    cv_y_train = cv_data['y_train']\n",
    "    \n",
    "cv_X_valid = cv_data['X_valid']\n",
    "cv_y_valid = cv_data['y_valid']\n",
    "\n",
    "no_of_folds = len(cv_X_train)\n",
    "\n",
    "save_model = False #do you want to save all classifiers per each fold? \n",
    "save_model_path = CV_model_dir + model_choice + '_' + clinical_scale[0]\n",
    "\n",
    "if train_clf:\n",
    "    CV_r_train=[] #pearson r score for each outer fold on train set\n",
    "    CV_r_valid=[] #pearson r score for each outer fold on validation set\n",
    "\n",
    "    CV_R2_train=[] #R2 score for each outer fold on train set\n",
    "    CV_R2_valid=[] #R2 score for each outer fold on validation set\n",
    "\n",
    "    CV_MSE_train=[] #MSE for each outer fold on train set\n",
    "    CV_MSE_valid=[] #MSE for each outer fold on validation set\n",
    "    # outer CV loop     \n",
    "    rc = ipp.Client()\n",
    "    dview = rc[:]\n",
    "    dview.push(dict(computeOuterFold = computeOuterFold))\n",
    "    dview.push(dict(innerCVLoop = innerCVLoop))\n",
    "    mapfunc = partial(computeOuterFold, model_clf=model_clf, hyper_params=hyper_params, inner_loop=inner_loop, \n",
    "                  save_model=save_model, save_model_path=save_model_path)\n",
    "    parallel_result = dview.map_sync(mapfunc, cv_X_train,cv_y_train,cv_X_valid,cv_y_valid)    \n",
    "    hp_dict = collections.defaultdict(list)\n",
    "    for pr in parallel_result:\n",
    "        CV_r_train.append(pr['r_train'])\n",
    "        CV_r_valid.append(pr['r_valid'])\n",
    "        CV_R2_train.append(pr['R2_train'])\n",
    "        CV_R2_valid.append(pr['R2_valid'])\n",
    "        CV_MSE_train.append(pr['MSE_train'])\n",
    "        CV_MSE_valid.append(pr['MSE_valid'])\n",
    "        \n",
    "        for hp in hyper_params:\n",
    "            hp_dict[hp].append(pr['hp_dict'][hp])\n",
    "            \n",
    "    #retrain model on the entire train + valid set with most frequent hyper-params during cross-val\n",
    "    if inner_loop:\n",
    "        hp_mode = {}\n",
    "        for hp in hyper_params:\n",
    "            hp_mode[hp] = mode(hp_dict[hp])[0][0]\n",
    "            \n",
    "        print 'most frequent hp:' + str(hp_mode)\n",
    "    \n",
    "else: \n",
    "    #Grabs the best classifer as a result of N-fold nested CV along with the MSE and R2 stats of the outerloop\n",
    "    print \"Loading previously saved model: \"\n",
    "    f = open(load_model_path)\n",
    "    result = pickle.load(f)\n",
    "    test_clf = result['best_clf']\n",
    "    CV_r_valid = result['CV_r']\n",
    "    CV_R2_valid = result['CV_R2']\n",
    "    CV_MSE_valid = result['CV_MSE']\n",
    "    f.close()\n",
    "\n",
    "print 'CV r (mean, std_err): ' + '{:04.2f}, {:04.2f}'.format(np.mean(zip(*CV_r_valid)[0]),stats.sem(zip(*CV_r_valid)[0]))\n",
    "print 'CV R2 (mean, std_err): ' + '{:04.2f}, {:04.2f}'.format(np.mean(CV_R2_valid),stats.sem(CV_R2_valid))\n",
    "print 'CV MSE (mean, std_err): ' + '{:04.2f}, {:04.2f}'.format(np.mean(CV_MSE_valid),stats.sem(CV_MSE_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37265, 76)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train r: (0.9910715046636237, 0.0)\n",
      "train R2 score: 0.909473086825\n",
      "test R2 score: 7.23562633082\n",
      "test r: (0.016415571716628191, 0.86484289011802784)\n",
      "test R2: -0.0323641854257\n",
      "test MSE: 86.7243566472\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the test set based on most frequnt hyper-params\n",
    "if train_clf:\n",
    "    #test_clf = LinearRegression()  \n",
    "    #test_clf = Lasso(alpha=0.02)    \n",
    "    #test_clf = SVR(kernel='linear',C=1)\n",
    "    test_clf = RandomForestRegressor(n_estimators=500,min_samples_split=1)\n",
    "   \n",
    "    save_model = True    \n",
    "    \n",
    "    if inflated_train:\n",
    "        save_model_path = CV_model_dir + model_choice + '_' + clinical_scale[0] + '_inflated_train_parallel'\n",
    "        exemplar_X = cv_X_train[0]\n",
    "        exemplar_y = cv_y_train[0]        \n",
    "    else:\n",
    "        exemplar_X = cv_data['X']\n",
    "        exemplar_y = cv_data['y']\n",
    "        save_model_path = CV_model_dir + model_choice + '_' + clinical_scale[0]\n",
    "    \n",
    "    test_clf.fit(exemplar_X,exemplar_y)\n",
    "    if save_model:\n",
    "        classifier_model_and_stats = {'best_clf':test_clf, 'CV_R2':CV_R2_valid, 'CV_MSE':CV_MSE_valid, 'CV_r': CV_r_valid}\n",
    "        save_classifier(classifier_model_and_stats,save_model_path)\n",
    "\n",
    "    # heldout set training perf\n",
    "    pearson_r_train = stats.pearsonr(test_clf.predict(exemplar_X),exemplar_y)\n",
    "    R2_train = test_clf.score(exemplar_X,exemplar_y)\n",
    "    MSE_train = mse(test_clf.predict(exemplar_X),exemplar_y)\n",
    "    \n",
    "    print \"train r: \" + str(pearson_r_train)\n",
    "    print \"train R2 score: \" + str(R2_train)\n",
    "    print \"test R2 score: \" + str(MSE_train)\n",
    "\n",
    "# heldout set test perf\n",
    "predicted_score = test_clf.predict(test_data['X'])\n",
    "actual_score = test_data['y']\n",
    "pearson_r_test = stats.pearsonr(predicted_score,actual_score)\n",
    "R2_test = test_clf.score(test_data['X'],actual_score)\n",
    "MSE_test = mse(predicted_score,actual_score)\n",
    "\n",
    "print \"test r: \" + str(pearson_r_test)\n",
    "print \"test R2: \" + str(R2_test)\n",
    "print \"test MSE: \" + str(MSE_test)\n",
    "\n",
    "save_plt_data = True\n",
    "if save_plt_data: #save the data to be plotter later.. (montages etc..)\n",
    "        montage_dir = '/projects/nikhil/ADNI_prediction/caffe_output/montage_data/'\n",
    "        mydict = {'predicted_score': predicted_score, 'actual_score': actual_score}\n",
    "        output = open('{}corr_data_{}_T{}.pkl'.format(montage_dir,model_choice+'_infl',1), 'wb')\n",
    "        pickle.dump(mydict, output)\n",
    "        output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.176370889054\n"
     ]
    }
   ],
   "source": [
    "load_saved_boxplots = False\n",
    "if load_saved_boxplots:\n",
    "    boxplots_dir = '/projects/nikhil/ADNI_prediction/caffe_output/montage_data/'\n",
    "    file_str = 'CV_corr_data_T11.pkl'\n",
    "    pkl_file = open(boxplots_dir + file_str.format(cat), 'rb')\n",
    "    boxplot_config_r = pickle.load(pkl_file)\n",
    "    pkl_file.close()    \n",
    "    file_str = 'CV_MSE_data_T11.pkl'\n",
    "    pkl_file = open(boxplots_dir + file_str.format(cat), 'rb')\n",
    "    boxplot_config_MSE = pickle.load(pkl_file)\n",
    "    pkl_file.close()    \n",
    "\n",
    "else:\n",
    "    #print np.mean(zip(*CV_r_train)[0])\n",
    "    print np.mean(zip(*CV_r_valid)[0])\n",
    "\n",
    "    boxplot_config_r[model_choice].append((zip(*CV_r_valid)[0]))\n",
    "    boxplot_config_R2[model_choice].append(CV_R2_valid)\n",
    "    boxplot_config_MSE[model_choice].append(CV_MSE_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil/.conda/envs/adni/lib/python2.7/site-packages/ipykernel/__main__.py:14: FutureWarning: \n",
      "The default value for 'return_type' will change to 'axes' in a future release.\n",
      " To use the future behavior now, set return_type='axes'.\n",
      " To keep the previous behavior and silence this warning, set return_type='dict'.\n"
     ]
    }
   ],
   "source": [
    "# Boxplots for CV statistics (r, mse, R2)\n",
    "stat_measure_list = [boxplot_config_r,boxplot_config_MSE]\n",
    "stat_measure_names = ['Pearson r', 'MSE']\n",
    "from matplotlib.artist import setp\n",
    "plt.figure()\n",
    "font_small = 12\n",
    "font_med = 16\n",
    "font_large = 24\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "for sm, stat_measure in enumerate(stat_measure_list):\n",
    "    plt.subplot(2,1,sm+1)\n",
    "    df_array = pd.DataFrame(dict([ (k,pd.Series(v[0])) for k,v in stat_measure.iteritems() ]))\n",
    "    bplot = df_array.boxplot(column=stat_measure.keys(), fontsize=font_large)\n",
    "    #plt.xlabel('Method',fontsize=font_large)\n",
    "    plt.ylabel(stat_measure_names[sm],fontsize=font_large)\n",
    "    plt.xticks(fontsize=font_med)\n",
    "    plt.yticks(fontsize=font_small)\n",
    "    setp(bplot['boxes'], linewidth=2)\n",
    "    setp(bplot['whiskers'], linewidth=2)\n",
    "    setp(bplot['fliers'], linewidth=2)\n",
    "    setp(bplot['medians'], linewidth=2)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute scores and MSEs\n",
    "\n",
    "y_cv_pred = test_clf.predict(exemplar_X)\n",
    "y_test_pred = test_clf.predict(test_data['X'])\n",
    "\n",
    "x_data_array = [exemplar_y,test_data['y']]\n",
    "y_data_array = [y_cv_pred,y_test_pred]\n",
    "lable_array = ['CV train performance','test performance']\n",
    "\n",
    "# only test perf\n",
    "#x_data_array = [test_data['y']]\n",
    "#y_data_array = [y_test_pred]\n",
    "#lable_array = ['test performance']\n",
    "\n",
    "plt.figure()\n",
    "font_small = 8\n",
    "font_med = 16\n",
    "font_large = 24\n",
    "no_of_plots = len(lable_array)\n",
    "plt.style.use('ggplot')\n",
    "plt_col = no_of_plots\n",
    "plt_row = 1\n",
    "\n",
    "for i in np.arange(no_of_plots):\n",
    "    x = x_data_array[i]\n",
    "    y = y_data_array[i]\n",
    "\n",
    "    plt.subplot(plt_row,plt_col,i+1)\n",
    "    plt.scatter(x, y, c='crimson', label=lable_array[i],s=40)\n",
    "    fit = np.polyfit(x,y,1)\n",
    "    fit_fn = np.poly1d(fit) \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    if p_value < 0.0001:\n",
    "        p_value_sig = '<0.0001'\n",
    "    else:\n",
    "        p_value_sig = str(p_value)\n",
    "        \n",
    "    label_str = 'r-value: {:04.2f}'.format(r_value) + '\\n' + 'p-value: ' + p_value_sig + '\\n' + 'std_err: {:04.2f}'.format(std_err) \n",
    "    # fit_fn is now a function which takes in x and returns an estimate for y\n",
    "    plt.plot(x, fit_fn(x),linewidth=3, label=label_str)\n",
    "    plt.title(model_choice,fontsize=font_large)\n",
    "    plt.xlabel('Actual Score',fontsize=font_large)\n",
    "    plt.ylabel('Predicted Score',fontsize=font_large)            \n",
    "    plt.legend(fontsize=font_med,loc=2)\n",
    "\n",
    "if feat_imp:\n",
    "    plt.figure()\n",
    "    #plt.subplot(plt_row,plt_col,4)\n",
    "    x_pos = np.arange(len(feature_cols))\n",
    "    \n",
    "    if model_choice == 'RFR':\n",
    "        feature_wts = test_clf.feature_importances_\n",
    "    elif model_choice == 'LR_L1':\n",
    "        feature_wts = np.squeeze(test_clf.coef_)\n",
    "    else: \n",
    "        print 'no feature_wt vector found'\n",
    "        \n",
    "    sorted_feat_idx = np.argsort(np.abs(feature_wts))[::-1]        \n",
    "    plt.bar(x_pos,feature_wts[sorted_feat_idx],color='crimson')\n",
    "    plt.ylabel('Feature Importance',fontsize=font_large)\n",
    "    #Sort the feature name list as well \n",
    "    sorted_feature_cols = []\n",
    "    for i,sort_idx in enumerate(sorted_feat_idx):\n",
    "        sorted_feature_cols.append(feature_cols[sort_idx])\n",
    "\n",
    "    plt.xticks(x_pos,sorted_feature_cols,rotation=70,fontsize=font_small)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot montages (across methods)\n",
    "mont_data_dir = '/projects/nikhil/ADNI_prediction/caffe_output/montage_data/'\n",
    "cats = ['LR_L1','SVR','RFR','LR_L1_infl','SVR_infl','RFR_infl'] #Montage Categories: methods / modalities\n",
    "file_str = 'corr_data_{}_T1.pkl'\n",
    "#cats = ['L_HC','R_HC','HC','HC_CT'] #Montage Categories: methods / modalities\n",
    "#file_str = 'corr_data_{}_T11.pkl'\n",
    "\n",
    "n_rows = 2\n",
    "n_cols = 3\n",
    "\n",
    "plt.figure()\n",
    "plt.style.use('ggplot')\n",
    "font_small = 8\n",
    "font_med = 16\n",
    "font_large = 24\n",
    "\n",
    "for c, cat in enumerate(cats):\n",
    "    if cat == 'HC_CT':\n",
    "        file_str = 'corr_data_{}_T11.pkl'\n",
    "        \n",
    "    pkl_file = open(mont_data_dir + file_str.format(cat), 'rb')\n",
    "    montage_data = pickle.load(pkl_file)\n",
    "    pkl_file.close()    \n",
    "    x = montage_data['predicted_score']\n",
    "    y = montage_data['actual_score']\n",
    "        \n",
    "    plt.subplot(n_rows,n_cols,c+1)\n",
    "    plt.scatter(x, y, c='crimson',s=20)\n",
    "    fit = np.polyfit(x,y,1)\n",
    "    fit_fn = np.poly1d(fit) \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)      \n",
    "    if p_value < 0.0001:\n",
    "        p_value_sig = '<0.0001'\n",
    "    else:\n",
    "        p_value_sig = '{:.5f}'.format(p_value)\n",
    "        \n",
    "    label_str = 'r-value: {:04.2f}'.format(r_value) + '\\n' + 'p-value: ' + p_value_sig + '\\n' + 'std_err: {:04.2f}'.format(std_err) \n",
    "    # fit_fn is now a function which takes in x and returns an estimate for y\n",
    "    plt.plot(x, fit_fn(x),linewidth=3, label=label_str)        \n",
    "    title_str = 'Heldout Testset, modality: {}'.format(cat)  #heldout Test set\n",
    "    plt.title(title_str,fontsize=font_med)\n",
    "    plt.xlabel('Actual Score',fontsize=font_med)\n",
    "    plt.ylabel('Predicted Score',fontsize=font_med)            \n",
    "    plt.legend(fontsize=font_med,loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Additional Scripts \n",
    "# Concat of Train + Valid (to generate multi-folds)\n",
    "train_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_train.pkl'\n",
    "valid_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_valid.pkl'\n",
    "t_data = pd.read_pickle(train_file)\n",
    "v_data = pd.read_pickle(valid_file)\n",
    "frames = [t_data, v_data]\n",
    "result = pd.concat(frames)\n",
    "result.to_pickle(\"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_train_plus_val.pkl\")\n",
    "\n",
    "# Generatng K-Folds\n",
    "#sampx = 100 #Train + Valid samples\n",
    "#foldx = 10   \n",
    "#kf = KFold(sampx, n_folds=foldx,shuffle=True)\n",
    "\n",
    "#for train, test in kf:\n",
    "#    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "4096*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
