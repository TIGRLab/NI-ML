{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "#from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import KFold\n",
    "import pickle\n",
    "import re\n",
    "import collections\n",
    "import tables as tb\n",
    "import ipyparallel as ipp\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasets\n",
    "\n",
    "#input data\n",
    "train_val_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_train_plus_val.pkl'\n",
    "test_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_test.pkl'\n",
    "\n",
    "#k-fold indices (from a saved file)\n",
    "kf_file = \"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_train_valid_KFold_idx.pkl\"\n",
    "\n",
    "#save dir for trained model \n",
    "CV_model_dir = '/projects/nikhil/ADNI_prediction/models/CV_pkls/'\n",
    "\n",
    "#inflated_train_val_file = '/projects/nikhil/ADNI_prediction/input_datasets/inflated_datasets/total_HC_vol_CT_inflated_CV_subsets_ROI_74.h5'\n",
    "#sub_sampx_map_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC_CT_inflated_CV_subsets.h5.pkl'\n",
    "inflated_train_val_file = '/projects/nikhil/ADNI_prediction/input_datasets/PE_inflated_samples/total_HC_vol_CT_inflated_CV_subsets_MC5.h5'\n",
    "sub_sampx_map_file = '/projects/nikhil/ADNI_prediction/input_datasets/HC_CT_inflated_CV_subsets_MC5.h5.pkl'\n",
    "\n",
    "AAL_roi_Name_featID_file = '/projects/nikhil/ADNI_prediction/input_datasets/CT/AAL_ROI_Name_featIDx.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grab CV data with specific feature columes (independent vars) and specific clinical scale (dependent var)\n",
    "def load_CV_data(in_file, kf_file, feature_cols, clinical_scale, scale_data):\n",
    "\n",
    "    data = pd.read_pickle(in_file)\n",
    "    data_trunc = data[['PTID','IID'] + clinical_scale + feature_cols]\n",
    "    # remove nans\n",
    "    print 'NaN shape: {}'.format(np.sum(np.isfinite(data_trunc[clinical_scale[0]])))\n",
    "    data_trunc = data_trunc[np.isfinite(data_trunc[clinical_scale[0]])]\n",
    "    X_raw = np.asarray(data_trunc[feature_cols],dtype=float)\n",
    "    y = np.asarray(data_trunc[clinical_scale[0]],dtype=float)\n",
    "    U = list(data_trunc['PTID'] + \"_\" + data_trunc['IID'])\n",
    "    \n",
    "    if scale_data:\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    else:\n",
    "        X = X_raw\n",
    "        \n",
    "    kf = pickle.load( open(kf_file, \"rb\" ) )\n",
    "    X_train = []\n",
    "    X_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    U_train = []\n",
    "    U_valid = []\n",
    "    for train, valid in kf:        \n",
    "        X_train.append(X[train])\n",
    "        X_valid.append(X[valid])\n",
    "        y_train.append(y[train])\n",
    "        y_valid.append(y[valid])\n",
    "        U_train.append([U[x] for x in train])\n",
    "        U_valid.append([U[x] for x in valid])\n",
    "        \n",
    "    #print valid\n",
    "    # Return train and validation lists comprising all folds as well as unsplit data\n",
    "    return {'X_train':X_train,'X_valid':X_valid,'y_train':y_train,'y_valid':y_valid,'X':X,'y':y, 'U_train':U_train,'U_valid':U_valid,'UIDs':U}\n",
    "\n",
    "def load_data(data_path, input_node):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X = data.get_node('/' + input_node)[:]\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "def load_inflated_CV_data(in_file,feature_cols,s_fold, e_fold, scale_data):\n",
    "    X_train = []\n",
    "    X_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    for i in np.arange(s_fold,e_fold+1,1):\n",
    "        for m,modality in enumerate(feature_cols):\n",
    "            train_name = 'Fold_{}_train_X_{}'.format(i,modality)\n",
    "            train_data = load_data(in_file, train_name)\n",
    "            #valid_name = 'Fold_{}_valid_X_{}'.format(i,modality)\n",
    "            #valid_data = load_data(in_file, valid_name)\n",
    "            \n",
    "            if modality in ['L_HC', 'R_HC']:\n",
    "                train_data = train_data.reshape(train_data.shape[0],1)            \n",
    "                #valid_data = valid_data.reshape(valid_data.shape[0],1)\n",
    "                \n",
    "            if m ==0:\n",
    "                train_X = train_data\n",
    "                #valid_X = valid_data\n",
    "            else:                 \n",
    "                train_X = np.hstack((train_X, train_data))\n",
    "                #valid_X = np.hstack((valid_X, valid_data))\n",
    "            \n",
    "        if scale_data:\n",
    "            train_X_scaled = preprocessing.scale(train_X)\n",
    "        else:\n",
    "            train_X_scaled = train_X\n",
    "                \n",
    "        X_train.append(train_X_scaled)\n",
    "        #X_valid.append(valid_X)\n",
    "        \n",
    "        train_y = load_data(in_file, 'Fold_{}_train_y'.format(i))\n",
    "        #valid_y = load_data(in_file, 'Fold_{}_valid_y'.format(i))\n",
    "        \n",
    "        y_train.append(train_y)\n",
    "        #y_valid.append(valid_y)\n",
    "    \n",
    "    return {'X_train':X_train,'X_valid':X_valid,'y_train':y_train,'y_valid':y_valid}\n",
    "        \n",
    "#Load test data\n",
    "def load_test_data(in_file, feature_cols, clinical_scale):\n",
    "\n",
    "    data = pd.read_pickle(in_file)\n",
    "    data_trunc = data[['PTID','IID'] + clinical_scale + feature_cols]\n",
    "    # remove nans\n",
    "    print 'NaN shape: {}'.format(np.sum(np.isfinite(data_trunc[clinical_scale[0]])))\n",
    "    data_trunc = data_trunc[np.isfinite(data_trunc[clinical_scale[0]])]\n",
    "    U = list(data_trunc['PTID'] + \"_\" + data_trunc['IID'])\n",
    "    \n",
    "    X_raw = np.asarray(data_trunc[feature_cols],dtype=float)\n",
    "    \n",
    "    if scale_data:\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    else:\n",
    "        X = X_raw\n",
    "        \n",
    "    y = np.asarray(data_trunc[clinical_scale[0]],dtype=float)\n",
    "    return {'X':X, 'y':y, 'U':U}\n",
    "\n",
    "def innerCVLoop(model_clf,hyper_params,fold_X, fold_y,save_model,save_model_path):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn import grid_search\n",
    "    clf = grid_search.GridSearchCV(model_clf, hyper_params,cv=3,verbose=0)\n",
    "    clf.fit(fold_X, fold_y)\n",
    "    #Save classifier\n",
    "    if save_model:\n",
    "        save_model(clf,save_model_path)\n",
    "        \n",
    "    return clf\n",
    "\n",
    "def save_classifier(clf,save_model_path):\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    save_model_filename = save_model_path + '_' + st + '.pkl'\n",
    "        \n",
    "    f = open(save_model_filename, 'wb')\n",
    "    pickle.dump(clf, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Baseline models with non-inflated training data\n",
    "# Grab specific columns as features from the original table\n",
    "data = pd.read_pickle(train_val_file)\n",
    "data_cols = data.columns\n",
    "#regex=re.compile(\".*(CT_).*\")\n",
    "#CT_cols_raw = [m.group(0) for l in data_cols for m in [regex.search(l)] if m] \n",
    "#print len(CT_cols_raw)\n",
    "#print CT_cols_raw\n",
    "\n",
    "CT_name_featID_dict = pd.read_pickle(AAL_roi_Name_featID_file)\n",
    "CT_vals = CT_name_featID_dict.values()\n",
    "#print len(CT_cols)\n",
    "#print CT_cols\n",
    "CT_cols = []\n",
    "for ct in CT_vals:\n",
    "    CT_cols.append('CT_' + ct.strip())    \n",
    "\n",
    "#print CT_cols\n",
    "regex=re.compile(\".*(_HC_)(\\d+)\")\n",
    "HC_cols = [m.group(0) for l in data_cols for m in [regex.search(l)] if m] \n",
    "\n",
    "feature_cols = ['L_HC_VOL','R_HC_VOL'] + CT_cols\n",
    "#feature_cols = CT_cols\n",
    "clinical_scale = ['ADAS13']\n",
    "\n",
    "scale_data = True\n",
    "cv_data = load_CV_data(train_val_file,kf_file, feature_cols, clinical_scale, scale_data)\n",
    "test_data = load_test_data(test_file, feature_cols, clinical_scale)\n",
    "\n",
    "#Data stats for QC\n",
    "print cv_data['X'].shape, test_data['X'].shape\n",
    "print 'L_HC corr: {}'.format(stats.pearsonr(np.sum(cv_data['X'][:,:1],axis=1),cv_data['y']))\n",
    "print 'R_HC corr: {}'.format(stats.pearsonr(np.sum(cv_data['X'][:,1:1+1],axis=1),cv_data['y']))\n",
    "print 'L_HC mean vol {}:'.format(np.mean(np.sum(cv_data['X'][:,:1],axis=1)))\n",
    "print 'R_HC mean vol {}:'.format(np.mean(np.sum(cv_data['X'][:,1:1+1],axis=1)))\n",
    "\n",
    "print 'L_HC corr: {}'.format(stats.pearsonr(cv_data['X_valid'][0][:,0],cv_data['y_valid'][0]))\n",
    "print 'R_HC corr: {}'.format(stats.pearsonr(cv_data['X_valid'][0][:,1],cv_data['y_valid'][0]))\n",
    "\n",
    "print 'means {}:'.format(np.mean(cv_data['X_valid'][0],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sanity check! Dosn't work ... \n",
    "new_fused_data = h5.File('/projects/nikhil/ADNI_prediction/input_datasets/HC_CT_fused_CV_subsets_C688_valid.h5','r')\n",
    "new_inflated_data = h5.File('/projects/nikhil/ADNI_prediction/input_datasets/inflated_datasets_C688/HC_CT_inflated_CV_OuterFold_1_valid_InnerFold_1_partition_ROI_74.h5','r')\n",
    "#new_inflated_data = h5.File('/scratch/nikhil/ADNI_prediction_data_bkup/HC_CT_inflated_CV_subsets_C688.h5','r')\n",
    "#new_fused_data = h5.File('/scratch/nikhil/ADNI_prediction_data_bkup/HC_CT_fused_CV_subsets_C688.h5','r')\n",
    "\n",
    "Fold_1_X_L_HC_fused = new_fused_data['Fold_1_X_L_HC'][:]\n",
    "Fold_1_X_R_HC_fused = new_fused_data['Fold_1_X_R_HC'][:]\n",
    "Fold_1_X_R_CT_fused = new_fused_data['Fold_1_X_R_CT'][:]\n",
    "Fold_1_y_fused = new_fused_data['Fold_1_y'][:]\n",
    "\n",
    "Fold_1_X_L_HC_infl = new_inflated_data['Fold_1_X_L_HC'][:]\n",
    "Fold_1_X_R_HC_infl = new_inflated_data['Fold_1_X_R_HC'][:]\n",
    "Fold_1_X_R_CT_infl = new_inflated_data['Fold_1_X_R_CT'][:]\n",
    "Fold_1_y_infl = new_inflated_data['Fold_1_y'][:]\n",
    "\n",
    "\n",
    "#infl_data = new_inflated_data['Fold_1_train_X'][:]\n",
    "#Fold_1_X_L_HC_infl = infl_data[:,:11427]\n",
    "#Fold_1_X_R_HC_infl = infl_data[:,11427:11427+10519]\n",
    "#Fold_1_X_R_CT_infl = infl_data[:,11427+10519:]\n",
    "#Fold_1_y_infl = new_inflated_data['Fold_1_train_y'][:]\n",
    "\n",
    "new_fused_data.close()\n",
    "new_inflated_data.close()\n",
    "print Fold_1_X_L_HC_fused.shape, Fold_1_X_L_HC_infl.shape\n",
    "print 'L_HC corr: {}'.format(stats.pearsonr(np.sum(Fold_1_X_L_HC_fused,axis=1),Fold_1_y_fused))\n",
    "print 'R_HC corr: {}'.format(stats.pearsonr(np.sum(Fold_1_X_R_HC_fused,axis=1),Fold_1_y_fused))\n",
    "print 'L_HC mean vol {}:'.format(np.mean(np.sum(Fold_1_X_L_HC_fused,axis=1)))\n",
    "print 'R_HC mean vol {}:'.format(np.mean(np.sum(Fold_1_X_R_HC_fused,axis=1)))\n",
    "\n",
    "print 'L_HC corr: {}'.format(stats.pearsonr(np.sum(Fold_1_X_L_HC_infl,axis=1),Fold_1_y_infl))\n",
    "print 'R_HC corr: {}'.format(stats.pearsonr(np.sum(Fold_1_X_R_HC_infl,axis=1),Fold_1_y_infl))\n",
    "print 'L_HC mean vol {}:'.format(np.mean(np.sum(Fold_1_X_L_HC_infl,axis=1)))\n",
    "print 'R_HC mean vol {}:'.format(np.mean(np.sum(Fold_1_X_R_HC_infl,axis=1)))\n",
    "\n",
    "print Fold_1_X_R_CT_fused.shape, Fold_1_X_R_CT_infl.shape\n",
    "ct_feat = Fold_1_X_R_CT_fused.shape[1]\n",
    "x = np.arange(ct_feat)\n",
    "ym_f = np.mean(Fold_1_X_R_CT_fused,axis=0)\n",
    "ym_i = np.mean(Fold_1_X_R_CT_infl,axis=0)\n",
    "ye = stats.sem(Fold_1_X_R_CT_fused, axis=0)\n",
    "\n",
    "cv_data_r = []\n",
    "cv_data_inflated_r = []\n",
    "for i in np.arange(ct_feat):\n",
    "    cv_data_r.append(stats.pearsonr(Fold_1_X_R_CT_fused[:,i], Fold_1_y_fused))        \n",
    "    cv_data_inflated_r.append(stats.pearsonr(Fold_1_X_R_CT_infl[:,i],Fold_1_y_infl))\n",
    "                               \n",
    "r_diff = np.array(zip(*cv_data_r)[0]) - np.array(zip(*cv_data_inflated_r)[0])\n",
    "    \n",
    "plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(ym_f-ym_i, label='diff')\n",
    "#plt.errorbar(x[2:],ct_data_inflated_roi_mean[2:],yerr=ct_data_inflated_roi_std_err[2:],label='inflated')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(r_diff ,label='diff')\n",
    "#plt.plot(zip(*cv_data_r)[0] - zip(*cv_data_inflated_r)[0] ,label='diff')\n",
    "#plt.plot(zip(*cv_data_inflated_r)[0],label='inflated')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print Fold_1_y_fused\n",
    "print cv_data['y_valid'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Baseline models with inflated training data\n",
    "feature_cols_inflated = ['L_HC', 'R_HC', 'CT']\n",
    "#feature_cols_inflated = ['CT']\n",
    "\n",
    "#Temp Change due to mismatch in CT columns\n",
    "scale_data = True\n",
    "cv_data_inflated = load_inflated_CV_data(inflated_train_val_file, feature_cols_inflated,1, 10, scale_data)\n",
    "\n",
    "HC_offset = 0 # if only using CT features\n",
    "\n",
    "#X_train_list = []\n",
    "#ignore_list_CT_idx = list(HC_offset + np.array([0,29,30,37,38]))\n",
    "#for X_train in cv_data_inflated_raw['X_train']:\n",
    "#    X_train_list.append(np.delete(X_train, np.s_[ignore_list_CT_idx], 1))\n",
    "        \n",
    "#cv_data_inflated = {'X_train':X_train_list,'y_train':cv_data_inflated_raw['y_train']}\n",
    "#######  \n",
    "\n",
    "#Data stats for QC\n",
    "print cv_data_inflated['X_train'][0].shape\n",
    "print 'L_HC corr: {}'.format(stats.pearsonr((cv_data_inflated['X_train'][0][:,0]),cv_data_inflated['y_train'][0]))\n",
    "print 'R_HC corr: {}'.format(stats.pearsonr((cv_data_inflated['X_train'][0][:,1]),cv_data_inflated['y_train'][0]))\n",
    "print 'L_HC mean vol {}:'.format(np.mean((cv_data_inflated['X_train'][0][:,0])))\n",
    "print 'R_HC mean vol {}:'.format(np.mean((cv_data_inflated['X_train'][0][:,1])))\n",
    "\n",
    "#sub_sampx_dict_list = pickle.load( open(sub_sampx_map_file, \"rb\" ) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cv_data['y_train'][0][:100]\n",
    "print cv_data_inflated['y_train'][0][:100]\n",
    "\n",
    "for f in np.arange(10):\n",
    "    verify=[]\n",
    "    pointer=0\n",
    "\n",
    "    d = sub_sampx_dict_list[f]\n",
    "    keys = d.keys()\n",
    "\n",
    "    #print len(d)\n",
    "\n",
    "    for ind, y1 in enumerate(cv_data['y_train'][f]):\n",
    "        #print d[keys[ind]]\n",
    "        y2 = cv_data_inflated['y_train'][f][pointer:pointer+d[keys[ind]]]\n",
    "        pointer = pointer+sub_sampx_dict_list[f][keys[ind]]\n",
    "        #print 'y1 {}, y2 {}, verify: {}'.format(y1,np.mean(y2),(float(y1)-float(np.mean(y2))))\n",
    "        verify.append(np.round(y1-np.mean(y2)))\n",
    "    \n",
    "   # print np.sum(verify)\n",
    "\n",
    "#print sub_sampx_dict_list[0]\n",
    "#print (cv_data['U_train'][0])\n",
    "\n",
    "#for f in np.arange(10):\n",
    "#    a = sorted(sub_sampx_dict_list[f].keys())\n",
    "#    b = sorted(cv_data['U_train'][f])\n",
    "#    c = sorted(cv_data['U_valid'][f])\n",
    "#    print set(a) == set(b)\n",
    "#    print set(c) & set(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct_data_roi_mean = np.mean(cv_data['X'],axis=0)\n",
    "ct_data_inflated_roi_mean = np.mean(cv_data_inflated['X_train'][0],axis=0)\n",
    "ct_data_roi_std_err = stats.sem(cv_data['X'],axis=0)\n",
    "ct_data_inflated_roi_std_err = stats.sem(cv_data_inflated['X_train'][0],axis=0)\n",
    "x=np.arange(len(feature_cols))\n",
    "cv_data_r = []\n",
    "cv_data_inflated_r = []\n",
    "\n",
    "\n",
    "for i in np.arange(len(feature_cols)):\n",
    "    cv_data_r.append(stats.pearsonr(cv_data['X'][:,i], cv_data['y']))        \n",
    "    cv_data_inflated_r.append(stats.pearsonr(cv_data_inflated['X_train'][0][:,i],cv_data_inflated['y_train'][0]))\n",
    "                               \n",
    "                               \n",
    "print np.shape(ct_data_roi_mean),np.shape(ct_data_inflated_roi_mean)\n",
    "\n",
    "plt.figure()\n",
    "plt.style.use('ggplot')\n",
    "plt.subplot(2,1,1)\n",
    "plt.errorbar(x[2:],ct_data_inflated_roi_mean[2:]-ct_data_roi_mean[2:],yerr=ct_data_roi_std_err[2:],label='diff')\n",
    "#plt.errorbar(x[2:],ct_data_inflated_roi_mean[2:],yerr=ct_data_inflated_roi_std_err[2:],label='inflated')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(zip(*cv_data_r)[0],label='basic')\n",
    "plt.plot(zip(*cv_data_inflated_r)[0],label='inflated')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this data for computing subject wise performance during outerloop cross-validation + held-out testset\n",
    "fused_data_dir = '/projects/nikhil/ADNI_prediction/input_datasets/'\n",
    "CV_fused_file = 'HC_CT_inflated_CV_OuterFolds_valid_partition_fused.h5'\n",
    "test_fused_file = 'HC_CT_inflated_CV_OuterFolds_test_partition_fused.h5'\n",
    "\n",
    "CV = True\n",
    "L_HC_offset=11427\n",
    "R_HC_offset=10519\n",
    "\n",
    "if CV:\n",
    "    cv_data_valid_X = cv_data['X_valid']     \n",
    "    cv_data_valid_y = cv_data['y_valid']\n",
    "\n",
    "    CV_fused_data = h5.File(fused_data_dir + CV_fused_file,'a')\n",
    "    for f in np.arange(10):\n",
    "        all_data = cv_data_valid_X[f]\n",
    "        CV_fused_data.create_dataset('Fold_{}_X_L_HC'.format(f+1),data=all_data[:,:L_HC_offset])\n",
    "        CV_fused_data.create_dataset('Fold_{}_X_R_HC'.format(f+1),data=all_data[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "        CV_fused_data.create_dataset('Fold_{}_X_R_CT'.format(f+1),data=all_data[:,L_HC_offset+R_HC_offset:])#Typo : R_CT\n",
    "        CV_fused_data.create_dataset('Fold_{}_y'.format(f+1),data=cv_data_valid_y[f])\n",
    "    \n",
    "    CV_fused_data.close()\n",
    "\n",
    "else:\n",
    "    test_fused_data = h5.File(fused_data_dir + test_fused_file,'a')\n",
    "    all_data = test_data['X']\n",
    "    test_fused_data.create_dataset('heldout_X_L_HC',data=all_data[:,:L_HC_offset])\n",
    "    test_fused_data.create_dataset('heldout_X_R_HC',data=all_data[:,L_HC_offset:L_HC_offset+R_HC_offset])\n",
    "    test_fused_data.create_dataset('heldout_X_CT',data=all_data[:,L_HC_offset+R_HC_offset:])    \n",
    "    test_fused_data.create_dataset('heldout_y',data=test_data['y'])\n",
    "    test_fused_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# K-fold validations (nested)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import grid_search\n",
    "import datetime\n",
    "import time\n",
    "import collections\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "model_list = ['LR', 'LR_L1', 'SVR', 'RFR']\n",
    "model_choice_id = 2\n",
    "model_choice = model_list[model_choice_id]\n",
    "inflated_train = False\n",
    "\n",
    "if model_choice == 'LR':\n",
    "    model_clf = LinearRegression()   \n",
    "    inner_loop = False #only needed to optimize hyper-params\n",
    "    feat_imp = False\n",
    "    if inflated_train:\n",
    "        print \"no trained model yet\"\n",
    "    else:\n",
    "        saved_model_name = 'LR_2015-10-13-16-38-28.pkl'\n",
    "    \n",
    "elif model_choice == 'LR_L1':\n",
    "    model_clf = Lasso()\n",
    "    hyper_params = {'alpha':[0.2, 0.1, 0.05, 0.01]} \n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = True\n",
    "    if clinical_scale[0] == 'ADAS13':\n",
    "        if inflated_train:\n",
    "            saved_model_name = 'LR_L1_ADAS13_inflated_train_parallel_2015-11-14-20-21-48.pkl'\n",
    "        else:\n",
    "            saved_model_name = 'LR_L1_ADAS13_2015-11-06-12-25-21.pkl'\n",
    "    else:\n",
    "        saved_model_name = 'LR_L1_MMSE_2015-10-16-16-27-29.pkl'\n",
    "    \n",
    "elif model_choice == 'SVR':\n",
    "    model_clf = SVR()\n",
    "    hyper_params = {'kernel':['linear','rbf'], 'C':[1,10]}\n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = True\n",
    "    if clinical_scale[0] == 'ADAS13':\n",
    "        if inflated_train:\n",
    "            saved_model_name = 'SVR_ADAS13_inflated_train_parallel_2015-11-27-18-43-41.pkl'\n",
    "        else:\n",
    "            saved_model_name = 'SVR_ADAS13_2015-11-27-18-48-33.pkl'\n",
    "    else:\n",
    "        saved_model_name = 'SVR_MMSE_2015-10-16-18-02-11.pkl'\n",
    "    \n",
    "elif model_choice == 'RFR':\n",
    "    model_clf = RandomForestRegressor(n_jobs=6)\n",
    "    hyper_params = {'n_estimators':[10,50,100,200,500],'min_samples_split':[2,4,8]}\n",
    "    #hyper_params = {'min_samples_split':[2,4,8,16]}\n",
    "    inner_loop = True #only needed to optimize hyper-params\n",
    "    feat_imp = True\n",
    "    if clinical_scale[0] == 'ADAS13':\n",
    "        if inflated_train:\n",
    "            #saved_model_name = 'RFR_ADAS13_inflated_train_parallel_2015-11-10-15-01-42.pkl'\n",
    "            saved_model_name = 'RFR_ADAS13_inflated_train_parallel_2015-11-16-14-21-15.pkl'\n",
    "        else:\n",
    "            saved_model_name = 'RFR_ADAS13_2015-11-05-18-02-11.pkl'\n",
    "    else:\n",
    "        saved_model_name = 'RFR_MMSE_2015-10-16-16-22-42.pkl'\n",
    "else:\n",
    "    print \"Unknown model choice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Outer Fold Computation\n",
    "\n",
    "def computeOuterFold(train_X, train_y, valid_X, valid_y, model_clf, hyper_params, inner_loop, save_model, save_model_path):\n",
    "    import collections\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn import grid_search\n",
    "    import datetime\n",
    "    import time\n",
    "    import collections\n",
    "    from scipy.stats import mode\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from scipy import stats\n",
    "    \n",
    "    print 'Starting Outerfold computation'\n",
    "    \n",
    "    hp_dict = collections.defaultdict(list) #store best hyper-parameters for each fold\n",
    "    \n",
    "    if inner_loop:     \n",
    "        print 'Starting Inner computation'\n",
    "        save_model_path_fold = save_model_path + '_fold_' \n",
    "        clf = innerCVLoop(model_clf,hyper_params,train_X, train_y,save_model,save_model_path_fold)\n",
    "        for hp in hyper_params:\n",
    "            hp_dict[hp].append(clf.best_estimator_.get_params()[hp])\n",
    "            \n",
    "        print 'Ending InnerFold computation'\n",
    "\n",
    "    else:\n",
    "        clf = model_clf\n",
    "        clf.fit(fold_X,fold_y)\n",
    "        \n",
    "    #CV_scores\n",
    "    r_train = stats.pearsonr(clf.predict(train_X),train_y)\n",
    "    r_valid = stats.pearsonr(clf.predict(valid_X),valid_y)\n",
    "        \n",
    "    R2_train = clf.score(train_X,train_y) \n",
    "    R2_valid = clf.score(valid_X,valid_y)\n",
    "        \n",
    "    MSE_train = mse(clf.predict(train_X),train_y)\n",
    "    MSE_valid = mse(clf.predict(valid_X),valid_y)\n",
    "    \n",
    "    print 'Ending OuterFold computation'\n",
    "    \n",
    "    return {'r_train':r_train, 'r_valid':r_valid, 'R2_train':R2_train, 'R2_valid':R2_valid,\n",
    "           'MSE_train':MSE_train, 'MSE_valid':MSE_valid, 'hp_dict':hp_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parallelize!!! \n",
    "from functools import partial\n",
    "\n",
    "inflated_train = False\n",
    "\n",
    "save_model_path = CV_model_dir + model_choice + '_' + clinical_scale[0]\n",
    "load_model_path = CV_model_dir + saved_model_name\n",
    "\n",
    "# train a new classifer? (if false then load a single pretrained classifier based on most frequent hyperparams found previously\n",
    "# This will NOT load N different classifiers each for outer-CV fold  \n",
    "train_clf = False\n",
    "\n",
    "if inflated_train:    \n",
    "    cv_X_train = cv_data_inflated['X_train']\n",
    "    cv_y_train = cv_data_inflated['y_train']\n",
    "else: \n",
    "    cv_X_train = cv_data['X_train']\n",
    "    cv_y_train = cv_data['y_train']\n",
    "    \n",
    "cv_X_valid = cv_data['X_valid']\n",
    "cv_y_valid = cv_data['y_valid']\n",
    "\n",
    "no_of_folds = len(cv_X_train)\n",
    "\n",
    "save_model = False #do you want to save all classifiers per each fold? \n",
    "save_model_path = CV_model_dir + model_choice + '_' + clinical_scale[0]\n",
    "\n",
    "if train_clf:\n",
    "    CV_r_train=[] #pearson r score for each outer fold on train set\n",
    "    CV_r_valid=[] #pearson r score for each outer fold on validation set\n",
    "\n",
    "    CV_R2_train=[] #R2 score for each outer fold on train set\n",
    "    CV_R2_valid=[] #R2 score for each outer fold on validation set\n",
    "\n",
    "    CV_MSE_train=[] #MSE for each outer fold on train set\n",
    "    CV_MSE_valid=[] #MSE for each outer fold on validation set\n",
    "    # outer CV loop     \n",
    "    rc = ipp.Client()\n",
    "    dview = rc[:]\n",
    "    dview.push(dict(computeOuterFold = computeOuterFold))\n",
    "    dview.push(dict(innerCVLoop = innerCVLoop))\n",
    "    mapfunc = partial(computeOuterFold, model_clf=model_clf, hyper_params=hyper_params, inner_loop=inner_loop, \n",
    "                  save_model=save_model, save_model_path=save_model_path)\n",
    "    parallel_result = dview.map_sync(mapfunc, cv_X_train,cv_y_train,cv_X_valid,cv_y_valid)    \n",
    "    hp_dict = collections.defaultdict(list)\n",
    "    for pr in parallel_result:\n",
    "        CV_r_train.append(pr['r_train'])\n",
    "        CV_r_valid.append(pr['r_valid'])\n",
    "        CV_R2_train.append(pr['R2_train'])\n",
    "        CV_R2_valid.append(pr['R2_valid'])\n",
    "        CV_MSE_train.append(pr['MSE_train'])\n",
    "        CV_MSE_valid.append(pr['MSE_valid'])\n",
    "        \n",
    "        for hp in hyper_params:\n",
    "            hp_dict[hp].append(pr['hp_dict'][hp])\n",
    "            \n",
    "    #retrain model on the entire train + valid set with most frequent hyper-params during cross-val\n",
    "    if inner_loop:\n",
    "        hp_mode = {}\n",
    "        for hp in hyper_params:\n",
    "            hp_mode[hp] = mode(hp_dict[hp])[0][0]\n",
    "            \n",
    "        print 'most frequent hp:' + str(hp_mode)\n",
    "    \n",
    "else: \n",
    "    #Grabs the best classifer as a result of N-fold nested CV along with the MSE and R2 stats of the outerloop\n",
    "    print \"Loading previously saved model: \"\n",
    "    f = open(load_model_path)\n",
    "    result = pickle.load(f)\n",
    "    test_clf = result['best_clf']\n",
    "    CV_r_valid = result['CV_r']\n",
    "    CV_R2_valid = result['CV_R2']\n",
    "    CV_MSE_valid = result['CV_MSE']\n",
    "    f.close()\n",
    "\n",
    "print 'CV r (mean, median, std_err): ' + '{:04.2f},{:04.2f},{:04.2f}'.format(np.mean(zip(*CV_r_valid)[0]),np.median(zip(*CV_r_valid)[0]),stats.sem(zip(*CV_r_valid)[0]))\n",
    "print 'CV R2 (mean, median, std_err): ' + '{:04.2f},{:04.2f}, {:04.2f}'.format(np.mean(CV_R2_valid),np.median(CV_R2_valid),stats.sem(CV_R2_valid))\n",
    "print 'CV MSE (mean, median, std_err): ' + '{:04.2f},{:04.2f}, {:04.2f}'.format(np.mean(CV_MSE_valid),np.median(CV_MSE_valid),stats.sem(CV_MSE_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVR (scaled)\n",
    "HC_CT (infl)\n",
    "most frequent hp:{'kernel': array(['rbf'], \n",
    "      dtype='|S3'), 'C': array([1])}\n",
    "CV r (mean, median, std_err): 0.54,0.54,0.03\n",
    "CV R2 (mean, median, std_err): 0.27,0.28, 0.03\n",
    "CV MSE (mean, median, std_err): 57.27,59.21, 2.73\n",
    "\n",
    "(simple)\n",
    "most frequent hp:{'kernel': array(['rbf'], \n",
    "      dtype='|S3'), 'C': array([10])}\n",
    "CV r (mean, median, std_err): 0.54,0.55,0.03\n",
    "CV R2 (mean, median, std_err): 0.27,0.29, 0.03\n",
    "CV MSE (mean, median, std_err): 57.23,58.26, 3.02\n",
    "\n",
    "\n",
    "RFR\n",
    "HC_CT (infl)\n",
    "most frequent hp:{'n_estimators': array([500]), 'min_samples_split': array([4])}\n",
    "CV r (mean, median, std_err): 0.51,0.51,0.03\n",
    "CV R2 (mean, median, std_err): 0.24,0.21, 0.03\n",
    "CV MSE (mean, median, std_err): 59.61,55.48, 2.92\n",
    "\n",
    "(simple)\n",
    "most frequent hp:{'n_estimators': array([100]), 'min_samples_split': array([2]), 'max_features': array([10])}\n",
    "CV r (mean, median, std_err): 0.54,0.55,0.03\n",
    "CV R2 (mean, median, std_err): 0.28,0.30, 0.03\n",
    "CV MSE (mean, median, std_err): 56.83,56.15, 3.15    \n",
    "    \n",
    "CT\n",
    "(infl)\n",
    "most frequent hp:{'n_estimators': array([500]), 'min_samples_split': array([2]), 'max_features': array([5])}\n",
    "CV r (mean, median, std_err): 0.49,0.49,0.03\n",
    "CV R2 (mean, median, std_err): 0.22,0.23, 0.03\n",
    "CV MSE (mean, median, std_err): 60.92,59.10, 2.90\n",
    "    \n",
    "(simple)\n",
    "most frequent hp:{'n_estimators': array([100]), 'min_samples_split': array([4]), 'max_features': array([5])}\n",
    "CV r (mean, median, std_err): 0.49,0.51,0.03\n",
    "CV R2 (mean, median, std_err): 0.22,0.25, 0.03\n",
    "CV MSE (mean, median, std_err): 60.83,58.77, 3.37\n",
    "    \n",
    "LR_L1\n",
    "HC + CT\n",
    "orig: most frequent hp:{'alpha': array([ 0.2])}\n",
    "CV r (mean, std_err): 0.51, 0.03\n",
    "CV R2 (mean, std_err): 0.25, 0.03\n",
    "CV MSE (mean, std_err): 58.99, 3.50\n",
    "\n",
    "infl: most frequent hp:{'alpha': array([ 0.1])}\n",
    "CV r (mean, std_err): 0.50, 0.03\n",
    "CV R2 (mean, std_err): 0.22, 0.02\n",
    "CV MSE (mean, std_err): 61.46, 3.76\n",
    "\n",
    "LR_L1 (scaled input)\n",
    "simple\n",
    "most frequent hp:{'alpha': array([ 0.2])}\n",
    "CV r (mean, median, std_err): 0.53,0.51,0.03\n",
    "CV R2 (mean, median, std_err): 0.26,0.23, 0.03\n",
    "CV MSE (mean, median, std_err): 58.04,60.30, 3.17\n",
    "\n",
    "infl\n",
    "most frequent hp:{'alpha': array([ 0.2])}\n",
    "CV r (mean, median, std_err): 0.53,0.52,0.03\n",
    "CV R2 (mean, median, std_err): 0.26,0.24, 0.03\n",
    "CV MSE (mean, median, std_err): 57.86,59.92, 3.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the test set based on most frequnt hyper-params\n",
    "if inflated_train:\n",
    "    save_model_path = CV_model_dir + model_choice + '_' + clinical_scale[0] + '_inflated_train_parallel'\n",
    "    exemplar_X = cv_X_train[0]\n",
    "    exemplar_y = cv_y_train[0]        \n",
    "else:\n",
    "    exemplar_X = cv_data['X']\n",
    "    exemplar_y = cv_data['y']\n",
    "    save_model_path = CV_model_dir + model_choice + '_' + clinical_scale[0]\n",
    "        \n",
    "if train_clf:\n",
    "    #test_clf = LinearRegression()  \n",
    "    #test_clf = Lasso(alpha=0.2)    \n",
    "    test_clf = SVR(kernel='rbf',C=10)\n",
    "    #test_clf = RandomForestRegressor(n_estimators=500,min_samples_split=4,n_jobs=4)\n",
    "   \n",
    "    save_model = True    \n",
    "    \n",
    "    test_clf.fit(exemplar_X,exemplar_y)\n",
    "    if save_model:\n",
    "        classifier_model_and_stats = {'best_clf':test_clf, 'CV_R2':CV_R2_valid, 'CV_MSE':CV_MSE_valid, 'CV_r': CV_r_valid}\n",
    "        save_classifier(classifier_model_and_stats,save_model_path)\n",
    "\n",
    "    # heldout set training perf\n",
    "    pearson_r_train = stats.pearsonr(test_clf.predict(exemplar_X),exemplar_y)\n",
    "    R2_train = test_clf.score(exemplar_X,exemplar_y)\n",
    "    MSE_train = mse(test_clf.predict(exemplar_X),exemplar_y)\n",
    "    \n",
    "    print \"train r: \" + str(pearson_r_train)\n",
    "    print \"train R2 score: \" + str(R2_train)\n",
    "    print \"test R2 score: \" + str(MSE_train)\n",
    "\n",
    "# heldout set test perf\n",
    "predicted_score = test_clf.predict(test_data['X'])\n",
    "actual_score = test_data['y']\n",
    "pearson_r_test = stats.pearsonr(predicted_score,actual_score)\n",
    "R2_test = test_clf.score(test_data['X'],actual_score)\n",
    "MSE_test = mse(predicted_score,actual_score)\n",
    "\n",
    "print \"test r: \" + str(pearson_r_test)\n",
    "print \"test R2: \" + str(R2_test)\n",
    "print \"test MSE: \" + str(MSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test r: (0.45497357666317029, 5.9285771224732239e-07)\n",
    "test R2: 0.206380314697\n",
    "test MSE: 66.6684854067\n",
    "    \n",
    "save_plt_data = True\n",
    "if save_plt_data: #save the data to be plotter later.. (montages etc..)\n",
    "        montage_dir = '/projects/nikhil/ADNI_prediction/caffe_output/montage_data/'\n",
    "        mydict = {'predicted_score': predicted_score, 'actual_score': actual_score}\n",
    "        output = open('{}corr_data_{}_T{}.pkl'.format(montage_dir,model_choice+'_infl',91), 'wb')\n",
    "        pickle.dump(mydict, output)\n",
    "        output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_saved_boxplots = True\n",
    "if load_saved_boxplots:\n",
    "    boxplot_config_r = collections.defaultdict(list)\n",
    "    boxplot_config_R2 = collections.defaultdict(list)\n",
    "    boxplot_config_MSE = collections.defaultdict(list)\n",
    "    boxplots_dir = '/projects/nikhil/ADNI_prediction/models/CV_pkls/'\n",
    "    saved_models = {'LR_L1_scaled': 'LR_L1_ADAS13_2015-11-27-18-29-55.pkl',\n",
    "                    'SVR_scaled':'SVR_ADAS13_2015-11-27-18-48-33.pkl', \n",
    "                    'RFR':'RFR_ADAS13_2015-11-05-18-02-11.pkl', \n",
    "                    'LR_L1_infl_scaled': 'LR_L1_ADAS13_inflated_train_parallel_2015-11-27-18-28-55.pkl',\n",
    "                    'SVR_infl_scaled':'SVR_ADAS13_inflated_train_parallel_2015-11-27-18-43-41.pkl',\n",
    "                    'RFR_infl':'RFR_ADAS13_inflated_train_parallel_2015-11-27-15-28-13.pkl'}\n",
    "    \n",
    "    # Not plotted:\n",
    "    # 'LR_L1': 'LR_L1_ADAS13_2015-11-06-12-25-21.pkl'\n",
    "    # 'LR_L1_infl':'LR_L1_ADAS13_inflated_train_parallel_2015-11-27-15-05-17.pkl'\n",
    "    \n",
    "    for key,val in saved_models.iteritems():\n",
    "        pkl_file = open(boxplots_dir + val, 'rb')\n",
    "        saved_data_r = pickle.load(pkl_file)\n",
    "        boxplot_config_r[key].append(zip(*saved_data_r['CV_r'])[0])\n",
    "        boxplot_config_MSE[key].append(saved_data_r['CV_MSE'])\n",
    "        pkl_file.close()    \n",
    "        \n",
    "else:\n",
    "    #print np.mean(zip(*CV_r_train)[0])\n",
    "    print np.mean(zip(*CV_r_valid)[0])\n",
    "\n",
    "    boxplot_config_r[model_choice].append((zip(*CV_r_valid)[0]))\n",
    "    boxplot_config_R2[model_choice].append(CV_R2_valid)\n",
    "    boxplot_config_MSE[model_choice].append(CV_MSE_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Boxplots for CV statistics (r, mse, R2)\n",
    "stat_measure_list = [boxplot_config_r,boxplot_config_MSE]\n",
    "stat_measure_names = ['Pearson r', 'MSE']\n",
    "from matplotlib.artist import setp\n",
    "plt.figure()\n",
    "font_small = 12\n",
    "font_med = 16\n",
    "font_large = 24\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "custom_cols = ['LR_L1_scaled','LR_L1_infl_scaled', 'SVR_scaled', 'SVR_infl_scaled', 'RFR', 'RFR_infl']\n",
    "for sm, stat_measure in enumerate(stat_measure_list):\n",
    "    plt.subplot(2,1,sm+1)\n",
    "    df_array = pd.DataFrame(dict([ (k,pd.Series(v[0])) for k,v in stat_measure.iteritems() ]))\n",
    "    bplot = df_array.boxplot(column=custom_cols, fontsize=font_large)\n",
    "    #plt.xlabel('Method',fontsize=font_large)\n",
    "    plt.ylabel(stat_measure_names[sm],fontsize=font_large)\n",
    "    plt.xticks(fontsize=font_med)\n",
    "    plt.yticks(fontsize=font_small)\n",
    "    setp(bplot['boxes'], linewidth=2, color='crimson')\n",
    "    setp(bplot['whiskers'], linewidth=2, color='crimson')\n",
    "    setp(bplot['fliers'], linewidth=2,color='crimson')\n",
    "    setp(bplot['medians'], linewidth=2,color='purple')\n",
    "    \n",
    "save_fig = True\n",
    "if save_fig:\n",
    "    fig1 = plt.gcf()\n",
    "    fig1.savefig('/projects/nikhil/ADNI_prediction/visuals/Baseline_Models_Corr_MSE.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute scores and MSEs\n",
    "\n",
    "y_cv_pred = test_clf.predict(exemplar_X)\n",
    "y_test_pred = test_clf.predict(test_data['X'])\n",
    "\n",
    "x_data_array = [exemplar_y,test_data['y']]\n",
    "y_data_array = [y_cv_pred,y_test_pred]\n",
    "lable_array = ['CV train performance','test performance']\n",
    "\n",
    "# only test perf\n",
    "#x_data_array = [test_data['y']]\n",
    "#y_data_array = [y_test_pred]\n",
    "#lable_array = ['test performance']\n",
    "\n",
    "plt.figure()\n",
    "font_small = 8\n",
    "font_med = 16\n",
    "font_large = 24\n",
    "no_of_plots = len(lable_array)\n",
    "plt.style.use('ggplot')\n",
    "plt_col = no_of_plots\n",
    "plt_row = 1\n",
    "\n",
    "for i in np.arange(no_of_plots):\n",
    "    x = x_data_array[i]\n",
    "    y = y_data_array[i]\n",
    "\n",
    "    plt.subplot(plt_row,plt_col,i+1)\n",
    "    plt.scatter(x, y, c='crimson', label=lable_array[i],s=40)\n",
    "    fit = np.polyfit(x,y,1)\n",
    "    fit_fn = np.poly1d(fit) \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    if p_value < 0.0001:\n",
    "        p_value_sig = '<0.0001'\n",
    "    else:\n",
    "        p_value_sig = str(p_value)\n",
    "        \n",
    "    label_str = 'r-value: {:04.2f}'.format(r_value) + '\\n' + 'p-value: ' + p_value_sig + '\\n' + 'std_err: {:04.2f}'.format(std_err) \n",
    "    # fit_fn is now a function which takes in x and returns an estimate for y\n",
    "    plt.plot(x, fit_fn(x),linewidth=3, label=label_str)\n",
    "    plt.title(model_choice,fontsize=font_large)\n",
    "    plt.xlabel('Actual Score',fontsize=font_large)\n",
    "    plt.ylabel('Predicted Score',fontsize=font_large)            \n",
    "    plt.legend(fontsize=font_med,loc=2)\n",
    "\n",
    "if feat_imp:\n",
    "    plt.figure()\n",
    "    #plt.subplot(plt_row,plt_col,4)\n",
    "    x_pos = np.arange(len(feature_cols))\n",
    "    \n",
    "    if model_choice == 'RFR':\n",
    "        feature_wts = test_clf.feature_importances_\n",
    "    elif model_choice == 'SVR':\n",
    "        feature_wts = np.squeeze(test_clf.coef_)\n",
    "    elif model_choice == 'LR_L1':\n",
    "        feature_wts = np.squeeze(test_clf.coef_)\n",
    "    else: \n",
    "        print 'no feature_wt vector found'\n",
    "        \n",
    "    sorted_feat_idx = np.argsort(np.abs(feature_wts))[::-1]        \n",
    "    plt.bar(x_pos,feature_wts[sorted_feat_idx],color='crimson')\n",
    "    plt.ylabel('Feature Importance',fontsize=font_large)\n",
    "    #Sort the feature name list as well \n",
    "    sorted_feature_cols = []\n",
    "    for i,sort_idx in enumerate(sorted_feat_idx):\n",
    "        sorted_feature_cols.append(feature_cols[sort_idx])\n",
    "\n",
    "    plt.xticks(x_pos,sorted_feature_cols,rotation=70,fontsize=font_small)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot montages (across methods)\n",
    "mont_data_dir = '/projects/nikhil/ADNI_prediction/caffe_output/montage_data/'\n",
    "cats = ['LR_L1','SVR','RFR','LR_L1_infl','SVR_infl','RFR_infl'] #Montage Categories: methods / modalities\n",
    "file_str = 'corr_data_{}_T1.pkl'\n",
    "#cats = ['L_HC','R_HC','HC','HC_CT'] #Montage Categories: methods / modalities\n",
    "#file_str = 'corr_data_{}_T11.pkl'\n",
    "\n",
    "n_rows = 2\n",
    "n_cols = 3\n",
    "\n",
    "plt.figure()\n",
    "plt.style.use('ggplot')\n",
    "font_small = 8\n",
    "font_med = 16\n",
    "font_large = 24\n",
    "\n",
    "for c, cat in enumerate(cats):\n",
    "    if cat == 'HC_CT':\n",
    "        file_str = 'corr_data_{}_T11.pkl'\n",
    "        \n",
    "    pkl_file = open(mont_data_dir + file_str.format(cat), 'rb')\n",
    "    montage_data = pickle.load(pkl_file)\n",
    "    pkl_file.close()    \n",
    "    x = montage_data['predicted_score']\n",
    "    y = montage_data['actual_score']\n",
    "        \n",
    "    plt.subplot(n_rows,n_cols,c+1)\n",
    "    plt.scatter(x, y, c='crimson',s=20)\n",
    "    fit = np.polyfit(x,y,1)\n",
    "    fit_fn = np.poly1d(fit) \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)      \n",
    "    if p_value < 0.0001:\n",
    "        p_value_sig = '<0.0001'\n",
    "    else:\n",
    "        p_value_sig = '{:.5f}'.format(p_value)\n",
    "        \n",
    "    label_str = 'r-value: {:04.2f}'.format(r_value) + '\\n' + 'p-value: ' + p_value_sig + '\\n' + 'std_err: {:04.2f}'.format(std_err) \n",
    "    # fit_fn is now a function which takes in x and returns an estimate for y\n",
    "    plt.plot(x, fit_fn(x),linewidth=3, label=label_str)        \n",
    "    title_str = 'Heldout Testset, modality: {}'.format(cat)  #heldout Test set\n",
    "    plt.title(title_str,fontsize=font_med)\n",
    "    plt.xlabel('Actual Score',fontsize=font_med)\n",
    "    plt.ylabel('Predicted Score',fontsize=font_med)            \n",
    "    plt.legend(fontsize=font_med,loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Additional Scripts \n",
    "# Concat of Train + Valid (to generate multi-folds)\n",
    "train_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_train.pkl'\n",
    "valid_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_valid.pkl'\n",
    "t_data = pd.read_pickle(train_file)\n",
    "v_data = pd.read_pickle(valid_file)\n",
    "frames = [t_data, v_data]\n",
    "result = pd.concat(frames)\n",
    "result.to_pickle(\"/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_seg_fused_train_plus_val.pkl\")\n",
    "\n",
    "# Generatng K-Folds\n",
    "#sampx = 100 #Train + Valid samples\n",
    "#foldx = 10   \n",
    "#kf = KFold(sampx, n_folds=foldx,shuffle=True)\n",
    "\n",
    "#for train, test in kf:\n",
    "#    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save k-fold UIDs\n",
    "kf_uid_file = '/projects/nikhil/ADNI_prediction/input_datasets/cli_ct_heldout_KFold_UIDs.pkl'\n",
    "heldout_UIDs={'heldout_UIDs':test_data['U']}\n",
    "f = open(kf_uid_file, 'wb')\n",
    "pickle.dump(heldout_UIDs, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
