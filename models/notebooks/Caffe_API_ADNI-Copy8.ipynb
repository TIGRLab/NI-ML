{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Code for running Caffe toolbox\n",
    "* **Objectives:** \n",
    "    1. Define feed-forward partitioned nets (APANN)\n",
    "    2. Train and test nets on HC, CT and HC+CT input modalities        \n",
    "        a. Hyper-parameter search\n",
    "        b. 10 rounds 10-kf validation\n",
    "\n",
    "\n",
    "* **Other functions:**\n",
    "    1. Net surgery to copy weights from pre-trained nets \n",
    "\n",
    "\n",
    "* **Useful links**\n",
    "    \n",
    "    1. http://stackoverflow.com/questions/33140000/how-to-feed-caffe-multi-label-data-in-hdf5-format\n",
    "    2. http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "    3. http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import caffe\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle\n",
    "import subprocess as sub\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tables as tb\n",
    "from sklearn import preprocessing\n",
    "import collections\n",
    "\n",
    "caffe_root = '/home/nikhil/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/'\n",
    "os.chdir(baseline_dir)\n",
    "\n",
    "caffe.set_device(3)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some defs\n",
    "#Loads hdf datasets with a preprocessing option\n",
    "def load_data(data_path, input_node, preproc):\n",
    "    data = tb.open_file(data_path, 'r')\n",
    "    X_raw = data.get_node('/' + input_node)[:]\n",
    "    if preproc == 'scale':\n",
    "        X = preprocessing.scale(X_raw)\n",
    "    elif preproc == 'norm_max':\n",
    "        X = preprocessing.normalize(X_raw, norm='max')\n",
    "    elif preproc == 'norm_l2':\n",
    "        X = preprocessing.normalize(X_raw, norm='l2')\n",
    "    else:\n",
    "        X = X_raw\n",
    "    data.close()\n",
    "    return X\n",
    "\n",
    "# Save data to disk\n",
    "def pickleIt(my_data,save_path):\n",
    "    f = open(save_path, 'wb')\n",
    "    pickle.dump(my_data, f)\n",
    "    f.close()\n",
    "\n",
    "# extract encodings (specifically multi-task outputs) from trained net\n",
    "def extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers, multi_task):\n",
    "    os.chdir(os.path.dirname(net_file))\n",
    "    net = caffe.Net(net_file, model_file, caffe.TEST)        \n",
    "    \n",
    "    #print net.blobs.items()[0]\n",
    "    #print net.blobs.items()[1]\n",
    "    \n",
    "    #Get weights    \n",
    "    layer_list = weight_layers\n",
    "    wt_dict = collections.OrderedDict()\n",
    "    for l, name in enumerate(net._layer_names):            \n",
    "        if name in layer_list:\n",
    "            wt_dict[name] = net.layers[l].blobs[0].data\n",
    "    \n",
    "    BATCH_SIZE = batch_size        \n",
    "    N = load_data(data_path, input_nodes[0],'no_preproc').shape[0]\n",
    "    iters = int(np.ceil(N / float(BATCH_SIZE)))\n",
    "\n",
    "    if not multi_task:\n",
    "        code_layer = net.blobs[encoding_layer]\n",
    "        out_shape = code_layer.data.shape    \n",
    "        X_out = np.zeros(shape=(N, out_shape[1]))        \n",
    "        #print 'X_out.shape: {}'.format(X_out.shape)\n",
    "        \n",
    "    else:\n",
    "        code_layer_adas13 = net.blobs[encoding_layer + '_ADAS13']\n",
    "        code_layer_mmse = net.blobs[encoding_layer + '_MMSE']\n",
    "        out_shape = code_layer_adas13.data.shape \n",
    "        X_out_adas13 = np.zeros(shape=(N, out_shape[1]))\n",
    "        X_out_mmse = np.zeros(shape=(N, out_shape[1]))\n",
    "        #print 'X_out.shape: {},{}'.format(X_out_adas13.shape,X_out_mmse.shape)\n",
    "    \n",
    "    X_list = []\n",
    "    data_layers = []\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        X_list.append(load_data(data_path, input_node,'no_preproc'))\n",
    "        #print net.blobs[input_node].shape\n",
    "        \n",
    "        data_layers.append(net.blobs[input_node])    \n",
    "        #print 'X_list shape: {}'.format(X_list[i].shape)\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "        data_layers[i].reshape(BATCH_SIZE, X_list[i].shape[1]) # TODO: only works for 2-D inputs\n",
    "        #print 'data_layers shape: {}'.format(data_layers[i].data.shape)\n",
    "    \n",
    "     \n",
    "    net.reshape()            \n",
    "    #print 'Extracting features from data...'\n",
    "    \n",
    "    for i in xrange(iters):\n",
    "        #print '.',\n",
    "        for m, X in enumerate(X_list):\n",
    "            X_b = X[i * BATCH_SIZE: (i+1) * BATCH_SIZE,:]\n",
    "            batch_sampx = X_b.shape[0]\n",
    "            # Pad last batch with zeros\n",
    "            if X_b.shape[0] < BATCH_SIZE:\n",
    "                #print 'Zero-padding last batch with {} rows'.format(BATCH_SIZE-X_b.shape[0])\n",
    "                X_b = np.vstack((X_b,np.zeros((BATCH_SIZE-X_b.shape[0],X_b.shape[1]))))                       \n",
    "            \n",
    "            data_layers[m].data[...] = X_b\n",
    "            \n",
    "        net.forward()\n",
    "        \n",
    "        if not multi_task:\n",
    "            X_out[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer.data[0:batch_sampx,:].copy()\n",
    "        else:\n",
    "            X_out_adas13[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_adas13.data[0:batch_sampx,:].copy()\n",
    "            X_out_mmse[i * BATCH_SIZE: min((i+1) * BATCH_SIZE, N)] = code_layer_mmse.data[0:batch_sampx,:].copy()\n",
    "            X_out = {'adas13':X_out_adas13,'mmse':X_out_mmse}\n",
    "    return {'X_out':X_out, 'wt_dict':wt_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defs for defining multi-task nets for different input modalities \n",
    "* **Input modalities:** \n",
    "    1. HC segmentations (L+R)\n",
    "    2. CT values\n",
    "    3. Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def adninet_ff_HC(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1, concat_param=dict(axis=1))\n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "  \n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "    else:\n",
    "        #n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['ff4'], param=dict(lr_mult=1), weight_filler=dict(type='gaussian',std=0.177))\n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff3, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def adninet_ff_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=2) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_CT_SpecCluster_dyn,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=3) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    n.ff1 = L.InnerProduct(n.X_CT_SpecCluster_dyn, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT'])) \n",
    "\n",
    "    #Task Layers\n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff1, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff1, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)        \n",
    "    else:\n",
    "        n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEnL2 = L.ReLU(n.ff2, in_place=True)\n",
    "        n.dropL2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff2, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    return n.to_proto()\n",
    "\n",
    "\n",
    "def adninet_ff_HC_CT(hdf5, batch_size, node_sizes, dr, lr, tr, Clinical_Scale):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    #------- Input -----------------#\n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas,n.mmse  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=4) #orig\n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.X_L_HC,n.X_R_HC,n.X_CT_SpecCluster_dyn,n.adas, n.dx_cat3  = L.HDF5Data(name='data',batch_size=batch_size, source=hdf5,ntop=5) #orig\n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "        \n",
    "    #-------Hidden Layers-----------#\n",
    "    #ff layers Left HC\n",
    "    #n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['L_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.L_ff1 = L.InnerProduct(n.X_L_HC, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnL1 = L.ReLU(n.L_ff1, in_place=True)\n",
    "    n.dropL1 = L.Dropout(n.L_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['L_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.L_ff2 = L.InnerProduct(n.L_ff1, num_output=node_sizes['HC_L_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnL2 = L.ReLU(n.L_ff2, in_place=True)\n",
    "#     n.dropL2 = L.Dropout(n.L_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers Right HC\n",
    "    #n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['R_ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.R_ff1 = L.InnerProduct(n.X_R_HC, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEnR1 = L.ReLU(n.R_ff1, in_place=True)\n",
    "    n.dropR1 = L.Dropout(n.R_ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    #n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['R_ff2'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.177))\n",
    "#     n.R_ff2 = L.InnerProduct(n.R_ff1, num_output=node_sizes['HC_R_ff'], param=dict(lr_mult=lr['HC']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEnR2 = L.ReLU(n.R_ff2, in_place=True)\n",
    "#     n.dropR2 = L.Dropout(n.R_ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['HC']))\n",
    "    \n",
    "    #ff layers CT\n",
    "    #n.ff1 = L.InnerProduct(n.X_CT, num_output=node_sizes['ff1'], param=dict(lr_mult=2), weight_filler=dict(type='gaussian',std=0.008))\n",
    "    n.ff1 = L.InnerProduct(n.X_CT_SpecCluster_dyn, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn1 = L.ReLU(n.ff1, in_place=True)\n",
    "    n.drop1 = L.Dropout(n.ff1, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))    \n",
    "#     n.ff2 = L.InnerProduct(n.ff1, num_output=node_sizes['CT_ff'], param=dict(lr_mult=lr['CT']), weight_filler=dict(type='xavier'))\n",
    "#     n.NLinEn2 = L.ReLU(n.ff2, in_place=True)\n",
    "#     n.drop2 = L.Dropout(n.ff2, in_place=True,dropout_param=dict(dropout_ratio=dr['CT']))\n",
    "    \n",
    "    #ff layers Concat\n",
    "    n.concat = L.Concat(n.L_ff1,n.R_ff1,n.ff1, concat_param=dict(axis=1))   \n",
    "    n.ff3 = L.InnerProduct(n.concat, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=lr['COMB']), weight_filler=dict(type='xavier'))\n",
    "    n.NLinEn3 = L.ReLU(n.ff3, in_place=True)\n",
    "    n.dropC1 = L.Dropout(n.ff3, in_place=True,dropout_param=dict(dropout_ratio=dr['COMB']))\n",
    "\n",
    "    #Task layers    \n",
    "    if Clinical_Scale == 'BOTH': ## Split layers (multitask)    \n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)        \n",
    "        \n",
    "        n.ff1_MMSE = L.InnerProduct(n.ff3, num_output=node_sizes['MMSE_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1MMSE = L.ReLU(n.ff1_MMSE, in_place=True)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.ff1_ADAS13 = L.InnerProduct(n.ff3, num_output=node_sizes['ADAS_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1ADAS13 = L.ReLU(n.ff1_ADAS13, in_place=True)\n",
    "        n.ff1_DX = L.InnerProduct(n.ff3, num_output=node_sizes['DX_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLin1DX = L.ReLU(n.ff1_DX, in_place=True)\n",
    "        \n",
    "    else:        \n",
    "        n.ff4 = L.InnerProduct(n.ff3, num_output=node_sizes['COMB_ff'], param=dict(lr_mult=1), weight_filler=dict(type='xavier'))\n",
    "        n.NLinEn4 = L.ReLU(n.ff4, in_place=True)\n",
    "    \n",
    "    #--------Output--------------#\n",
    "    #n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='gaussian',std=0.177))          \n",
    "    if Clinical_Scale == 'ADAS13':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.adas)            \n",
    "    elif Clinical_Scale == 'MMSE':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.EuclideanLoss(n.output, n.mmse)\n",
    "    elif Clinical_Scale == 'BOTH':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_MMSE = L.InnerProduct(n.ff1_MMSE, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.MMSE_loss = L.EuclideanLoss(n.output_MMSE, n.mmse,loss_weight=tr['MMSE'])  \n",
    "    elif Clinical_Scale == 'DX':\n",
    "        n.output = L.InnerProduct(n.ff4, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.loss = L.SoftmaxWithLoss(n.output, n.dx_cat3)\n",
    "        \n",
    "    elif Clinical_Scale == 'ADAS13_DX':\n",
    "        n.output_ADAS13 = L.InnerProduct(n.ff1_ADAS13, num_output=1, weight_filler=dict(type='xavier'))\n",
    "        n.ADAS13_loss = L.EuclideanLoss(n.output_ADAS13, n.adas,loss_weight=tr['ADAS'])          \n",
    "        n.output_DX = L.InnerProduct(n.ff1_DX, num_output=3, weight_filler=dict(type='xavier'))\n",
    "        n.DX_loss = L.SoftmaxWithLoss(n.output_DX, n.dx_cat3,loss_weight=tr['DX'])  \n",
    "    \n",
    "    else:\n",
    "        print 'Unknow Clinical Scale: {}'.format(Clinical_Scale)\n",
    "    \n",
    "    return n.to_proto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Def for running caffe with a given solver and collecting error logs\n",
    "def run_caffe(solver,niter,batch_size,multi_task):\n",
    "    # each output is (batch size, feature dim, spatial dim)\n",
    "    #print [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    test_interval = 500\n",
    "    test_iter = 20    \n",
    "    test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "    if not multi_task:\n",
    "        train_loss = zeros(niter)\n",
    "        test_loss = zeros(int(np.ceil(niter / test_interval)))  \n",
    "\n",
    "    else: \n",
    "        train_ADAS13_loss = zeros(niter)\n",
    "        test_ADAS13_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "        train_MMSE_loss = zeros(niter)\n",
    "        test_MMSE_loss = zeros(int(np.ceil(niter / test_interval)))\n",
    "\n",
    "    #output = zeros((niter, batch_size))\n",
    "    #solver.restore()\n",
    "    \n",
    "    #the main solver loop\n",
    "    for it in range(niter):\n",
    "        #solver.net.forward()\n",
    "        solver.step(1)  # SGD by Caffe    \n",
    "        # store the train loss\n",
    "        if not multi_task:\n",
    "            train_loss[it] = solver.net.blobs['loss'].data        \n",
    "        else:             \n",
    "            train_ADAS13_loss[it] = solver.net.blobs['ADAS13_loss'].data\n",
    "            train_MMSE_loss[it] = solver.net.blobs['MMSE_loss'].data\n",
    "\n",
    "        # store the output on the first test batch\n",
    "        # (start the forward pass at conv1 to avoid loading new data)\n",
    "        #solver.test_nets[0].forward()\n",
    "        #output[it] = solver.test_nets[0].blobs['output'].data\n",
    "\n",
    "        # run a full test every so often\n",
    "        # (Caffe can also do this for us and write to a log, but we show here\n",
    "        #  how to do it directly in Python, where more complicated things are easier.)\n",
    "        if it % test_interval == 0:        \n",
    "            t_loss = 0\n",
    "            t_ADAS13_loss = 0\n",
    "            t_MMSE_loss = 0\n",
    "            t_DX_loss = 0\n",
    "            correct = 0\n",
    "            for test_it in range(test_iter):\n",
    "                solver.test_nets[0].forward()                \n",
    "                if not multi_task:\n",
    "                    t_loss += solver.test_nets[0].blobs['loss'].data                    \n",
    "                else:                                         \n",
    "                    t_ADAS13_loss += solver.test_nets[0].blobs['ADAS13_loss'].data\n",
    "                    t_MMSE_loss += solver.test_nets[0].blobs['MMSE_loss'].data\n",
    "\n",
    "            if not multi_task:\n",
    "                test_loss[it // test_interval] = t_loss/(test_iter)                \n",
    "                print 'Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}, test acc: {}'.format(\n",
    "                    it, train_loss[it], np.sum(train_loss)/it, test_loss[it // test_interval], test_acc[it // test_interval])\n",
    "            else:\n",
    "                test_ADAS13_loss[it // test_interval] = t_ADAS13_loss/(test_iter)\n",
    "                test_MMSE_loss[it // test_interval] = t_MMSE_loss/(test_iter)             \n",
    "\n",
    "                print 'ADAS Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                    it, train_ADAS13_loss[it], np.sum(train_ADAS13_loss)/it, test_ADAS13_loss[it // test_interval])\n",
    "                print 'MMSE Loss Iteration: {}, train loss(batch, sum): ({},{}), test loss: {}'.format(\n",
    "                    it, train_MMSE_loss[it], np.sum(train_MMSE_loss)/it, test_MMSE_loss[it // test_interval])\n",
    "\n",
    "    if not multi_task:\n",
    "        perf = {'train_loss':[train_loss],'test_loss':[test_loss],'test_acc':[test_acc]}\n",
    "    else:\n",
    "        perf = {'train_loss':[train_ADAS13_loss,train_MMSE_loss],'test_loss':[test_ADAS13_loss,test_MMSE_loss]}\n",
    "\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "\n",
    "### define solver\n",
    "def adni_solver(train_net_path, test_net_path,solver_configs,snap_prefix):    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    \n",
    "    # Set a seed for reproducible experiments:\n",
    "    # this controls for randomization in training.\n",
    "    s.random_seed = 0xCAFFE\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 500  # Test after every 500 training iterations.\n",
    "    s.test_iter.append(30) # Test on 100 batches each time we test.\n",
    "\n",
    "    s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    # EDIT HERE to try different solvers\n",
    "    # solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "    #s.solver_type = \"Nesterov\"\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    #s.base_lr = 0.00001  # EDIT HERE to try different learning rates\n",
    "    s.base_lr = solver_configs['base_lr']\n",
    "    # Set momentum to accelerate learning by\n",
    "    # taking weighted average of current and previous updates.\n",
    "    #if not s.type == \"AdaGrad\":\n",
    "    #    s.momentum = 0.9\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # This is the same policy as our default LeNet.\n",
    "    s.lr_policy = \"step\"\n",
    "    s.stepsize = 100000\n",
    "    s.gamma = 0.5\n",
    "    #s.power = 0.75\n",
    "    # EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "    # `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "    # s.lr_policy = 'fixed'\n",
    "    \n",
    "    # Set weight decay to regularize and prevent overfitting\n",
    "    #s.weight_decay = 1e-3\n",
    "    s.weight_decay = solver_configs['wt_decay']\n",
    "    \n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every 5K iterations -- twice during training.\n",
    "    s.snapshot = 2000\n",
    "    s.snapshot_prefix = snap_prefix\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# Deign Net Architecutre and Run Caffe\n",
    "* **Training loops:** \n",
    "    1. KF: 10-fold crossvalidation \n",
    "    2. MC: 10 monte-carlo rounds 10-fold CV\n",
    "    3. hyper-parameter search during each training round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MC # 1, Hype # hyp4, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold10/pretrained_models/ADNI2_ff_hyp5_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (157.434051514,inf), test loss: 166.959426498\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (424.470733643,inf), test loss: 385.654095459\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (43.2947845459,83.6989811821), test loss: 45.8994345665\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.72325456142,28.3010620626), test loss: 2.95185936689\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (37.5349960327,67.2675491986), test loss: 47.6531564236\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.05800199509,15.534906147), test loss: 2.99813116789\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (58.3781394958,61.5176280298), test loss: 48.9545495033\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.25556159019,11.2393385759), test loss: 3.17205613703\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (17.2977905273,58.5485888829), test loss: 49.84263587\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.91606974602,9.0661839067), test loss: 3.21274686158\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (29.6430206299,56.5850156599), test loss: 45.4256120443\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.863447189331,7.74748176938), test loss: 3.05844490528\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (59.9423980713,55.2796832264), test loss: 42.8175143242\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.26234674454,6.85511642752), test loss: 3.23427430391\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (133.253356934,54.1909587108), test loss: 47.6311264992\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.50238740444,6.20591140293), test loss: 3.71321912408\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (65.6427841187,53.2801090629), test loss: 48.0084322929\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.02014994621,5.71528581958), test loss: 3.51061824262\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (101.576728821,52.5708796451), test loss: 42.1799834013\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.84923315048,5.32800903551), test loss: 2.95143324435\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (39.4306335449,51.86266658), test loss: 42.154802084\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.31859898567,5.0112600049), test loss: 2.96584072113\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (41.3419456482,51.2383700745), test loss: 41.0031682968\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.57247018814,4.75318636758), test loss: 2.85131892562\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (35.8280181885,50.6716424616), test loss: 42.8143949986\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.664221405983,4.53328257178), test loss: 2.91085256934\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (49.5419807434,50.1243468239), test loss: 42.3486948729\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.44296371937,4.34493769687), test loss: 3.02983696759\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (22.4530677795,49.5792855319), test loss: 40.1595434666\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.10678911209,4.18364493899), test loss: 2.84699113667\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (32.5575294495,49.0730586337), test loss: 39.549894619\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.06676888466,4.04133298732), test loss: 2.88952296376\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (30.1542472839,48.5616737956), test loss: 42.2798076153\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.3677740097,3.91488788716), test loss: 3.09870598018\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (23.6919994354,48.0551513979), test loss: 42.7581935406\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.43115079403,3.80365565909), test loss: 3.18296900988\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (29.6901283264,47.561483049), test loss: 38.821191597\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.3999838829,3.70290974112), test loss: 3.12944438756\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (50.1180381775,47.0626406481), test loss: 37.2733664036\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.21235799789,3.61176658636), test loss: 3.33807157129\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (7.11421442032,46.5661607635), test loss: 40.8909916997\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.87223958969,3.5288713785), test loss: 3.8494068414\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (12.3575363159,46.0586047889), test loss: 39.8827488899\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.01554787159,3.45404822538), test loss: 3.62574691474\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (17.6151390076,45.5739585921), test loss: 35.256193614\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.29439020157,3.38500974699), test loss: 3.46162022948\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (36.0816802979,45.0664002205), test loss: 34.8778796911\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.352297723293,3.32082315616), test loss: 2.95421048403\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (17.4964103699,44.5623990925), test loss: 32.9408452511\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.89955997467,3.26228955562), test loss: 2.84617286623\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (92.347114563,44.0780497884), test loss: 34.8571297169\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (3.30966210365,3.20794586117), test loss: 3.041399315\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (21.7327346802,43.5754561733), test loss: 34.5637587547\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.45928549767,3.15640031114), test loss: 3.05791571736\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (20.3382835388,43.0723818004), test loss: 33.5354075909\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.36619997025,3.10970666608), test loss: 3.01993090212\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (22.4704532623,42.5823008315), test loss: 32.5635118008\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.643691062927,3.06525125393), test loss: 2.98457272947\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (31.4515476227,42.0933113954), test loss: 34.5793724537\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.3381986618,3.02340516142), test loss: 3.16511969566\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (22.8915958405,41.5960505022), test loss: 34.1465094328\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.816002368927,2.98484546722), test loss: 3.13631349057\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (9.25218486786,41.1131359978), test loss: 33.7731020451\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.5021905899,2.94805805859), test loss: 3.24420683682\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (29.0990486145,40.6356628982), test loss: 31.2662214279\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.69905233383,2.9130300399), test loss: 3.43310374171\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (18.6511459351,40.157534367), test loss: 33.932692194\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.11405134201,2.88051155419), test loss: 3.42477011979\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (21.138715744,39.6906257658), test loss: 33.1943460226\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.12835621834,2.84935683066), test loss: 3.66630584002\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (22.6067466736,39.2321735358), test loss: 34.1795070171\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.1003446579,2.81965812399), test loss: 3.60517528504\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (8.31141376495,38.7799169558), test loss: 28.9639632225\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.03748440742,2.79124407014), test loss: 2.92770298719\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (11.4314508438,38.3343084352), test loss: 27.8522853851\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.81673800945,2.76467182615), test loss: 2.9378549546\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (24.5429916382,37.9078673419), test loss: 29.1491428375\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (3.97686052322,2.7390218406), test loss: 2.99490475059\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (11.803237915,37.4819268663), test loss: 29.9443122864\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.535218715668,2.7141329764), test loss: 3.01599171013\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (25.0186271667,37.0667634209), test loss: 30.2614424229\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.26126885414,2.69072907335), test loss: 3.02301194221\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (51.9577903748,36.6708225512), test loss: 29.8862353504\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.67767119408,2.66827145272), test loss: 3.00517374277\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (15.677230835,36.2806569649), test loss: 30.0935901165\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.94273495674,2.64618555136), test loss: 3.12505115867\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (8.96426200867,35.8963523911), test loss: 30.4909842968\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.33359313011,2.62575486604), test loss: 3.0938468039\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (21.343421936,35.5299476162), test loss: 31.4654465675\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.746522665024,2.60568620764), test loss: 3.34041389823\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (29.6840553284,35.1750090337), test loss: 29.4511280775\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.901980996132,2.58622466789), test loss: 3.49557159133\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (20.0856666565,34.8221053302), test loss: 28.1024816275\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.908029675484,2.56798248343), test loss: 3.31940806359\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (14.7980747223,34.485371951), test loss: 32.137128973\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.76915717125,2.55009280172), test loss: 3.84168601781\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (21.4630851746,34.1597392108), test loss: 33.1199177742\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.987756490707,2.53265441548), test loss: 3.49335656017\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (21.1523990631,33.8383373949), test loss: 27.9954419613\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.50421047211,2.51623592421), test loss: 2.93900112808\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (21.4338493347,33.5291059306), test loss: 27.3331102371\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.90808522701,2.50015551373), test loss: 2.9158824563\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (17.1754989624,33.2300003261), test loss: 27.8851850033\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.01017403603,2.4845538285), test loss: 2.92429634333\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (14.1745710373,32.9368619156), test loss: 29.0425548792\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.17539453506,2.46932041055), test loss: 3.01153998077\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (21.8322639465,32.6513149949), test loss: 29.8089490652\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.52882623672,2.45500002032), test loss: 2.94351643324\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (30.2397212982,32.3781851578), test loss: 30.4585845947\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.34554624557,2.44092882972), test loss: 3.06144509912\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (4.71889686584,32.1069733979), test loss: 30.4353127599\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.18306136131,2.42704300247), test loss: 3.00191551745\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (17.0621795654,31.8425986499), test loss: 29.8292470217\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.630885481834,2.41388022279), test loss: 3.09305017591\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (45.8429412842,31.5905461499), test loss: 31.1538033962\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.41551280022,2.40112551047), test loss: 3.30355501175\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (13.4492874146,31.3426445939), test loss: 29.0097654104\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (2.0649356842,2.38830489323), test loss: 3.24715533406\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (14.8640289307,31.0978738242), test loss: 27.6142228842\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.46030676365,2.3764309293), test loss: 3.31863206774\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (16.9092082977,30.863517299), test loss: 31.4419269562\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.75525021553,2.36460611562), test loss: 3.86045313776\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (23.0703468323,30.6366813698), test loss: 32.2754895687\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.592990815639,2.35295927682), test loss: 3.48556749821\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (20.8982276917,30.4100766753), test loss: 27.8254595041\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.618656039238,2.34199958599), test loss: 2.88439525068\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (12.1224279404,30.1927617655), test loss: 28.1368910551\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.918932557106,2.33111911128), test loss: 2.95025522262\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (12.2009563446,29.981974963), test loss: 27.1726615429\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.342080175877,2.32040431523), test loss: 2.9204416275\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (21.6879959106,29.7729093978), test loss: 28.0230754614\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (5.99168968201,2.31025466987), test loss: 3.00476495028\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (18.1459522247,29.5704497884), test loss: 28.6661000133\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.18906021118,2.30021223016), test loss: 3.00045531541\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (15.287317276,29.3742907407), test loss: 30.7620610952\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.582974791527,2.29037024149), test loss: 2.94851755798\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (21.7289733887,29.1806706437), test loss: 29.5322161913\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.04202604294,2.28062949823), test loss: 2.98595831692\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (38.269821167,28.9913970867), test loss: 28.966191721\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.62777352333,2.27152017145), test loss: 3.07383732796\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (27.2557220459,28.8094229609), test loss: 30.4185870409\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.29341864586,2.26245116516), test loss: 3.29186862111\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (9.02270126343,28.6282546742), test loss: 28.7352263927\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (4.29859542847,2.25337071674), test loss: 3.19571305513\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (14.8472232819,28.4505765619), test loss: 28.152342701\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.491151571274,2.24473834553), test loss: 3.51056334674\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.8787918091,28.2799792718), test loss: 30.6693038881\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.36496090889,2.23632218456), test loss: 3.90205844343\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (13.2682905197,28.1129130812), test loss: 30.4708440781\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.68615341187,2.22776186788), test loss: 3.49637992084\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (21.3900413513,27.9463139767), test loss: 28.1952838421\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.89283788204,2.21982441423), test loss: 3.20891053975\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (12.162229538,27.7859580741), test loss: 27.9576160192\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (2.09830665588,2.21185365036), test loss: 2.94246012866\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (25.7305240631,27.6303860216), test loss: 26.6455914736\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.498269885778,2.2039198275), test loss: 2.84694959521\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (9.63477706909,27.473912126), test loss: 27.3570966244\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.1580851078,2.19644882876), test loss: 3.0032664001\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (10.3716163635,27.3236662916), test loss: 28.194503212\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.6096342206,2.18896672956), test loss: 3.01871108711\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (7.30677938461,27.1772048447), test loss: 30.6938579082\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.152080878615,2.18153983275), test loss: 2.96479128972\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (16.6887607574,27.0312167033), test loss: 29.34182868\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (5.03396701813,2.17444738238), test loss: 3.03564728796\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (12.1332015991,26.8891841512), test loss: 29.5697473526\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.78131961823,2.16743734226), test loss: 3.08327152431\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (8.48760032654,26.7512654799), test loss: 29.5171953678\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.0553406477,2.16052854456), test loss: 3.11845155656\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (19.851726532,26.6144446257), test loss: 29.7744423151\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.35695576668,2.15358630499), test loss: 3.22200464308\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (39.9157409668,26.4801771714), test loss: 28.3895698547\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.44331264496,2.14712370496), test loss: 3.47991306782\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (5.63833761215,26.3502217103), test loss: 29.7764316082\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.396984755993,2.14059384493), test loss: 3.76643722653\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (8.84553337097,26.2211275069), test loss: 29.7603663087\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (4.82171392441,2.13411294244), test loss: 3.56314919442\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (12.4488620758,26.0935340944), test loss: 30.5663443089\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.73499828577,2.12788026828), test loss: 3.50819802284\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (7.14084243774,25.9704586207), test loss: 27.459706831\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.64218902588,2.12177763587), test loss: 2.95729198754\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.97634410858,25.8498580718), test loss: 26.2572014809\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.82763028145,2.11550002903), test loss: 2.86315016299\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (25.3436527252,25.7289959568), test loss: 26.9907207489\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.45587420464,2.10969544381), test loss: 3.00057286322\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (7.66634988785,25.6121151999), test loss: 27.860667181\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.31941699982,2.10384237226), test loss: 3.04837902486\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (24.2957630157,25.498453424), test loss: 29.7476333141\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.481536775827,2.09795983411), test loss: 2.98725412041\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (9.08199501038,25.3837541903), test loss: 29.2628263831\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.939394891262,2.09241512586), test loss: 2.98440986872\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (11.676867485,25.2731536628), test loss: 29.7981659174\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.1515557766,2.08684338577), test loss: 3.15062088072\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (6.95387935638,25.1652876338), test loss: 29.0521436691\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.32023665309,2.08128638106), test loss: 3.09778611958\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (7.5905752182,25.0569049853), test loss: 30.3877520025\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (4.12352895737,2.07592149902), test loss: 3.32706306279\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (9.82547855377,24.9515644531), test loss: 28.588530302\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.17679810524,2.07065434296), test loss: 3.47777311504\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (10.5015153885,24.849013762), test loss: 27.8145062447\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.799761295319,2.06541409993), test loss: 3.32884271145\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (14.4566440582,24.7466988158), test loss: 29.8344414234\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.787010669708,2.06011061521), test loss: 3.71516951174\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (37.2739982605,24.6460594925), test loss: 30.9995999694\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.72465705872,2.0551793282), test loss: 3.5267348446\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (3.91964697838,24.5485531328), test loss: 27.4074732065\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.225837826729,2.05018072203), test loss: 3.03765277863\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (9.08631515503,24.4512682064), test loss: 26.3637117386\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.77673006058,2.04514479722), test loss: 2.9141626358\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (17.374376297,24.3549048564), test loss: 26.6097718716\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.936674833298,2.04036200006), test loss: 2.99324643463\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (10.7229585648,24.2615700564), test loss: 27.6335725546\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.68769168854,2.03562996487), test loss: 3.07200396955\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (6.0572924614,24.169967537), test loss: 28.5845434666\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.64067411423,2.03071741184), test loss: 2.99343631566\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (16.8982391357,24.0777530662), test loss: 29.2795477152\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.26544380188,2.02618136931), test loss: 2.99510520697\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (3.92979884148,23.9885814724), test loss: 29.3568448305\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.29226922989,2.02159443), test loss: 3.08917753398\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (22.4190101624,23.9016129294), test loss: 28.6278526545\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.23486042023,2.01695915092), test loss: 3.12345071435\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (5.47878646851,23.8135291747), test loss: 29.5651180744\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.40476179123,2.01257396463), test loss: 3.28782940507\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (14.948184967,23.728497458), test loss: 28.3854061365\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.18258619308,2.00815747624), test loss: 3.33272835165\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (11.0886497498,23.6453636831), test loss: 27.126061058\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.321911692619,2.00373412943), test loss: 3.30814858377\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (7.84098339081,23.5615169277), test loss: 29.6377996922\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.79028701782,1.99940398685), test loss: 3.79534856379\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (11.1617240906,23.4798635341), test loss: 30.571770668\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.34768390656,1.99520244516), test loss: 3.46018224061\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (7.41256046295,23.4002514835), test loss: 27.103943193\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.5284204483,1.99101981697), test loss: 2.91047284454\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.99455690384,23.3204493699), test loss: 27.539911437\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.718206167221,1.98673286579), test loss: 2.94740958512\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (21.4725265503,23.241827749), test loss: 26.2227046013\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.34792304039,1.98274511704), test loss: 2.92737992704\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (3.71516680717,23.165860511), test loss: 27.4958931923\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.240232422948,1.97870379439), test loss: 3.11749683917\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (16.0181350708,23.0895668081), test loss: 28.0052290201\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (3.25084137917,1.97456848157), test loss: 3.02693079114\n",
      "run time for single CV loop: 1109.77357411\n",
      "\n",
      "MC # 1, Hype # hyp1, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (136.578918457,inf), test loss: 145.974521446\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (440.214813232,inf), test loss: 382.432757568\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (45.9854164124,63.6582493429), test loss: 46.6814662933\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.39403367043,13.7981706057), test loss: 3.13725066185\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (37.768081665,57.4850723829), test loss: 47.8933121681\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.70519304276,8.27461991602), test loss: 3.23032025695\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (58.6009941101,55.0889505968), test loss: 48.9856911659\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.37282335758,6.42415054135), test loss: 3.48144604713\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (16.5724010468,53.7210196157), test loss: 49.7002940178\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.93702232838,5.49213516107), test loss: 3.57142902017\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (30.1519165039,52.6653531178), test loss: 44.9753991127\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.916345536709,4.92780600809), test loss: 3.46645622849\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (60.1826057434,51.9057553174), test loss: 42.2296977758\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.57125043869,4.54803265302), test loss: 3.53115024865\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (126.781570435,51.1437753455), test loss: 47.0198805332\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.9018406868,4.27190213267), test loss: 4.17796899676\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (60.6639213562,50.4184965403), test loss: 47.1143092632\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.07269096375,4.06441154457), test loss: 4.0958363384\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (98.4206848145,49.7924585722), test loss: 40.7903272629\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.32062244415,3.9003350185), test loss: 3.28558022678\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (35.24269104,49.0916483301), test loss: 40.2616110325\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.22170901299,3.76423404614), test loss: 3.39439105019\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (40.034778595,48.4080760855), test loss: 38.6772605419\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.74890756607,3.65431165049), test loss: 3.23934194744\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (36.0881690979,47.7330546196), test loss: 39.8107386589\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.956746101379,3.5585005454), test loss: 3.29821328819\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (45.0872993469,47.0320526125), test loss: 39.0815864086\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.9395031929,3.47599842051), test loss: 3.49134818912\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (17.6932315826,46.2897651275), test loss: 37.1261688232\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.71465528011,3.40593881402), test loss: 3.1800229758\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (24.3114070892,45.5586059242), test loss: 35.7685379982\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.6434648037,3.34253920913), test loss: 3.20626313984\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (25.2189674377,44.806082039), test loss: 37.8848642349\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.55822277069,3.28572635116), test loss: 3.49297487438\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (22.5914726257,44.0348627497), test loss: 38.4948150635\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.06939125061,3.23644507178), test loss: 3.61631365567\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (25.3895282745,43.2750423), test loss: 34.9358540535\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.044485569,3.19029062003), test loss: 3.57087982297\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (42.5642547607,42.5219059984), test loss: 33.2695371151\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.70954453945,3.14875800889), test loss: 3.71936452687\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (2.39804887772,41.7752151554), test loss: 38.780275178\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.68588614464,3.11084629191), test loss: 4.45329037607\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (10.7132472992,41.0492616406), test loss: 38.6206524372\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.65502929688,3.07642969978), test loss: 4.26984791756\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (17.0947990417,40.3713876516), test loss: 33.3948543072\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.6511118412,3.04471119848), test loss: 3.79867782891\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (11.4521389008,39.7021938954), test loss: 32.6599996805\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.69951325655,3.01469124229), test loss: 3.35332500935\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (31.2176322937,39.0680073596), test loss: 31.6480929136\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.54520130157,2.98729216961), test loss: 3.2779133983\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (58.0041122437,38.4844390384), test loss: 33.2822324991\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.46475219727,2.96188920776), test loss: 3.44463787973\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (13.3613586426,37.9191787423), test loss: 34.8375570774\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.95578324795,2.93731248684), test loss: 3.47248325497\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (9.22829055786,37.377115768), test loss: 35.5283929825\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.35213220119,2.91546949497), test loss: 3.27952444851\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (26.7686004639,36.8807567242), test loss: 33.6314047575\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.689402580261,2.89412933659), test loss: 3.27257898152\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (33.922492981,36.41126798), test loss: 35.5265321732\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.77564287186,2.87415751277), test loss: 3.56086429656\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (19.1922683716,35.9520571273), test loss: 35.7223910809\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.23902142048,2.85605638131), test loss: 3.53455179036\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (8.16351795197,35.5286494537), test loss: 35.9687372923\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.4658741951,2.83820716709), test loss: 3.62993861586\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (27.3824863434,35.1250643133), test loss: 33.6967812538\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.10952472687,2.82138710472), test loss: 3.74498048127\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (17.4575977325,34.7294842187), test loss: 38.5788798332\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.47403430939,2.80606121154), test loss: 4.02088929713\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (20.0800132751,34.3606821379), test loss: 39.1310121536\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (1.85480797291,2.79088648676), test loss: 4.29445507526\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (18.6447296143,34.0116674283), test loss: 40.3863832235\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.76339972019,2.77661254452), test loss: 4.2098056674\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (11.9192647934,33.6735010076), test loss: 32.593799448\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.50793266296,2.76288307752), test loss: 3.30089369714\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (15.2141361237,33.3526659155), test loss: 31.5346043348\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (2.9121119976,2.75010050809), test loss: 3.34183987528\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (54.1566848755,33.055378388), test loss: 32.5080783367\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (5.97565221786,2.73785525547), test loss: 3.35584068447\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (8.50833129883,32.7600998085), test loss: 34.6358906984\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.02915394306,2.72570828514), test loss: 3.38246824443\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (33.319858551,32.479138732), test loss: 34.8705873489\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.06547307968,2.71430200223), test loss: 3.30023397207\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (47.0463867188,32.2209764776), test loss: 34.3511530876\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.36397266388,2.70355387085), test loss: 3.27812873125\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (14.2430582047,31.9675550644), test loss: 34.3899748325\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.35466742516,2.69276672078), test loss: 3.4726793468\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (10.4867620468,31.7195105963), test loss: 34.9413713932\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.82404613495,2.68300906224), test loss: 3.46375941336\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (22.2799053192,31.4909001488), test loss: 35.6097673655\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.642364144325,2.6731507435), test loss: 3.6930508554\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (37.4399490356,31.2722828219), test loss: 33.7375538111\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.2977836132,2.66368754894), test loss: 3.86100797169\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (19.6063613892,31.0522904897), test loss: 32.240762043\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.939824283123,2.65501248007), test loss: 3.65425616503\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (14.403506279,30.8494296153), test loss: 39.3385412216\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.08593225479,2.64615410432), test loss: 4.46355516165\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (23.4190196991,30.6552516285), test loss: 40.5591240048\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.59560918808,2.6375493072), test loss: 4.08600077033\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (19.7974433899,30.4611871974), test loss: 32.1892446518\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.22858905792,2.62972954735), test loss: 3.26396864951\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (21.8904151917,30.2798753398), test loss: 31.4127544641\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.64406108856,2.62173933043), test loss: 3.30183292627\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (14.2429485321,30.1058567154), test loss: 32.0828488827\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.38143932819,2.61407774478), test loss: 3.31931228191\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (17.931974411,29.9339636328), test loss: 33.9368165731\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.30735349655,2.60655529818), test loss: 3.35674206764\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (31.6008796692,29.7690564075), test loss: 34.3336047649\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.48139953613,2.59948689906), test loss: 3.24500339925\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (52.6783752441,29.6150883222), test loss: 35.0846822262\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (5.38412857056,2.59256186281), test loss: 3.27793704718\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (6.69352149963,29.4585087096), test loss: 34.5364023566\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.01344704628,2.58555320457), test loss: 3.26122638285\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (21.8677330017,29.3076843046), test loss: 34.0026642561\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.01608979702,2.57888480109), test loss: 3.46271716803\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (46.8597412109,29.169494682), test loss: 35.65044806\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.95290660858,2.57257346467), test loss: 3.65680058897\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (14.7327003479,29.0307533386), test loss: 33.3759605408\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (4.23689365387,2.56606839056), test loss: 3.61274361461\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (20.6856193542,28.8929275874), test loss: 31.6860942125\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.51339101791,2.56017293871), test loss: 3.59323431849\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (18.8970718384,28.76549196), test loss: 38.8442381859\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.558477401733,2.55412884843), test loss: 4.41466858983\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (31.6107788086,28.6425317135), test loss: 39.722571826\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.669021785259,2.54820433029), test loss: 4.08929884732\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (22.1124038696,28.516403985), test loss: 32.1346488953\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.732498347759,2.54278050531), test loss: 3.18796939999\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (11.2413663864,28.3999283397), test loss: 32.2101249695\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.07795274258,2.5371240633), test loss: 3.3131990239\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (17.3303031921,28.2873723883), test loss: 31.5561183929\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.836600065231,2.53159324984), test loss: 3.29172647595\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (20.6358699799,28.1730290604), test loss: 32.6680324554\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (7.10811710358,2.5265368206), test loss: 3.26270641685\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (21.2701683044,28.065722268), test loss: 33.6360196114\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.47878456116,2.52129656347), test loss: 3.31351850927\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (14.1804094315,27.9621549583), test loss: 35.0267946005\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.559180855751,2.51621054353), test loss: 3.17229216099\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (26.6430931091,27.8583667034), test loss: 33.382024765\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.13412094116,2.51115331662), test loss: 3.20608870536\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (60.6281509399,27.7580817786), test loss: 33.4096481144\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (4.61888599396,2.50643528218), test loss: 3.37570466399\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (53.1531677246,27.664266773), test loss: 35.2164594173\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (6.41107177734,2.50176616475), test loss: 3.59522319436\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.8928813934,27.5671574291), test loss: 32.9995377064\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (4.84300041199,2.49693737021), test loss: 3.46096830964\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (14.6780767441,27.4726635034), test loss: 32.3630106449\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.491030186415,2.49231621539), test loss: 3.79595987052\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (21.8597488403,27.3863154215), test loss: 38.2791377544\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.63665604591,2.48793272961), test loss: 4.40856105834\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (14.1034965515,27.2993394543), test loss: 38.3492458344\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.3512699604,2.48335348732), test loss: 4.1501423806\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (28.3585262299,27.2111584567), test loss: 32.6605428219\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.88048887253,2.47921293903), test loss: 3.40811195672\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (13.9254322052,27.1297866427), test loss: 32.1703333735\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.1518843174,2.47491664519), test loss: 3.22944855988\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (37.0847663879,27.050793831), test loss: 30.9663125753\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.807545185089,2.47066363955), test loss: 3.19071946293\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (11.4369850159,26.9683436499), test loss: 32.1907458305\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.767241060734,2.46676406833), test loss: 3.25519456267\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (11.4749336243,26.892848435), test loss: 33.5149387598\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.815418720245,2.46264564777), test loss: 3.29700991511\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (7.83910751343,26.8189620716), test loss: 34.9389057636\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.213511496782,2.45858566552), test loss: 3.15497738719\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (17.8389034271,26.7432404502), test loss: 33.279171896\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (5.84159469604,2.45485019579), test loss: 3.18951822221\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (16.8319950104,26.6719526269), test loss: 34.2515810728\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (3.25709319115,2.45100330377), test loss: 3.37198690027\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (8.57255935669,26.6030241222), test loss: 34.5913509548\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.12235593796,2.44722853147), test loss: 3.47432534397\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (21.3526077271,26.5330040492), test loss: 34.2846489429\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.82042241096,2.44338772289), test loss: 3.49207042605\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (62.2819519043,26.4650883851), test loss: 32.6043027163\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (4.65941524506,2.43985503431), test loss: 3.6429366827\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (8.23958015442,26.4008809892), test loss: 37.866750598\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.22334778309,2.43623941498), test loss: 4.26129430383\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (12.5805053711,26.3352368756), test loss: 37.8666653156\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (5.83034753799,2.43266955492), test loss: 4.09421648756\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (12.3741111755,26.2699143743), test loss: 38.1520497799\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.685407876968,2.42913992994), test loss: 3.89912662655\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (13.3206300735,26.2105661269), test loss: 31.8563663006\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.35268545151,2.42577909422), test loss: 3.21362324953\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.81991767883,26.1504373611), test loss: 30.6268713474\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.35175704956,2.422203183), test loss: 3.19978386164\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (32.8178329468,26.0886376611), test loss: 32.1911293983\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.87789285183,2.41901627987), test loss: 3.22951878011\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (12.1174573898,26.0318204119), test loss: 33.4493030548\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.4773516655,2.41568718354), test loss: 3.26090198755\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (33.0481452942,25.9763417377), test loss: 34.3025763512\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.926669120789,2.41235411613), test loss: 3.20119835287\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (10.8798007965,25.9178818571), test loss: 33.3039857626\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.416869819164,2.40929214096), test loss: 3.12099270523\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (15.4236650467,25.8644173745), test loss: 34.3721710682\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.771229684353,2.40604678984), test loss: 3.35553475767\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (7.36875772476,25.812063662), test loss: 34.3072417498\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.46090811491,2.40283816349), test loss: 3.38345898688\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (8.17572402954,25.7573565452), test loss: 35.1152529359\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (4.56388902664,2.39981763639), test loss: 3.56180976033\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (18.3478546143,25.7064074923), test loss: 33.0465396643\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.64988565445,2.39678973528), test loss: 3.66079984009\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (14.2793121338,25.6569310001), test loss: 33.3048149109\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.06899297237,2.39375974058), test loss: 3.69899820983\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (15.9668073654,25.6059645401), test loss: 38.124983263\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.06620883942,2.39064733558), test loss: 4.19563147873\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (59.7442092896,25.5563509328), test loss: 39.3222094059\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (3.18855333328,2.38778692665), test loss: 3.95351801738\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (9.51114273071,25.5099436194), test loss: 32.126619482\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.25057148933,2.38488481993), test loss: 3.27195566893\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (13.4874038696,25.4616764966), test loss: 30.7063175201\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (5.79883146286,2.38194933097), test loss: 3.14893784523\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (22.6443424225,25.4134553188), test loss: 31.6801252723\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.902043581009,2.37909728483), test loss: 3.19155008942\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (24.3050937653,25.3698233505), test loss: 33.4368032217\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (3.85249614716,2.3763372547), test loss: 3.26164567471\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (5.53717136383,25.3252943941), test loss: 33.7312512398\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.78893685341,2.37336349972), test loss: 3.15591130257\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (23.6767501831,25.2789578646), test loss: 33.4716356516\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.60401117802,2.37074289736), test loss: 3.08428440392\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (8.20893573761,25.2368935258), test loss: 33.8485069513\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (3.32924437523,2.36799193702), test loss: 3.21601137668\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (30.0493297577,25.1955003544), test loss: 33.7755560637\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.32299804688,2.36521543736), test loss: 3.42496626079\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (7.68098545074,25.1513861233), test loss: 34.6704762459\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.14892101288,2.36265465627), test loss: 3.52014225423\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (21.1771354675,25.1113317164), test loss: 33.0274822891\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.748850643635,2.35994503176), test loss: 3.49320163578\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (13.5937833786,25.0719203639), test loss: 31.3912800074\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.560502052307,2.35725805086), test loss: 3.5123489514\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (8.76219177246,25.0302521789), test loss: 38.2691306114\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.71516478062,2.35465187693), test loss: 4.19985894263\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (16.1415252686,24.9915353794), test loss: 38.870752573\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.28839457035,2.35212364546), test loss: 3.84140252322\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (11.2356309891,24.9539689276), test loss: 31.5359277964\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.20864319801,2.34960111943), test loss: 3.11353855506\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (8.44933319092,24.914783193), test loss: 31.8519727707\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.36290764809,2.34695445168), test loss: 3.14785511494\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (31.3392219543,24.8763510542), test loss: 31.0949894905\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.05413198471,2.34451792523), test loss: 3.17389854938\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (5.97413349152,24.8412540605), test loss: 33.5325253487\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.690644025803,2.34205495221), test loss: 3.31885704696\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (23.2702407837,24.803965384), test loss: 33.4155689001\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (5.61167478561,2.33951603614), test loss: 3.15112704933\n",
      "run time for single CV loop: 1061.20697188\n",
      "\n",
      "MC # 1, Hype # hyp3, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold10/pretrained_models/ADNI2_ff_hyp2_HC_CT_HC_snap_20000_CT_snap_20000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (136.578918457,inf), test loss: 146.744434929\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (440.214813232,inf), test loss: 387.842182922\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (45.6950874329,77.7248509011), test loss: 46.4415818691\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (2.17768740654,16.4554563878), test loss: 3.12921357453\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (38.9151763916,64.9992308934), test loss: 48.4527096748\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (1.71027541161,9.61501453197), test loss: 3.22555814087\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (59.216583252,60.5000049014), test loss: 50.0740410805\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.43036317825,7.32524594021), test loss: 3.45536976755\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (17.6537780762,58.2191926826), test loss: 51.0613642693\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.95223069191,6.17318986565), test loss: 3.56977312863\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (32.3780670166,56.7259290168), test loss: 46.6909554958\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.926539182663,5.47692495253), test loss: 3.46146177351\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (63.6744155884,55.782542201), test loss: 44.2042839527\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.59862661362,5.00936487241), test loss: 3.52204639316\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (140.691726685,54.9931865954), test loss: 49.4213881493\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.93039190769,4.66999638717), test loss: 4.15077105761\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (68.0979690552,54.3495640873), test loss: 50.190912056\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (4.00972938538,4.41461502744), test loss: 4.05512520373\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (108.17930603,53.8873354643), test loss: 44.5110631704\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (5.56908988953,4.21402662262), test loss: 3.22331824601\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (43.7527275085,53.4077176238), test loss: 44.743768692\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.2400302887,4.04939126016), test loss: 3.37569767833\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (48.34400177,53.0096004122), test loss: 43.7420705795\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.80246853828,3.91678534591), test loss: 3.21533574164\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (42.821056366,52.6627251687), test loss: 45.9907147646\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.970407545567,3.80269596588), test loss: 3.27033521533\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (53.2531890869,52.3297228698), test loss: 45.8236500263\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.97158360481,3.70521340951), test loss: 3.43276238739\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (25.4545478821,52.0023271124), test loss: 43.4093770027\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.80182611942,3.62259049089), test loss: 3.13773864508\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (37.5201568604,51.7157112826), test loss: 43.2565714836\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.7439904213,3.5487256004), test loss: 3.17048881054\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (33.1944274902,51.4207481572), test loss: 46.8901767015\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (4.49468898773,3.48288690756), test loss: 3.44327706695\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (29.7660140991,51.1365088983), test loss: 47.634453249\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (2.03177976608,3.42579381132), test loss: 3.56562316716\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (37.0374755859,50.8684083722), test loss: 43.5556884766\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.37109375,3.37285571394), test loss: 3.50220265388\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (57.458480835,50.5918984016), test loss: 42.225893116\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.68531632423,3.32534413433), test loss: 3.63066491187\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (10.200966835,50.32253978), test loss: 46.8828138351\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.5935087204,3.28208876512), test loss: 4.28449615836\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (18.6430225372,50.0406567624), test loss: 46.0567197323\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (1.70075821877,3.24278607642), test loss: 4.10004114211\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (20.9430198669,49.7845871739), test loss: 41.3398560047\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.76806294918,3.20663601908), test loss: 3.63397893608\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (54.3397140503,49.5023676988), test loss: 41.3476313353\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.570892214775,3.17263125768), test loss: 3.27146768868\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (19.3786849976,49.2251407679), test loss: 39.3504209995\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (3.28197860718,3.14158941862), test loss: 3.19427303225\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (116.46661377,48.9689571172), test loss: 42.1415603638\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (4.94109773636,3.11283241164), test loss: 3.3295717597\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (33.8770637512,48.6862526543), test loss: 41.9533557415\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (2.15125322342,3.08514543302), test loss: 3.40918603539\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (33.4952392578,48.4039336781), test loss: 39.6548436403\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.55486857891,3.06039548498), test loss: 3.2078941375\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (29.4170436859,48.1302794), test loss: 39.2508140087\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.761936962605,3.03633293443), test loss: 3.17781241238\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (37.4524230957,47.8493400581), test loss: 42.2689063072\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.83053290844,3.01376477557), test loss: 3.4915792793\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (29.6772575378,47.5561399077), test loss: 42.0375445366\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (1.23599052429,2.99320587486), test loss: 3.46642145514\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (12.1965179443,47.2706505238), test loss: 41.0198312759\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.37307882309,2.97304875288), test loss: 3.56854130998\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (36.5603179932,46.9786531899), test loss: 38.5097561359\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (4.17048358917,2.95394622174), test loss: 3.67126140594\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (28.2394866943,46.6791918197), test loss: 42.7647799253\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (2.70898485184,2.93651717246), test loss: 3.95313736498\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (30.0574302673,46.3819210402), test loss: 41.1779810429\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.02577280998,2.91925444769), test loss: 4.16570005119\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (30.6988067627,46.0786221185), test loss: 42.5278093338\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.60815751553,2.90298473202), test loss: 4.08110119104\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (10.1827249527,45.7726815625), test loss: 35.7951117992\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (3.70270109177,2.88743985945), test loss: 3.25703271627\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (17.1863479614,45.4584409039), test loss: 34.0751948595\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (3.02664661407,2.872817684), test loss: 3.29421551526\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (20.0884742737,45.1553102189), test loss: 36.6166400909\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (5.47496461868,2.85878806034), test loss: 3.29920488745\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (36.1789474487,44.8361987017), test loss: 36.8165829182\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (1.278870821,2.844990335), test loss: 3.33712385297\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (33.4563026428,44.5183732122), test loss: 35.7475575924\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (2.20978498459,2.8319557058), test loss: 3.26187781394\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (72.4370727539,44.2113269438), test loss: 34.9008643985\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (2.39958071709,2.81958319071), test loss: 3.18723113537\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (17.8852081299,43.8944796674), test loss: 35.563936758\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (3.76986694336,2.80732134846), test loss: 3.44010286629\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (20.0631713867,43.5749464959), test loss: 36.5450128555\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.9110866785,2.79609689079), test loss: 3.43986288905\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (24.4966144562,43.2647298885), test loss: 36.6005892277\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.70660752058,2.78481354657), test loss: 3.66092871428\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (36.948589325,42.9551867742), test loss: 34.4220486641\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (1.36353445053,2.77396248512), test loss: 3.78829757385\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (21.6913871765,42.6386595442), test loss: 32.7840308189\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (1.0220451355,2.76394033517), test loss: 3.61702764034\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (13.7828950882,42.3326581141), test loss: 38.7481368065\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (2.18977451324,2.75379684217), test loss: 4.37484545559\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (27.2488822937,42.0293445064), test loss: 40.1413118124\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (1.63705706596,2.74392851614), test loss: 4.0183237955\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (23.7368164062,41.7238575341), test loss: 32.5012647152\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (4.30633020401,2.73493618678), test loss: 3.231997177\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (26.0085372925,41.4268585584), test loss: 31.5832369328\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.71641278267,2.72576532254), test loss: 3.28073597848\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (20.5942726135,41.1337024304), test loss: 33.1968413115\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (1.32012343407,2.71698457214), test loss: 3.2968687281\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (22.0524291992,40.8430102493), test loss: 34.2119329214\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (4.55044269562,2.70840980664), test loss: 3.33081408888\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (33.9796638489,40.5553016646), test loss: 34.7759220839\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (4.46381998062,2.70027464285), test loss: 3.2235722065\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (27.2641448975,40.2804840241), test loss: 34.063828516\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (4.97375297546,2.69234685369), test loss: 3.24198545218\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (13.5771512985,40.0022889134), test loss: 34.3777463555\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.23160409927,2.68438946304), test loss: 3.23977925479\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (27.0216636658,39.7298564462), test loss: 34.9299806356\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (1.08672714233,2.67676791308), test loss: 3.45650589913\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (58.5874938965,39.4710589413), test loss: 36.5323748589\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (2.10423874855,2.66951814586), test loss: 3.63450772762\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (12.0362358093,39.2121767109), test loss: 33.5029335737\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (4.46811485291,2.66213432785), test loss: 3.60589123964\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (17.8928375244,38.9554252079), test loss: 31.7396378279\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.48223900795,2.65536799634), test loss: 3.59449549392\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (26.6828861237,38.7108320191), test loss: 39.0734150648\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.499032199383,2.64848104279), test loss: 4.37978844941\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (26.1774864197,38.4717404092), test loss: 40.0398258448\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.743803143501,2.64175319572), test loss: 4.05909839123\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (23.2396068573,38.2316027675), test loss: 32.5806851506\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.794211685658,2.63554001913), test loss: 3.18788865358\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (11.7853307724,38.002700403), test loss: 32.7914723396\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (1.11551713943,2.62912733951), test loss: 3.31355842203\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (17.0202026367,37.7794445703), test loss: 32.2300866604\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.851780295372,2.62286340034), test loss: 3.2898427695\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (26.2775268555,37.5569002873), test loss: 33.7941817522\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (7.08978414536,2.61711282916), test loss: 3.24756065011\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (26.8715686798,37.3428583581), test loss: 34.4744272232\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.35788440704,2.61116837736), test loss: 3.30461481661\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (17.6865749359,37.1344462994), test loss: 35.152763629\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.642386317253,2.60542664209), test loss: 3.16908007562\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (29.8590393066,36.9285857389), test loss: 33.4789973021\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (3.26463389397,2.59972906869), test loss: 3.20440388471\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (65.0139770508,36.7269678809), test loss: 34.4478146553\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (4.5380616188,2.59436300078), test loss: 3.38519196361\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (35.1139755249,36.5351158475), test loss: 36.187057519\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (6.0834274292,2.58909472164), test loss: 3.60175009668\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (13.130282402,36.3416614943), test loss: 33.5357382774\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (5.09058570862,2.58368863222), test loss: 3.47201215625\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (20.0765380859,36.1524566871), test loss: 32.8184723854\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.470008969307,2.57847890055), test loss: 3.79947835952\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (20.9407615662,35.973465425), test loss: 39.0775349855\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.6832498312,2.57352964498), test loss: 4.38359971941\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (13.0053415298,35.7960184797), test loss: 39.286024642\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (3.42899465561,2.56841388901), test loss: 4.15214051753\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (22.3531322479,35.6187141566), test loss: 33.2144759178\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.90580046177,2.56373428536), test loss: 3.414744699\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (20.0116653442,35.4501919611), test loss: 33.045092082\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (3.2338681221,2.55891436939), test loss: 3.24070822895\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (39.9036216736,35.2854072005), test loss: 31.9444599867\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.863235533237,2.55415892779), test loss: 3.19262626767\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (9.80637359619,35.1186699416), test loss: 33.2870160103\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (0.784686267376,2.54975666471), test loss: 3.25906001329\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (12.3535575867,34.9602173366), test loss: 34.7029849768\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.799987614155,2.54515425538), test loss: 3.30018732846\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (10.3412876129,34.8050120847), test loss: 35.4748153687\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.235223382711,2.54063173664), test loss: 3.16578779221\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (20.5980434418,34.6495905231), test loss: 33.6420379877\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (5.70038986206,2.53644666026), test loss: 3.18307799995\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (23.1955986023,34.499817482), test loss: 35.0685680866\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (3.22623229027,2.53214223264), test loss: 3.38275932819\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (9.95732879639,34.3536209054), test loss: 35.5381506801\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (1.328769207,2.52794630355), test loss: 3.46606325507\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (23.7852745056,34.2079232616), test loss: 34.9571101427\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.83272302151,2.52369177531), test loss: 3.51125474274\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (67.2919921875,34.0645450562), test loss: 32.9244063616\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (4.54865407944,2.51973463751), test loss: 3.65559216738\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (6.52132129669,33.9265031024), test loss: 38.6995973349\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (1.27120649815,2.51573898659), test loss: 4.22952450812\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (17.2691078186,33.7870544021), test loss: 38.9847654104\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (5.97824525833,2.51179102254), test loss: 4.1008909896\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (13.5273084641,33.6487167056), test loss: 38.8047106266\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.678423762321,2.50788128857), test loss: 3.90976047665\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (12.0213575363,33.5177417103), test loss: 32.3913785934\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (3.47618746758,2.50416315178), test loss: 3.21511128247\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.97072505951,33.3872380566), test loss: 31.2648851991\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (2.30853676796,2.50024777528), test loss: 3.21353311539\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (25.4773521423,33.2560228473), test loss: 32.9814497232\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (1.9061435461,2.49672111926), test loss: 3.24652129263\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (13.5943193436,33.1312682638), test loss: 34.4842011929\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (3.62326741219,2.49306311946), test loss: 3.26037856042\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (36.103061676,33.0091385323), test loss: 35.0073458195\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.952510714531,2.48941721498), test loss: 3.2054176122\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (9.69029998779,32.8849818203), test loss: 33.7608602285\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.426408022642,2.48603920481), test loss: 3.13166919425\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (15.4122800827,32.7670947693), test loss: 34.8665473461\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (0.763263344765,2.48248730168), test loss: 3.36524159759\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (7.92333889008,32.6515454182), test loss: 35.1899551153\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.465821743011,2.47898965308), test loss: 3.3940912798\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (10.9717378616,32.5347248387), test loss: 35.8196787894\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (4.42829608917,2.47568952202), test loss: 3.58452880979\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (20.0573806763,32.4227868767), test loss: 33.5107198\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.66702795029,2.47237007079), test loss: 3.6753491044\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (15.3699293137,32.3133242667), test loss: 34.2455384016\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (1.32370507717,2.46908026421), test loss: 3.69818097353\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (17.0716285706,32.2035760457), test loss: 39.3646115541\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (1.0706448555,2.46570614362), test loss: 4.22129791528\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (59.7486114502,32.0960270388), test loss: 40.6490160704\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (3.11870718002,2.46257234521), test loss: 3.97816761173\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (10.3997612,31.9932576059), test loss: 32.415916729\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (1.45187413692,2.45943342119), test loss: 3.22740731835\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (16.2869701385,31.8890891378), test loss: 31.4845363379\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (5.9874753952,2.45625515843), test loss: 3.17168421447\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (22.8861961365,31.7859765302), test loss: 32.6587675571\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.879267334938,2.45315707418), test loss: 3.20857559592\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (23.7447280884,31.6886939394), test loss: 34.4105294704\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (4.00784921646,2.45017206648), test loss: 3.24889136851\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (5.77675676346,31.5914491913), test loss: 34.7897241116\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.70031535625,2.44698608395), test loss: 3.17631175518\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (20.2515411377,31.4931767804), test loss: 34.091467452\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (1.6474891901,2.44415420851), test loss: 3.10873799771\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (8.14615631104,31.4001055011), test loss: 34.341970849\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (3.52464675903,2.44119275131), test loss: 3.20921978354\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (34.1386604309,31.3087594139), test loss: 34.5292155266\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (2.47410726547,2.43821609145), test loss: 3.42918834686\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (6.90187263489,31.2153055867), test loss: 35.3362388611\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.15835797787,2.43544795264), test loss: 3.54446190298\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (21.4718151093,31.1268288778), test loss: 33.7230262101\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (0.777434587479,2.43253775467), test loss: 3.51966498941\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (15.0650959015,31.0398471656), test loss: 31.9723184586\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.533633291721,2.42966259805), test loss: 3.53613025397\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (9.63508987427,30.951351676), test loss: 39.5102156162\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.81126356125,2.42687420765), test loss: 4.23844772279\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (16.4243850708,30.8666046287), test loss: 40.2697666645\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.33346533775,2.42414324429), test loss: 3.87160944492\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (12.8731136322,30.7837320492), test loss: 32.3186496019\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (2.43234300613,2.42144194427), test loss: 3.11692562103\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (8.46427154541,30.7000268589), test loss: 32.5768172026\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (1.38977408409,2.41860896468), test loss: 3.17107764482\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (30.3009357452,30.6176743837), test loss: 31.9908343792\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.00893902779,2.41597305163), test loss: 3.18696804792\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (8.00271987915,30.5397970249), test loss: 33.93517766\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.823463201523,2.41334245625), test loss: 3.25282618999\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (22.2558517456,30.4599352454), test loss: 34.4783361435\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (5.81926584244,2.41062787848), test loss: 3.16656119823\n",
      "run time for single CV loop: 1063.82504201\n",
      "\n",
      "MC # 1, Hype # hyp2, Fold # 10\n",
      "loading pretrained weights from /projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/API/data/MC_1/fold10/pretrained_models/ADNI2_ff_hyp5_HC_CT_HC_snap_20000_CT_snap_80000_Sup_Concat.caffemodel\n",
      "ADAS Loss Iteration: 0, train loss(batch, sum): (157.434051514,inf), test loss: 166.385263062\n",
      "MMSE Loss Iteration: 0, train loss(batch, sum): (424.470733643,inf), test loss: 383.22008667\n",
      "ADAS Loss Iteration: 500, train loss(batch, sum): (43.1429519653,65.1434361906), test loss: 45.8160671234\n",
      "MMSE Loss Iteration: 500, train loss(batch, sum): (1.86218214035,23.2782971686), test loss: 2.95642288625\n",
      "ADAS Loss Iteration: 1000, train loss(batch, sum): (35.8927078247,57.3044863963), test loss: 46.7281980038\n",
      "MMSE Loss Iteration: 1000, train loss(batch, sum): (2.10861349106,13.0357544515), test loss: 3.00010116696\n",
      "ADAS Loss Iteration: 1500, train loss(batch, sum): (57.5186080933,54.2689347518), test loss: 47.3174638271\n",
      "MMSE Loss Iteration: 1500, train loss(batch, sum): (1.24598073959,9.58199932261), test loss: 3.17555808872\n",
      "ADAS Loss Iteration: 2000, train loss(batch, sum): (15.7288722992,52.4793503923), test loss: 47.8072340965\n",
      "MMSE Loss Iteration: 2000, train loss(batch, sum): (1.90327858925,7.8267643189), test loss: 3.19935989082\n",
      "ADAS Loss Iteration: 2500, train loss(batch, sum): (26.6435012817,51.0705919645), test loss: 42.8428580761\n",
      "MMSE Loss Iteration: 2500, train loss(batch, sum): (0.86464548111,6.75570718075), test loss: 3.04573979378\n",
      "ADAS Loss Iteration: 3000, train loss(batch, sum): (54.5418281555,49.9723940185), test loss: 39.7895559311\n",
      "MMSE Loss Iteration: 3000, train loss(batch, sum): (2.1991276741,6.02551439658), test loss: 3.23284249604\n",
      "ADAS Loss Iteration: 3500, train loss(batch, sum): (113.701789856,48.8753715459), test loss: 43.6914159298\n",
      "MMSE Loss Iteration: 3500, train loss(batch, sum): (1.40290534496,5.48977085566), test loss: 3.71667089164\n",
      "ADAS Loss Iteration: 4000, train loss(batch, sum): (54.5381698608,47.7151307178), test loss: 42.4326868057\n",
      "MMSE Loss Iteration: 4000, train loss(batch, sum): (3.05952644348,5.08258487044), test loss: 3.52584655881\n",
      "ADAS Loss Iteration: 4500, train loss(batch, sum): (84.32762146,46.6215113251), test loss: 35.8870522022\n",
      "MMSE Loss Iteration: 4500, train loss(batch, sum): (4.54827737808,4.75877547384), test loss: 3.00701420307\n",
      "ADAS Loss Iteration: 5000, train loss(batch, sum): (25.7863349915,45.4503483087), test loss: 34.7259768486\n",
      "MMSE Loss Iteration: 5000, train loss(batch, sum): (1.34832251072,4.49204640753), test loss: 2.9930381082\n",
      "ADAS Loss Iteration: 5500, train loss(batch, sum): (27.8633823395,44.2802711329), test loss: 32.6938204288\n",
      "MMSE Loss Iteration: 5500, train loss(batch, sum): (1.38238370419,4.27400895947), test loss: 2.88892236352\n",
      "ADAS Loss Iteration: 6000, train loss(batch, sum): (24.5072517395,43.1273386538), test loss: 32.9612542629\n",
      "MMSE Loss Iteration: 6000, train loss(batch, sum): (0.753860235214,4.08738074668), test loss: 2.96026081443\n",
      "ADAS Loss Iteration: 6500, train loss(batch, sum): (35.2791748047,41.9770371325), test loss: 32.131793344\n",
      "MMSE Loss Iteration: 6500, train loss(batch, sum): (1.40707182884,3.92702947202), test loss: 3.10893831551\n",
      "ADAS Loss Iteration: 7000, train loss(batch, sum): (11.3800392151,40.819145721), test loss: 31.5524908066\n",
      "MMSE Loss Iteration: 7000, train loss(batch, sum): (1.07995438576,3.78970505155), test loss: 2.92591517419\n",
      "ADAS Loss Iteration: 7500, train loss(batch, sum): (13.5702857971,39.7194857155), test loss: 30.2539073467\n",
      "MMSE Loss Iteration: 7500, train loss(batch, sum): (3.03251671791,3.66823148612), test loss: 2.95611671507\n",
      "ADAS Loss Iteration: 8000, train loss(batch, sum): (20.7395019531,38.6671605822), test loss: 31.4279212236\n",
      "MMSE Loss Iteration: 8000, train loss(batch, sum): (3.22072315216,3.56011561807), test loss: 3.16939198673\n",
      "ADAS Loss Iteration: 8500, train loss(batch, sum): (12.2661428452,37.6561309027), test loss: 32.1220099926\n",
      "MMSE Loss Iteration: 8500, train loss(batch, sum): (1.37613368034,3.46504596415), test loss: 3.24308872521\n",
      "ADAS Loss Iteration: 9000, train loss(batch, sum): (20.5646648407,36.7151135085), test loss: 30.1002776623\n",
      "MMSE Loss Iteration: 9000, train loss(batch, sum): (2.10011911392,3.37871295944), test loss: 3.19234132767\n",
      "ADAS Loss Iteration: 9500, train loss(batch, sum): (30.1519145966,35.8385348527), test loss: 28.8776546478\n",
      "MMSE Loss Iteration: 9500, train loss(batch, sum): (1.23741829395,3.30034473378), test loss: 3.41137478203\n",
      "ADAS Loss Iteration: 10000, train loss(batch, sum): (2.27187991142,35.0099555896), test loss: 32.3377771258\n",
      "MMSE Loss Iteration: 10000, train loss(batch, sum): (1.87551796436,3.22891935417), test loss: 3.9074624747\n",
      "ADAS Loss Iteration: 10500, train loss(batch, sum): (6.9606552124,34.2359243333), test loss: 32.0361340523\n",
      "MMSE Loss Iteration: 10500, train loss(batch, sum): (0.927562773228,3.16453963015), test loss: 3.64995308518\n",
      "ADAS Loss Iteration: 11000, train loss(batch, sum): (17.3273239136,33.525627172), test loss: 29.5536837578\n",
      "MMSE Loss Iteration: 11000, train loss(batch, sum): (1.24229383469,3.10500583553), test loss: 3.49722727239\n",
      "ADAS Loss Iteration: 11500, train loss(batch, sum): (6.83172607422,32.8467860993), test loss: 28.3246595383\n",
      "MMSE Loss Iteration: 11500, train loss(batch, sum): (0.458045899868,3.04940814924), test loss: 2.9478972286\n",
      "ADAS Loss Iteration: 12000, train loss(batch, sum): (21.3926773071,32.2111068411), test loss: 27.159587574\n",
      "MMSE Loss Iteration: 12000, train loss(batch, sum): (1.65715432167,2.99868078719), test loss: 2.83599580377\n",
      "ADAS Loss Iteration: 12500, train loss(batch, sum): (44.1551895142,31.6262562489), test loss: 28.2700192928\n",
      "MMSE Loss Iteration: 12500, train loss(batch, sum): (2.96494221687,2.95152851969), test loss: 3.0737505734\n",
      "ADAS Loss Iteration: 13000, train loss(batch, sum): (8.68040561676,31.0701524707), test loss: 29.1846943378\n",
      "MMSE Loss Iteration: 13000, train loss(batch, sum): (1.43859052658,2.90656382744), test loss: 3.03240542263\n",
      "ADAS Loss Iteration: 13500, train loss(batch, sum): (6.06453752518,30.5395671606), test loss: 31.1190662146\n",
      "MMSE Loss Iteration: 13500, train loss(batch, sum): (1.24086523056,2.86591475778), test loss: 2.98691219687\n",
      "ADAS Loss Iteration: 14000, train loss(batch, sum): (18.3034973145,30.0483666409), test loss: 29.632300806\n",
      "MMSE Loss Iteration: 14000, train loss(batch, sum): (0.666100919247,2.82706299462), test loss: 2.97181961834\n",
      "ADAS Loss Iteration: 14500, train loss(batch, sum): (24.1524887085,29.5868561396), test loss: 30.6413131714\n",
      "MMSE Loss Iteration: 14500, train loss(batch, sum): (1.33528757095,2.79041129979), test loss: 3.12751006186\n",
      "ADAS Loss Iteration: 15000, train loss(batch, sum): (17.2892303467,29.1379652442), test loss: 29.9958935261\n",
      "MMSE Loss Iteration: 15000, train loss(batch, sum): (0.818564832211,2.75668202272), test loss: 3.09389364421\n",
      "ADAS Loss Iteration: 15500, train loss(batch, sum): (6.87691879272,28.7201428458), test loss: 30.8253699541\n",
      "MMSE Loss Iteration: 15500, train loss(batch, sum): (2.33392572403,2.72440814033), test loss: 3.20001366511\n",
      "ADAS Loss Iteration: 16000, train loss(batch, sum): (20.0315551758,28.3262990279), test loss: 29.2601736307\n",
      "MMSE Loss Iteration: 16000, train loss(batch, sum): (2.39641094208,2.69359937697), test loss: 3.42912216485\n",
      "ADAS Loss Iteration: 16500, train loss(batch, sum): (11.204536438,27.9446237381), test loss: 29.2659316063\n",
      "MMSE Loss Iteration: 16500, train loss(batch, sum): (1.91882205009,2.66499875733), test loss: 3.35397126675\n",
      "ADAS Loss Iteration: 17000, train loss(batch, sum): (18.6125679016,27.5844161963), test loss: 30.146695441\n",
      "MMSE Loss Iteration: 17000, train loss(batch, sum): (2.00398492813,2.63761622183), test loss: 3.57831225246\n",
      "ADAS Loss Iteration: 17500, train loss(batch, sum): (14.9439315796,27.243737058), test loss: 31.4137921095\n",
      "MMSE Loss Iteration: 17500, train loss(batch, sum): (1.13691759109,2.61148193309), test loss: 3.5343947798\n",
      "ADAS Loss Iteration: 18000, train loss(batch, sum): (7.44673061371,26.9144119755), test loss: 27.4628021479\n",
      "MMSE Loss Iteration: 18000, train loss(batch, sum): (2.84644079208,2.58642866391), test loss: 2.87820378542\n",
      "ADAS Loss Iteration: 18500, train loss(batch, sum): (10.2868537903,26.599943593), test loss: 26.4511648893\n",
      "MMSE Loss Iteration: 18500, train loss(batch, sum): (1.72020173073,2.56318369986), test loss: 2.89460724294\n",
      "ADAS Loss Iteration: 19000, train loss(batch, sum): (38.5372848511,26.3043110905), test loss: 26.95789814\n",
      "MMSE Loss Iteration: 19000, train loss(batch, sum): (4.02158260345,2.54073165293), test loss: 2.9773509711\n",
      "ADAS Loss Iteration: 19500, train loss(batch, sum): (8.97047233582,26.0148740902), test loss: 27.9242569923\n",
      "MMSE Loss Iteration: 19500, train loss(batch, sum): (0.555060744286,2.51888723995), test loss: 2.98386490941\n",
      "ADAS Loss Iteration: 20000, train loss(batch, sum): (16.7210903168,25.7371499184), test loss: 29.3833114147\n",
      "MMSE Loss Iteration: 20000, train loss(batch, sum): (1.15416145325,2.49847801644), test loss: 2.99050990641\n",
      "ADAS Loss Iteration: 20500, train loss(batch, sum): (39.4860534668,25.4758734296), test loss: 29.9573174834\n",
      "MMSE Loss Iteration: 20500, train loss(batch, sum): (1.54801273346,2.4789070985), test loss: 3.04988436699\n",
      "ADAS Loss Iteration: 21000, train loss(batch, sum): (11.8447380066,25.2228884874), test loss: 29.1399826527\n",
      "MMSE Loss Iteration: 21000, train loss(batch, sum): (1.753895998,2.45954007806), test loss: 3.08552538753\n",
      "ADAS Loss Iteration: 21500, train loss(batch, sum): (5.77557468414,24.9757727535), test loss: 28.6088038445\n",
      "MMSE Loss Iteration: 21500, train loss(batch, sum): (1.28126621246,2.44177631918), test loss: 3.05605155826\n",
      "ADAS Loss Iteration: 22000, train loss(batch, sum): (18.3348045349,24.7423575342), test loss: 30.2028321743\n",
      "MMSE Loss Iteration: 22000, train loss(batch, sum): (0.744639992714,2.42431439982), test loss: 3.30522640049\n",
      "ADAS Loss Iteration: 22500, train loss(batch, sum): (25.2445640564,24.5195750525), test loss: 28.7033826828\n",
      "MMSE Loss Iteration: 22500, train loss(batch, sum): (0.839037537575,2.40730301013), test loss: 3.4876489196\n",
      "ADAS Loss Iteration: 23000, train loss(batch, sum): (20.8776111603,24.2985673412), test loss: 27.2630812645\n",
      "MMSE Loss Iteration: 23000, train loss(batch, sum): (0.880917608738,2.39147212772), test loss: 3.2855584532\n",
      "ADAS Loss Iteration: 23500, train loss(batch, sum): (11.128698349,24.0891429385), test loss: 29.734378767\n",
      "MMSE Loss Iteration: 23500, train loss(batch, sum): (1.58707976341,2.37596243407), test loss: 3.7867420271\n",
      "ADAS Loss Iteration: 24000, train loss(batch, sum): (15.9963188171,23.8892263776), test loss: 30.8962418318\n",
      "MMSE Loss Iteration: 24000, train loss(batch, sum): (0.848164498806,2.36078526306), test loss: 3.46191453636\n",
      "ADAS Loss Iteration: 24500, train loss(batch, sum): (19.7997493744,23.6917176539), test loss: 27.0024568319\n",
      "MMSE Loss Iteration: 24500, train loss(batch, sum): (3.27815198898,2.34653487998), test loss: 2.9550108254\n",
      "ADAS Loss Iteration: 25000, train loss(batch, sum): (19.0196304321,23.5027248385), test loss: 26.524322319\n",
      "MMSE Loss Iteration: 25000, train loss(batch, sum): (1.83384895325,2.33263278924), test loss: 2.89609621763\n",
      "ADAS Loss Iteration: 25500, train loss(batch, sum): (12.1658811569,23.3219183302), test loss: 26.1584763527\n",
      "MMSE Loss Iteration: 25500, train loss(batch, sum): (0.989177942276,2.31909461059), test loss: 2.90838941336\n",
      "ADAS Loss Iteration: 26000, train loss(batch, sum): (10.8845443726,23.1442778817), test loss: 27.2294464469\n",
      "MMSE Loss Iteration: 26000, train loss(batch, sum): (3.10499858856,2.30585690584), test loss: 2.9990505442\n",
      "ADAS Loss Iteration: 26500, train loss(batch, sum): (15.4320268631,22.9726056988), test loss: 28.4513244152\n",
      "MMSE Loss Iteration: 26500, train loss(batch, sum): (2.34699702263,2.29352828365), test loss: 2.94285922796\n",
      "ADAS Loss Iteration: 27000, train loss(batch, sum): (35.6317062378,22.8092811132), test loss: 30.343420887\n",
      "MMSE Loss Iteration: 27000, train loss(batch, sum): (3.31800436974,2.28134913204), test loss: 3.08121986985\n",
      "ADAS Loss Iteration: 27500, train loss(batch, sum): (9.29803180695,22.647142896), test loss: 29.7698222041\n",
      "MMSE Loss Iteration: 27500, train loss(batch, sum): (2.17911100388,2.26927344396), test loss: 3.02196563482\n",
      "ADAS Loss Iteration: 28000, train loss(batch, sum): (9.97175312042,22.4898601653), test loss: 28.3181007624\n",
      "MMSE Loss Iteration: 28000, train loss(batch, sum): (0.624849081039,2.25792942248), test loss: 3.08628626168\n",
      "ADAS Loss Iteration: 28500, train loss(batch, sum): (39.0358543396,22.3404013435), test loss: 30.1094066381\n",
      "MMSE Loss Iteration: 28500, train loss(batch, sum): (1.3025188446,2.24691910425), test loss: 3.33600897193\n",
      "ADAS Loss Iteration: 29000, train loss(batch, sum): (12.4895915985,22.1939851328), test loss: 28.0477880955\n",
      "MMSE Loss Iteration: 29000, train loss(batch, sum): (1.80851197243,2.23576171094), test loss: 3.23606314957\n",
      "ADAS Loss Iteration: 29500, train loss(batch, sum): (15.9306592941,22.0494632807), test loss: 27.0118401289\n",
      "MMSE Loss Iteration: 29500, train loss(batch, sum): (1.44683170319,2.22552200997), test loss: 3.31132737547\n",
      "ADAS Loss Iteration: 30000, train loss(batch, sum): (13.1657094955,21.9112563379), test loss: 29.2361866951\n",
      "MMSE Loss Iteration: 30000, train loss(batch, sum): (0.798545777798,2.21530690266), test loss: 3.83952543736\n",
      "ADAS Loss Iteration: 30500, train loss(batch, sum): (23.6501197815,21.7787206961), test loss: 30.140858984\n",
      "MMSE Loss Iteration: 30500, train loss(batch, sum): (0.556474685669,2.20517397111), test loss: 3.47701165974\n",
      "ADAS Loss Iteration: 31000, train loss(batch, sum): (23.361541748,21.64561221), test loss: 27.0725527287\n",
      "MMSE Loss Iteration: 31000, train loss(batch, sum): (0.658779203892,2.19570451466), test loss: 2.88321378231\n",
      "ADAS Loss Iteration: 31500, train loss(batch, sum): (8.47256946564,21.5181892683), test loss: 27.527257967\n",
      "MMSE Loss Iteration: 31500, train loss(batch, sum): (0.774028897285,2.18630254853), test loss: 2.95591364801\n",
      "ADAS Loss Iteration: 32000, train loss(batch, sum): (9.00113868713,21.3957273512), test loss: 26.2599715233\n",
      "MMSE Loss Iteration: 32000, train loss(batch, sum): (0.239084616303,2.17698878589), test loss: 2.92307940125\n",
      "ADAS Loss Iteration: 32500, train loss(batch, sum): (19.5704154968,21.2736051479), test loss: 26.663211298\n",
      "MMSE Loss Iteration: 32500, train loss(batch, sum): (5.77069950104,2.16819111272), test loss: 3.04371269345\n",
      "ADAS Loss Iteration: 33000, train loss(batch, sum): (13.2272081375,21.1554458121), test loss: 27.6691861391\n",
      "MMSE Loss Iteration: 33000, train loss(batch, sum): (3.10235977173,2.15950525426), test loss: 3.01153104305\n",
      "ADAS Loss Iteration: 33500, train loss(batch, sum): (11.9810752869,21.0419592826), test loss: 30.0886598349\n",
      "MMSE Loss Iteration: 33500, train loss(batch, sum): (0.574194192886,2.15094302126), test loss: 2.94049719572\n",
      "ADAS Loss Iteration: 34000, train loss(batch, sum): (17.5925273895,20.9291604911), test loss: 29.2622732162\n",
      "MMSE Loss Iteration: 34000, train loss(batch, sum): (2.09146976471,2.14244717547), test loss: 3.00211700201\n",
      "ADAS Loss Iteration: 34500, train loss(batch, sum): (25.089214325,20.8193642427), test loss: 28.1678400278\n",
      "MMSE Loss Iteration: 34500, train loss(batch, sum): (2.38383579254,2.13456786971), test loss: 3.08135040849\n",
      "ADAS Loss Iteration: 35000, train loss(batch, sum): (26.1955356598,20.7140896801), test loss: 29.7926568508\n",
      "MMSE Loss Iteration: 35000, train loss(batch, sum): (3.1246278286,2.12667772724), test loss: 3.30833461583\n",
      "ADAS Loss Iteration: 35500, train loss(batch, sum): (10.7420253754,20.6088475176), test loss: 28.3274877787\n",
      "MMSE Loss Iteration: 35500, train loss(batch, sum): (3.97711992264,2.11872735282), test loss: 3.22086729109\n",
      "ADAS Loss Iteration: 36000, train loss(batch, sum): (11.1291303635,20.5057932721), test loss: 27.8935487747\n",
      "MMSE Loss Iteration: 36000, train loss(batch, sum): (0.51440513134,2.11122808125), test loss: 3.50934836864\n",
      "ADAS Loss Iteration: 36500, train loss(batch, sum): (16.0882205963,20.4066959417), test loss: 29.4011418104\n",
      "MMSE Loss Iteration: 36500, train loss(batch, sum): (1.24776887894,2.10389740813), test loss: 3.9417154789\n",
      "ADAS Loss Iteration: 37000, train loss(batch, sum): (12.9540233612,20.3101351628), test loss: 28.6286817551\n",
      "MMSE Loss Iteration: 37000, train loss(batch, sum): (1.5576004982,2.09638262876), test loss: 3.50104192495\n",
      "ADAS Loss Iteration: 37500, train loss(batch, sum): (23.3302459717,20.2134266328), test loss: 27.8741791725\n",
      "MMSE Loss Iteration: 37500, train loss(batch, sum): (1.8564953804,2.08946395541), test loss: 3.24278161526\n",
      "ADAS Loss Iteration: 38000, train loss(batch, sum): (10.446680069,20.120146607), test loss: 27.6909331083\n",
      "MMSE Loss Iteration: 38000, train loss(batch, sum): (1.97418391705,2.08249706247), test loss: 2.9492970705\n",
      "ADAS Loss Iteration: 38500, train loss(batch, sum): (23.4531040192,20.0303400583), test loss: 26.083498764\n",
      "MMSE Loss Iteration: 38500, train loss(batch, sum): (0.483077764511,2.0755132841), test loss: 2.86076196879\n",
      "ADAS Loss Iteration: 39000, train loss(batch, sum): (13.144235611,19.9392950882), test loss: 26.5540017605\n",
      "MMSE Loss Iteration: 39000, train loss(batch, sum): (1.19757890701,2.06897477382), test loss: 3.02439644635\n",
      "ADAS Loss Iteration: 39500, train loss(batch, sum): (6.89932250977,19.8519471554), test loss: 27.033851862\n",
      "MMSE Loss Iteration: 39500, train loss(batch, sum): (0.559481620789,2.06241565456), test loss: 3.04132549167\n",
      "ADAS Loss Iteration: 40000, train loss(batch, sum): (5.6783323288,19.767311289), test loss: 30.1666445971\n",
      "MMSE Loss Iteration: 40000, train loss(batch, sum): (0.103972882032,2.05586236213), test loss: 2.96246539354\n",
      "ADAS Loss Iteration: 40500, train loss(batch, sum): (14.3209896088,19.6822964884), test loss: 29.3909119129\n",
      "MMSE Loss Iteration: 40500, train loss(batch, sum): (4.90415334702,2.04961322681), test loss: 3.07435581684\n",
      "ADAS Loss Iteration: 41000, train loss(batch, sum): (7.25539302826,19.5994688066), test loss: 29.4739139557\n",
      "MMSE Loss Iteration: 41000, train loss(batch, sum): (2.75798940659,2.04343835511), test loss: 3.10242494047\n",
      "ADAS Loss Iteration: 41500, train loss(batch, sum): (7.02616930008,19.5197089307), test loss: 29.0374745131\n",
      "MMSE Loss Iteration: 41500, train loss(batch, sum): (0.992848694324,2.03732392074), test loss: 3.14153246135\n",
      "ADAS Loss Iteration: 42000, train loss(batch, sum): (18.2476234436,19.4398188348), test loss: 29.506888032\n",
      "MMSE Loss Iteration: 42000, train loss(batch, sum): (1.38669252396,2.0311556666), test loss: 3.24053002298\n",
      "ADAS Loss Iteration: 42500, train loss(batch, sum): (26.5982589722,19.3614691038), test loss: 28.7246225595\n",
      "MMSE Loss Iteration: 42500, train loss(batch, sum): (3.29732394218,2.0254444283), test loss: 3.4982185483\n",
      "ADAS Loss Iteration: 43000, train loss(batch, sum): (6.28606462479,19.2857163241), test loss: 29.7538407326\n",
      "MMSE Loss Iteration: 43000, train loss(batch, sum): (0.356941729784,2.01963680385), test loss: 3.85798170567\n",
      "ADAS Loss Iteration: 43500, train loss(batch, sum): (5.13881111145,19.2101443889), test loss: 28.8809300542\n",
      "MMSE Loss Iteration: 43500, train loss(batch, sum): (4.43953037262,2.01385036626), test loss: 3.63369167149\n",
      "ADAS Loss Iteration: 44000, train loss(batch, sum): (9.89925956726,19.1354361143), test loss: 29.7771220684\n",
      "MMSE Loss Iteration: 44000, train loss(batch, sum): (0.761083364487,2.0083054602), test loss: 3.54063389152\n",
      "ADAS Loss Iteration: 44500, train loss(batch, sum): (5.37248277664,19.062949294), test loss: 27.6493382454\n",
      "MMSE Loss Iteration: 44500, train loss(batch, sum): (2.35882568359,2.00285718351), test loss: 2.98453075886\n",
      "ADAS Loss Iteration: 45000, train loss(batch, sum): (9.65695571899,18.992301562), test loss: 26.1034314394\n",
      "MMSE Loss Iteration: 45000, train loss(batch, sum): (1.79949235916,1.99721486163), test loss: 2.87230484784\n",
      "ADAS Loss Iteration: 45500, train loss(batch, sum): (26.4561157227,18.9210945638), test loss: 26.6869710445\n",
      "MMSE Loss Iteration: 45500, train loss(batch, sum): (2.50952744484,1.9920186598), test loss: 3.0227280587\n",
      "ADAS Loss Iteration: 46000, train loss(batch, sum): (6.93364334106,18.8518726035), test loss: 27.1672344208\n",
      "MMSE Loss Iteration: 46000, train loss(batch, sum): (2.19526219368,1.98676041783), test loss: 3.0721565634\n",
      "ADAS Loss Iteration: 46500, train loss(batch, sum): (22.652973175,18.7849086995), test loss: 29.4911509275\n",
      "MMSE Loss Iteration: 46500, train loss(batch, sum): (0.475004255772,1.98144090231), test loss: 3.00579687059\n",
      "ADAS Loss Iteration: 47000, train loss(batch, sum): (11.7999706268,18.7168084217), test loss: 29.4523673296\n",
      "MMSE Loss Iteration: 47000, train loss(batch, sum): (0.95241767168,1.97644316815), test loss: 3.00044541657\n",
      "ADAS Loss Iteration: 47500, train loss(batch, sum): (9.69532585144,18.6509874348), test loss: 30.2157002449\n",
      "MMSE Loss Iteration: 47500, train loss(batch, sum): (1.19908094406,1.97140621745), test loss: 3.17366543412\n",
      "ADAS Loss Iteration: 48000, train loss(batch, sum): (5.57893896103,18.5871600117), test loss: 29.2277896404\n",
      "MMSE Loss Iteration: 48000, train loss(batch, sum): (0.300471186638,1.96635603851), test loss: 3.12849196494\n",
      "ADAS Loss Iteration: 48500, train loss(batch, sum): (4.97649335861,18.5223074675), test loss: 30.5727477074\n",
      "MMSE Loss Iteration: 48500, train loss(batch, sum): (4.02041053772,1.96147870231), test loss: 3.34806689322\n",
      "ADAS Loss Iteration: 49000, train loss(batch, sum): (5.77201557159,18.4591303315), test loss: 29.2347454071\n",
      "MMSE Loss Iteration: 49000, train loss(batch, sum): (3.2065346241,1.95668195853), test loss: 3.50026035011\n",
      "ADAS Loss Iteration: 49500, train loss(batch, sum): (6.86431026459,18.3980345177), test loss: 27.8228395939\n",
      "MMSE Loss Iteration: 49500, train loss(batch, sum): (0.717551469803,1.95188946344), test loss: 3.34489898384\n",
      "ADAS Loss Iteration: 50000, train loss(batch, sum): (12.888874054,18.3363893138), test loss: 29.4036075592\n",
      "MMSE Loss Iteration: 50000, train loss(batch, sum): (0.780584394932,1.94702730588), test loss: 3.74398671985\n",
      "ADAS Loss Iteration: 50500, train loss(batch, sum): (26.5510501862,18.275687092), test loss: 30.6984589338\n",
      "MMSE Loss Iteration: 50500, train loss(batch, sum): (2.64433407784,1.94251258302), test loss: 3.56761954576\n",
      "ADAS Loss Iteration: 51000, train loss(batch, sum): (3.48854351044,18.2167864822), test loss: 28.7521432161\n",
      "MMSE Loss Iteration: 51000, train loss(batch, sum): (0.207032188773,1.93790931813), test loss: 3.10827937424\n",
      "ADAS Loss Iteration: 51500, train loss(batch, sum): (6.96234607697,18.1577250928), test loss: 26.9163171291\n",
      "MMSE Loss Iteration: 51500, train loss(batch, sum): (3.45188045502,1.9332577089), test loss: 2.94663276076\n",
      "ADAS Loss Iteration: 52000, train loss(batch, sum): (13.5884418488,18.0991107311), test loss: 26.6948484898\n",
      "MMSE Loss Iteration: 52000, train loss(batch, sum): (0.949000835419,1.92884285121), test loss: 3.01708811224\n",
      "ADAS Loss Iteration: 52500, train loss(batch, sum): (7.71374893188,18.0418500249), test loss: 27.6260096073\n",
      "MMSE Loss Iteration: 52500, train loss(batch, sum): (2.40900182724,1.92445750176), test loss: 3.10341572165\n",
      "ADAS Loss Iteration: 53000, train loss(batch, sum): (6.93758296967,17.9859125582), test loss: 28.6333455563\n",
      "MMSE Loss Iteration: 53000, train loss(batch, sum): (1.68306446075,1.91988529812), test loss: 3.01690434217\n",
      "ADAS Loss Iteration: 53500, train loss(batch, sum): (15.1127214432,17.9291651415), test loss: 29.6414110661\n",
      "MMSE Loss Iteration: 53500, train loss(batch, sum): (2.35916900635,1.91566271876), test loss: 3.00785197616\n",
      "ADAS Loss Iteration: 54000, train loss(batch, sum): (4.2112865448,17.8739467046), test loss: 29.9653142691\n",
      "MMSE Loss Iteration: 54000, train loss(batch, sum): (2.20736408234,1.91137643865), test loss: 3.11732013226\n",
      "ADAS Loss Iteration: 54500, train loss(batch, sum): (19.2789325714,17.8202894893), test loss: 29.1693738937\n",
      "MMSE Loss Iteration: 54500, train loss(batch, sum): (1.2024269104,1.90702650882), test loss: 3.15574918687\n",
      "ADAS Loss Iteration: 55000, train loss(batch, sum): (6.65268659592,17.7654964559), test loss: 29.9850426674\n",
      "MMSE Loss Iteration: 55000, train loss(batch, sum): (1.31878757477,1.90291022164), test loss: 3.30425621569\n",
      "ADAS Loss Iteration: 55500, train loss(batch, sum): (11.0793609619,17.7123079264), test loss: 28.7368386149\n",
      "MMSE Loss Iteration: 55500, train loss(batch, sum): (1.2308113575,1.89875243686), test loss: 3.35070341527\n",
      "ADAS Loss Iteration: 56000, train loss(batch, sum): (7.42793703079,17.6605497464), test loss: 27.7818676949\n",
      "MMSE Loss Iteration: 56000, train loss(batch, sum): (0.305391639471,1.89456946322), test loss: 3.31997453421\n",
      "ADAS Loss Iteration: 56500, train loss(batch, sum): (5.58141708374,17.6077672988), test loss: 29.6357357979\n",
      "MMSE Loss Iteration: 56500, train loss(batch, sum): (1.78422224522,1.89046708889), test loss: 3.82600813806\n",
      "ADAS Loss Iteration: 57000, train loss(batch, sum): (7.87203359604,17.5560854965), test loss: 30.9438231707\n",
      "MMSE Loss Iteration: 57000, train loss(batch, sum): (1.41014504433,1.88647305147), test loss: 3.50957134962\n",
      "ADAS Loss Iteration: 57500, train loss(batch, sum): (4.65474557877,17.5059090854), test loss: 27.590160954\n",
      "MMSE Loss Iteration: 57500, train loss(batch, sum): (1.42994093895,1.88248187803), test loss: 2.92646539584\n",
      "ADAS Loss Iteration: 58000, train loss(batch, sum): (5.36818408966,17.4550387361), test loss: 28.1081691027\n",
      "MMSE Loss Iteration: 58000, train loss(batch, sum): (0.671194374561,1.87838707994), test loss: 2.96107217073\n",
      "ADAS Loss Iteration: 58500, train loss(batch, sum): (15.5717954636,17.4048340917), test loss: 26.8186079025\n",
      "MMSE Loss Iteration: 58500, train loss(batch, sum): (2.24983930588,1.8745647408), test loss: 2.95466135889\n",
      "ADAS Loss Iteration: 59000, train loss(batch, sum): (3.71113562584,17.356146645), test loss: 28.9210121393\n",
      "MMSE Loss Iteration: 59000, train loss(batch, sum): (0.237982094288,1.8706764027), test loss: 3.18210178018\n",
      "ADAS Loss Iteration: 59500, train loss(batch, sum): (14.6368312836,17.3069598314), test loss: 28.5927224636\n",
      "MMSE Loss Iteration: 59500, train loss(batch, sum): (2.98192405701,1.86669120317), test loss: 3.07214646339\n",
      "run time for single CV loop: 1103.37222195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:63: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/mnt/tigrlab/projects/nikhil/ADNI_prediction/code/conda_envs/adni-conda/lib/python2.7/site-packages/ipykernel/__main__.py:65: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "exp_name = 'Exp11_MC'\n",
    "cohort = 'ADNI2'\n",
    "preproc = 'no_preproc'\n",
    "modality = 'HC_CT'\n",
    "Clinical_Scale = 'BOTH'    \n",
    "\n",
    "start_MC=1\n",
    "n_MC=1\n",
    "MC_list = np.arange(start_MC,n_MC+1,1)\n",
    "start_fold = 10\n",
    "n_folds = 10\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "niter = 60000\n",
    "batch_size = 256\n",
    "\n",
    "pretrain = False\n",
    "multimodal_autoencode = False\n",
    "load_pretrained_weights = True\n",
    "multi_task = True\n",
    "\n",
    "#Hyperparameter Search\n",
    "#these are target net-architectures need to match with soruce (pretrained net)\n",
    "hype_configs = {\n",
    "                'hyp1':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':50,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':0,'CT':0,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':5,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},\n",
    "    \n",
    "    \n",
    "                'hyp2':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':200,'HC_CT_ff':25,'COMB_ff':50,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':0.1,'CT':0.1,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':5,'DX':1},'solver_conf':{'base_lr':5e-6, 'wt_decay':1e-3}},\n",
    "    \n",
    "    \n",
    "                'hyp3':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':50,'HC_CT_ff':25,'COMB_ff':50,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':0,'CT':0,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':10,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}},\n",
    "    \n",
    "    \n",
    "                'hyp4':{'node_sizes':{'HC_L_ff':50,'HC_R_ff':50,'CT_ff':200,'HC_CT_ff':25,'COMB_ff':50,\n",
    "                                       'ADAS_ff':25,'MMSE_ff':25,'DX_ff':25,\n",
    "                                       'En1':10000,'En2':2000,'code':600,'out':686},\n",
    "                      'dr':{'HC':0,'CT':0,'HC_CT':0,'COMB':0},'lr':{'HC':0.1,'CT':0.1,'COMB':1},\n",
    "                      'tr':{'ADAS':1,'MMSE':10,'DX':1},'solver_conf':{'base_lr':2e-6, 'wt_decay':1e-3}},\n",
    "                }\n",
    "\n",
    "CV_perf_MC = {}\n",
    "for mc in MC_list:\n",
    "    CV_perf_hype = {}\n",
    "    for hype in hype_configs.keys():    \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "        tr = hype_configs[hype]['tr']\n",
    "        solver_configs = hype_configs[hype]['solver_conf']\n",
    "        \n",
    "        if hype in ['hyp1','hyp3']:\n",
    "            HC_snap = 20000 #20000 for ADNI2 5000 for ADNI1\n",
    "            CT_snap = 20000 #20000 for ADNI2 5000 for ADNI1\n",
    "            pre_hype = 'hyp2'\n",
    "        elif hype in ['hyp2','hyp4']:\n",
    "            HC_snap = 20000 #20000 for ADNI2 5000 for ADNI1\n",
    "            CT_snap = 80000 #20000 for ADNI2 5000 for ADNI1\n",
    "            pre_hype = 'hyp5'\n",
    "        else:\n",
    "            print 'unknown hyp config'\n",
    "            \n",
    "            print hype, pre_hype,HC_snap,CT_snap\n",
    "            \n",
    "        CV_perf = {}\n",
    "        start_time = time.time()\n",
    "        for fid in fid_list:            \n",
    "            print ''\n",
    "            print 'MC # {}, Hype # {}, Fold # {}'.format(mc, hype, fid)\n",
    "            snap_prefix = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}'.format(mc,fid,exp_name,hype,modality)\n",
    "\n",
    "            train_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/train_C688.txt'.format(mc,fid)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "\n",
    "            train_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_train/{}.h5'.format(mc,fid,exp_name)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/inner_test/{}.h5'.format(mc,fid,exp_name)\n",
    "            with open(train_filename_txt, 'w') as f:\n",
    "                    f.write(train_filename_hdf + '\\n')    \n",
    "\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            # Define Net (examples: 'ADNI_AE_train.prototxt', 'ADNI_FF_train.prototxt')\n",
    "            if pretrain:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_train.prototxt'.format(mc,fid)\n",
    "                with open(train_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(train_filename_txt, batch_size, node_sizes, modality)))            \n",
    "            else:\n",
    "                train_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(train_net_path, 'w') as f:            \n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(train_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            if pretrain:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_AE_test.prototxt'.format(mc,fid)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    f.write(str(adninet_ae(test_filename_txt, batch_size, node_sizes,modality)))\n",
    "            else:\n",
    "                test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test_{}_{}.prototxt'.format(mc,fid,hype,modality)\n",
    "                with open(test_net_path, 'w') as f:\n",
    "                    if modality == 'HC':\n",
    "                          f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'CT':\n",
    "                          f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT':\n",
    "                          f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale)))\n",
    "                    elif modality == 'HC_CT_unified_hyp1':\n",
    "                          f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr, lr, tr, Clinical_Scale )))\n",
    "                    else:\n",
    "                          print 'Wrong modality'\n",
    "\n",
    "            # Define Solver\n",
    "            solver_path = baseline_dir + 'API/model_configs/adninet_solver.prototxt'\n",
    "            with open(solver_path, 'w') as f:\n",
    "                f.write(str(adni_solver(train_net_path, test_net_path, solver_configs, snap_prefix)))\n",
    "\n",
    "            ### load the solver and create train and test nets\n",
    "            #solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "            #solver = caffe.get_solver(solver_path)\n",
    "            #solver = caffe.NesterovSolver(solver_path)\n",
    "            solver = caffe.NesterovSolver(solver_path)\n",
    "\n",
    "            if load_pretrained_weights:                    \n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_HC_CT_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                snap_path = baseline_dir + net_name.format(mc,fid,cohort,pre_hype,HC_snap,CT_snap)\n",
    "                print \"loading pretrained weights from {}\".format(snap_path)        \n",
    "                solver.net.copy_from(snap_path)\n",
    "\n",
    "            #run caffe\n",
    "            results = run_caffe(solver,niter,batch_size,multi_task)\n",
    "            CV_perf[fid] = results\n",
    "            \n",
    "        print('run time for single CV loop: {}'.format(time.time() - start_time))\n",
    "\n",
    "        CV_perf_hype[hype] = CV_perf\n",
    "        \n",
    "CV_perf_MC[mc] = CV_perf_hype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-afca03343747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtrain_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCV_perf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mtest_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCV_perf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# plot train loss and acc\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "#niter = 30000\n",
    "test_interval = 500\n",
    "n_mc = len(CV_perf_MC.keys())\n",
    "\n",
    "for m, mc in enumerate(CV_perf_MC.keys()): \n",
    "    CV_perf_hype = CV_perf_MC[mc]\n",
    "    \n",
    "    for hype in CV_perf_hype.keys(): \n",
    "        CV_perf = CV_perf_hype[hype]\n",
    "        n_CV_configs = len(CV_perf)\n",
    "        pid = m*n_CV_configs + 1\n",
    "        \n",
    "        for f, fid in enumerate(fid_list):  \n",
    "            train_loss_list = CV_perf[fid]['train_loss']\n",
    "            test_loss_list = CV_perf[fid]['test_loss']\n",
    "            \n",
    "            for train_loss, test_loss in zip(train_loss_list,test_loss_list):\n",
    "                #plt.figure()\n",
    "                ax1 = plt.subplot(n_mc,n_CV_configs,pid)            \n",
    "                ax1.plot(arange(niter), train_loss, label='train',linewidth='.5',alpha=0.25)\n",
    "                ax1.plot(test_interval * arange(len(test_loss)), test_loss, label='test_{}'.format(hype), linewidth='3')                            \n",
    "                ax1.set_xlabel('iteration')\n",
    "                ax1.set_ylabel('loss')\n",
    "                ax1.set_title('fid: {}, hyp: {}, Test loss: {:.2f}'.format(fid, hype, test_loss[-1]))\n",
    "                ax1.legend(loc=1)\n",
    "                ax1.set_ylim(0,100)\n",
    "            pid += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test performance \n",
    "* **Steps** \n",
    "    1. Get train and test performance different MC,KF and hyper-params\n",
    "    2. Save Multi-task performance separately (ADAS & MMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get encodings after training\n",
    "#Get test performance \n",
    "#Save test performance for differnt folds + hyperparameters \n",
    "#niter = 40000\n",
    "start_fold = 1\n",
    "n_folds = 10\n",
    "fid_list = np.arange(start_fold,n_folds+1,1)\n",
    "snap_interval = 2000\n",
    "snap_start = 2000\n",
    "encoding_layer = 'output'\n",
    "weight_layers = 'output'\n",
    "cohort = 'outer_test'\n",
    "\n",
    "if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "    fold_euLoss = {}\n",
    "    fold_r = {}\n",
    "    fold_act_scores = {}\n",
    "    fold_pred_scores = {}\n",
    "elif Clinical_Scale == 'BOTH':\n",
    "    fold_euLoss_adas13 = {}\n",
    "    fold_r_adas13 = {}\n",
    "    fold_act_scores_adas13 = {}\n",
    "    fold_pred_scores_adas13 = {}\n",
    "    fold_euLoss_mmse = {}\n",
    "    fold_r_mmse = {}\n",
    "    fold_act_scores_mmse = {}\n",
    "    fold_pred_scores_mmse = {}\n",
    "    \n",
    "idx = 0  \n",
    "df_perf_dict_adas = {}\n",
    "df_perf_dict_mmse = {}\n",
    "df_perf_dict_adas_tuned = {}\n",
    "df_perf_dict_mmse_tuned = {}\n",
    "df_perf_dict_opt = {}\n",
    "\n",
    "for mc in MC_list:\n",
    "    print exp_name, mc, modality\n",
    "\n",
    "    for hype in hype_configs.keys():      \n",
    "        node_sizes = hype_configs[hype]['node_sizes']\n",
    "        dr = hype_configs[hype]['dr']\n",
    "        lr = hype_configs[hype]['lr']\n",
    "\n",
    "        for fid in fid_list:\n",
    "            if fid in [4,5,6,7]:\n",
    "                niter = 40000\n",
    "            else:\n",
    "                niter = 20000\n",
    "                \n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)\n",
    "            #test_filename_hdf = baseline_dir + 'API/data/fold{}/outer_test/{}_{}.h5'.format(fid,exp_name,preproc)\n",
    "            #with open(test_filename_txt, 'w') as f:\n",
    "            #        f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            test_net_path = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            with open(test_net_path, 'w') as f:\n",
    "                if modality == 'HC':\n",
    "                    f.write(str(adninet_ff_HC(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC']\n",
    "                elif modality == 'CT':\n",
    "                    f.write(str(adninet_ff_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_CT_SpecCluster_dyn']\n",
    "                elif modality == 'HC_CT':\n",
    "                    f.write(str(adninet_ff_HC_CT(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_L_HC','X_R_HC','X_CT_SpecCluster_dyn']\n",
    "                elif modality == 'HC_CT_unified_hyp1':\n",
    "                    f.write(str(adninet_ff_HC_CT_unified(test_filename_txt, batch_size, node_sizes,dr,lr,tr,Clinical_Scale)))\n",
    "                    input_nodes = ['X_HC_CT']\n",
    "                else:\n",
    "                    print 'Wrong modality'\n",
    "\n",
    "            #print 'Hype # {}, MC # {}, Fold # {}, Clinical_Scale {}'.format(hype, mc, fid, Clinical_Scale)\n",
    "            data_path = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            if Clinical_Scale == 'ADAS13':\n",
    "                act_scores = load_data(data_path, 'adas','no_preproc')\n",
    "            elif Clinical_Scale == 'MMSE': \n",
    "                act_scores = load_data(data_path, 'mmse','no_preproc')\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                act_scores_adas13 = load_data(data_path, 'adas','no_preproc')\n",
    "                act_scores_mmse = load_data(data_path, 'mmse','no_preproc')\n",
    "            else:\n",
    "                print 'unknown clinical scale'\n",
    "\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_test.prototxt'.format(mc,fid)\n",
    "            test_filename_hdf = baseline_dir + 'API/data/MC_{}/fold{}/{}/{}.h5'.format(mc,fid,cohort,exp_name)\n",
    "            test_filename_txt = baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort)\n",
    "            with open(test_filename_txt, 'w') as f:\n",
    "                    f.write(test_filename_hdf + '\\n')  \n",
    "\n",
    "            sub.call([\"cp\", baseline_dir + 'API/data/MC_{}/fold{}/{}_C688.txt'.format(mc,fid,cohort), baseline_dir + \n",
    "                      'API/data/MC_{}/fold{}/test_C688.txt'.format(mc,fid)])\n",
    "            #data_path = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/CV_Exp4_ADNI1_ADAS13_NN_valid.h5'\n",
    "            #adas_scores = load_data(data_path, 'Fold_{}_y'.format(fid),'no_preproc')\n",
    "\n",
    "            if Clinical_Scale in ['ADAS13', 'MMSE']:                \n",
    "                multi_task = False\n",
    "                cs_list = ['opt']\n",
    "                iter_euLoss = []\n",
    "                iter_r = []        \n",
    "                iter_pred_scores = []\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = np.squeeze(results['X_out'])            \n",
    "                    iter_pred_scores.append(np.squeeze(results['X_out']))            \n",
    "                    iter_euLoss.append(0.5*mse(encodings,act_scores))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r.append(stats.pearsonr(encodings,act_scores)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss[config_idx] = np.array(iter_euLoss)\n",
    "                fold_r[config_idx] = np.array(iter_r)\n",
    "                fold_act_scores[fid] = act_scores\n",
    "                fold_pred_scores[config_idx] = np.array(iter_pred_scores)        \n",
    "\n",
    "            elif Clinical_Scale == 'BOTH':\n",
    "                multi_task = True\n",
    "                cs_list = ['adas','mmse']\n",
    "                iter_euLoss_adas13 = []\n",
    "                iter_r_adas13 = []        \n",
    "                iter_pred_scores_adas13 = []\n",
    "                iter_euLoss_mmse = []\n",
    "                iter_r_mmse = []        \n",
    "                iter_pred_scores_mmse = []\n",
    "\n",
    "                for snap_iter in np.arange(snap_start,niter+1,snap_interval):\n",
    "                    model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hype,modality,snap_iter)        \n",
    "                    results = extract_features(net_file, model_file, data_path, input_nodes, batch_size, encoding_layer, weight_layers,multi_task)\n",
    "                    encodings = results['X_out']   \n",
    "                    encodings_adas13 = np.squeeze(encodings['adas13'])\n",
    "                    encodings_mmse = np.squeeze(encodings['mmse'])\n",
    "                    iter_pred_scores_adas13.append(encodings_adas13)            \n",
    "                    iter_pred_scores_mmse.append(encodings_mmse)                 \n",
    "                    iter_euLoss_adas13.append(0.5*mse(encodings_adas13,act_scores_adas13))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_euLoss_mmse.append(0.5*mse(encodings_mmse,act_scores_mmse))  #This is to be consistent with the caffe loss funtion\n",
    "                    iter_r_adas13.append(stats.pearsonr(encodings_adas13,act_scores_adas13)[0])                                \n",
    "                    iter_r_mmse.append(stats.pearsonr(encodings_mmse,act_scores_mmse)[0])\n",
    "\n",
    "                config_idx = '{}_{}'.format(hype,fid)\n",
    "                fold_euLoss_adas13[config_idx] = np.array(iter_euLoss_adas13)\n",
    "                fold_r_adas13[config_idx] = np.array(iter_r_adas13)\n",
    "                fold_act_scores_adas13[fid] = act_scores_adas13\n",
    "                fold_pred_scores_adas13[config_idx] = np.array(iter_pred_scores_adas13)        \n",
    "                fold_euLoss_mmse[config_idx] = np.array(iter_euLoss_mmse)\n",
    "                fold_r_mmse[config_idx] = np.array(iter_r_mmse)\n",
    "                fold_act_scores_mmse[fid] = act_scores_mmse\n",
    "                fold_pred_scores_mmse[config_idx] = np.array(iter_pred_scores_mmse)        \n",
    "\n",
    "    \n",
    "    if Clinical_Scale in ['ADAS13', 'MMSE']:\n",
    "        fold_perf_dict = {'fold_r':fold_r,'fold_euLoss':fold_euLoss,'fold_act_scores':fold_act_scores,'fold_pred_scores':fold_pred_scores}\n",
    "    else: \n",
    "        fold_perf_dict = {'fold_r_adas13':fold_r_adas13,'fold_r_mmse':fold_r_mmse,'fold_euLoss_adas13':fold_euLoss_adas13,'fold_euLoss_mmse':fold_euLoss_mmse,\n",
    "                         'fold_act_scores_adas13':fold_act_scores_adas13,'fold_act_scores_mmse':fold_act_scores_mmse,\n",
    "                          'fold_pred_scores_adas13':fold_pred_scores_adas13,'fold_pred_scores_mmse':fold_pred_scores_mmse}\n",
    "        \n",
    "    opt_metric = 'euLoss' #euLoss or corr or both\n",
    "    #task_weights = {'ADAS':1,'MMSE':0} \n",
    "    save_multitask_results = True\n",
    "    CV_model_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/output/'  \n",
    "    \n",
    "    \n",
    "    # Create dictionaries of perfromance based on differnt task weights \n",
    "    for key, task_weights in {'adas':{'ADAS':1,'MMSE':0},'mmse':{'ADAS':0,'MMSE':1},'both':{'ADAS':1,'MMSE':1}}.items():\n",
    "        print 'Weights Tuned for: {}'.format(key)            \n",
    "        save_path = '{}{}_{}_NN_{}_{}'.format(CV_model_dir,exp_name,mc,modality,key)\n",
    "        results = compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path)  \n",
    "\n",
    "        # populate the perf dictionary for 10 MC x 10 folds\n",
    "        model_choice = 'APANN'    \n",
    "        for f, fid in enumerate(fid_list): \n",
    "            if key in ['adas','mmse']:\n",
    "                r_valid = results['{}_r'.format(key)][f]\n",
    "                MSE_valid = results['{}_mse'.format(key)][f]\n",
    "                RMSE_valid = results['{}_rmse'.format(key)][f]\n",
    "\n",
    "                if key == 'adas':\n",
    "                    df_perf_dict_adas_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                elif key == 'mmse':\n",
    "                    df_perf_dict_mmse_tuned[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                else: \n",
    "                    print 'unknown key'\n",
    "\n",
    "            elif key == 'both':                    \n",
    "                for cs in cs_list:            \n",
    "                    r_valid = results['{}_r'.format(cs)][f]\n",
    "                    MSE_valid = results['{}_mse'.format(cs)][f]\n",
    "                    RMSE_valid = results['{}_rmse'.format(cs)][f]\n",
    "\n",
    "                    if cs == 'adas':\n",
    "                        df_perf_dict_adas[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "                    elif cs == 'mmse':\n",
    "                        df_perf_dict_mmse[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}                    \n",
    "                    else: \n",
    "                        print 'unknown key'\n",
    "\n",
    "\n",
    "            else: #single task dictionary\n",
    "                df_perf_dict_opt[idx] = {'MC':mc,'modality':modality,'model_choice':model_choice,'KF':fid,\n",
    "                                         'CV_MSE':MSE_valid,'CV_RMSE':RMSE_valid,'CV_r':r_valid,'CV_R2':0}\n",
    "\n",
    "            idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [0.60903169673792368, 0.71434949656721292, 0.49791200608909475, 0.69978293192390562, 0.64053223720204533, \n",
    "#  0.71143760788509269, 0.65628427463395012, 0.43717377780089195, 0.66996926753104591, 0.61654300081125735]\n",
    "\n",
    "\n",
    "#Save df style dictionaly for seaborn plots\n",
    "cohort = 'ADNI1'\n",
    "update = 1\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_up_{}.pkl'.format(exp_name, cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse,df_perf_dict_path)\n",
    "\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_ADAS13_tuned_up_{}.pkl'.format(exp_name,cohort,modality,update)                \n",
    "pickleIt(df_perf_dict_adas_tuned,df_perf_dict_path)\n",
    "df_perf_dict_path = CV_model_dir + 'df_perf_dict_{}_{}_{}_MMSE_tuned_up_{}.pkl'.format(exp_name, cohort,modality,update)  \n",
    "pickleIt(df_perf_dict_mmse_tuned,df_perf_dict_path)\n",
    "\n",
    "\n",
    "print 'saving results at: {}'.format(CV_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_MC_results(fold_perf_dict, multi_task, opt_metric, task_weights, save_multitask_results, save_path):\n",
    "    fold_r_dict = {}\n",
    "    fold_euLoss_dict = {}\n",
    "    fold_act_scores_dict = {}\n",
    "    fold_pred_scores_dict = {}\n",
    "\n",
    "    if multi_task:\n",
    "        fold_r_dict['ADAS'] = fold_perf_dict['fold_r_adas13']\n",
    "        fold_r_dict['MMSE'] = fold_perf_dict['fold_r_mmse']\n",
    "        fold_euLoss_dict['ADAS'] = fold_perf_dict['fold_euLoss_adas13']\n",
    "        fold_euLoss_dict['MMSE'] = fold_perf_dict['fold_euLoss_mmse']\n",
    "        fold_act_scores_dict['ADAS'] = fold_perf_dict['fold_act_scores_adas13']\n",
    "        fold_act_scores_dict['MMSE'] = fold_perf_dict['fold_act_scores_mmse']\n",
    "        fold_pred_scores_dict['ADAS'] = fold_perf_dict['fold_pred_scores_adas13']\n",
    "        fold_pred_scores_dict['MMSE'] = fold_perf_dict['fold_pred_scores_mmse']\n",
    "        \n",
    "        NN_results = computePerfMetrics(fold_r_dict, fold_euLoss_dict, fold_act_scores_dict, fold_pred_scores_dict, opt_metric, hype_configs, \n",
    "                                        Clinical_Scale,task_weights)\n",
    "        adas_r = NN_results['opt_ADAS']['CV_r'].values()\n",
    "        mmse_r = NN_results['opt_MMSE']['CV_r'].values()\n",
    "        adas_mse = NN_results['opt_ADAS']['CV_MSE'].values()\n",
    "        mmse_mse = NN_results['opt_MMSE']['CV_MSE'].values()\n",
    "        adas_rmse = NN_results['opt_ADAS']['CV_RMSE'].values()\n",
    "        mmse_rmse = NN_results['opt_MMSE']['CV_RMSE'].values()\n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['opt_ADAS']['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['opt_ADAS']['actual_CV_scores']\n",
    "        \n",
    "        print 'ADAS corr: {}'.format(adas_r)\n",
    "        print 'ADAS mse: {}'.format(adas_mse)\n",
    "        print 'ADAS means: {}, {}'.format(np.mean(adas_r),np.mean(adas_mse))\n",
    "        print ''\n",
    "        print 'MMSE corr: {}'.format(mmse_r)\n",
    "        print 'MMSE mse: {}'.format(mmse_mse)\n",
    "        print 'MMSE means: {}, {}'.format(np.mean(mmse_r),np.mean(mmse_mse))\n",
    "        print ''\n",
    "        print 'opt_hyp: {}'.format(opt_hyp)\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        \n",
    "        if save_multitask_results:\n",
    "            ts = time.time()\n",
    "            st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')        \n",
    "            save_path = save_path + '_' + st + '.pkl' \n",
    "            print 'saving results at: {}'.format(save_path)\n",
    "            output = open(save_path, 'wb')\n",
    "            pickle.dump(NN_results, output)\n",
    "            output.close()\n",
    "        \n",
    "        return {'adas_r':adas_r, 'adas_mse':adas_mse, 'adas_rmse':adas_rmse, 'mmse_r':mmse_r, 'mmse_mse':mmse_mse, 'mmse_rmse':mmse_rmse,\n",
    "               'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        NN_results = computeSingleTaskPerfMetrics(fold_perf_dict, opt_metric, hype_configs)\n",
    "        opt_r = NN_results['opt_r'].values()        \n",
    "        opt_mse = NN_results['opt_mse'].values()        \n",
    "        opt_rmse = NN_results['opt_rmse'].values()        \n",
    "        opt_hyp = NN_results['opt_hype']\n",
    "        opt_snap = NN_results['opt_snap']\n",
    "        predicted_CV_scores = NN_results['predicted_CV_scores']\n",
    "        actual_CV_scores = NN_results['actual_CV_scores']\n",
    "        print 'opt corr: {}'.format(opt_r)\n",
    "        print 'opt mse: {}'.format(opt_mse)\n",
    "        print 'means: {}, {}'.format(np.mean(opt_r),np.mean(opt_mse))\n",
    "        print ''\n",
    "        print 'opt_snap: {}'.format(opt_snap)\n",
    "        print ''\n",
    "        \n",
    "        if save_multitask_results:\n",
    "            ts = time.time()\n",
    "            st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')        \n",
    "            save_path = save_path + '_' + st + '.pkl' \n",
    "            print 'saving results at: {}'.format(save_path)\n",
    "            output = open(save_path, 'wb')\n",
    "            pickle.dump(NN_results, output)\n",
    "            output.close()\n",
    "    \n",
    "        return {'opt_r':opt_r, 'opt_mse':opt_mse, 'opt_rmse':opt_rmse,'predicted_CV_scores':predicted_CV_scores,'actual_CV_scores':actual_CV_scores}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute ptimal hyp_config for each fold(based on inner_test)\n",
    "# if multi_task is set then compute hyp_config based on ADAS+MMSE perf (mse, r values)\n",
    "# if multi_task is set then ADAS and MMSE act_scores and pred_scores are wrapped in dictionaries \n",
    "# task_weights: dict of weights for each task --> only used in Clinical_Scale = BOTH\n",
    "# opt_metric = mse or corr\n",
    "def computePerfMetrics(fold_r, fold_euLoss, fold_act_scores, fold_pred_scores, opt_metric, hype_configs, Clinical_Scale,task_weights):\n",
    "    # First generate lists per fold with all hyp results\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    \n",
    "     # find optimal hyp_snp combination for each fold based on corr and euLoss\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "\n",
    "    opt_r_adas = {}\n",
    "    opt_mse_adas = {}\n",
    "    opt_rmse_adas = {}\n",
    "    actual_scores_adas = defaultdict(list)\n",
    "    opt_pred_scores_adas = defaultdict(list)\n",
    "\n",
    "    opt_r_mmse = {}\n",
    "    opt_mse_mmse = {}\n",
    "    opt_rmse_mmse = {}\n",
    "    actual_scores_mmse = defaultdict(list)\n",
    "    opt_pred_scores_mmse = defaultdict(list)\n",
    "\n",
    "    print 'Clinical_Scale: {}'.format(Clinical_Scale)\n",
    "    if not Clinical_Scale == 'BOTH':  #Does not produce all the metrics yet..\n",
    "        print 'This function does not work for single clinical scale models. Use the code from the next cell'\n",
    "    \n",
    "    else:\n",
    "        fold_r_ADAS = fold_r['ADAS']\n",
    "        fold_r_MMSE = fold_r['MMSE']\n",
    "        fold_euLoss_ADAS = fold_euLoss['ADAS']\n",
    "        fold_euLoss_MMSE = fold_euLoss['MMSE']\n",
    "        fid_r_perf_ADAS = defaultdict(list)\n",
    "        fid_r_perf_MMSE = defaultdict(list)\n",
    "        fid_euLoss_perf_ADAS = defaultdict(list)\n",
    "        fid_euLoss_perf_MMSE = defaultdict(list)\n",
    "        for hype_fid in fold_euLoss_ADAS.keys():\n",
    "            hype = int(hype_fid.split('_')[0][3])\n",
    "            fid = int(hype_fid.split('_')[1]) \n",
    "            \n",
    "            fid_r_perf_ADAS[fid].append(fold_r_ADAS[hype_fid])\n",
    "            fid_r_perf_MMSE[fid].append(fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf_ADAS[fid].append(fold_euLoss_ADAS[hype_fid])\n",
    "            fid_euLoss_perf_MMSE[fid].append(fold_euLoss_MMSE[hype_fid])\n",
    "            fid_hype_map[fid].append(hype)\n",
    "            \n",
    "            #Joint scores (weighted addition)\n",
    "            fid_r_perf[fid].append(task_weights['ADAS']*fold_r_ADAS[hype_fid] + task_weights['MMSE']*fold_r_MMSE[hype_fid])\n",
    "            fid_euLoss_perf[fid].append(task_weights['ADAS']*fold_euLoss_ADAS[hype_fid] + task_weights['MMSE']*fold_euLoss_MMSE[hype_fid])\n",
    "\n",
    "        fold_act_scores_adas = fold_act_scores['ADAS']\n",
    "        fold_act_scores_mmse = fold_act_scores['MMSE']\n",
    "        fold_pred_scores_adas = fold_pred_scores['ADAS']\n",
    "        fold_pred_scores_mmse = fold_pred_scores['MMSE']\n",
    "\n",
    "        for fid in fid_hype_map.keys():\n",
    "            r_perf_array_adas = np.array(fid_r_perf_ADAS[fid])\n",
    "            r_perf_array_mmse = np.array(fid_r_perf_MMSE[fid])\n",
    "            r_perf_array = np.array(fid_r_perf[fid])\n",
    "            euLoss_perf_array_adas = np.array(fid_euLoss_perf_ADAS[fid])\n",
    "            euLoss_perf_array_mmse = np.array(fid_euLoss_perf_MMSE[fid])\n",
    "            euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "            if opt_metric == 'euLoss':\n",
    "                h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "            else:\n",
    "                h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "\n",
    "            opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "            opt_snap[fid] = snp\n",
    "\n",
    "            opt_r_adas[fid] = r_perf_array_adas[h,snp]\n",
    "            opt_mse_adas[fid] = 2*euLoss_perf_array_adas[h,snp] #Convert back to MSE\n",
    "            opt_rmse_adas[fid] = np.sqrt(2*euLoss_perf_array_adas[h,snp]) #RMSE\n",
    "            opt_r_mmse[fid] = r_perf_array_mmse[h,snp] \n",
    "            opt_mse_mmse[fid] = 2*euLoss_perf_array_mmse[h,snp] #Convert back to MSE\n",
    "            opt_rmse_mmse[fid] = np.sqrt(2*euLoss_perf_array_mmse[h,snp]) #RMSE\n",
    "\n",
    "            actual_scores_adas[fid].append(fold_act_scores_adas[fid])\n",
    "            opt_pred_scores_adas[fid].append(fold_pred_scores_adas['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "            actual_scores_mmse[fid].append(fold_act_scores_mmse[fid])\n",
    "            opt_pred_scores_mmse[fid].append(fold_pred_scores_mmse['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp])\n",
    "\n",
    "        opt_ADAS = {'CV_r':opt_r_adas,'CV_MSE':opt_mse_adas,'CV_RMSE':opt_rmse_adas,'actual_CV_scores':actual_scores_adas,'predicted_CV_scores':opt_pred_scores_adas}\n",
    "        opt_MMSE = {'CV_r':opt_r_mmse,'CV_MSE':opt_mse_mmse,'CV_RMSE':opt_rmse_mmse,'actual_CV_scores':actual_scores_mmse,'predicted_CV_scores':opt_pred_scores_mmse}\n",
    "    \n",
    "    return {'opt_hype':opt_hype, 'opt_snap':opt_snap, 'opt_ADAS':opt_ADAS, 'opt_MMSE':opt_MMSE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal config based on inner_test\n",
    "def computeSingleTaskPerfMetrics(fold_perf_dict,opt_metric,hype_configs):\n",
    "    corrs = fold_perf_dict['fold_r']\n",
    "    euLosses = fold_perf_dict['fold_euLoss']\n",
    "    act_scores = fold_perf_dict['fold_act_scores']\n",
    "    pred_scores = fold_perf_dict['fold_pred_scores']\n",
    "\n",
    "    \n",
    "    NN_multitask_results = {}\n",
    "   \n",
    "    snap_array = np.arange(snap_start,niter+1,snap_start)\n",
    "\n",
    "    fid_hype_map = defaultdict(list)\n",
    "    fid_euLoss_perf= defaultdict(list)\n",
    "    fid_r_perf= defaultdict(list)\n",
    "    for hype_fid in fold_euLoss.keys():\n",
    "        hype = int(hype_fid.split('_')[0][3])\n",
    "        fid = int(hype_fid.split('_')[1])\n",
    "        fid_euLoss_perf[fid].append(fold_euLoss[hype_fid])\n",
    "        fid_r_perf[fid].append(fold_r[hype_fid])\n",
    "        fid_hype_map[fid].append(hype)\n",
    "\n",
    "    opt_r = {}\n",
    "    opt_mse = {}\n",
    "    opt_rmse = {}\n",
    "    opt_hype = {}\n",
    "    opt_snap = {}\n",
    "    actual_scores = {}\n",
    "    opt_pred_scores = {}\n",
    "\n",
    "    for fid in fid_hype_map.keys():\n",
    "        r_perf_array = np.array(fid_r_perf[fid])\n",
    "        euLoss_perf_array = np.array(fid_euLoss_perf[fid])\n",
    "\n",
    "        # if want to find best hyp from mse values\n",
    "        if opt_metric == 'euLoss':\n",
    "            h,snp = np.unravel_index(euLoss_perf_array.argmin(), euLoss_perf_array.shape)\n",
    "        else:\n",
    "            h,snp = np.unravel_index(r_perf_array.argmax(), r_perf_array.shape)\n",
    "            \n",
    "        eu_loss = euLoss_perf_array[h,snp]\n",
    "        r = r_perf_array[h,snp]        \n",
    "        opt_r[fid] = r\n",
    "        opt_mse[fid] = 2*eu_loss\n",
    "        opt_rmse[fid] = np.sqrt(2*eu_loss)\n",
    "        #print 'fid:{}, best hype:{}, snap: {}, euLoss:{}'.format(fid, fid_hype_map[fid][h],snap_array[snp],eu_loss)        \n",
    "        opt_snap[fid] = snp\n",
    "        opt_hype[fid] = hype_configs['hyp{}'.format(fid_hype_map[fid][h])]\n",
    "        actual_scores[fid] = fold_act_scores[fid]\n",
    "        opt_pred_scores[fid] = fold_pred_scores['hyp{}_{}'.format(fid_hype_map[fid][h],fid)][snp]\n",
    "\n",
    "    #print 'CV Perf: r:{}, mse:{}'.format(np.mean(opt_r.values()), np.mean(opt_mse.values()))\n",
    "    #print opt_r\n",
    "    #print opt_mse\n",
    "    return {'opt_r':opt_r,'opt_mse':opt_mse,'opt_rmse':opt_rmse,'opt_hype':opt_hype,'opt_snap':opt_snap, 'actual_scores':actual_scores,'opt_pred_scores':opt_pred_scores}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Net surgery FF pretrained weights\n",
    "* **Steps** \n",
    "    1. Copy weights from pre-trained models from individual modalities (HC,CT)\n",
    "    2. Use these weights as initializations for the HC+CT net (only lower layers for indvidual modalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Review new FF net params\n",
    "cohort = 'ADNI2'\n",
    "modality = 'HC_CT'\n",
    "pretrain_snap_HC = 20000 #ADNI1: 5000\n",
    "pretrain_snap_CT = 20000 #ADNI1: 5000\n",
    "n_folds = 10\n",
    "baseline_dir = '/projects/nikhil/ADNI_prediction/input_datasets/exp_data/NN/NN_MC/' \n",
    "#Custom snaps per fold\n",
    "#HC_iter = [8000,8000,8000,8000,6000,6000,8000,8000,8000,6000]\n",
    "#CT_iter = [12000,12000,12000,12000,12000,6000,6000,6000,6000,6000]\n",
    "#mc=1\n",
    "hc_hyp = 'hyp1'\n",
    "ct_hyp = 'hyp1'\n",
    "pretrain_hyp = 'hyp2' #This is hyp generated using optimal combination of hc and ct hyps. Prototxt file is saved with this hyp extension\n",
    "\n",
    "exp_name = 'Exp11_MC'\n",
    "\n",
    "for mc in np.arange(6,11,1):\n",
    "    for fid in np.arange(1,n_folds+1,1):\n",
    "        print 'fid: {}'.format(fid)\n",
    "        #for AE_branch in ['CT','R_HC','L_HC']:\n",
    "        for AE_branch in ['CT','HC']:\n",
    "            print 'AE_branch: {}'.format(AE_branch)\n",
    "\n",
    "            if AE_branch == 'L_HC':\n",
    "                params_FF = ['L_ff1', 'L_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'R_HC':\n",
    "                params_FF = ['R_ff1', 'R_ff2']            \n",
    "                AE_iter = 12000\n",
    "            elif AE_branch == 'HC':\n",
    "                params_FF = ['L_ff1','R_ff1']\n",
    "                AE_iter = pretrain_snap_HC\n",
    "                hyp = hc_hyp\n",
    "            elif AE_branch == 'CT':\n",
    "                #params_FF = ['ff1', 'ff2']\n",
    "                params_FF = ['ff1']\n",
    "                AE_iter = pretrain_snap_CT\n",
    "                hyp = ct_hyp\n",
    "                #fid for pretain is 1 because it's same definition for all the folds.\n",
    "                #Only use this during 1 of the modalities to avoid overwritting\n",
    "                print 'Spawning new net'\n",
    "                pretrain_net = caffe.Net(baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,pretrain_hyp,modality), caffe.TRAIN)\n",
    "            else:\n",
    "                print 'Wrong AE branch'\n",
    "\n",
    "            # conv_params = {name: (weights, biases)}\n",
    "            conv_params = {pr: (pretrain_net.params[pr][0].data, pretrain_net.params[pr][1].data) for pr in params_FF}\n",
    "\n",
    "            for conv in params_FF:\n",
    "                print 'target {} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)\n",
    "\n",
    "            # Review AE net params \n",
    "            #fid for pretain is 1 because it's same definition for all the folds.\n",
    "            net_file = baseline_dir + 'API/data/MC_{}/fold{}/ADNI_FF_train_{}_{}.prototxt'.format(mc,fid,hyp,AE_branch)\n",
    "            #model_file = baseline_dir + 'API/data/fold{}/train_snaps/AE_snaps/AE_{}_iter_{}.caffemodel'.format(fid,AE_branch,AE_iter) \n",
    "            model_file = baseline_dir + 'API/data/MC_{}/fold{}/train_snaps/{}_{}_{}_iter_{}.caffemodel'.format(mc,fid,exp_name,hyp,AE_branch,AE_iter) \n",
    "\n",
    "            AE_net = caffe.Net(net_file, model_file, caffe.TEST)\n",
    "            #params_AE = ['encoder1', 'code']\n",
    "            params_AE = params_FF #if you are using pretrained NN\n",
    "\n",
    "            # fc_params = {name: (weights, biases)}\n",
    "            fc_params = {pr: (AE_net.params[pr][0].data, AE_net.params[pr][1].data) for pr in params_AE}\n",
    "\n",
    "            for fc in params_AE:\n",
    "                print 'pretrained {} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)\n",
    "\n",
    "            #transplant net parameters\n",
    "            for pr, pr_conv in zip(params_AE, params_FF):\n",
    "                conv_params[pr_conv][0].flat = fc_params[pr][0].flat  # flat unrolls the arrays\n",
    "                conv_params[pr_conv][1][...] = fc_params[pr][1]\n",
    "\n",
    "            save_net = True\n",
    "            if save_net:\n",
    "                net_name = 'API/data/MC_{}/fold{}/pretrained_models/{}_ff_{}_{}_HC_snap_{}_CT_snap_{}_Sup_Concat.caffemodel'\n",
    "                save_path = baseline_dir + net_name.format(mc,fid,cohort,pretrain_hyp,modality,pretrain_snap_HC,pretrain_snap_CT)\n",
    "                print \"Saving net to \" + save_path\n",
    "                pretrain_net.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
